include/deal.II-translator/A-headers/automatic_and_symbolic_differentiation_0.txt
[0.x.0]*


* 
*  [2.x.0] 
* 

* 
*  [2.x.1]  to automatic and symbolic differentiation.
*  Below we provide a very brief introduction as to what automatic and symbolic differentiation are, what variations of these computational/numerical schemes exist, and how they are integrated within deal.II's framework. The purpose of all of these schemes is to automatically compute the derivative of functions, or approximations of it, in cases where one does not want to compute them by hand. Common examples are situations in the finite element context is where one wants to solve a nonlinear problem that is given by requiring that some residual  [2.x.2]  where  [2.x.3]  is a complicated function that needs to be differentiated to apply Newton's method; and situations where one is given a parameter dependent problem  [2.x.4]  and wants to form derivatives with regards to the parameters  [2.x.5] , for example to optimize an output functional with regards to  [2.x.6] , or for a sensitivity analysis with regards to  [2.x.7] . One should think of  [2.x.8]  as design parameters: say, the width or shape of a wing, the stiffness coefficients of a material chosen to build an object, the power sent to a device, the chemical composition of the gases sent to a burner. In all of these cases, one should think of  [2.x.9]  and  [2.x.10]  as [1.x.0] and cumbersome to differentiate
* 
*  -  at least when doing it by hand. A relatively simple case of a nonlinear problem that already highlights the tedium of computing derivatives by hand is shown in  [2.x.11] . However, in reality, one might, for example, think about problems such as chemically reactive flows where the fluid equations have coefficients such as the density and viscosity that depend strongly and nonlinearly on the chemical composition, temperature, and pressure of the fluid at each point; and where the chemical species react with each other based on reaction coefficients that also depend nonlinearly and in complicated ways on the chemical composition, temperature, and pressure. In many cases, the exact formulas for all of these coefficients can take several lines to write out, may include exponentials and (harmonic or geometric) averages of several nonlinear terms, and/or may contain table lookup of and interpolation between data points. Just getting these terms right is difficult enough; computing derivatives of these terms is impractical in most applications and, in reality, impossible to get right. Higher derivatives are even more impossible to do without computer aid. Automatic or symbolic differentiation is a way out of this: One only has to implement the function that computes these coefficients in terms of their inputs only once, and gets the (correct!) derivatives without further coding effort (though at a non-negligible computational cost either at run time, compile time, or both).
* 

* 

* 
*  [2.x.12]  auto_diff_1 Automatic differentiation
*  [1.x.1] (commonly also referred to as algorithmic differentiation), is a numerical method that can be used to "automatically" compute the first, and perhaps higher-order, derivatives of function(s) with respect to one or more input variables. Although this comes at a certain computational cost, the benefits to using such a tool may be significant. When used correctly the derivatives of often complicated functions can be computed to a very high accuracy. Although the exact accuracy achievable by these frameworks largely depends on their underlying mathematical formulation, some implementations compute with a precision on the order of machine accuracy. Note that this is different to classical numerical differentiation (using, for example, a finite difference approximation of a function by evaluating it at different points), which has an accuracy that depends on both the perturbation size as well as the chosen finite-difference scheme; the error of these methods is measurably larger than well-formulated automatic differentiation approaches.
*  Three practical examples of auto-differentiation use within a finite-element context would then be
* 

* 
* 
*  - the quick prototyping of a new nonlinear formulation without the need to hand-compute   the linearization itself,
* 

* 
* 
*  - automatic linearization of finite-element residuals additively formed within complex   multiphysics frameworks, and
* 

* 
* 
*  - verification of user-implementations of linearizations for both cell-based calculations   (e.g. a residual) and those based at a continuum point (e.g. tangents for nonlinear   constitutive laws).
*  There are quite a number of implementations for auto-differentiable numbers. They primarily fall into two broad categories, namely  [2.x.13] source code transformation [2.x.14]  and  [2.x.15] operator overloading [2.x.16] . The first method generates new, compilable code based on some input function that, when executed, returns the derivatives of the input function. The second exploits the capability of <tt>C++</tt> operator definitions to be overloaded for custom class types. Therefore  a class that represents such an auto-differentiable number can, following each mathematical operation performed on or with it, in principle evaluate and keep track of its value as well as that of its directional derivative(s). As the libraries exclusively implementing the  [2.x.17] source code transformation [2.x.18]  approach collectively describe highly specialized tools that are to be used as function preprocessors, they have no direct support within deal.II itself. The latter, however, represent specialized number types that can be supported through the use of template  metaprogramming in the appropriate context. Given the examples presented above, this means that the FEValues class (and friends), as well as the Tensor and SymmetricTensor classes should support calculations performed with these specialized numbers. (In theory an entire program could be made differentiable. This could be useful in, for example, the sensitivity analysis of solutions with respect to input parameters. However, to date this has not been tested.)
*  Implementations of specialized frameworks based on  [2.x.19] operator overloading [2.x.20]  typically fall into one of three categories. In each, some customized data classes representing the floating point value of an evaluated function and its derivative(s) by
* 

* 
* 
*  - exploiting  [2.x.21] dual [2.x.22] / [2.x.23] complex-step [2.x.24] / [2.x.25] hyper-dual [2.x.26]  formulations (occasionally    called  [2.x.27] tapeless [2.x.28]  methods),
* 

* 
* 
*  - those utilizing  [2.x.29] taping [2.x.30]  strategies, and
* 

* 
* 
*  - those using compile-time optimization through  [2.x.31] expression templates [2.x.32] .
*  To provide some tentative insight into how these various implementations might look like in practice, we offer the following generic summary of these approaches:
* 

* 
* 
*  - The first two  [2.x.33] tapeless [2.x.34]  approaches listed above (dual numbers and complex-step method) use some    variation of a truncated Taylor series, along with a particular choice of definition for the perturbation    parameter, to compute function derivatives using a finite-difference based approach. The "dual" number    constitutes the accumulated directional derivatives computed simultaneously as the function values are    evaluated; in the complex-step approach, the imaginary value effectively serves this purpose. The choice of    the perturbation parameter determines the numerical qualities of the scheme, such as the influence of the    truncation of the Taylor scheme; dual numbers do not contain any higher-order terms in their first derivative,    while for the complex-step method these existent higher-order terms are neglected. It can be shown that    both of these methods are not subject to subtractive cancellation errors and that, within their    finite-difference scheme, they are not numerically sensitive to the internal  [2.x.35] size chosen for the    numerical perturbation. The dual number approach thus produces exact first derivatives, while the    complex-step approximation does not. The standard implementation of the dual numbers, however, cannot yield    exact values for second derivatives. Hyper-dual numbers take a different view of this idea, with numbers    being represented in a form similar to quaternions (i.e. carrying additional non-real components) and the    derivatives being computed from a high-order truncation of the Taylor series all four components. The outcome    is that, with the appropriate implementation, both first and second derivatives can be computed exactly.
* 

* 
* 
*  - With  [2.x.36] taped [2.x.37]  approaches, a specified subregion of code is selected as one for which all    operations executed with active (marked) input variables are tracked and recorded in a data structure    referred to as a tape. At the end of the taped region, the recorded function(s) may be reevaluated    by "replaying" the tape with a different set of input variables instead of recomputing the function    directly. Assuming that the taped region represents a smooth function, arbitrarily high-order    derivatives of the function then can be computed by referring to the code path tracked and stored on    the tape.    (This could perhaps be achieved, for example, through evaluation of the function around the point    of interest.) There exist strategies to deal with situations where the taped function is not    smooth at the evaluated point, or if it is not analytic. Furthermore, one might need to consider the    case of branched functions, where the tape is no longer sequential, but rather forks off on a different    evaluation path to that due to the original recorded inputs.
* 

* 
* 
*  - Methods based on [1.x.2]    leverage the computational graph    (in this case, a [1.x.3]),    constructed from the abstract syntax tree (AST), that resolves the function output from its input values.    The outermost leaves on the tree represent the independent variables or constants, and are transformed by unary    operators and connected by binary operators (in the most simple case). Therefore, the operations performed on    the function inputs is known at compile time, and with that the associated derivative operation can also be defined    at the same time using the well-known rules of computing the derivative of an operation (such as    the associativity of derivatives under addition and subtraction, the product rule, and the chain    rule). The compiled output type returned by this operator need not be generic, but can rather be    specialized based on the specific inputs (possibly carrying a differential history) given to that specific    operator on the vertex of the DAG. In this way, a compile-time optimized set of instructions can be generated    for the very specialized individual operations used to evaluate each intermediate result of the dependent    function.
*  Each of these methods, of course, has its advantages and disadvantages, and one may be more appropriate than another for a given problem that is to be solved. As the aforementioned implementational details (and others not discussed) may be hidden from the user, it may still be important to understand the implications, run-time cost,  and potential limitations, of using any one of these "black-box" auto-differentiable numbers.
*  In addition to the supplied linked articles, resources used to furnish the details supplied here include:
* 

* 
* [1.x.4]
* 
* 

* 
* [1.x.5]
* 
*  ### Exploitation of the chain-rule
*  In the most practical sense, any of the above categories exploit the chain-rule to compute the total derivative of a composite function. To perform this action, they typically use one of two mechanisms to compute derivatives, specifically
* 

* 
* 
*  -  [2.x.38] forward-mode [2.x.39]  (or  [2.x.40] forward accumulation [2.x.41] ) auto-differentiation, or
* 

* 
* 
*  -  [2.x.42] reverse-mode [2.x.43]  (or  [2.x.44] reverse accumulation [2.x.45] ) auto-differentiation.
*  As a point of interest, the  [2.x.46] optimal Jacobian accumulation [2.x.47] , which performs a minimal set of computations, lies somewhere between these two limiting cases. Its computation for a general composite function remains an open problem in graph theory.
*  With the aid of the diagram below (it and some of the listed details courtesy of this [1.x.6]), let us think about the represention of the calculation of the function  [2.x.48]  and its derivatives:
*   [2.x.49]       [2.x.50]    </div>    [2.x.51]       [2.x.52]    </div> </div>
*  Specifically, we will briefly describe what forward and reverse auto-differentiation are. Note that in the diagram, along the edges of the graph in text are the directional derivative of function  [2.x.53]  with respect to the  [2.x.54] -th variable, represented by the notation  [2.x.55] . The specific computations used to render the function value and its directional derivatives for this example are tabulated in the [1.x.7]. For a second illustrative example, we refer the interested reader to [1.x.8].
*  Consider first that any composite function  [2.x.56] , here represented as having two independent variables, can be dissected into a composition of its elementary functions [1.x.9] As was previously mentioned, if each of the primitive operations  [2.x.57]  is smooth and differentiable, then the chain-rule can be universally employed to compute the total derivative of  [2.x.58] , namely  [2.x.59] . What distinguishes the "forward" from the "reverse" mode is how the chain-rule is evaluated, but ultimately both compute the total derivative [1.x.10]
*  In forward-mode, the chain-rule is computed naturally from the "inside out". The independent variables are therefore fixed, and each sub-function  [2.x.60]  is computed recursively and its result returned as inputs to the parent function. Encapsulating and fixing the order of operations using parentheses, this means that we compute [1.x.11] The computational complexity of a forward-sweep is proportional to that of the input function. However, for each directional derivative that is to be computed one sweep of the computational graph is required.
*  In reverse-mode, the chain-rule is computed somewhat unnaturally from the "outside in". The values of the dependent variables first get computed and fixed, and then the preceding differential operations are evaluated and multiplied in succession with the previous results from left to right. Again, if we encapsulate and fix the order of operations using parentheses, this implies that the reverse calculation is performed by [1.x.12] The intermediate values  [2.x.61]  are known as  [2.x.62] adjoints [2.x.63] , which must be computed and stored as the computational graph is traversed. However, for each dependent scalar function one sweep of the computational graph renders all directional derivatives at once.
*  Overall, the efficiency of each mode is determined by the number of independent (input) variables and dependent (output) variables. If the outputs greatly exceed the inputs in number, then forward-mode can be shown to be more efficient than reverse-mode. The converse is true when the number of input variables greatly exceeds that of the output variables. This point may be used to help inform which number type is most suitable for which set of operations are to be performed using automatic differentiation. For example, in many applications for which second derivatives are to be computed it is appropriate to combine both reverse- and forward-modes. The former would then typically be used to calculate the first derivatives, and the latter the second derivatives.
* 

* 
*  [2.x.64]  auto_diff_1_1 Supported automatic differentiation libraries
*  We currently have validated implementations for the following number types and combinations:
* 

* 
* 

* 
* 
*  - Taped ADOL-C (n-differentiable, in theory, but internal drivers for up to second-order    derivatives will be implemented)
* 

* 
* 

* 
* 
*  - Tapeless ADOL-C (once differentiable)
* 

* 
* 

* 
* 
*  - Forward-mode Sacado with dynamic memory allocation using expression templates (once differentiable)
* 

* 
* 

* 
* 
*  - Nested forward-mode Sacado using expression templates (twice differentiable)
* 

* 
* 

* 
* 
*  - Reverse-mode Sacado (once differentiable)
* 

* 
* 

* 
* 
*  - Nested reverse and dynamically-allocated forward-mode Sacado (twice differentiable, but results memory leak described in  [2.x.65] 
*  Note that in the above, "dynamic memory allocation" refers to the fact that the number of independent variables need not be specified at compile time.
*  The [1.x.13]
* 

* 
* [1.x.14]
* 
*  provides the principle insights into their taped and tapeless implementations, and how ADOL-C can be incorporated into a user code. Some further useful resources for understanding the implementation of ADOL-C, and possibilities for how it may be used within a numerical code, include:
* 

* 
* [1.x.15]
* 

* 
* [1.x.16]
* 

* 
* [1.x.17]
* 

* 
* [1.x.18]
* 
*  Similarly, a selection of useful resources for understanding the implementation of Sacado number types (in particular, how expression templating is employed and exploited) include:
* 

* 
* [1.x.19]
* 

* 
* [1.x.20]
* 

* 
* [1.x.21]
* 
*  The implementation of both forward- and reverse-mode Sacado numbers is quite intricate. As of Trilinos 12.12, the implementation of math operations involves a lot of preprocessor directives and macro programming. Accordingly, the code may be hard to follow and there exists no meaningful companion documentation for these classes. So, a useful resource for understanding the principle implementation of these numbers can be found at [1.x.22] that outlines a reference (although reportedly inefficient) implementation of a forward-mode auto-differentiable number that does not use expression templates. (Although not explicitly stated, it would appear that the  [2.x.66]  class is implemented in the spirit of dual numbers.)
* 

* 
*  [2.x.67]  auto_diff_1_2 How automatic differentiation is integrated into deal.II
*  Since the interface to each automatic differentiation library is so vastly different, a uniform internal interface to each number will be established in the near future. The goal will be to allow some driver classes (that provide the core functionality, and will later be introduced in the next section) to have a consistent mechanism to interact with different auto-differentiation libraries. Specifically, they need to be able to correctly initialize and finalize data that is to be interpreted as the dependent and independent variables of a formula.
*  A summary of the files that implement the interface to the supported auto-differentiable numbers is as follows:
* 

* 
* 
*  - ad_drivers.h: Provides classes that act as drivers to the interface of internally supported   automatic differentiation libraries. These are used internally as an intermediary to the   helper classes that we provide.
* 

* 
* 
*  - ad_helpers.h: Provides a set of classes to help perform automatic differentiation in a   number of different contexts. These are detailed in  [2.x.68] .
* 

* 
* 
*  - ad_number_types.h: Introduces an enumeration (called a type code) for the   auto-differentiable number combinations that will be supported by the driver classes.   The rationale behind the use of this somewhat restrictive mechanism is discussed below.
* 

* 
* 
*  - ad_number_traits.h: Declare some internal classes that are to be specialized for   each auto-differentiation library and/or number type. These are subsequently used to   provide a uniform interface to the classes through the NumberTraits and ADNumberTraits   classes which are extensively used throughout of drivers. We also provide some mechanisms   to easily query select properties of these numbers, i.e. some type traits.
* 

* 
* 
*  - adolc_math.h: Extension of the ADOL-C math operations that allow these numbers to be used   consistently throughout the library.
* 

* 
* 
*  - adolc_number_types.h: Implementation of the internal classes that define how we   use ADOL-C numbers.
* 

* 
* 
*  - adolc_product_types.h: Defines some product and scalar types that allow the use of   ADOL-C numbers in conjunction with the Tensor and SymmetricTensor classes.
* 

* 
* 
*  - sacado_math.h: Extension of the Sacado math operations that allow these numbers to be used   consistently throughout the library.
* 

* 
* 
*  - sacado_number_types.h: Implementation of the internal classes that define how we   use the supported Sacado numbers.
* 

* 
* 
*  - sacado_product_types.h: Defines some product and scalar types that allow the use of   the supported Sacado numbers in conjunction with the Tensor and SymmetricTensor   classes.
*  By using type codes for each supported number type, we artificially limit the type of auto-differentiable numbers that can be used within the library. This design choice is due to the fact that its not trivial to ensure that each number type is correctly initialized and that all combinations of nested (templated) types remain valid for all operations performed by the library. Furthermore, there are some lengthy functions within the library that are instantiated for the supported number types and have internal checks that are only satisfied when a auto-differentiable number, of which the library has knowledge, is used. This again ensures that the integrity of all computations is maintained. Finally, using a simple enumeration as a class template parameter ultimately makes it really easy to switch between the type used in production code with little to no further amendments required to user code.
*   [2.x.69]  auto_diff_1_3 User interface to the automatic differentiation libraries
*  The deal.II library offers a unified interface to the automatic differentiation libraries that we support. To date, the helper classes have been developed for the following contexts:
* 

* 
* 
*  - Classes designed to operate at the quadrature point level (or any general continuum point):
* 

* 
* 

* 
* 

* 
* 
*  -  [2.x.70]  %Differentiation of a scalar-valued function.       One typical use would be the the development of constitutive laws directly from a strain       energy function. An example of this exact use case is given in  [2.x.71] .
* 

* 
* 

* 
* 

* 
* 
*  -  [2.x.72]  %Differentiation of a vector-valued function.       This could be used to linearize the kinematic variables of a constitutive law, or assist       in solving the evolution equations of local internal variables.
* 

* 
* 
*  - Classes designed to operate at the cell level:
* 

* 
* 

* 
* 

* 
* 
*  -  [2.x.73]  %Differentiation of a scalar-valued energy functional,       such as might arise from variational formulations. An example of where this class is used       is in  [2.x.74] .
* 

* 
* 

* 
* 

* 
* 
*  -  [2.x.75]  %Differentiation of a vector-valued finite element       residual, leading to its consistent linearization.  [2.x.76]  also provides a demonstration       of how this class can be used.
*  Naturally, it is also possible for users to manage the initialization and derivative computations themselves.
*  The most up-to-date examples of how this is done using ADOL-C can be found in
* 

* 
* 
*  - their [1.x.23],
* 

* 
* 
*  - their [1.x.24], and
* 

* 
* 
*  - our [1.x.25],
*  while for Sacado, illustrative examples can be found in
* 

* 
* 
*  - their [1.x.26],
* 

* 
* 
*  - a [1.x.27], and
* 

* 
* 
*  - our [1.x.28].
* 

* 

* 
*  [2.x.77]  symb_diff_1 Symbolic expressions and differentiation
*  [1.x.29] is, in terms of its design and usage, quite different to automatic differentiation. Underlying any symbolic library is a computer algebra system (CAS) that implements a language and collection of algorithms to manipulate symbolic (or "string-like") expressions. This is most similar, from a philosophical point of view, to how algebraic operations would be performed by hand.
*  To help better distinguish between symbolic differentiation and numerical methods like automatic differentiation, let's consider a very simple example. Suppose that the function  [2.x.78] , where  [2.x.79]  and  [2.x.80]  are variables that are independent of one another. By applying the chain-rule, the derivatives of this function are simply  [2.x.81]  and  [2.x.82] . These are exactly the results that you get from a CAS after defining the symbolic variables `x` and `y`, defining the symbolic expression `f = pow(2x+1, y)` and computing the derivatives `diff(f, x)` and `diff(f, y)`. At this point there is no assumption of what `x` and `y` represent; they may later be interpreted as plain (scalar) numbers, complex numbers, or something else for which the power and natural logarithm functions are well defined. Obviously this means that there is also no assumption about which point to evaluate either the expression or its derivatives. One could readily take the expression for  [2.x.83]  and evaluate it at  [2.x.84]  and then later, with no recomputation of the derivative expression itself, evaluate it at  [2.x.85] . In fact, the interpretation of any symbolic variable or expression, and the inter-dependencies between variables, may be defined or redefined at any point during their manipulation; this leads to a degree of flexibility in computations that cannot be matched by auto-differentiation. For example, one could perform the permanent substitution  [2.x.86]  and then recompute  [2.x.87]  for several different values of  [2.x.88] . One could also post-factum express an interdependency between `x` and `y`, such as  [2.x.89] . For such a case, this means that the initially computed derivatives  [2.x.90]  and  [2.x.91]  truly represent partial derivatives rather than total derivatives. Of course, if such an inter-dependency was explicitly defined before the derivatives  [2.x.92]  and  [2.x.93]  are computed, then this could correspond to the total derivative (which is the only result that auto-differentiation is able to achieve for this example).
*  Due to the sophisticated CAS that forms the foundation of symbolic operations, the types of manipulations are not necessarily restricted to differentiation alone, but rather may span a spectrum of manipulations relevant to discrete differential calculus, topics in pure mathematics, and more. The documentation for the [1.x.30] library gives plenty of examples that highlight what a fully-fledged CAS is capable of. Through the  [2.x.94]  class, and the associated functions in the  [2.x.95]  namespace, we provide a wrapper to the high-performance [1.x.31] symbolic manipulation library that has enriched operator overloading and a consistent interface that makes it easy and "natural" to use. In fact, this class can be used as a "drop-in" replacement for arithmetic types in many situations, transforming the operations from being numeric to symbolic in nature; this is made especially easy when classes are templated on the underlying number type. Being focused on numerical simulation of PDEs, the functionality of the CAS that is exposed within deal.II focuses on symbolic expression creation, manipulation, and differentiation.
*  The convenience wrappers to SymEngine functionality are primarily focused on manipulations that solely involve dictionary-based (i.e., something reminiscent of "string-based") operations. Although SymEngine performs these operations in an efficient manner, they are still known to be computationally expensive, especially when the operations are performed on large expressions. It should therefore be expected that the performance of the parts of code that perform differentiation, symbolic substitution, etc.,  [2.x.96]  may be a limiting factor when using this in production code. deal.II therefore provides an interface to accelerate the evaluation of lengthy symbolic expression through the  [2.x.97]  class (itself often leveraging functionality provided by SymEngine). In particular, the  [2.x.98]  simultaneously optimizes a collection of symbolic expressions using methods such as common subexpression elimination (CSE), as well as by generating high performance code-paths to evaluate these expressions through the use of a custom-generated  [2.x.99]  or by compiling the expression using the LLVM JIT compiler. The usage of the  [2.x.100]  class is exemplified in  [2.x.101] .
*  As a final note, it is important to recognize the remaining major deficiencies in deal.II's current implementation of the interface to the supported symbolic library. The level of functionality currently implemented effectively limits the use of symbolic algebra to the traditional use case (i.e. scalar and tensor algebra, as might be useful to define constitutive relations or complex functions for application as boundary conditions or source terms). In fact,  [2.x.102]  demonstrates how it can be used to implement challenging constitutive models. In the future we will also implement classes to assist in performing assembly operations in the same spirit as that which has been done in the  [2.x.103]  namespace.
*  A summary of the files that implement the interface to the supported symbolic differentiable numbers is as follows:
* 

* 
* 
*  - symengine_math.h: Implementation of math operations that allow the class that implements   symbolic expressions to be used consistently throughout the library and in user code.   It provides counterpart definitions for many of the math functions found in the standard   namespace.
* 

* 
* 
*  - symengine_number_traits.h: Provides some mechanisms to easily query select properties of   symbolic numbers, i.e. some type traits.
* 

* 
* 
*  - symengine_number_types.h: Implementation of the  [2.x.104]  class that can   be used to represent scalar symbolic variables, scalar symbolic expressions, and more.   This Expression class has been given a full set of operators overloaded for all mathematical   and logical operations that are supported by the SymEngine library and are considered useful   within the context of numerical modeling.
* 

* 
* 
*  - symengine_optimizer.h: Implementation of the  [2.x.105]  class that   can be used to accelerate (in some cases, significantly) evaluation of the symbolic   expressions using an assortment of techniques.
* 

* 
* 
*  - symengine_product_types.h: Defines some product and scalar types that allow the use of symbolic   expressions in conjunction with the Tensor and SymmetricTensor classes.
* 

* 
* 
*  - symengine_scalar_operations.h: Defines numerous operations that can be performed either on or   with scalar symbolic expressions or variables.   This includes (but is not limited to) the creation of scalar symbols, performing differentiation   with respect to scalars, and symbolic substitution within scalar expressions.
* 

* 
* 
*  - symengine_tensor_operations.h: Defines numerous operations that can be performed either on or   with tensors of symbolic expressions or variables.   This includes (but is not limited to) the creation of tensors of symbols, performing   differentiation with respect to tensors of symbols, differentiation of tensors of symbols, and   symbolic substitution within tensor expressions.
* 

* 
* 
*  - symengine_types.h: Provides aliases for some types that are commonly used within the context of   symbolic computations.
* 

* 
* 
*  - symengine_utilities.h: Provides some utility functions that are useful within the context of   symbolic computations.

* 
* [0.x.1]

include/deal.II-translator/A-headers/c++_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  Since version 9.0, deal.II requires a compiler that supports at least [1.x.0]. As part of this, many places in the internal implementation of deal.II are now using features that were only introduced in C++11. That said, deal.II also has functions and classes that make using it with C++11 features easier.
*  One example is support for C++11 [1.x.1]. deal.II-based codes often have many loops of the kind

* 
* [1.x.2]
*  Using C++11's range-based for loops, you can now write this as follows:

* 
* [1.x.3]
*  This relies on functions such as  [2.x.1]  and equivalents in the DoF handler classes,  [2.x.2]   [2.x.3]  There are variants of these functions that provide iterator ranges for all cells (not just the active ones) and for cells on individual levels.
*  There are numerous other functions in the library that allow for the idiomatic use of range-based for loops. Examples are  [2.x.4]   [2.x.5]   [2.x.6]  among many others.
*  C++11 also introduces the concept of [constexpr](https://en.cppreference.com/w/cpp/language/constexpr) variables and function. The variables defined as `constexpr` are constant values that are computed during the compilation of the program and therefore have zero runtime cost associated with their initialization. Additionally, `constexpr` constants have properly defined lifetimes which prevent the so-called "static initialization order fiasco" completely. %Functions can be marked as `constexpr`, indicating that they can produce compile-time constant return values if their input arguments are constant expressions. Additionally, classes with at least one `constexpr` constructor can be initialized as `constexpr`.
*  As an example, since the constructor  [2.x.7]  array_type &) is `constexpr`, we can initialize a tensor with an array during compile time as:

* 
* [1.x.4]
*  Here, the contents of A are not stored on the stack. Rather, they are initialized during compile time and inserted into the `.data` portion of the executable program. The program can use these values at runtime without spending time for initialization. Initializing tensors can be simplified in one line.

* 
* [1.x.5]
*  Some functions such as determinant() are specified as `constexpr` but they require a compiler with C++14 capability. As such, this function is internally declared as:

* 
* [1.x.6]
*  The macro  [2.x.8]  simplifies to `constexpr` if a C++14-capable compiler is available. Otherwise, for old compilers, it ignores DEAL_II_CONSTEXPR altogether. Therefore, with newer compilers, the user can write

* 
* [1.x.7]
*  assuming `A` is declared with the `constexpr` specifier. This example shows the performance gains of using `constexpr` because here we performed an operation with  [2.x.9]  complexity during compile time, avoiding any runtime cost.

* 
* [0.x.1]*
 deal.II currently only requires a C++11-conforming compiler, but there are a number of functions and classes from the C++14 standard that are easy to provide also in case the compiler only supports C++11. These are collected in the current namespace.
*  The most notable example is the [1.x.8] function which is arguably an oversight for not having been included in C++11 (given that there is [1.x.9] in C++11).
*  There are other small additions in this namespace that allow us to use C++14 features at this point already, even though we don't require a C++14-compliant compiler.
* 

* 
*  [2.x.10]  If the compiler in use actually does support C++14, then the   contents of this namespace are simply imported classes and   functions from namespace `std`. That is, we fall back to what the   compiler provides, rather than our own implementations.

* 
* [0.x.2]*
 deal.II currently only requires a C++11-conforming compiler, but there are a number of functions and classes from the C++17 standard that are easy to provide also in case the compiler only supports C++11. These are collected in the current namespace.
*  The most notable example is the [1.x.10] class that was introduced to C++ starting with the C++17 standard.
*  There are other small additions in this namespace that allow us to use C++17 features at this point already, even though we don't require a C++17-compliant compiler.
* 

* 
*  [2.x.11]  If the compiler in use actually does support C++17, then the   contents of this namespace are simply imported classes and   functions from namespace `std`. That is, we fall back to what the   compiler provides, rather than our own implementations.

* 
* [0.x.3]*
 deal.II currently only requires a C++11-conforming compiler, but there are a number of functions and classes from the C++20 standard that are easy to provide also in case the compiler only supports C++11. These are collected in the current namespace.
*  One example is the [1.x.11] class that was introduced to C++ starting with the C++20 standard. It is used as the return type for the  [2.x.12]   [2.x.13]  and  [2.x.14]  functions, among others, to support range-based for loops (see  [2.x.15]  for examples of range-based for loops, as well as the documentation of the functions mentioned above).
*  There are other small additions in this namespace that allow us to use C++20 features at this point already, even though we don't require a C++20-compliant compiler.
* 

* 
*  [2.x.16]  If the compiler in use actually does support C++20, then the   contents of this namespace are simply imported classes and   functions from namespace `std`. That is, we fall back to what the   compiler provides, rather than our own implementations.

* 
* [0.x.4]

include/deal.II-translator/A-headers/coding_conventions_0.txt
[0.x.0]*
  [2.x.0] 
* Throughout deal.II, we strive to keep our programming style and the kind ofinterfaces we provide as consistent as possible. To this end, we have adopteda set of coding conventions that we attempt to follow wherever possible. Theyhave two parts: style issues, and something we call "defensive programming",the latter being an attempt to let our code help us find bugs.  When readingthrough them, it is important to remember that styles are not god given orbetter than any other set of conventions; their purpose is merely to keepdeal.II as uniform as possible. Uniformity reduces the number of bugs weproduce because we can, for example, always assume that input arguments comebefore output arguments of a function call. They also simplify reading codebecause some things become clear already by looking at the style a piece ofcode is written, without having to look up the exact definition of something.
* [1.x.0]
*  [2.x.1] deal.II uses  [2.x.2]  6.0 to normalize indentation. Astyle file is provided at
* [1.x.1]
* 
*  [2.x.3] Before a commit, you should run
* [1.x.2]
* on each of your files. This will make sure indentation is conforming to thestyle guidelines outlined in this page.
* This is cumbersome. Consequently, and more easily, you can just run
* [1.x.3]
* in whatever directory you set up the library to be compiled in, to indent allsource files that have been changed recently. If you want to make sure thatthe indenting is correct for all your commits, you might want to set up apre-commit hook. One way to do so, is to copy [2.x.4]  to [2.x.5]  and make sure it isexecutable.
* If the system you are working on has more than one version of [2.x.6]  installed (or if it is not in the path)you should replace the above  [2.x.7]  command with
* [1.x.4]
* to point to the correct executable. [2.x.8] 
* [1.x.5]
*  [2.x.9]  [2.x.10]  %Functions which return the number of something (number of cells,  degrees of freedom, etc) should start with  [2.x.11] . Example:   [2.x.12] 
*  [2.x.13]  %Functions which set a bit or flag should start with  [2.x.14] ;  functions which clear bits or flags should be named  [2.x.15] .  Example:  [2.x.16] 
*  [2.x.17]  Traditional logical operators should be used instead of their English  equivalents (i.e., use  [2.x.18]   instead of  [2.x.19] ).
*  [2.x.20]  In the implementation files, after each function, three empty lines are  expected to enable better readability. One empty line occurs in functions to  group blocks of code, since two empty lines are not enough to visibly  distinguish sufficiently that the code belongs to two different  functions. [2.x.21] 
*  [2.x.22]  Whenever an integer variable can only assume nonnegative values,  it is marked as unsigned. The same applies to functions that can only  return positive or zero values. Example:  [2.x.23] 
*  [2.x.24]  Whenever an argument to a function will not be changed, it should be marked  const, even if passed by value. Generally, we mark input parameters as  const. This aids as an additional documentation tool to clarify the  intent of a parameter (input, output, or both)  and lets the compiler issue warnings if such a parameter is  changed, which is often either involuntarily or poor style. [2.x.25] 
*  [2.x.26]  Whenever a function does not change any of the member variable of  the embedding class/object, it should be marked as const. [2.x.27] 
*  [2.x.28]  %Function and variable names may not consist of only one or two  letters, unless the variable is a pure counting index. [2.x.29] 
*  [2.x.30]  Type aliases ( [2.x.31] -declarations) are preferred to   [2.x.32] -declarations. [2.x.33] 
*  [2.x.34]  Use the geometry information in GeometryInfo to get the  number of faces per cell, the number of children per cell, the  child indices of the child cells adjacent to face 3, etc, rather  than writing them into the code directly as  [2.x.35] ,   [2.x.36]  and   [2.x.37] . This reduces the possibilities for errors and enhances  readability of code. [2.x.38] 
*  [2.x.39]  The layout of class declarations is the following: first the  block of public functions, beginning with the constructors, then  the destructors. If there are public member variables, these have  to occur before the constructor. Public variables shall only be  used if constant (in particular if they are static and constant)  or unavoidable.   [2.x.40]   After the public members, the protected and finally the private  members are to be listed. The order is as above: first variables  then functions.   [2.x.41]   Exceptions shall be declared at the end of the public section  before the non-public sections start.   [2.x.42]   We do not use the C++11-style class member initialization for member variables  that are neither  [2.x.43] ;  i.e., instead of
* [1.x.6]
*   write
* [1.x.7]
*    [2.x.44] 
*  [2.x.45]  If a function has both input and output parameters, usually the  input parameters shall precede the output parameters, unless there  are good reasons to change this order. (The most common reason is trailing  input parameters with default values.)  [2.x.46] 
*  [2.x.47]  Exceptions are used for %internal parameter checking and for  consistency checks through the Assert macro. Exception handling  like done by the C++ language ( [2.x.48] , and using the  AssertThrow macro) are used to  handle run time errors (like I/O failures) which must be on  in any case, not only in debug mode. [2.x.49] 
*  [2.x.50]  Sometimes it makes sense to implement a class by using several  non-member functions that are not part of the public interface and are only  meant to be called in the current source file. Such free functions should be  put in an internal namespace structured in the following way: 
* [1.x.8]
*   where  [2.x.51]  is the name of the calling class.
*  [2.x.52]  Classes, namespaces and types generally are named using uppercase letters  to denote word beginnings (e.g. TriaIterator) &mdash; sometimes called  [1.x.9][1.x.10] &mdash; while functions and variables  use lowercase letters and underscores to separate words.  The only exception are the iterator alias in Triangulation  and DoFHandler (named cell_iterator, active_line_iterator, etc)  to make the connection to the standard library container classes clear. [2.x.53] 
*  [2.x.54]  For classes with multiple template arguments, the dimension is usually  put before the data type specifier, i.e., we use Point<dim,number> and not  Point<number,dim>.
*  [2.x.55]  There are several places in deal.II where we use forward declarations in  header files. The reason for this is that we can, hopefully, improve  compilation speeds by not using headers when we just need to mark a certain  type as an argument to a function. The convention used in deal.II is that, if  all we need is a type name, then the type may be forward declared in the  header where we need it; if a function (or member function) can return a value  then a declaration of that value's type should be available (by including the  necessary header). For example,  [2.x.56]   includes  [2.x.57]  so that one can write  something like  [2.x.58]  without  explicitly including the header declaring the type of the object returned by   [2.x.59] .
*  [2.x.60]  Each class has to have at least 200 pages of documentation ;-) [2.x.61] 
*  [2.x.62] 
* 

* [1.x.11]
*  [2.x.63] The majority of classes and functions in deal.II are templated. This brings aquestion of how and where such objects are instantiated, if at all. Throughoutdeal.II we adopt the following convention: [2.x.64] 
*  [2.x.65] 
*  [2.x.66]  If we can enumerate all possible template arguments (e.g., the dimensioncan only be 1, 2, or 3), then a function template goes into the  [2.x.67] file and we explicitly instantiate all possibilities. Users will not have anyneed to ever see these function templates because they will not want toinstantiate these functions for any other template arguments anyway.  [2.x.68] 
*  [2.x.69]  If we can not enumerate all possible template arguments (e.g., vectortypes
* 
*  -  because users might want to define their own vector kinds) but atleast know a few common usage cases, then the function is put into a [2.x.70]  fileand instantiate the functions for all of the common arguments. For almost allusers, this will be just fine
* 
*  -  they only use the (vector, matrix, ...) typeswe already instantiate, and for them the  [2.x.71]  file will notbe of any interest. It will also not slow down their compilations becausenothing they see will \#include the  [2.x.72]  file. But users whodefine their own (vector, matrix, ...) types can instantiate the templatefunctions with their own user-defined types by including the [2.x.73]  files.
*  [2.x.74]  Finally, if we can not assume in advance which values template argumentswill take (e.g., any class derived from Subscriptor can be used as an argument),the definitions of functions are provided at the bottom of the headerfile with declarations. The definitions should be guarded with <code>\#ifndefDOXYGEN ... \#endif</code> to prevent Doxygen from picking them up. [2.x.75] 
*  [2.x.76] 
*  [2.x.77]  For the first two cases, instantiation instructions are defined in [2.x.78]  files. They are processed by a binary calledexpand_instantiations (built from [2.x.79] ) and the parameters aredefined dynamically through cmake depending on your configuration (see [2.x.80]  in your build directory).It is those  [2.x.81]  files that are eventually included from thecorresponding  [2.x.82]  files.  [2.x.83] 
* 

* [1.x.12]
*  [2.x.84]  Defensive programming is a term that we use frequently when we talk aboutwriting code while in the mindset that errors will happen. Here, errors cancome in two ways: first, I can make a mistake myself while writing afunctions; and secondly, someone else can make a mistake while calling myfunction. In either case, I would like to write my code in such a way thaterrors are (i) as unlikely as possible, (ii) that the compiler can alreadyfind some of the mistakes, and (iii) that the remaining mistakes arerelatively easy to find, for example because the program aborts. Defensiveprogramming is then a set of strategies that make these goals more likely. [2.x.85] 
*  [2.x.86] Over time, we have learned a number of techniques to this end, some of whichwe list here: [2.x.87]  [2.x.88]  [1.x.13] People call functions with wrong  or nonsensical parameters, all the time. As the prototypical example,  consider a trivial implementation of vector addition: 
* [1.x.14]
*   While correct, this function will get into trouble if the two vectors  do not have the same size. You think it is silly to call this function  with vectors of different size? Yes, of course it is. But it happens  all the time: people forget to reinitialize a vector, or it is reset in  a different function, etc. It happens. So if you are in such an unlucky  case, it can take a long time to figure out what's going on because  you are likely to just read uninitialized memory, or maybe you are  writing to memory the  [2.x.89]  vector doesn't actually own.  Neither is going to lead to immediate termination of the program,  but you'll probably get random errors at a later time. It would be  much easier if the program just stopped here right away. The following  implementation will do exactly this: 
* [1.x.15]
*   The  [2.x.90]  macro ensures that the condition is true  at run time, and otherwise prints a string containing information  encoded by the second argument and aborts the program. This way,  when you write a new program that happens to call this function,  you will learn of your error right away and have the opportunity  to fix it without ever having to seriously debug anything.   [2.x.91]   As a general guideline, whenever you implement a new function,  think about the [1.x.16] on parameter, i.e. what does the  function expect to be true about each of them, or their combination.  Then write assertions for all of these preconditions. This may be  half a dozen assertions in some cases but remember that each assertion  is a potential bug already found through trivial means.   [2.x.92]   In a final note, let us remark that assertions are of course expensive:  they may make a program 3 or 5 times slower when you link it against  the debug version of the library. But if you consider your [1.x.17]  development time, the ability to find bugs quickly probably far outweighs  the time you spend waiting for your program to finish. Furthermore,  calls to the Assert macro are removed from the program in optimized mode  (which you presumably only use once you know that everything runs just  fine in debug mode. The optimized libraries are faster by a factor of  3-5 than the debug libraries, at the price that it's much harder to find  bugs.   [2.x.93] 
*  [2.x.94]  [1.x.18] If a function computes something  non-trivial there may be a bug in the code. To find these, use  postconditions: just like you have certain knowledge about useful values  for input parameters, you have knowledge about what you expect possible  return values to be. For example, a function that computes the norm of  a vector would expect the norm to be positive. You can write this as  follows: 
* [1.x.19]
*   This function is too simple to really justify this assertion, but imagine  the computation to be lengthier and you can see how the assertion helps  you ensure (or [1.x.20]) yourself against mistakes. Note that one  could argue that the assertion should be removed once we've run the program  a number of times and found that the condition never triggers. But it's  better to leave it right where it is: it encodes for the future (and for  readers) knowledge you have about the function; if someone comes along  and replaced the implementation of the function by a more efficient  algorithm, the assertion can help make sure that the function continues  to do what it is supposed to do.   [2.x.95] 
*  [2.x.96]  [1.x.21] In a similar vein, if you have a  complex algorithm, use assertions to ensure that your mental model  of what is going on matches what is indeed true. For example, assume  you are writing a function that ensures that mesh sizes do not change  too much locally. You may end up with a code of the following kind: 
* [1.x.22]
*   The conditions that got us into the else-branch may be  complicated, and while it may be true that we believed that the  only possibility we got here is that the neighbor is at the boundary,  there may have been a bug in our implementation. There may also have been  a bug in our thinking, or someone changes the code way above in the same  function and forgets about the issue here, or a change at a completely  different location in the library makes the assumption untenable. In  all of these cases, the explicit statement of our assertion makes sure  that these problems are easily found.   [2.x.97] 
*  [2.x.98]  [1.x.23]  Traditional C required that variables are declared at the beginning of  the function even if they are only used further below. This leads to  code like this that we may imagine in a 1d code: 
* [1.x.24]
*   The problem is that if the code between the declaration and initialization  is long and complicated, you can't look up on one page what the type of  a variable is and what it's value may be. In fact, it may not even be  quite clear that the variable is used initialized at all, or whether it  is accidentally left uninitialized.   [2.x.99]   A better way to do this would be as follows: 
* [1.x.25]
*   This makes it much clearer what the type of the variable is  and that it is in fact only ever used when initialized. Furthermore,  if someone wants to read the code to see what the variable is in fact  doing, declaring and initializing it in the innermost possible scope  makes this task easier: we don't have to look upwards for it beyond  the declaration, and we don't have to look downward beyond the end  of the current scope since this is where the variable dies.   [2.x.100]   As a final note, it is clear that you can only do this sort of stuff  for variables that completely live on the stack without allocating memory  on the heap. Within deal.II, this is only true for builtin types like   [2.x.101] , etc, as well as the Point and Tensor  classes. Everything else has something like a  [2.x.102]   as a member variable, which requires memory allocation &mdash; you don't  want to declare these inside loops, at least not if the loop is  traversed frequently.   [2.x.103] 
*  [2.x.104]  [1.x.26] To pick up on the example above, note  that in most cases we will never change the variable so initialized  any more. In other words, if this is the case, we may as well write  things as follows: 
* [1.x.27]
*   By marking the variable as constant we make sure that we don't accidentally  change it. For example, the compiler could catch code like this: 
* [1.x.28]
*   This was most likely meant to be a  [2.x.105]  rather than an  assignment. By marking the variable as const, the compiler would have  told us about this bug. Maybe equally importantly, human readers of the  code need not look further down whether the value of the variable may  actually be changed somewhere between declaration and use &mdash; it  can't be if it is marked as const.   [2.x.106] 
*  [2.x.107]  [1.x.29] The same essentially  holds true as well as for function arguments: If you have no intention  of changing a variable (which is typically the case for input arguments),  then mark it as constant. For example, the following function should take  its argument as a constant value: 
* [1.x.30]
*   Here, the user calls  [2.x.108] , for example. There really  is no reason why the function would ever want to change the value of  the  [2.x.109]  argument &mdash; so mark it as constant:  this both helps the reader of the code understand that this is an  input argument of the function for which we need not search below whether  it is ever changed, and it helps the compiler help us finding bugs if  we ever accidentally change the value. [2.x.110] 
* 

* 
* [0.x.1]

include/deal.II-translator/A-headers/concepts_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  Sometimes imposing constraints on the type of an object without requiring it to belong to a specific inheritance hierarchy is useful. These are usually referred to as  [2.x.1] concepts [2.x.2]  in the C++ community. This module lists the concepts commonly used in deal.II with brief descriptions of their intent. The convention in deal.II for listing constraints on a type is to provide the name of the concept as a  [2.x.3]  in a template: for example, the type of a Vector depends on the type of the underlying field, and so it is defined as a template:

* 
* [1.x.0]
*  The point here is that you are creating a vector that can store elements of type  [2.x.4]  But there are some underlying assumptions on this. For example, the deal.II Vector class is not intended to be used just as a collection (unlike  [2.x.5] ) but defines vector space operations such as addition of vectors, or the norm of vectors. Consequently, the data type users can specify for  [2.x.6]  must satisfy certain conditions (i.e., it must conform to or "model" a "concept"): Specifically, the type must denote objects that represent the elements of what mathematically call a "field" (which you can think of as, well, "numbers": things we can add, multiply, divide, take the absolute value of, etc). The point of a concept is then to describe  [2.x.7] what conditions a type must satisfy [2.x.8]  to be a valid template argument in a given context.
*  This page describes these conditions for a number of concepts used throughout deal.II. Specifically, in the example above, the  [2.x.9]  "Number concept" discussed below describes the types that could be used as argument for the Vector class.
*  Concepts have been proposed as a language extension to C++ for a long time already. They would allow us to describe that a class or function has certain properties in order to be a qualified template argument. For example, it would allow us to express in C++ code that the first argument to, say,  [2.x.10]  must have a type that represents an actual mesh
* 
*  -  which we can currently only describe in words, see below. Using C++ concepts would allow us to describe this in code and trying to call such a function with an object as first argument that is not, in fact, a mesh would yield a compiler error that makes the mismatch clear.
*  Unfortunately, these proposals to C++ have never made it into any official C++ standard; they are proposed for C++20 however. We may start to use them once the vast majority of our users have compilers that support this standard.
*  More information on the topic can be found at [1.x.1].
* 

*   [2.x.11] 
*  <dt class="concepts"> [2.x.12]  ConceptDoFHandlerType [1.x.2]</dt>
*   [2.x.13]  deal.II includes both DoFHandler and  [2.x.14]  as objects which manage degrees of freedom on a mesh. Though the two do not share any sort of inheritance relationship, they are similar enough that many functions just need something which resembles a DoFHandler to work correctly.  [2.x.15] 
*  <dt class="concepts"> [2.x.16]  ConceptMatrixType [1.x.3]</dt>
*   [2.x.17]  Many functions and classes in deal.II require an object which knows how to calculate matrix-vector products (the member function  [2.x.18] ), transposed matrix-vector products (the member function  [2.x.19] ), as well as the `multiply and add' equivalents  [2.x.20] . Some functions only require  [2.x.21] , but an object should implement all four member functions if the template requires a MatrixType argument. Writing classes that satisfy these conditions is a sufficiently common occurrence that the LinearOperator class was written to make things easier; see  [2.x.22]  for more information.
*  One way to think of  [2.x.23]  is to pretend it is a base class with the following signature (this is nearly the interface provided by SparseMatrix):
* 

* 
* [1.x.4]
* 
*  Template functions in C++ cannot be virtual (which is the main reason why this approach is not used in deal.II), so implementing this interface with inheritance will not work, but it is still a good way to think about this template concept. One can use the LinearOperator class to implement  [2.x.24]  instead of implementing them manually.  [2.x.25] 
*  <dt class="concepts"> [2.x.26]  ConceptMeshType [1.x.5]</dt>
*   [2.x.27]  Meshes can be thought of as arrays of vertices and connectivities, but a more fruitful view is to consider them as [1.x.6]. In C++, collections are often called [1.x.7] (typical containers are  [2.x.28]   [2.x.29]  etc.) and they are characterized by the ability to iterate over the elements of the collection. The <tt>MeshType</tt> concept refers to any container which defines appropriate methods (such as  [2.x.30]  and <tt>typedefs</tt> (such as  [2.x.31]  for managing collections of cells.
*  Instances of Triangulation, DoFHandler, and  [2.x.32]  may all be considered as containers of cells. In fact, the most important parts of the public interface of these classes consists simply of the ability to get iterators to their elements. Since these parts of the interface are generic, i.e., the functions have the same name in all classes, it is possible to write operations that do not actually care whether they work on a triangulation or a DoF handler object. Examples abound, for example, in the GridTools namespace, underlining the power of the abstraction that meshes and DoF handlers can all be considered simply as collections (containers) of cells.
*  On the other hand, meshes are non-standard containers unlike  [2.x.33]  or  [2.x.34]  in that they can be sliced several ways. For example, one can iterate over the subset of active cells or over all cells; likewise, cells are organized into levels and one can get iterator ranges for only the cells on one level. Generally, however, all classes that implement the containers-of-cells concept use the same function names to provide the same functionality.
*  %Functions that may be called with either class indicate this by accepting a template parameter like

* 
* [1.x.8]
*  or

* 
* [1.x.9]
*  The classes that satisfy this concept are collectively referred to as  [2.x.35] mesh classes [2.x.36] . The exact definition of <tt>MeshType</tt> relies a lot on library internals, but it can be summarized as any class with the following properties:  [2.x.37]     [2.x.38] A <tt>typedef</tt> named <tt>active_cell_iterator</tt>.    [2.x.39]     [2.x.40] A method <tt>get_triangulation()</tt> which returns a reference to   the underlying geometrical description (one of the Triangulation classes)   of the collection of cells. If the mesh happens to be a Triangulation,   then the mesh just returns a reference to itself.    [2.x.41]     [2.x.42] A method <tt>begin_active()</tt> which returns an iterator pointing   to the first active cell.    [2.x.43]     [2.x.44] A static member value <tt>dimension</tt> containing the dimension in       which the object lives.    [2.x.45]     [2.x.46] A static member value <tt>space_dimension</tt> containing the dimension       of the object (e.g., a 2D surface in a 3D setting would have       <tt>space_dimension = 2</tt>).    [2.x.47]   [2.x.48]   [2.x.49] 
*  <dt class="concepts"> [2.x.50]  ConceptNumber [1.x.10]</dt>
*   [2.x.51]  This concept describes scalars which make sense as vector or matrix entries, which is usually some finite precision approximation of a field element. The canonical examples are  [2.x.52]  and  [2.x.53]  for floating point type  [2.x.54]  in many places as well.  [2.x.55] 
*  <dt class="concepts"> [2.x.56]  ConceptPolynomialType [1.x.11]</dt>
*   [2.x.57]  See the description in  [2.x.58]  for more information. In some contexts, anything that satisfies the interface resembling

* 
* [1.x.12]
* 
*  may be considered as a polynomial for the sake of implementing finite elements.  [2.x.59] 
*  <dt class="concepts"> [2.x.60]  ConceptPreconditionerType [1.x.13]</dt>
*   [2.x.61]  This is essentially a synonym for  [2.x.62] , but usually only requires that  [2.x.63]  be defined. Most of the time defining  [2.x.64]  is not necessary. One should think of  [2.x.65]  as applying some approximation of the inverse of a linear operator to a vector, instead of the action of a linear operator to a vector, for the preconditioner classes.  [2.x.66] 
*  <dt class="concepts"> [2.x.67]  ConceptRelaxationType [1.x.14]</dt>
*   [2.x.68]  This is an object capable of relaxation for multigrid methods. One can think of an object satisfying this constraint as having the following interface as well as the constraints required by  [2.x.69]  "MatrixType":

* 
* [1.x.15]
*  where these two member functions perform one step (or the transpose of such a step) of the smoothing scheme. In other words, the operations performed by these functions are  [2.x.70]  and  [2.x.71] .  [2.x.72] 
*  <dt class="concepts"> [2.x.73]  ConceptSparsityPatternType [1.x.16]</dt>
*   [2.x.74]  Almost all functions (with the notable exception of  [2.x.75]  which take a sparsity pattern as an argument can take either a regular SparsityPattern or a DynamicSparsityPattern, or even one of the block sparsity patterns. See  [2.x.76]  for more information.  [2.x.77] 
*  <dt class="concepts"> [2.x.78]  ConceptStreamType [1.x.17]</dt>
*   [2.x.79]  Deriving new stream classes in C++ is well-known to be difficult. To get around this, some functions accept a parameter which defines  [2.x.80] , which allows for easy output to any kind of output stream.  [2.x.81] 
*  <dt class="concepts"> [2.x.82]  ConceptVectorType [1.x.18]</dt>
*   [2.x.83]  deal.II supports many different vector classes, including bindings to vectors in other libraries. These are similar to standard library vectors (i.e., they define  [2.x.84] ,  [2.x.85] ) but also define numerical operations like  [2.x.86] . Some examples of VectorType include Vector,  [2.x.87]  and BlockVector.  [2.x.88] 
*   [2.x.89] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/constraints_0.txt
[0.x.0]*


* 
*  [2.x.0] 

* 
*  [2.x.1] 
*  This module deals with constraints on degrees of freedom. The central class to deal with constraints is the AffineConstraints class.
*  Constraints typically come from several sources, for example:
* 

* 
* 
*  - If you have Dirichlet-type boundary conditions,  [2.x.2] ,   one usually enforces   them by requiring that degrees of freedom on the boundary have   particular values, for example  [2.x.3]  if the boundary condition    [2.x.4]  requires that the finite element solution  [2.x.5]    at the location of degree   of freedom 12 has the value 42. Such constraints are generated by   those versions of the  [2.x.6]    function that take a AffineConstraints argument (though there are   also other ways of dealing with Dirichlet conditions, using    [2.x.7]  see for example  [2.x.8]  and  [2.x.9] ).
* 

* 
* 
*  - If you have boundary conditions that set a certain part of the   solution's value, for example no normal flux,  [2.x.10]  (as happens in flow problems and is handled by the    [2.x.11]  function) or   prescribed tangential components,  [2.x.12]  (as happens in electromagnetic problems and   is handled by the  [2.x.13]    function). For the former case, imagine for example that we are at   at vertex where the normal vector has the form  [2.x.14]  and that the  [2.x.15] -,  [2.x.16] - and  [2.x.17] -components of the flow   field at this vertex are associated with degrees of freedom 12, 28,   and 40. Then the no-normal-flux condition means that we need to have   the condition  [2.x.18] .   The prescribed tangential component leads to similar constraints   though there is often something on the right hand side.
* 

* 
* 
*  - If you have hanging node constraints, for example in a mesh like this:         [2.x.19]    Let's assume the bottom right one of the two red degrees of freedom   is  [2.x.20]  and that the two yellow neighbors on its left and right   are  [2.x.21]  and  [2.x.22] . Then, requiring that the finite element   function be continuous is equivalent to requiring that  [2.x.23] . A similar situation occurs in the   context of hp-adaptive finite element methods.   For example, when using Q1 and Q2 elements (i.e. using   FE_Q(1) and FE_Q(2)) on the two marked cells of the mesh        [2.x.24]    there are three constraints: first  [2.x.25] ,   then  [2.x.26] , and finally the identity    [2.x.27] . Similar constraints occur as hanging nodes even if all   cells used the same finite elements. In all of these cases, you   would use the  [2.x.28]  function to   compute such constraints.
* 

* 
* 
*  - Other linear constraints, for example when you try to impose a certain   average value for a problem that would otherwise not have a unique   solution. An example of this is given in the  [2.x.29]  tutorial program.
*  In all of these examples, constraints on degrees of freedom are linear and possibly inhomogeneous. In other words, they always have the form  [2.x.30] . The deal.II class that deals with storing and using these constraints is AffineConstraints.
* 

*  [1.x.0]
*  When building the global system matrix and the right hand sides, one can build them without taking care of the constraints, i.e. by simply looping over cells and adding the local contributions to the global matrix and right hand side objects. In order to do actual calculations, you have to 'condense' the linear system: eliminate constrained degrees of freedom and distribute the appropriate values to the unconstrained dofs. This changes the sparsity pattern of the sparse matrices used in finite element calculations and is thus a quite expensive operation. The general scheme of things is then that you build your system, you eliminate (condense) away constrained nodes using the  [2.x.31]  functions, then you solve the remaining system, and finally you compute the values of constrained nodes from the values of the unconstrained ones using the  [2.x.32]  function. Note that the  [2.x.33]  function is applied to matrix and right hand side of the linear system, while the  [2.x.34]  function is applied to the solution vector.
*  This scheme of first building a linear system and then eliminating constrained degrees of freedom is inefficient, and a bottleneck if there are many constraints and matrices are full, i.e. especially for 3d and/or higher order or hp-finite elements. Furthermore, it is impossible to implement for %parallel computations where a process may not have access to elements of the matrix. We therefore offer a second way of building linear systems, using the  [2.x.35]  and  [2.x.36]  functions discussed below. The resulting linear systems are equivalent to those one gets after calling the  [2.x.37]  functions.
* 

* 
*  [2.x.38]  Both ways of applying constraints set the value of the matrix diagonals to constrained entries to a [1.x.1] entry of the same magnitude as the other entries in the matrix. As a consequence, you need to set up your problem such that the weak form describing the main matrix contribution is not [1.x.2]. Otherwise, iterative solvers such as CG will break down or be considerably slower as GMRES.
* 

* 
*  [2.x.39]  While these two ways are [1.x.3], i.e., the solution of linear systems computed via either approach is the same, the linear systems themselves do not necessarily have the same matrix and right hand side vector entries. Specifically, the matrix diagonal and right hand side entries corresponding to constrained degrees of freedom may be different as a result of the way in which we compute them; they are, however, always chosen in such a way that the solution to the linear system is the same.
*  [1.x.4]
*  As mentioned above, the first way of using constraints is to build linear systems without regards to constraints and then "condensing" them away. Condensation of a matrix is done in four steps:
* 

* 
* 
*  - first one builds the   sparsity pattern (e.g. using  [2.x.40] 
* 

* 
* 
*  - then the sparsity pattern of the condensed matrix is made out of   the original sparsity pattern and the constraints;
* 

* 
* 
*  - third, the global matrix is assembled;
* 

* 
* 
*  - and fourth, the matrix is finally condensed.
*  In the condensation process, we are not actually changing the number of rows or columns of the sparsity pattern, matrix, and vectors. Instead, the condense functions add nonzero entries to the sparsity pattern of the matrix (with constrained nodes in it) where the condensation process of the matrix will create additional nonzero elements. In the condensation process itself, rows and columns subject to constraints are distributed to the rows and columns of unconstrained nodes. The constrained degrees of freedom remain in place. In order not to disturb the solution process, these rows and columns are filled with zeros and an appropriate positive value on the main diagonal (we choose an average of the magnitudes of the other diagonal elements, so as to make sure that the new diagonal entry has the same order of magnitude as the other entries; this preserves the scaling properties of the matrix). The corresponding value in the right hand sides is set to zero. This way, the constrained node will always get the value zero upon solution of the equation system and will not couple to other nodes any more.
*  Keeping the entries in the matrix has the advantage over creating a new and smaller matrix, that only one matrix and sparsity pattern is needed thus less memory is required. Additionally, the condensation process is less expensive, since not all but only constrained values in the matrix have to be copied. On the other hand, the solution process will take a bit longer, since matrix vector multiplications will incur multiplications with zeroes in the lines subject to constraints. Additionally, the vector size is larger, resulting in more memory consumption for those iterative solution methods using a larger number of auxiliary vectors (e.g. methods using explicit orthogonalization procedures). Nevertheless, this process is more efficient due to its lower memory consumption.
*  The condensation functions exist for different argument types: SparsityPattern, SparseMatrix and BlockSparseMatrix. Note that there are no versions for arguments of type  [2.x.41]  or any of the other PETSc or Trilinos matrix wrapper classes. This is due to the fact that it is relatively hard to get a representation of the sparsity structure of PETSc matrices, and to modify them efficiently; this holds in particular, if the matrix is actually distributed across a cluster of computers. If you want to use PETSc/Trilinos matrices, you can either copy an already condensed deal.II matrix, or assemble the PETSc/Trilinos matrix in the already condensed form, see the discussion below.
* 

*  [1.x.5]
*  Condensing vectors works exactly as described above for matrices. Note that condensation is an idempotent operation, i.e. doing it more than once on a vector or matrix yields the same result as doing it only once: once an object has been condensed, further condensation operations don't change it any more.
*  In contrast to the matrix condensation functions, the vector condensation functions exist in variants for PETSc and Trilinos vectors. However, using them is typically expensive, and should be avoided. You should use the same techniques as mentioned above to avoid their use.
* 

*  [1.x.6]
*  Sometimes, one wants to avoid explicit condensation of a linear system after it has been built at all. There are two main reasons for wanting to do so:
*   [2.x.42]   [2.x.43]  Condensation is an expensive operation, in particular if there are many constraints and/or if the matrix has many nonzero entries. Both is typically the case for 3d, or high polynomial degree computations, as well as for hp-finite element methods, see for example the  [2.x.44]  "hp-paper". This is the case discussed in the hp-tutorial program,  [2.x.45]  step_27 " [2.x.46] ", as well as in  [2.x.47]  and  [2.x.48]  " [2.x.49] ".
*   [2.x.50]  There may not be an  [2.x.51]  function for the matrix you use (this is, for example, the case for the PETSc and Trilinos wrapper classes where we have no access to the underlying representation of the matrix, and therefore cannot efficiently implement the  [2.x.52]  operation). This is the case discussed in  [2.x.53] ,  [2.x.54] ,  [2.x.55] , and  [2.x.56] .  [2.x.57] 
*  In this case, one possibility is to distribute local entries to the final destinations right at the moment of transferring them into the global matrices and vectors, and similarly build a sparsity pattern in the condensed form at the time it is set up originally.
*  The AffineConstraints class offers support for these operations as well. For example, the  [2.x.58]  function adds nonzero entries to a sparsity pattern object. It not only adds a given entry, but also all entries that we will have to write to if the current entry corresponds to a constrained degree of freedom that will later be eliminated. Similarly, one can use the  [2.x.59]  functions to directly distribute entries in vectors and matrices when copying local contributions into a global matrix or vector. These calls make a subsequent call to  [2.x.60]  unnecessary. For examples of their use see the tutorial programs referenced above.
*  Note that, despite their name which describes what the function really does, the  [2.x.61]  functions has to be applied to matrices and right hand side vectors, whereas the  [2.x.62]  function discussed below is applied to the solution vector after solving the linear system.
* 

*  [1.x.7]
*  After solving the condensed system of equations, the solution vector has to be "distributed": the modification to the original linear system that results from calling  [2.x.63]  leads to a linear system that solves correctly for all degrees of freedom that are unconstrained but leaves the values of constrained degrees of freedom undefined. To get the correct values also for these degrees of freedom, you need to "distribute" the unconstrained values also to their constrained colleagues. This is done by the  [2.x.64]  function. The operation of distribution undoes the condensation process in some sense, but it should be noted that it is not the inverse operation. Basically, distribution sets the values of the constrained nodes to the value that is computed from the constraint given the values of the unconstrained nodes plus possible inhomogeneities.
* 

*  [1.x.8]
*  In case some constraint lines have inhomogeneities (which is typically the case if the constraint comes from implementation of inhomogeneous boundary conditions), the situation is a bit more complicated than if the only constraints were due to hanging nodes alone. This is because the elimination of the non-diagonal values in the matrix generate contributions in the eliminated rows in the vector. This means that inhomogeneities can only be handled with functions that act simultaneously on a matrix and a vector. This means that all inhomogeneities are ignored in case the respective condense function is called without any matrix (or if the matrix has already been condensed before).
*  The use of the AffineConstraints class for implementing Dirichlet boundary conditions is discussed in the  [2.x.65]  tutorial program. A further example that utilizes AffineConstraints is  [2.x.66] . The situation here is little more complicated, because there we have some constraints which are not at the boundary. There are two ways to apply inhomogeneous constraints after creating an AffineConstraints object:
*  First approach:
* 

* 
* 
*  - Apply the  [2.x.67]  function to the   system matrix and the right-hand-side with the parameter   use_inhomogeneities_for_rhs = false (i.e., the default)
* 

* 
* 
*  - Set the solution to zero in the inhomogeneous constrained components   using the  [2.x.68]  function (or start with a solution   vector equal to zero)
* 

* 
* 
*  - solve() the linear system
* 

* 
* 
*  - Apply  [2.x.69]  to the solution
*  Second approach:
* 

* 
* 
*  - Use the  [2.x.70]  function with   the parameter use_inhomogeneities_for_rhs = true and apply it to the   system matrix and the right-hand-side
* 

* 
* 
*  - Set the concerning components of the solution to the inhomogeneous   constrained values (for example using  [2.x.71] 
* 

* 
* 
*  - solve() the linear system
* 

* 
* 
*  - Depending on the solver now you have to apply the    [2.x.72]  function to the solution, because the   solver could change the constrained values in the solution. For a   Krylov based solver this should not be strictly necessary, but it is   still possible that there is a difference between the inhomogeneous   value and the solution value in the order of machine precision, and   you may want to call  [2.x.73]  anyway if you   have additional constraints such as from hanging nodes.
*  Of course, both approaches lead to the same final answer but in different ways. Using the first approach (i.e., when using  [2.x.74]  in  [2.x.75]  the linear system we build has zero entries in the right hand side in all those places where a degree of freedom is constrained, and some positive value on the matrix diagonal of these lines. Consequently, the solution vector of the linear system will have a zero value for inhomogeneously constrained degrees of freedom and we need to call  [2.x.76]  to give these degrees of freedom their correct nonzero values.
*  On the other hand, in the second approach, the matrix diagonal element and corresponding right hand side entry for inhomogeneously constrained degrees of freedom are so that the solution of the linear system already has the correct value (e.g., if the constraint is that  [2.x.77]  then row  [2.x.78]  if the matrix is empty with the exception of the diagonal entry, and  [2.x.79]  so that the solution of  [2.x.80]  must satisfy  [2.x.81]  as desired). As a consequence, we do not need to call  [2.x.82]  after solving to fix up inhomogeneously constrained components of the solution, though there is also no harm in doing so.
*  There remains the question of which of the approaches to take and why we need to set to zero the values of the solution vector in the first approach. The answer to both questions has to do with how iterative solvers solve the linear system. To this end, consider that we typically stop iterations when the residual has dropped below a certain fraction of the norm of the right hand side, or, alternatively, a certain fraction of the norm of the initial residual. Now consider this:
* 

* 
* 
*  - In the first approach, the right hand side entries for constrained   degrees of freedom are zero, i.e., the norm of the right hand side   really only consists of those parts that we care about. On the other   hand, if we start with a solution vector that is not zero in   constrained entries, then the initial residual is very large because   the value that is currently in the solution vector does not match the   solution of the linear system (which is zero in these components).   Thus, if we stop iterations once we have reduced the initial residual   by a certain factor, we may reach the threshold after a single   iteration because constrained degrees of freedom are resolved by   iterative solvers in just one iteration. If the initial residual   was dominated by these degrees of freedom, then we see a steep   reduction in the first step although we did not really make much   progress on the remainder of the linear system in this just one   iteration. We can avoid this problem by either stopping iterations   once the norm of the residual reaches a certain fraction of the   [1.x.9], or we can set the solution   components to zero (thus reducing the initial residual) and iterating   until we hit a certain fraction of the [1.x.10].
* 

* 
* 
*  - In the second approach, we get the same problem if the starting vector   in the iteration is zero, since then the residual may be   dominated by constrained degrees of freedom having values that do not   match the values we want for them at the solution. We can again   circumvent this problem by setting the corresponding elements of the   solution vector to their correct values, by calling    [2.x.83]  [1.x.11] solving the linear system   (and then, as necessary, a second time after solving).
*  In addition to these considerations, consider the case where we have inhomogeneous constraints of the kind  [2.x.84] , e.g., from a hanging node constraint of the form  [2.x.85]  where  [2.x.86]  is itself constrained by boundary values to  [2.x.87] . In this case, the AffineConstraints container can of course not figure out what the final value of  [2.x.88]  should be and, consequently, can not set the solution vector's third component correctly. Thus, the second approach will not work and you should take the first.
* 

*  [1.x.12]
*  There are situations where degrees of freedom are constrained in more than one way, and sometimes in conflicting ways. Consider, for example the following situation:      [2.x.89]  Here, degree of freedom  [2.x.90]  marked in blue is a hanging node. If we used trilinear finite elements, i.e. FE_Q(1), then it would carry the constraint  [2.x.91] . On the other hand, it is at the boundary, and if we have imposed boundary conditions  [2.x.92]  then we will have the constraint  [2.x.93]  where  [2.x.94]  is the value of the boundary function  [2.x.95]  at the location of this degree of freedom.
*  So, which one will win? Or maybe: which one [1.x.13] win? There is no good answer to this question:
* 

* 
* 
*  - If the hanging node constraint is the one that is ultimately enforced,   then the resulting solution does not satisfy boundary   conditions any more for general boundary functions  [2.x.96] .
* 

* 
* 
*  - If it had been done the other way around, the solution would not satisfy   hanging node constraints at this point and consequently would not   satisfy the regularity properties of the element chosen (e.g. would not   be continuous despite using a  [2.x.97]  element).
* 

* 
* 
*  - The situation becomes completely hopeless if you consider   curved boundaries since then the edge midpoint (i.e. the hanging node)   does in general not lie on the mother edge. Consequently, the solution   will not be  [2.x.98]  conforming anyway, regardless of the priority of   the two competing constraints. If the hanging node constraint wins, then   the solution will be neither conforming, nor have the right boundary   values. In other words, it is not entirely clear what the "correct" solution would be. In most cases, it will not matter much: in either case, the error introduced either by the non-conformity or the incorrect boundary values will be at worst at the same order as the discretization's overall error.
*  That said, what should you do if you know what you want is this:
* 

* 
* 
*  - If you want the hanging node constraints to win, then first build   these through the  [2.x.99]  function.   Then interpolate the boundary values using    [2.x.100]  into the same   AffineConstraints object. If the latter function encounters a boundary   node that already is constrained, it will simply ignore the boundary   values at this node and leave the constraint untouched.
* 

* 
* 
*  - If you want the boundary value constraint to win, build the hanging   node constraints as above and use these to assemble the matrix using   the  [2.x.101]  function (or,   alternatively, assemble the matrix and then use    [2.x.102]  on it). In a second step, use the    [2.x.103]  function that returns   a  [2.x.104]  and use it as input for  [2.x.105]    to set boundary nodes to their correct value.
*  Either behavior can also be achieved by building two separate AffineConstraints objects and calling  [2.x.106]  function with a particular second argument.
* 

*  [1.x.14]
*  Sometimes it is either not desirable, or not possible to directly condense, or eliminate constraints from a system of linear equations. In particular if there is no underlying matrix object that could be condensed (or taken care of constraints during assembly). This is usually the case if the system is described by a LinearOperator.
*  In this case we can solve the modified system [1.x.15] instead [1] (M. S. Shephard. Linear multipoint constraints applied via transformation as part of a direct stiffness assembly process. [1.x.16] 20(11):2107-2112, 1985).
*  Here,  [2.x.107]  is a given (unconstrained) system matrix for which we only assume that we can apply it to a vector but can not necessarily access individual matrix entries.  [2.x.108]  is the corresponding right hand side of a system of linear equations  [2.x.109] . The matrix  [2.x.110]  describes the homogeneous part of the linear constraints stored in an AffineConstraints object and the vector  [2.x.111]  is the vector of corresponding inhomogeneities. More precisely, the  [2.x.112]  operation applied on a vector  [2.x.113]  is the operation [1.x.17] And finally,  [2.x.114]  denotes the identity on the subspace of constrained degrees of freedom.
*  The corresponding solution of  [2.x.115]  that obeys these constraints is then recovered by distributing constraints:  [2.x.116] .
*  The whole system can be set up and solved with the following snippet of code:

* 
* [1.x.18]
* 

* 
* [0.x.1]

include/deal.II-translator/A-headers/cuda_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  The classes in this module are concerned with the description of features to be run on GPUs using CUDA.

* 
* [0.x.1]

include/deal.II-translator/A-headers/distributed_0.txt
[0.x.0]*


* 
*  [2.x.0] 

* 
*  [2.x.1] 
* 

* 
*  [2.x.2]  clusters.
*   [2.x.3] 
*  [1.x.0]
*  deal.II can use multiple machines connected via MPI to parallelize computations, in addition to the parallelization within a shared memory machine discussed in the  [2.x.4]  module. There are essentially two ways to utilize multiple machines:
* 

* 
* 
*  - Each machine keeps the entire mesh and DoF handler locally, but   only a share of the global matrix, sparsity pattern, and solution   vector is stored on each machine.
* 

* 
* 
*  - The mesh and DoF handler are also distributed, i.e. each processor   stores only a share of the cells and degrees of freedom. No   processor has knowledge of the entire mesh, matrix, or solution,   and in fact problems solved in this mode are usually so large   (say, 100s of millions to billions of degrees of freedom) that no   processor can or should store even a single solution vector.
*  The first of these two options is relatively straightforward because most of the things one wants to do in a finite element program still work in essentially the same way, and handling distributed matrices, vectors, and linear solvers is something for which good external libraries such as Trilinos or PETSc exist that can make things look almost exactly the same as they would if everything was available locally. The use of this mode of parallelization is explained in the tutorial programs  [2.x.5] , and  [2.x.6]  and will not be discussed here in more detail.
*  The use of truly distributed meshes is somewhat more complex because it changes or makes impossible some of the things that can otherwise be done with deal.II triangulations, DoF handlers, etc. This module documents these issues with a vantage point at 50,000 ft above ground without going into too many details. All the algorithms described below are implement in classes and functions in namespace  [2.x.7] 
*  One important aspect in parallel computations using MPI is that write access to matrix and vector elements requires a call to compress() after the operation is finished and before the object is used (for example read from). Also see  [2.x.8] .
*  [1.x.1]
*  A complete discussion of the algorithms used in this namespace, as well as a thorough description of many of the terms used here, can be found in the  [2.x.9]  "Distributed Computing paper". In particular, the paper shows that the methods discussed in this module scale to thousands of processors and well over a billion degrees of freedom. The paper also gives a concise definition of many of the terms that are used here and in other places of the library related to distributed computing.  The  [2.x.10]  tutorial program shows an application of the classes and methods of this namespace to the Laplace equation, while  [2.x.11]  does so for a vector-valued problem.  [2.x.12]  extends the  [2.x.13]  program to massively parallel computations and thereby explains the use of the topic discussed here to more complicated applications.
*  For a discussion of what we consider "scalable" programs, see  [2.x.14]  "this glossary entry".
* 

*  [1.x.2]
*  In %parallel %distributed mode, objects of type  [2.x.15]  on each processor only store a subset of cells. In particular, the global mesh can be thought of as decomposed so that each MPI process "owns" a number of cells. The mesh each process then stores locally consists of exactly those cells that it owns, as well as one layer of  [2.x.16]  GlossGhostCell "ghost cells" around the ones it locally owns, and a number of cells we call  [2.x.17]  "artificial". The latter are cells that ensure that each processor has a mesh that has all the coarse level cells and that respects the invariant that neighboring cells can not differ by more than one level of refinement. The following pictures show such a mesh, %distributed across four processors, and the collection of cells each of these processors stores locally:
*   [2.x.18] 
*  The cells are colored based on the  [2.x.19]  "subdomain id", which identifies which processor owns a cell: turquoise for processor 0, green for processor 1, yellow for processor 2, and red for processor 3. As can be seen, each process has one layer of ghost cells around its own cells, which are correctly colored by the subdomain id that identifies the processor that owns each of these cells. Note also how each processor stores a number of artificial cells, indicated in blue, that only exist to ensure that each processor knows about all coarse grid cells and that the meshes have the 2:1 refinement property; however, in the area occupied by these artificial cells, a processor has no knowledge how refined the mesh there really is, as these are areas that are owned by other processors. As a consequence, all algorithms we will develop can only run over the locally owned cells and if necessary the ghost cells; trying to access data on any of the artificial cells is most likely an error. Note that we can determine whether we own a cell by testing that <code>cell- [2.x.20]  == triangulation.locally_owned_subdomain()</code>.
*  The "real" mesh one has to think of here is the one that would result from forming the union of cells each of the processes own, i.e. from the overlap of the turquoise, green, yellow and red areas, disregarding the blue areas.
* 

* 
*  [2.x.21]  The decomposition of this "real" mesh into the pieces stored   by each processes is provided by the [1.x.3]   library. p4est stores the complete mesh in a distributed data structure   called a parallel forest (thus the name). A parallel forest consists of   quad-trees (in 2d) or oct-trees (in 3d) originating in each   coarse mesh cell and representing the refinement structure   from parent cells to their four (in 2d) or eight (in 3d)   children. Internally, this parallel forest is represented by   a (distributed) linear array of cells that corresponds to a   depth-first traverse of each tree, and each process then stores   a contiguous section of this linear array of cells. This results   in partitions such as the one shown above that are not optimal   in the sense that they do not minimize the length of the   interface between subdomains (and consequently do not minimize   the amount of communication) but that in practice are very   good and can be manipulated with exceedingly fast algorithms.   The efficiency of storing and manipulating cells in this way   therefore often outweighs the loss in optimality of communication.   (The individual subdomains resulting from this method of   partitioning may also sometimes consist of disconnected   parts, such as shown at the top right. However, it can be   proven that each subdomain consists of at most two disconnected   pieces; see C. Burstedde, J. Holke, T. Isaac: "Bounds on the number of   discontinuities of Morton-type space-filling curves",   [1.x.4],   2017.)
* 

*  [1.x.5]
*  The DoFHandler class builds on the Triangulation class, but it can detect whenever we actually use an object of type  [2.x.22]  as triangulation. In that case, it assigns global %numbers for all degrees of freedom that exist, given a finite element, on the global mesh, but each processor will only know about those that are defined on locally relevant cells (i.e. cells either locally owned or that are ghost cells). Internally, the algorithm essentially works by just looping over all cells we own locally and assigning DoF indices to the degrees of freedom defined on them and, in the case of degrees of freedom at the interface between subdomains owned by different processors, that are not owned by the neighboring processor. All processors then exchange how many degrees of freedom they locally own and shift their own indices in such a way that every degree of freedom on all subdomains are uniquely identified by an index between zero and  [2.x.23]  (this function returns the global number of degrees of freedom, accumulated over all processors). Note that after this step, the degrees of freedom owned by each process form a contiguous range that can, for example, be obtained by the contiguous index set returned by  [2.x.24]  After assigning unique indices to all degrees of freedom, the  [2.x.25]  function then loops over all ghost cells and communicates with neighboring processors to ensure that the global indices of degrees of freedom on these ghost cells match the ones that the neighbor has assigned to them.
*  Through this scheme, we can make sure that each cell we locally own as well as all the ghost cells can be asked to yield the globally correct indices for the degrees of freedom defined on them. However, asking for degrees of freedom on artificial cells is likely going to lead to nothing good, as no information is available for these cells (in fact, it isn't even known whether these cells are active on the global mesh, or are further refined).
*  As usual, degrees of freedom can be renumbered after being enumerated, using the functions in namespace DoFRenumbering.
* 

*  [1.x.6]
*  One thing one learns very quickly when working with very large numbers of processors is that one can not store information about every degree of freedom on each processor, even if this information is "this degree of freedom doesn't live here". An example for this is that we can create an object for a (compressed) sparsity pattern that has  [2.x.26]  rows, but for which we fill only those rows that correspond to the  [2.x.27]  locally owned degrees of freedom. The reason is simple: for the sake of example, let's assume we have 1 billion degrees of freedom distributed across 100 processors; if we even only hold 16 bytes per line in this sparsity pattern (whether we own the corresponding DoF or not), we'll need 16 GB for this object even if every single line is empty. Of course, only 10 million lines will be non-empty, for which we need 160 MB plus whatever is necessary to store the actual column indices of nonzero entries. Let's say we have a moderately complex problem with 50 entries per row, for each of which we store the column index worth 4 bytes, then we'll need 216 bytes for each of the 10 million lines that correspond to the degrees of freedom we own, for a total of 2.16 GB. And we'll need 16 bytes for each of the 990 million lines that we don't own, for a total of 15.840 GB. It is clear that this ratio doesn't become any better if we go to even higher %numbers of processors.
*  The solution to this problem is to really only use any memory at all for those parts of the linear system that we own, or need for some other reason. For all other parts, we must know that they exist, but we can not set up any part of our data structure. To this end, there exists a class called IndexSet that denotes a set of indices which we care for, and for which we may have to allocate memory. The data structures for sparsity patterns, constraint matrices, matrices and vector can be initialized with these IndexSet objects to really only care for those rows or entries that correspond to indices in the index set, and not care about all others. These objects will then ask how many indices exist in the set, allocate memory for each one of them (e.g. initialize the data structures for a line of a sparsity pattern), and when you want to access data for global degree of freedom  [2.x.28]  you will be redirected to the result of calling  [2.x.29]  with index  [2.x.30]  instead. Accessing data for elements  [2.x.31]  for which  [2.x.32]  is false will yield an error.
*  The remaining question is how to identify the set of indices that correspond to degrees of freedom we need to worry about on each processor. To this end, you can use the  [2.x.33]  function to get at all the indices a processor owns. Note that this is a subset of the degrees of freedom that are defined on the locally owned cells (since some of the degrees of freedom at the interface between two different subdomains may be owned by the neighbor). This set of degrees of freedom defined on cells we own can be obtained using the function  [2.x.34]  Finally, one sometimes needs the set of all degrees of freedom on the locally owned subdomain as well as the adjacent ghost cells. This information is provided by the  [2.x.35]  function.
* 

*  [1.x.7]
*  A typical parallel application is dealing with two different kinds of parallel vectors: vectors with ghost elements (also called ghosted vectors) and vectors without ghost elements.  (Both kinds can typically be represented by the same data type, but there are of course different vector types that can each represent both flavors: for example  [2.x.36]   [2.x.37]  and BlockVector objects built on these). You can find a discussion of what distinguishes these kinds of vectors in the  [2.x.38]  "glossary entry on ghosted vectors".
*  From a usage point of view, ghosted vectors are typically used for data output, postprocessing, error estimation, input in integration. This is because in these operations, one typically needs access not only to  [2.x.39]  "locally owned dofs" but also to  [2.x.40]  "locally active dofs" and sometimes to  [2.x.41]  "locally relevant dofs", and their values may not be stored in non-ghosted vectors on the processor that needs them. The operations listed above also only require read-only access to vectors, and ghosted vectors are therefore usable in these contexts.
*  On the other hand, vectors without ghost entries are used in all other places like assembling, solving, or any other form of manipulation. These are typically write-only operations and therefore need not have read access to vector elements that may be owned by another processor.
*  You can copy between vectors with and without ghost elements (you can see this in  [2.x.42] ,  [2.x.43] , and  [2.x.44] ) using operator=.
* 

*  [1.x.8]
*  At the time of writing this, the only class equipped to deal with the situation just explained is DynamicSparsityPattern. A version of the function  [2.x.45]  exists that takes an IndexSet argument that indicates which lines of the sparsity pattern to allocate memory for. In other words, it is safe to create such an object that will report as its size 1 billion, but in fact only stores only as many rows as the index set has elements. You can then use the usual function  [2.x.46]  to build the sparsity pattern that results from assembling on the locally owned portion of the mesh. The resulting object can be used to initialize a PETSc or Trilinos matrix which support very large object sizes through completely distributed storage. The matrix can then be assembled by only looping over those cells owned by the current processor.
*  The only thing to pay attention to is for which degrees of freedom the sparsity needs to store entries. These are, in essence, the ones we could possibly store values to in the matrix upon assembly. It is clear that these are certainly the locally active degrees of freedom (which live on the cells we locally own) but through constraints, it may also be possible to write to entries that are located on ghost cells. Consequently, you need to pass the index set that results from  [2.x.47]  upon initializing the sparsity pattern.
* 

*  [1.x.9]
*  When creating the sparsity pattern as well as when assembling the linear system, we need to know about constraints on degrees of freedom, for example resulting from hanging nodes or boundary conditions. Like the DynamicSparsityPattern class, the AffineConstraints container can also take an IndexSet upon construction that indicates for which of the possibly very large number of degrees of freedom it should actually store constraints. Unlike for the sparsity pattern, these are now only those degrees of freedom which we work on locally when assembling, namely those returned by  [2.x.48]  (a superset of the locally owned ones).
*  There are, however, situations where more complicated constraints appear in finite element programs. An example is in  [2.x.49]  adaptive computations where degrees of freedom can be constrained against other degrees of freedom that are themselves constrained. In a case like this, in order to fully resolve this chain of constraints, it may not be sufficient to only store constraints on locally active degrees of freedom but one may also need to have constraints available on locally relevant ones. In that case, the AffineConstraints object needs to be initialized with the IndexSet produced by  [2.x.50]  .
*  In general, your program will continue to do something if you happen to not store all necessary constraints on each processor: you will just generate wrong matrix entries, but the program will not abort. This is opposed to the situation of the sparsity pattern: there, if the IndexSet passed to the DynamicSparsityPattern indicates that it should store too few rows of the matrix, the program will either abort when you attempt to write into matrix entries that do not exist or the matrix class will silently allocate more memory to accommodate them. As a consequence, it is useful to err on the side of caution when indicating which constraints to store and use the result of  [2.x.51]  rather than  [2.x.52]  . This is also affordable since the set of locally relevant degrees of freedom is only marginally larger than the set of locally active degrees of freedom. We choose this strategy in  [2.x.53] ,  [2.x.54] , and  [2.x.55] .
* 

*  [1.x.10]
*  Like everything else, you can only do postprocessing on cells a local processor owns. The DataOut and KellyErrorEstimator classes do this automatically: they only operate on locally owned cells without the need to do anything in particular. At least for large computations, there is also no way to merge the results of all these local computations on a single machine, i.e. each processor has to be self-sufficient. For example, each processor has to generate its own parallel output files that have to be visualized by a program that can deal with multiple input files rather than merging the results of calling DataOut to a single processor before generating a single output file. The latter can be achieved, for example, using the  [2.x.56]  and  [2.x.57]  functions.
*  These same considerations hold for all other postprocessing actions as well: while it is, for example, possible to compute a global energy dissipation rate by doing the computations locally and accumulating the resulting single number processor to a single number for the entire communication, it is in general not possible to do the same if the volume of data produced by every processor is significant.
*  There is one particular consideration for postprocessing, however: whatever you do on each cell a processor owns, you need access to at least all those values of the solution vector that are active on these cells (i.e. to the set of all [1.x.11], in the language of the  [2.x.58]  "Distributed Computing paper"), which is a superset of the degrees of freedom this processor actually owns (because it may not own all degrees of freedom on the interface between its own cells and those cells owned by other processors). Sometimes, however, you need even more information: for example, to compute the KellyErrorIndicator results, one needs to evaluate the gradient at the interface on the current as well as its neighbor cell; the latter may be owned by another processor, so we need those degrees of freedom as well. In general, therefore, one needs access to the solution values for all degrees of freedom that are [1.x.12]. On the other hand, both of the packages we can use for parallel linear algebra (PETSc and Trilinos) as well as  [2.x.59]  subdivide vectors into chunks each processor owns and chunks stored on other processors. To postprocess stuff therefore means that we have to tell PETSc or Trilinos that it should also import [1.x.13], i.e. additional vector elements of the solution vector other than the ones we own locally. For ghosted vectors, this can be achieved by using operator= with a distributed vector as argument.

* 
* [0.x.1]*
   A namespace for class and   functions that support %parallel   computing on %distributed memory   machines. See the  [2.x.60]    distributed module for an   overview of the facilities this   namespace offers.    
*  [2.x.61]   
* [0.x.2]

include/deal.II-translator/A-headers/dofs_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This module groups classes and namespaces that have to do with handling degrees of freedom. The central class of this group is the DoFHandler class: it is built on top of a triangulation and a finite element class and allocated degrees of freedom on each cell of the triangulation as required for the finite element space described by the finite element object. There are other variants of the DoFHandler class such as  [2.x.1]  that do similar things for more special cases.
*  DoFHandler objects are used together with objects of type FiniteElement (or  [2.x.2]  in the case of  [2.x.3]  to enumerate all the degrees of freedom that exist in a triangulation for this particular finite element. As such, the combination of mesh, finite element, and DoF handler object can be thought of as providing a [1.x.0] of the finite element space: the mesh provides the locations at which basis functions are defined; the finite element describes what kinds of basis functions exist; and the DoF handler object provides an enumeration of the basis, i.e., it is provides a concrete structure of the space so that we can describe functions in this finite dimensional space by vectors of coefficients.
*  DoFHandlers extend Triangulation objects (and the other classes in the  [2.x.4]  grid module) in that they, too, offer iterators that run over all cells, faces, or other geometric objects that make up a triangulation. These iterators are derived from the triangulation iterators and therefore offer the same functionality, but they also offer additional functions. For example, they allow to query the indices of the degrees of freedom associated with the present cell. Note that DoFHandler classes are [1.x.1] from Triangulation, though they use Triangulation objects; the reason is that there can be more than one DoFHandler object that works on the same Triangulation object.
*  In addition to the DoF handler classes, this module holds a number of auxiliary classes not commonly used in application programs, as well as three classes that are not directly associated with the data structures of the DoFHandler class. The first of these is the AffineConstraints class that stores and treats the constraints associated with hanging nodes. Secondly, the DoFRenumbering namespace offers functions that can reorder degrees of freedom; among its functions are ones that sort degrees of freedom in downstream direction, for example, and ones that sort degrees of freedom in such a way that the bandwidth of associated matrices is minimized. Finally, the DoFTools namespace offers a variety of algorithms around handling degrees of freedom.
*  In the grand scheme of things, the pieces of this module interact with a variety of other parts of the library:

* 
* [1.x.2]

* 
* [0.x.1]

include/deal.II-translator/A-headers/exceptions_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This module contains classes that are used in the exception mechanism of deal.II.
*  [1.x.0]
*  Exceptions are used in two different ways:  [2.x.1] 
*     [2.x.2]  Static assertions: These are checks that are only enabled in debug   mode, not in release (or optimized, production) mode. In deal.II, static   assertions are typically used to check that parameters to functions satisfy   certain properties, that internal data structures are consistent, and similar   assertions. For example, static assertions are used to make sure that two   vectors that are added together have the same number of components
* 
*  -    everything else would not make any sense anyway.
*    Such checks are performed by the  [2.x.3]  macro in several thousand places   within the library. Also, several tutorial programs starting with  [2.x.4]    show how to do this.
*    If a static assertion is violated, the exception mechanism generates an   exception of a type that indicates what exactly goes wrong, displays   appropriate information including the exact location where the problem   was detected, and then aborts the program
* 
*  -  if you try to add   two vectors of different length, there is nothing that can be done within   the program to cope with the situation, you have to go fix the program   code instead. There is generally not even a reason to  [2.x.5]  an exception   object using the usual C++ exception mechanism because there is nothing   a function higher up could do in such cases to rectify the situation   and deal with it in a useful way
* 
*  -  it's not that the program received   bad data; the program is just buggy, and one can not intelligently   work around that.
*    (It is sometimes useful to change the behavior of the  [2.x.6]  macro   from aborting a program to throwing exceptions. On the other hand,   exceptions are not allowed to propagate out of destructors of classes.   For this purpose, there is a variant of the macro, called  [2.x.7]    that can be used in destructors. These use cases are discussed further   down below on this page.)
* 

*     [2.x.8]  Dynamic assertions: These are used to check conditions that depend on   external things that may be different from one program run to the next, such   as whether an output file can be written to.
*    These are things that shouldn't   be checked statically, because it is not guaranteed that a program for which   the condition is satisfied in a debug mode run, will also have the condition   satisfied in a subsequent release mode run
* 
*  -  in other words, it is not   sufficient to only check these situations in debug mode.
*    Rather, one has to check them every time during execution of a   program. Within deal.II, this is done using the  [2.x.9]  macro   introduced in  [2.x.10] ,  [2.x.11] , and   following tutorial programs. The macro checks a condition, and if   violated throws an exception of one of the types declared in this   module, using the C++  [2.x.12]  mechanism. Since these   are run-time exceptions, this gives the program the chance to   catch the exception and, for example, write the output to a   writable file instead.  [2.x.13] 
* 

*  [1.x.1]
*   The error handling mechanism in <tt>deal.II</tt> is generally used in two ways.  The first uses error checking in debug mode only and is useful for programs  which are not fully tested. When the program shows no errors anymore, one may  switch off error handling and get better performance by this, since checks  for errors are done quite frequently in the library (a typical speed up is  a factor of four!). This mode of exception generation is most useful for  internal consistency checks such as range checking or checking of the  validity of function arguments. Errors of this kind usually are programming  errors and the program should abort with as detailed a message as possible,  including location and reason for the generation of the exception.
*   The second mode is for error checks which should always be on, such as for  I/O errors, failing memory requests and the like. It does not make much  sense to turn this mode off, since this kind of errors may happen in tested  and untested programs likewise. Exceptions of this kind do not terminate the  program, rather they throw exceptions in the <tt>C++</tt> manner, allowing the  program to catch them and eventually do something about it. As it may be  useful to have some information printed out if an exception could not be  handled properly, additional information is passed along as for the first  mode. The latter makes it necessary to provide a family of macros which  enter this additional information into the exception class; this could  in principle be done by the programmer himself each time by hand, but since  the information can be obtained automatically, a macro is provided for  this.
*   Both modes use exception classes, which need to have special features  in additional to the <tt>C++</tt> standard's  [2.x.14]  class.  Such a class is declared by the following lines of code: 
* [1.x.2]
* 
*   This declares an exception class named <tt>ExcDomain</tt>, which  has two variables as additional information (named <tt>arg1</tt>  and <tt>arg2</tt> by default) and which outputs the given sequence  (which is appended to an  [2.x.15]  variable's name,  thus the weird syntax). There are other <tt>DeclExceptionN</tt>  macros for exception classes with more or no parameters. By  convention, the name of all exception classes starts with  <tt>Exc...</tt> and most of them are declared locally to the class  it is to be used in (a few very frequently found ones are also  declared in the StandardExceptions namespace and are available  everywhere). Declaring exceptions globally is possible but  pollutes the global namespace, is less readable and most of the time  unnecessary.
*   Since exception classes are declared the same way for both modes  of error checking, it is possible to use an exception declared  through the <tt>DeclExceptionN(...)</tt> macro family for both  static as well as dynamic checks.
* 

*   [1.x.3]
*   To use the exception mechanism for debug mode error checking, write lines  like the following in your source code: 
* [1.x.4]
*   which by macro expansion does essentially the following (though the actual  code is slightly more complicated): 
* [1.x.5]
*   That is, it issues an error only if the preprocessor variable  <tt>DEBUG</tt> is set and if the given condition (in this case  <tt>n < dim</tt> is violated).
*   If the exception was declared using the <tt>DeclException0 (...)</tt>  macro, i.e., without any additional parameters, its name has  nonetheless to be given with parentheses:  <tt>Assert (i>m, ExcSomewhat());</tt>
*   [1.x.6]
*   If the <tt>DEBUG</tt> preprocessor directive is set, the call <tt>Assert  (cond, exc);</tt> is basically converted by the preprocessor into the  following sequence: 
* [1.x.7]
* 
*   (Note that function names and exact calling sequences may change  over time, but the general principle remains the same.) I.e., if  the given condition is violated, then the file and line in which  the exception occurred as well as the condition itself and the call  sequence of the exception object is passed to the   [2.x.16]   function. Additionally an object of the form given by <tt>exc</tt>  is created (this is normally an unnamed object like in  <tt>ExcDomain (n, dim)</tt> of class <tt>ExcDomain</tt>) and  transferred to this function.
*   <tt>__PRETTY_FUNCTION__</tt> is a macro defined by some compilers and  gives the name of the function. If another compiler is used, we  try to set this function to something reasonable, if the compiler  provides us with that, and <tt>"(not available)"</tt> otherwise.
*   In <tt>issue_error_noreturn</tt>, the given data is transferred into the  <tt>exc</tt> object by calling the set_fields() function; Afterwards the  program is either aborted (and information about the exception is printed  to deallog) or the exception is thrown. The <tt>Assert</tt> macro does the  first path (print and abort); <tt>AssertThrow</tt> does the second  (throw). This behavior is consistent with the descriptions of static and  dynamic assertions earlier in this document. If it can be obtained from  the operating system, the output may also contain a stacktrace to show  where the error happened. Several of the  [2.x.17]  programs show a  typical output.
*   If the preprocessor variable <tt>DEBUG</tt> is not set then the  <tt>Assert</tt> macro is expanded to <tt>{}</tt>.
*   Sometimes, there is no useful condition for an exception other  than that the program flow should not have reached a certain point,  e.g. a <tt>default</tt> section of a <tt>switch</tt> statement. In this case,  raise the exception by the following construct: 
* [1.x.8]
*   See the  [2.x.18]  and several other of the tutorial programs for  a use of this construct.
*   As mentioned above, the program is terminated once a call to  <tt>Assert</tt> fails. However, there is one case where we do not want  to do this, namely when a C++ exception is active. The usual case  where this happens is that someone throws an exception through the  <tt>AssertThrow</tt> mechanism (see below) which, while the stack is  unwound, leads to the destruction of other objects in stack frames  above. If other objects refer to the objects being thus destroyed,  some destructors raise an exception through <tt>Assert</tt>. If we  would abort the program then, we would only ever see the message  that an object is being destroyed which is still referenced from  somewhere, but we would never see the original exception that  triggered this. (You can see it in the debugger by putting a break  point on the function <tt>__throw</tt>, but you cannot see it from the  program itself.) In that case, we use a C++ standard library  function to detect the presence of another active exception and do  not terminate the program to allow that the thrown exception  propagates to some place where its message can be displayed.
*   Since it is common that one failed assertion leads to a whole  chain of others, we only ever print the very first message. If the  program is then aborted, that is no problem. If it is not (since a  C++ exception is active), only the first is displayed and a  message about suppressed follow-up messages is shown.
* 

*   [1.x.9]
*   C++ has a mechanism to indicate that something exceptional has  happened: exceptions that can be triggered by <tt>throw</tt> statements  and captured by <tt>catch</tt> clauses, see for example  https://en.wikipedia.org/wiki/C%2B%2B#Exception_handling and  http://www.cplusplus.com/doc/tutorial/exceptions/ .
*   At some fundamental level, a typical C++ exception is an object that  is placed in some special place, and then the function exits the current  scope (e.g., the current function) through an exceptional return path.  This is often enough to tell what problem triggered the exception,  but more frequently it would be nice if one had more information: for  example, in which line of the code the problem happened, or what  non-existent entry of a sparse matrix the code wanted to write into.
*   Dynamic assertions in deal.II therefore extend this mechanism a bit.  Typically, one would raise an exception by code such as 
* [1.x.10]
*   and catch it using the statement 
* [1.x.11]
*    [2.x.19]  is a standard <tt>C++</tt> class providing basic functionality for  exceptions, such as the virtual function <tt>what()</tt> that returns some  information on the exception itself. This information is useful if an  exception can't be handled properly, in which case as precise a description  as possible should be printed.
*   The problem here is that to get significant and useful information out  of <tt>what()</tt>, it is necessary to overload this function in our exception  class and call the <tt>throw</tt> operator with additional arguments to the  exception class. The first thing, overloading the <tt>what</tt> function is  done using the <tt>DeclExceptionN</tt> macros, but putting the right information,  which is the same as explained above for the <tt>Assert</tt> expansion, requires  some work if one would want to write it down each time: 
* [1.x.12]
* 
*   For this purpose, the macro <tt>AssertThrow</tt> was invented. It does  mainly the same job as does the <tt>Assert</tt> macro, but it does not  abort the program; rather, it throws an exception as shown above. The mode  of usage is 
* [1.x.13]
* 
*   The condition to be checked is incorporated into the macro in order to  allow passing the violated condition as a string. The expansion of the  <tt>AssertThrow</tt> macro is not affected by the <tt>DEBUG</tt>  preprocessor variable.
* 

*   [1.x.14]
*   There is a whole family of <tt>DeclExceptionX</tt> macros  where <tt>X</tt> is to be replaced by the number of additional  parameters (0 to 5 presently).  These macros are used to declare exception classes in the following  way: 
* [1.x.15]
*   The first argument denotes the name of the exception class to be created.  The next arguments are the types of the parameters (in this  case there two types, corresponding to the <tt>X</tt> in  <tt>DeclExceptionX</tt>) and finally the output  sequence with which you can print additional information.
*   The syntax of the output sequence is a bit weird but gets  clearer once you see how this macro is defined (again schematically, actual  function names and definitions may change over time and be different): 
* [1.x.16]
* 
*   If declared as specified, you can later use this exception class  in the following manner: 
* [1.x.17]
*   and the output if the condition fails will be 
* [1.x.18]
* 
*   Obviously for the <tt>DeclException0(name)</tt> macro, no types and  also no output sequence is allowed.
* 

*   [1.x.19]
*   The default implementation of the  [2.x.20]  macro, as discussed above,  prints detailed information about what exactly went wrong to the  screen and then aborts the program. Aborting the program is useful  because it allows easily finding the place where something went  wrong
* 
*  -  including all of the information how we got to that  place
* 
*  -  by running the program in a debugger.
*   On the other hand, there are cases where aborting a program may be  undesirable and one needs to exit in a somewhat more graceful  way
* 
*  -  even if there is really not very much one can do in these  cases to still produce a meaningful result. An example is if a  deal.II program is run a one module in a bigger framework of  software. Think, for example, of a case where a deal.II program  computed the flow field that corresponds to a set of input  variables provided by some optimization routine: if the optimizer  on the outside provided a negative density as input (a condition  one might want to check via  [2.x.21]  then this  clearly makes no sense, and the flow solver cannot produce a  meaningful answer; but it should tell that to the optimizer nicely,  rather than just aborting the entire process (optimizer and flow  solver).
*   For this purpose, one can call   [2.x.22]  that switches  what  [2.x.23]  does from aborting the program to essentially the  same as  [2.x.24]  does, namely using the C++  [2.x.25]  mechanism  to raise an exception. This exception can then be caught at a higher  level
* 
*  -  e.g., in the optimization routine that sits atop the flow  solver, and that can then decide what it wants to do with the  situation.
*   This is all nice and good, but C++ does not allow throwing exceptions  inside the destructors of classes, or in a function that is currently  being called from a destructor higher up in the call stack. To this  end, there is a separate macro,  [2.x.26]  that can be used in  destructors: It acts just like  [2.x.27]  usually does
* 
*  -  in particular,  it only checks the condition in debug mode
* 
*  -  but it is immune to the  effect of  [2.x.28]  It will  only ever abort the program, and never throw an exception.
* 

*   [2.x.29]  Wolfgang Bangerth, 1998-2017

* 
* [0.x.1]

include/deal.II-translator/A-headers/fe_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  All classes related to shape functions and to access to shape functions.  This concerns the actual values of finite elements. For the numbering of degrees of freedom refer to the module on  [2.x.1] .
*  The classes and functions of this module fall into several sub-groups that are discussed in their respective sub-modules listed above. In addition, the FETools class provides functions that provide information on finite elements, transformations between elements, etc.
*  In the grand scheme of things, the pieces of this module interact with a variety of other parts of the library:

* 
* [1.x.0]

* 
* [0.x.1]*


* 
*  [2.x.2] 
*  The members of this sub-module describe the implementation mechanics of finite element classes, without actually implementing a concrete element. For example, the FiniteElement base class declares the virtual functions a derived class has to implement if it wants to describe a finite element space. Likewise, the FiniteElementData holds variables that describe certain values characterizing a finite element, such as the number of degrees of freedom per vertex, line, or face.
*  On the other hand, classes like FE_Poly and FE_PolyTensor are higher abstractions. They describe finite elements that are built atop polynomial descriptions of the shape functions on the unit cell. Classes derived from them then only have to provide a description of the particular polynomial from which a finite element is built. For example, the FE_Q class that implements the usual Lagrange elements uses the FE_Poly base class to generate a finite element by providing it with a set of Lagrange interpolation polynomials corresponding to an equidistant subdivision of interpolation points.
*  Finally, the FESystem class is used for vector-valued problems. There, one may want to couple a number of scalar (or also vector-valued) base elements together to form the joint finite element of a vector-valued operator. As an example, for 3d Navier-Stokes flow, one may want to use three Q1 elements for the three components of the velocity, and a piecewise constant Q0 element for the pressure. The FESystem class can be used to couple these four base elements together into a single, vector-valued element with 4 vector components. The  [2.x.3] ,  [2.x.4] , and  [2.x.5]  tutorial programs give an introduction into the use of this class in the context of the vector-valued elasticity (Lam&eacute;) equations.  [2.x.6]  discusses a mixed Laplace discretization that also uses vector-valued elements.
* 

* 
*  [2.x.7] 

* 
* [0.x.2]*


* 
*  [2.x.8] 
*  The classes in this module are used when one wants to assemble matrices or vectors. They link finite elements, quadrature objects, and mappings: the finite element classes describe a finite element space on a unit cell (i.e. the unit line segment, square, or cube <tt>[0,1]^d</tt>), the quadrature classes describe where quadrature points are located and what weight they have, and the mapping classes describe how to map a point from the unit cell to a real cell and back. Since integration happens at quadrature points on the real cell, and needs to know their location as well as the values and gradients of finite element shape functions at these points. The FEValues class coordinates getting this information. For integrations on faces (for example for integration on the boundary, or interfaces between cells), the FEFaceValues class offers similar functionality as the FEValues class does for cells. Finally, the FESubfaceValues class offers the possibility to ingrate on parts of faces if the neighboring cell is refined and the present cell shares only a part of its face with the neighboring cell. If vector-valued elements are used, the FEValues and related classes allow access to all vector components; if one wants to pick individual components, there are extractor classes that make this task simpler, as described in the  [2.x.9]  module.
*  The last member of this group, the UpdateFlags enumeration, is used as an optimization: instead of letting the FEValues class compute every possible piece of data relating to a given finite element on a cell, you have to specify up front which information you are actually interested in. The UpdateFlags enumeration is used to offer symbolic names denoting what you want the FEValues class to compute.
*  All these classes are used in all  [2.x.10]  "tutorial programs" from  [2.x.11]  onward, and are described there in significant detail.
*  The actual workings of the FEValues class and friends is complicated because it has to be general yet efficient. The page on  [2.x.12]  attempts to give an overview of how this works.
*  In the grand scheme of things, the pieces of this module interact with a variety of other parts of the library:

* 
* [1.x.1]
* 

* 
*  [2.x.13] 

* 
* [0.x.3]*


* 
*  [2.x.14] 
*  The classes here describe finite element spaces, such as the simplest Q1 (bi-/trilinear) spaces, and higher order Lagrangian spaces Qp, but also more specialized spaces such as Nedelec or Raviart-Thomas ones. Concrete implementations are derived from the abstract FiniteElement base class.
*  In essence, the functions these classes have to implement provide the ability to query the value or derivatives of a shape function at a given point on the unit cell. To be useful in integrating matrix and right hand side entries, one has to have the ability to map these shape functions and gradients to the real cell. This is done using classes derived from the Mapping base class (see  [2.x.15] ) in conjunction with the FEValues class (see  [2.x.16] ).
*  [1.x.2]
*  deal.II provides two different kinds of vector valued elements. First, there is a group of genuine vector elements, usually distinguished by the fact, that each vector component consists of a different set of anisotropic polynomials. These elements are typically associated with differential forms. Currently, they are
*   [2.x.17]   [2.x.18]  FE_ABF  [2.x.19]  FE_BDM, FE_DGBDM  [2.x.20]  FE_Nedelec, FE_DGNedelec  [2.x.21]  FE_RaviartThomas, FE_DGRaviartThomas  [2.x.22] 
*  Additionally, deal.II offers a mechanism to create a vector element from existing scalar or vector elements. The FESystem class is responsible for this: it doesn't describe shape functions itself, but assembles a vector-valued finite element from other finite element objects. This functionality is described  [2.x.23] ,  [2.x.24]  and other tutorial programs after that.
* 

* 
*  [2.x.25]  Support  for the implementation of  vector-valued elements is provided  by  the  class  FE_PolyTensor. Typically,  a  new  vector element should be derived from this class.
*  [1.x.3]
*  For each finite element conforming to any space of weakly differentiable functions like [1.x.4] or [1.x.5], we can define an analogue DG space by simply assigning all degrees of freedom on vertices, edges or faces to the interior of the cell. This is to be understood in the topological sense. The interpolation operator for such a degree of freedom would still be on the boundary.  While not done so consistently, we provide quite a few of these elements, plus those, which have no conforming counterparts, like FE_DGP. Here is a list of the current DG elements:  [2.x.26]   [2.x.27]  scalar: FE_DGP, FE_DGQ  [2.x.28]  scalar, different shape functions: FE_DGPMonomial, FE_DGPNonparametric, FE_DGQArbitraryNodes  [2.x.29]  vector-valued:  FE_DGBDM, FE_DGNedelec, FE_DGRaviartThomas  [2.x.30] 
* 

* 
*  [2.x.31]  The implementation of vector valued DG elements is supported by the class FE_DGVector, in the way, that only the vector polynomial space has to be provided. The actual class derived from this only has to implement a constructor and  [2.x.32] 
*  
*  [2.x.33] 

* 
* [0.x.4]*


* 
*  [2.x.34] 
*  The classes in this module are used to map from unit coordinates to the coordinates of a cell in real cell. Most commonly, one uses the MappingQ1 class that provides a Q1 (bi-/trilinear) mapping (i.e. a mapping that is isoparametric for the usual Q1 elements). However, there are other classes that implement higher-order mappings as well to provide for curvilinear elements. These are discussed in the  [2.x.35]  and  [2.x.36]  tutorial programs.
*  The MappingQ1Eulerian class is an extension to the MappingQ1 class in that it accepts a vector that describes a displacement field for each position of the domain. This is used in Eulerian computations without the need to actually move vertices after each time step.
*  In addition, the MappingC1 class provides for a boundary of the computational domain that is not only curved, but also has a continuous derivative at the interface between two cells on the boundary.
*  Finally, the MappingCartesian class is an optimization for elements that are brick-shaped and with edges parallel to the coordinate axes.
*  In the grand scheme of things, the pieces of this module interact with a variety of other parts of the library:

* 
* [1.x.6]
* 

* 
*  [2.x.37] 

* 
* [0.x.5]

include/deal.II-translator/A-headers/fe_vs_mapping_vs_fevalues_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  [1.x.0]
*  Most people create finite element (and, potentially, mapping) objects once but then never actually call any member functions on them
* 
*  -  they simply use them for assembly via the FEValues interface. The only other interaction most will have is by reading the  [2.x.1]  variable, but that is also just set during construction time. In other words, people never observe FiniteElement or Mapping objects actually [1.x.1] anything
* 
*  -  and that is completely by design.
*  This document is therefore for those who are interested in writing finite element or mapping classes and want to understand how FEValues works and interacts with the FiniteElement and Mapping classes. In the following, we will not make a distinction between FEValues (which acts on cells), FEFaceValues (which acts on faces), and FESubfaceValues (which acts on the children of a face of a cell) as they conceptually all work the same. Consequently, the term "FEValues" will be used generally for all three of these classes in the text below.
* 

*  [1.x.2]
*  Before going into detail about data and control flow, let us define which class is responsible for providing what kind of information.
*  [1.x.3]
*  FEValues is an abstraction that derived from the observation that almost everything one ever does in finite element codes only requires the evaluation of finite element shape functions at quadrature points. This could be, for example, the approximation of an integral of the form   [1.x.4] by quadrature   [1.x.5] but it is equally valid when wanting to generate graphical output: there we only need to know the values of a finite element field at the vertices of a mesh, and this too can be written as evaluating everything at quadrature points
* 
*  -  these quadrature points are then simply the vertices of the cells (provided, for example, by QTrapez).
*  FEValues' role is to provide a user the values of shape functions, their gradients, etc, at quadrature points. The same is true with some geometric information, e.g., the normal vectors at the quadrature points. To this end, it provides a large number of member functions in the FEValuesBase base class that allow a user to query basically everything one can ask for in regard to shape functions and geometry information, but only at the quadrature points for which the FEValues object was initialized.
*  FEValues does not actually compute this information itself. It really only provides a place to store it, and then orchestrates the interaction between mapping and finite element classes to have them compute what is requested and store the result in the locations provided by FEValues.
*  As a final note, recall that FEValues can provide an incredible array of information, but that almost all of it is not necessary in any given context. For example, to compute the integral above, it is not necessary to know the second derivatives of the shape functions, or to know the normal vectors at quadrature points. To this end, FEValues uses UpdateFlags in its interactions with the Mapping and FiniteElement class to determine what actually needs to be computed. This is discussed in slightly more detail in  [2.x.2] .
* 

*  [1.x.6]
*  Mappings (i.e., classes derived from the Mapping base class) are responsible for everything that has to do with the mapping from the reference (unit) cell  [2.x.3]  to each of the actual cells  [2.x.4] . This is facilitated by a mapping function  [2.x.5] . The mapping classes therefore implement interfaces that allow evaluating  [2.x.6]  to map forward points  [2.x.7]  from the reference cell to  [2.x.8] , and to map backward from the real cell to the reference cell using  [2.x.9] . Other common operations that mappings provide is to map vectors (which you can think of as vectors attached to a point  [2.x.10]  on the reference cell and pointing in certain directions) to their equivalent vectors on the real cell. This is, for example, what one needs to do for the gradients of shape functions: these are vectors defined on the reference cell, and we need to map these gradients to the real cell  [2.x.11] . Similar operations can also be defined for matrices (tensors of rank 2, as opposed to vectors which are tensors of rank 1) and higher order tensors.
*  Many of these mappings do not only need the map  [2.x.12]  itself, but also the gradients of this mapping, typically referred to as the Jacobian  [2.x.13] , as well as higher derivatives.
*  Since FEValues only ever needs to evaluate these things at quadrature points, mappings do not in general need to provide the ability to evaluate at [1.x.7] points. Rather, as we will see below, they will be initialized to use a set of quadrature points defined on the reference cell, will then be "re-initialized" for a particular cell, and all further operations will then only require the evaluation of  [2.x.14]  at these quadrature points on the real cell.
*  The mapping classes then have the dual role to (i) compute geometric information (e.g., the normal vectors, determinants of the Jacobians, etc) and putting them into the data structures from which FEValues can provide them to the user, and (ii) to provide the support finite elements need to map shape functions and their derivatives from the reference cell to the real cell.
* 

*  [1.x.8]
*  Finite element classes (i.e., classes derived from FiniteElement) are responsible for defining their shape functions, derivatives, and many other aspects on the reference cell, but also for computing the mapped values and derivatives on actual cells (obviously with the help of a mapping object). For the current discussion, only the latter role is important.
*  As with mappings, all that is important for us here is that the finite element classes can provide this information at given quadrature points, and that they can put the computed information into structures provided by FEValues and from which FEValues member functions can then pass it on to the user through the member functions in FEValuesBase.
* 

*  [1.x.9]
*  Let's say a user wants to compute the gradients of shape functions, for example to compute the integral above. Then they would initialize an FEValues object by giving the update_gradients flag (as is done in basically every tutorial program, starting with  [2.x.15] ). What this indicates is that the user expects the FEValues object to be able to provide the gradients of shape functions on the real cell, but expressed no expectation of any other information.
*  FEValues will then first have to find out what the mapping and finite element objects actually require of each other to make this happen. This already happens at the time the FEValues constructor is run. Because the mapping does not depend on the finite element (though the latter does depend on the former), FEValues first asks the finite element via  [2.x.16]  which [1.x.10] pieces of information it also requires to make the user request happen. As an example, if the finite element were of type FE_Q, then it would determine that in order to compute the gradients of the shape functions on the real cell  [2.x.17] , it will need to compute the gradients of the shape functions on the reference cell (something it can do on its own, without any external help) but that these reference gradients will then have to be multiplied by the inverse of the Jacobian of the mapping,  [2.x.18] , at each of the quadrature points. This multiplication is typically referred to as a [1.x.11], and so FE_Q's implementation of  [2.x.19]  function (provided in the intermediate class FE_Poly) will return both the original update_gradients flag as well as update_covariant_transformation.
*  In a second step, the FEValues object will then call the corresponding function in the mapping,  [2.x.20]  to determine what is required to provide both update_gradients and update_covariant_transformation. The former is not within the realm of the mapping, so is ignored. The latter will typically require the computation of the Jacobian matrix  [2.x.21]  first, which a typical mapping class will indicate by adding update_contravariant_transformation to the list.
* 

*  [1.x.12]
*  At this point, the FEValues object has found out the complete set of flags indicating what everyone has to compute to satisfy the user request. The next step, still during the construction of the FEValues object, stems from the realization that many things could be pre-computed once and then re-used every time we move to a real cell. An example would be the fact that to compute the gradients of the shape functions on the real cell, we need to know the gradients of the shape functions on the reference cell (at the quadrature points on the reference cell) and that these will always be the same: every time we visit a new cell, these values will remain the same, so it would be inefficient to re-compute them every time. Similar arguments can be made for some of the information computed by some of the mapping classes.
*  The FEValues object therefore initializes both the mapping and the finite element object it points to, using both the quadrature object and the final set of update flags computed as described in the previous section. This initialization involves pre-computing as much as these classes can already pre-compute given the set of update flags, and then storing this information for later use.
*  The question then arises: where to store this information. In practice, we do not want to store this information in the mapping or finite element object itself, because this would mean that (i) only one FEValues object could use any given mapping or finite element object at a time, and (ii) that these objects could not be used in a multithreaded context.
*  Rather, the approach works like this:
* 

* 
* 
*  - FEValues calls  [2.x.22]  (and FEFaceValues calls    [2.x.23]  and FESubfaceValues calls    [2.x.24]  with the quadrature object and   the final set of update flags. The implementation of these   functions in the classes derived from Mapping will then   allocate an object of a type derived from    [2.x.25]  where they can store essentially whatever   it is they find useful for later re-use.  [2.x.26]    itself does not actually provide any member variables of significance,   but it is really left to derived classes what they think they can   usefully pre-compute and store already at this time. If a mapping   has nothing to pre-compute (or the author of the mapping class is   lazy and does not want to think about what could possibly be   pre-computed), then such a class would simply derive its   own InternalData object from  [2.x.27]  without   actually adding any member variables.
*    The object so produced is then returned to the calling site   in FEValues and stored by the FEValues object. It will be handed   back every time later on the FEValues object wants any information   from the mapping, thereby providing the mapping object the   ability to read the data it had previously stored.
* 

* 
* 
*  - Secondly, FEValues also calls  [2.x.28]  (and FEFaceValues   calls  [2.x.29]  and FESubfaceValues calls    [2.x.30]  again with the quadrature object and   the final set of update flags. These functions do essentially the   same as their counterparts in the mappings, and again the object   so initialized, this time of a type derived from    [2.x.31]  will always be given back to the finite   element whenever the FEValues object wants something from the finite   element object at a later time.
*  This approach allows us to use finite element and mapping objects from multiple FEValues objects at the same time, and possibly from multiple threads at the same time. The point is simply that every user of a finite element or mapping object would hold their own, unique, object returned from the  [2.x.32]  functions, and that everything that ever happens happens on these objects, rather than on the member variables of the mapping or finite element object itself.
* 

*  [1.x.13]
*  All of the previous steps happened at the time the FEValues object was created. Up to this point, all we did was set up data structures, but nothing useful has been computed so far from the perspective of the user. This only happens when  [2.x.33]  is called on a concrete cell  [2.x.34] .
*  The things FEValues then does are, in this order:
* 

* 
* 
*  - FEValues figures out whether the cell is a translation   or other similarly simple transformation of the previous cell for which    [2.x.35]  was called. The result of this, stored in a    [2.x.36]  object will then be passed to mapping and   finite element to potentially simplify some computations. For example,   if the current cell is simply a translation of the previous one, then   there is no need to re-compute the Jacobian matrix  [2.x.37]  of the   mapping (or its inverse) because it will be the same as for the   previous cell.
* 

* 
* 
*  - Next,  [2.x.38]  calls    [2.x.39]  (and, obviously,   FEFaceValues calls  [2.x.40]  and   FESubfaceValues calls  [2.x.41]  The arguments   to this function include the cell (or face, or subface) which we are   asked to visit, as well as the cell similarity argument from   above, a reference to the object we had previously obtained from    [2.x.42]  and a reference to an object of type    [2.x.43]  into which the mapping is   supposed to write its results. In particular, it will need to   compute all mapping related information previously specified by   the update flags, and then write them into the output object.   Examples of fields in the output object that the mapping needs   to fill are the computation of JxW values, the computation of   Jacobian matrices and their inverses, and the normal vectors to   cells (if dim is less than spacedim) and faces.
* 

* 
* 
*  - Finally,  [2.x.44]  calls    [2.x.45]  (and, obviously,   FEFaceValues calls  [2.x.46]  and   FESubfaceValues calls  [2.x.47]  The arguments   to this function include the cell (or face, or subface) which we are   asked to visit, as well as the cell similarity argument from   above, a reference to the object we had previously obtained from    [2.x.48]  and a reference to an object of type    [2.x.49]  into which the mapping is   supposed to write its results.
*    In addition to these, the  [2.x.50]  function   also receives references to the mapping object in use, as well as the    [2.x.51]  object we had previously received from    [2.x.52]  The reason is that typically, the finite   element wants to map values or gradients of shape functions from the reference   cell to the actual cell, and these mappings are facilitated by the   various  [2.x.53]  functions
* 
*  -  which all require a reference   to the internal object that the FEValues object had previously acquired   from the mapping. This is probably best understood by looking at actual code,   and a simple yet instructive example can be found in    [2.x.54]  a function that works on general scalar,   polynomial finite element bases.
*    As with the mapping, the  [2.x.55]  functions then   use whatever information they had previously computed upon construction   of the FEValues object (i.e., when it called  [2.x.56]    and use this and the functions in the mapping to compute whatever was   requested as specified by the update flags.
*  This all done, we are finally in a position to offer the owner of the FEValues access to the fields originally requested via the update flags.
* 

* 
*  [2.x.57] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/functions_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  Functions are used in various places in deal.II, for example to describe boundary conditions, coefficients in equations, forcing terms, or exact solutions. Since closed form expressions for equations are often hard to pass along as function arguments, deal.II uses the Function base class to describe these objects. Essentially, the interface of this base class requires derived classes to implement the ability to return the value of a function at one or a list of particular locations, and possibly (if needed) of gradients or second derivatives of the function. With this, function objects can then be used by algorithms like  [2.x.1]   [2.x.2]  and other functions.
*  Some functions are needed again and again, and are therefore already provided in deal.II. This includes a function with a constant value; a function that is zero everywhere, or a vector-valued function for which only one vector component has a particular value and all other components are zero. Some more specialized functions are also defined in the Functions namespace.
* 

*  [1.x.0]
*  For time dependent computations, boundary conditions and/or right hand side functions may also change with time. Since at a given time step one is usually only interested in the spatial dependence of a function, it would be awkward if one had to pass a value for the time variable to all methods that use function objects. For example, the  [2.x.3]  function would have to take a time argument which it can use when it wants to query the value of the boundary function at a given time step. However, it would also have to do so if we are considering a stationary problem, for which there is nothing like a time variable.
*  To circumvent this problem, function objects are always considered spatial functions only. However, the Function class is derived from the FunctionTime base class that stores a value for a time variable, if so necessary. This way, one can define a function object that acts as a spatial function but can do so internally by referencing a particular time. In above example, one would set the time of the function object to the present time step before handing it off to the  [2.x.4]  method.
* 

*  [1.x.1]
*  The Function class is the most frequently used, but sometimes one needs a function the values of which are tensors, rather than scalars. The TensorFunction template can do this for you. Apart from the return type, the interface is most the same as that of the Function class.

* 
* [0.x.1]

include/deal.II-translator/A-headers/geodynamics_0.txt
[0.x.0]*
*  [2.x.0] 
* deal.II's  [2.x.1]  "tutorial" contains a set of programs that togetherform the geodynamics demonstration suite. The idea of these programs is todemonstrate techniques for advanced finite element software usingapplications from geodynamics, i.e. the investigation of processes in thesolid earth. By doing so, these programs are supposed to provide a basisfor more specialized, dedicated programs that can solve actual geodynamicsproblems, for example as part of the work of graduate students orpostdocs. A more thorough discussion of the motivation for these programsfollows below.
* Currently, the geodynamics testsuite contains the followingprograms:
* 
*  -  [2.x.2] : Elasticity
* 
*  -  [2.x.3] : A %parallel elasticity solver
* 
*  -  [2.x.4] : Porous media flow
* 
*  -  [2.x.5] : Multiphase flow through porous media
* 
*  -  [2.x.6] : Stokes flow
* 
*  -  [2.x.7] : Thermal convection (Boussinesq flow)
* 
*  -  [2.x.8] : A %parallel Boussinesq solver for mantle convection
* Some of these programs were developed under contract from the CaliforniaInstitute of Technology with support by the National Science Foundationunder Award No. EAR-0426271, the first of the grants that fundedthe [1.x.0] initiative. The recipient, Wolfgang Bangerth, gratefullyacknowledges this source of support.
* 

* [1.x.1]
* Adaptive mesh refinement (AMR) has long been identified as a key technologythat would aid in the accurate and efficient numerical solution of a number ofgeodynamics applications. It has been discussed in the geodynamics communityfor several years and has been a continuous topic on the task list of CIGsince its inception. Yet, relatively little has happened in this direction sofar. Only recently have there been attempts to use AMR in geodynamics: CIGsponsored a workshop on AMR technique in Boulder in October 2007; acollaboration between George Biros, Omar Ghattas, Mike Gurnis, and ShijieZhong's groups is currently developing a %parallel adaptive mantle convectionsolver; and some of the principal developers of deal.II eventually developedthe [1.x.2] for the simulationof mantle convection that is by now a rather established and widely usedcode.
* One of the reasons for the slow adoption of AMR techniques in geodynamics isthe relatively steep initial hurdle: codes have to provide the data structuresand algorithms to deal with adaptive meshes, finite elements have to be ableto deal with hanging nodes, etc. To do so efficiently and in sufficientgenerality adds several 10,000 lines of code to finite element programs, toomuch for the average student to do within the time frame of a dissertation. Onthe other hand, there are libraries that provide the infrastructure code onwhich applications supporting AMR can rapidly be built. deal.IIof course provides exactly this infrastructure.
* The goal of the geodynamics testsuite is to write programs for a variety oftopics relevant to geodynamics. Continuing in the style of the existing tutorialprograms
* 
*  -  an extensive introduction explaining the background andformulation of an application as well as the concepts of the numerical schemeused in its solution; detailed comments throughout the code explainingimplementation details; and a section showing numerical results
* 
*  -  we intend toprovide the resulting programs as well-documented applications solving modelproblems. In particular, they are aimed at the following goals: [2.x.9]  [2.x.10]  [1.x.3] The existing tutorial of deal.II has proven to  be an excellent starting point for graduate students and researchers to  jump-start developing their own applications. By providing programs that are  already close to the targeted application, first results can often be  obtained very quickly, both maintaining the initial enthusiasm during  development as well as allowing to spend research time on implementing  application specific behavior rather than using months of work on basic  infrastructure code supporting AMR.
*   Supporting this point is the fact that although there are  [1.x.4] presenting results obtained with deal.II, we are aware of  only a relatively small number of applications that have been built with deal.II from  scratch; all others have started as modifications of one of the tutorial  programs.
*  [2.x.11]  [1.x.5] The tutorial programs we propose to write will  provide students and researchers with a reference implementation of current  numerical technology such as AMR, higher order elements, sophisticated  linear and nonlinear solvers, stabilization techniques, etc. Providing these  as starting points for further development by others will also serve the  goal of training a new generation of geodynamicists in modern numerical  algorithms.
*  [2.x.12]  [1.x.6] In deal.II, it is fairly  simple to extend a set of equations by another equation, for example an  additional advected quantity that enters the existing equations as a right  hand side or in one of the coefficients. Since applications typically use  blocked matrices rather than the one-big-matrix-for-everything approach, it  is also not complicated to find suitable linear solvers for augmented  equations. Consequently, deal.II is a good tool for trying out more complex  formulations of problems, or more complete models and their effects on the  accuracy of solutions.
*  [2.x.13]  [1.x.7] deal.II provides many  interchangeable components that allow rapid prototyping of finite element  kinds and orders, stabilization techniques, or linear solvers. For example,  typically only a few lines of code have to be changed to replace low-order  by high-order elements. Through this, it becomes relatively simple to try  out higher order elements, a different block elimination solver, or a  different stabilization technique. In turn, this may help in benchmarking  applications both regarding computing times to solve as well as concerning  the accuracy of numerical solutions.
*   The applications in this module will already have been benchmarked for  correctness. Existing tutorial programs typically employ simpler rather than  more complicated solver schemes for exposition but frequently suggest more  complicated schemes including hints on how they might be implemented in an  appendix.
*  [2.x.14]  [1.x.8] The rapid prototyping abilities of deal.II may  also help in determining best algorithms on the scale of programs to which  deal.II is applicable, and then to implement this particular algorithm  (without the ability to change it easily) in a dedicated program that can  run on larger scale machines. For example, a small mantle convection code  built on deal.II may be used to determine whether second order elements are  useful for this purpose (see, for example, the results shown in   [2.x.15] ). If so, then one may use this kind of knowledge in larger codes,  such as the ASPECT code mentioned above. [2.x.16] 
* 

* 
* [0.x.1]

include/deal.II-translator/A-headers/geometry_and_primitives_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This group contains a number of classes that act as geometric primitives or primitives for other mathematical objects. For example, the Tensor  [2.x.1]  class provides tensors of rank  [2.x.2]  space dimensions. Likewise, the SymmetricTensor offers symmetric tensors.
*  Geometrically, the Point class is the foundation of all geometric descriptions in the deal.II library. It denotes a geometric point in  [2.x.3]  dimensional space. One can view a point as a vector the with  [2.x.4]  coordinates that connects the origin with that particular point; as such, the Point class is derived from tensors of rank 1 (i.e. vectors), but in contrast to arbitrary tensors points have the special connotation of points in space, and therefore have some additional properties.
*  In deal.II, meshes are built from line segments, quadrilaterals, or hexahedra (depending on the space dimension). The GeometryInfo class is used to describe properties of these basic objects in unit space (i.e. for the unit line, unit square, and unit cube). It offers static data members denoting the number of vertices per cell, lines per face, or where which vertex is located. This abstraction allows to write applications mostly independently of the actual space dimension: loops over all vertices would simply run from zero to  [2.x.5]  instead of from 0 to 4 (in 2d) or 0 to 8 (in 3d). In this way, the program will be correct in 2d as well as 3d, and one can run a program in a different space dimension simply by recompilation instead of having to change a significant portion of the code. These dimension-independent programming techniques are extensively discussed in the first few tutorial programs and are used throughout deal.II.

* 
* [0.x.1]

include/deal.II-translator/A-headers/global_dof_index_0.txt
[0.x.0]*
  [2.x.0] 
*  deal.II can be configured to use 64-bit indices for degrees of freedom, rather than the usual unsigned integers that default to 32-bit on most current systems. This is necessary since we want to be able to solve problems with more than 4 billion unknowns (the limit of what can be represented with 32-bit unsigned integers). At the same time, we do not want to indiscriminately replace all integers in deal.II with 64-bit versions, since this would increase memory use in many places where we represent quantities that will most definitely not be larger than 4 billion.
*  The data type we define for these indices to keep the bulk of the code base free of  [2.x.1] s is  [2.x.2]  If deal.II is configured as normal, this type is  [2.x.3] , but can be switched to  [2.x.4]  if the right flag is provided (see the ReadMe file). This page is intended to clarify when  [2.x.5]  must be used and when one can use a regular unsigned integer:
*   [2.x.6] 
*  <dt class="glossary"> [2.x.7]  GlobalDoFIndexBlockIndices [1.x.0]</dt>  [2.x.8]  The number of blocks is an unsigned int because the number is expected to be low, i.e less than four billions. However, the size of the block is a  [2.x.9]  because each block can be arbitrary large.  [2.x.10] 
*  <dt class="glossary"> [2.x.11]  GlobalDoFIndexCell [1.x.1]</dt>  [2.x.12]  The ID of cell is not unique: Cells with different levels of refinement and/or on different processors can have the same ID. Thus, all the data associated to cells can be unsigned int because on a single processor, one one mesh level, there will definitely not be more than 4 billion cells.  [2.x.13] 
*  <dt class="glossary"> [2.x.14]  GlobalDoFIndexDoFHandler [1.x.2]</dt>  [2.x.15]  The ID of each degree of freedom is unique in a parallel computation. Therefore, degrees of freedom are  [2.x.16]   [2.x.17] 
*  <dt class="glossary"> [2.x.18]  GlobalDoFIndexFullMatrix [1.x.3]</dt>  [2.x.19]  The numbers of row and column are  [2.x.20]  even if it is not expected that someone will create a FullMatrix with so many entries. However, some methods of the AffineConstraints class are templated on the matrix type and thus, the size of a FullMatrix has to be of the same type than the size of SparseMatrix.  [2.x.21] 
*  <dt class="glossary"> [2.x.22]  GlobalDoFIndexSparseMatrix [1.x.4]</dt>  [2.x.23]  The size of SparseMatrix can be arbitrary large and it is conceivable that with sufficient memory on a single node, one may generate a matrix with more than 4 billion rows or columns. Therefore,  [2.x.24]  is used. However, even for a large complex problem we can solve now, it is not reasonable to expect the number of non-zero entries in a sparse matrix to go over four billion. Thus, we still use unsigned int for, e.g.,  [2.x.25]  and similar functions.  [2.x.26] 
*   [2.x.27] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/glossary_0.txt
[0.x.0]*
  [2.x.0] 
*  This glossary explains a few terms that are frequently used in the documentation of classes of deal.II. The glossary often only gives a microscopic view of a particular concept; if you struggle with the bigger picture, it may therefore also be worth to consult the global overview of classes on the  [2.x.1]  page.
*   [2.x.2] 
*  <dt class="glossary"> [2.x.3]  GlossActive [1.x.0]</dt>  [2.x.4] A cell, face, or edge is defined as [1.x.1] if it is not refined any further, i.e., if it does not have children. Once a cell, face, or edge becomes a parent it is no longer active. Unless working with a multigrid algorithm, active cells are the only ones carrying degrees of freedom.  [2.x.5] 
* 

* 
*  <dt class="glossary"> [2.x.6]  GlossArtificialCell [1.x.2]</dt>  [2.x.7]  If a mesh is distributed across multiple MPI processes using the  [2.x.8]  class, each processor stores only the cells it owns, one layer of adjacent cells that are owned by other processors (called  [2.x.9]  "ghost cells"), all coarse level cells, and all cells that are necessary to maintain the invariant that adjacent cells must differ by at most one refinement level. The cells stored on each process that are not owned by this process and that are not ghost cells are called "artificial cells", and for these cells the predicate  [2.x.10]  returns true. Artificial cells are guaranteed to exist in the globally distributed mesh but they may be further refined on other processors. See the  [2.x.11]  "Distributed Computing paper" for more information.
*  The concept of artificial cells has no meaning for triangulations that store the entire mesh on each processor, i.e. the  [2.x.12]  class.  [2.x.13] 
* 

*  <dt class="glossary"> [2.x.14]  GlossBlockLA [1.x.3]</dt>
*   [2.x.15] It is often convenient to treat a matrix or vector as a collection of individual blocks. For example, in  [2.x.16]  (and other tutorial programs), we want to consider the global linear system  [2.x.17]  in the form [1.x.4]
*  where  [2.x.18]  are the values of velocity and pressure degrees of freedom, respectively,  [2.x.19]  is the mass matrix on the velocity space,  [2.x.20]  corresponds to the negative divergence operator, and  [2.x.21]  is its transpose and corresponds to the negative gradient.
*  Using such a decomposition into blocks, one can then define preconditioners that are based on the individual operators that are present in a system of equations (for example the Schur complement, in the case of  [2.x.22] ), rather than the entire matrix. In essence, blocks are used to reflect the structure of a PDE system in linear algebra, in particular allowing for modular solvers for problems with multiple solution components. On the other hand, the matrix and right hand side vector can also treated as a unit, which is convenient for example during assembly of the linear system when one may not want to make a distinction between the individual components, or for an outer Krylov space solver that doesn't care about the block structure (e.g. if only the preconditioner needs the block structure).
*  Splitting matrices and vectors into blocks is supported by the BlockSparseMatrix, BlockVector, and related classes. See the overview of the various linear algebra classes in the  [2.x.23]  module. The objects present two interfaces: one that makes the object look like a matrix or vector with global indexing operations, and one that makes the object look like a collection of sub-blocks that can be individually addressed. Depending on context, one may wish to use one or the other interface.
*  Typically, one defines the sub-structure of a matrix or vector by grouping the degrees of freedom that make up groups of physical quantities (for example all velocities) into individual blocks of the linear system. This is defined in more detail below in the glossary entry on  [2.x.24]  "Block (finite element)".  [2.x.25] 
* 

*  <dt class="glossary"> [2.x.26]  GlossBlock [1.x.5]</dt>  [2.x.27]  [1.x.6] Blocks are a generalization of  [2.x.28]  "components" in that they group together one or more components of a vector-valued finite element that one would like to consider jointly. One often wants to do this to define operators that correspond to the structure of a (part of a) differential operator acting on the vector-valued solution, such as the Schur complement solver in  [2.x.29] , or the block solvers and preconditioners of  [2.x.30] .
*  For the purpose of a discretization, blocks are the better concept to use since it is not always possible to address individual components of a solution. This is, in particular, the case for non- [2.x.31]  GlossPrimitive "primitive" elements. Take for instance the solution of the mixed Laplacian system with the FE_RaviartThomas element (see  [2.x.32] ). There, the first <tt>dim</tt> components are the directional velocities. Since the shape functions are linear combinations of those, these <tt>dim</tt> components constitute only a single block. On the other hand, the pressure variable is scalar and would form a the second block, but in the <tt>dim+1</tt>st component.
*  The minimal size of each block is dictated by the underlying finite element (a block consists of a single component for scalar elements, but in the case of the FE_RaviartThomas, for example, a block consists of <tt>dim</tt> components). However, several such minimal blocks can be grouped together into user defined blocks at will, and in accordance with the application. For instance, for the [1.x.7]<sub>2</sub><sup>[1.x.8]</sup>-[1.x.9]<sub>1</sub> (Taylor-Hood) Stokes element, there are [1.x.10]+1 components each of which could in principle form its own block. But we are typically more interested in having only two blocks, one of which consists of all the velocity vector components (i.e. this block would have [1.x.11] components) and the other having only the single pressure component.
*  [1.x.12] deal.II has a number of different finite element classes, all of which are derived from the FiniteElement base class (see the  [2.x.33]  "module on finite element classes"). With one exception, whether they are scalar or vector valued, they all define a single block: all vector components the finite element defines through its  [2.x.34]  function form a single block, i.e.  [2.x.35]  returns one.
*  The exception is the FESystem class that takes multiple simpler elements and connects them into more complicated ones. Consequently, it can have more than one block. A FESystem has as many blocks as it has base elements times their multiplicity (see the constructors of FESystem to understand this statement). In other words, it does not care how many blocks each base element has, and consequently you can produce a Stokes element that has only two blocks by creating the object

* 
* [1.x.13]
*  On the other hand, we could have produced a similar object with dim+1 blocks using

* 
* [1.x.14]
*  With the exception of the number of blocks, the two objects are the same for all practical purposes, however.
*  [1.x.15] While we have defined blocks above in terms of the vector components of a vector-valued solution function (or, equivalently, in terms of the vector-valued finite element space), every shape function of a finite element is part of one block or another. Consequently, we can partition all degrees of freedom defined on a DoFHandler into individual blocks. Since by default the DoFHandler class enumerates degrees of freedom in a more or less random way, you will first want to call the  [2.x.36]  function to make sure that all degrees of freedom that correspond to a single block are enumerated consecutively.
*  If you do this, you naturally partition matrices and vectors into blocks as well (see  [2.x.37]  "block (linear algebra)).  In most cases, when you subdivide a matrix or vector into blocks, you do so by creating one block for each block defined by the finite element (i.e. in most practical cases the FESystem object). However, this needs not be so: the  [2.x.38]  function allows to group several vector components or finite element blocks into the same logical block (see, for example, the  [2.x.39]  " [2.x.40] " or  [2.x.41]  tutorial programs, as opposed to  [2.x.42] ). As a consequence, using this feature, we can achieve the same result, i.e. subdividing matrices into  [2.x.43]  blocks and vectors into 2 blocks, for the second way of creating a Stokes element outlined above using an extra argument as we would have using the first way of creating the Stokes element with two blocks right away.
*  More information on this topic can be found in the documentation of FESystem, the  [2.x.44]  module and the tutorial programs referenced therein.
*  [1.x.16] Many functions allow you to restrict their operation to certain vector components or blocks. For example, this is the case for the functions that interpolate boundary values: one may want to only interpolate the boundary values for the velocity block of a finite element field but not the pressure block. The way to do this is by passing a BlockMask argument to such functions, see the  [2.x.45]  "block mask entry of this glossary".  [2.x.46] 
* 

*  <dt class="glossary"> [2.x.47]  GlossBlockMask [1.x.17]</dt>
*   [2.x.48]  In much the same way as one can think of elements as being composed of physical vector components (see  [2.x.49] ) or logical blocks (see  [2.x.50] ), there is frequently a need to select a set of such blocks for operations that are not intended to be run on [1.x.18] blocks of a finite element space. Selecting which blocks to work on happens using the BlockMask class.
*  Block masks work in much the same way as component masks, including the fact that the BlockMask class has similar semantics to the ComponentMask class. See  [2.x.51]  "the glossary entry on component masks" for more information.
* 

* 
*  [2.x.52]  While components and blocks provide two alternate but equally valid viewpoints on finite elements with multiple vector components, the fact is that throughout the library there are far more places where you can pass a ComponentMask argument rather than a BlockMask argument. Fortunately, one can be converted into the other, using the syntax  [2.x.53]  is a variable of type BlockMask. In other words, if you have a block mask but need to call a function that only accepts a component mask, this syntax can be used to obtain the necessary component mask.
*  [1.x.19] Block masks are typically created by asking the finite element to generate a block mask from certain selected vector components using code such as this where we create a mask that only denotes the velocity components of a Stokes element (see  [2.x.54] ):

* 
* [1.x.20]
*  The result is a block mask that, in 1d as well as 2d and 3d, would have values  [2.x.55] . Similarly, using

* 
* [1.x.21]
*  would result in a mask  [2.x.56]  in any dimension.
*  Note, however, that if we had defined the finite element in the following way:

* 
* [1.x.22]
*  then the code

* 
* [1.x.23]
*  would yield a block mask that in 2d has elements  [2.x.57]  because the element has  [2.x.58]  components and equally many blocks. See the discussion on what a block represents exactly in the  [2.x.59]  "block entry of this glossary".  [2.x.60] 
* 

*  <dt class="glossary"> [2.x.61]  GlossBoundaryForm [1.x.24]</dt>
*   [2.x.62] For a dim-dimensional triangulation in dim-dimensional space, the boundary form is a vector defined on faces. It is the vector product of the image of coordinate vectors on the surface of the unit cell. It is a vector normal to the surface, pointing outwards and having the length of the surface element.
*  A more general definition would be that (at least up to the length of this vector) it is exactly that vector that is necessary when considering integration by parts, i.e. equalities of the form  [2.x.63] . Using this definition then also explains what this vector should be in the case of domains (and corresponding triangulations) of dimension  [2.x.64]  that are embedded in a space  [2.x.65] : in that case, the boundary form is still a vector defined on the faces of the triangulation; it is orthogonal to all tangent directions of the boundary and within the tangent plane of the domain. Note that this is compatible with case  [2.x.66]  since there the tangent plane is the entire space  [2.x.67] .
*  In either case, the length of the vector equals the determinant of the transformation of reference face to the face of the current cell.  [2.x.68] 
* 

*  <dt class="glossary"> [2.x.69]  GlossBoundaryIndicator [1.x.25]</dt>
*   [2.x.70]  In a Triangulation object, every part of the boundary may be associated with a unique number (of type  [2.x.71]  that is used to determine what kinds of boundary conditions are to be applied to a particular part of a boundary. The boundary is composed of the faces of the cells and, in 3d, the edges of these faces.
*  By default, all boundary indicators of a mesh are zero, unless you are reading from a mesh file that specifically sets them to something different, or unless you use one of the mesh generation functions in namespace GridGenerator that have a  [2.x.72]  "colorize" option. A typical piece of code that sets the boundary indicator on part of the boundary to something else would look like this, here setting the boundary indicator to 42 for all faces located at  [2.x.73] :

* 
* [1.x.26]
*  This calls functions  [2.x.74]  In 3d, it may also be appropriate to call  [2.x.75]  instead on each of the selected faces. To query the boundary indicator of a particular face or edge, use  [2.x.76] 
*  Many of the functions in namespaces DoFTools and VectorTools take arguments that specify which part of the boundary to work on, and they specifically refer to boundary_ids. Examples are  [2.x.77]   [2.x.78]   [2.x.79]  and  [2.x.80]   [2.x.81] 
* 

* 
*  [2.x.82]  Boundary indicators are inherited from mother faces and edges to their children upon mesh refinement. Some more information about boundary indicators is also presented in a section of the documentation of the Triangulation class.
* 

* 
*  [2.x.83]  For parallel triangulations of type  [2.x.84]  it is not enough to set boundary indicators only once at the beginning. See the long discussion on this topic in the class documentation of  [2.x.85]  .  [2.x.86] 
* 

*  <dt class="glossary"> [2.x.87]  GlossCoarseMesh [1.x.27]</dt>  [2.x.88]    A "coarse mesh" in deal.II is a triangulation object that consists only   of cells that are not refined, i.e., a mesh in which no cell is a child   of another cell. This is generally how triangulations are first   constructed in deal.II, for example using (most of) the functions in   namespace GridGenerator, the functions in class GridIn, or directly   using the function  [2.x.89]  One can of   course do computations on such meshes, but most of the time (see, for   example, almost any of the tutorial programs) one first refines the   coarse mesh globally (using  [2.x.90]    or adaptively (in that case first computing a refinement   criterion, then one of the functions in namespace GridRefinement,   and finally calling    [2.x.91]  The mesh is   then no longer a "coarse mesh", but a "refined mesh".
*    In some contexts, we also use the phrase "the coarse mesh of a   triangulation", and by that mean that set of cells that the triangulation   started out with, i.e., from which all the currently    [2.x.92]  "active cells" of the triangulation have been obtained   by mesh refinement. (Some of the coarse mesh cells may of course also   be active if they have never been refined.)
*    Triangulation objects store cells in [1.x.28]: in   particular, all cells of a coarse mesh are on level zero. Their   children (if we executed  [2.x.93]  on a   coarse mesh) would then be at level one, etc. The coarse mesh of a   triangulation (in the sense of the previous paragraph) then   consists of exactly the level-zero cells of a triangulation. (Whether   they are active (i.e., have no children) or have been refined is not   important for this definition.)
*    Most of the triangulation classes in deal.II store the entire coarse   mesh along with at least some of the refined cells. (Both the    [2.x.94]  and  [2.x.95]  classes   actually store [1.x.29] cells of the entire mesh, whereas some   other classes such as  [2.x.96]  only   store [1.x.30] of the  [2.x.97]  "active cells" on   each process in a parallel computation.) In those cases,   one can query the triangulation for all coarse mesh   cells. Other triangulation classes (e.g.,    [2.x.98]  only store a part   of the coarse mesh. See also    [2.x.99]  "the concept of coarse cell ids"   for that case.  [2.x.100] 
* 

*  <dt class="glossary"> [2.x.101]  GlossCoarseCellId [1.x.31]</dt>  [2.x.102]    Most of the triangulation classes in deal.II, notably    [2.x.103]   [2.x.104]  and    [2.x.105]  store the entire    [2.x.106]  "coarse mesh"   of a triangulation on each process of a parallel computation. On the   other hand, this is not the case for other classes, notably for    [2.x.107]  which is designed for cases   where even the coarse mesh is too large to be stored on each process   and needs to be partitioned.
*    In those cases, it is often necessary in algorithms to reference a coarse   mesh cell uniquely. Because the triangulation object on the current   process does not actually store the entire coarse mesh, one needs to have   a globally unique identifier for each coarse mesh cell that is independent   of the index within level zero of the triangulation stored locally. This   globally unique ID is called the "coarse cell ID". It can be accessed via   the function call  
* [1.x.32]
*    where `triangulation` is the triangulation to which the iterator   `coarse_cell` pointing to a cell at level zero belongs. Here,   `coarse_cell->index()` returns the index of that cell within its   refinement level (see  [2.x.108]  This is a number   between zero and the number of coarse mesh cells stored on the   current process in a parallel computation; it uniquely identifies   a cell on that parallel process, but different parallel processes may   use that index for different cells located at different coordinates.
*    For those classes that store all coarse mesh cells on each process,   the  [2.x.109]  simply   returns a permutation of the possible argument values. In the   simplest cases, such as for a sequential or a parallel shared   triangulation, the function will in fact simply return the   value of the argument. For others, such as    [2.x.110]  the ordering of   coarse cell IDs is not the same as the ordering of coarse   cell indices. Finally, for classes such as    [2.x.111]  the function returns   the globally unique ID, which is from a larger set of possible   indices than the indices of the coarse cells actually stored on   the current process.  [2.x.112] 
* 

*  <dt class="glossary"> [2.x.113]  GlossColorization [1.x.33]</dt>  [2.x.114]  [2.x.115] Colorization [2.x.116]  is the process of marking certain parts of a Triangulation with different labels. The use of the word  [2.x.117] color [2.x.118]  comes from cartography, where countries on a map are made visually distinct from each other by assigning them different colors. Using the same term  [2.x.119] coloring [2.x.120]  is common in mathematics, even though we assign integers and not hues to different regions. deal.II refers to two processes as coloring:
*   [2.x.121]     [2.x.122]  Most of the functions in the GridGenerator namespace take an optional   argument  [2.x.123] . This argument controls whether or not the   different parts of the boundary will be assigned different    [2.x.124]  "boundary indicators".   Some functions also assign different    [2.x.125]  "material indicators"   as well. [2.x.126]     [2.x.127]  The function  [2.x.128]  computes a   decomposition of a Triangulation (more exactly, a range of iterators). No   two adjacent cells are given the same color. [2.x.129]   [2.x.130]   [2.x.131] 
* 

*  <dt class="glossary"> [2.x.132]  GlossComponent [1.x.34]</dt>
*   [2.x.133]  When considering systems of equations in which the solution is not just a single scalar function, we say that we have a [1.x.35] with a [1.x.36]. For example, the vector solution in the elasticity equation considered in  [2.x.134]  is  [2.x.135]  consisting of the displacements in each of the three coordinate directions. The solution then has three elements. Similarly, the 3d Stokes equation considered in  [2.x.136]  has four elements:  [2.x.137] . We call the elements of the vector-valued solution [1.x.37] in deal.II. To be well-posed, for the solution to have  [2.x.138]  components, there need to be  [2.x.139]  partial differential equations to describe them. This concept is discussed in great detail in the  [2.x.140]  module.
*  In finite element programs, one frequently wants to address individual elements (components) of this vector-valued solution, or sets of components. For example, we do this extensively in  [2.x.141] , and a lot of documentation is also provided in the module on  [2.x.142]  "Handling vector valued problems". If you are thinking only in terms of the partial differential equation (not in terms of its discretization), then the concept of [1.x.38] is the natural one.
*  On the other hand, when talking about finite elements and degrees of freedom, [1.x.39] are not always the correct concept because components are not always individually addressable. In particular, this is the case for  [2.x.143]  "non-primitive finite elements". Similarly, one may not always [1.x.40] to address individual components but rather sets of components &mdash; e.g. all velocity components together, and separate from the pressure in the Stokes system, without further splitting the velocities into their individual components. In either case, the correct concept to think in is that of a  [2.x.144]  "block".  Since each component, if individually addressable, is also a block, thinking in terms of blocks is most frequently the better strategy.
*  For a given finite element, the number of components can be queried using the  [2.x.145]  function, and you can find out which vector components are nonzero for a given finite element shape function using  [2.x.146]  The values and gradients of individual components of a shape function (if the element is primitive) can be queried using the  [2.x.147]  and  [2.x.148]  functions on the reference cell. The  [2.x.149]  and  [2.x.150]  functions do the same on a real cell. See also the documentation of the FiniteElement and FEValues classes.
*  [1.x.41] Many functions allow you to restrict their operation to certain vector components or blocks. For example, this is the case for the functions that interpolate boundary values: one may want to only interpolate the boundary values for the velocity components of a finite element field but not the pressure component. The way to do this is by passing a ComponentMask argument to such functions, see the  [2.x.151]  "component mask entry of this glossary".  [2.x.152] 
* 

*  <dt class="glossary"> [2.x.153]  GlossComponentMask [1.x.42]</dt>
*   [2.x.154]  When using vector-valued elements (see  [2.x.155] ) to solve systems of equations, one frequently wants to restrict some operations to only certain solution variables. For example, when solving the Stokes equations, one may wish to only interpolate boundary values for the velocity components but not the pressure. In deal.II, this is typically done by passing functions a [1.x.43]. Component masks are always specified as a ComponentMask object which one can think of as an array with as many entries as the finite element has components (e.g., in the Stokes case, there are  [2.x.156]  components) and where each entry is either true or false. In the example where we would like to interpolate boundary values only for the velocity components of the Stokes system, this component mask would then be  [2.x.157]  in 3d to indicate that no boundary values shall be set for the pressure variable (the last of the  [2.x.158]  vector components of the solution.
*  There are many functions that take such component masks, for example  [2.x.159]   [2.x.160]   [2.x.161]  etc. In some cases, there are multiple functions with these names but only some of them have a component mask argument.
*  [1.x.44] Many of the functions that take a component mask object that has been default constructed to indicate [1.x.45], i.e., as if the vector had the correct length and was filled with only  [2.x.162]  values. The reason is that default initialized objects can be constructed in place using the code snippet  [2.x.163]  and can thus be used as a default argument in function signatures.
*  In other words, ComponentMask objects can be in one of two states: They can have been initialized by a vector of booleans with a nonzero length; in that case, they represent a mask of a particular length where some elements may be true and others may be false. Or, the ComponentMask may have been default initialized (using the default constructor) in which case it represents an array of indefinite length (i.e., a length appropriate to the circumstances) in which [1.x.46] is true.
*  [1.x.47] Component masks are typically created by asking the finite element to generate a component mask from certain selected components using code such as this where we create a mask that only denotes the velocity components of a Stokes element (see  [2.x.164] ):

* 
* [1.x.48]
*  The result is a component mask that, in 2d, would have values  [2.x.165] . Similarly, using

* 
* [1.x.49]
*  would result in a mask  [2.x.166]  in 2d. Of course, in 3d, the result would be  [2.x.167] .
* 

* 
*  [2.x.168]  Just as one can think of composed elements as being made up of  [2.x.169]  "components" or  [2.x.170]  "blocks", there are component masks (represented by the ComponentMask class) and  [2.x.171]  "block masks" (represented by the BlockMask class). The FiniteElement class has functions that convert between the two kinds of objects.
* 

* 
*  [2.x.172]  Not all component masks actually make sense. For example, if you have a FE_RaviartThomas object in 2d, then it doesn't make any sense to have a component mask of the form  [2.x.173]  because you try to select individual vector components of a finite element where each shape function has both  [2.x.174]  and  [2.x.175]  velocities. In essence, while you can of course create such a component mask, there is nothing you can do with it.  [2.x.176] 
* 

* 
*  <dt class="glossary"> [2.x.177]  GlossCompress [1.x.50]</dt>
*   [2.x.178]  For %parallel computations, deal.II uses the vector and matrix classes defined in the PETScWrappers and TrilinosWrappers namespaces. When running programs in %parallel using MPI, these classes only store a certain number of rows or elements on the current processor, whereas the rest of the vector or matrix is stored on the other processors that belong to our MPI universe. This presents a certain problem when you assemble linear systems: we add elements to the matrix and right hand side vectors that may or may not be stored locally. Sometimes, we may also want to just [1.x.51] an element, not add to it.
*  Both PETSc and Trilinos allow adding to or setting elements that are not locally stored. In that case, they write the value that we want to store or add into a cache, and we need to call one of the functions  [2.x.179]   [2.x.180]   [2.x.181]  or  [2.x.182]  which will then ship the values in the cache to the MPI process that owns the element to which it is supposed to be added or written to. Due to the MPI model that only allows to initiate communication from the sender side (i.e. in particular, it is not a remote procedure call), these functions are collective, i.e. they need to be called by all processors.
*  There is one snag, however: both PETSc and Trilinos need to know whether the operation that these  [2.x.183]  functions invoke applies to adding elements or setting them.  In some cases, not all processors may be adding elements, for example if a processor does not own any cells when using a very  [2.x.184]  "coarse (initial) mesh". For this reason, compress() takes an argument of type VectorOperation, which can be either ::%add, or ::%insert. This argument is required for vectors and matrices starting with the 7.3 release.
*  In short, you need to call compress() in the following cases (and only in those cases, though calling compress() in other cases just costs some performance):
*  1. At the end of your assembly loop on matrices and vectors. This needs to be done if you write entries directly or if you use  [2.x.185]  Use  [2.x.186] 
*  2. When you are done setting individual elements in a matrix/vector before any other operations are done (adding to elements, other operations like scaling, solving, reading, etc.). Use  [2.x.187] 
*  3. Like in 2., but for adding values to individual elements. Use  [2.x.188] 
*  All other operations like scaling or adding vectors, assignments, calls into deal.II (VectorTools, AffineConstraints, ...) or solvers do not require calls to compress().  [2.x.189] 
* 

* 
*  [2.x.190]  Compressing is an operation that only applies to vectors whose elements are uniquely owned by one and only one processor in a parallel MPI universe. It does not apply to  [2.x.191]  "vectors with ghost elements".
* 

*  <dt class="glossary"> [2.x.192]  GlossConcept [1.x.52]</dt>
*   [2.x.193]  There are several places in deal.II where we require that a type in a template match a certain interface or behave in a certain way: such constraints are called  [2.x.194] concepts [2.x.195]  in C++. See the discussion in  [2.x.196]  for more information and a list of concepts in deal.II.  [2.x.197] 
* 

*  <dt class="glossary"> [2.x.198]  GlossDimension [1.x.53]</dt>
*   [2.x.199]  Many classes and functions in deal.II have two template parameters,  [2.x.200]  and  [2.x.201]  An example is the basic Triangulation class:

* 
* [1.x.54]
*  In all of these contexts where you see `dim` and `spacedim` referenced, these arguments have the following meaning:
*   [2.x.202]     [2.x.203]   [2.x.204]  denotes the dimensionality of the mesh. For example, a mesh   that consists of line segments is one-dimensional and consequently   corresponds to `dim==1`. A mesh consisting of quadrilaterals then has   `dim==2` and a mesh of hexahedra has `dim==3`. [2.x.205] 
*     [2.x.206]   [2.x.207]  denotes the dimensionality of the space in which such a   mesh lives. Generally, one-dimensional meshes live in a one-dimensional   space, and similarly for two-dimensional and three-dimensional meshes   that subdivide two- and three-dimensional domains. Consequently, the  [2.x.208]    spacedim template argument has a default equal to  [2.x.209]  But this need   not be the case: For example, we may want to solve an equation for   sediment transport on the surface of the Earth. In this case, the domain   is the two-dimensional surface of the Earth (`dim==2`) that lives in a   three-dimensional coordinate system (`spacedim==3`). [2.x.210]   [2.x.211] 
*  More generally, deal.II can be used to solve partial differential equations on [1.x.55] that are embedded in higher dimensional space. In other words, these two template arguments need to satisfy `dim <= spacedim`, though in many applications one simply has `dim == spacedim`.
*  Following the convention in geometry, we say that the "codimension" is defined as `spacedim-dim`. In other words, a triangulation consisting of quadrilaterals whose coordinates are three-dimensional (for which we would then use a `Triangulation<2,3>` object) has "codimension one".
*  Examples of uses where these two arguments are not the same are shown in  [2.x.212] ,  [2.x.213] ,  [2.x.214] .  [2.x.215] 
* 

*  <dt class="glossary"> [2.x.216]  GlossDoF [1.x.56]</dt>
*   [2.x.217]  The term "degree of freedom" (often abbreviated as "DoF") is commonly used in the finite element community to indicate two slightly different, but related things. The first is that we'd like to represent the finite element solution as a linear combination of shape functions, in the form  [2.x.218] . Here,  [2.x.219]  is a vector of expansion coefficients. Because we don't know their values yet (we will compute them as the solution of a linear or nonlinear system), they are called "unknowns" or "degrees of freedom". The second meaning of the term can be explained as follows: A mathematical description of finite element problem is often to say that we are looking for a finite dimensional function  [2.x.220]  that satisfies some set of equations (e.g.  [2.x.221]  for all test functions  [2.x.222] ). In other words, all we say here that the solution needs to lie in some space  [2.x.223] . However, to actually solve this problem on a computer we need to choose a basis of this space; this is the set of shape functions  [2.x.224]  we have used above in the expansion of  [2.x.225]  with coefficients  [2.x.226] . There are of course many bases of the space  [2.x.227] , but we will specifically choose the one that is described by the finite element functions that are traditionally defined locally on the cells of the mesh. Describing "degrees of freedom" in this context requires us to simply [1.x.57] the basis functions of the space  [2.x.228] . For  [2.x.229]  elements this means simply enumerating the vertices of the mesh in some way, but for higher elements one also has to enumerate the shape functions that are associated with edges, faces, or cell interiors of the mesh. The class that provides this enumeration of the basis functions of  [2.x.230]  is called DoFHandler.  The process of enumerating degrees of freedom is referred to as "distributing DoFs" in deal.II.  [2.x.231] 
*  <dt class="glossary"> [2.x.232]  GlossDirectionFlag [1.x.58]</dt>
*   [2.x.233] The [1.x.59] is used in triangulations embedded in a higher dimensional space to denote the orientation of cells and make the manifold oriented. It is accessed using  [2.x.234]  and set by the Triangulation class upon creation of a triangulation. You can change all direction flags of a triangulation using the  [2.x.235]  function.
*  The flag is necessary to make cases like this work: assume we have a one-dimensional mesh embedded in a two-dimensional space,
*     [2.x.236] 
*  In one dimensional meshes in one dimensional space, we can always make sure that the location of the left vertex of a cell has a smaller value than the location of the right vertex. However, if we embed a mesh in a higher dimensional space, we can no longer do this. For example, the cells in the mesh above may be described by the following vertex sets: <code>(0,1), (1,2), (3,2), (4,3), (4,5)</code>. (As a side remark, note that here we have vertices
* 
*  -  e.g. vertex 2
* 
*  -  that are the right end points of more than one cell.) If we define the normal to each cell as that unit vector that is right perpendicular to the vector that connects the first to the second vertex of the line, then we would end up with the following picture:
*     [2.x.237] 
*  In other words, this one-dimensional manifold is not oriented. We could in principle revert the order of vertices when creating such a mesh (though there are good reasons not to do so, for example because this mesh may have resulted from extracting the surface mesh of a two dimensional mesh, and we want to preserve the order of vertices of each line segment because they currently match the order of vertices of the faces of the 2d cells). An alternative strategy, chosen in deal.II, is to simply associate with each cell whether the normal should be the left or right normal to the cell. (The default is right normals.) In the example above, the flags for the five cells would be <code>true, true, false, false, true</code>. Multiplying the right normal with plus or minus one, depending on the value of the flag on each cell, yields a set of normal vectors that orient the manifold.
*  Similar issues happen with two-dimensional meshes in three space dimensions. We note that it would not be possible to find consistent direction flags if the two-dimensional manifold is not orientable; such manifolds are not currently supported by deal.II.  [2.x.238] 
* 

*  <dt class="glossary"> [2.x.239]  GlossDistorted [1.x.60]</dt>
*   [2.x.240] A [1.x.61] is a cell for which the mapping from the reference cell to real cell has a Jacobian whose determinant is non-positive somewhere in the cell. Typically, we only check the sign of this determinant at the vertices of the cell. The function  [2.x.241]  computes these determinants at the vertices.
*  By way of example, if all of the determinants are of roughly equal value and on the order of  [2.x.242]  then the cell is well-shaped. For example, a square cell or face has determinants equal to  [2.x.243]  whereas a strongly sheared parallelogram has a determinant much smaller. Similarly, a cell with very unequal edge lengths will have widely varying determinants. Conversely, a pinched cell in which the location of two or more vertices is collapsed to a single point has a zero determinant at this location. Finally, an inverted or twisted cell in which the location of two vertices is out of order will have negative determinants.
*  The following two images show a well-formed, a pinched, and a twisted cell for both 2d and 3d:
*   [2.x.244] 
*   [2.x.245] 
*  Distorted cells can appear in two different ways: The original  [2.x.246]  "coarse mesh" can already contain such cells, or they can be created as the result of moving or distorting a mesh by a relatively large amount.
*  If the appropriate flag is given upon creation of a triangulation, the function  [2.x.247]  which is called by the various functions in GridGenerator and GridIn (but can also be called from user code, see  [2.x.248]  and the example at the end of  [2.x.249] ), will signal the creation of coarse meshes with distorted cells by throwing an exception of type  [2.x.250]  There are legitimate cases for creating meshes with distorted cells (in particular collapsed/pinched cells) if you don't intend to assemble anything on these cells. For example, consider a case where one would like to simulate the behavior of an elastic material with a fluid-filled crack such as an oil reservoir. If the pressure becomes too large, the crack is closed
* 
*  -  and the cells that discretize the crack volume are collapsed to zero volume. As long as you don't integrate over these cells to simulate the behavior of the fluid (of which there isn't any if the crack has zero volume), such meshes are perfectly legitimate. As a consequence,  [2.x.251]  does not simply abort the program, but throws an exception that contains a list of cells that are distorted; this exception can be caught and, if you believe that you can ignore this condition, you can react by doing nothing with the caught exception.
*  The function  [2.x.252]  can, in some cases, fix distorted cells on refined meshes by moving around the vertices of a distorted child cell that has an undistorted parent.
*  Note that the Triangulation class does not test for the presence of distorted cells by default, since the determination whether a cell is distorted or not is not a cheap operation. If you want a Triangulation object to test for distortion of cells, you need to specify this upon creation of the object by passing the appropriate flag.  [2.x.253] 
* 

*  <dt class="glossary"> [2.x.254]  distributed_paper                           [1.x.62]</dt>
*   [2.x.255] The "distributed computing paper" is a paper by W. Bangerth, C. Burstedde, T. Heister and M. Kronbichler titled "Algorithms and Data Structures for Massively Parallel Generic Finite Element Codes" that describes the implementation of %parallel distributed computing in deal.II, i.e. computations where not only the linear system is split onto different machines as in, for example,  [2.x.256] , but also the Triangulation and DoFHandler objects. In essence, it is a guide to the  [2.x.257]  namespace and the techniques used in  [2.x.258] .
*  The full reference for the paper is as follows:

* 
* [1.x.63]
* 
*  For massively %parallel computations, deal.II builds on the [1.x.64] library. If you use this functionality, please also cite the p4est paper listed at their website.  [2.x.259] 
* 

*  <dt class="glossary"> [2.x.260]  GlossFaceOrientation [1.x.65]</dt>  [2.x.261] In a triangulation, the normal vector to a face can be deduced from the face orientation by applying the right hand side rule (x,y
* 
-> normal).  We note, that in the standard orientation of faces in 2d, faces 0 and 2 have normals that point into the cell, and faces 1 and 3 have normals pointing outward. In 3d, faces 0, 2, and 4 have normals that point into the cell, while the normals of faces 1, 3, and 5 point outward. This information, again, can be queried from  [2.x.262] 
*  However, it turns out that a significant number of 3d meshes cannot satisfy this convention. This is due to the fact that the face convention for one cell already implies something for the neighbor, since they share a common face and fixing it for the first cell also fixes the normal vectors of the opposite faces of both cells. It is easy to construct cases of loops of cells for which this leads to cases where we cannot find orientations for all faces that are consistent with this convention.
*  For this reason, above convention is only what we call the  [2.x.263] standard orientation [2.x.264] . deal.II actually allows faces in 3d to have either the standard direction, or its opposite, in which case the lines that make up a cell would have reverted orders, and the normal vector would have the opposite direction. You can ask a cell whether a given face has standard orientation by calling <tt>cell->face_orientation(face_no)</tt>: if the result is  [2.x.265]  then the face has standard orientation, otherwise its normal vector is pointing the other direction. There are not very many places in application programs where you need this information actually, but a few places in the library make use of this. Note that in 2d, the result is always  [2.x.266]  However, while every face in 2d is always in standard orientation, you can sometimes specify something to assume that this is not so; an example is the function  [2.x.267] 
*  There are two other flags that describe the orientation of a face: face_flip and face_rotation. Some documentation for these exists in the GeometryInfo class. An example of their use in user code is given in the  [2.x.268]  function.  [2.x.269] 
* 

*  <dt class="glossary"> [2.x.270]  GlossGeneralizedSupport [1.x.66]</dt>  [2.x.271] "Generalized support points" are, as the name suggests, a generalization of  [2.x.272]  "support points". The latter are used to describe that a finite element simply [1.x.67] values at individual points (the "support points"). If we call these points  [2.x.273]  (where the hat indicates that these points are defined on the reference cell  [2.x.274] ), then one typically defines shape functions  [2.x.275]  in such a way that the [1.x.68]  [2.x.276]  simply evaluate the function at the support point, i.e., that  [2.x.277] , and the basis is chosen so that  [2.x.278]  where  [2.x.279]  is the Kronecker delta function. This leads to the common  [2.x.280]  "Lagrange elements".
*  (In the vector valued case, the only other piece of information besides the support points  [2.x.281]  that one needs to provide is the [1.x.69]  [2.x.282]  the  [2.x.283] th node functional corresponds, so that  [2.x.284] .)
*  On the other hand, there are other kinds of elements that are not defined this way. For example, for the lowest order Raviart-Thomas element (see the FE_RaviartThomas class), the node functional evaluates not individual components of a vector-valued finite element function with  [2.x.285]  components, but the [1.x.70] of this vector:  [2.x.286] , where the  [2.x.287]  are the normal vectors to the face of the cell on which  [2.x.288]  is located. In other words, the node functional is a [1.x.71] of the components of  [2.x.289]  when evaluated at  [2.x.290] . Similar things happen for the BDM, ABF, and Nedelec elements (see the FE_BDM, FE_ABF, FE_Nedelec classes).
*  In these cases, the element does not have [1.x.72] because it is not purely interpolatory; however, some kind of interpolation is still involved when defining shape functions as the node functionals still require point evaluations at special points  [2.x.291] . In these cases, we call the points [1.x.73].
*  Finally, there are elements that still do not fit into this scheme. For example, some hierarchical basis functions (see, for example the FE_Q_Hierarchical element) are defined so that the node functionals are [1.x.74] of finite element functions,  [2.x.292]  in 2d, and similarly for 3d, where the  [2.x.293]  are the order of the moment described by shape function  [2.x.294] . Some other elements use moments over edges or faces. In all of these cases, node functionals are not defined through interpolation at all, and these elements then have neither support points, nor generalized support points.  [2.x.295] 
* 

*  <dt class="glossary"> [2.x.296]  geometry_paper [1.x.75]</dt>  [2.x.297] The "geometry paper" is a paper by L. Heltai, W. Bangerth, M. Kronbichler, and A. Mola, titled "Using exact geometry information in finite element computations", that describes how deal.II describes the geometry of domains. In particular, it discusses the algorithmic foundations on which the Manifold class is based, and what kind of information it needs to provide for mesh refinement, the computation of normal vectors, and the many other places where geometry enters into finite element computations.
*  The paper is currently available on arXiv at https://arxiv.org/abs/1910.09824 . The full reference for this paper is as follows:

* 
* [1.x.76]
*   [2.x.298] 
* 

*  <dt class="glossary"> [2.x.299]  GlossGhostCell [1.x.77]</dt>  [2.x.300]  If a mesh is distributed across multiple MPI processes using the  [2.x.301]  class, each processor stores only the cells it owns, one layer of adjacent cells that are owned by other processors, all  [2.x.302]  "coarse level cells", and all cells that are necessary to maintain the invariant that adjacent cells must differ by at most one refinement level. The cells stored on each process that are not owned by this process but that are adjacent to the ones owned by this process are called "ghost cells", and for these cells the predicate  [2.x.303]  returns true. Ghost cells are guaranteed to exist in the globally distributed mesh, i.e. these cells are actually owned by another process and are not further refined there. See the  [2.x.304]  "Distributed Computing paper" for more information.
*  The layer of ghost cells consists of all cells that are face, edge, or vertex neighbors of any locally owned cell and that are not locally owned themselves. In other word, the ghost cells completely enclose the subdomain of locally owned cells (with the exception of the boundary of the domain, of course).
*  The concept of ghost cells has no meaning for triangulations that store the entire mesh on each processor, i.e. the Triangulation and the  [2.x.305]  classes.  [2.x.306] 
* 

*  <dt class="glossary"> [2.x.307]  GlossGhostedVector [1.x.78]</dt>  [2.x.308]  In parallel computations, vectors come in two general kinds: without and with ghost elements. Vectors without ghost elements uniquely partition the vector elements between processors: each vector entry has exactly one processor that owns it, and this is the only processor that stores the value of this entry. In other words, if processor zero stores elements 0...49 of a vector and processor one stores elements 50...99, then processor one is out of luck accessing element 42 of this vector: it is not stored here and the value can not be assessed. This will result in an assertion.
*  On the other hand, there are many situations where one needs to know vector elements that aren't locally owned, for example to evaluate the solution on a locally owned cell (see  [2.x.309] ) for which one of the degrees of freedom is at an interface to a cell that we do not own locally (which, in this case must then be a  [2.x.310]  "ghost cell") and for which the neighboring cell may be the owner
* 
*  -  in other words, the degree of freedom is not a  [2.x.311]  "locally owned" but instead only a  [2.x.312]  "locally active DoFs". The values of such degrees of freedom are typically stored on the machine that owns the degree of freedom and, consequently, would not be accessible on the current machine.
*  Because one often needs these values anyway, there is a second kind of vector, often called "ghosted vector". Ghosted vectors store some elements on each processor for which that processor is not the owner. For such vectors, you can read those elements that the processor you are currently on stores but you cannot write into them because to make this work would require propagating the new value to all other processors that have a copy of this value (the list of such processors may be something which the current processor does not know and has no way of finding out efficiently). Since you cannot write into ghosted vectors, the only way to initialize such a vector is by assignment from a non-ghosted vector. This implies having to import those elements we locally want to store from other processors.
*  The way ghosted vectors are actually stored is different between the various implementations of parallel vectors. For PETSc (and the corresponding  [2.x.313]  class), ghosted vectors store the same elements as non-ghosted ones would, plus some additional elements that are owned by other processors. In other words, for each element there is a clear owner among all of the processors and those elements that the current processor stores but does not own (i.e., the "ghost elements") are simply mirror images of a primary value somewhere else
* 
*  -  thus, the name "ghost". This is also the case for the  [2.x.314]  class.
*  On the other hand, in Trilinos (and consequently in  [2.x.315]  a ghosted vector is simply a view of the parallel vector where the element distributions overlap. The 'ghosted' Trilinos vector in itself has no idea of which entries are ghosted and which are locally owned. In fact, a ghosted vector may not even store all of the elements a non-ghosted vector would store on the current processor. Consequently, for Trilinos vectors, there is no notion of an 'owner' of vector elements in the way we have it in the the non-ghost case view (or in the PETSc case) and the name "ghost element" may be misleading since in this view, every element we have available locally may or may not be stored somewhere else as well, but even if it is, the local element is not a mirror value of a primary location as there is no owner of each element.
* 

* 
*  [2.x.316]  The  [2.x.317]  documentation module provides a brief overview of where the different kinds of vectors are typically used.  [2.x.318] 
* 

*  <dt class="glossary"> [2.x.319]  hp_paper [1.x.79]</dt>  [2.x.320] The "hp-paper" is a paper by W. Bangerth and O. Kayser-Herold, titled "Data Structures and Requirements for hp Finite Element Software", that describes many of the algorithms and data structures used in the implementation of the hp-framework of deal.II. In particular, it summarizes many of the tricky points that have to be considered for %hp-finite elements using continuous elements.
*  The full reference for this paper is as follows:

* 
* [1.x.80]
*  It is available from [1.x.81], also see [1.x.82] for details.
*  The numerical examples shown in that paper are generated with a slightly modified version of  [2.x.321] . The main difference to that tutorial program is that various operations in the program were timed for the paper to compare different options and show that  [2.x.322]  methods are really not all that expensive.  [2.x.323] 
* 

*  <dt class="glossary"> [2.x.324]  GlossInterpolation [1.x.83]</dt>  [2.x.325] The purpose of interpolation with finite elements is computing a vector of coefficients representing a finite element function, such that the  [2.x.326]  "node values" of the original function and the finite element function coincide. Therefore, the interpolation process consists of evaluating all  [2.x.327]  "node functionals" [1.x.84] for the given function [1.x.85] and store the result as entry [1.x.86] in the coefficient vector.  [2.x.328] 
* 

*  <dt class="glossary"> [2.x.329]  GlossLagrange [1.x.87]</dt>  [2.x.330] Finite elements based on Lagrangian interpolation at  [2.x.331]  "support points".  [2.x.332] 
* 

*  <dt class="glossary"> [2.x.333]  GlossLocallyOwnedCell [1.x.88]</dt>  [2.x.334] This concept identifies a subset of all cells when using distributed meshes, see the  [2.x.335]  module. In such meshes, each cell is owned by exactly one processor. The locally owned ones are those owned by the current processor.
*  Each processor in a parallel computation has a triangulation covering the entire domain that consists of cells that are locally owned, of  [2.x.336]  "ghost cells" and of  [2.x.337]  "artificial cells".  [2.x.338] 
* 

*  <dt class="glossary"> [2.x.339]  GlossLocallyOwnedDof [1.x.89]</dt>  [2.x.340] This concept identifies a subset of all degrees of freedom when using distributed meshes, see the  [2.x.341]  module.  Locally owned degrees of freedom live on locally owned cells. Since degrees of freedom are owned by only one processor, degrees of freedom on interfaces between cells owned by different processors may be owned by one or the other, so not all degrees of freedom on a locally owned cell are also locally owned degrees of freedom.
*  Locally owned DoFs are a subset of the  [2.x.342]  "locally active DoFs".  [2.x.343] 
* 

*  <dt class="glossary"> [2.x.344]  GlossLocallyActiveDof [1.x.90]</dt>  [2.x.345] This concept identifies a subset of all degrees of freedom when using distributed meshes, see the  [2.x.346]  module.  Locally active degrees of freedom are those that live on locally owned cells. Degrees of freedom on interfaces between cells owned by different processors therefore belong to the set of locally active degrees of freedom for more than one processor.
*  Locally active DoFs are a superset of the  [2.x.347]  "locally owned DoFs" and a subset of the  [2.x.348]  "locally relevant DoFs".  [2.x.349] 
* 

*  <dt class="glossary"> [2.x.350]  GlossLocallyRelevantDof [1.x.91]</dt>  [2.x.351] This concept identifies a subset of all degrees of freedom when using distributed meshes, see the  [2.x.352]  module.  Locally relevant degrees of freedom are those that live on locally owned or ghost cells. Consequently, they may be owned by different processors.
*  Locally relevant DoFs are a superset of the  [2.x.353]  "locally active DoFs."  [2.x.354] 
* 

*  <dt class="glossary"> [2.x.355]  GlossManifoldIndicator [1.x.92]</dt>
*   [2.x.356]  Every object that makes up a Triangulation (cells, faces, edges, etc.), is associated with a unique number (of type  [2.x.357]  that is used to identify which manifold object is responsible to generate new points when the mesh is refined.
*  By default, all manifold indicators of a mesh are set to  [2.x.358]  A typical piece of code that sets the manifold indicator on a object to something else would look like this, here setting the manifold indicator to 42 for all cells whose center has an  [2.x.359]  component less than zero:
* 

* 
* [1.x.93]
* 
*  Here we call the function  [2.x.360]  It may also be appropriate to call  [2.x.361]  instead, to set recursively the manifold id on each face (and edge, if in 3d). To query the manifold indicator of a particular object edge, use  [2.x.362] 
*  The code above only sets the manifold indicators of a particular part of the Triangulation, but it does not by itself change the way the Triangulation class treats this object for the purposes of mesh refinement. For this, you need to call  [2.x.363]  to associate a manifold object with a particular manifold indicator. This allows the Triangulation objects to use a different method of finding new points on cells, faces or edges to be refined; the default is to use a FlatManifold object for all faces and edges.
* 

* 
*  [2.x.364]  Manifold indicators are inherited from parents to their children upon mesh refinement. Some more information about manifold indicators is also presented in a section of the documentation of the Triangulation class as well as in the  [2.x.365]  "Manifold documentation module". Manifold indicators are used in  [2.x.366]  and  [2.x.367] .  [2.x.368] 
*   [2.x.369]   [2.x.370]  "The module on Manifolds"
* 

*  <dt class="glossary"> [2.x.371]  GlossMaterialId [1.x.94]</dt>  [2.x.372] Each cell of a triangulation has associated with it a property called "material id". It is commonly used in problems with heterogeneous coefficients to identify which part of the domain a cell is in and, consequently, which value the coefficient should have on this particular cell. In practice, the material id of a cell is typically used to identify which cells belong to a particular part of the domain, e.g., when you have different materials (steel, concrete, wood) that are all part of the same domain. One would then usually query the material id associated with a cell during assembly of the bilinear form, and use it to determine (e.g., by table lookup, or a sequence of if-else statements) what the correct material coefficients would be for that cell.
*  This material_id may be set upon construction of a triangulation (through the CellData data structure), or later through use of cell iterators. For a typical use of this functionality, see the  [2.x.373]  tutorial program. The functions of the GridGenerator namespace typically set the material ID of all cells to zero. When reading a triangulation through the GridIn class, different input file formats have different conventions, but typically either explicitly specify the material id, or if they don't, then GridIn simply sets them to zero. Because the material of a cell is intended to pertain to a particular region of the domain, material ids are inherited by child cells from their parent upon mesh refinement.
*  The material id is set and queried using the  [2.x.374]   [2.x.375]  and  [2.x.376]  functions.  [2.x.377] 
* 

*  <dt class="glossary"> [2.x.378]  GlossMPICommunicator [1.x.95]</dt>  [2.x.379]  In the language of the Message Passing Interface (MPI), a communicator can be thought of as a mail system that allows sending messages to other members of the mail system. Within each communicator, each  [2.x.380]  "process" has a  [2.x.381]  "rank" (the equivalent of a house number) that allows to identify senders and receivers of messages. It is not possible to send messages via a communicator to receivers that are not part of this communicator/mail service.
*  When starting a parallel program via a command line call such as

* 
* [1.x.96]
*  (or the equivalent used in the batch submission system used on your cluster) the MPI system starts 32 copies of the  [2.x.382]  executable. Each of these has access to the  [2.x.383]  communicator that then consists of all 32 processors, each with its own rank. A subset of processes within this MPI universe can later agree to create other communicators that allow communication between only a subset of processes.  [2.x.384] 
* 

*  <dt class="glossary"> [2.x.385]  GlossMPIProcess [1.x.97]</dt>  [2.x.386]  When running parallel jobs on distributed memory machines, one almost always uses MPI. There, a command line call such as

* 
* [1.x.98]
*  (or the equivalent used in the batch submission system used on your cluster) starts 32 copies of the  [2.x.387]  executable. Some of these may actually run on the same machine, but in general they will be running on different machines that do not have direct access to each other's memory space.
*  In the language of the Message Passing Interface (MPI), each of these copies of the same executable running on (possibly different) machines are called [1.x.99]. The collection of all processes running in parallel is called the "MPI Universe" and is identified by the  [2.x.388]  "MPI communicator"  [2.x.389] .
*  Each process has immediate access only to the objects in its own memory space. A process can not read from or write into the memory of other processes. As a consequence, the only way by which processes can communicate is by sending each other messages. That said (and as explained in the introduction to  [2.x.390] ), one typically calls higher level MPI functions in which all processes that are part of a communicator participate. An example would be computing the sum over a set of integers where each process provides one term of the sum.  [2.x.391] 
* 

*  <dt class="glossary"> [2.x.392]  GlossMPIRank [1.x.100]</dt>  [2.x.393]  In the language of the Message Passing Interface (MPI), the [1.x.101] of an  [2.x.394]  "MPI process" is the number this process carries within the set  [2.x.395]  of all processes currently running as one parallel job. More correctly, it is the number within an  [2.x.396]  "MPI communicator" that groups together a subset of all processes with one parallel job (where  [2.x.397]  simply denotes the [1.x.102] set of processes).
*  Within each communicator, each process has a unique rank, distinct from the all other processes' ranks, that allows identifying one recipient or sender in MPI communication calls. Each process, running on one processor, can inquire about its own rank within a communicator by calling  [2.x.398]  The total number of processes participating in a communicator (i.e., the [1.x.103] of the communicator) can be obtained by calling  [2.x.399]   [2.x.400] 
* 

*  <dt class="glossary"> [2.x.401]  mg_paper [1.x.104]</dt>  [2.x.402] The "multigrid paper" is a paper by B. Janssen and G. Kanschat, titled "Adaptive Multilevel Methods with Local Smoothing for H1- and Hcurl-Conforming High Order Finite Element Methods", that describes many of the algorithms and data structures used in the implementation of the multigrid framework of deal.II. It underlies the implementation of the classes that are used in  [2.x.403]  for multigrid methods.
*  The full reference for this paper is as follows:

* 
* [1.x.105]
*  See [1.x.106] for the paper and [1.x.107] for more details.  [2.x.404] 
* 

*  <dt class="glossary"> [2.x.405]  GlossNodes [1.x.108]</dt>
*   [2.x.406] It is customary to define a finite element as a triple  [2.x.407]  where
* 

* 
* 
*  -  [2.x.408]  is the cell, where in deal.II this is always a line segment,   quadrilateral, or hexahedron;
* 

* 
* 
*  -  [2.x.409]  is a finite-dimensional space, e.g., a polynomial space mapped   from the  [2.x.410]  "reference cell" to  [2.x.411] ;
* 

* 
* 
*  -  [2.x.412]  is a set of "node functionals", i.e., functionals    [2.x.413] . The dimension of  [2.x.414]  must be equal to the number of node functionals. With this definition, we can define a basis of the local function space, i.e., a set of "shape functions"  [2.x.415] , by requiring that  [2.x.416] , where  [2.x.417]  is the Kronecker delta.
*  This definition of what a finite element is has several advantages, concerning analysis as well as implementation. For the analysis, it means that conformity with certain spaces  [2.x.418]  e.g. continuity, is up to the node functionals. In deal.II, it helps simplifying the implementation of more complex elements like FE_RaviartThomas considerably.
*  Examples for node functionals are values in  [2.x.419]  "support points" and moments with respect to Legendre polynomials. Examples:
*   [2.x.420] 
*  The construction of finite elements as outlined above allows writing code that describes a finite element simply by providing a polynomial space (without having to give it any particular basis
* 
*  -  whatever is convenient is entirely sufficient) and the nodal functionals. This is used, for example in the  [2.x.421]  function.  [2.x.422] 
* 

*  <dt class="glossary"> [2.x.423]  GlossParallelScaling [1.x.113]</dt>  [2.x.424] When we say that a parallel program "scales", what we mean is that the program does not become unduly slow (or takes unduly much memory) if we make the problem it solves larger, and that run time and memory consumption decrease proportionally if we keep the problem size the same but increase the number of processors (or cores) that work on it.
*  More specifically, think of a problem whose size is given by a number  [2.x.425]  (which could be the number of cells, the number of unknowns, or some other indicative quantity such as the number of CPU cycles necessary to solve it) and for which you have  [2.x.426]  processors available for solution. In an ideal world, the program would then require a run time of  [2.x.427] , and this would imply that we could reduce the run time to any desired value by just providing more processors. Likewise, for a program to be scalable, its overall memory consumption needs to be  [2.x.428]  and on each involved process needs to be  [2.x.429] , again implying that we can fit any problem into the fixed amount of memory computers attach to each processor, by just providing sufficiently many processors.
*  For practical assessments of scalability, we often distinguish between "strong" and "weak" scalability. These assess asymptotic statements such as  [2.x.430]  run time in the limits  [2.x.431]  and/or  [2.x.432] . Specifically, when we say that a program is "strongly scalable", we mean that if we have a problem of fixed size  [2.x.433] , then we can reduce the run time and memory consumption (on every processor) inversely proportional to  [2.x.434]  by just throwing more processors at the problem. In particular, strong scalability implies that if we provide twice as many processors, then run time and memory consumption on every process will be reduced by a factor of two. In other words, we can solve the [1.x.114] faster and faster by providing more and more processors.
*  Conversely, "weak scalability" means that if we increase the problem size  [2.x.435]  by a fixed factor, and increase the number of processors  [2.x.436]  available to solve the problem by the same factor, then the overall run time (and the memory consumption on every processor) remains the same. In other words, we can solve [1.x.115] within the same amount of wallclock time by providing more and more processors.
*  No program is truly scalable in this theoretical sense. Rather, all programs cease to scale once either  [2.x.437]  or  [2.x.438]  grows larger than certain limits. We therefore often say things such as "the program scales up to 4,000 cores", or "the program scales up to 100,000,000 unknowns". There are a number of reasons why programs cannot scale without limit; these can all be illustrated by just looking at the (relatively simple)  [2.x.439]  tutorial program:
* 

* 
* 
*  - Sequential sections: Many programs have sections of code that   either cannot or are not parallelized, i.e., where one processor has to do   a certain, fixed amount of work that does not decrease just because   there are a total of  [2.x.440]  processors around. In  [2.x.441] , this is   the case when generating graphical output: one processor creates   the graphical output for the entire problem, i.e., it needs to do    [2.x.442]  work. That means that this function has a run time   of  [2.x.443] , regardless of  [2.x.444] , and consequently the overall   program will not be able to achieve  [2.x.445]  run time but   have a run time that can be described as  [2.x.446]  where   the first term comes from scalable operations such as assembling   the linear system, and the latter from generating graphical   output on process 0. If  [2.x.447]  is sufficiently small, then the   program might look like it scales strongly for small numbers of   processors, but eventually strong scalability will cease. In   addition, the program can not scale weakly either because   increasing the size  [2.x.448]  of the problem while increasing the   number of processors  [2.x.449]  at the same rate does not keep the   run time of this one function constant.
* 

* 
* 
*  - Duplicated data structures: In  [2.x.450] , each processor stores the entire   mesh. That is, each processor has to store a data structure of size    [2.x.451] , regardless of  [2.x.452] . Eventually, if we make the problem   size large enough, this will overflow each processor's memory space   even if we increase the number of processors. It is thus clear that such   a replicated data structure prevents a program from scaling weakly.   But it also prevents it from scaling strongly because in order to   create an object of size  [2.x.453] , one has to at the very   least write into  [2.x.454]  memory locations, costing    [2.x.455]  in CPU time. Consequently, there is a component of the   overall algorithm that does not behave as  [2.x.456]  if we   provide more and more processors.
* 

* 
* 
*  - Communication: If, to pick just one example, you want to compute   the  [2.x.457]  norm of a vector of which all MPI processes store a few   entries, then every process needs to compute the sum of squares of   its own entries (which will require  [2.x.458]  time, and   consequently scale perfectly), but then every process needs to   send their partial sum to one process that adds them all up and takes   the square root. In the very best case, sending a message that   contains a single number takes a constant amount of time,   regardless of the overall number of processes. Thus, again, every   program that does communication cannot scale strongly because   there are parts of the program whose CPU time requirements do   not decrease with the number of processors  [2.x.459]  you allocate for   a fixed size  [2.x.460] . In reality, the situation is actually even   worse: the more processes are participating in a communication   step, the longer it will generally take, for example because   the one process that has to add everyone's contributions has   to add everything up, requiring  [2.x.461]  time. In other words,   CPU time [1.x.116] with the number of processes, therefore   not only preventing a program from scaling strongly, but also from   scaling weakly. (In reality, MPI libraries do not implement  [2.x.462]    norms by sending every message to one process that then adds everything   up; rather, they do pairwise reductions on a tree that doesn't   grow the run time as  [2.x.463]  but as  [2.x.464] ,   at the expense of more messages sent around. Be that as it may,   the fundamental point is that as you add more processors, the   run time will grow with  [2.x.465]  regardless of the way the operation   is actually implemented, and it can therefore not scale.)
*  These, and other reasons that prevent programs from scaling perfectly can be summarized in [1.x.117][1.x.118] that states that if a fraction  [2.x.466]  of a program's overall work  [2.x.467]  can be parallelized, i.e., it can be run in  [2.x.468]  time, and a fraction  [2.x.469]  of the program's work can not be parallelized (i.e., it consists either of work that only one process can do, such as generating graphical output in  [2.x.470] ; or that every process has to execute in a replicated way, such as sending a message with a local contribution to a dedicated process for accumulation), then the overall run time of the program will be

* 
* [1.x.119]
*  Consequently, the "speedup" you get, i.e., the factor by which your programs run faster on  [2.x.471]  processors compared to running the program on a single process (assuming this is possible) would be

* 
* [1.x.120]
*  If  [2.x.472] , which it is for all practically existing programs, then  [2.x.473]  as  [2.x.474] , implying that there is a point where it does not pay off in any significant way any more to throw more processors at the problem.
*  In practice, what matters is [1.x.121] or [1.x.122] or [1.x.123] a program scales. For deal.II, experience shows that on most clusters with a reasonable fast network, one can solve problems up to a few billion unknowns, up to a few thousand processors, and down to somewhere between 40,000 and 100,000 unknowns per process. The last number is the most relevant: if you have a problem with, say,  [2.x.475]  unknowns, then it makes sense to solve it on 1000-2500 processors since the number of degrees of freedom each process handles remains at more than 40,000. Consequently, there is enough work every process has to do so that the  [2.x.476]  time for communication does not dominate. But it doesn't make sense to solve such a problem with 10,000 or 100,000 processors, since each of these processor's local problem becomes so small that they spend most of their time waiting for communication, rather than doing work on their part of the work.  [2.x.477] 
*  <dt class="glossary"> [2.x.478]  GlossPeriodicConstraints [1.x.124]</dt>  [2.x.479] Periodic boundary condition are often used when only part of the physical relevant domain is modeled. One assumes that the solution simply continues periodically with respect to the boundaries that are considered periodic. In deal.II, support for this is through  [2.x.480]  and  [2.x.481]  As soon as a  [2.x.482]  is used also  [2.x.483]  has to be called to make sure that all the processes know about relevant parts of the triangulation on both sides of the periodic boundary. A typical process for distributed triangulations would be:
* 

* 
* 
*  - Create a mesh
* 

* 
* 
*  - Gather the periodic faces using  [2.x.484]  (Triangulation)
* 

* 
* 
*  - Add the periodicity information to the mesh using  [2.x.485] 
* 

* 
* 
*  - Gather the periodic faces using  [2.x.486]  (DoFHandler)
* 

* 
* 
*  - Add periodicity constraints using  [2.x.487] 
*  An example for this can be found in  [2.x.488] .  [2.x.489] 
* 

*  <dt class="glossary"> [2.x.490]  GlossPrimitive [1.x.125]</dt>  [2.x.491] A finite element (described by its shape functions) is primitive if there is a unique relation from shape function number to vector  [2.x.492]  GlossComponent "component". What this means is that each shape function of a vector-valued element has exactly one nonzero component if an element is primitive. This includes, in particular, all scalar elements as well as vector-valued elements assembled via the FESystem class from other primitive (for example scalar) elements as shown in  [2.x.493] ,  [2.x.494] ,  [2.x.495]  and several others. On the other hand, the FE_RaviartThomas class used in  [2.x.496]  and  [2.x.497] , or the FE_Nedelec class provide non-primitive finite elements because there, each vector-value shape function may have several non-zero components.  [2.x.498] 
* 

*  <dt class="glossary"> [2.x.499]  GlossReferenceCell [1.x.126]</dt>  [2.x.500] The hypercube [0,1]<sup>dim</sup>, on which all parametric finite element shape functions are defined. Many properties of the reference cell are described by the GeometryInfo class.  [2.x.501] 
* 

*  <dt class="glossary"> [2.x.502]  GlossSerialization [1.x.127]</dt>
*   [2.x.503] The term "serialization" refers to the process of writing the state of an object to a stream and later retrieve it again. A typical use case is to save the state of a program to disk for possible later resurrection, often in the context of checkpoint/restart strategies for long running computations or on computers that aren't very reliable (e.g. on very large clusters where individual nodes occasionally fail and then bring down an entire MPI job). In either case, one wants to occasionally save the state of the program so that, upon failure, one can restart it at that point rather than having to run it again from the beginning.
*  deal.II implements serialization facilities by implementing the necessary interfaces for the [1.x.128] library. See there for examples on how to save and restore objects.  [2.x.504] 
* 

*  <dt class="glossary"> [2.x.505]  GlossShape [1.x.129]</dt>  [2.x.506] The restriction of the finite element basis functions to a single grid cell.  [2.x.507] 
* 

*  <dt class="glossary"> [2.x.508]  GlossSubdomainId [1.x.130]</dt>  [2.x.509] Each cell of a triangulation has associated with it a property called the "subdomain id" that can be queried using a call like  [2.x.510]  and that can be set for example by using  [2.x.511] . (These calls resolve to  [2.x.512]  and  [2.x.513]  respectively.) While in principle this property can be used in any way application programs deem useful (it is simply an integer associated with each cell that can indicate whatever you want), at least for programs that run in %parallel it usually denotes the  [2.x.514]  "MPI rank" of the processor that "owns" this cell.
*  For programs that are parallelized based on MPI but where each processor stores the entire triangulation (as in, for example,  [2.x.515]  and  [2.x.516] , but not in  [2.x.517] ), subdomain ids are assigned to cells by partitioning a mesh, and each MPI process then only works on those cells it "owns", i.e., that belong to a subdomain the processor owns (traditionally, this is the case for the subdomain id whose numerical value coincides with the rank of the MPI process within the MPI communicator). Partitioning is typically done using the  [2.x.518]  function, but any other method can also be used to do this. (Alternatively, the  [2.x.519]  class can partition the mesh automatically using a similar approach.)
*  On the other hand, for programs that are parallelized using MPI but where meshes are held distributed across several processors using the  [2.x.520]  class, the subdomain id of cells is tied to the processor that owns the cell. In other words, querying the subdomain id of a cell tells you if the cell is owned by the current processor (i.e. if <code>cell- [2.x.521]  ==  [2.x.522]  or by another processor. In the %parallel distributed case, subdomain ids are only assigned to cells that the current processor owns as well as the immediately adjacent  [2.x.523]  "ghost cells". Cells further away are held on each processor to ensure that every MPI process has access to the full  [2.x.524]  "coarse grid" as well as to ensure the invariant that neighboring cells differ by at most one refinement level. These cells are called "artificial" (see  [2.x.525]  "here") and have the special subdomain id value  [2.x.526] 
*  In addition to regular subdomain ids, there is a second, closely related set of flags that are associated with each cell: "level subdomain ids." These exist not only for active cells but, in fact, for every cell in a mesh hierarchy. Their meaning is entirely analogous to the regular subdomain ids, but they are read and written by the  [2.x.527]  and  [2.x.528]  functions.  [2.x.529] 
* 

*  <dt class="glossary"> [2.x.530]  GlossSupport [1.x.131]</dt>  [2.x.531] Support points are by definition those points  [2.x.532] , such that for the shape functions  [2.x.533]  holds  [2.x.534] . Therefore, a finite element interpolation can be defined uniquely by the values in the support points.
*  Lagrangian elements fill the vector accessed by  [2.x.535]  such that the function  [2.x.536]  returns <tt>true</tt>. Naturally, these support points are on the  [2.x.537]  "reference cell".  Then, FEValues can be used (in conjunction with a Mapping) to access support points on the actual grid cells.
* 

* 
*  [2.x.538]  The concept of  [2.x.539]  "support points" is restricted to the finite element families based on Lagrange interpolation. For a more general concept, see  [2.x.540]  "generalized support points".  [2.x.541] 
* 

*  <dt class="glossary"> [2.x.542]  GlossTargetComponent [1.x.132]</dt>  [2.x.543]  When vectors and matrices are grouped into blocks by component, it is often desirable to collect several of the original components into a single one. This could be for instance, grouping the velocities of a Stokes system into a single block.  [2.x.544] 
* 

*  <dt class="glossary"> [2.x.545]  GlossUnitCell [1.x.133]</dt>  [2.x.546] See  [2.x.547]  "Reference cell".  [2.x.548] 
* 

*  <dt class="glossary"> [2.x.549]  GlossUnitSupport [1.x.134]</dt>  [2.x.550] These are the  [2.x.551]  "support points" on the reference cell, defined in FiniteElement. For example, the usual Q1 element in 1d has support points  at <tt>x=0</tt> and <tt>x=1</tt> (and similarly, in higher dimensions at the vertices of the unit square or cube). On the other hand, higher order Lagrangian elements have unit support points also in the interior of the unit line, square, or cube.  [2.x.552] 
* 

*  <dt class="glossary"> [2.x.553]  GlossUserFlags [1.x.135]</dt>  [2.x.554]    A triangulation offers one bit per line, quad, etc for user flags.   This field can be   accessed as all other data using iterators, using the syntax  
* [1.x.136]
*    Typically, this user flag is   used if an algorithm walks over all cells and needs information whether   another cell, e.g. a neighbor, has already been processed. Similarly,   it can be used to flag faces, quads or lines at the boundary for which   some operation has already been performed. The latter is often useful   since a loop such as  
* [1.x.137]
*    encounters some boundary lines more than once. Consequently, one would   set the user flag of the line in the body of the loop, and only enter the   body if the user flag had not previously been set. There are a number of   additional functions that can be accessed through the iterator interface;   see the TriaAccessor class for more information. Note that there are no   user flags that can be associated with vertices; however, since vertices   are numbered consecutively, this can easily be emulated in user code   using a vector of bools.
*    There are two functions,  [2.x.555]  and    [2.x.556]  which   write and read these flags to and from a stream or a vector of bools. Unlike    [2.x.557]  and  [2.x.558]    these two functions store   and read the flags of all used lines, quads, etc, i.e., not only of the   active ones.
*    If you want to store more specific user flags, you can use the functions    [2.x.559]  and  [2.x.560]    and the similarly for quads, etc.
*    As for the refinement and coarsening flags, there exist two versions of these   functions, one which reads/writes from a stream and one which does so from   a <tt>vector [2.x.561]  The latter is used to store flags temporarily, while the   first is used to store them in a file.
*    It is good practice to clear the user flags using the    [2.x.562]  function before usage, since it is   often necessary to use the flags in more than one function. If the flags may   be in use at the time a function that needs them is called, then this function   should save and restore the flags as described above.
*   
*  [2.x.563]  If more information than just a single boolean flag needs to be stored   with a cell, line, or face, then see about  [2.x.564]  "user data".  [2.x.565] 
* 

*  <dt class="glossary"> [2.x.566]  GlossUserData [1.x.138]</dt>  [2.x.567]    Just like the  [2.x.568]  "user flags", the Triangulation class offers a   field for each line, quad and hex in which to store more descriptive data than just   a single boolean flag. This is called "user data" and the data that can be stored   in it is either a single unsigned integer or a void pointer. Both are typically   used to index into a bigger array that contains more detailed data an application   wants to attach to a mesh entity.
*    User data is stored and retrieved in the following manner:  
* [1.x.139]
*    Similarly, there are functions  [2.x.569]  to set a pointer, and    [2.x.570]  and  [2.x.571]  to retrieve the index   and pointer. To clear all user indices or pointers, use  [2.x.572]    As with flags, there are functions that allow to save and restore user data,   either for all entities of the mesh hierarchy or for lines, quads or hexes   separately. There are a number of additional functions that can be accessed   through the iterator interface; see the TriaAccessor class for more information.
*   
*  [2.x.573]  User pointers and user indices are stored in the same   place. In order to avoid unwanted conversions, Triangulation   checks which one of them is in use and does not allow access to   the other one, until  [2.x.574]  has been called.
*   
*  [2.x.575]  The usual warning about the missing type safety of  [2.x.576]  pointers are   obviously in place here; responsibility for correctness of types etc   lies entirely with the user of the pointer.  [2.x.577] 
* 

*  <dt class="glossary"> [2.x.578]  workstream_paper [1.x.140]</dt>  [2.x.579] The "WorkStream paper" is a paper by B. Turcksin, M. Kronbichler and W. Bangerth   that discusses the design and implementation of WorkStream. WorkStream is, at its   core, a design pattern, i.e., something that is used over and over in finite element   codes and that can, consequently, be implemented generically. In particular, the   paper lays out the motivation for this pattern and then proposes different ways   of implementing it. It also compares the performance of different implementations.
*  The full reference for this paper is as follows:

* 
* [1.x.141]
*  It is available from [1.x.142], also see [1.x.143] for details.  [2.x.580] 
* 

*  <dt class="glossary"> [2.x.581]  GlossZOrder [1.x.144]</dt>  [2.x.582]   The "Z order" of cells describes an order in which cells are traversed.
*   By default, if you write a loop over all cells in deal.II, the cells  will be traversed in an order where coarser cells (i.e., cells that were  obtained from   [2.x.583]  "coarse mesh" cells with fewer refinement steps) come  before cells that are finer (i.e., cells that were obtained with more refinement  steps). Within each refinement level, cells are traversed in an order  that has something to do with the order in which they were created;  in essence, however, this order is best of thought of as "unspecified":  you will visit each cell on a given refinement level exactly once, in  some order, but you should not make any assumptions about this order.
*   Because the order in which cells are created factors into the order  of cells, it can happen that the order in which you traverse cells is  different for two identical meshes. For example, think of a 1d (coarse)  mesh with two cells: If you first refine the first of these cells and then  the other, then you will traverse the four cells on refinement level 1  in a different order than if you had first refined the second coarse  cell and then the first coarse cell.
*   This order is entirely practical for almost all applications because  in most cases, it does not actually matter in which order one traverses  cells. Furthermore, it allows using data structures that lead to  particularly low cache miss frequencies and are therefore efficient  for high performance computing applications.
*   On the other hand, there are cases where one would want to traverse  cells in a particular, specified and reproducible order that only  depends on the mesh itself, not its creation history or any other  seemingly arbitrary design decisions. The "Z order" is one way  to achieve this goal.
*   To explain the concept of the Z order, consider the following sequence  of meshes (with each cell numbered using the "level.index" notation,  where "level" is the number of refinements necessary to get from a   [2.x.584]  "coarse mesh" cell to a particular cell, and "index" the index of this  cell within a particular refinement level):
*    [2.x.585]    [2.x.586]    [2.x.587]    [2.x.588] 
*   Note how the cells on level 2 are ordered in the order in which they  were created. (Which is not always the case: if cells had been removed  in between, then newly created cells would have filled in the holes  so created.)
*   The "natural" order in which deal.II traverses cells would then be  0.0
* 
-> 1.0
* 
-> 1.1
* 
-> 1.2
* 
-> 1.3
* 
-> 2.0
* 
-> 2.1
* 
-> 2.2
* 
-> 2.3
* 
-> 2.4
* 
->  2.5
* 
-> 2.6
* 
-> 2.7. (If you want to traverse only over the   [2.x.589]  "active cells", then omit all cells from this  list that have children.)  This can be thought of as the "lexicographic"  order on the pairs of numbers "level.index", but because the index  within each level is not well defined, this is not a particularly useful  notion. Alternatively, one can also think of it as one possible breadth-first  traversal of the tree that corresponds to this mesh and that represents  the parent-child relationship between cells:
*    [2.x.590] 
*   On the other hand, the Z order corresponds to a particular  depth-first traversal of the tree. Namely: start with a cell, and if it  has children then iterate over these cell's children; this rule is  recursively applied as long as a child has children.
*   For the given mesh above, this yields the following order: 0.0
* 
-> 1.0
* 
-> 2.4
* 

* 
* 

* 
* 
-> 2.5
* 
-> 2.6
* 
-> 2.7
* 
-> 1.1
* 
-> 1.2
* 
-> 1.3
* 
-> 1.4
* 
-> 2.0
* 
-> 2.1
* 
-> 2.2
* 
-> 2.3.  (Again, if you only care about active cells, then remove 0.0, 1.0, and 1.3  from this list.) Because the order of children of a cell is well defined  (as opposed to the order of cells within each level), this "hierarchical"  traversal makes sense and is, in particular, independent of the history  of a triangulation.
*   In practice, it is easily implemented using a recursive function: 
* [1.x.145]
*   This function is then called as follows: 
* [1.x.146]
* 
*   Finally, as an explanation of the term "Z" order: if you draw a line through  all cells in the order in which they appear in this hierarchical fashion,  then it will look like a left-right inverted Z on each refined cell. Indeed,  the curve so defined can be thought of a space-filling curve and is also  sometimes called "Morton ordering", see  https://en.wikipedia.org/wiki/Z-order_curve .  [2.x.591] 
* 

* 
*   [2.x.592] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/grid_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This module groups functions and classes that have to do with the topology and geometry of meshes. A mesh can be thought of as a collection of cells; if the mesh has been refined (possibly in an adaptive way), then this collection is grouped into a hierarchy of refinement levels. In addition to cells, the geometric objects that make up a triangulation are the faces of cells (and in 3d the edges of cells) as well as the vertices of the cells. Note that we abuse the word [1.x.0] somewhat, since deal.II only implements triangulations made up of linear, quadrilateral, and hexahedral cells; triangles and tetrahedra are not supported.
*  This collection of cells is managed by the Triangulation class and derived classes such as  [2.x.1]  and  [2.x.2]  It holds the relevant data in memory and offers interfaces to query it. Most things you want to do on cells are performed in loops over all cells. For this purpose, the Triangulation class offers the concept of iterators (see  [2.x.3] ): although implemented differently, they behave like pointers to cells or faces and can be queried for the geometric properties of cells as well as information like neighboring cells or faces of a cell.
*  It is worth noting that the Triangulation class only stores geometry (i.e. the location of vertices and cells) and topology of a mesh (i.e. which cells are neighbors of which other cells, etc). It has nothing to do with finite elements or degrees of freedom that might be defined on a mesh. These functions are performed by the DoFHandler class (see the  [2.x.4]  module) that gets a description of the finite element space and the allocates and manages degrees of freedom on vertices, faces, or cells, as described by the finite element class. This separation makes it possible to have multiple DoFHandler classes work on the same mesh at the same time.
*  In the grand scheme of things, triangulations in deal.II interact with a variety of other parts of the library:

* 
* [1.x.1]
*  [1.x.2]
*  There are three ways to create a mesh:  [2.x.5]   [2.x.6]  Creation by the GridGenerator class;  [2.x.7]  Reading from a file;  [2.x.8]  Creation by hand.  [2.x.9] 
*  For the first case, the GridGenerator class provides functions that can generate the simplest and most common geometries automatically. For example, a rectangular (or brick) geometry as well as circles, spheres, or cylinders can be generate with the functions in this class. Most of the tutorial programs use this mechanism.
*  Secondly, it is possible to read in meshes from an input file in a number of different formats using the GridIn class. Using this class, it is possible to read meshes with several 10 or 100 thousand cells, although this is not really recommended: the power of adaptive finite element methods only comes to bear if the initial mesh is as coarse as possible and there is room for a number of adaptive refinement steps. If the initial mesh is too fine already, then one runs out of memory or compute time before adaptive mesh refinement is able to do much good. Nevertheless, the GridIn class can be used in cases of complicated geometries or for comparison or interaction with other programs that compute on meshes that are then exchanged through this class The  [2.x.10]  tutorial program shows how to use the GridIn class.
*  The third way is to create a mesh by hand, by building a data structure that describes the vertices and cells of a triangulation. This is useful in cases of moderate complexity where a mesh can still be built by hand without resorting to a mesh generator, but where the domain is not one of those already supported by the GridIn class. In this method, the data structure so built is handed to the create_triangulation() function of the Triangulation class. The  [2.x.11]  tutorial program shows how this can be done.
* 

*  [1.x.3]
*  Meshes can be written to output files in a number of different formats. If this involves simulation results obtained on this mesh, then this is done using the DataOut class (described in more detail in the  [2.x.12]  module). On the other hand, if only the geometry and topology of the mesh is to be written to a file, the GridOut class can do this for you.
* 

*  [1.x.4]
*  The GridTool class offers an assortment of functions that act on grids. For example, this includes moving around nodes, stretching or rotating entire triangulations, computing the diameter of a domain, or subdividing it into chunks of roughly equal size for parallel computations.
*  The GridRefinement class implements a number of mesh refinement algorithms, based on refinement indicators given to its member functions.
* 

*  [1.x.5]
*  In addition to the above, there are a significant number of classes in this module that are only used in the internal data structures of mesh handling. They are generally in the internal namespace, and not meant for use in application code.
* 

*   [2.x.13]  Wolfgang Bangerth, 1998-2006

* 
* [0.x.1]

include/deal.II-translator/A-headers/hp_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  Classes and functions that have to do with hp-finite elements. The  [2.x.1]  tutorial program gives an overview of how to use the classes in this namespace. A slightly more exotic application is given in  [2.x.2] .
*  The hp-namespace implements the algorithms and data structures used for the hp-framework in deal.II. An overview over the details of how these algorithms work and what data structures are used is given in the  [2.x.3]  "hp-paper".

* 
* [0.x.1]*


* 
*  [2.x.4] 
*  In the implementation of the hp-finite element method, each cell might have a different finite element associated with it. To handle this, the  [2.x.5]  must have a whole set of finite element classes associated with it. This concept is represented by the  [2.x.6]  class: Objects of this type act as containers that hold a whole set of finite element objects. Instead of storing pointers to finite element objects on each cell, we then only store an index for each cell that identifies the finite element object within the collection that should be used by this cell. The DoFHandler object associated with the given cell can then assign degrees of freedom to each cell in accordance with the finite element used for it.
*  A similar situation arises when integrating terms on a cell: one may want to use different quadrature formulas for different finite elements. For example, on cells where we use a Q1 element, a QGauss(2) object (i.e. a quadrature formula with two points in each space direction) may be sufficient, but on another cell where a Q3 element is used, this would lead to underintegration and we should use a QGauss(4) formula instead. Just as above, there exists a class  [2.x.7]  that acts as a collection of quadrature formulas.
*  Finally, one may want to use different orders for the boundary approximation for cells with different orders for the finite element. The  [2.x.8]  class allows to do this.
*  All of these three classes, the  [2.x.9]   [2.x.10]  and  [2.x.11]  classes, implement an interface very similar to that of  [2.x.12] . They have functions  [2.x.13]  to add a finite element, quadrature formula, or mapping to the collection. They have an  [2.x.14]  function that allows to retrieve a reference to a given element of the collection. And they have a  [2.x.15]  function that returns the number of elements in the collection. Some of the classes, in particular that holding finite element objects, also implement other functions specific to their purpose.
*  The similarity goes beyond the interface: When adding an element to the collection, all of the classes create a copy of the argument. This allows to pass a temporary object to the function adding the element. For example, the following works:

* 
* [1.x.0]
* 
*  This way, one can add elements of polynomial degree 1 through 4 to the collection. It is not necessary to retain the added object: the collection makes a copy of it, it does not only store a pointer to the given finite element object. This same observation also holds for the other collection classes.
*  It is customary that within an hp-finite element program, one keeps collections of finite elements and quadrature formulas with the same number of elements, each element of the one collection matching the element in the other. This is not necessary, but it often makes coding a lot simpler. If a collection of mappings is used, the same holds for  [2.x.16]  objects as well.
*  Whenever p-adaptivity is considered in an hp-finite element program, a hierarchy of finite elements needs to be established to determine succeeding finite elements for refinement and preceding ones for coarsening. Typically, this hierarchy considers how finite element spaces are nested: for example, a  [2.x.17]  element describes a sub-space of a  [2.x.18]  element, and so doing  [2.x.19]  refinement usually means using a larger (more accurate) finite element space. In other words, the hierarchy of finite elements is built by considering whether some elements of the collection are sub- or super-spaces of others.
*  By default, we assume that finite elements are stored in an ascending order based on their polynomial degree. If the order of elements differs, a corresponding hierarchy needs to be supplied to the collection via the  [2.x.20]  member function.
* 

* 
*  [2.x.21] 

* 
* [0.x.2]*
 A namespace for the implementation of hp-finite element specific algorithms and data structures.
* 

* 
*  [2.x.22] 

* 
* [0.x.3]

include/deal.II-translator/A-headers/instantiations_0.txt
[0.x.0]*
  [2.x.0] 
*  Instantiation of complex class and function templates is expensive both in terms of compile time and disk space. Therefore, we try to separate declaration and implementation of templates as far as possible, and make sure that implementations are read by the compiler only when necessary.
*  Template classes in <tt>deal.II</tt> can be grouped into three categories, depending on the number of probable different instantiations. These three groups are discussed in the following.
* 

* 

* 
*  [2.x.1]  Inst1 Known and fixed number of instantiations
*  These are the classes having template parameters with very predictable values. The typical prototype is

* 
* [1.x.0]
* 
*  Here, we have a small number of instantiations ( [2.x.2] ) known at the time of design of the library. Therefore, member functions of this class are defined in a <tt>.cc</tt> file in the source directory and we instantiate the template for these known values explicitly in the source file.
*  From an application viewpoint, all you actually get to see then is the declaration of the template. Actual instantiations of member functions happens inside the library and is done when you compile the library, not when you compile your application code.
*  For these classes, adding instantiations for new parameters involves changing the library. However, this is rarely needed, of course, unless you are not content with computing only in 1d, 2d, or 3d.
* 

* 

* 
*  [2.x.3]  Inst1a Available instances
*  If the template parameter is <tt>dim</tt>, the available instances are for <tt>dim=1,2,3</tt>, if there is no other information.
*  There are other cases of classes (not depending on the spatial dimension) for which only a certain, small number of template arguments is supported and explicit instantiations are provided in the library. In particular, this includes all the linear algebra classes that are templatized on the type of the scalar underlying stored values: we only support  [2.x.4] , and in some cases  [2.x.5]  and  [2.x.6] .
* 

* 

* 
*  [2.x.7]  Inst2 A few instantiations, most of which are known
*  These are class templates usually having a small number of instantiations, but additional instantiations may be necessary. Therefore, a set of instantiations for the most likely parameters is provided precompiled in the libraries, but the implementation of the templates are provided in a special header file so that it is accessible in case someone wants to instantiate it for an unforeseen argument.
*  Typical examples for this would be some of the linear algebra classes that take a vector type as template argument. They would be instantiated within the library for  [2.x.8] ,  [2.x.9] , and  [2.x.10] , for example. However, they may also be used with other vector types, such as   [2.x.11]  and  [2.x.12] , as long as they satisfy certain interfaces, including vector types that are not part of the library but possibly defined in an application program. In such a case, applications can instantiate these templates by hand as described in the next section.
* 

* 

* 
*  [2.x.13]  Inst2c Creating new instances
*  Choose one of your source files to provide the required instantiations. Say that you want the class template <tt>XXXX</tt>, defined in the header file <tt>xxxx.h</tt>, instantiated with the template parameter <tt>long double</tt>. Then, your file should contain the lines

* 
* [1.x.1]
* 
* 

* 

* 
*  [2.x.14]  Inst2p Provided instances
*  Like with the classes in section  [2.x.15] , the instances provided in the library are often listed in the documentation of that class in a form similar to this:

* 
* [1.x.2]
* 
* 

* 

* 
*  [2.x.16]  Inst3 Many unknown instantiations
*  These are the classes, where no reasonable predetermined set of instances exists. Therefore, all member definitions are included in the header file and are instantiated wherever needed.  An example would be the SmartPointer class template that can be used with virtually any template argument.

* 
* [0.x.1]

include/deal.II-translator/A-headers/integrators_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  A collection of namespaces and functions which simplify the coding of forms and bilinear forms on finite element spaces. Functions for two different purposes are gathered here: the abstract integration on finite element meshes in MeshWorker and actual implementations of the integration of cell and face terms for concrete problems in LocalIntegrators.
* 

* 
*  [2.x.1]  Documentation on coding conventions, relations between classes, and details of the implementation is found in the documentation of namespaces in this module.
*  [1.x.0]
*  When we integrate a function or a functional on a finite element space, the structure of the integration loop is always the same. We have between 3 and 5 nested loops, from outside to inside:  [2.x.2]   [2.x.3]  Loop over all cells  [2.x.4]  Optionally, loop over all faces to compute fluxes  [2.x.5]  Loop over all quadrature points of the cell/face  [2.x.6]  Optionally, loop over all test functions to compute forms  [2.x.7]  Optionally, loop over all trial functions to compute bilinear forms  [2.x.8] 
*  These loops naturally fall into two classes, namely the computation of cell and face contributions (loops 3 to 5), and the outer loops over the mesh objects, often referred to as  [2.x.9] assembling [2.x.10] .
*  Support for the outer loop in deal.II can be found in the namespace MeshWorker (see the documentation there). In order to support the cell and face contributions (referred to as local contributions from now on), deal.II offers FEValuesBase and its derived classes. While the outer loop is generic (with exception of the data types), the computation of local contributions is problem dependent. Therefore, no generic algorithm is possible here. Nevertheless, we can define a generic interface for functions for this purpose and provide a library of local integrators for use in applications. These are collected in the namespace LocalIntegrators

* 
* [0.x.1]

include/deal.II-translator/A-headers/io_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This module collects the classes used for reading and writing meshes and data. There are two sub-modules for each of these operations.

* 
* [0.x.1]*


* 
*  [2.x.1] 
*  deal.II can read meshes in a number of different formats. However, all of them are constrained to so-called "coarse meshes", i.e. meshes that have no refinement hierarchy and in particular no hanging nodes. The GridIn class describes in detail what formats are supported.
*  In addition, deal.II can read an intermediate graphics format using the DataOutReader. This format is used as an intermediate step between data associated with a simulation and is written by the DataOutBase class (or through the more derived classes described in the \ref output module). The DataOutReader class reads this data back in, and it can then be converted to any of a number of data formats supported by visualization programs.
*  Finally, the ParameterHandler and MultipleParameterLoop classes (and the associated Patterns namespace) are used to deal with parameter files describing run-time parameters to a program that one doesn't want to hard-code within the program source.
* 

*  [1.x.0]
*  The PathSearch class is a helper class in input handling. It is used to find a file in a list of directories, in much the same way as unix systems find executables among the directories listed in the  [2.x.2]  environment variable.
* 

* 
*  [2.x.3] 

* 
* [0.x.2]*


* 
*  [2.x.4] 
*  deal.II generates three types of output: it can write triangulations/meshes in formats understood by several mesh readers (including those of deal.II itself), and it can create output used for visualization of data. Finally, it can output matrices in a graphical format.
* 

*  [1.x.1]
*  deal.II supports, through the DataOutBase class, a large number of popular visualization formats, such as those used by the OpenDX, gmv, or gnuplot programs. A complete list of supported formats is listed in the documentation of the DataOutBase class.
*  The DataOutBase class is only responsible for actually writing some intermediate format in a number of different visualization formats. This intermediate format is generated by classes derived, directly or indirectly, from DataOutBase. For example, the DataOut class is most often used to generate this intermediate format from a triangulation, a DoFHandler object (that associates a particular finite element class with the triangulation), and one or more data vectors. The DataOut class creates intermediate data from each cell, which is subsequently written by the DataOutBase class in some final format. Almost all example programs, starting with  [2.x.5] , make use of this method of generating output.
*  The DataOutFaces class is another way to create intermediate format from simulation data. However, instead of creating visualization data from each cell of the triangulation, it only creates information for all faces of cells that are located on the surface (though the class has a way to override the choice for which faces output should be generated). While this may not be particularly interesting in 2d (the faces would only be line segments) it is often helpful in 3d if what one really wants to know is the shape of the domain or the value of one variable on the surface. Using the DataOutFaces class then saves the effort of generating and storing data for all interior cells, which can be very expensive for large 3d simulations.
*  A third class, the DataOutRotation class, allows to take a two-dimensional simulation and generate three-dimensional data from it by rotating the two-dimensional domain around a given axis. This is mostly useful for the visualization of simulations that use the rotational symmetry of, for example, a cylinder.
*  Finally, the DataOutStack class allows to visualize data from time dependent simulations in the space-time domain: it collects the results from each time step and at the end outputs all of this information at once as a space-time file.
* 

*  [1.x.2]
*  Meshes, without any data vectors associated with it, can be written in a number of formats as well. This is done through the GridOut class, and the documentation of that class lists the supported formats.
*  Several of the tutorial programs, notably  [2.x.6] ,  [2.x.7] ,  [2.x.8] ,  [2.x.9] ,  [2.x.10] b, and  [2.x.11]  demonstrate the use of the GridOut class.
* 

*  [1.x.3]
*  Through the MatrixOut class, deal.II can also give a graphical visualization of matrices, in the form of color or skyline plots. The MatrixOut class uses the DataOutBase for output. Therefore, matrices can be visualized in all formats supported by the latter class.
* 

* 
*  [2.x.12] 

* 
* [0.x.3]*


* 
*  [2.x.13] 
*  In addition to classes that provide graphical output formats (see the  [2.x.14]  output module), deal.II has a number of classes that facilitate textual output in a number of ways. They are collected in this module. See the documentation of these classes for more details.
* 

* 
*  [2.x.15] 

* 
* [0.x.4]

include/deal.II-translator/A-headers/iterators_0.txt
[0.x.0]*


* 
*  [2.x.0]   [2.x.1] 
* deal.II has several classes which are understood conceptually asmeshes. Apart from the obvious Triangulation, these are, for example,DoFHandler and  [2.x.2]  All of those define a setof iterators, allowing the user to traverse the whole mesh, i.e. theset of cells, faces, edges, etc that comprise the mesh, or portions ofit. These iterators are all in a sense derived from the TriaIteratorclass.
* Basically, the template signature of TriaIterator is
* [1.x.0]
* 
* Conceptually, this type represents something like a pointer to an objectrepresented by the  [2.x.3]  class.  Usually, you will not use theactual class names spelled out directly, but employ one of the typedefsprovided by the mesh classes, such as <code>typename [2.x.4]  Before going into this, let usfirst discuss the concept of iterators, before delving into what the accessorsdo.
* As usual in C++, iterators, just as pointers, are incremented to the nextelement using <tt>operator ++</tt>, and decremented to the previous elementusing <tt>operator
* 
*  - </tt>. One can also jump <tt>n</tt> elements ahead usingthe addition operator, <tt>it=it+n</tt>, and correspondingly to move a numberof elements back. In addition, and keeping with the tradition of the standardtemplate library, meshes provide member functions <tt>begin()</tt> and<tt>end()</tt> that provide the first element of a collection and aone-past-the-end iterator, respectively. Since there are a number of differentiterators available, there is actually a whole family of such functions, suchas <tt>begin_active()</tt>, <tt>begin_face()</tt>, etc.
* In terms of the concepts for iterators defined in the C++ standard, thedeal.II mesh iterators are bi-directional iterators: they can be incrementedand decremented, but an operation like <tt>it=it+n</tt> takes a compute timeproportional to <tt>n</tt>, since it is implemented as a sequence of<tt>n</tt> individual unit increments. Note that this is in contrast to thenext more specialized iterator concept, random access iterators, for whichaccess to an arbitrary object requires only constant time, rather than linear.
* 

* 
*  [2.x.5]  IteratorsAndSets Iterators as pointers into sets of objects
* As mentioned above, iterators in deal.II can be considered as iterating overall the objects that constitute a mesh. (These objects are lines, quads, andhexes, and are represented by the type of Accessor class given as template argument to the iterator.) This suggests to view a triangulation as acollection of cells and other objects that are held together by a certain datastructure that links all these objects, in the same was as a linked list isthe data structure that connects objects in a linear fashion.
* Triangulations in deal.II can indeed be considered in this way. In particular,they use the computational notion of a forest of regular trees to store theirdata. This can be understood as follows: Consider the cells of the coarse meshas roots; then, if one of these coarse mesh cells is refined, it will have2<sup>dim</sup> children, which in turn can, but do not have to have2<sup>dim</sup> children of their own, and so on. This means, that each cellof the coarse mesh can be considered the root of a binary tree (in 1d), aquadtree (in 2d), or an octree (in 3d). The collection of these treesemanating from the cells of the coarse mesh then constitutes the forest thatcompletely describes the triangulation, including all of its active andinactive cells. In particular, the active cells are those terminal nodes inthe tree that have no descendants, i.e. cells which are not furtherrefined. Correspondingly, inactive cells correspond to nodes in the tree withdescendants, i.e. cells that are further refined.
* A triangulation contains forests for lines (each of which may have 2children), quads (each with possibly four children), and hexes (each with noor 8 children). Depending on the dimension, these objects are also termedcells or faces.
* Iterators loop over the elements of such forests. While the usual iteratorsloop over all nodes of a forest, active iterators iterate over theelements in the same order, but skip all non-active entries and therefore onlyvisit terminal nodes (i.e. active cells, faces, etc). There are many ways totraverse the elements of a forest, for example breadth first or depthfirst. Depending on the type of data structure used to store the forest, someways are more efficient than others. At present, the way iterators traverseforests in deal.II is breadth first. I.e., iterators first visit all theelements (cells, faces, etc) of the coarse mesh before moving on to all theelements of the immediate level, i.e. the immediate children of the coarsemesh objects; after this come the grandchildren of the coarse mesh, and so on.However, it must be noted that programs should not rely on this particularorder of traversing a tree: this is considered an implementation detail thatcan change between versions, even if we consider this an unlikely option atthe present time.
* 

* 
*  [2.x.6]  IteratorsDifferences Different kinds of iterators
* Iterators have two properties: what they point to (i.e. the type of theAccessor template argument), and the exact definition of the set they iterateover. In general, iterators are always declared as
* [1.x.1]
* 
* Here, <tt>Kind</tt> determines what property an accessor needs to have to bereached by this iterator (or omitted, for that matter). For example,
* [1.x.2]
* iterates over all objects of kind Accessor that make up the mesh (for exampleall cells, whether they are further refined and have children, or not), whereas
* [1.x.3]
* skips all objects that have children, i.e. objects that are not active.Active iterators therefore operate on a subset of the objectsthat normal iterators act on, namely those that possess the property thatthey are active. Note that this is independent of the kind of object weare operating on: all valid accessor classes have to provide the iteratorclasses a method to find out whether they are active or not.
* (For completeness, let us mention that there is a third kind of iterators: "rawiterators" also traverse objects that are unused in the triangulation, butallocated anyway for efficiency reasons. User code should never use rawiterators, they are only for %internal purposes of the library.)
* Whether an object is active can be considered a "predicate": a property thatis either true or false. Filtered iterators can be used to restrict the scopeof existing iterators even more. For instance, you could imagine to iterateover the subset of those  [2.x.7]  "active cells" having their userflag set or belonging to a certain subdomain (both properties are either trueor false for a given object).
* This is achieved by using an object of type FilteredIterator&lt;BaseIterator&gt;, where BaseIterator usually is one of thestandard iterators discussed above.
* The FilteredIterator gets an additional Predicate in its constructor and willskip all objects where this Predicate evaluates to <tt>false</tt>. Acollection of predicates already implemented can be found in the namespaceIteratorFilters.
* 

* 
*  [2.x.8]  IteratorsLoops Iterating over objects
* All iterators of the same kind and iterating over thesame kind of geometrical objects traverse the mesh in the sameorder. Take this code example:
* [1.x.4]
* 
* Here, all iterators will always point to the same mesh cell, even though<tt>DoFHandler</tt> and <tt>Triangulation</tt> are very different classes,and even if the DoFHandlers are handling different finite elements: theyall access cells in the same order, the difference is only in the Accessor.As mentioned above, the order in which iterators traverse the forest ofobjects is actually well-defined, but application programs should notassume any such order, but rather consider this an implementation detailof the library.
* Corresponding to above example, the order in which iterators traverse activeobjects is the same for all iterators in the following snippet, the difference to the previous example being that here we only consider active cells:
* [1.x.5]
* 
* 

* 
*  [2.x.9]  IteratorsAccessors Accessors
* Iterators are like pointers: they can be incremented and decremented, but theyare really rather dumb. Their magic only lies in the fact that they point tosome useful object, in this case the Accessor. For pointers, they point to anactual object that stores some data. On the other hand, the deal.II iterators,when dereferenced, do not return a reference to an actual object, but returnan object that knows how to get at the data that represents cells. In general, thisobject doesn't store itself where the vertices of a cell are or what its neighborsare. However, it knows how to tease this sort of information from out of thearrays and tables and lists that the Triangulation class sets up to describe amesh.
* Accessing data that characterizes a cell is always done through the Accessor,i.e. the expression  [2.x.10]  grants access to [1.x.6]attributes of this Accessor. Examples of properties you can query from aniterator are
* [1.x.7]
* 
* Since dereferencing iterators yields accessor objects, these calls are tomember functions  [2.x.11] , [2.x.12]  etc. These in turn figure out the relevant datafrom the various data structures that store this data. How this is actuallydone and what data structures are used is not really of concern to authors ofapplications in deal.II. In particular, by hiding the actual data structureswe are able to store data in an efficient way, not necessarily in a way thatmakes it easily accessible or understandable to application writers.
* 

* 
*  [2.x.13]  IteratorsTypedefs Kinds of accessors
* Depending on what sort of data you want to access, there are different kindsof accessor classes:
* 
*  - The TriaAccessor class provides you with data that identifies the geometric  properties of cells, faces, lines, quads, and hexes that make up a  triangulation, as well as parent-child relationships.
* 
*  - The CellAccessor class is derived from the TriaAccessor class for cases  where an object has full dimension, i.e. is a cell rather than for example a  line bounding a cell. In that case, additional information about the  topological connection of a mesh is available from an accessor such as to  request iterators pointing to neighbors of a cell.
* 
*  - The DoFAccessor class lets you access information related to degrees  of freedom associated with cells, faces, etc; it does so for both  DoFHandler and  [2.x.14]  objects. Note that the DoFAccessor  class is derived from either TriaAccessor or CellAccessor (depending  on whether the DoFAccessor points to an object of full dimension or  not) and so is able to provide a superset of information over its  base classes. Additionally, the DoFAccessor class comes in two  flavors, one accessing degrees of freedom on the level of a cell and  the other accessing the active dofs of an active cell.
* 
*  - The DoFCellAccessor class has the same purpose and relation to  DoFCellAccessor as the CellAccessor has to TriaAccessor.
* Except to look up member documentation, you will not usually have to deal withthe actual class names listed above. Rather, one uses the typedefs provided bythe mesh classes Triangulation, DoFHandler and  [2.x.15]  as wellas the function that generate such objects:
* <table border=1>  <tr>    <th>Class</th>    <th>cell_iterator type</th>    <th>function call</th>  </tr>  <tr>    <th>Triangulation</th>    <td>typename  [2.x.16]      [2.x.17]   </tr>  <tr>    <th>DoFHandler</th>    <td>typename  [2.x.18]      [2.x.19]   </tr>  <tr>     [2.x.20]     <td>typename  [2.x.21]      [2.x.22]   </tr></table>
* The Triangulation class supports iterating across cell faces with <tt>typename [2.x.23]  which is the type returned by [2.x.24] 
* Active iterators have the following properties:
* <table border=1>  <tr>    <th>Class</th>    <th>cell_iterator type</th>    <th>function call</th>  </tr>  <tr>    <th>Triangulation</th>    <td>typename  [2.x.25]      [2.x.26]   </tr>  <tr>    <th>DoFHandler</th>    <td>typename  [2.x.27]      [2.x.28]   </tr>  <tr>     [2.x.29]     <td>typename  [2.x.30]      [2.x.31]   </tr></table>
* The Triangulation class also supports iterating across active cell faces with<tt>typename  [2.x.32]  which is the type returned by [2.x.33] 
* In addition to these types and calls that act on cells and faces (logicalconcepts that depend on the dimension: a cell is a quadrilateral in 2d, buta hexahedron in 3d), there are corresponding types and calls like [2.x.34]  that act on thedimension independent geometric objects line, quad, and hex. These calls,just as the ones above, exist in active and non-active forms.
* The actual definition of all the typedefs local to the mesh classes arestated in the
* 
*  -  [2.x.35]    [2.x.36]  and   [2.x.37]  classes for Triangulation  iterators,
* 
*  - [1.x.8], [1.x.9], and [1.x.10] classes for DoFHandler and  [2.x.38]  iterators.
*  [2.x.39]  IteratorAccessorInternals Iterator and accessor internals
* Iterators, being like pointers, act as if they pointed to an actualobject, but in reality all they do is to return an accessor whendereferenced. The accessor object contains the state, i.e. it knowswhich object it represents, by storing for example which Triangulationit belongs to, and the level and index within this level of a cell. Itis therefore able to access the data that corresponds to the cell (orface, or edge) it represents
* There is a representation of past-the-end-pointers, denoted by specialvalues of the member variables  [2.x.40]  inthe TriaAccessor class: If  [2.x.41]   [2.x.42]  =0,then the object is valid; if [2.x.43] ==-1, then the iterator pointspast the end; in all other cases, the iterator is considered invalid.You can check this by calling the  [2.x.44]  function.
* Past-the-end iterators may also be used to compare an iterator withthe before-the-start value, when running backwards. There is nodistinction between the iterators pointing past the two ends of avector.
* Cells are stored based on a hierarchical structure of levels, therefore theabove mentioned structure is useful. Faces however are not organized inlevels, and accessors for objects of lower dimensionality do not have a [2.x.45]  member variable.
* 

* 
*  [2.x.46] 
* [0.x.1]*


* 
*  [2.x.47] 

* 
*  [2.x.48] 

* 
* [0.x.2]

include/deal.II-translator/A-headers/lac_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This module contains classes that involve linear algebra, i.e., those associated with matrices, vectors, and the solution of linear systems.
*  The description of individual groups of classes can be found in sub-modules.
*  The files implementing linear algebra functionality are in the  [2.x.1]  subdirectory, an abbreviation for  [2.x.2] L [2.x.3] inear  [2.x.4] A [2.x.5] lgebra  [2.x.6] C [2.x.7] lasses.

* 
* [0.x.1]

include/deal.II-translator/A-headers/laoperators_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  [1.x.0]
*  deal.II includes support for describing linear transformations in a very general way. This is done with a LinearOperator class that, like  [2.x.1]  "the MatrixType concept", defines a minimal interface for [1.x.1] a linear operation on a vector.
* 

* 
* [1.x.2]
* 
*  Thus, such an object can be used as a matrix object in all  [2.x.2]  "iterative solver" classes, either as a matrix object, or as  [2.x.3]  "preconditioner".
*  The big advantage of the LinearOperator class is that it provides syntactic sugar for complex matrix-vector operations. As an example consider the operation  [2.x.4] , where  [2.x.5] ,  [2.x.6]  and  [2.x.7]  denote (possibly different) SparseMatrix objects. In order to construct a LinearOperator  [2.x.8]  that performs above computation when applied on a vector, one can write:

* 
* [1.x.3]
*  Now,  [2.x.9]  can be used as a matrix object for further computation.
*  The linear_operator() function can be used to wrap an ordinary matrix or preconditioner object into a LinearOperator. A linear operator can be transposed with transpose_operator(), or inverted by using the inverse_operator() together with an iterative solver.
*  For objects of type LinearOperator, all vector space operations, i.e., addition and subtraction, scalar multiplication and composition (of compatible linear operators) are implemented:

* 
* [1.x.4]
* 
*  block_operator() and block_diagonal_operator() provide further encapsulation of individual linear operators into blocked linear operator variants.
*  The  [2.x.10]  tutorial program has a detailed usage example of the LinearOperator class.
* 

* 
*  [2.x.11]  As explained below, when using LinearOperator as  [2.x.12]  a PackagedOperation class instance is generated behind-the-curtains. Consequently, the user program has to include header files for both classes for compilation to be successful. In an attempt to make easier the decision of which headers to include in what circumstances and to prevent hidden templates-related compiler errors, all headers relevant to LinearOperator are grouped in the `<deal.II/lac/linear_operator_tools.h>` header file.
*  [1.x.5]
*  An  application of a LinearOperator object to a vector via  [2.x.13]  yields a PackagedOperation object that stores this computation.
*  The PackagedOperation class allows lazy evaluation of expressions involving vectors and linear operators. This is done by storing the computational expression and only performing the computation when either the object is implicitly converted to a vector object, or  [2.x.14]  (or  [2.x.15]  is invoked by hand. This avoids unnecessary temporary storage of intermediate results.
*  As an example consider the addition of multiple vectors:

* 
* [1.x.6]
*  Converting the PackagedOperation <code>a + b
* 
*  - c + d</code> to a vector results in code equivalent to the following code

* 
* [1.x.7]
*  that avoids any intermediate storage. As a second example (involving a LinearOperator object) consider the computation of a residual  [2.x.16] :
* 

* 
* [1.x.8]
*  Here, the expression <code>b
* 
*  - op_a x</code> results again in an object of type PackagedOperation that stores the [1.x.9] that should be performed using the two vectors and the linear operator. Converting the expression to a vector (as happens here with the assignment to the vector  [2.x.17] ) executes the computation (see the following note).
* 

* 
*  [2.x.18]  Lazy evaluation of a computational expression necessarily involves references to the underlying vector and matrix objects. For example, the creation of a  [2.x.19]  object

* 
* [1.x.10]
*  stores the computational expression of the residual with references to the vector  [2.x.20] . It does not perform any computation at this point. In particular, if  [2.x.21]  or  [2.x.22]  are changed [1.x.11] the creation of  [2.x.23]  every subsequent evaluation of the expression is performed with the new values

* 
* [1.x.12]
*  Thus, as a safeguard, if you want to compute the result of an expression right away, always explicitly use a vector type on the left side (and not  [2.x.24] ):

* 
* [1.x.13]
* 
* 

* 
*  [2.x.25]  The  [2.x.26]  tutorial program has a detailed usage example of the PackagedOperation class.
* 

* 
*  [2.x.27]  Many use cases of LinearOperator lead to intermediate expressions requiring a PackagedOperation. In order to include all necessary header files in one go consider using

* 
* [1.x.14]
* 
* 

* 

* 
*  [2.x.28] 

* 
*  [2.x.29] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/main_0.txt
[0.x.0]*
  [2.x.0] 
*  This is the main starting page for the deal.II class and function documentation. Documentation on other aspects, such as the build system, can be found elsewhere. In addition, there are [1.x.0].
*  Many of the classes in the deal.II library can be grouped into modules (see the [1.x.1] or the corresponding entry in the menu at the top of this page). These modules form around the building blocks of any finite element program. An outline of how the primary groups of classes in deal.II interact is given by the following clickable graph, with a more detailed description below (gray boxes denote a subset of the optional external libraries, gray ovals a subset of the optional external applications with which deal.II can interact):
* 

* 
* [1.x.2]
*  These groups are all covered in the tutorial programs, with a first overview of how they fit together given in  [2.x.1] . The following is a guide to this classification of groups, as well as links to the documentation pertaining to each of them:
*   [2.x.2]     [2.x.3]  [1.x.3]: Triangulations are collections of   cells and their lower-dimensional boundary objects. Cells are   images of the reference hypercube [0,1]<sup>dim</sup> under a   suitable mapping in the module on  [2.x.4] .
*    The triangulation stores geometric and topological   properties of a mesh: how are the cells connected and where are   their vertices. A triangulation doesn't know   anything about the finite elements that you may want to used on   this mesh, and a triangulation does not even know anything about   the shape of its cells: in 2d it only knows that a cell has 4   faces (lines) and 4 vertices (and in 3d that it has 6 faces   (quadrilaterals), 12 lines, and 8 vertices), but everything else   is defined by a mapping class.
*    The properties and data of triangulations are almost always   queried through loops over all cells, possibly querying all faces   of each cell as well. Most   of the knowledge about a mesh is therefore hidden behind    [2.x.5]  iterators, i.e. pointer-like structures that one can   iterate from one cell to the next, and that one can ask for   information about the cell it presently points to.
*    The classes that describe triangulations and cells are located   and documented in the  [2.x.6]  module. Iterators are described   in the  [2.x.7]  module.
*     [2.x.8]  [1.x.4]: Manifolds describe the shape of cells and,   more generally, the geometry of the domain on which one wants   to solve an equation. They use the language of differential   geometry. More information can be found in  [2.x.9] .
*     [2.x.10]  [1.x.5]: Finite element classes describe the   properties of a finite element space as defined on the unit   cell. This includes, for example, how many degrees of freedom are   located at vertices, on lines, or in the interior of cells. In   addition to this, finite element classes of course have to   provide values and gradients of individual shape functions at   points on the unit cell.
*    The finite element classes are described in the  [2.x.11]  module.
*     [2.x.12]  [1.x.6]: As with finite elements, quadrature   objects are defined on the unit cell. They only describe the   location of quadrature points on the unit cell, and the weights   of quadrature points thereon.
*    The documentation of the classes describing particular quadrature   formulas is found in the  [2.x.13]  module.
*     [2.x.14]  [1.x.7]: %DoFHandler objects are the confluence   of triangulations and finite elements: the finite element class   describes how many degrees of freedom it needs per vertex, line,   or cell, and the DoFHandler class allocates this space so that   each vertex, line, or cell of the triangulation has the correct   number of them. It also gives them a global numbering.
*    A different viewpoint is this: While the mesh and finite element describe   abstract properties of the the finite dimensional space  [2.x.15]  in which we   seek the discrete solution, the %DoFHandler classes enumerate a concrete   basis of this space so that we can represent the discrete solution as    [2.x.16]  by an ordered set of   coefficients  [2.x.17] .
*    Just as with triangulation objects, most operations on   DoFHandlers are done by looping over all cells and doing something   on each or a subset of them. The interfaces of the two classes are   therefore rather similar: they allow to get iterators to the   first and last cell (or face, or line, etc) and offer information   through these iterators. The information that can be gotten from   these iterators is the geometric and topological information that   can already be gotten from the triangulation iterators (they are   in fact derived classes) as well as things like the global   numbers of the degrees of freedom on the present cell. On can   also ask an iterator to extract the values corresponding to the   degrees of freedom on the present cell from a data vector that   stores values for all degrees of freedom associated with a   triangulation.
*    It is worth noting that, just as triangulations, DoFHandler   classes do not know anything about the mapping from the unit cell   to its individual cells. It is also ignorant of the shape   functions that correspond to the degrees of freedom it manages:   all it knows is that there are, for example, 2 degrees of freedom   for each vertex and 4 per cell interior. Nothing about their   specifics is relevant to the DoFHandler class with the exception of   the fact that they exist.
*    The DoFHandler class and its associates are described in the  [2.x.18]    dofs module. In addition, there are specialized versions that can   handle multilevel and hp-discretizations. These are described in   the  [2.x.19]  and  [2.x.20]  modules. Finite element methods frequently   imply constraints on degrees of freedom, such as for hanging nodes   or nodes at which boundary conditions apply; dealing with such   constraints is described in the  [2.x.21]  module.
*     [2.x.22]  [1.x.8]: The next step in a finite element program   is that one would want to compute matrix and right hand side   entries or other quantities on each cell of a triangulation,   using the shape functions of a finite element and quadrature   points defined by a quadrature rule. To this end, it is necessary   to map the shape functions, quadrature points, and quadrature   weights from the unit cell to each cell of a triangulation. This   is not directly done by, but facilitated by the Mapping and   derived classes: they describe how to map points from unit to   real space and back, as well as provide gradients of this   derivative and Jacobian determinants.
*    These classes are all described in the  [2.x.23]  module.
*     [2.x.24]  [1.x.9]: The next step is to actually take a finite   element and evaluate its shape functions and their gradients at   the points defined by a quadrature formula when mapped to the   real cell. This is the realm of the FEValues class and siblings:   in a sense, they offer a point-wise view of the finite element   function space.
*    This seems restrictive: in mathematical analysis, we always write   our formulas in terms of integrals over cells, or faces of cells,   involving the finite element shape functions. One would therefore   think that it is necessary to describe finite element spaces as   continuous spaces. However, in practice, this is not necessary:   all integrals are in actual computations replaced by   approximations using quadrature formula, and what is therefore   really only necessary is the ability to evaluate shape functions   at a finite number of given locations inside a domain. The   FEValues classes offer exactly this information: Given finite   element, quadrature, and mapping objects, they compute the   restriction of a continuous function space (as opposed to   discrete, not as opposed to discontinuous) to a discrete number   of points.
*    There are a number of objects that can do this: FEValues for   evaluation on cells, FEFaceValues for evaluation on faces of   cells, and FESubfaceValues for evaluation on parts of faces of   cells. All these classes are described in the  [2.x.25]    module.
*     [2.x.26]  [1.x.10]: If one knows how to evaluate the   values and gradients of shape functions on individual cells using   FEValues and friends, and knows how to get the global numbers of   the degrees of freedom on a cell using the DoFHandler iterators,   then the next step is to use the bilinear form of the problem to   assemble the system matrix (and right hand side) of the linear   system. We will then determine the solution of our problem from   this linear system.
*    To do this, we need to have classes that store and manage the   entries of matrices and vectors. deal.II comes with a whole set   of classes for this purpose, as well as with interfaces to other   software packages that offer similar functionality. Documentation   to this end can be found in the  [2.x.27]  module.
*     [2.x.28]  [1.x.11]: In order to determine the solution of   a finite-dimensional, linear system of equations, one needs   linear solvers. In finite element applications, they are   frequently iterative, but sometimes one may also want to use   direct or sparse direct solvers. deal.II has quite a number of   these. They are documented in the  [2.x.29]  module.
*     [2.x.30]  [1.x.12]: Finally, once one has obtained a solution of   a finite element problem on a given triangulation, one will often   want to postprocess it using a visualization program. This   library doesn't do the visualization by itself, but rather generates output   files in a variety of graphics formats understood by widely   available visualization tools.
*    A description of the classes that do so is given in the  [2.x.31]    output module.  [2.x.32] 
*  In addition, deal.II has a number of groups of classes that go beyond the ones listed here. They pertain to more refined concepts of the hierarchy presented above, or to tangential aspects like handling of input and output that are not necessarily specific to finite element programs, but appear there as well. These classes are all listed in the Classes and Namespaces views reachable from the menu bar at the top of this page, and are also grouped into modules of their own (see the [1.x.13] at the top of this page).
*  We provide the Doxygen tag file for those of you who would like to directly link the documentation of application programs to the deal.II online documentation. The tag file is at [1.x.14]. For each release of deal.II, it resides in the directory right above the Doxygen reference documentation. In order to use the tag file, you have to download it into a place where Doxygen can find it. After that, find the key  [2.x.33]  in your Doxygen options file and write something like <pre> TAGFILES = deal.tag=http://www.dealii.org/X.Y.Z/doxygen/deal.II </pre> where  [2.x.34]  refers to the release you want to link to. Be sure you use the matching tag file. In theory, you can also link against the developing revisions of deal.II, but then you have to fear that your links may become invalid if the deal.II structure changes.

* 
* [0.x.1]

include/deal.II-translator/A-headers/manifold_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  [1.x.0]
*  The classes in this module are concerned with the description of the manifold in which the domain that a Triangulation describes lives. This manifold description is necessary in several contexts:
*   [2.x.1] 
*     [2.x.2]  Mesh refinement: Whenever a cell is refined, it is necessary   to introduce new vertices in the Triangulation. In the   simplest case, one assumes that the objects that make up the   Triangulation are straight line segments, a bi-linear surface or   a tri-linear volume. The next vertex is then simply put into the   middle of the old ones (where "middle" means a suitable average of the   locations of the pre-existing vertices). This is the default behavior of   the Triangulation class, and is described by the FlatManifold class.
*    On the other hand, if one deals with curved geometries, or geometries which   require a denser refinement in some direction, this is not the appropriate   thing to do. The classes derived from the Manifold base class therefore   describe the geometry of a domain. One can then attach an object of a class   derived from this base class to the Triangulation object using the    [2.x.3]  function associating it with a manifold_id   (see  [2.x.4]  use this manifold_id on the cells, faces or edges   of the triangulation that should be described by this manifold using the    [2.x.5]  function, and then the Triangulation will   ask the manifold object where a new vertex to be located on a cell, face or   edge so attributed should be located upon mesh refinement. Several classes   already exist to support the most common geometries, e.g.,   CylindricalManifold, or PolarManifold, which represent respectively the   geometry obtained when describing your space in cylindrical coordinates or   in polar coordinates. By default, all curved geometries generated using   functions in the GridGenerator namespace attach the correct Manifold object   to the curved parts of the domain.
*     [2.x.6]  Integration: When using higher order finite element methods, it is   often necessary to compute cell terms (like cell contributions to the   matrix and right hand side of the linear system) using curved   approximations of the boundary, rather than the straight line   approximation. The actual implementation of such curved elements happens   in the Mapping class (see the  [2.x.7]  module), which however obtains   its information about the boundary of the domain from the classes   described here. The same is, of course, true when integrating boundary   terms (e.g., inhomogeneous Neumann boundary conditions).
*     [2.x.8]  Domains with nonzero codimension: In cases where a Triangulation is   embedded into a higher dimensional space, i.e., whenever the second   template argument of the Triangulation class is explicitly specified and   larger than the first (for an example, see  [2.x.9] ), the manifold   description objects serve as a tool to describe the geometry not only of   the boundary of the domain but of the domain itself, in case the domain   is a manifold that is in fact curved. In these cases, one can use the    [2.x.10]  function to indicate what manifold   description to use when refining the curve, or when computing integrals   using high order mappings.
*   [2.x.11]  Many other examples, as well as much theoretical underpinning for the implementation in deal.II, is provided in the  [2.x.12]  "geometry paper".
*  In deal.II, a Manifold is seen as a collection of points, together with a notion of distance between points (on the manifold). New points are typically obtained by providing a local coordinate system on the manifold, identifying existing points in the local coordinate system (pulling them back using the local map to obtain their local coordinates), find the new point in the local coordinate system by weighted sums of the existing points, and transforming back the point in the real space (pushing it forward using the local map). The main class that implements this mechanism is the ChartManifold class, and this is the class that users will likely overload for complex geometries.
*  While this process is non trivial in most cases of interest, for most of the trivial geometries, like cylinders, spheres or shells, deal.II provides reasonable implementations. More complicated examples can be described using the techniques shown in  [2.x.13]  and  [2.x.14] .
*  In the grand scheme of things, the classes of this module interact with a variety of other parts of the library:

* 
* [1.x.1]
* 

*  [1.x.2]
*  A simple example why dealing with curved geometries is already provided by  [2.x.15] , though it is not elaborated there. By default, the functions in GridGenerator will attach manifolds to meshes when needed. In each code snippet below we call  [2.x.16]  to remove these manifolds and handle all Manifold attachment in the example itself to make the impact of the choice of Manifold clear.
*  Consider this small variation of the  [2.x.17]  function shown there, where we simply refine [1.x.3] cell several times:

* 
* [1.x.4]
*  This code leads to a mesh that looks like this:
*   [2.x.18] 
*  Our intention was to get a mesh that resembles a ring. However, since we did not describe this to the triangulation, what happens is that we start with the 10 coarse cells in circumferential direction we told  [2.x.19]  to create, and each of these is then 3 times globally refined. Each time refinement requires a new vertex, it is placed in the middle of the existing ones, regardless of what we may have intended (but omitted to describe in code).
*  This is easily remedied. Consider this code:

* 
* [1.x.5]
*  This code is better, producing the following mesh:
*   [2.x.20] 
*  The mesh looks better in that it faithfully reproduces the circular inner and outer boundaries of the domain. However, it is still possible to identify 20 kinks in the tangential lines. They result from the fact that every time a cell is refined, new vertices on interior lines are just placed into the middle of the existing line (the boundary lines are handled differently because we have attached a manifold object). In the first refinement with 10 cells, we got improved points because both outer boundaries have provided a curved description according to the description on blending different manifolds below. In other words, the new points after the first refinement end up in places that may be in the geometric middle of a straight line, but not on a circle around the center.
*  This can be remedied by assigning a manifold description not only to the lines along the boundary, but also to the radial lines and cells (which, in turn, will inherit it to the new lines that are created upon mesh refinement). This is exactly what  [2.x.21]  does by default. For demonstration purposes, we disable the default Manifold behavior and then duplicate it manually:

* 
* [1.x.6]
*  This leads to the following mesh:
*   [2.x.22] 
*  So why does this matter? After all, the last two meshes describe the exact same domain and we know that upon mesh refinement we obtain the correct solution regardless of the choice of cells, as long as the diameter of the largest cell goes to zero.
*  There are two answers to this question. First, the numerical effort of solving a partial differential equation to a certain accuracy typically depends on the [1.x.7] of cells since the constant  [2.x.23]  in error estimates of the form  [2.x.24]  depends on factors such as the maximal ratio of radii of the smallest circumscribed to largest inscribed circle over all cells (for triangles; or a suitable generalization for other types of cells). Thus, it is worthwhile creating meshes with cells that are as well-formed as possible. This is arguably not so much of an issue for the meshes shown above, but is sometimes an issue. Consider, for example, the following code and mesh:

* 
* [1.x.8]
* 
*   [2.x.25] 
*  Here, we create only three circumferential cells in the beginning, and refining them leads to the mesh shown. Clearly, we have cells with bad aspect ratios, despite the first refinement that puts the new point into the middle.
*  If we drive this further and start with a coarse mesh of a much thinner rim between the radii 0.8 and 1.0 and only three cells (which may be inappropriate here, since we know that it is not sufficient, but may also be impossible to avoid for complex geometries generated in mesh generators), we observe the following:
* 

* 
* [1.x.9]
* 
*   [2.x.26] 
*  This mesh neither has the correct geometry after refinement, nor do all cells have positive area as is necessary for the finite element method to work. However, even when starting with such an inopportune mesh, we can make things work by attaching a suitable geometry description not only to the boundary but also to interior cells and edges, using the same code as above:

* 
* [1.x.10]
* 
*   [2.x.27] 
*  In this last example we finally let GridGenerator do its job and we keep the default manifold configuration, which is a SphericalManifold on every cell and face.
*  Here, even starting with an initial, inappropriately chosen mesh retains our ability to adequately refine the mesh into one that will serve us well. This example may be manufactured here, but it is relevant, for example in the context of what  [2.x.28]  produces in 3d (see the documentation of this function). It is also germane to the cases discussed in the  [2.x.29]  "glossary entry on distorted cells".
*   [2.x.30]   [2.x.31]  "Glossary entry on manifold indicators"
*  [1.x.11]
*  In a realistic application, it happens regularly that different manifold descriptions need to be combined. The simplest case is when a curved description is only available for the boundary but not for the interior of the computational domain. The manifold description for a ball also falls into this category, as it needs to combine a spherical manifold at the circular part with a straight-sided description in the center of the domain where the spherical manifold is not valid.
*  In general, the process of blending different manifold descriptions in deal.II is achieved by the so-called transfinite interpolation. Its formula in 2D is, for example, described on [1.x.12]. Given a point  [2.x.32]  on a chart, the image of this point in real space is given by

* 
* [1.x.13]
*  where  [2.x.33]  denote the four vertices bounding the image space and  [2.x.34]  are the four curves describing the lines of the cell.
*  If we want to find the center of the cell according to the manifold (that is also used when the grid is refined), the chart is the unit cell  [2.x.35]  and we want to evaluate this formula in the point  [2.x.36] . In that case,  [2.x.37]  is the position of the midpoint of the lower face (indexed by 2 in deal.II's ordering) that is derived from its own manifold,  [2.x.38]  is the position of the midpoint of the upper face (indexed by 3 in deal.II),  [2.x.39]  is the midpoint of the face on the left (indexed by 0), and  [2.x.40]  is the midpoint of the right face. In this formula, the weights equate to  [2.x.41]  for the four midpoints in the faces and to  [2.x.42]  for the four vertices. These weights look weird at first sight because the vertices enter with negative weight but the mechanism does what we want: In case of a cell with curved description on two opposite faces but straight lines on the other two faces, the negative weights of  [2.x.43]  in the vertices balance with the center of the two straight lines in radial direction that get weight  [2.x.44] . Thus, the average is taken over the two center points in curved direction, exactly placing the new point in the middle.
*  In three spatial dimensions, the weights are  [2.x.45]  for the face midpoints,  [2.x.46]  for the line mid points, and  [2.x.47]  for the vertices, again balancing the different entities. In case all the surrounding of a cell is straight, the formula reduces to the obvious weight  [2.x.48]  on each of the eight vertices.
*  In the MappingQGeneric class, a generalization of this concept to the support points of a polynomial representation of curved cells, the nodes of the Gauss-Lobatto quadrature, is implemented by evaluating the boundary curves in the respective Gauss-Lobatto points  [2.x.49]  and combining them with the above formula. The weights have been verified to yield optimal convergence rates  [2.x.50]  also for very high polynomial degrees, say  [2.x.51] .
*  In the literature, other boundary descriptions are also used. Before version 9.0 deal.II used something called Laplace smoothing where the weights that are applied to the nodes on the circumference to get the position of the interior nodes are determined by solving a Laplace equation on the unit element. However, this led to boundary layers close to the curved description, i.e., singularities in the higher derivatives of the mapping from unit to real cell.
*  If the transition from a curved boundary description to a straight description in the interior is done wrong, it is typically impossible to achieve high order convergence rates. For example, the Laplace smoothing inside a single cell leads to a singularity in the fourth derivative of the mapping from the reference to the real cell, limiting the convergence rate to 3 in the cells at the boundary (and 3.5 if global L2 errors were measured in 2D). Other more crude strategies, like completely ignoring the presence of two different manifolds and simply computing the additional points of a high-order mapping in a straight coordinate system, could lead to even worse convergence rates. The current implementation in deal.II, on the other hand, has been extensively verified in this respect and should behave optimally.
*  A bad strategy for blending a curved boundary representation with flat interior representations obviously also reflects mesh quality. For example, the above case with only 3 circumferential cells leads to the following mesh with Laplace manifold smoothing rather than the interpolation from the boundary as is implemented in deal.II:
*   [2.x.52] 
*  To use a more practical example, consider the refinement of a ball with a SphericalManifold attached to the spherical surface. The Laplace-type smoothing gives the following rather poor mesh:
*   [2.x.53] 
*  If we, instead, use the weights derived from transfinite interpolation, the situation is considerably improved:
*   [2.x.54] 
*  Of course, one could get even better meshes by applying the TransfiniteInterpolationManifold to the whole domain except the boundary where SphericalManifold is attached, as shown by the figures in that class, but in principle, the mesh smoothing implemented in deal.II is as good as it can get from a boundary description alone.
* 

* 
*  [2.x.55]   [2.x.56]  Luca Heltai, 2013, Martin Kronbichler, 2017

* 
* [0.x.1]

include/deal.II-translator/A-headers/matrices_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  deal.II comes with a number of different matrix classes, tailored to the various purposes for which matrices are used. For example, there are full matrices, sparse matrices using different storage schemes, matrices composed of individual blocks, and matrices implemented as interfaces to other linear algebra classes. As far as possible, all these implementations share a common interface that contains at least the operations necessary to write iterative linear solvers (see  [2.x.1] ), but also element-wise access to read from and write to a matrix.
*  This module is split into different parts.  [2.x.2]  "Basic matrices" contains all the matrix classes actually storing entries.  [2.x.3]  "Derived matrices", on the other hand, only use basic matrices, but implement certain operations on them. For example, TransposeMatrix provides a matrix-vector multiplication that acts as if the underlying matrix had been transposed, without actually ever storing a transposed matrix.
*   [2.x.4]  are matrix classes as well, since they perform linear operations on vectors.
* 

* 
*  [2.x.5] 

* 
* [0.x.1]*


* 
*  [2.x.6] 
*  These are the actual matrix classes provided by deal.II. It is possible to store values in them and retrieve them. Furthermore, they provide the full interface required by linear solvers (see  [2.x.7] ).
*  Among the matrices in this group are full matrices, different sparse matrices, and block matrices. In addition, some of the classes in the interfaces to other linear algebra libraries (for example the PETScWrappers) are matrices.
*  Most of the deal.II sparse matrix classes are separated from their sparsity patterns, to make storing several matrices with the same sparsity pattern more efficient. See  [2.x.8]  for more information.
* 

* 
*  [2.x.9] 

* 
* [0.x.2]*


* 
*  [2.x.10] 
*  These matrices are built on top of the basic matrices. They perform special operations using the interface defined by  [2.x.11]  "the MatrixType concept".
* 

* 
*  [2.x.12] 

* 
* [0.x.3]

include/deal.II-translator/A-headers/matrixfree_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This module describes the matrix-free infrastructure in deal.II. An outline of how the primary groups of classes in deal.II interact with the matrix-free infrastructure is given by the following clickable graph, with a more detailed description below:
* 

* 
* [1.x.0]
*  In essence, the framework provided by the FEEvaluation class on top of the data storage in MatrixFree is a specialized operator evaluation framework. It is currently only compatible with a subset of the elements provided by the library which have a special structure, namely those where the basis can be described as a tensor product of one-dimensional polynomials. This opens for efficient transformation between vector entries and values or gradients in quadrature points with a technique that is called sum factorization. This technique has its origin in the spectral element community, started by the work of Orszag in 1980. While this technique is initially nothing else than a particular technique for assembling vectors (or matrices) that is faster than the general-purpose vehicle FEValues, its efficiency makes it possible to use these integration facilities to directly evaluate the matrix-vector products in iterative solvers, rather than first assembling a matrix and then using that matrix for doing matrix-vector products. This step is initially non-intuitive and goes against what many people were taught in their mathematics and computer science education, including most of the deal.II developers, because it appears to be wasteful to re-compute integrals over and over again, instead of using precomputed data. However, as the tutorial programs  [2.x.1] ,  [2.x.2] ,  [2.x.3] ,  [2.x.4] , and  [2.x.5]  show, these concepts usually outperform traditional algorithms on modern computer architectures.
*  The two main reasons that favor matrix-free computations are the following:  [2.x.6]   [2.x.7]  Matrix-free methods skip the storage of big global sparse matrices and compute the underlying weak forms on the fly. Since the memory transfer, i.e., the speed at which the data can be read from RAM memory, is the bottleneck for matrix-based computations rather than the actual arithmetic done using this data, a matrix-free evaluation that reads less data can be advantageous even if it does more computations. This concept is building upon a trend in computer architecture which is best described by the term [1.x.1], saying that compute performance has increased more rapidly than the memory performance. Thus, a certain degree of arithmetic operations is essentially for free, and this share has become larger during the last twenty years. It has enabled this radical algorithm switch going from a matrix-based to a matrix-free implementation of matrix-vector products for iterative solvers, besides their classical use in explicit time integration. Of course, the implementation must be efficient and there cannot be an excess in computations to make it a win in total. The deal.II library uses SIMD vectorization and highly optimized kernels based on templates of the polynomial degree to achieve this goal. To give a perspective, a sparse matrix-vector product for quadratic elements FE_Q used to be equally fast as the matrix-free implementation on processors designed around 2005-2007 (e.g. Pentium 4 or AMD Opteron Barcelona with 2-4 cores per chip). By 2018, the matrix-free evaluation is around eight times as fast (measured on Intel Skylake Server, 14 cores).  [2.x.8]  Matrix-free methods have a better complexity per degree of freedom as the degree is increased, due to sum factorization. The work per degree of freedom increases as  [2.x.9]  in the degree  [2.x.10]  for matrix-free schemes, whereas it increases as  [2.x.11]  for matrix-based methods. This gives higher order schemes an edge. A particularly nice feature in matrix-free evaluation is that the  [2.x.12]  terms often dominate, so it appears that higher order methods are as fast in terms of evaluation time as low order ones, when they have the same number of degrees of freedom. For the implementation in deal.II, best throughput is typically achieved for polynomial degrees between three and six.  [2.x.13] 
*  To summarize, matrix-free computations are the way to go for higher order elements (where higher order means everything except linear shape functions) and use in explicit time stepping ( [2.x.14] ) or iterative solvers where also preconditioning can be done in a matrix-free way, as demonstrated in the  [2.x.15]  and  [2.x.16]  tutorial programs.
*  [1.x.2]
*  The top level interface is provided by the FEEvaluation class, which also contains an extensive description of different use cases.
*  [1.x.3]
*  The class FEEvaluation is derived from the class FEEvaluationAccess, which in turn inherits from FEEvaluationBase. The FEEvaluation class itself is templated not only on the dimension, the number of components, and the number type (e.g. double or float), but also on the polynomial degree and on the number of quadrature points per spatial direction. This information is used to pass the loop lengths in sum factorization to the respective kernels (see `tensor_product_kernels.h` and `evaluation_kernels.h`) and ensure optimal efficiency. All methods that access the vectors or provide access into the data fields on an individual quadrature point are inherited from FEEvaluationAccess.
*  The motivation for the FEEvaluationAccess classes is to allow for specializations of the value and gradient access of interpolated solution fields depending on the number of components. Whereas the base class FEEvaluationBase returns the gradient as a `Tensor<1,n_components,Tensor<1,dim,VectorizedArray<Number>>>`, with the outer tensor going over the components and the inner tensor going through the `dim` components of the gradient. For a scalar field, i.e., `n_components=1`, we can skip the outer tensor and simply use `Tensor<1,dim,VectorizedArray<Number>>` as the gradient type. Likewise, for a system with `n_components=dim`, the appropriate format for the gradient is `Tensor<2,dim,VectorizedArray<Number>>`.
*  [1.x.4]
*  Face integrals, like for inhomogeneous Neumann conditions in continuous FEM or for the large class of discontinuous Galerkin schemes, require the evaluation of quantities on the quadrature point of a face, besides the cell integrals. The facilities for face evaluation are mostly shared with FEEvaluation, in the sense that FEFaceEvaluation also inherits from FEEvaluationAccess. All data fields regarding the degrees of freedom and shape functions can be reused, the latter because all information consists of 1D shape data anyway. With respect to the mapping data, however, a specialization is used because the data is of `structdim=dim-1`. As a consequence, the FEEvaluationAccess and FEEvaluationBase are given a template argument `is_face` to hold pointers to the cell and face mapping information, respectively. Besides access to the function values with  [2.x.17]  or gradients with  [2.x.18]  the face evaluator also enables the access to the normal vector by  [2.x.19]  and a specialized field  [2.x.20]  which returns the derivative of the solution field normal to the face. This quantity is computed as the gradient (in real space) multiplied by the normal vector. The combination of the gradient and normal vector is typical of many (simple) second-order elliptic equations, such as the discretization of the Laplacian with the interior penalty method. If the gradient alone is not needed, the combined operation significantly reduces the data access, because only `dim` data entries for `normal Jacobian` per quadrature point are necessary, as opposed to `dim^2` fields for the Jacobian and `dim` fields for the normal when accessing them individually.
*  An important optimization for the computation of face integrals is to think about the amount of vector data that must be accessed to evaluate the integrals on a face. Think for example of the case of FE_DGQ, i.e., Lagrange polynomials that have some of their nodes on the element boundary. For evaluation of the function values, only  [2.x.21]  degrees of freedom contribute via a non-zero basis function, whereas the rest of the  [2.x.22]  basis functions evaluate to zero on that boundary. Since vector access is one of the bottlenecks in matrix-free computations, the access to the vector should be restricted to the interesting entries. To enable this setup, the method  [2.x.23]  (and  [2.x.24]  for the integration equivalent) combines the vector access with the interpolation to the quadrature points. There exist two specializations, including the aforementioned "non-zero" value case, which is stored as the field  [2.x.25]  A similar property is also possible for the case where only the value and the first derivative of a selected number of basis functions evaluate to nonzero on a face. The associated element type is FE_DGQHermite and the decision is stored on the property  [2.x.26]  The decision on whether such an optimized kernel can be used is made automatically inside  [2.x.27]  and  [2.x.28]  It might seem inefficient to do this decision for every integration task, but in the end this is a single `if` statement (conditional jump) that is easily predicable for a modern CPU as the decision is always the same inside an integration loop. (One only pays by somewhat increased compile times because the compiler needs to generate code for all paths, though).
*  [1.x.5]
*  The tasks performed by FEEvaluation and FEFaceEvaluation can be split into the three categories: [1.x.6], [1.x.7], and [1.x.8]. This split is reflected by the major data fields contained by MatrixFree, using  [2.x.29]   [2.x.30]  and  [2.x.31]  for each these three categories, respectively. Their design principles and internal layout is described in the following subsections.
*  The main interface all these data structure adhere to is that integration tasks are broken down into a range of cells or faces that one can index into by a single integer index. The information about an integer range for the cell integrals, inner face integrals, and boundary integrals is provided by the class  [2.x.32]  using the data fields `cell_partition_data`, `face_partition_data`, and `boundary_partition_data`. This class also contains information about subranges of indices for scheduling tasks in parallel using threads, and a grouping of the index range within `{cell,face,boundary}_partition_data` for interleaving cell and face integrals such that the access to vector entries for cell and face integrals re-uses data already in caches.
*  [1.x.9]
*  The main purpose of the DoFInfo class is to provide the indices consumed by the vector access functions  [2.x.33]  and  [2.x.34]  The indices are laid out as follows:  [2.x.35]   [2.x.36] Indices are stored in MPI-local index space to enable direct array access, rather than translating a global index into a local one. The latter would be absolutely detrimental to performance. [2.x.37]   [2.x.38] The indices are stored in a field called  [2.x.39]  which is a long index array. The access granularity in terms of a [1.x.10] is controlled by the auxiliary field  [2.x.40]  that is similar to the rowstart index in a compressed matrix storage. The scheme supports variable lengths because we support hp-adaptivity and index indirections due to constraints that are contained in the main index array. Due to vectorization over cells, the access granularity would initially be in terms of [1.x.11]. However, we must be able to access also an individual cell, for example for face integrals with the batches of faces that are in general different from the cell batches and access is thus not linear. Furthermore, the support for multi-component systems becomes transparent if we provide a [1.x.12] to every single component separately. Thus, the `row_starts` field is of length  [2.x.41]   [2.x.42]   [2.x.43]  The translation between components within a system of multiple base elements is controlled by the four variables  [2.x.44]     [2.x.45]  int> n_components` (components per base element),      [2.x.46]     [2.x.47]  int> start_components` (translation from the base     element to the unique component number), [2.x.48]     [2.x.49]  int> component_to_base_index` (translation from     unique component number to base index), and  [2.x.50]     [2.x.51]  int>> component_dof_indices_offset`    (offset of the particular component's range of degrees of freedom within    the full list of degrees of freedom on a cell). [2.x.52]   [2.x.53]   [2.x.54] 
*   [2.x.55] Information to extract the FE index in hp-adaptive computations. [2.x.56]   [2.x.57] Information about the 'first access' into a particular vector entry that is used to zero out the entries in a destination vectors within the  [2.x.58]  shortly before accessing them the first time. This is used to avoid writing zeros to the whole vector which destroys data locality. [2.x.59]   [2.x.60] 
*  The setup of the data structures in  [2.x.61]  is done in  [2.x.62]  where we first assume a very general finite element layout, be it continuous or discontinuous elements, and where we resolve the constraints due to hanging nodes. This initial step is done in the original ordering of cells. In a later stage, these cells will in general be rearranged to reflect the order by which we go through the cells in the final loop, and we also look for patterns in the DoF indices that can be utilized, such as contiguous index ranges within a cell. This reordering is done to enable overlap of communication and computation with MPI (if enabled) and to form better group of batches with vectorization over cells. The data storage of indices is linear in this final order, and arranged in  [2.x.63] 
*  Since the amount of data to store indices is not negligible, it is worthwhile to reduce the amount of data for special configurations that carry more structure. One example is the case of FE_DGQ where a single index per cell is enough to describe all its degrees of freedom, with the others coming in consecutive order. The class  [2.x.64]  contains a special array of vectors  [2.x.65]  that contains a single number per cell. Since both cell and face integrals use different access patterns and the data in this special case is small, we are better off storing 3 such vectors, one for the faces decorated as `interior` (index 0), one for the faces decorated as `exterior` (index 1), and one for the cells (index 2), rather than using the indirection through  [2.x.66]  There is a series of additional special storage formats available in DoFInfo. We refer to the documentation of the struct  [2.x.67]  for the options implemented in deal.II and their motivation.
*  Finally, the DoFInfo class also holds a shared pointer describing the parallel partitioning of the vectors. Due to the restriction of  [2.x.68]  the indices within an individual DoFHandler object passed to the  [2.x.69]  function must be contiguous within each MPI process, i.e., the local range must consist of at most one chunk. Besides the basic partitioner, the class also provides a set of tighter index sets involving only a subset of all ghost indices that are added to the vectors' ghost range. These exchange patterns are designed to be combined with the reduced index access via the  [2.x.70]  for example.
*  The MatrixFree class supports multiple DoFHandler objects to be passed to the  [2.x.71]  function. For each of these DoFHandler objects, a separate  [2.x.72]  object is created. In MatrixFree, we store a  [2.x.73]  of  [2.x.74]  objects to account for this fact.
*  [1.x.13]
*  The evaluation of one-dimensional shape functions on one-dimensional quadrature points is stored in the class  [2.x.75]  More precisely, we hold all function values, gradients, and hessians. Furthermore, the values and derivatives of shape functions on the faces, i.e., the points 0 and 1 of the unit interval, are also stored. For face integrals on hanging nodes, the coarser of the two adjacent cells must interpolate the values not to the full quadrature but to a subface only (evaluation points either scaled to [0, 1/2] or [1/2, 1]). This case is handled by the data fields `values_within_subface`, `gradients_within_subface`, and `hessians_within_subface`. This data structure also checks for symmetry in the shape functions with respect to the center of the reference cell (in which case the so-called even-odd transformation is applied, further reducing computations).
*  [1.x.14]
*  The evaluated geometry information is stored in the class  [2.x.76]  Similarly to the  [2.x.77]  class, multiple variants are possible within a single MatrixFree instance, in this case based on multiple quadrature formulas. Furthermore, separate data for both cells and faces is stored. Since there is more logic involved and there are synergies between the fields, the  [2.x.78]  of fields is kept within  [2.x.79]  The individual field is of type  [2.x.80]  and holds arrays with the inverse Jacobians, the JxW values, normal vectors, normal vectors times inverse Jacobians (for  [2.x.81]  quadrature points in real space, and quadrature points on the reference element. We use an auxiliary index array that points to the start of the data for each cell, namely the `data_index_offsets` field for the Jacobians, JxW values, and normal vectors, and `quadrature_point_offsets` for the quadrature points. This offset enables hp-adaptivity with variable lengths of fields similar to what is done for DoFInfo, but it also enables something we call [1.x.15]. In order to reduce the data access, we detect simple geometries of cells where Jacobians are constant within a cell or also across cells, using  [2.x.82] 
*   [2.x.83]   [2.x.84]  Cartesian cells are cells where the Jacobian is diagonal and the same on every quadrature point of the cell. Only a single field needs to be stored per cell. Due to the similarity within the cell, we also check for other cell batches with the same Jacobian for all cells on the current processor. This can further reduce the memory access. Since the JxW values in the general case store the Jacobian times the quadrature weight, but we only want to keep a single field for a Cartesian cell, we misuse the name [1.x.16] in the Cartesian case and only store the determinant of the Jacobian, without the quadrature weight. As a consequence, we need to be careful in  [2.x.85]  and similar for this case as we must still multiply by the weight.  [2.x.86]  Affine cells have constant Jacobian within the whole cell, so only a single field needs to be stored per cell. Due to the similarity within the cell, we also check for other cell batches with the same Jacobian for all cells on the current processor. Since the JxW values in the general case store the Jacobian times the quadrature weight, but we only want to keep a single field for an affine cell, we misuse the name [1.x.17] in the affine case, just as in the Cartesian case, and only store the determinant of the Jacobian, without the quadrature weight. As a consequence, we need to be careful in  [2.x.87]  and similar for this case as we must still multiply by the weight.  [2.x.88]  On faces, we can have the special case that the normal vector is the same in all quadrature points also when the JxW values are different. This is the case for faces which are flat. To reduce the data access, we keep this as a third option of compressed indices in  [2.x.89]  As opposed to the Cartesian and affine case where only a single field is reserved in the arrays, flat faces keep a separate entry for all quadrature points (to keep a single index field `data_index_offsets`), but only access the first one.  [2.x.90]  The general type indices a cell or face where no compression was found. In this case, we also do not look for opportunities to find the same pattern on more than one cell, even though such cases might exist such as for extruded meshes. This search operation, which is based on inserting data into a  [2.x.91]  using a custom floating point comparator `FPArrayComparator`, is efficient enough when a single data field per cell is used. However, it would be pretty expensive if done for all quadrature points of all cells (with many different cases).  [2.x.92] 
*  The implementation of  [2.x.93]  is split into cell and face parts, so the two components can be easily held apart. What makes the code a bit awkward to read is the fact that we need to batch several objects together from the original scalar evaluation done in an FEValues object, that we need to identify data fields that are repetitive, and that we need to define the compression over several cells with a  [2.x.94]  for the Cartesian and affine cases.
*  The data computation part of  [2.x.95]  is parallelized by tasks besides the obvious MPI parallelization. Each processor computes the information on a subrange, before the data is eventually copied into a single combined data field.
*  [1.x.18]
*  The current scheme for face integrals in MatrixFree builds an independent list of tasks for all of the faces, rather than going through the `2*dim` faces of a cell explicitly. This has the advantage that all information on a face is processed only once. Typical DG methods compute numerical fluxes that are conservative, i.e., that look the same from both sides of the face and whatever information leaves one cell must exactly enter the neighbor again. With this scheme, they must only be computed once. Also, this ensures that the geometry information must only be loaded once, too. (A possible disadvantage is that a face-based approach with independent numbering makes thread-based parallelism much more complicated than a cell-based approach where only the information of the current cell is written into and neighbors are only read.)
*  Since faces are independent of cells, they get their own layout of vectorization. It is the nature of faces that whatever is a contiguous batch of cells gets intertwined when seen from a batch of faces (where we only keep faces together that have the same face index within a cell and so on). The setup of the face loop, which is done in the file `face_setup_internal.h`, tries to provide face batches that at least partly resemble the cell patches, to increase the data locality. Along these lines, the face work is also interleaved with cell work in the typical  [2.x.96]  context, i.e., the `cell_range` and `face_range` arguments returned to the function calls are usually pretty short.
*  Since all integrals from both sides are performed at once, the question arises which one of the two processors at subdomain boundaries is assigned a face. The authors of this module have performed extensive experiments and found out that the scheme that is applied for the degree of freedom storage, namely to assign all items with possible overlap to a single processor, is pretty imbalanced with up to 20% difference in the number of faces. For better performance, a balanced scheme is implemented in `face_setup_internal.h` that splits all interfaces between each pair of processors into two chunks, one being done by one processor and one by the other. Even though this increases the number of messages to be sent over MPI, this is worth it because the load gets more balanced. Also, messages are rather big at around 5-50kB when the local problem size is 100,000 DoFs in 3D. At this message size, the latency is typically less than the throughput anyway.
*  Face data is not initialized by default, but must be triggered by the face update flags in  [2.x.97]  namely `mapping_update_flags_inner_faces` or `mapping_update_flags_boundary_faces` set to a value different from `update_default`.
*  [1.x.19]
*  The MatrixFree class supports two types of loops over the entities. The first one, which has been available on the deal.II master branch since 2012, is to only perform cell integrals, using one of the three `cell_loop` functions that takes a function pointer to the cell operation. The second setup, introduced in 2018, is a loop where also face and/or boundary integrals can be performed, called simply `loop`. This takes three function pointers, addressing the cell work, inner face work, and boundary face work, respectively.
*  Besides scheduling the work in an appropriate way, the loop performs two more tasks:  [2.x.98]   [2.x.99]  Data exchange on the `src` and `dst` vector, calling `update_ghost_values()` and  [2.x.100]  respectively. The exchange can be done in an asynchronous fashion overlapping the communication with work on cells that do not need data from remote processors, if the respective flag  [2.x.101]  is set to true (the default).  [2.x.102]  Zero the `dst` vector using the respective flag. The advantage of doing this inside the loop is that the loop knows which entries in the vectors are (first) touched by some of the subranges in the cell and face loops. Thus, it can zero the vector piece by piece to ensure that we do not need to access the vector entries twice (once for zeroing, once for adding contributions). This might seem like a tiny optimization, but indeed the operator evaluation can be so quick that simply zeroing a vector can take around 20% of the operator evaluation time, so it is really worth the effort! Since there is some experimentation to this parameter, the DoFInfo class keeps a static variable  [2.x.103]  where this can be adjusted (if someone thinks that something else would be better, for example because future computers look different than they did in 2018 when this was introduced).  [2.x.104] 
*  Finally, the  [2.x.105]  functions also take an argument to pass the type of data access on face integrals, described by the struct  [2.x.106]  to reduce the amount of data that needs to be exchanged between processors. Unfortunately, there is currently no way of communicating this information, that gets available inside FEFaceEvaluation by the combination of the type of evaluation (values and/or gradients) and the underlying shape functions, to the  [2.x.107]  for avoiding to manually set this kind of information at a second spot.

* 
* [0.x.1]

include/deal.II-translator/A-headers/memory_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This group has some basic classes and namespaces for memory handling. The Subscriptor and SmartPointer classes are used for counted memory handling, i.e. whenever a SmartPointer is set to point to an object, it increases a counter in that object; when the pointer is set to point elsewhere, it decreases it again. This way, one always knows how many users of an object there still are. While this is rarely useful in itself, it is used to generate an exception if an object is destroyed while a pointer somewhere is still pointing to it, as any access through that pointer at a later time would otherwise lead to access of invalid memory regions.
*  In contrast to this, the MemoryConsumption namespace provides functions that can be used to determine the memory consumption of objects. For some simple classes, like the standard library containers, it directly determines how much memory they need (or at least gives an estimate). For deal.II classes, it uses the  [2.x.1]  member function that most classes have.
* 

* 
*  [2.x.2] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/mesh_worker_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  A collection of classes and functions simplifying the coding of loops over all cells and faces. All classes and functions of this module are in the MeshWorker namespace, which also contains documentation on the usage.
* 

* 
*  [2.x.1] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/mg_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  Classes that have to do with multigrid algorithms.
*  The main class with implementation of the multigrid scheme is Multigrid with its function  [2.x.1]  It uses the following abstract classes in order to perform the multigrid cycle:
*   [2.x.2]   [2.x.3]  MGMatrixBase contains the level matrices with a fairly general implementation in  [2.x.4]   [2.x.5]  MGCoarseGridBase is the solver on the coarsest level.  [2.x.6]  MGSmootherBase performs smoothing on each level.  [2.x.7]  MGTransferBase organizes the transfer between levels.  [2.x.8] 
*  Additionally, there is a class PreconditionMG, which is a wrapper around Multigrid with the standard interface of deal.II  [2.x.9]  Preconditioners. PreconditionMG also uses the classes inheriting from MGTransferBase, for instance MGTransferPrebuilt, where it uses  [2.x.10]  and  [2.x.11]  which transfer between the global vector and the level vectors.
*  Finally, we have several auxiliary classes, namely MGLevelObject, which stores an object on each level*
*  See the  [2.x.12] ,  [2.x.13] b, and  [2.x.14]  example programs on how to use this functionality.
*  [1.x.0]
*  Using multigrid methods on adaptively refined meshes involves more infrastructure than with regular refinement. First, in order to keep the complexity optimal, we need to decide how to do the smoothing on each level. And to this end, we have to define what a level is in the sense of multilevel decomposition.
*  First, we define that a level in the multigrid sense is constituted by all cells of a certain level in the mesh hierarchy. Thus, smoothing on a certain level is restricted to the subdomain which consists of cells of this level or finer. This is usually referred to as local smoothing. The advantage of this definition is, that level matrices for the multigrid scheme can be assembled easily by traversing to all cells of a certain level, and that these level matrices do not contain hanging nodes.
*  The disadvantage of this decomposition is, that we need additional matrices to handle the issues that arise at refinement edges. Furthermore, the treatment is different, depending on whether the method is continuous (thus having degrees of freedom on the refinement edge) or discontinuous (employs flux matrices at the refinement edge). While these matrices are small, we have to assemble them and notify the multigrid method of them.

* 
* [0.x.1]*
 This namespace contains the reimplementation of multilevel support after we know what is needed in the context of local refinement and block systems.
* 

* 
*  [2.x.15] 

* 
* [0.x.2]

include/deal.II-translator/A-headers/multithreading_0.txt
[0.x.0]*


* 
*  [2.x.0] 

* 
*  [2.x.1] 
* 

* 
*  [2.x.2]  machines. See the detailed documentation and  [2.x.3]  "Table of Contents" below the lengthy list of members of this module.
*   [2.x.4] 
*  On machines with more than one processor (or multicore processors), it is often profitable to run several parts of the computations in %parallel. For example, one could have several threads running in %parallel, each of which assembles the cell matrices of a subset of the triangulation and then writes them into the global matrix. Since assembling matrices is often an expensive operation, this frequently leads to significant savings in compute time on multiprocessor machines.
*  deal.II supports operations running in %parallel on shared-memory (SMP) machines through the functions and classes in the Threads namespace. The MultithreadInfo class allows to query certain properties of the system, such as the number of CPUs. These facilities for %parallel computing are described in the following. The  [2.x.5] ,  [2.x.6] ,  [2.x.7] ,  [2.x.8] ,  [2.x.9]  and  [2.x.10]  tutorial programs also show their use in practice, with the ones starting with  [2.x.11]  using a more modern style of doing things in which essentially we describe [1.x.0] can be done in %parallel, while the older tutorial programs describe [1.x.1] things have to be done in %parallel.
*  On the other hand, programs running on distributed memory machines (i.e. clusters) need a different programming model built on top of MPI and PETSc or Trilinos. This is described in the  [2.x.12] ,  [2.x.13]  and  [2.x.14]  example programs.
*   [2.x.15]  MTToC  [2.x.16] 
* 

*   [2.x.17]  MTTasks [1.x.3]
*  The traditional view of parallelism on shared memory machines has been to decompose a program into [1.x.4], i.e. running different parts of the program in %parallel [1.x.5] (if there are more threads than processor cores on your machine, the operating system will run each thread round-robin for a brief amount of time before switching execution to another thread, thereby simulating that threads run concurrently). deal.II's facilities for threads are described below (see  [2.x.18]  "Thread-based parallelism"), but we would first like to discuss an abstraction that is often more suitable than threads: [1.x.6].
*  Tasks are essentially the individual parts of a program. Some of them are independent, whereas others depend on previous tasks to be completed first. By way of example, consider the typical layout of a part of the  [2.x.19]  function that most of the tutorial programs have:

* 
* [1.x.7]
* 
*  Here, each of the operations require a significant amount of computations. But note that not all of them depend on each other: clearly we can not run statements 2-4 before 1, and 4 needs to wait for the completion of statements 2 and 3. But statements 2 and 3 are independent: they could be run in any order, or in %parallel. In essence, we have identified four [1.x.8], some of which are dependent on each other, whereas others are independent. In the current example, tasks are identified with individual C++ statements, but often they more generally coincide with entire code blocks.
*  The point here is this: If we wanted to use threads to exploit the independence of tasks 2 and 3, we would start two threads and run each of tasks 2 and 3 on its own thread; we would then wait for the two threads to finish (an operation called "joining a thread") and go on with statement 4. Code to achieve this would look like this (the actual syntax is explained in more detail below):

* 
* [1.x.9]
* 
*  But what if your computer has only one processor core, or if we have two but there is already a different part of the program running in %parallel to the code above? In that case, the code above would still start new threads, but the program is not going to run faster since no additional compute resources are available; rather, the program will run slower since threads have to be created and destroyed, and the operating system has to schedule threads to oversubscribed compute resources.
*  A better scheme would identify independent tasks and then hand them off to a scheduler that maps tasks to available compute resources. This way, the program could, for example, start one thread per processor core and then let threads work on tasks. Tasks would run to completion, rather than concurrently, avoiding the overhead of interrupting threads to run a different thread. In this model, if two processor cores are available, tasks 2 and 3 above would run in %parallel; if only one is available, the scheduler would first completely execute task 2 before doing task 3, or the other way around. This model is able to execute much more efficiently in particular if a large number of tasks is available for execution, see for example the discussion below in section  [2.x.20]  "Abstractions for tasks: Work streams". In essence, tasks are a high-level description of what needs to be done, whereas threads are a low-level way of implementing how these tasks can be completed. As in many other instances, being able to use a high-level description allows to find efficient low-level implementations; in this vein, it often pays off to use tasks, rather than threads, in a program.
*  deal.II does not implement scheduling tasks to threads itself. For this, we use the [1.x.10] for which we provide simple wrappers. TBB abstracts the details of how to start or stop threads, start tasks on individual threads, etc, and provides interfaces that are portable across many different systems.
* 

* 
*   [2.x.21]  MTUsing [1.x.11]
*  Ideally, the syntax to start tasks (and similarly for threads, for that matter), would be something like this for the example above:

* 
* [1.x.12]
*  In other words, we would like to indicate the fact that the function call should be run on a separate task by simply prefixing the call with a keyword (such as  [2.x.22]  here, with a similar keyword  [2.x.23]  for threads). Prefixing a call would return a handle for the task that we can use to wait for the task's completion and that we may use to query the return value of the function called (unless it is void, as it is here).
*  Since C++ does not support the creation of new keywords, we have to be a bit more creative. The way chosen is to introduce a function  [2.x.24]  that takes as arguments the function to call as well as the arguments to the call. The  [2.x.25]  function is overloaded to accommodate starting tasks with functions that take no, one, two, and up to 9 arguments. In deal.II, these functions live in the Threads namespace. Consequently, the actual code for what we try to do above looks like this:

* 
* [1.x.13]
* 
*  Similarly, if we want to call a member function on a different task, we can do so by specifying the object on which to call the function as first argument after the function pointer:

* 
* [1.x.14]
*  Here, note first how we pass the object  [2.x.26]  (i.e. the  [2.x.27]  will see) as if it was the first argument to the function. Secondly, note how we can acquire the value returned by the function on the separate task by calling  [2.x.28]  This function implies waiting for the completion of the task, i.e. the last line is completely equivalent to

* 
* [1.x.15]
* 
*  Note also that it is entirely valid if  [2.x.29]  wants to start tasks of its own:

* 
* [1.x.16]
*  Here, we let  [2.x.30]  compute its return value as  [2.x.31] . If sufficient CPU resources are available, then the two parts of the addition as well as the other things in  [2.x.32]  will all run in %parallel. If not, then we will eventually block at one of the places where the return value is needed, thereby freeing up the CPU resources necessary to run all those spawned tasks to completion.
* 

*  In many cases, such as the introductory example of the  [2.x.33]  function outlined above, one can identify several independent jobs that can be run as tasks, but will have to wait for all of them to finish at one point. One can do so by storing the returned object from all the  [2.x.34]  calls, and calling  [2.x.35]  on each one of them. A simpler way to do this is to put all of these task objects into a  [2.x.36]  object and waiting for all of them at once. The code would then look like this:

* 
* [1.x.17]
* 
* 

*   [2.x.37]  MTHow [1.x.18]
*  The exact details of how tasks are scheduled to run are %internal to the Threading Building Blocks (TBB) library that deal.II uses for tasks. The documentation of TBB gives a detailed description of how tasks are scheduled to threads but is rather quiet on how many threads are actually used. However, a reasonable guess is probably to assume that TBB creates as many threads as there are processor cores on your system. This way, it is able to fully utilize the entire system, without having too many threads that the operating system will then have to interrupt regularly so that other threads can run on the available processor cores.
*  The point then is that the TBB scheduler takes tasks and lets threads execute them. %Threads execute tasks completely, i.e. the TBB scheduler does not interrupt a task half way through to make some halfway progress with another task. This makes sure that caches are always hot, for example, and avoids the overhead of preemptive interrupts.
*  The downside is that the CPU cores are only fully utilized if the threads are actually doing something, and that means that (i) there must be enough tasks available, and (ii) these tasks are actually doing something. Note that both conditions must be met; in particular, this means that CPU cores are underutilized if we have identified a sufficient number of tasks but if some of them twiddle thumbs, for example because a task is writing data to disk (a process where the CPU frequently has to wait for the disk to complete a transaction) or is waiting for input. Other cases are where tasks block on other external events, for example synchronising with other tasks or threads through a mutex. In such cases, the scheduler would let a task run on a thread, but doesn't notice that that thread doesn't fully utilize the CPU core.
*  In cases like these, it [1.x.19] make sense to create a new thread (see  [2.x.38]  "Thread-based parallelism" below) that the operating system can put on hold while they are waiting for something external, and let a different thread (for example one running a task scheduled by TBB) use the CPU at the same time.
* 

*   [2.x.39]  MTSimpleLoops [1.x.20]
*  Some loops execute bodies on data that is completely independent and that can therefore be executed in %parallel. Rather than a priori split the loop into a fixed number of chunks and executing them on tasks or threads, the TBB library uses the following concept: the range over which the loop iterates is split into a certain number of sub-ranges (for example two or three times as many as there are CPU cores) and are equally distributed among threads; threads then execute sub-ranges and, if they are done with their work, steal entire or parts of sub-ranges from other threads to keep busy. This way, work is load-balanced even if not every loop iteration takes equally much work, or if some of the CPU cores fall behind because the operating system interrupted them for some other work.
*  The TBB library primitives for this are a bit clumsy so deal.II has wrapper routines for the most frequently used operations. The simplest one is akin to what the  [2.x.40]  does: it takes one or more ranges of input operators, one output iterator, and a function object. A typical implementation of  [2.x.41]  would look like this:

* 
* [1.x.21]
* 
*  In many cases,  [2.x.42]  has no state, and so we can split this loop into several sub-ranges as explained above. Consequently, deal.II has a set of functions  [2.x.43]  that look like the one above but that do their work in %parallel (there are several versions with one, two, and more input iterators for function objects that take one, two, or more arguments). The only difference in calling these functions is that they take an additional last argument that denotes the minimum size of sub-ranges of  [2.x.44] ; it should be big enough so that we don't spend more time on scheduling sub-ranges to processors but small enough that processors can be efficiently load balanced. A rule of thumb appears to be that a sub-range is too small if it takes less than 2000 instructions to execute it.
*  An example of how to use these functions are vector operations like the addition in  [2.x.45]  where all three objects are of type Vector<Number>:

* 
* [1.x.22]
* 
*  In this example, we used a [1.x.23] to construct, on the fly, a function object that takes two arguments and returns the sum of the two. This is exactly what we needed when we want to add the individual elements of vectors  [2.x.46]  and  [2.x.47]  and write the sum of the two into the elements of  [2.x.48] . The function object that we get here is completely known to the compiler and when it expands the loop that results from  [2.x.49]  will be as if we had written the loop in its obvious form:

* 
* [1.x.24]
* 
*  Note also that we have made sure that no CPU ever gets a chunk of the whole loop that is smaller than 1000 iterations (unless the whole range is smaller).
* 

*   [2.x.50]  MTComplexLoops [1.x.25]
*  The scheme shown in the previous section is effective if the operation done in each iteration is such that it does not require significant setup costs and can be inlined by the compiler. [1.x.26] are exactly of this kind, thereby eliminating the overhead of calling an external function. However, there are cases where it is inefficient to call some object or function within each iteration.
*  An example for this case is sparse matrix-vector multiplication. If you know how data is stored in compressed row format like in the SparseMatrix class, then a matrix-vector product function looks like this:

* 
* [1.x.27]
*  Inside the for loop, we compute the dot product of a single row of the matrix with the right hand side vector  [2.x.51]  and write it into the corresponding element of the  [2.x.52]  vector. The code is made more efficient by utilizing that the elements of the [1.x.28] row follow the ones of the current row [1.x.29], i.e. at the beginning of the loop body we do not have to re-set the pointers that point to the values and column %numbers of each row.
*  Using the  [2.x.53]  function above, we could in principle write this code as follows:

* 
* [1.x.30]
*  Note how we use  [2.x.54]  to [1.x.31] certain arguments to the  [2.x.55]  function, leaving one argument open and thus allowing the  [2.x.56]  function to consider the passed function argument as unary. Also note that we need to make the source and destination vectors as (const) references to prevent  [2.x.57]  from passing them by value (implying a copy for  [2.x.58]  and writing the result into a temporary copy of  [2.x.59] , neither of which is what we desired). Finally, notice the grainsize of a minimum of 200 rows of a matrix that should be processed by an individual CPU core.
*  The point is that while this is correct, it is not efficient: we have to set up the  [2.x.60]  variables in each iteration of the loop. Furthermore, since now the function object to be called on each row is not a simple [1.x.32] any more, there is an implicit function call including argument passing in each iteration of the loop.
*  A more efficient way is to let TBB split the original range into sub-ranges, and then call a target function not on each individual element of the loop, but on the entire range. This is facilitated by the  [2.x.61]  function:

* 
* [1.x.33]
*  Here, we call the  [2.x.62]  function on sub-ranges of at least 200 elements each, so that the initial setup cost can amortize.
*  A related operation is when the loops over elements each produce a result that must then be accumulated (other reduction operations than addition of numbers would work as well). An example is to form the matrix norm  [2.x.63]  (it really is only a norm if  [2.x.64]  is positive definite, but let's assume for a moment that it is). A sequential implementation would look like this for sparse matrices:

* 
* [1.x.34]
* 
*  It would be nice if we could split this operation over several sub-ranges of rows, each of which compute their part of the square of the norm, add results together from the various sub-ranges, and then take the square root of the result. This is what the  [2.x.65]  function does (note that you have to specify the result type as a template argument and that, as usual, the minimum number of elements of the outer loop that can be scheduled on a single CPU core is given as the last argument):

* 
* [1.x.35]
* 
* 

*   [2.x.66]  MTWorkStream [1.x.36]
*  In the examples shown in the introduction we had identified a number of functions that can be run as independent tasks. Ideally, this number of tasks is larger than the number of CPU cores (to keep them busy) but is also not exceedingly huge (so as not to inundate the scheduler with millions of tasks that will then have to be distributed to 2 or 4 cores, for example). There are, however, cases where we have many thousands or even millions of relatively independent jobs: for example, assembling local contributions to the global linear system on each cell of a mesh; evaluating an error estimator on each cell; or postprocessing on each cell computed data for output fall into this class. These cases can be treated using a software design pattern we call WorkStream. In the following, we will walk through the rationale for this pattern and its implementation; more details as well as examples for the speedup that can be achieved with it are given in the  [2.x.67]  "WorkStream paper".
*  Code like this could then be written like this:

* 
* [1.x.37]
*  On a big mesh, with maybe a million cells, this would create a massive number of tasks; while it would keep all CPU cores busy for a while, the overhead of first creating so many tasks, scheduling them, and then waiting for them would probably not lead to efficient code. A better strategy would be if the scheduler could somehow indicate that it has available resources, at which point we would feed it another newly created task, and we would do so until we run out of tasks and the ones that were created have been worked on.
*  This is essentially what the  [2.x.68]  function does: You give it an iterator range from which it can draw objects to work on (in the above case it is the interval given by  [2.x.69]  to  [2.x.70] ), and a function that would do the work on each item (the function  [2.x.71] ) together with an object if it is a member function.
*  In the following, let us lay out a rationale for why the functions in the WorkStream namespace are implemented the way they are. More information on their implementation can be found in the  [2.x.72]  "WorkStream paper". To see the WorkStream class used in practice on tasks like the ones outlined above, take a look at the  [2.x.73] ,  [2.x.74] ,  [2.x.75] ,  [2.x.76] ,  [2.x.77]  or  [2.x.78]  tutorial programs.
*  To begin with, given the brief description above, the way the  [2.x.79]  function could then be written is like this (note that this is not quite the correct syntax, as will be described below):

* 
* [1.x.38]
* 
*  There are at least three problems with this, however: [2.x.80]  [2.x.81] First, let us take a look at how the  [2.x.82]    function likely looks:

* 
* [1.x.39]
* 
*    The problem here is that several tasks, each running    [2.x.83] , could potentially try   to write into the object  [2.x.84]  [1.x.40]. This could be avoided by explicit synchronisation   using a  [2.x.85]  for example, and would look like this:

* 
* [1.x.41]
* 
*    By making the mutex a static variable, it exists only once globally   (i.e. once for all tasks that may be running in %parallel) and only one of   the tasks can enter the region protected by the acquire/release calls on   the mutex. As an aside, a better way to write this code would be like   this, ensuring that the mutex is released even in case an exception is   thrown, and without the need to remember to write the call to    [2.x.86] 

* 
* [1.x.42]
*    Here, the mutex remains locked from the time the ScopedLock is created to   where it is destroyed, at the end of the code block.
*    Note that although we now avoid the race condition that multiple threads   could be writing to the same object, this code is not very efficient:   mutexes are expensive on multicore machines, and we also block threads   some of the time which is inefficient with tasks as explained above in   the section on    [2.x.87]  "How scheduling tasks works and when task-based programming is not efficient".
*  [2.x.88] A second correctness problem is that even if we do lock the global matrix   and right hand side objects using a mutex, we do so in a more or less   random order: while tasks are created in the order in which we traverse   cells normally, there is no guarantee that by the time we get to the   point where we want to copy the local into the global contributions the   order is still as if we computed things sequentially. In other words, it   may happen that we add the contributions of cell 1 before those of cell   0. That may seem harmless because addition is commutative and   associative, but in fact it   is not if done in floating point arithmetic:  [2.x.89] 
* 
*  -  take   for example  [2.x.90]  (because  [2.x.91]  in floating   point arithmetic, using double precision).
*    As a consequence, the exact values that end up in the global matrix and   right hand side will be close but may differ by amounts close to   round-off depending on the order in which tasks happened to finish their   job. That's not a desirable outcome, since results will not be   reproducible this way.
*    As a consequence, the way the WorkStream class is designed is to use two   functions: the  [2.x.92]  computes the   local contributions and stores them somewhere (we'll get to that next), and   a second function, say  [2.x.93] , that   copies the results computed on each cell into the global objects. The   trick implemented in the WorkStream class is that (i) the    [2.x.94]  never runs more than once in   %parallel, so we do not need to synchronise execution through a mutex, and   (ii) it runs in exactly the same order on cells as they appear in the   iterator range, i.e. we add elements into the global matrix the same way   [1.x.43].
*    We now only have to discuss how the    [2.x.95]  communicates to    [2.x.96]  what it has computed. The way   this is done is to use an object that holds all temporary data:

* 
* [1.x.44]
* 
*    The way this works is that we create a sample  [2.x.97]    object that the work stream object will replicate once per task that runs   in %parallel. For each task, this object will be passed first to one of   possibly several instances of  [2.x.98]    running in %parallel which fills it with the data obtained on a single   cell, and then to a sequentially running    [2.x.99]  that copies data into the   global object. In practice, of course, we will not generate millions of    [2.x.100]  objects if we have millions of cells; rather,   we recycle these objects after they have been used by    [2.x.101]  and feed them back into   another instance of  [2.x.102] ; this   means that the number of such objects we actually do create is a small   multiple of the number of threads the scheduler uses, which is typically   about as many as there are CPU cores on a system.
*   [2.x.103] The last issue that is worth addressing is that the way we wrote the    [2.x.104]  function above, we create and   destroy an FEValues object every time the function is called, i.e. once   for each cell in the triangulation. That's an immensely expensive   operation because the FEValues class tries to do a lot of work in its   constructor in an attempt to reduce the number of operations we have to   do on each cell (i.e. it increases the constant in the  [2.x.105]    effort to initialize such an object in order to reduce the constant in   the  [2.x.106]  operations to call  [2.x.107]  on the  [2.x.108]  cells of   a triangulation). Creating and destroying an FEValues object on each cell   invalidates this effort.
*    The way to avoid this is to put the FEValues object into a second   structure that will hold scratch data, and initialize it in the   constructor:

* 
* [1.x.45]
*  and then use this FEValues object in the assemble function:

* 
* [1.x.46]
*    Just as for the  [2.x.109]  structure, we will create a   sample  [2.x.110]  object and pass it to the work stream   object, which will replicate it as many times as necessary. For this   to work  [2.x.111]  structures need to copyable. Since FEValues   objects are rather complex and cannot be copied implicitly, we provided   our own copy constructor for the  [2.x.112]  structure.
*    The same approach, putting things into the  [2.x.113]    data structure, should be used for everything that is expensive to   construct. This holds, in particular, for everything that needs to   allocate memory upon construction; for example, if the values of a   function need to be evaluated at quadrature points, then this is   expensive:

* 
* [1.x.47]
*  whereas this is a much cheaper way:

* 
* [1.x.48]
* 
*   [2.x.114] 
*  As a final point: What if, for some reason, my assembler and copier function do not match the above signature with three and one argument, respectively? That's not a problem either. The WorkStream namespace offers two versions of the  [2.x.115]  function: one that takes an object and the addresses of two member functions, and one that simply takes two function objects that can be called with three and one argument, respectively. So, in other words, the following two calls are exactly identical:

* 
* [1.x.49]
*  Note how  [2.x.116]  produces a function object that takes three arguments by binding the member function to the  [2.x.117]  object.  [2.x.118]  are placeholders for the first, second and third argument that can be specified later on. In other words, for example if  [2.x.119]  is the result of the first call to  [2.x.120] , then the call <code>p(cell, scratch_data, per_task_data)</code> will result in executing  [2.x.121] , i.e.  [2.x.122]  has bound the object to the function pointer but left the three arguments open for later.
*  Similarly, let us assume that  [2.x.123]  has the following signature in the solver of a nonlinear, time-dependent problem:

* 
* [1.x.50]
*  Because WorkStream expects to be able to call the worker function with just three arguments, the first of which is the iterator and the second and third the ScratchData and PerTaskData objects, we need to pass the following to it:

* 
* [1.x.51]
*  Here, we bind the object, the linearization point argument, and the current time argument to the function before we hand it off to  [2.x.124]   [2.x.125]  will then simply call the function with the cell and scratch and per task objects which will be filled in at the positions indicated by  [2.x.126]  and  [2.x.127] .
*  There are refinements to the  [2.x.128]  function shown above. For example, one may realize that the basic idea above can only scale if the copy-local-to-global function is much quicker than the local assembly function because the former has to run sequentially. This limitation can only be improved upon by scheduling more work in parallel. This leads to the notion of coloring the graph of cells (or, more generally, iterators) we work on by recording which write operations conflict with each other. Consequently, there is a third version of  [2.x.129]  that doesn't just take a range of iterators, but instead a vector of vectors consisting of elements that can be worked on at the same time. This concept is explained in great detail in the  [2.x.130]  "WorkStream paper", along with performance evaluations for common examples.
* 

*   [2.x.131]  MTTaskSynchronization [1.x.52]
*  Tasks are powerful but they do have their limitation: to make things efficient, the task scheduler never interrupts tasks by itself. With the exception of the situation where one calls the  [2.x.132]  function to wait for another task to finish, the task scheduler always runs a task to completion. The downside is that the scheduler does not see if a task is actually idling, for example if it waits for something else to happen (file IO to finish, input from the keyboard, etc). In cases like this, the task scheduler could in principle run a different task, but since it doesn't know what tasks are doing it doesn't. Functions that do wait for external events to happen are therefore not good candidates for tasks and should use threads (see below).
*  However, there are cases where tasks are not only a bad abstraction for a job but can actually not be used: As a matter of principle, tasks can not synchronize with other tasks through the use of a mutex or a condition variable (see the  [2.x.133]  and  [2.x.134]  classes). The reason is that if task A needs to wait for task B to finish something, then this is only going to work if there is a guarantee that task B will eventually be able to run and finish the task. Now imagine that you have 2 processors, and tasks A1 and A2 are currently running; let's assume that they have queued tasks B1 and B2, and are now waiting with a mutex for these queued tasks to finish (part of) their work. Since the machine has only two processors, the task scheduler will only start B1 or B2 once either A1 or A2 are done
* 
*  -  but this isn't happening since they are waiting using operating system resources (a mutex) rather than task scheduler resources. The result is a deadlock.
*  The bottom line is that tasks can not use mutexes or condition variables to synchronize with other tasks. If communication between tasks is necessary, you need to use threads because the operating system makes sure that all threads eventually get to run, independent of the total number of threads. Note however that the same is not true if you only use a  [2.x.135]  on each task separately to protect access to a variable that the tasks may write to: this use of mutexes is ok; tasks may simply not want to wait for another task to do something.
* 

*   [2.x.136]  MTThreads [1.x.53]
*  Even though tasks are a higher-level way to describe things, there are cases that are poorly suited to a task (for a discussion of some of these cases see  [2.x.137]  "How scheduling tasks works and when task-based programming is not efficient" above). Generally, jobs that are not able to fully utilize the CPU are bad fits for tasks and good fits for threads.
*  In a case like this, you can resort to explicitly start threads, rather than tasks, using pretty much the same syntax as above. For example, if you had a function in your application that generates graphical output and then estimates the error to refine the mesh for the next iteration of an adaptive mesh scheme, it could look like this:

* 
* [1.x.54]
* 
*  Here,  [2.x.138]  starts the given function that writes to the output file on a new thread that can run in %parallel to everything else: In %parallel to the  [2.x.139]  function, the  [2.x.140]  function will run on a separate thread. This execution is independent of the scheduler that takes care of tasks, but that is not a problem because writing lots of data to a file is not something that will keep a CPU very busy.
*  Creating threads works pretty much the same way as tasks, i.e. you can wait for the termination of a thread using  [2.x.141]  query the return value of a finished thread using  [2.x.142]  and you can group threads into a  [2.x.143]  object and wait for all of them to finish.
* 

*   [2.x.144]  MTTaskThreads [1.x.55] As mentioned earlier, deal.II does not implement scheduling tasks to threads or even starting threads itself. The TBB library does a good job at deciding how many threads to use and they do not recommend setting the number of threads explicitly. However, on large symmetric multiprocessing (SMP) machines, especially ones with a resource/job manager or on systems on which access to some parts of the memory is possible but very expensive for processors far away (e.g. very large NUMA SMP machines), it may be necessary to explicitly set the number of threads to prevent the TBB from using too many CPUs. Another use case is if you run multiple MPI jobs on a single machine and each job should only use a subset of the available processor cores.
*  Setting the number of threads explicitly is done by calling  [2.x.145]  before any other calls to functions that may create threads. In practice, it should be one of the first functions you call in  [2.x.146] .
*  If you run your program with MPI, then you can use the optional third argument to the constructor of the MPI_InitFinalize class to achieve the same goal.
* 

* 
*  [2.x.147]  A small number of places inside deal.II also uses thread-based parallelism explicitly, for example for running background tasks that have to wait for input or output to happen and consequently do not consume much CPU time. Such threads do not run under the control of the TBB task scheduler and, therefore, are not affected by the procedure above. Under some circumstances, deal.II also calls the BLAS library which may sometimes also start threads of its own. You will have to consult the documentation of your BLAS installation to determine how to set the number of threads for these operations.

* 
* [0.x.1]

include/deal.II-translator/A-headers/namespace_dealii_0.txt
[0.x.0]*
 This is the namespace in which everything in deal.II is. To avoid prefixing every class and function call with the namespace name, the  [2.x.0]  therefore have a  [2.x.1]  at the top of the code.
*  Throughout the documentation, the namespace prefix is suppressed for brevity.

* 
* [0.x.1]

include/deal.II-translator/A-headers/numerical_algorithms_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This module groups a diverse set of classes that generally implement some sort of numerical algorithm on top all the basic triangulation, DoFHandler, and finite element classes in the library. They are generally unconnected to each other.
*  Some of the classes, like DerivativeApproximation, KellyErrorEstimator and SolutionTransfer, act on solutions already obtained, and compute derived quantities in the first two cases, or help transferring a set of vectors from one mesh to another.
*  The namespaces MatrixCreator, MatrixTools, and VectorTools provide an assortment of services, such as creating a Laplace matrix, projecting or interpolating a function onto the present finite element space, etc.  The difference to the functions in the DoFTools and FETools functions is that they work on vectors (i.e. members of a finite element function space on a given triangulation) or help in the creation of it. On the other hand, the DoFTools functions only act on a given DoFHandler object without reference to a data vector, and the FETools objects generally work with finite element classes but again without any associated data vectors.

* 
* [0.x.1]

include/deal.II-translator/A-headers/parallel_0.txt
[0.x.0]*


* 
*  [2.x.0] 
* 

* 
*  [2.x.1] 
*  This module contains information on %parallel computing. It is subdivided into parts on  [2.x.2]  and on  [2.x.3] .

* 
* [0.x.1]*
 A namespace in which we define classes and algorithms that deal with running in %parallel on shared memory machines when deal.II is configured to use multiple threads (see  [2.x.4] ), as well as running things in %parallel on %distributed memory machines (see  [2.x.5] ).
* 

* 
*  [2.x.6]   [2.x.7]  Wolfgang Bangerth, 2008, 2009

* 
* [0.x.2]

include/deal.II-translator/A-headers/petsc_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  The classes in this module are wrappers around functionality provided by the PETSc library. They provide a modern object-oriented interface that is compatible with the interfaces of the other linear algebra classes in deal.II. All classes and functions in this group reside in a namespace  [2.x.1] 
*  These classes are only available if a PETSc installation was detected during configuration of deal.II. Refer to the README file for more details about this.
*   [2.x.2]  Wolfgang Bangerth, 2004
* 

* 
*  [2.x.3] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/physics_0.txt
[0.x.0]*


* 
*  [2.x.0] 
* 

* 
*  [2.x.1]  classes that relate to continuum physics, physical fields and materials.

* 
* [0.x.1]*
 A collection of namespaces and utilities to assist in the definition, construction and manipulation of data related to physical fields and materials.

* 
* [0.x.2]*
   Notations that reduce the order of tensors, effectively storing them in some   sort of consistent compressed storage pattern. An example is storing the   6 independent components of  [2.x.2]  symmetric tensors of rank 2 as a   vector with 6 components, and then representing the 36 independent elements   of symmetric  [2.x.3]  tensors of rank 4 (which when   applied to a symmetric rank-2 tensor yields another symmetric rank-2 tensor)   as a  [2.x.4]  matrix.     Although this method of representing tensors is most regularly associated with   the efficient storage of the fourth-order elasticity tensor, with its   generalization it has wider applicability. This representation is also common   in the physics, material science and FEM literature.     There are several variations of tensor notation, each a slightly different   structure. The primary difference between the various forms of tensor notation   is the weighting prescribed to the various elements of the compressed tensors.   This [1.x.0] has   some further general insights on this topic.    
*  [2.x.5]       [2.x.6]  Jean-Paul Pelteret, 2017  
* [0.x.3]*
  A collection of operations to assist in the transformation of tensor  quantities from the reference to spatial configuration, and vice versa.  These types of transformation are typically used to re-express quantities  measured or computed in one configuration in terms of a second configuration.
*   [1.x.1]
*   We will use the same notation for the coordinates  [2.x.7] ,  transformations  [2.x.8] , differential operator  [2.x.9]  and deformation  gradient  [2.x.10]  as discussed for namespace  [2.x.11] 
*   As a further point on notation, we will follow Holzapfel (2007) and denote  the push forward transformation as  [2.x.12]  and  the pull back transformation as  [2.x.13] .  We will also use the annotation  [2.x.14]  to indicate  that a tensor  [2.x.15]  is a contravariant tensor,  and  [2.x.16]  that it is covariant. In other  words, these indices do not actually change the tensor, they just indicate  the [1.x.2] of object a particular tensor is.
*  
*  [2.x.17]  For these transformations, unless otherwise stated, we will strictly  assume that all indices of the transformed tensors derive from one coordinate  system; that is to say that they are not multi-point tensors (such as the  Piola stress in elasticity).
*  
*  [2.x.18] 
*    [2.x.19]  Jean-Paul Pelteret, Andrew McBride, 2016 
* [0.x.4]*
   This namespace provides a collection of definitions that   conform to standard notation used in (nonlinear) elasticity.     [1.x.3]     References for this notation include:  
* [1.x.4]
*      For convenience we will predefine some commonly referenced tensors and   operations.   Considering the position vector  [2.x.20]  in the referential (material)   configuration, points  [2.x.21]  are transformed to points  [2.x.22]    in the current (spatial) configuration through the nonlinear map   [1.x.5]   where the  [2.x.23]  represents the displacement vector.   From this we can compute the deformation gradient tensor as   [1.x.6]   wherein the differential operator  [2.x.24]  is defined as    [2.x.25]  and  [2.x.26]  is the identity   tensor.     Finally, two common tensor operators are represented by  [2.x.27]  and  [2.x.28]    operators. These respectively represent a single and double contraction over   the inner tensor indices.   Vectors and second-order tensors are highlighted by bold font, while   fourth-order tensors are denoted by calliagraphic font.     One can think of fourth-order tensors as linear operators mapping second-order   tensors (matrices) onto themselves in much the same way as matrices map   vectors onto vectors.   To provide some context to the implemented class members and functions,   consider the following fundamental operations performed on tensors with special   properties:     If we represent a general second-order tensor as  [2.x.29] , then the general   fourth-order unit tensors  [2.x.30]  and  [2.x.31]  are   defined by   [1.x.7]   or, in indicial notation,   [1.x.8]   with the Kronecker deltas taking their common definition.   Note that  [2.x.32] .     We then define the symmetric and skew-symmetric fourth-order unit tensors by   [1.x.9]   such that   [1.x.10]   The fourth-order symmetric tensor returned by identity_tensor() is    [2.x.33] .      [2.x.34]  Jean-Paul Pelteret, Andrew McBride, 2016    
*  [2.x.35]   
* [0.x.5]

include/deal.II-translator/A-headers/polynomials_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This module groups classes that define certain families of polynomial functions. In particular, this includes Lagrangian interpolation polynomials for equidistant support points and their tensor products in higher dimensions, but also more exotic ones like Brezzi-Douglas-Marini or Raviart-Thomas spaces.

* 
* [0.x.1]

include/deal.II-translator/A-headers/preconditioners_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  [1.x.0]
*  Preconditioners are used to accelerate the iterative solution of linear systems. Typical preconditioners are Jacobi, Gauss-Seidel, or SSOR, but the library also supports more complex ones such as Vanka or incomplete LU decompositions (ILU). In addition, sparse direct solvers can be used as preconditioners when available.
*  Broadly speaking, preconditioners are operators, which are multiplied with a matrix to improve conditioning. The idea is, that the preconditioned system [1.x.1] is much easier to solve than the original system [1.x.2]. What this means exactly depends on the structure of the matrix and cannot be discussed here in generality. For symmetric, positive definite matrices [1.x.3] and [1.x.4], it means that the spectral condition number (the quotient of greatest and smallest eigenvalue) of [1.x.5] is much smaller than the one of [1.x.6].
*  At hand of the simplest example, Richardson iteration, implemented in SolverRichardson, the preconditioned iteration looks like [1.x.7] Accordingly, preconditioning amounts to applying a linear operator to the residual, and consequently, the action of the preconditioner [1.x.8] is implemented as <tt>vmult()</tt>. Templates in deal.II that require a preconditioner indicate the requirement with  [2.x.1]  "the PreconditionerType concept". In practice, one can usually treat any matrix-like object which defines  [2.x.2]  as a preconditioner. All preconditioner classes in this module implement this interface.
*  When used in Krylov space methods, it is up to the method, whether it simply replaces multiplications with [1.x.9] by those with [1.x.10] (for instance SolverBicgstab), or does more sophisticated things. SolverCG for instance uses [1.x.11] to define an inner product, which is the reason why it requires a symmetric, positive definite operator [1.x.12].
*  [1.x.13]
*  Many preconditioners rely on an additive splitting [1.x.14] into two matrices. In this case, the iteration step of the Richardson method above can be simplified to [1.x.15] thus avoiding multiplication with [1.x.16] completely. We call operators mapping the previous iterate [1.x.17] to the next iterate in this way relaxation operators. Their generic interface is given by  [2.x.3]  "the RelaxationType concept". The classes with names starting with <tt>Relaxation</tt> in this module implement this interface, as well as the preconditioners PreconditionJacobi, PreconditionSOR, PreconditionBlockJacobi, PreconditionBlockSOR, and PreconditionBlockSSOR.
*  [1.x.18]
*  In this section, we discuss the interface preconditioners usually have to provide to work inside the deal.II library.
*  [1.x.19]
*  In order to be able to be stored in containers, all preconditioners have a constructor with no arguments. Since this will typically produce a useless object, all preconditioners have a function

* 
* [1.x.20]
* 
*  This function receives the matrix to be preconditioned as well as additional required parameters and sets up the internal structures of the preconditioner.
*  [1.x.21]
*  Some preconditioners, like SOR and Jacobi, were used as iterative solvers long before they were used as preconditioners. Thus, they satisfy both  [2.x.4]  "MatrixType" and  [2.x.5]  "RelaxationType" concepts.
* 

* 
*  [2.x.6] 

* 
*  [2.x.7] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/quadrature_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This module contains the base class Quadrature as well as the quadrature formulas provided by deal.II. Quadrature formulas provide two essential pieces of data: the locations of quadrature points on the unit cell [0,1]^d, and the weight of each quadrature point.
*  Since deal.II uses quadrilaterals and hexahedra, almost all quadrature formulas are generated as tensor products of 1-dimensional quadrature formulas defined on the unit interval [0,1], which makes their definition for the higher-dimensional case almost trivial. However, the library also allows anisotropic tensor products (more quadrature points in one coordinate direction than in another) through the QAnisotropic class, as well as the definition of quadrature formulas that are not tensor products.
*  In the grand scheme of things, the classes of this module interact with a variety of other parts of the library:

* 
* [1.x.0]
* 

*  [1.x.1]
*  Quadrature formulas are used, among other uses, when integrating matrix entries and the components of the right hand side vector. To this end, the quadrature point defined on the unit cell have to be mapped to the respective locations on a real cell, and the weights have to be multiplied by the determinant of the Jacobian. This step is done by classes derived from the Mapping base class, although this is often hidden since many parts of the library fall back to using an object of type MappingQ1 if no particular mapping is provided.
*  The next step is to evaluate shape functions and their gradients at these locations. While the classes derived from the FiniteElement base class provide a description of the shape functions on the unit cell, the actual evaluation at quadrature points and joining this with the information gotten from the mapping is done by the FEValues class and its associates. In essence, the FEValues class is therefore a view to the finite element space (defined by the FiniteElement classes) evaluated at quadrature points (provided by the Quadrature classes) mapped to locations inside cells in real, as opposed to unit, space (with the mapping provided by the Mapping classes).
*  The FEValues class provides, as a side product, the location of the quadrature points as mapped to a real cell, for other uses as well. This can then be used, for example, to evaluate a right hand side function at these points.
* 

*  [1.x.2]
*  The class QIterated is used to construct an iterated quadrature formula out of an existing one, thereby increasing the accuracy of the formula without increasing the order. For example, by iterating the trapezoidal rule with points at 0 and 1 and weights 1/2 and 1/2 twice, we get a quadrature formula with points at 0, 1/2, and 1 with weights 1/4, 1/2, and 1/4, respectively. This formula is obtained by projecting the quadrature formula onto the subintervals [0,1/2] and [1/2,1], respectively, and then merging the right endpoint of the left interval with the left endpoint of the right interval. In the same way, all one-dimensional quadrature formulas can be iterated. Higher dimensional iterated formulas are generated as tensor products of one-dimensional iterated formulas.
* 

*  [1.x.3]
*  While the usual quadrature formulas of higher dimensions generate tensor products which are equal in each direction, the class QAnisotropic generates tensor products of possibly different formulas in each direction.
* 

*  [1.x.4]
*  The class QProjector is not actually a quadrature rule by itself, but it provides functions for computing quadrature formulas on the surfaces of higher dimensional cells.
*  All other classes in this module actually implement quadrature rules of different order and other characteristics.
* 

*  [1.x.5]
*  This class is used to generate a quadrature object based on a string that identifies the quadrature formula. This is useful in cases where one wants to specify a certain quadrature formula in an input file, rather than hardcode it in the program.

* 
* [0.x.1]

include/deal.II-translator/A-headers/reordering_0.txt
[0.x.0]*


* 
*  [2.x.0] 
* 

* 
*  [2.x.1]  objects.
*  [1.x.0]
*  Triangulations in deal.II have a special structure, in that there are not only cells, but also faces, and in 3d also edges, that are objects of their own right. Faces and edges have unique orientations, and they have a specified orientation also with respect to the cells that are adjacent. Thus, a line that separates two cells in two space dimensions does not only have a direction, but it must also have a well-defined orientation with respect to the other lines bounding the two quadrilaterals adjacent to the first line. Likewise definitions hold for three dimensional cells and the objects (lines, quads) that separate them.
*  For example, in two dimensions, a quad consists of four lines which have a direction, which is by definition as follows:

* 
* [1.x.1]
*  Now, two adjacent cells must have a vertex numbering such that the direction of the common side is the same. For example, the following two quads

* 
* [1.x.2]
*  may be characterised by the vertex numbers <tt>(0 1 3 4)</tt> and <tt>(1 2 4 5)</tt>, since the middle line would get the direction <tt>1->4</tt> when viewed from both cells.  The numbering <tt>(0 1 3 4)</tt> and <tt>(5 4 2 1)</tt> would not be allowed, since the left quad would give the common line the direction <tt>1->4</tt>, while the right one would want to use <tt>4->1</tt>, leading to an ambiguity.
*  As a sidenote, we remark that if one adopts the idea that having directions of faces is useful, then the orientation of the four faces of a cell as shown above is almost necessary. In particular, it is not possible to orient them such that they represent a (counter-)clockwise sense, since then we couldn't already find a valid orientation of the following patch of three cells:

* 
* [1.x.3]
*  (The reader is asked to try to find a conforming choice of line directions; it will soon be obvious that there can't exists such a thing, even if we allow that there might be cells with clockwise and counterclockwise orientation of the lines at the same time.)
*  One might argue that the definition of unique directions for faces and edges, and the definition of directions relative to the cells they bound, is a misfeature of deal.II. In fact, it makes reading in grids created by mesh generators rather difficult, as they usually don't follow these conventions when generating their output. On the other hand, there are good reasons to introduce such conventions, as they can make programming much simpler in many cases, leading to an increase in speed of some computations as one can avoid expensive checks in many places because the orientation of faces is known by assumption that it is guaranteed by the triangulation.
*  The purpose of this class is now to find an ordering for a given set of cells such that the generated triangulation satisfies all the requirements stated above. To this end, we will first show some examples why this is a difficult problem, and then develop algorithms that finds such a reordering. Note that the algorithm operates on a set of CellData objects that are used to describe a mesh to the triangulation class. These objects are, for example, generated by the GridIn class, when reading in grids from input files.
*  As a last question for this first section: is it guaranteed that such orientations of faces always exist for a given subdivision of a domain into cells? The linear complexity algorithm described below for 2d also proves that the answer is yes for 2d. For 3d, the answer is no (which also underlines that using such orientations might be an
* 
*  -  unfortunately uncurable
* 
*  -  misfeature of deal.II). A simple counter-example in 3d illustrates this: take a string of 3d cells and bend it together to a torus. Since opposing lines in a cell need to have the same direction, there is a simple ordering for them, for example all lines radially outward, tangentially clockwise, and axially upward. However, if before joining the two ends of the string of cells, the string is twisted by 180 degrees, then no such orientation is possible any more, as can easily be checked. In effect, some meshes could not be used in deal.II. In order to overcome this problem, the  [2.x.2] ,  [2.x.3]  flags have been introduced. With these, it is possible to treat all purely hexahedral meshes. However, in order to reduce the effect of possible bugs, it should still be tried to reorder a grid. Only if this procedure fails, the original connectivity information should be used.
* 

*  [1.x.4]
*  As noted, reordering the vertex lists of cells such that the resulting grid is not a trivial problem. In particular, it is often not sufficient to only look at the neighborhood of a cell that cannot be added to a set of other cells without violating the requirements stated above. We will show two examples where this is obvious.
*  The first such example is the following, which we will call the ``four cells at the end'' because of the four cells that close of the right end of a row of three vertical cells each (in the following picture we only show one such column of three cells at the left, but we will indicate what happens if we prolong this list):

* 
* [1.x.5]
*  Assume that you had numbered the vertices in the cells at the left boundary in a way, that the following line directions are induced:

* 
* [1.x.6]
*  (This could for example be done by using the indices <tt>(0 1 3 4)</tt>, <tt>(3 4 6 7)</tt>, <tt>(6 7 9 10)</tt> for the three cells). Now, you will not find a way of giving indices for the right cells, without introducing either ambiguity for one line or other, or without violating that within each cells, there must be one vertex from which both lines are directed away and the opposite one to which both adjacent lines point to.
*  The solution in this case is to renumber one of the three left cells, e.g. by reverting the sense of the line between vertices 7 and 10 by numbering the top left cell by <tt>(9 6 10 7)</tt>:

* 
* [1.x.7]
* 
*  The point here is the following: assume we wanted to prolong the grid to the left like this:

* 
* [1.x.8]
*  Then we run into the same problem as above if we order the cells at the left uniformly, thus forcing us to revert the ordering of one cell (the one which we could order as <tt>(9 6 7 10)</tt> above). However, since opposite lines have to have the same direction, this in turn would force us to rotate the cell left of it, and then the one left to that, and so on until we reach the left end of the grid. This is therefore an example we have to track back right until the first column of three cells to find a consistent ordering, if we had initially ordered them uniformly.
*  As a second example, consider the following simple grid, where the order in which the cells are numbered is important:

* 
* [1.x.9]
*  We have here only indicated the numbers of the vertices that are relevant. Assume that the user had given the cells 0 and 1 by the vertex indices <tt>0 1 3 2</tt> and <tt>6 7 5 4</tt>. Then, if we follow this orientation, the grid after creating the lines for these two cells would look like this:

* 
* [1.x.10]
*  Now, since opposite lines must point in the same direction, we can only add the cells 2 through N-1 to cells 1 such that all vertical lines point down. Then, however, we cannot add cell N in any direction, as it would have two opposite lines that do not point in the same direction. We would have to rotate either cell 0 or 1 in order to be able to add all the other cells such that the requirements of deal.II triangulations are met.
*  These two examples demonstrate that if we have added a certain number of cells in some orientation of faces and can't add the next one without introducing faces that had already been added in another direction, then it might not be sufficient to only rotate cells in the neighborhood of the cell that we failed to add. It might be necessary to go back a long way and rotate cells that have been entered long ago.
* 

*  [1.x.11]
*  From the examples above, it is obvious that if we encounter a cell that cannot be added to the cells which have already been entered, we can not usually point to a cell that is the culprit and that must be entered in a different orientation. Furthermore, even if we knew which cell, there might be large number of cells that would then cease to fit into the grid and which we would have to find a different orientation as well (in the second example above, if we rotated cell 1, then we would have to rotate the cells 1 through N-1 as well).
*  A brute force approach to this problem is the following: if cell N can't be added, then try to rotate cell N-1. If we can't rotate cell N-1 any more, then try to rotate cell N-2 and try to add cell N with all orientations of cell N-1. And so on. Algorithmically, we can visualize this by a tree structure, where node N has as many children as there are possible orientations of node N+1 (in two space dimensions, there are four orientations in which each cell can be constructed from its four vertices; for example, if the vertex indices are <tt>(0 1 3 2)</tt>, then the four possibilities would be <tt>(0 1 3 2)</tt>, <tt>(1 3 2 0)</tt>, <tt>(3 2 0 1)</tt>, and <tt>(2 0 1 3)</tt>). When adding one cell after the other, we traverse this tree in a depth-first (pre-order) fashion. When we encounter that one path from the root (cell 0) to a leaf (the last cell) is not allowed (i.e. that the orientations of the cells which are encoded in the path through the tree does not lead to a valid triangulation), we have to track back and try another path through the tree.
*  In practice, of course, we do not follow each path to a final node and then find out whether a path leads to a valid triangulation, but rather use an inductive argument: if for all previously added cells the triangulation is a valid one, then we can find out whether a path through the tree can yield a valid triangulation by checking whether entering the present cell would introduce any faces that have a nonunique direction; if that is so, then we can stop following all paths below this point and track back immediately.
*  Nevertheless, it is already obvious that the tree has  [2.x.4]  leaves in two space dimensions, since each of the  [2.x.5]  cells can be added in four orientations. Most of these nodes can be discarded rapidly, since firstly the orientation of the first cell is irrelevant, and secondly if we add one cell that has a neighbor that has already been added, then there are already only two possible orientations left, so the total number of checks we have to make until we find a valid way is significantly smaller than  [2.x.6] . However, the algorithm is still exponential in time and linear in memory (we only have to store the information for the present path in form of a stack of orientations of cells that have already been added).
*  In fact, the two examples above show that the exponential estimate is not a pessimistic one: we indeed have to track back to one of the very first cells there to find a way to add all cells in a consistent fashion.
*  This discouraging situation is greatly improved by the fact that we have an alternative algorithm for 2d that is always linear in runtime (discovered and implemented by Michael Anderson of TICAM, University of Texas, in 2003), and that for 3d we can find an algorithm that in practice is usually only roughly linear in time and memory. We will describe these algorithms in the following. A full description and theoretical analysis is given in  [2.x.7]  .
* 

*  [1.x.12]
*  The algorithm uses the fact that opposite faces of a cell need to have the same orientation. So you start with one arbitrary line, choose an orientation. Then the orientation of the opposite face is already fixed. Then go to the two cells across the two faces we have fixed: for them, one face is fixed, so we can also fix the opposite face. Go on with doing so. Eventually, we have done this for a string of cells. Then take one of the non-fixed faces of a cell which has already two fixed faces and do all this again.
*  In more detail, the algorithm is best illustrated using an example. We consider the mesh below:

* 
* [1.x.13]
*  First a cell is chosen ( (0,1,3,4) in this case). A single side of the cell is oriented arbitrarily (3->4). This choice of orientation is then propagated through the mesh, across sides and elements. (0->1), (6->7) and (9->10). The involves edge-hopping and face hopping, giving a path through the mesh shown in dots.

* 
* [1.x.14]
*  This is then repeated for the other sides of the chosen element, orienting more sides of the mesh.

* 
* [1.x.15]
*  Once an element has been completely oriented it need not be considered further. These elements are filled with o's in the diagrams. We then move to the next element.

* 
* [1.x.16]
*  Repeating this gives

* 
* [1.x.17]
*  and the final oriented mesh is

* 
* [1.x.18]
*  It is obvious that this algorithm has linear run-time, since it only ever touches each face exactly once.
*  The algorithm just described in the two-dimensional case is implemented for both 2d and (in generalized form) for 3d in this class. The 3d case uses sheets instead of strings of cells to work on. If a grid is orientable, then the algorithm is able to do its work in linear time; if it is not orientable, then it aborts in linear time as well.
*  Both algorithms are described in the paper "On orienting edges of unstructured two- and three-dimensional meshes", R. Agelek, M. Anderson, W.  Bangerth, W. L. Barth, ACM Transactions on Mathematical Software, vol. 44, article 5, 2017. A preprint is available as [1.x.19].
* 

*  [1.x.20]
*  Prior to the implementation of the algorithms described above (originally implemented by Michael Anderson in 2002, and re-implemented by Wolfgang Bangerth in 2016 based on the work in  [2.x.8] ), we used a branch-and-cut algorithm initially implemented in 2000 by Wolfgang Bangerth. Although it is no longer used, here is how it works, and why it doesn't always work for large meshes since its run-time can be exponential in bad cases.
*  The first observation is that although there are counterexamples, problems are usually local. For example, in the second example mentioned above, if we had numbered the cells in a way that neighboring cells have similar cell numbers, then the amount of backtracking needed is greatly reduced. Therefore, in the implementation of the algorithm, the first step is to renumber the cells in a Cuthill-McKee fashion: start with the cell with the least number of neighbors and assign to it the cell number zero. Then find all neighbors of this cell and assign to them consecutive further numbers. Then find their neighbors that have not yet been numbered and assign to them numbers, and so on. Graphically, this represents finding zones of cells consecutively further away from the initial cells and number them in this front-marching way. This already greatly improves locality of problems and consequently reduced the necessary amount of backtracking.
*  The second point is that we can use some methods to prune the tree, which usually lead to a valid orientation of all cells very quickly.
*  The first such method is based on the observation that if we fail to insert one cell with number N, then this may not be due to cell N-1 unless N-1 is a direct neighbor of N. The reason is obvious: the chosen orientation of cell M could only affect the possibilities to add cell N if either it were a direct neighbor or if there were a sequence of cells that were added after M and that connected cells M and N. Clearly, for M=N-1, the latter cannot be the case. Conversely, if we fail to add cell N, then it is not necessary to track back to cell N-1, but we can track back to the neighbor of N with the largest cell index and which has already been added.
*  Unfortunately, this method can fail to yield a valid path through the tree if not applied with care. Consider the following situation, initially extracted from a mesh of 950 cells generated automatically by the program BAMG (this program usually generates meshes that are quite badly balanced, often have many
* 
*  -  sometimes 10 or more
* 
*  -  neighbors of one vertex, and exposed several problems in the initial algorithm; note also that the example is in 2d where we now have the much better algorithm described above, but the same observations also apply to 3d):

* 
* [1.x.21]
*  Note that there is a hole in the middle. Assume now that the user described the first cell 0 by the vertex numbers <tt>2 3 6 7</tt>, and cell 5 by <tt>15 14 11 10</tt>, and assume that cells 1, 2, 3, and 4 are numbered such that 5 can be added in initial rotation. All other cells are numbered in the usual way, i.e. starting at the bottom left and counting counterclockwise. Given this description of cells, the algorithm will start with cell zero and add one cell after the other, up until the sixth one. Then the situation will be the following:

* 
* [1.x.22]
*  Coming now to cell 7, we see that the two opposite lines at its top and bottom have different directions; we will therefore find no orientation of cell 7 in which it can be added without violation of the consistency of the triangulation. According to the rule stated above, we track back to the neighbor with greatest index, which is cell 6, but since its bottom line is to the right, its top line must be to the right as well, so we won't be able to find an orientation of cell 6 such that 7 will fit into the triangulation. Then, if we have finished all possible orientations of cell 6, we track back to the neighbor of 6 with the largest index and which has been added already. This would be cell 0. However, we know that the orientation of cell 0 can't be important, so we conclude that there is no possible way to orient all the lines of the given cells such that they satisfy the requirements of deal.II triangulations. We know that this can't be, so it results in an exception be thrown.
*  The bottom line of this example is that when we looked at all possible orientations of cell 6, we couldn't find one such that cell 7 could be added, and then decided to track back to cell 0. We did not even attempt to turn cell 5, after which it would be simple to add cell 7. Thus, the algorithm described above has to be modified: we are only allowed to track back to that neighbor that has already been added, with the largest cell index, if we fail to add a cell in any orientation. If we track back further because we have exhausted all possible orientations but could add the cell (i.e. we track back since another cell, further down the road couldn't be added, irrespective of the orientation of the cell which we are presently considering), then we are not allowed to track back to one of its neighbors, but have to track back only one cell index.
*  The second method to prune the tree is that usually we cannot add a new cell since the orientation of one of its neighbors that have already been added is wrong. Thus, if we may try to rotate one of the neighbors (of course making sure that rotating that neighbor does not violate the consistency of the triangulation) in order to allow the present cell to be added.
*  While the first method could be explained in terms of backtracking in the tree of orientations more than one step at once, turning a neighbor means jumping to a totally different place in the tree. For both methods, one can find arguments that they will never miss a path that is valid and only skip paths that are invalid anyway.
*  These two methods have proven extremely efficient. We have been able to read very large grids (several ten thousands of cells) without the need to track back much. In particular, the time to find an ordering of the cells was found to be mostly linear in the number of cells, and the time to reorder them is usually much smaller (for example by one order of magnitude) than the time needed to read the data from a file, and also to actually generate the triangulation from this data using the  [2.x.9]  function.
* 

* 
*  [2.x.10] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/simplex_0.txt
[0.x.0]*


* 
*  [2.x.0] 
* 

* 
*  [2.x.1] 
*  Simplex and mixed meshes in deal.II are still experimental, i.e., work in progress. Large parts of the library have been ported to be able to operate on such kind of meshes. However, there are still many functions that need to be generalized. You can get a good overview of the ported functionalities by taking a look at the tests in the folder "tests/simplex". In the following, we provide two very basic examples to get started and provide some implementation details.
* 

* 
*  [2.x.2]  simplex_reference_example_simplex Example: simplex mesh
*  The following code shows how to work with simplex meshes:
*   [2.x.3] 
* 

* 
*  [2.x.4]  simplex_reference_example_mixed Example: mixed mesh
*  The following code shows how to work with mixed meshes:
*   [2.x.5] 
* 

* 
*  [2.x.6]  simplex_reference_cells Reference cells
*  In 2D, we provide triangles and quadrilaterals with the following possible orientations in 3D:
*   [2.x.7]       [2.x.8]    </div>    [2.x.9]       [2.x.10]    </div> </div>
*  In 3D, tetrahedra, pyramids, wedges, and hexahedra are available:
*     [2.x.11]       [2.x.12]    </div>
*     [2.x.13]       [2.x.14]    </div>
*     [2.x.15]       [2.x.16]    </div>
*     [2.x.17]       [2.x.18]    </div>
*  Each surface of a 3D reference cell consists of 2D reference cells. The documentation of the enumeration of the numbering of their vertices and lines are given in the right columns.
* 

* 
* [0.x.1]

include/deal.II-translator/A-headers/slepc_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  The classes in this module are wrappers around functionality provided by the SLEPc library. All classes and functions in this group reside in a namespace  [2.x.1] 
*  These classes are only available if a SLEPc installation and a PETSc installation was detected during configuration of deal.II. Refer to the README file for more details about this.
*   [2.x.2]  Toby D. Young, 2011
* 

* 
*  [2.x.3] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/solvers_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This module groups iterative and direct solvers, eigenvalue solvers, and some control classes. All these classes operate on objects of the  [2.x.1]  "matrix" and  [2.x.2]  "vector classes" defined in deal.II.
*  In order to work properly, solvers that take matrix and vector classes as template arguments require that these classes satisfy a certain minimal interface that can be used from inside the solver. For iterative solvers, this interface is defined in the Solver class. In addition, solvers are controlled using objects of classes that are derived from the SolverControl class (for example its derived class ReductionControl), in order to determine the maximal number of iterations or a desired tolerance.
*  If detected during configuration (see the ReadMe file), some sparse direct solvers are also supported.
* 

* 
*  [2.x.3] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/sparsity_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  Almost all finite element formulations lead to matrices that are "sparse", i.e., for which the number of nonzero elements per row is (i) relatively small compared to the overall size of the matrix, and (ii) bounded by a fixed number that does not grow if the mesh is refined. For such cases, it is more efficient to not store [1.x.0] elements of the matrix, but only those that are actually (or may be) nonzero. This requires storing, for each row, the column indices of the nonzero entries (we call this the "sparsity pattern") as well as the actual values of these nonzero entries. (In practice, it sometimes happens that some of the nonzero values are, in fact, zero. Sparsity patterns and sparse matrices only intend to provision space for entries that [1.x.1] be nonzero, and do so at a time when we don't know yet what values these entries will ultimately have; they may have a zero value if a coefficient or cell happens to have particular values.)
*  In deal.II, sparsity patterns are typically separated from the actual sparse matrices (with the exception of the SparseMatrixEZ class and some classes from interfaces to external libraries such as PETSc). The reason is that one often has several matrices that share the same sparsity pattern; examples include the stiffness and mass matrices necessary for time stepping schemes, or the left and right hand side matrix of generalized eigenvalue problems. It would therefore be wasteful if each of them had to store their sparsity pattern separately.
*  Consequently, deal.II has sparsity pattern classes that matrix classes build on. There are two main groups of sparsity pattern classes, as discussed below:
* 

*  [1.x.2]
*  The main sparse matrix class in deal.II, SparseMatrix, only stores a value for each matrix entry, but not where these entries are located. For this, it relies on the information it gets from a sparsity pattern object associated with this matrix. This sparsity pattern object must be of type SparsityPattern.
*  Because matrices are large objects and because it is comparatively expensive to change them, SparsityPattern objects are built in two phases: first, in a "dynamic" phase, one allocates positions where one expects matrices built on it to have nonzero entries; in a second "static" phase, the representation of these nonzero locations is "compressed" into the usual Compressed Sparse Row (CSR) format. After this, no new nonzero locations may be added. Only after compression can a sparsity pattern be associated to a matrix, since the latter requires the efficient compressed data format of the former. Building a sparsity pattern during the dynamic phase often happens with the  [2.x.1]  function. Although this may appear a restriction, it is typically not a significant problem to first build a sparsity pattern and then to write into the matrix only in the previously allocated locations, since in finite element codes it is normally quite clear which elements of a matrix can possibly be nonzero and which are definitely zero.
*  The advantage of this two-phase generation of a sparsity pattern is that when it is actually used with a matrix, a very efficient format is available. In particular, the locations of entries are stored in a linear array that allows for rapid access friendly to modern CPU types with deep hierarchies of caches. Consequently, the static SparsityPattern class is the only one on which deal.II's main SparseMatrix class can work.
*  The main drawback of static sparsity patterns is that their efficient construction requires a reasonably good guess how many entries each of the rows may maximally have. During the actual construction, for example in the  [2.x.2]  function, only at most as many entries can be allocated as previously stated. This is a problem because it is often difficult to estimate the maximal number of entries per row. Consequently, a common strategy is to first build and intermediate sparsity pattern that uses a less efficient storage scheme during construction of the sparsity pattern and later copy it directly into the static, compressed form. Most tutorial programs do this, starting at  [2.x.3]  (see also, for example the  [2.x.4] ,  [2.x.5] , and  [2.x.6]  tutorial programs).
* 

*  [1.x.3]
*  As explained above, it is often complicated to obtain good estimates for the maximal number of entries in each row of a sparsity pattern. Consequently, any attempts to allocate a regular SparsityPattern with bad estimates requires huge amounts of memory, almost all of which will not be used and be de-allocated upon compression.
*  To avoid this, deal.II contains a "dynamic" or "compressed" sparsity pattern called DynamicSparsityPattern that only allocates as much memory as necessary to hold the currently added entries. While this saves much memory compared to the worst-case behavior mentioned above, it requires the use of less efficient storage schemes for insertion of elements, and the frequent allocation of memory often also takes significant compute time. The tradeoff to avoid excessive memory allocation cannot be avoided, however.
*  The class is typically used in the following way

* 
* [1.x.4]
* 
*  The intermediate, compressed sparsity pattern is directly copied into the "compressed" form of the final static pattern.
*  [1.x.5]
*  The class BlockDynamicSparsityPattern implements an array of dynamic sparsity patterns for constructing block matrices. See the documentation and  [2.x.7]  for more information.
* 

* 
*  [2.x.8] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/trilinos_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  The classes in this module are wrappers around functionality provided by the Trilinos library. They provide a modern object-oriented interface that is compatible with the interfaces of the other linear algebra classes in deal.II. All classes and functions in this group reside in a namespace  [2.x.1] 
*  These classes are only available if a Trilinos installation was detected during configuration of deal.II. Refer to the README file for more details about this.
*   [2.x.2]  Martin Kronbichler, Wolfgang Bangerth, 2008
* 

* 
*  [2.x.3] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/update_flags_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  [1.x.0]
*  In order to compute local contributions of an individual cell to the global matrix and right hand side, we usually employ two techniques:
* 

* 
* 
*  - First, the integral is transformed from the actual cell  [2.x.1]  to the   unit/reference cell  [2.x.2] . For example, for the Laplace equation, we   transform   [1.x.1]   into   [1.x.2]   where a hat indicates reference coordinates, and  [2.x.3]  is the Jacobian    [2.x.4]  of the mapping    [2.x.5] .
* 

* 
* 
*  - Second, this integral is then approximated through quadrature. This yields   the formula   [1.x.3]   where  [2.x.6]  indicates the index of the quadrature point,  [2.x.7]  its   location on the reference cell, and  [2.x.8]  its weight.
*  In order to evaluate such an expression in an application code, we have to access three different kinds of objects: a quadrature object that describes locations  [2.x.9]  and weights  [2.x.10]  of quadrature points on the reference cell; a finite element object that describes the gradients  [2.x.11]  of shape functions on the unit cell; and a mapping object that provides the Jacobian as well as its determinant. Dealing with all these objects would be cumbersome and error prone.
*  On the other hand, these three kinds of objects almost always appear together, and it is in fact very rare for deal.II application codes to do anything with quadrature, finite element, or mapping objects besides using them together. For this reason, deal.II uses the FEValues abstraction combining information on the shape functions, the geometry of the actual mesh cell and a quadrature rule on a reference cell. Upon construction it takes one object of each of the three mentioned categories. Later, it can be "re-initialized" for a concrete grid cell and then provides mapped quadrature points and weights, mapped shape function values and derivatives as well as some properties of the transformation from the reference cell to the actual mesh cell.
*  Since computation of any of these values is potentially expensive (for example when using high order mappings with high order elements), the FEValues class only computes what it is explicitly asked for. To this end, it takes a list of flags of type UpdateFlags at construction time specifying which quantities should be updated each time a cell is visited. In the case above, you want the gradients of the shape functions on the real cell, which is encoded by the flag  [2.x.12] , as well as the product of the determinant of the Jacobian times the quadrature weight, which is mnemonically encoded using the term  [2.x.13] . Because these flags are represented by single bits in integer numbers, producing a [1.x.4] amounts to setting multiple bits in an integer, which is facilitated using the operation  [2.x.14]  (in other words, and maybe slightly confusingly so, the operation  [2.x.15]  operation [1.x.5] that operation [2.x.16]  is represented by the expression  [2.x.17]  [1.x.6] single-bit-in-an-integer-for-that-operation [2.x.18]  To make operations cheaper, FEValues and the mapping and finite element objects it depends on really only compute those pieces of information that you have specified in the update flags (plus some information necessary to compute what has been specified, see below), and not everything that could possibly be computed on a cell. This optimization makes it much cheaper to iterate over cells for assembly, but it also means that one should take care to provide the minimal set of flags possible.
*  In addition, once you pass a set of flags for what you want, the functions filling the data fields of FEValues are able to distinguish between values that have to be recomputed on each cell (for example mapped gradients) and quantities that do not change from cell to cell (for example the values of shape functions of the usual  [2.x.19]  finite elements at the same quadrature points on different cells; this property does not hold for the shape functions of Raviart-Thomas elements, however, which must be rotated with the local cell). This allows further optimization of the computations underlying assembly.
* 

*  [1.x.7]
*  Let's say you want to compute the Laplace matrix as shown above. In that case, you need to specify the  [2.x.20]  flag (for  [2.x.21] ) and the  [2.x.22]  flag (for computing  [2.x.23] ). Internally, however, the finite element requires the computation of the inverse of the full Jacobian matrix,  [2.x.24]  (and not just the determinant of the matrix), and to compute the inverse of the Jacobian, it is also necessary to compute the Jacobian matrix first.
*  Since these are requirements that are not important to the user, it is not necessary to specify this in user code. Rather, given a set of update flags, the FEValues object first asks the finite element object what information it needs to compute in order to satisfy the user's request provided in the update flags. The finite element object may therefore add other flags to the update flags (e.g., in the example above, an FE_Q object would add  [2.x.25]  to the list, since that is the necessary transformation from  [2.x.26]  to  [2.x.27] ). With these updated flags, FEValues then asks the mapping whether it also wants to add more flags to the list to satisfy the needs of both the user and the finite element object, by calling  [2.x.28]  (This procedure of first asking the finite element and then the mapping does not have to be iterated because mappings never require information computed by the finite element classes, while finite element classes typically need information computed by mappings.) Using this final list, the FEValues object then asks both the finite element object and mapping object to create temporary structures into which they can store some temporary information that can be computed once and for all, and these flags will be used when re-computing data on each cell we will visit later on.
* 

*  [1.x.8]
*  As outlined above, we have now determined the final set of things that are necessary to satisfy a user's desired pieces of information as conveyed by the update flags they provided. This information will then typically be queried on every cell the user code visits in a subsequent integration loop.
*  Given that many of the things mappings or finite element classes compute are potentially expensive, FEValues employs a system whereby mappings and finite element objects are encouraged to pre-compute information that can be computed once without reference to a concrete cell, and make use of this when asked to visit a particular cell of the mesh. An example is that the values of the shape functions of the common FE_Q element are defined on the reference cell, and the values on the actual cell are simply exactly the values on the reference cell
* 
*  -  there is consequently no need to evaluate shape functions on every cell, but it is sufficient to do this once at the beginning, store the values somewhere, and when visiting a concrete cell simply copying these values from their temporary location to the output structure. (Note, however, that this is specific to the FE_Q element: this is not the case if we used a FE_RaviartThomas element, since there, computing the values of the shape functions on a cell involves knowing the Jacobian of the mapping which depends on the geometry of the cell we visit; thus, for this element, simply copying pre-computed information is not sufficient to evaluate the values of shape functions on a particular cell.)
*  To accommodate this structure, both mappings and finite element classes may internally split the update flags into two sets commonly referenced as  [2.x.29]  (though these names do not appear in any public interfaces). The former contains all those pieces of information that can be pre-computed once at the time the FEValues object starts to interact with a mapping or finite element, whereas the latter contains those flags corresponding to things that need to be computed on every cell. For example, if  [2.x.30] , then the FE_Q class will set  [2.x.31]  and  [2.x.32] , whereas the Raviart-Thomas element will do it the other way around.
*  These sets of flags are intended to be mutually exclusive. There is, on the other hand, nothing that ever provides this decomposition to anything outside the mapping or finite element classes
* 
*  -  it is a purely internal decomposition.
* 

*  [1.x.9]
*  As outlined above, data is computed at two different times: once at the beginning on the reference cell, and once whenever we move to an actual cell. The functions involved in each of these steps are discussed next:
* 

*  [1.x.10]
*  Computing data on the reference cell before we even visit the first real cell is a two-step process. First, the constructor of FEValues, FEFaceValues and FESubfaceValues, respectively, need to allow the Mapping and FiniteElement objects to set up internal data structures. These structures are internal in the following sense: the FEValues object asks the finite element and mapping objects to create an object of type  [2.x.33]  and  [2.x.34]  each; the actual finite element and mapping class may in fact create objects of a derived type if they wish to store some data beyond what these base classes already provide. The functions involved in this are  [2.x.35]   [2.x.36]   [2.x.37]   [2.x.38]   [2.x.39]   [2.x.40]   [2.x.41]   [2.x.42] 
*  The FEValues object then takes over ownership of these objects and will destroy them at the end of the FEValues object's lifetime. After this, the FEValues object asks the FiniteElement and Mapping objects to fill these InternalDataBase objects with the data that pertains to what can and needs to be computed on the reference cell. This is done in these functions:  [2.x.43]   [2.x.44]   [2.x.45]   [2.x.46]   [2.x.47] 
* 

*  [1.x.11]
*  Once initialization is over and we call  [2.x.48]   [2.x.49]  or  [2.x.50]  to move to a concrete cell or face, we need to calculate the "update_each" kinds of data. This is done in the following functions:  [2.x.51]   [2.x.52]  calls  [2.x.53]  then  [2.x.54]   [2.x.55]  calls  [2.x.56]  then  [2.x.57]   [2.x.58]  calls  [2.x.59]  then  [2.x.60]   [2.x.61] 
*  This is where the actual data fields for FEValues, stored in  [2.x.62]  and  [2.x.63]  objects, are computed. These functions call the function in Mapping first, such that all the mapping data required by the finite element is available. Then, the FiniteElement function is called.
* 

* 
*  [2.x.64] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/utilities_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This module simply collects a number of functions and classes that provide general tools for tasks that do not usually have much to do with finite element programs in particular, but happen to be required there just as well.

* 
* [0.x.1]*


* 
*  [2.x.1] 
*  Here are a few simple classes that help in storage and viewing data. For example, the Table templates allow to use not only arrays of objects (for which one might want to use the  [2.x.2]  class), but also two-dimensional (rectangular) tables of arbitrary objects, as well as higher-order analogs up to tables with (presently) seven indices.
*  Similarly, the VectorSlice function is a primitive that takes anything that has an interface that resembles a vector (for example the deal.II Vector or the  [2.x.3]  classes) and presents a view on it as if it were a vector in itself.
* 

* 
*  [2.x.4] 

* 
* [0.x.2]

include/deal.II-translator/A-headers/vector_memory_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  This module groups a few classes that are used to avoid allocating and deallocating vectors over and over in iterative procedures. These methods all use an object of the base class VectorMemory to get their auxiliary vectors.
*  Some discussion on this topic can be found in the discussion of the InverseMatrix class in  [2.x.1] .
* 

* 
*  [2.x.2] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/vectors_0.txt
[0.x.0]*


* 
*  [2.x.0] 
*  Here, we list all the classes that satisfy the  [2.x.1]  concept and may be used in linear solvers (see  [2.x.2] ) and for matrix-vector operations.
* 

* 
*  [2.x.3] 

* 
* [0.x.1]

include/deal.II-translator/A-headers/vector_valued_0.txt
[0.x.0]*


* 
*  [2.x.0] 
* 

*  Vector-valued problems are systems of partial differential equations. These are problems where the solution variable is not a scalar function, but a vector-valued function or a set of functions. This includes, for example:  [2.x.1]     [2.x.2] The elasticity equation discussed in  [2.x.3] ,        [2.x.4] , and  [2.x.5]  in which the       solution is the vector-valued displacement at each point.    [2.x.6] The mixed Laplace equation and its extensions discussed in        [2.x.7] , and  [2.x.8]  in which the       solution is the scalar pressure and the vector-valued velocity       at each point.    [2.x.9] The Stokes equation and its extensions discussed in        [2.x.10] , and  [2.x.11]  in which again       the solution is the scalar pressure and the vector-valued velocity       at each point.    [2.x.12] Complex-valued solutions consisting of real and imaginary parts, as       discussed for example in  [2.x.13] .  [2.x.14] 
*  This page gives an overview of how to implement such vector-valued problems easily in deal.II. In particular, it explains the usage of the class FESystem, which allows us to write code for systems of partial differential very much like we write code for single equations.
*   [2.x.15] 
*   [2.x.16] 
* 

* 
*   [2.x.17]  VVExamples [1.x.1]
*  The way one deals systematically with vector-valued problems is not fundamentally different from scalar problems: first, we need a weak (variational) formulation of the problem that takes into account all the solution variables. After we did so, generating the system matrix and solving the linear system follows the same outlines that we are used to already.
*  [1.x.2]
*  Let us take for example the elasticity problem from  [2.x.18]  and even simplify it by choosing  [2.x.19]  and  [2.x.20]  to highlight the important concepts. Therefore, let consider the following weak formulation: find  [2.x.21]  such that for all  [2.x.22]  holds [1.x.3] Here, [1.x.4] denotes the symmetric gradient defined by  [2.x.23]  and the colon indicates double contraction of two tensors of rank 2 (the Frobenius inner product). This bilinear form looks indeed very much like the bilinear form of the Poisson problem in  [2.x.24] . The only differences are  [2.x.25]   [2.x.26] We replaced the gradient operator by the symmetric gradient; this is actually not a significant difference, and everything said here is true if your replace  [2.x.27]  by  [2.x.28] . Indeed, let us do this to simplify the discussion: [1.x.5] Note though, that this system is not very exciting, since we could solve for the three components of [1.x.6] separately.
*   [2.x.29]  The trial and test functions are now from the space  [2.x.30] , which can be viewed as three copies of the scalar space  [2.x.31] . And this is exactly, how we are going to implement this space below, using FESystem.  [2.x.32] 
*  But for now, let us look at the system a little more closely. First, let us exploit that [1.x.7]=([1.x.8]<sub>1</sub>,[1.x.9]<sub>2</sub>,[1.x.10]<sub>3</sub>)<sup>T</sup> and [1.x.11] accordingly. Then, we can write the simplified equation in coordinates as [1.x.12] We see, that this is just three copies of the bilinear form of the Laplacian, one applied to each component (this is where the formulation with the  [2.x.33]  is more exciting, and we want to derive a framework that applies to that one as well). We can make this weak form a system of differential equations again by choosing special test functions: first, choose [1.x.13]=([1.x.14]<sub>1</sub>,0,0)<sup>T</sup>, then [1.x.15]=(0,[1.x.16]<sub>2</sub>,0)<sup>T</sup>, and finally [1.x.17]=(0,0,[1.x.18]<sub>3</sub>)<sup>T</sup>. writing the outcomes below each other, we obtain the system [1.x.19] where we used the standard inner product notation  [2.x.34] . It is important for our understanding, that we keep in mind that the latter form as a system of PDE is completely equivalent to the original definition of the bilinear form [1.x.20]([1.x.21],[1.x.22]), which does not immediately exhibit this system structure. Let us close by writing the full system of the elastic equation with symmetric gradient [1.x.23]: [1.x.24] Very formally, if we believe in operator valued matrices, we can rewrite this in the form [1.x.25]<sup>T</sup>[1.x.26] = [1.x.27]<sup>T</sup>[1.x.28] or [1.x.29]
*  [1.x.30] Now, let us consider a more complex example, the mixed Laplace equations discussed in  [2.x.35]  in three dimensions:[1.x.31]
* 
*  Here, we have four solution components: the scalar pressure  [2.x.36]  and the vector-valued velocity  [2.x.37]  with three vector components. Note as important difference to the previous example, that the vector space [1.x.32] is not just simply a copy of three identical spaces/
*  A systematic way to get a weak or variational form for this and other vector problems is to first consider it as a problem where the operators and solution variables are written in vector and matrix form. For the example, this would read as follows:[1.x.33]
* 
*  This makes it clear that the solution[1.x.34]
*  indeed has four components. We note that we could change the ordering of the solution components  [2.x.38]  and  [2.x.39]  inside  [2.x.40]  if we also change columns of the matrix operator.
*  Next, we need to think about test functions  [2.x.41] . We want to multiply both sides of the equation with them, then integrate over  [2.x.42] . The result should be a scalar equality. We can achieve this by choosing  [2.x.43]  also vector valued as[1.x.35]
* 
*  It is convenient to multiply the matrix-vector equation by the test function from the left, since this way we automatically get the correct matrix later on (in the linear system, the matrix is also multiplied from the right with the solution variable, not from the left), whereas if we multiplied from the right then the matrix so assembled is the transpose of the one we really want.
*  With this in mind, let us multiply by  [2.x.44]  and integrate to get the following equation which has to hold for all test functions  [2.x.45] :[1.x.36]
*  or equivalently:[1.x.37]
* 
*
* We get the final form by integrating by part the second term:[1.x.38]
* 
*  It is this form that we will later use in assembling the discrete weak form into a matrix and a right hand side vector: the form in which we have solution and test functions  [2.x.46]  that each consist of a number of vector components that we can extract.
* 

*   [2.x.47]  VVFEs [1.x.39]
*  Once we have settled on a bilinear form and a functional setting, we need to find a way to describe the vector-valued finite element spaces from which we draw solution and test functions. This is where the FESystem class comes in: it composes vector-valued finite element spaces from simpler ones. In the example of the elasticity problem, we need  [2.x.48]  copies of the same element, for instance

* 
* [1.x.40]
*  This will generate a vector valued space of dimension  [2.x.49] , where each component is a continuous bilinear element of type FE_Q. It will have  [2.x.50]  times as many basis functions as the corresponding FE_Q, and each of these basis functions is a basis function of FE_Q, lifted into one of the components of the vector.
*  For the mixed Laplacian, the situation is more complex. First, we have to settle on a pair of discrete spaces  [2.x.51] . One option would be the stable Raviart-Thomas pair

* 
* [1.x.41]
*  The first element in this system is already a vector valued element of dimension  [2.x.52] , while the second is a regular scalar element.
*  Alternatively to using the stable Raviart-Thomas pair, we could consider a stabilized formulation for the mixed Laplacian, for instance the LDG method. There, we have the option of using the same spaces for velocity components and pressure, namely

* 
* [1.x.42]
*  This system just has  [2.x.53]  equal copies of the same discontinuous element, which not really reflects the structure of the system. Therefore, we prefer

* 
* [1.x.43]
*  Here, we have a system of two elements, one vector-valued and one scalar, very much like with the  [2.x.54] . Indeed, in many codes, the two can be interchanged. This element also allows us easily to switch to an LDG method with lower order approximation in the velocity, namely

* 
* [1.x.44]
*  It must be pointed out, that this element is different from

* 
* [1.x.45]
*  While the constructor call is very similar to  [2.x.55] , the result actually resembles more  [2.x.56]  in that this element produces  [2.x.57]  independent components. A more detailed comparison of the resulting FESystem objects is below.
*  [1.x.46]
*  FESystem has a few internal variables which reflect the internal structure set up by the constructor. These can then also be used by application programs to give structure to matrix assembling and linear algebra. We give the names and values of these variables for the examples above in the following table: <table border="1"> <tr><th>System Element</th>  [2.x.58]   [2.x.59]   [2.x.60]  </tr> <tr><td> [2.x.61] </td><td>1</td> </tr> <tr><td> [2.x.62] </td><td>2</td> </tr> <tr><td> [2.x.63] </td><td>2</td> </tr> <tr><td> [2.x.64] </td><td>1</td> </tr> <tr><td> [2.x.65] </td><td>2</td> </tr> </table>
*  From this table, it is clear that the FESystem reflects a lot of the structure of the system of differential equations in the cases of the  [2.x.66]  and the  [2.x.67] , in that we have a vector valued and a scalar variable. On the other hand, the convoluted elements do not have this structure and we have to reconstruct it somehow when assembling systems, as described below.
*  At this point, it is important to note that nesting of two FESystem object can give the whole FESystem a richer structure than just concatenating them. This structure can be exploited by application programs, but is not automatically so.
*   [2.x.68]  VVAssembling [1.x.47] The next step is to assemble the linear system. How to do this for the simple case of a scalar problem has been shown in many tutorial programs, starting with  [2.x.69] . Here we will show how to do it for vector problems. Corresponding to the different characterizations of weak formulations above and the different system elements created, we have a few options which are outlined below.
*  The whole concept is probably best explained by showing an example illustrating how the local contribution of a cell to the weak form of above mixed Laplace equations could be assembled.
*  [1.x.48] This is essentially how  [2.x.70]  does it:

* 
* [1.x.49]
* 
*  So here's what is happening:  [2.x.71]     [2.x.72]  The first thing we do is to declare "extractors" (see the        FEValuesExtractors namespace). These are objects that don't        do much except store which components of a vector-valued finite        element constitute a single scalar component, or a tensor of        rank 1 (i.e. what we call a "physical vector", always consisting        of  [2.x.73]  components). Here, we declare        an object that represents the velocities consisting of         [2.x.74]  components starting at component zero, and the        extractor for the pressure, which is a scalar component at        position  [2.x.75] .
*     [2.x.76]  We then do our usual loop over all cells, shape functions, and        quadrature points. In the innermost loop, we compute the local        contribution of a pair of shape functions to the global matrix        and right hand side vector. Recall that the cell contributions        to the bilinear form (i.e. neglecting boundary terms) looked as        follows, based on shape functions         [2.x.77] :          [1.x.50]
*         whereas the implementation looked like this:       
* [1.x.51]
*         The similarities are pretty obvious.
*     [2.x.78]  Essentially, what happens in above code is this: when you do         [2.x.79] , a so-called "view" is created, i.e.        an object that unlike the full FEValues object represents not all        components of a finite element, but only the one(s) represented by        the extractor object  [2.x.80]  or         [2.x.81] .
*     [2.x.82]  These views can then be asked for information about these individual        components. For example, when you write         [2.x.83]  you get the        value of the pressure component of the  [2.x.84] th shape function  [2.x.85]  at        the  [2.x.86] th quadrature point. Because the extractor         [2.x.87]  represents a scalar component, the results of        the operator  [2.x.88]  is a scalar        number. On the other hand, the call         [2.x.89]  would produce the        value of a whole set of  [2.x.90]  components, which would        be of type  [2.x.91] .
*     [2.x.92]  Other things that can be done with views is to ask for the gradient        of a particular shape function's components described by an        extractor. For example,  [2.x.93]         would represent the gradient of the scalar pressure component, which        is of type  [2.x.94] , whereas the gradient of the        velocities components,  [2.x.95]         is a  [2.x.96] , i.e. a matrix  [2.x.97]  that consists        of entries  [2.x.98] . Finally,        both scalar and vector views can be asked for the second derivatives        ("Hessians") and vector views can be asked for the symmetric gradient,        defined as  [2.x.99]  as well as the        divergence  [2.x.100] .  [2.x.101]  Other examples of using extractors and views are shown in tutorial programs  [2.x.102] ,  [2.x.103] ,  [2.x.104]  and several other programs.
* 

* 
*  [2.x.105]  In the current context, when we talk about a vector (for example in extracting the velocity components above), we mean the word in the sense physics uses it: it has  [2.x.106]  components that behave in specific ways under coordinate system transformations. Examples include velocity or displacement fields. This is opposed to how mathematics uses the word "vector" (and how we use this word in other contexts in the library, for example in the Vector class), where it really stands for a collection of numbers. An example of this latter use of the word could be the set of concentrations of chemical species in a flame; however, these are really just a collection of scalar variables, since they do not change if the coordinate system is rotated, unlike the components of a velocity vector, and consequently, this  [2.x.107]  class should not be used for this case.
* 

*   [2.x.108]  VVAlternative [1.x.52]
*  There are situations in which one can optimize the assembly of a matrix or right hand side vector a bit, using knowledge of the finite element in use. Consider, for example, the bilinear form of the elasticity equations which we are concerned with first in  [2.x.109] :
* [1.x.53]
*  Here,  [2.x.110]  is a vector function with  [2.x.111]  components,  [2.x.112]  the corresponding test function, and  [2.x.113]  are material parameters. Given our discussions above, the obvious way to implement this bilinear form would be as follows, using an extractor object that interprets all  [2.x.114]  components of the finite element as single vector, rather than disjoint scalar components:
* 

* 
* [1.x.54]
* 
*  Now, this is not the code used in  [2.x.115] . In fact, if one used the above code over the one implemented in that program, it would run about 8 per cent slower. It can be improved (bringing down the penalty to about 4 per cent) by taking a close look at the bilinear form. In fact, we can transform it as follows:[1.x.55]
*  where  [2.x.116]  is the symmetrized gradient. In the second to last step, we used that the scalar product between an arbitrary tensor  [2.x.117]  and a symmetric tensor  [2.x.118]  equals the scalar product of the symmetric part of the former with the second tensor. Using the techniques discussed above, the obvious way to implement this goes like this:
* 

* 
* [1.x.56]
* 
*  So if, again, this is not the code we use in  [2.x.119] , what do we do there? The answer rests on the finite element we use. In  [2.x.120] , we use the following element:

* 
* [1.x.57]
*  In other words, the finite element we use consists of  [2.x.121]  copies of the same scalar element. This is what we call a  [2.x.122]  "primitive" element: an element that may be vector-valued but where each shape function has exactly one non-zero component. In other words: if the  [2.x.123] -component of a displacement shape function is nonzero, then the  [2.x.124] - and  [2.x.125] -components must be zero and similarly for the other components. What this means is that also derived quantities based on shape functions inherit this sparsity property. For example: the divergence  [2.x.126]  of a vector-valued shape function  [2.x.127]  is, in the present case, either  [2.x.128] ,  [2.x.129] , or  [2.x.130] , because exactly one of the  [2.x.131]  is nonzero. Knowing this means that we can save a number of computations that, if we were to do them, would only yield zeros to add up.
*  In a similar vein, if only one component of a shape function is nonzero, then only one row of its gradient  [2.x.132]  is nonzero. What this means for terms like  [2.x.133] , where the scalar product between two tensors is defined as  [2.x.134] , is that the term is only nonzero if both tensors have their nonzero entries in the same row, which means that the two shape functions have to have their single nonzero component in the same location.
*  If we use this sort of knowledge, then we can in a first step avoid computing gradient tensors if we can determine up front that their scalar product will be nonzero, in a second step avoid building the entire tensors and only get its nonzero components, and in a final step simplify the scalar product by only considering that index  [2.x.135]  for the one nonzero row, rather than multiplying and adding up zeros.
*  The vehicle for all this is the ability to determine which vector component is going to be nonzero. This information is provided by the  [2.x.136]  function. What can be done with it, using the example above, is explained in detail in  [2.x.137] .
* 

*   [2.x.138]  VVBlockSolvers [1.x.58]
*  Using techniques as shown above, it isn't particularly complicated to assemble the linear system, i.e. matrix and right hand side, for a vector-valued problem. However, then it also has to be solved. This is more complicated. Naively, one could just consider the matrix as a whole. For most problems, this matrix is not going to be definite (except for special cases like the elasticity equations covered in  [2.x.139]  and  [2.x.140] ). It will, often, also not be symmetric. This rather general class of matrices presents problems for iterative solvers: the lack of structural properties prevents the use of most efficient methods and preconditioners. While it can be done, the solution process will therefore most often be slower than necessary.
*  The answer to this problem is to make use of the structure of the problem. For example, for the mixed Laplace equations discussed above, the operator has the form[1.x.59]
* 
*  It would be nice if this structure could be recovered in the linear system as well. For example, after discretization, we would like to have a matrix with the following block structure:[1.x.60]
*  where  [2.x.141]  represents the mass matrix that results from discretizing the identity operator  [2.x.142]  and  [2.x.143]  the equivalent of the gradient operator.
*  By default, this is not what happens, however. Rather, deal.II assigns %numbers to degrees of freedom in a rather random manner. Consequently, if you form a vector out of the values of degrees of freedom will not be neatly ordered in a vector like[1.x.61]
*  Rather, it will be a permutation of this, with %numbers of degrees of freedom corresponding to velocities and pressures intermixed. Consequently, the system matrix will also not have the nice structure mentioned above, but with the same permutation or rows and columns.
*  What is needed is to re-enumerate degrees of freedom so that velocities come first and pressures last. This can be done using the  [2.x.144]  function, as explained in  [2.x.145]  " [2.x.146] ",  [2.x.147] ,  [2.x.148] , and  [2.x.149]  " [2.x.150] ". After this, at least the degrees of freedom are partitioned properly.
*  But then we still have to make use of it, i.e. we have to come up with a solver that uses the structure. For example, in  [2.x.151] , we do a block elimination of the linear system[1.x.62]
*  What this system means, of course, is[1.x.63]
* 
*  So, if we multiply the first equation by  [2.x.152]  and subtract the second from the result, we get[1.x.64]
* 
*  This is an equation that now only contains the pressure variables. If we can solve it, we can in a second step solve for the velocities using[1.x.65]
* 
*  This has the advantage that the matrices  [2.x.153]  and  [2.x.154]  that we have to solve with are both symmetric and positive definite, as opposed to the large whole matrix we had before.
*  How a solver like this is implemented is explained in more detail in  [2.x.155]  step_20 " [2.x.156] ",  [2.x.157] , and a few other tutorial programs. What we would like to point out here is that we now need a way to extract certain parts of a matrix or vector: if we are to multiply, say, the  [2.x.158]  part of the solution vector by the  [2.x.159]  part of the global matrix, then we need to have a way to access these parts of the whole.
*  This is where the BlockVector, BlockSparseMatrix, and similar classes come in. For all practical purposes, then can be used as regular vectors or sparse matrices, i.e. they offer element access, provide the usual vector operations and implement, for example, matrix-vector multiplications. In other words, assembling matrices and right hand sides works in exactly the same way as for the non-block versions. That said, internally they store the elements of vectors and matrices in "blocks"; for example, instead of using one large array, the BlockVector class stores it as a set of arrays each of which we call a block. The advantage is that, while the whole thing can be used as a vector, one can also access an individual block which then, again, is a vector with all the vector operations.
*  To show how to do this, let us consider the second equation  [2.x.160]  to be solved above. This can be achieved using the following sequence similar to what we have in  [2.x.161] :

* 
* [1.x.66]
* 
*  What's happening here is that we allocate a temporary vector with as many elements as the first block of the solution vector, i.e. the velocity component  [2.x.162] , has. We then set this temporary vector equal to the  [2.x.163]  block of the matrix, i.e.  [2.x.164] , times component 1 of the solution which is the previously computed pressure  [2.x.165] . The result is multiplied by  [2.x.166] , and component 0 of the right hand side,  [2.x.167]  is added to it. The temporary vector now contains  [2.x.168] . The rest of the code snippet simply solves a linear system with  [2.x.169]  as right hand side and the  [2.x.170]  block of the global matrix, i.e.  [2.x.171] . Using block vectors and matrices in this way therefore allows us to quite easily write rather complicated solvers making use of the block structure of a linear system.
* 

* 
*   [2.x.172]  VVExtracting [1.x.67]
*  Once one has computed a solution, it is often necessary to evaluate it at quadrature points, for example to evaluate nonlinear residuals for the next Newton iteration, to evaluate the finite element residual for error estimators, or to compute the right hand side for the next time step in a time dependent problem.
*  The way this is done us to again use an FEValues object to evaluate the shape functions at quadrature points, and with those also the values of a finite element function. For the example of the mixed Laplace problem above, consider the following code after solving:

* 
* [1.x.68]
* 
*  After this, the variable  [2.x.173]  is a list of vectors of a length equal to the number of quadrature points we have initialized the FEValues object with; each of these vectors has  [2.x.174]  elements containing the values of the  [2.x.175]  velocities and the one pressure at a quadrature point.
*  We can use these values to then construct other things like residuals. However, the construct is a bit awkward. First, we have a  [2.x.176] s, which always looks strange. It is also inefficient because it implies dynamic memory allocation for the outer vector as well as for all the inner vectors. Secondly, maybe we are only interested in the velocities, for example to solve an advection problem in a second stage (as, for example, in  [2.x.177]  or  [2.x.178] ). In that case, one would have to hand-extract these values like so:

* 
* [1.x.69]
*  Note how we convert from a  [2.x.179]  (which is simply a collection of vector elements) into a  [2.x.180]  because the velocity is a quantity characterized by  [2.x.181]  elements that have certain transformation properties under rotations of the coordinate system.
*  This code can be written more elegantly and efficiently using code like the following:

* 
* [1.x.70]
* 
*  As a result, we here get the velocities right away, and in the right data type (because we have described, using the extractor, that the first  [2.x.182]  components of the finite element belong together, forming a tensor). The code is also more efficient: it requires less dynamic memory allocation because the Tensor class allocates its components as member variables rather than on the heap, and we save cycles because we don't even bother computing the values of the pressure variable at the quadrature points. On the other hand, if we had been interested in only the pressure and not the velocities, then the following code extracting scalar values would have done:

* 
* [1.x.71]
* 
*  In similar cases, one sometimes needs the gradients or second derivatives of the solution, or of individual scalar or vector components. To get at those of all components of the solution, the functions  [2.x.183]  and  [2.x.184]  are the equivalent of the function  [2.x.185]  used above.
*  Likewise, to extract the gradients of scalar components,  [2.x.186]  and  [2.x.187]  do the job. For vector- (tensor-)valued quantities, there are functions  [2.x.188]  and  [2.x.189]  and in addition  [2.x.190]  and  [2.x.191] 
*  Moreover, there is a shortcut available in case when only the Laplacians of the solution (which is the trace of the hessians) is needed, usable for both scalar and vector-valued problems as  [2.x.192]  and  [2.x.193] 
* 

*   [2.x.194]  VVOutput [1.x.72]
*  As mentioned above, an FESystem object may hold multiple vector components, but it doesn't have a notion what they actually mean. As an example, take the object

* 
* [1.x.73]
*  It has  [2.x.195]  vector components, but what do they mean? Are they the  [2.x.196]  components of a velocity vector plus one pressure? Are they the pressure plus the  [2.x.197]  velocity components? Or are they a collection of scalars?
*  The point is that the FESystem class doesn't care. The [1.x.74] of what the components mean is up to the person who uses the element later, for example in assembling a linear form, or in extracting data solution components for a linearized system in the next Newton step. In almost all cases, this interpretation happens at the place where it is needed.
*  There is one case where one has to be explicit, however, and that is in generating graphical output. The reason is that many file formats for visualization want data that represents vectors (e.g. velocities, displacements, etc) to be stored separately from scalars (pressures, densities, etc), and there often is no way to group a bunch of scalars into a vector field from within a visualization program.
*  To achieve this, we need to let the DataOut class and friends know which components of the FESystem form vectors (with  [2.x.198]  components) and which are scalars. This is shown, for example, in  [2.x.199]  where we generate output as follows:

* 
* [1.x.75]
*  In other words, we here create an array of  [2.x.200]  elements in which we store which elements of the finite element are vectors and which are scalars; the array is filled with  [2.x.201]  copies of  [2.x.202]  and a single trailing element of  [2.x.203]  . The array is then given as an extra argument to  [2.x.204]  to explain how the data in the given solution vector is to be interpreted. Visualization programs like VisIt and Paraview will then offer to show these  [2.x.205]  components as vector fields, rather than as individual scalar fields.
* 

* 

* 
*  [2.x.206] 

* 
* [0.x.1]

