<!-- HTML header for doxygen 1.8.13-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="canonical" href="https://www.dealii.org/current/doxygen/deal.II/step_17.html" />
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>The deal.II Library: The step-17 tutorial program</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link rel="SHORTCUT ICON" href="deal.ico"></link>
<script type="text/javascript" src="custom.js"></script>
<meta name="author" content="The deal.II Authors <authors@dealii.org>"></meta>
<meta name="copyright" content="Copyright (C) 1998 - 2021 by the deal.II authors"></meta>
<meta name="deal.II-version" content="10.0.0-pre"></meta>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="logo200.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">
   &#160;<span id="projectnumber">Reference documentation for deal.II version 10.0.0-pre</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!--Extra macros for MathJax:-->
<div style="display:none">
\(\newcommand{\dealvcentcolon}{\mathrel{\mathop{:}}}\)
\(\newcommand{\dealcoloneq}{\dealvcentcolon\mathrel{\mkern-1.2mu}=}\)
\(\newcommand{\jump}[1]{\left[\!\left[ #1 \right]\!\right]}\)
\(\newcommand{\average}[1]{\left\{\!\left\{ #1 \right\}\!\right\}}\)
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">The step-17 tutorial program </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This tutorial depends on <a class="el" href="step_8.html">step-8</a>.</p>
<p> 
<table class="tutorial" width="50%">
<tr><th colspan="2"><b><small>Table of contents</small></b></th></tr>
<tr><td width="50%" valign="top">
<ol>
  <li> <a href="#Intro" class=bold>Introduction</a>
    <ul>
        <li><a href="#Overview">Overview</a>
        <li><a href="#ParallelizingsoftwarewithMPI">Parallelizing software with MPI</a>
        <li><a href="#Whatthisprogramdoes">What this program does</a>
    </ul>
  <li> <a href="#CommProg" class=bold>The commented program</a>
    <ul>
        <li><a href="#Includefiles">Include files</a>
        <li><a href="#ThecodeElasticProblemcodeclasstemplate">The <code>ElasticProblem</code> class template</a>
        <li><a href="#Righthandsidevalues">Right hand side values</a>
        <li><a href="#ThecodeElasticProblemcodeclassimplementation">The <code>ElasticProblem</code> class implementation</a>
      <ul>
        <li><a href="#ElasticProblemElasticProblem">ElasticProblem::ElasticProblem</a>
        <li><a href="#ElasticProblemsetup_system">ElasticProblem::setup_system</a>
        <li><a href="#ElasticProblemassemble_system">ElasticProblem::assemble_system</a>
        <li><a href="#ElasticProblemsolve">ElasticProblem::solve</a>
        <li><a href="#ElasticProblemrefine_grid">ElasticProblem::refine_grid</a>
        <li><a href="#ElasticProblemoutput_results">ElasticProblem::output_results</a>
        <li><a href="#ElasticProblemrun">ElasticProblem::run</a>
      </ul>
        <li><a href="#Thecodemaincodefunction">The <code>main</code> function</a>
      </ul>
</ol></td><td width="50%" valign="top"><ol>
  <li value="3"> <a href="#Results" class=bold>Results</a>
    <ul>
        <li><a href="#Possibilitiesforextensions">Possibilities for extensions</a>
    </ul>
  <li> <a href="#PlainProg" class=bold>The plain program</a>
</ol> </td> </tr> </table>
 <a class="anchor" id="Intro"></a> <a class="anchor" id="Introduction"></a></p><h1>Introduction</h1>
<p><a class="anchor" id="Overview"></a></p><h3>Overview</h3>
<p>This program does not introduce any new mathematical ideas; in fact, all it does is to do the exact same computations that <a class="el" href="step_8.html">step-8</a> already does, but it does so in a different manner: instead of using deal.II's own linear algebra classes, we build everything on top of classes deal.II provides that wrap around the linear algebra implementation of the <a href="http://www.mcs.anl.gov/petsc/" target="_top">PETSc</a> library. And since PETSc allows to distribute matrices and vectors across several computers within an MPI network, the resulting code will even be able to solve the problem in parallel. If you don't know what PETSc is, then this would be a good time to take a quick glimpse at their homepage.</p>
<p>As a prerequisite of this program, you need to have PETSc installed, and if you want to run in parallel on a cluster, you also need <a href="http://www-users.cs.umn.edu/~karypis/metis/index.html" target="_top">METIS</a> to partition meshes. The installation of deal.II together with these two additional libraries is described in the <a href="../../readme.html" target="body">README</a> file.</p>
<p>Now, for the details: as mentioned, the program does not compute anything new, so the use of finite element classes, etc., is exactly the same as before. The difference to previous programs is that we have replaced almost all uses of classes <code><a class="el" href="classVector.html">Vector</a></code> and <code><a class="el" href="classSparseMatrix.html">SparseMatrix</a></code> by their near-equivalents <code><a class="el" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a></code> and <code><a class="el" href="classPETScWrappers_1_1MPI_1_1SparseMatrix.html">PETScWrappers::MPI::SparseMatrix</a></code> that store data in a way so that every processor in the MPI network only stores a part of the matrix or vector. More specifically, each processor will only store those rows of the matrix that correspond to a degree of freedom it "owns". For vectors, they either store only elements that correspond to degrees of freedom the processor owns (this is what is necessary for the right hand side), or also some additional elements that make sure that every processor has access the solution components that live on the cells the processor owns (so-called <a class="el" href="DEALGlossary.html#GlossLocallyActiveDof">locally active DoFs</a>) or also on neighboring cells (so-called <a class="el" href="DEALGlossary.html#GlossLocallyRelevantDof">locally relevant DoFs</a>).</p>
<p>The interface the classes from the PETScWrapper namespace provide is very similar to that of the deal.II linear algebra classes, but instead of implementing this functionality themselves, they simply pass on to their corresponding PETSc functions. The wrappers are therefore only used to give PETSc a more modern, object oriented interface, and to make the use of PETSc and deal.II objects as interchangeable as possible. The main point of using PETSc is that it can run in parallel. We will make use of this by partitioning the domain into as many blocks ("subdomains") as there are processes in the MPI network. At the same time, PETSc also provides dummy MPI stubs, so you can run this program on a single machine if PETSc was configured without MPI.</p>
<p><a class="anchor" id="ParallelizingsoftwarewithMPI"></a></p><h3>Parallelizing software with MPI</h3>
<p>Developing software to run in parallel via MPI requires a bit of a change in mindset because one typically has to split up all data structures so that every processor only stores a piece of the entire problem. As a consequence, you can't typically access all components of a solution vector on each processor &ndash; each processor may simply not have enough memory to hold the entire solution vector. Because data is split up or "distributed" across processors, we call the programming model used by MPI "distributed memory
computing" (as opposed to "shared memory computing", which would mean that multiple processors can all access all data within one memory space, for example whenever multiple cores in a single machine work on a common task). Some of the fundamentals of distributed memory computing are discussed in the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using distributed memory</a> documentation module, which is itself a sub-module of the <a class="el" href="group__Parallel.html">Parallel computing</a> module.</p>
<p>In general, to be truly able to scale to large numbers of processors, one needs to split between the available processors <em>every</em> data structure whose size scales with the size of the overall problem. (For a definition of what it means for a program to "scale", see <a class="el" href="DEALGlossary.html#GlossParallelScaling">this glossary entry</a>.) This includes, for example, the triangulation, the matrix, and all global vectors (solution, right hand side). If one doesn't split all of these objects, one of those will be replicated on all processors and will eventually simply become too large if the problem size (and the number of available processors) becomes large. (On the other hand, it is completely fine to keep objects with a size that is independent of the overall problem size on every processor. For example, each copy of the executable will create its own finite element object, or the local matrix we use in the assembly.)</p>
<p>In the current program (as well as in the related <a class="el" href="step_18.html">step-18</a>), we will not go quite this far but present a gentler introduction to using MPI. More specifically, the only data structures we will parallelize are matrices and vectors. We do, however, not split up the <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> classes: each process still has a complete copy of these objects, and all processes have exact copies of what the other processes have. We will then simply have to mark, in each copy of the triangulation on each of the processors, which processor owns which cells. This process is called "partitioning" a mesh into <a class="el" href="DEALGlossary.html#GlossSubdomainId">subdomains</a>.</p>
<p>For larger problems, having to store the <em>entire</em> mesh on every processor will clearly yield a bottleneck. Splitting up the mesh is slightly, though not much more complicated (from a user perspective, though it is <em>much</em> more complicated under the hood) to achieve and we will show how to do this in <a class="el" href="step_40.html">step-40</a> and some other programs. There are numerous occasions where, in the course of discussing how a function of this program works, we will comment on the fact that it will not scale to large problems and why not. All of these issues will be addressed in <a class="el" href="step_18.html">step-18</a> and in particular <a class="el" href="step_40.html">step-40</a>, which scales to very large numbers of processes.</p>
<p>Philosophically, the way MPI operates is as follows. You typically run a program via </p><div class="fragment"><div class="line">mpirun -np 32 ./step-17</div></div><!-- fragment --><p> which means to run it on (say) 32 processors. (If you are on a cluster system, you typically need to <em>schedule</em> the program to run whenever 32 processors become available; this will be described in the documentation of your cluster. But under the hood, whenever those processors become available, the same call as above will generally be executed.) What this does is that the MPI system will start 32 <em>copies</em> of the <code><a class="el" href="step_17.html">step-17</a></code> executable. (The MPI term for each of these running executables is that you have 32 <a class="el" href="DEALGlossary.html#GlossMPIProcess">MPI processes</a>.) This may happen on different machines that can't even read from each others' memory spaces, or it may happen on the same machine, but the end result is the same: each of these 32 copies will run with some memory allocated to it by the operating system, and it will not directly be able to read the memory of the other 31 copies. In order to collaborate in a common task, these 32 copies then have to <em>communicate</em> with each other. MPI, short for <em>Message Passing Interface</em>, makes this possible by allowing programs to <em>send messages</em>. You can think of this as the mail service: you can put a letter to a specific address into the mail and it will be delivered. But that's the extent to which you can control things. If you want the receiver to do something with the content of the letter, for example return to you data you want from over there, then two things need to happen: (i) the receiver needs to actually go check whether there is anything in their mailbox, and (ii) if there is, react appropriately, for example by sending data back. If you wait for this return message but the original receiver was distracted and not paying attention, then you're out of luck: you'll simply have to wait until your requested over there will be worked on. In some cases, bugs will lead the original receiver to never check your mail, and in that case you will wait forever &ndash; this is called a <em>deadlock</em>. (See also <a href="http://www.math.colostate.edu/~bangerth/videos.676.39.html">video lecture 39</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.html">video lecture 41</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.25.html">video lecture 41.25</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.5.html">video lecture 41.5</a>.)</p>
<p>In practice, one does not usually program at the level of sending and receiving individual messages, but uses higher level operations. For example, in the program we will use function calls that take a number from each processor, add them all up, and return the sum to all processors. Internally, this is implemented using individual messages, but to the user this is transparent. We call such operations <em>collectives</em> because <em>all</em> processors participate in them. Collectives allow us to write programs where not every copy of the executable is doing something completely different (this would be incredibly difficult to program) but where in essence all copies are doing the same thing (though on different data) for themselves, running through the same blocks of code; then they communicate data through collectives; and then go back to doing something for themselves again running through the same blocks of data. This is the key piece to being able to write programs, and it is the key component to making sure that programs can run on any number of processors, since we do not have to write different code for each of the participating processors.</p>
<p>(This is not to say that programs are never written in ways where different processors run through different blocks of code in their copy of the executable. Programs internally also often communicate in other ways than through collectives. But in practice, parallel finite element codes almost always follow the scheme where every copy of the program runs through the same blocks of code at the same time, interspersed by phases where all processors communicate with each other.)</p>
<p>In reality, even the level of calling MPI collective functions is too low. Rather, the program below will not contain any direct calls to MPI at all, but only deal.II functions that hide this communication from users of the deal.II. This has the advantage that you don't have to learn the details of MPI and its rather intricate function calls. That said, you do have to understand the general philosophy behind MPI as outlined above.</p>
<p><a class="anchor" id="Whatthisprogramdoes"></a></p><h3>What this program does</h3>
<p>The techniques this program then demonstrates are:</p><ul>
<li>How to use the PETSc wrapper classes; this will already be visible in the declaration of the principal class of this program, <code>ElasticProblem</code>.</li>
<li>How to partition the mesh into subdomains; this happens in the <code>ElasticProblem::setup_system()</code> function.</li>
<li>How to parallelize operations for jobs running on an MPI network; here, this is something one has to pay attention to in a number of places, most notably in the <code>ElasticProblem::assemble_system()</code> function.</li>
<li>How to deal with vectors that store only a subset of vector entries and for which we have to ensure that they store what we need on the current processors. See for example the <code>ElasticProblem::solve()</code> and <code>ElasticProblem::refine_grid()</code> functions.</li>
<li>How to deal with status output from programs that run on multiple processors at the same time. This is done via the <code>pcout</code> variable in the program, initialized in the constructor.</li>
</ul>
<p>Since all this can only be demonstrated using actual code, let us go straight to the code without much further ado.</p>
<p><a class="anchor" id="CommProg"></a> </p><h1>The commented program</h1>
<p><a class="anchor" id="Includefiles"></a> </p><h3>Include files</h3>
<p>First the usual assortment of header files we have already used in previous example programs:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="quadrature__lib_8h.html">deal.II/base/quadrature_lib.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="function_8h.html">deal.II/base/function.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="logstream_8h.html">deal.II/base/logstream.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="multithread__info_8h.html">deal.II/base/multithread_info.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8h.html">deal.II/lac/vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="full__matrix_8h.html">deal.II/lac/full_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="affine__constraints_8h.html">deal.II/lac/affine_constraints.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dynamic__sparsity__pattern_8h.html">deal.II/lac/dynamic_sparsity_pattern.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="sparsity__tools_8h.html">deal.II/lac/sparsity_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid_2tria_8h.html">deal.II/grid/tria.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__generator_8h.html">deal.II/grid/grid_generator.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid_2grid__refinement_8h.html">deal.II/grid/grid_refinement.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dofs_2dof__handler_8h.html">deal.II/dofs/dof_handler.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__tools_8h.html">deal.II/dofs/dof_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe_2fe__values_8h.html">deal.II/fe/fe_values.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__system_8h.html">deal.II/fe/fe_system.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__q_8h.html">deal.II/fe/fe_q.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector__tools_8h.html">deal.II/numerics/vector_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="matrix__tools_8h.html">deal.II/numerics/matrix_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="numerics_2data__out_8h.html">deal.II/numerics/data_out.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="error__estimator_8h.html">deal.II/numerics/error_estimator.h</a>&gt;</span></div></div><!-- fragment --><p>And here come the things that we need particularly for this example program and that weren't in <a class="el" href="step_8.html">step-8</a>. First, we replace the standard output <code>std::cout</code> by a new stream <code>pcout</code> which is used in parallel computations for generating output only on one of the MPI processes.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="conditional__ostream_8h.html">deal.II/base/conditional_ostream.h</a>&gt;</span></div></div><!-- fragment --><p>We are going to query the number of processes and the number of the present process by calling the respective functions in the <a class="el" href="namespaceUtilities_1_1MPI.html">Utilities::MPI</a> namespace.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="mpi_8h.html">deal.II/base/mpi.h</a>&gt;</span></div></div><!-- fragment --><p>Then, we are going to replace all linear algebra components that involve the (global) linear system by classes that wrap interfaces similar to our own linear algebra classes around what PETSc offers (PETSc is a library written in C, and deal.II comes with wrapper classes that provide the PETSc functionality with an interface that is similar to the interface we already had for our own linear algebra classes). In particular, we need vectors and matrices that are distributed across several <a class="el" href="DEALGlossary.html#GlossMPIProcess">processes</a> in MPI programs (and simply map to sequential, local vectors and matrices if there is only a single process, i.e., if you are running on only one machine, and without MPI support):</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__vector_8h.html">deal.II/lac/petsc_vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__sparse__matrix_8h.html">deal.II/lac/petsc_sparse_matrix.h</a>&gt;</span></div></div><!-- fragment --><p>Then we also need interfaces for solvers and preconditioners that PETSc provides:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__solver_8h.html">deal.II/lac/petsc_solver.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__precondition_8h.html">deal.II/lac/petsc_precondition.h</a>&gt;</span></div></div><!-- fragment --><p>And in addition, we need some algorithms for partitioning our meshes so that they can be efficiently distributed across an MPI network. The partitioning algorithm is implemented in the <code><a class="el" href="namespaceGridTools.html">GridTools</a></code> namespace, and we need an additional include file for a function in <code><a class="el" href="namespaceDoFRenumbering.html">DoFRenumbering</a></code> that allows to sort the indices associated with degrees of freedom so that they are numbered according to the subdomain they are associated with:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__tools_8h.html">deal.II/grid/grid_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__renumbering_8h.html">deal.II/dofs/dof_renumbering.h</a>&gt;</span></div></div><!-- fragment --><p>And this is simply C++ again:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div></div><!-- fragment --><p>The last step is as in all previous programs:</p>
<div class="fragment"><div class="line"><span class="keyword">namespace </span>Step17</div><div class="line">{</div><div class="line">  <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div></div><!-- fragment --><p><a class="anchor" id="ThecodeElasticProblemcodeclasstemplate"></a> </p><h3>The <code>ElasticProblem</code> class template</h3>
<p>The first real part of the program is the declaration of the main class. As mentioned in the introduction, almost all of this has been copied verbatim from <a class="el" href="step_8.html">step-8</a>, so we only comment on the few differences between the two tutorials. There is one (cosmetic) change in that we let <code>solve</code> return a value, namely the number of iterations it took to converge, so that we can output this to the screen at the appropriate place.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keyword">class </span>ElasticProblem</div><div class="line">{</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  ElasticProblem();</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">run</a>();</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line">  <span class="keywordtype">void</span>         setup_system();</div><div class="line">  <span class="keywordtype">void</span>         assemble_system();</div><div class="line">  <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> solve();</div><div class="line">  <span class="keywordtype">void</span>         refine_grid();</div><div class="line">  <span class="keywordtype">void</span>         output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div></div><!-- fragment --><p>The first change is that we have to declare a variable that indicates the <a class="el" href="DEALGlossary.html#GlossMPICommunicator">MPI communicator</a> over which we are supposed to distribute our computations.</p>
<div class="fragment"><div class="line"><a class="code" href="classMPI__Comm.html">MPI_Comm</a> mpi_communicator;</div></div><!-- fragment --><p>Then we have two variables that tell us where in the parallel world we are. The first of the following variables, <code>n_mpi_processes</code>, tells us how many MPI processes there exist in total, while the second one, <code>this_mpi_process</code>, indicates which is the number of the present process within this space of processes (in MPI language, this corresponds to the <a class="el" href="DEALGlossary.html#GlossMPIRank">rank</a> of the process). The latter will have a unique value for each process between zero and (less than) <code>n_mpi_processes</code>. If this program is run on a single machine without MPI support, then their values are <code>1</code> and <code>0</code>, respectively.</p>
<div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>;</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>;</div></div><!-- fragment --><p>Next up is a stream-like variable <code>pcout</code>. It is, in essence, just something we use for convenience: in a parallel program, if each process outputs status information, then there quickly is a lot of clutter. Rather, we would want to only have one <a class="el" href="DEALGlossary.html#GlossMPIProcess">process</a> output everything once, for example the one with <a class="el" href="DEALGlossary.html#GlossMPIRank">rank</a> zero. At the same time, it seems silly to prefix <em>every</em> place where we create output with an <code>if (my_rank==0)</code> condition.</p>
<p>To make this simpler, the <a class="el" href="classConditionalOStream.html">ConditionalOStream</a> class does exactly this under the hood: it acts as if it were a stream, but only forwards to a real, underlying stream if a flag is set. By setting this condition to <code>this_mpi_process==0</code> (where <code>this_mpi_process</code> corresponds to the rank of an MPI process), we make sure that output is only generated from the first process and that we don't get the same lines of output over and over again, once per process. Thus, we can use <code>pcout</code> everywhere and in every process, but on all but one process nothing will ever happen to the information that is piped into the object via <code>operator&lt;&lt;</code>.</p>
<div class="fragment"><div class="line"><a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div></div><!-- fragment --><p>The remainder of the list of member variables is fundamentally the same as in <a class="el" href="step_8.html">step-8</a>. However, we change the declarations of matrix and vector types to use parallel PETSc objects instead. Note that we do not use a separate sparsity pattern, since PETSc manages this internally as part of its matrix data structures.</p>
<div class="fragment"><div class="line">  <a class="code" href="classTriangulation.html">Triangulation&lt;dim&gt;</a> <a class="code" href="p4est__wrappers_8cc.html#ace00f2f80d9780ef9aa1007e1c22c6a4">triangulation</a>;</div><div class="line">  <a class="code" href="classFESystem.html">FESystem&lt;dim&gt;</a>      fe;</div><div class="line">  <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a>    dof_handler;</div><div class="line"></div><div class="line">  <a class="code" href="classAffineConstraints.html">AffineConstraints&lt;double&gt;</a> hanging_node_constraints;</div><div class="line"></div><div class="line">  <a class="code" href="classPETScWrappers_1_1MPI_1_1SparseMatrix.html">PETScWrappers::MPI::SparseMatrix</a> system_matrix;</div><div class="line"></div><div class="line">  <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> solution;</div><div class="line">  <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> system_rhs;</div><div class="line">};</div></div><!-- fragment --><p><a class="anchor" id="Righthandsidevalues"></a> </p><h3>Right hand side values</h3>
<p>The following is taken from <a class="el" href="step_8.html">step-8</a> without change:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keyword">class </span>RightHandSide : <span class="keyword">public</span> <a class="code" href="classFunction.html">Function</a>&lt;dim&gt;</div><div class="line">{</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  <span class="keyword">virtual</span> <span class="keywordtype">void</span> <a class="code" href="classFunction.html#ae316ebc05d21989d573024f8a23c49cb">vector_value</a>(<span class="keyword">const</span> <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &amp;p,</div><div class="line">                            <a class="code" href="classVector.html">Vector&lt;double&gt;</a> &amp;  values)<span class="keyword"> const override</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(values.<a class="code" href="classVector.html#a81dcfa5c77bdd426603386c0844149ae">size</a>() == dim, <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a>(values.<a class="code" href="classVector.html#a81dcfa5c77bdd426603386c0844149ae">size</a>(), dim));</div><div class="line">    <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(dim &gt;= 2, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line"></div><div class="line">    <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> point_1, point_2;</div><div class="line">    point_1(0) = 0.5;</div><div class="line">    point_2(0) = -0.5;</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (((p - point_1).norm_square() &lt; 0.2 * 0.2) ||</div><div class="line">        ((p - point_2).norm_square() &lt; 0.2 * 0.2))</div><div class="line">      <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(0) = 1;</div><div class="line">    <span class="keywordflow">else</span></div><div class="line">      <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(0) = 0;</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (p.<a class="code" href="classPoint.html#a859ea7f3bf3e64be2e0f5ed1bfcc8550">square</a>() &lt; 0.2 * 0.2)</div><div class="line">      <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(1) = 1;</div><div class="line">    <span class="keywordflow">else</span></div><div class="line">      <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(1) = 0;</div><div class="line">  }</div><div class="line"></div><div class="line">  <span class="keyword">virtual</span> <span class="keywordtype">void</span></div><div class="line">  <a class="code" href="classFunction.html#aa041dde994d40c068e00661197ac75a6">vector_value_list</a>(<span class="keyword">const</span> std::vector&lt;<a class="code" href="classPoint.html">Point&lt;dim&gt;</a>&gt; &amp;points,</div><div class="line">                    std::vector&lt;<a class="code" href="classVector.html">Vector&lt;double&gt;</a>&gt; &amp;  value_list)<span class="keyword"> const override</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_points = points.size();</div><div class="line"></div><div class="line">    <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(value_list.size() == n_points,</div><div class="line">           <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a>(value_list.size(), n_points));</div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p = 0; p &lt; n_points; ++p)</div><div class="line">      RightHandSide&lt;dim&gt;::vector_value(points[p], value_list[p]);</div><div class="line">  }</div><div class="line">};</div></div><!-- fragment --><p><a class="anchor" id="ThecodeElasticProblemcodeclassimplementation"></a> </p><h3>The <code>ElasticProblem</code> class implementation</h3>
<p><a class="anchor" id="ElasticProblemElasticProblem"></a> </p><h4>ElasticProblem::ElasticProblem</h4>
<p>The first step in the actual implementation is the constructor of the main class. Apart from initializing the same member variables that we already had in <a class="el" href="step_8.html">step-8</a>, we here initialize the MPI communicator variable we shall use with the global MPI communicator linking all processes together (in more complex applications, one could here use a communicator object that only links a subset of all processes), and call the <a class="el" href="namespaceUtilities_1_1MPI.html">Utilities::MPI</a> helper functions to determine the number of processes and where the present one fits into this picture. In addition, we make sure that output is only generated by the (globally) first process. We do so by passing the stream we want to output to (<code>std::cout</code>) and a true/false flag as arguments where the latter is determined by testing whether the process currently executing the constructor call is the first in the MPI universe.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">ElasticProblem&lt;dim&gt;::ElasticProblem()</div><div class="line">  : mpi_communicator(MPI_COMM_WORLD)</div><div class="line">  , <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(mpi_communicator))</div><div class="line">  , <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator))</div><div class="line">  , pcout(<a class="code" href="namespacestd.html">std</a>::cout, (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0))</div><div class="line">  , fe(<a class="code" href="classFE__Q.html">FE_Q</a>&lt;dim&gt;(1), dim)</div><div class="line">  , dof_handler(triangulation)</div><div class="line">{}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemsetup_system"></a> </p><h4>ElasticProblem::setup_system</h4>
<p>Next, the function in which we set up the various variables for the global linear system to be solved needs to be implemented.</p>
<p>However, before we proceed with this, there is one thing to do for a parallel program: we need to determine which MPI process is responsible for each of the cells. Splitting cells among processes, commonly called "partitioning the mesh", is done by assigning a <a class="el" href="DEALGlossary.html#GlossSubdomainId">subdomain id</a> to each cell. We do so by calling into the METIS library that does this in a very efficient way, trying to minimize the number of nodes on the interfaces between subdomains. Rather than trying to call METIS directly, we do this by calling the <a class="el" href="namespaceGridTools.html#a99eba8e3b388258eda37a2724579dd1d">GridTools::partition_triangulation()</a> function that does this at a much higher level of programming.</p>
<dl class="section note"><dt>Note</dt><dd>As mentioned in the introduction, we could avoid this manual partitioning step if we used the <a class="el" href="classparallel_1_1shared_1_1Triangulation.html">parallel::shared::Triangulation</a> class for the triangulation object instead (as we do in <a class="el" href="step_18.html">step-18</a>). That class does, in essence, everything a regular triangulation does, but it then also automatically partitions the mesh after every mesh creation or refinement operation.</dd></dl>
<p>Following partitioning, we need to enumerate all degrees of freedom as usual. However, we would like to enumerate the degrees of freedom in a way so that all degrees of freedom associated with cells in subdomain zero (which resides on process zero) come before all DoFs associated with cells on subdomain one, before those on cells on process two, and so on. We need this since we have to split the global vectors for right hand side and solution, as well as the matrix into contiguous chunks of rows that live on each of the processors, and we will want to do this in a way that requires minimal communication. This particular enumeration can be obtained by re-ordering degrees of freedom indices using <a class="el" href="namespaceDoFRenumbering.html#a166d0082503ec2b8d9c560222b48f9d2">DoFRenumbering::subdomain_wise()</a>.</p>
<p>The final step of this initial setup is that we get ourselves an <a class="el" href="classIndexSet.html">IndexSet</a> that indicates the subset of the global number of unknowns this process is responsible for. (Note that a degree of freedom is not necessarily owned by the process that owns a cell just because the degree of freedom lives on this cell: some degrees of freedom live on interfaces between subdomains, and are consequently only owned by one of the processes adjacent to this interface.)</p>
<p>Before we move on, let us recall a fact already discussed in the introduction: The triangulation we use here is replicated across all processes, and each process has a complete copy of the entire triangulation, with all cells. Partitioning only provides a way to identify which cells out of all each process "owns", but it knows everything about all of them. Likewise, the <a class="el" href="classDoFHandler.html">DoFHandler</a> object knows everything about every cell, in particular the degrees of freedom that live on each cell, whether it is one that the current process owns or not. This can not scale to large problems because eventually just storing the entire mesh, and everything that is associated with it, on every process will become infeasible if the problem is large enough. On the other hand, if we split the triangulation into parts so that every process stores only those cells it "owns" but nothing else (or, at least a sufficiently small fraction of everything else), then we can solve large problems if only we throw a large enough number of MPI processes at them. This is what we are going to in <a class="el" href="step_40.html">step-40</a>, for example, using the <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> class. On the other hand, most of the rest of what we demonstrate in the current program will actually continue to work whether we have the entire triangulation available, or only a piece of it.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::setup_system()</div><div class="line">{</div><div class="line">  <a class="code" href="namespaceGridTools.html#a99eba8e3b388258eda37a2724579dd1d">GridTools::partition_triangulation</a>(<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>, triangulation);</div><div class="line"></div><div class="line">  dof_handler.<a class="code" href="classDoFHandler.html#a553ca864aaf70330d9be86bc78f36d1e">distribute_dofs</a>(fe);</div><div class="line">  <a class="code" href="namespaceDoFRenumbering.html#a166d0082503ec2b8d9c560222b48f9d2">DoFRenumbering::subdomain_wise</a>(dof_handler);</div></div><!-- fragment --><p>We need to initialize the objects denoting hanging node constraints for the present grid. As with the triangulation and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects, we will simply store <em>all</em> constraints on each process; again, this will not scale, but we show in <a class="el" href="step_40.html">step-40</a> how one can work around this by only storing on each MPI process the constraints for degrees of freedom that actually matter on this particular process.</p>
<div class="fragment"><div class="line">hanging_node_constraints.clear();</div><div class="line"><a class="code" href="group__constraints.html#ga3b4ea7dfd313e388d868c4e4aa685799">DoFTools::make_hanging_node_constraints</a>(dof_handler,</div><div class="line">                                        hanging_node_constraints);</div><div class="line">hanging_node_constraints.close();</div></div><!-- fragment --><p>Now we create the sparsity pattern for the system matrix. Note that we again compute and store all entries and not only the ones relevant to this process (see <a class="el" href="step_18.html">step-18</a> or <a class="el" href="step_40.html">step-40</a> for a more efficient way to handle this).</p>
<div class="fragment"><div class="line"><a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>(), dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>());</div><div class="line"><a class="code" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a>(dof_handler,</div><div class="line">                                dsp,</div><div class="line">                                hanging_node_constraints,</div><div class="line">                                <span class="keyword">false</span>);</div></div><!-- fragment --><p>Now we determine the set of locally owned DoFs and use that to initialize parallel vectors and matrix. Since the matrix and vectors need to work in parallel, we have to pass them an MPI communication object, as well as information about the partitioning contained in the <a class="el" href="classIndexSet.html">IndexSet</a> <code>locally_owned_dofs</code>. The <a class="el" href="classIndexSet.html">IndexSet</a> contains information about the global size (the <em>total</em> number of degrees of freedom) and also what subset of rows is to be stored locally. Note that the system matrix needs that partitioning information for the rows and columns. For square matrices, as it is the case here, the columns should be partitioned in the same way as the rows, but in the case of rectangular matrices one has to partition the columns in the same way as vectors are partitioned with which the matrix is multiplied, while rows have to partitioned in the same way as destination vectors of matrix-vector multiplications:</p>
<div class="fragment"><div class="line">  <span class="keyword">const</span> std::vector&lt;IndexSet&gt; locally_owned_dofs_per_proc =</div><div class="line">    <a class="code" href="namespaceDoFTools.html#a180d7118ecf27d72afbfecb7978c5e09">DoFTools::locally_owned_dofs_per_subdomain</a>(dof_handler);</div><div class="line">  <span class="keyword">const</span> <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs =</div><div class="line">    locally_owned_dofs_per_proc[<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>];</div><div class="line"></div><div class="line">  system_matrix.reinit(locally_owned_dofs,</div><div class="line">                       locally_owned_dofs,</div><div class="line">                       dsp,</div><div class="line">                       mpi_communicator);</div><div class="line"></div><div class="line">  solution.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line">  system_rhs.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemassemble_system"></a> </p><h4>ElasticProblem::assemble_system</h4>
<p>We now assemble the matrix and right hand side of the problem. There are some things worth mentioning before we go into detail. First, we will be assembling the system in parallel, i.e., each process will be responsible for assembling on cells that belong to this particular process. Note that the degrees of freedom are split in a way such that all DoFs in the interior of cells and between cells belonging to the same subdomain belong to the process that <code>owns</code> the cell. However, even then we sometimes need to assemble on a cell with a neighbor that belongs to a different process, and in these cases when we add up the local contributions into the global matrix or right hand side vector, we have to transfer these entries to the process that owns these elements. Fortunately, we don't have to do this by hand: PETSc does all this for us by caching these elements locally, and sending them to the other processes as necessary when we call the <code><a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a></code> functions on the matrix and vector at the end of this function.</p>
<p>The second point is that once we have handed over matrix and vector contributions to PETSc, it is a) hard, and b) very inefficient to get them back for modifications. This is not only the fault of PETSc, it is also a consequence of the distributed nature of this program: if an entry resides on another processor, then it is necessarily expensive to get it. The consequence of this is that we should not try to first assemble the matrix and right hand side as if there were no hanging node constraints and boundary values, and then eliminate these in a second step (using, for example, <a class="el" href="classAffineConstraints.html#a5a1bc1bb2d705b582889ebaa24bcae5c">AffineConstraints::condense()</a>). Rather, we should try to eliminate hanging node constraints before handing these entries over to PETSc. This is easy: instead of copying elements by hand into the global matrix (as we do in <a class="el" href="step_4.html">step-4</a>), we use the <a class="el" href="classAffineConstraints.html#a373fbdacd8c486e675b8d2bff8943192">AffineConstraints::distribute_local_to_global()</a> functions to take care of hanging nodes at the same time. We also already did this in <a class="el" href="step_6.html">step-6</a>. The second step, elimination of boundary nodes, could also be done this way by putting the boundary values into the same <a class="el" href="classAffineConstraints.html">AffineConstraints</a> object as hanging nodes (see the way it is done in <a class="el" href="step_6.html">step-6</a>, for example); however, it is not strictly necessary to do this here because eliminating boundary values can be done with only the data stored on each process itself, and consequently we use the approach used before in <a class="el" href="step_4.html">step-4</a>, i.e., via <a class="el" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values()</a>.</p>
<p>All of this said, here is the actual implementation starting with the general setup of helper variables. (Note that we still use the deal.II full matrix and vector types for the local systems as these are small and need not be shared across processes.)</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::assemble_system()</div><div class="line">{</div><div class="line">  <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a>   quadrature_formula(fe.<a class="code" href="classFiniteElementData.html#a2cbf5ad6b464871261dbd054bced18a8">degree</a> + 1);</div><div class="line">  <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values(fe,</div><div class="line">                          quadrature_formula,</div><div class="line">                          <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                            <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> dofs_per_cell = fe.<a class="code" href="classFiniteElementData.html#a33b522422da89e5c080e7405ad49d7c7">n_dofs_per_cell</a>();</div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_q_points    = quadrature_formula.size();</div><div class="line"></div><div class="line">  <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a> <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(dofs_per_cell, dofs_per_cell);</div><div class="line">  <a class="code" href="classVector.html">Vector&lt;double&gt;</a>     cell_rhs(dofs_per_cell);</div><div class="line"></div><div class="line">  std::vector&lt;types::global_dof_index&gt; local_dof_indices(dofs_per_cell);</div><div class="line"></div><div class="line">  std::vector&lt;double&gt; lambda_values(n_q_points);</div><div class="line">  std::vector&lt;double&gt; mu_values(n_q_points);</div><div class="line"></div><div class="line">  <a class="code" href="classFunctions_1_1ConstantFunction.html">Functions::ConstantFunction&lt;dim&gt;</a> <a class="code" href="namespaceDifferentiation_1_1SD.html#a841a7b84dc17bf2ba675522093a97e8ba945f3fc449518a73b9f5f32868db466c">lambda</a>(1.), <a class="code" href="namespacemu.html">mu</a>(1.);</div><div class="line"></div><div class="line">  RightHandSide&lt;dim&gt;          right_hand_side;</div><div class="line">  std::vector&lt;Vector&lt;double&gt;&gt; rhs_values(n_q_points, <a class="code" href="classVector.html">Vector&lt;double&gt;</a>(dim));</div></div><!-- fragment --><p>The next thing is the loop over all elements. Note that we do not have to do <em>all</em> the work on every process: our job here is only to assemble the system on cells that actually belong to this MPI process, all other cells will be taken care of by other processes. This is what the if-clause immediately after the for-loop takes care of: it queries the subdomain identifier of each cell, which is a number associated with each cell that tells us about the owner process. In more generality, the subdomain id is used to split a domain into several parts (we do this above, at the beginning of <code>setup_system()</code>), and which allows to identify which subdomain a cell is living on. In this application, we have each process handle exactly one subdomain, so we identify the terms <code>subdomain</code> and <code>MPI process</code>.</p>
<p>Apart from this, assembling the local system is relatively uneventful if you have understood how this is done in <a class="el" href="step_8.html">step-8</a>. As mentioned above, distributing local contributions into the global matrix and right hand sides also takes care of hanging node constraints in the same way as is done in <a class="el" href="step_6.html">step-6</a>.</p>
<div class="fragment"><div class="line"><span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;cell : dof_handler.<a class="code" href="group__CPP11.html#gacdb6d3adec96a13a7079f6c893e1d3ff">active_cell_iterators</a>())</div><div class="line">  <span class="keywordflow">if</span> (cell-&gt;subdomain_id() == <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>)</div><div class="line">    {</div><div class="line">      cell_matrix = 0;</div><div class="line">      cell_rhs    = 0;</div><div class="line"></div><div class="line">      fe_values.<a class="code" href="classFEValues.html#a21f914e63d588e2652a9514620653d77">reinit</a>(cell);</div><div class="line"></div><div class="line">      <a class="code" href="namespaceDifferentiation_1_1SD.html#a841a7b84dc17bf2ba675522093a97e8ba945f3fc449518a73b9f5f32868db466c">lambda</a>.value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(), lambda_values);</div><div class="line">      <a class="code" href="namespacemu.html">mu</a>.value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(), mu_values);</div><div class="line"></div><div class="line">      <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">        {</div><div class="line">          <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_i =</div><div class="line">            fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j = 0; j &lt; dofs_per_cell; ++j)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_j =</div><div class="line">                fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(j).first;</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points;</div><div class="line">                   ++q_point)</div><div class="line">                {</div><div class="line">                  <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(i, j) +=</div><div class="line">                    ((fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)[component_i] *</div><div class="line">                      fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)[component_j] *</div><div class="line">                      lambda_values[q_point]) +</div><div class="line">                     (fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)[component_j] *</div><div class="line">                      fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)[component_i] *</div><div class="line">                      mu_values[q_point]) +</div><div class="line">                     ((component_i == component_j) ?</div><div class="line">                        (fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point) *</div><div class="line">                         fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point) *</div><div class="line">                         mu_values[q_point]) :</div><div class="line">                        0)) *</div><div class="line">                    fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">                }</div><div class="line">            }</div><div class="line">        }</div><div class="line"></div><div class="line">      right_hand_side.vector_value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(),</div><div class="line">                                        rhs_values);</div><div class="line">      <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">        {</div><div class="line">          <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_i =</div><div class="line">            fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points; ++q_point)</div><div class="line">            cell_rhs(i) += fe_values.<a class="code" href="classFEValuesBase.html#a1dd48cb744013c448d57f8f77640c08d">shape_value</a>(i, q_point) *</div><div class="line">                           rhs_values[q_point](component_i) *</div><div class="line">                           fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">        }</div><div class="line"></div><div class="line">      cell-&gt;get_dof_indices(local_dof_indices);</div><div class="line">      hanging_node_constraints.distribute_local_to_global(cell_matrix,</div><div class="line">                                                          cell_rhs,</div><div class="line">                                                          local_dof_indices,</div><div class="line">                                                          system_matrix,</div><div class="line">                                                          system_rhs);</div><div class="line">    }</div></div><!-- fragment --><p>The next step is to "compress" the vector and the system matrix. This means that each process sends the additions that were made to those entries of the matrix and vector that the process did not own itself to the process that owns them. After receiving these additions from other processes, each process then adds them to the values it already has. These additions are combining the integral contributions of shape functions living on several cells just as in a serial computation, with the difference that the cells are assigned to different processes.</p>
<div class="fragment"><div class="line">system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div></div><!-- fragment --><p>The global matrix and right hand side vectors have now been formed. We still have to apply boundary values, in the same way as we did, for example, in <a class="el" href="step_3.html">step-3</a>, <a class="el" href="step_4.html">step-4</a>, and a number of other programs.</p>
<p>The last argument to the call to <a class="el" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values()</a> below allows for some optimizations. It controls whether we should also delete entries (i.e., set them to zero) in the matrix columns corresponding to boundary nodes, or to keep them (and passing <code>true</code> means: yes, do eliminate the columns). If we do eliminate columns, then the resulting matrix will be symmetric again if it was before; if we don't, then it won't. The solution of the resulting system should be the same, though. The only reason why we may want to make the system symmetric again is that we would like to use the CG method, which only works with symmetric matrices. The reason why we may <em>not</em> want to make the matrix symmetric is because this would require us to write into column entries that actually reside on other processes, i.e., it involves communicating data. This is always expensive.</p>
<p>Experience tells us that CG also works (and works almost as well) if we don't remove the columns associated with boundary nodes, which can be explained by the special structure of this particular non-symmetry. To avoid the expense of communication, we therefore do not eliminate the entries in the affected columns.</p>
<div class="fragment"><div class="line">  std::map&lt;types::global_dof_index, double&gt; boundary_values;</div><div class="line">  <a class="code" href="namespaceVectorTools.html#af27ac28c698a9ed0199faed50a204538">VectorTools::interpolate_boundary_values</a>(dof_handler,</div><div class="line">                                           0,</div><div class="line">                                           <a class="code" href="classFunctions_1_1ZeroFunction.html">Functions::ZeroFunction&lt;dim&gt;</a>(dim),</div><div class="line">                                           boundary_values);</div><div class="line">  <a class="code" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values</a>(</div><div class="line">    boundary_values, system_matrix, solution, system_rhs, <span class="keyword">false</span>);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemsolve"></a> </p><h4>ElasticProblem::solve</h4>
<p>Having assembled the linear system, we next need to solve it. PETSc offers a variety of sequential and parallel solvers, for which we have written wrappers that have almost the same interface as is used for the deal.II solvers used in all previous example programs. The following code should therefore look rather familiar.</p>
<p>At the top of the function, we set up a convergence monitor, and assign it the accuracy to which we would like to solve the linear system. Next, we create an actual solver object using PETSc's CG solver which also works with parallel (distributed) vectors and matrices. And finally a preconditioner; we choose to use a block Jacobi preconditioner which works by computing an incomplete LU decomposition on each diagonal block of the matrix. (In other words, each MPI process computes an ILU from the rows it stores by throwing away columns that correspond to row indices not stored locally; this yields a square matrix block from which we can compute an ILU. That means that if you run the program with only one process, then you will use an ILU(0) as a preconditioner, while if it is run on many processes, then we will have a number of blocks on the diagonal and the preconditioner is the ILU(0) of each of these blocks. In the extreme case of one degree of freedom per processor, this preconditioner is then simply a Jacobi preconditioner since the diagonal matrix blocks consist of only a single entry. Such a preconditioner is relatively easy to compute because it does not require any kind of communication between processors, but it is in general not very efficient for large numbers of processors.)</p>
<p>Following this kind of setup, we then solve the linear system:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> ElasticProblem&lt;dim&gt;::solve()</div><div class="line">{</div><div class="line">  <a class="code" href="classSolverControl.html">SolverControl</a> solver_control(solution.size(), 1<a class="code" href="namespacePhysics_1_1Elasticity_1_1Kinematics.html#a9587d5229555daa5b1fa1ba2f8a40adb">e</a>-8 * system_rhs.l2_norm());</div><div class="line">  <a class="code" href="classPETScWrappers_1_1SolverCG.html">PETScWrappers::SolverCG</a> cg(solver_control, mpi_communicator);</div><div class="line"></div><div class="line">  <a class="code" href="classPETScWrappers_1_1PreconditionBlockJacobi.html">PETScWrappers::PreconditionBlockJacobi</a> preconditioner(system_matrix);</div><div class="line"></div><div class="line">  cg.solve(system_matrix, solution, system_rhs, preconditioner);</div></div><!-- fragment --><p>The next step is to distribute hanging node constraints. This is a little tricky, since to fill in the value of a constrained node you need access to the values of the nodes to which it is constrained (for example, for a Q1 element in 2d, we need access to the two nodes on the big side of a hanging node face, to compute the value of the constrained node in the middle).</p>
<p>The problem is that we have built our vectors (in <code>setup_system()</code>) in such a way that every process is responsible for storing only those elements of the solution vector that correspond to the degrees of freedom this process "owns". There are, however, cases where in order to compute the value of the vector entry for a constrained degree of freedom on one process, we need to access vector entries that are stored on other processes. PETSc (and, for that matter, the MPI model on which it is built) does not allow to simply query vector entries stored on other processes, so what we do here is to get a copy of the "distributed" vector where we store all elements locally. This is simple, since the deal.II wrappers have a conversion constructor for the deal.II <a class="el" href="classVector.html">Vector</a> class. (This conversion of course requires communication, but in essence every process only needs to send its data to every other process once in bulk, rather than having to respond to queries for individual elements):</p>
<div class="fragment"><div class="line"><a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution(solution);</div></div><!-- fragment --><p>Of course, as in previous discussions, it is clear that such a step cannot scale very far if you wanted to solve large problems on large numbers of processes, because every process now stores <em>all elements</em> of the solution vector. (We will show how to do this better in <a class="el" href="step_40.html">step-40</a>.) On the other hand, distributing hanging node constraints is simple on this local copy, using the usual function AffineConstraints::distributed(). In particular, we can compute the values of <em>all</em> constrained degrees of freedom, whether the current process owns them or not:</p>
<div class="fragment"><div class="line">hanging_node_constraints.distribute(localized_solution);</div></div><!-- fragment --><p>Then transfer everything back into the global vector. The following operation copies those elements of the localized solution that we store locally in the distributed solution, and does not touch the other ones. Since we do the same operation on all processors, we end up with a distributed vector (i.e., a vector that on every process only stores the vector entries corresponding to degrees of freedom that are owned by this process) that has all the constrained nodes fixed.</p>
<p>We end the function by returning the number of iterations it took to converge, to allow for some output.</p>
<div class="fragment"><div class="line">  solution = localized_solution;</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> solver_control.last_step();</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemrefine_grid"></a> </p><h4>ElasticProblem::refine_grid</h4>
<p>Using some kind of refinement indicator, the mesh can be refined. The problem is basically the same as with distributing hanging node constraints: in order to compute the error indicator (even if we were just interested in the indicator on the cells the current process owns), we need access to more elements of the solution vector than just those the current processor stores. To make this happen, we do essentially what we did in <code>solve()</code> already, namely get a <em>complete</em> copy of the solution vector onto every process, and use that to compute. This is in itself expensive as explained above and it is particular unnecessary since we had just created and then destroyed such a vector in <code>solve()</code>, but efficiency is not the point of this program and so let us opt for a design in which every function is as self-contained as possible.</p>
<p>Once we have such a "localized" vector that contains <em>all</em> elements of the solution vector, we can compute the indicators for the cells that belong to the present process. In fact, we could of course compute <em>all</em> refinement indicators since our <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects store information about all cells, and since we have a complete copy of the solution vector. But in the interest in showing how to operate in parallel, let us demonstrate how one would operate if one were to only compute <em>some</em> error indicators and then exchange the remaining ones with the other processes. (Ultimately, each process needs a complete set of refinement indicators because every process needs to refine their mesh, and needs to refine it in exactly the same way as all of the other processes.)</p>
<p>So, to do all of this, we need to:</p><ul>
<li>First, get a local copy of the distributed solution vector.</li>
<li>Second, create a vector to store the refinement indicators.</li>
<li>Third, let the <a class="el" href="classKellyErrorEstimator.html">KellyErrorEstimator</a> compute refinement indicators for all cells belonging to the present subdomain/process. The last argument of the call indicates which subdomain we are interested in. The three arguments before it are various other default arguments that one usually does not need (and does not state values for, but rather uses the defaults), but which we have to state here explicitly since we want to modify the value of a following argument (i.e., the one indicating the subdomain).</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::refine_grid()</div><div class="line">{</div><div class="line">  <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution(solution);</div><div class="line"></div><div class="line">  <a class="code" href="classVector.html">Vector&lt;float&gt;</a> local_error_per_cell(triangulation.n_active_cells());</div><div class="line">  <a class="code" href="classKellyErrorEstimator.html#ae2269e1c9903e9d863b7abd54948af00">KellyErrorEstimator&lt;dim&gt;::estimate</a>(dof_handler,</div><div class="line">                                     <a class="code" href="classQGauss.html">QGauss&lt;dim - 1&gt;</a>(fe.<a class="code" href="classFiniteElementData.html#a2cbf5ad6b464871261dbd054bced18a8">degree</a> + 1),</div><div class="line">                                     {},</div><div class="line">                                     localized_solution,</div><div class="line">                                     local_error_per_cell,</div><div class="line">                                     <a class="code" href="classComponentMask.html">ComponentMask</a>(),</div><div class="line">                                     <span class="keyword">nullptr</span>,</div><div class="line">                                     <a class="code" href="classMultithreadInfo.html#ad0b84ae105b385b88bdd4bfc0c530995">MultithreadInfo::n_threads</a>(),</div><div class="line">                                     <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>);</div></div><!-- fragment --><p>Now all processes have computed error indicators for their own cells and stored them in the respective elements of the <code>local_error_per_cell</code> vector. The elements of this vector for cells not owned by the present process are zero. However, since all processes have a copy of the entire triangulation and need to keep these copies in sync, they need the values of refinement indicators for all cells of the triangulation. Thus, we need to distribute our results. We do this by creating a distributed vector where each process has its share and sets the elements it has computed. Consequently, when you view this vector as one that lives across all processes, then every element of this vector has been set once. We can then assign this parallel vector to a local, non-parallel vector on each process, making <em>all</em> error indicators available on every process.</p>
<p>So in the first step, we need to set up a parallel vector. For simplicity, every process will own a chunk with as many elements as this process owns cells, so that the first chunk of elements is stored with process zero, the next chunk with process one, and so on. It is important to remark, however, that these elements are not necessarily the ones we will write to. This is a consequence of the order in which cells are arranged, i.e., the order in which the elements of the vector correspond to cells is not ordered according to the subdomain these cells belong to. In other words, if on this process we compute indicators for cells of a certain subdomain, we may write the results to more or less random elements of the distributed vector; in particular, they may not necessarily lie within the chunk of vector we own on the present process. They will subsequently have to be copied into another process' memory space, an operation that PETSc does for us when we call the <code><a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a></code> function. This inefficiency could be avoided with some more code, but we refrain from it since it is not a major factor in the program's total runtime.</p>
<p>So here is how we do it: count how many cells belong to this process, set up a distributed vector with that many elements to be stored locally, copy over the elements we computed locally, and finally compress the result. In fact, we really only copy the elements that are nonzero, so we may miss a few that we computed to zero, but this won't hurt since the original values of the vector are zero anyway.</p>
<div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_local_cells =</div><div class="line">  <a class="code" href="namespaceGridTools.html#a8c212a30784bec20b1ae13fad3fd579c">GridTools::count_cells_with_subdomain_association</a>(triangulation,</div><div class="line">                                                    <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>);</div><div class="line"><a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> distributed_all_errors(</div><div class="line">  mpi_communicator, triangulation.n_active_cells(), n_local_cells);</div><div class="line"></div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; local_error_per_cell.size(); ++i)</div><div class="line">  <span class="keywordflow">if</span> (local_error_per_cell(i) != 0)</div><div class="line">    distributed_all_errors(i) = local_error_per_cell(i);</div><div class="line">distributed_all_errors.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae5042eefddc828c7c31e1e8e26da8b09">VectorOperation::insert</a>);</div></div><!-- fragment --><p>So now we have this distributed vector that contains the refinement indicators for all cells. To use it, we need to obtain a local copy and then use it to mark cells for refinement or coarsening, and actually do the refinement and coarsening. It is important to recognize that <em>every</em> process does this to its own copy of the triangulation, and does it in exactly the same way.</p>
<div class="fragment"><div class="line">  <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;float&gt;</a> localized_all_errors(distributed_all_errors);</div><div class="line"></div><div class="line">  <a class="code" href="namespaceGridRefinement.html#a48e5395381ed87155942a61a1edd134d">GridRefinement::refine_and_coarsen_fixed_number</a>(triangulation,</div><div class="line">                                                  localized_all_errors,</div><div class="line">                                                  0.3,</div><div class="line">                                                  0.03);</div><div class="line">  triangulation.execute_coarsening_and_refinement();</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemoutput_results"></a> </p><h4>ElasticProblem::output_results</h4>
<p>The final function of significant interest is the one that creates graphical output. This works the same way as in <a class="el" href="step_8.html">step-8</a>, with two small differences. Before discussing these, let us state the general philosophy this function will work: we intend for all of the data to be generated on a single process, and subsequently written to a file. This is, as many other parts of this program already discussed, not something that will scale. Previously, we had argued that we will get into trouble with triangulations, DoFHandlers, and copies of the solution vector where every process has to store all of the data, and that there will come to be a point where each process simply doesn't have enough memory to store that much data. Here, the situation is different: it's not only the memory, but also the run time that's a problem. If one process is responsible for processing <em>all</em> of the data while all of the other processes do nothing, then this one function will eventually come to dominate the overall run time of the program. In particular, the time this function takes is going to be proportional to the overall size of the problem (counted in the number of cells, or the number of degrees of freedom), independent of the number of processes we throw at it.</p>
<p>Such situations need to be avoided, and we will show in <a class="el" href="step_18.html">step-18</a> and <a class="el" href="step_40.html">step-40</a> how to address this issue. For the current problem, the solution is to have each process generate output data only for its own local cells, and write them to separate files, one file per process. This is how <a class="el" href="step_18.html">step-18</a> operates. Alternatively, one could simply leave everything in a set of independent files and let the visualization software read all of them (possibly also using multiple processors) and create a single visualization out of all of them; this is the path <a class="el" href="step_40.html">step-40</a>, <a class="el" href="step_32.html">step-32</a>, and all other parallel programs developed later on take.</p>
<p>More specifically for the current function, all processes call this function, but not all of them need to do the work associated with generating output. In fact, they shouldn't, since we would try to write to the same file multiple times at once. So we let only the first process do this, and all the other ones idle around during this time (or start their work for the next iteration, or simply yield their CPUs to other jobs that happen to run at the same time). The second thing is that we not only output the solution vector, but also a vector that indicates which subdomain each cell belongs to. This will make for some nice pictures of partitioned domains.</p>
<p>To implement this, process zero needs a complete set of solution components in a local vector. Just as with the previous function, the efficient way to do this would be to re-use the vector already created in the <code>solve()</code> function, but to keep things more self-contained, we simply re-create one here from the distributed solution vector.</p>
<p>An important thing to realize is that we do this localization operation on all processes, not only the one that actually needs the data. This can't be avoided, however, with the simplified communication model of MPI we use for vectors in this tutorial program: MPI does not have a way to query data on another process, both sides have to initiate a communication at the same time. So even though most of the processes do not need the localized solution, we have to place the statement converting the distributed into a localized vector so that all processes execute it.</p>
<p>(Part of this work could in fact be avoided. What we do is send the local parts of all processes to all other processes. What we would really need to do is to initiate an operation on all processes where each process simply sends its local chunk of data to process zero, since this is the only one that actually needs it, i.e., we need something like a gather operation. PETSc can do this, but for simplicity's sake we don't attempt to make use of this here. We don't, since what we do is not very expensive in the grand scheme of things: it is one vector communication among all processes, which has to be compared to the number of communications we have to do when solving the linear system, setting up the block-ILU for the preconditioner, and other operations.)</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword"></span>{</div><div class="line">  <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution(solution);</div></div><!-- fragment --><p>This being done, process zero goes ahead with setting up the output file as in <a class="el" href="step_8.html">step-8</a>, and attaching the (localized) solution vector to the output object.</p>
<div class="fragment"><div class="line"><span class="keywordflow">if</span> (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0)</div><div class="line">  {</div><div class="line">    std::ofstream output(<span class="stringliteral">&quot;solution-&quot;</span> + <a class="code" href="namespacePatterns_1_1Tools.html#a72743302dcb1a0fb1f2f8dc5122d299e">std::to_string</a>(cycle) + <span class="stringliteral">&quot;.vtk&quot;</span>);</div><div class="line"></div><div class="line">    <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a6ed7c846331069f406b8c9933c37fda4">attach_dof_handler</a>(dof_handler);</div><div class="line"></div><div class="line">    std::vector&lt;std::string&gt; solution_names;</div><div class="line">    <span class="keywordflow">switch</span> (dim)</div><div class="line">      {</div><div class="line">        <span class="keywordflow">case</span> 1:</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;displacement&quot;</span>);</div><div class="line">          <span class="keywordflow">break</span>;</div><div class="line">        <span class="keywordflow">case</span> 2:</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">          <span class="keywordflow">break</span>;</div><div class="line">        <span class="keywordflow">case</span> 3:</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;z_displacement&quot;</span>);</div><div class="line">          <span class="keywordflow">break</span>;</div><div class="line">        <span class="keywordflow">default</span>:</div><div class="line">          <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(<span class="keyword">false</span>, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line">      }</div><div class="line"></div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(localized_solution, solution_names);</div></div><!-- fragment --><p>The only other thing we do here is that we also output one value per cell indicating which subdomain (i.e., MPI process) it belongs to. This requires some conversion work, since the data the library provides us with is not the one the output class expects, but this is not difficult. First, set up a vector of integers, one per cell, that is then filled by the subdomain id of each cell.</p>
<p>The elements of this vector are then converted to a floating point vector in a second step, and this vector is added to the <a class="el" href="classDataOut.html">DataOut</a> object, which then goes off creating output in VTK format:</p>
<div class="fragment"><div class="line">      std::vector&lt;unsigned int&gt; partition_int(triangulation.n_active_cells());</div><div class="line">      <a class="code" href="namespaceGridTools.html#ac41d959ae1723a898b616c3320241ffe">GridTools::get_subdomain_association</a>(triangulation, partition_int);</div><div class="line"></div><div class="line">      <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> partitioning(partition_int.begin(),</div><div class="line">                                        partition_int.end());</div><div class="line"></div><div class="line">      data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(partitioning, <span class="stringliteral">&quot;partitioning&quot;</span>);</div><div class="line"></div><div class="line">      data_out.<a class="code" href="classDataOut.html#a087f63e22f0614bca326dbdca288c646">build_patches</a>();</div><div class="line">      data_out.<a class="code" href="classDataOutInterface.html#acad99726038e4fca7f605fdffb3317e4">write_vtk</a>(output);</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemrun"></a> </p><h4>ElasticProblem::run</h4>
<p>Lastly, here is the driver function. It is almost completely unchanged from <a class="el" href="step_8.html">step-8</a>, with the exception that we replace <code>std::cout</code> by the <code>pcout</code> stream. Apart from this, the only other cosmetic change is that we output how many degrees of freedom there are per process, and how many iterations it took for the linear solver to converge:</p>
<div class="fragment"><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">ElasticProblem&lt;dim&gt;::run</a>()</div><div class="line">  {</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle = 0; cycle &lt; 10; ++cycle)</div><div class="line">      {</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">          {</div><div class="line">            <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a>(triangulation, -1, 1);</div><div class="line">            triangulation.refine_global(3);</div><div class="line">          }</div><div class="line">        <span class="keywordflow">else</span></div><div class="line">          refine_grid();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">              &lt;&lt; triangulation.n_active_cells() &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        setup_system();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span> &lt;&lt; dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>()</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot; (by partition:&quot;</span>;</div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p = 0; p &lt; <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>; ++p)</div><div class="line">          pcout &lt;&lt; (p == 0 ? <span class="charliteral">&#39; &#39;</span> : <span class="charliteral">&#39;+&#39;</span>)</div><div class="line">                &lt;&lt; (<a class="code" href="namespaceDoFTools.html#ac704c6d311cd0f289d625427e03708ac">DoFTools::count_dofs_with_subdomain_association</a>(dof_handler,</div><div class="line">                                                                    p));</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;)&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        assemble_system();</div><div class="line">        <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_iterations = solve();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Solver converged in &quot;</span> &lt;&lt; n_iterations &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        output_results(cycle);</div><div class="line">      }</div><div class="line">  }</div><div class="line">} <span class="comment">// namespace Step17</span></div></div><!-- fragment --><p><a class="anchor" id="Thecodemaincodefunction"></a> </p><h3>The <code>main</code> function</h3>
<p>The <code>main()</code> works the same way as most of the main functions in the other example programs, i.e., it delegates work to the <code>run</code> function of a managing object, and only wraps everything into some code to catch exceptions:</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> **argv)</div><div class="line">{</div><div class="line">  <span class="keywordflow">try</span></div><div class="line">    {</div><div class="line">      <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">      <span class="keyword">using namespace </span>Step17;</div></div><!-- fragment --><p>Here is the only real difference: MPI and PETSc both require that we initialize these libraries at the beginning of the program, and un-initialize them at the end. The class MPI_InitFinalize takes care of all of that. The trailing argument <code>1</code> means that we do want to run each MPI process with a single thread, a prerequisite with the PETSc parallel linear algebra.</p>
<div class="fragment"><div class="line">      <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">      ElasticProblem&lt;2&gt; elastic_problem;</div><div class="line">      elastic_problem.run();</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (...)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p> <a class="anchor" id="Results"></a></p><h1>Results</h1>
<p>If the program above is compiled and run on a single processor machine, it should generate results that are very similar to those that we already got with <a class="el" href="step_8.html">step-8</a>. However, it becomes more interesting if we run it on a multicore machine or a cluster of computers. The most basic way to run MPI programs is using a command line like </p><div class="fragment"><div class="line">mpirun -np 32 ./step-17</div></div><!-- fragment --><p> to run the <a class="el" href="step_17.html">step-17</a> executable with 32 processors.</p>
<p>(If you work on a cluster, then there is typically a step in between where you need to set up a job script and submit the script to a scheduler. The scheduler will execute the script whenever it can allocate 32 unused processors for your job. How to write such job scripts differs from cluster to cluster, and you should find the documentation of your cluster to see how to do this. On my system, I have to use the command <code>qsub</code> with a whole host of options to run a job in parallel.)</p>
<p>Whether directly or through a scheduler, if you run this program on 8 processors, you should get output like the following: </p><div class="fragment"><div class="line">Cycle 0:</div><div class="line">   Number of active cells:       64</div><div class="line">   Number of degrees of freedom: 162 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 22+22+20+20+18+16+20+24)</div><div class="line">   Solver converged in 23 iterations.</div><div class="line">Cycle 1:</div><div class="line">   Number of active cells:       124</div><div class="line">   Number of degrees of freedom: 302 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 38+42+36+34+44+44+36+28)</div><div class="line">   Solver converged in 35 iterations.</div><div class="line">Cycle 2:</div><div class="line">   Number of active cells:       238</div><div class="line">   Number of degrees of freedom: 570 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 68+80+66+74+58+68+78+78)</div><div class="line">   Solver converged in 46 iterations.</div><div class="line">Cycle 3:</div><div class="line">   Number of active cells:       454</div><div class="line">   Number of degrees of freedom: 1046 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 120+134+124+130+154+138+122+124)</div><div class="line">   Solver converged in 55 iterations.</div><div class="line">Cycle 4:</div><div class="line">   Number of active cells:       868</div><div class="line">   Number of degrees of freedom: 1926 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 232+276+214+248+230+224+234+268)</div><div class="line">   Solver converged in 77 iterations.</div><div class="line">Cycle 5:</div><div class="line">   Number of active cells:       1654</div><div class="line">   Number of degrees of freedom: 3550 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 418+466+432+470+442+474+424+424)</div><div class="line">   Solver converged in 93 iterations.</div><div class="line">Cycle 6:</div><div class="line">   Number of active cells:       3136</div><div class="line">   Number of degrees of freedom: 6702 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 838+796+828+892+866+798+878+806)</div><div class="line">   Solver converged in 127 iterations.</div><div class="line">Cycle 7:</div><div class="line">   Number of active cells:       5962</div><div class="line">   Number of degrees of freedom: 12446 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 1586+1484+1652+1552+1556+1576+1560+1480)</div><div class="line">   Solver converged in 158 iterations.</div><div class="line">Cycle 8:</div><div class="line">   Number of active cells:       11320</div><div class="line">   Number of degrees of freedom: 23586 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 2988+2924+2890+2868+2864+3042+2932+3078)</div><div class="line">   Solver converged in 225 iterations.</div><div class="line">Cycle 9:</div><div class="line">   Number of active cells:       21424</div><div class="line">   Number of degrees of freedom: 43986 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 5470+5376+5642+5450+5630+5470+5416+5532)</div><div class="line">   Solver converged in 282 iterations.</div><div class="line">Cycle 10:</div><div class="line">   Number of active cells:       40696</div><div class="line">   Number of degrees of freedom: 83754 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 10660+10606+10364+10258+10354+10322+10586+10604)</div><div class="line">   Solver converged in 392 iterations.</div><div class="line">Cycle 11:</div><div class="line">   Number of active cells:       76978</div><div class="line">   Number of degrees of freedom: 156490 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 19516+20148+19390+19390+19336+19450+19730+19530)</div><div class="line">   Solver converged in 509 iterations.</div><div class="line">Cycle 12:</div><div class="line">   Number of active cells:       146206</div><div class="line">   Number of degrees of freedom: 297994 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 37462+37780+37000+37060+37232+37328+36860+37272)</div><div class="line">   Solver converged in 705 iterations.</div><div class="line">Cycle 13:</div><div class="line">   Number of active cells:       276184</div><div class="line">   Number of degrees of freedom: 558766 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 69206+69404+69882+71266+70348+69616+69796+69248)</div><div class="line">   Solver converged in 945 iterations.</div><div class="line">Cycle 14:</div><div class="line">   Number of active cells:       523000</div><div class="line">   Number of degrees of freedom: 1060258 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 132928+132296+131626+132172+132170+133588+132252+133226)</div><div class="line">   Solver converged in 1282 iterations.</div><div class="line">Cycle 15:</div><div class="line">   Number of active cells:       987394</div><div class="line">   Number of degrees of freedom: 1994226 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 253276+249068+247430+248402+248496+251380+248272+247902)</div><div class="line">   Solver converged in 1760 iterations.</div><div class="line">Cycle 16:</div><div class="line">   Number of active cells:       1867477</div><div class="line">   Number of degrees of freedom: 3771884 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 468452+474204+470818+470884+469960+</div><div class="line">471186+470686+475694)</div><div class="line">   Solver converged in 2251 iterations.</div></div><!-- fragment --><p> (This run uses a few more refinement cycles than the code available in the examples/ directory. The run also used a version of METIS from 2004 that generated different partitionings; consequently, the numbers you get today are slightly different.)</p>
<p>As can be seen, we can easily get to almost four million unknowns. In fact, the code's runtime with 8 processes was less than 7 minutes up to (and including) cycle 14, and 14 minutes including the second to last step. (These are numbers relevant to when the code was initially written, in 2004.) I lost the timing information for the last step, though, but you get the idea. All this is after release mode has been enabled by running <code>make release</code>, and with the generation of graphical output switched off for the reasons stated in the program comments above. (See also <a href="http://www.math.colostate.edu/~bangerth/videos.676.18.html">video lecture 18</a>.) The biggest 2d computations I did had roughly 7.1 million unknowns, and were done on 32 processes. It took about 40 minutes. Not surprisingly, the limiting factor for how far one can go is how much memory one has, since every process has to hold the entire mesh and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects, although matrices and vectors are split up. For the 7.1M computation, the memory consumption was about 600 bytes per unknown, which is not bad, but one has to consider that this is for every unknown, whether we store the matrix and vector entries locally or not.</p>
<p>Here is some output generated in the 12th cycle of the program, i.e. with roughly 300,000 unknowns:</p>
<table align="center" style="width:80%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-ux.png" width="100%"/>
</div>
 </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-uy.png" width="100%"/>
</div>
  </td></tr>
</table>
<p>As one would hope for, the x- (left) and y-displacements (right) shown here closely match what we already saw in <a class="el" href="step_8.html">step-8</a>. As shown there and in <a class="el" href="step_22.html">step-22</a>, we could as well have produced a vector plot of the displacement field, rather than plotting it as two separate scalar fields. What may be more interesting, though, is to look at the mesh and partition at this step:</p>
<table align="center" width="80%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-grid.png" width="100%"/>
</div>
 </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-partition.png" width="100%"/>
</div>
  </td></tr>
</table>
<p>Again, the mesh (left) shows the same refinement pattern as seen previously. The right panel shows the partitioning of the domain across the 8 processes, each indicated by a different color. The picture shows that the subdomains are smaller where mesh cells are small, a fact that needs to be expected given that the partitioning algorithm tries to equilibrate the number of cells in each subdomain; this equilibration is also easily identified in the output shown above, where the number of degrees per subdomain is roughly the same.</p>
<p>It is worth noting that if we ran the same program with a different number of processes, that we would likely get slightly different output: a different mesh, different number of unknowns and iterations to convergence. The reason for this is that while the matrix and right hand side are the same independent of the number of processes used, the preconditioner is not: it performs an ILU(0) on the chunk of the matrix of <em>each processor separately</em>. Thus, it's effectiveness as a preconditioner diminishes as the number of processes increases, which makes the number of iterations increase. Since a different preconditioner leads to slight changes in the computed solution, this will then lead to slightly different mesh cells tagged for refinement, and larger differences in subsequent steps. The solution will always look very similar, though.</p>
<p>Finally, here are some results for a 3d simulation. You can repeat these by changing </p><div class="fragment"><div class="line">ElasticProblem&lt;2&gt; elastic_problem;</div></div><!-- fragment --><p> to </p><div class="fragment"><div class="line">ElasticProblem&lt;3&gt; elastic_problem;</div></div><!-- fragment --><p> in the main function. If you then run the program in parallel, you get something similar to this (this is for a job with 16 processes): </p><div class="fragment"><div class="line">Cycle 0:</div><div class="line">   Number of active cells:       512</div><div class="line">   Number of degrees of freedom: 2187 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 114+156+150+114+114+210+105+102+120+120+96+123+141+183+156+183)</div><div class="line">   Solver converged in 27 iterations.</div><div class="line">Cycle 1:</div><div class="line">   Number of active cells:       1604</div><div class="line">   Number of degrees of freedom: 6549 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 393+291+342+354+414+417+570+366+444+288+543+525+345+387+489+381)</div><div class="line">   Solver converged in 42 iterations.</div><div class="line">Cycle 2:</div><div class="line">   Number of active cells:       4992</div><div class="line">   Number of degrees of freedom: 19167 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 1428+1266+1095+1005+1455+1257+1410+1041+1320+1380+1080+1050+963+1005+1188+1224)</div><div class="line">   Solver converged in 65 iterations.</div><div class="line">Cycle 3:</div><div class="line">   Number of active cells:       15485</div><div class="line">   Number of degrees of freedom: 56760 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 3099+3714+3384+3147+4332+3858+3615+3117+3027+3888+3942+3276+4149+3519+3030+3663)</div><div class="line">   Solver converged in 96 iterations.</div><div class="line">Cycle 4:</div><div class="line">   Number of active cells:       48014</div><div class="line">   Number of degrees of freedom: 168762 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 11043+10752+9846+10752+9918+10584+10545+11433+12393+11289+10488+9885+10056+9771+11031+8976)</div><div class="line">   Solver converged in 132 iterations.</div><div class="line">Cycle 5:</div><div class="line">   Number of active cells:       148828</div><div class="line">   Number of degrees of freedom: 492303 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 31359+30588+34638+32244+30984+28902+33297+31569+29778+29694+28482+28032+32283+30702+31491+28260)</div><div class="line">   Solver converged in 179 iterations.</div><div class="line">Cycle 6:</div><div class="line">   Number of active cells:       461392</div><div class="line">   Number of degrees of freedom: 1497951 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 103587+100827+97611+93726+93429+88074+95892+88296+96882+93000+87864+90915+92232+86931+98091+90594)</div><div class="line">   Solver converged in 261 iterations.</div></div><!-- fragment --><p>The last step, going up to 1.5 million unknowns, takes about 55 minutes with 16 processes on 8 dual-processor machines (of the kind available in 2003). The graphical output generated by this job is rather large (cycle 5 already prints around 82 MB of data), so we contend ourselves with showing output from cycle 4:</p>
<table width="80%" align="center">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.4-3d-partition.png" width="100%"/>
</div>
 </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.4-3d-ux.png" width="100%"/>
</div>
  </td></tr>
</table>
<p>The left picture shows the partitioning of the cube into 16 processes, whereas the right one shows the x-displacement along two cutplanes through the cube.</p>
<p><a class="anchor" id="extensions"></a> <a class="anchor" id="Possibilitiesforextensions"></a></p><h3>Possibilities for extensions</h3>
<p>The program keeps a complete copy of the <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects on every processor. It also creates complete copies of the solution vector, and it creates output on only one processor. All of this is obviously the bottleneck as far as parallelization is concerned.</p>
<p>Internally, within deal.II, parallelizing the data structures used in hierarchic and unstructured triangulations is a hard problem, and it took us a few more years to make this happen. The <a class="el" href="step_40.html">step-40</a> tutorial program and the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> documentation module talk about how to do these steps and what it takes from an application perspective. An obvious extension of the current program would be to use this functionality to completely distribute computations to many more processors than used here.</p>
<p><a class="anchor" id="PlainProg"></a> </p><h1>The plain program</h1>
<div class="fragment"><div class="line"><span class="comment">/* ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Copyright (C) 2000 - 2021 by the deal.II authors</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * This file is part of the deal.II library.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * The deal.II library is free software; you can use it, redistribute</span></div><div class="line"><span class="comment"> * it, and/or modify it under the terms of the GNU Lesser General</span></div><div class="line"><span class="comment"> * Public License as published by the Free Software Foundation; either</span></div><div class="line"><span class="comment"> * version 2.1 of the License, or (at your option) any later version.</span></div><div class="line"><span class="comment"> * The full text of the license can be found in the file LICENSE.md at</span></div><div class="line"><span class="comment"> * the top level directory of deal.II.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Author: Wolfgang Bangerth, University of Texas at Austin, 2000, 2004</span></div><div class="line"><span class="comment"> *         Wolfgang Bangerth, Texas A&amp;M University, 2016</span></div><div class="line"><span class="comment"> */</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="quadrature__lib_8h.html">deal.II/base/quadrature_lib.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="function_8h.html">deal.II/base/function.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="logstream_8h.html">deal.II/base/logstream.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="multithread__info_8h.html">deal.II/base/multithread_info.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8h.html">deal.II/lac/vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="full__matrix_8h.html">deal.II/lac/full_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="affine__constraints_8h.html">deal.II/lac/affine_constraints.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dynamic__sparsity__pattern_8h.html">deal.II/lac/dynamic_sparsity_pattern.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="sparsity__tools_8h.html">deal.II/lac/sparsity_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid_2tria_8h.html">deal.II/grid/tria.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__generator_8h.html">deal.II/grid/grid_generator.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid_2grid__refinement_8h.html">deal.II/grid/grid_refinement.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dofs_2dof__handler_8h.html">deal.II/dofs/dof_handler.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__tools_8h.html">deal.II/dofs/dof_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe_2fe__values_8h.html">deal.II/fe/fe_values.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__system_8h.html">deal.II/fe/fe_system.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__q_8h.html">deal.II/fe/fe_q.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector__tools_8h.html">deal.II/numerics/vector_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="matrix__tools_8h.html">deal.II/numerics/matrix_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="numerics_2data__out_8h.html">deal.II/numerics/data_out.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="error__estimator_8h.html">deal.II/numerics/error_estimator.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="conditional__ostream_8h.html">deal.II/base/conditional_ostream.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="mpi_8h.html">deal.II/base/mpi.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__vector_8h.html">deal.II/lac/petsc_vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__sparse__matrix_8h.html">deal.II/lac/petsc_sparse_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__solver_8h.html">deal.II/lac/petsc_solver.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__precondition_8h.html">deal.II/lac/petsc_precondition.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__tools_8h.html">deal.II/grid/grid_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__renumbering_8h.html">deal.II/dofs/dof_renumbering.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>Step17</div><div class="line">{</div><div class="line">  <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keyword">class </span>ElasticProblem</div><div class="line">  {</div><div class="line">  <span class="keyword">public</span>:</div><div class="line">    ElasticProblem();</div><div class="line">    <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">run</a>();</div><div class="line"></div><div class="line">  <span class="keyword">private</span>:</div><div class="line">    <span class="keywordtype">void</span>         setup_system();</div><div class="line">    <span class="keywordtype">void</span>         assemble_system();</div><div class="line">    <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> solve();</div><div class="line">    <span class="keywordtype">void</span>         refine_grid();</div><div class="line">    <span class="keywordtype">void</span>         output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div><div class="line"></div><div class="line">    <a class="code" href="classMPI__Comm.html">MPI_Comm</a> mpi_communicator;</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>;</div><div class="line"></div><div class="line">    <a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div><div class="line"></div><div class="line">    <a class="code" href="classTriangulation.html">Triangulation&lt;dim&gt;</a> <a class="code" href="p4est__wrappers_8cc.html#ace00f2f80d9780ef9aa1007e1c22c6a4">triangulation</a>;</div><div class="line">    <a class="code" href="classFESystem.html">FESystem&lt;dim&gt;</a>      fe;</div><div class="line">    <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a>    dof_handler;</div><div class="line"></div><div class="line">    <a class="code" href="classAffineConstraints.html">AffineConstraints&lt;double&gt;</a> hanging_node_constraints;</div><div class="line"></div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1SparseMatrix.html">PETScWrappers::MPI::SparseMatrix</a> system_matrix;</div><div class="line"></div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> solution;</div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> system_rhs;</div><div class="line">  };</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keyword">class </span>RightHandSide : <span class="keyword">public</span> <a class="code" href="classFunction.html">Function</a>&lt;dim&gt;</div><div class="line">  {</div><div class="line">  <span class="keyword">public</span>:</div><div class="line">    <span class="keyword">virtual</span> <span class="keywordtype">void</span> vector_value(<span class="keyword">const</span> <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &amp;p,</div><div class="line">                              Vector&lt;double&gt; &amp;  values)<span class="keyword"> const override</span></div><div class="line"><span class="keyword">    </span>{</div><div class="line">      <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(values.size() == dim, <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a>(values.size(), dim));</div><div class="line">      <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(dim &gt;= 2, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line"></div><div class="line">      <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> point_1, point_2;</div><div class="line">      point_1(0) = 0.5;</div><div class="line">      point_2(0) = -0.5;</div><div class="line"></div><div class="line">      <span class="keywordflow">if</span> (((p - point_1).norm_square() &lt; 0.2 * 0.2) ||</div><div class="line">          ((p - point_2).norm_square() &lt; 0.2 * 0.2))</div><div class="line">        <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(0) = 1;</div><div class="line">      <span class="keywordflow">else</span></div><div class="line">        <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(0) = 0;</div><div class="line"></div><div class="line">      <span class="keywordflow">if</span> (p.<a class="code" href="classPoint.html#a859ea7f3bf3e64be2e0f5ed1bfcc8550">square</a>() &lt; 0.2 * 0.2)</div><div class="line">        <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(1) = 1;</div><div class="line">      <span class="keywordflow">else</span></div><div class="line">        <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(1) = 0;</div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="keyword">virtual</span> <span class="keywordtype">void</span></div><div class="line">    vector_value_list(<span class="keyword">const</span> std::vector&lt;<a class="code" href="classPoint.html">Point&lt;dim&gt;</a>&gt; &amp;points,</div><div class="line">                      std::vector&lt;Vector&lt;double&gt;&gt; &amp;  value_list)<span class="keyword"> const override</span></div><div class="line"><span class="keyword">    </span>{</div><div class="line">      <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_points = points.size();</div><div class="line"></div><div class="line">      <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(value_list.size() == n_points,</div><div class="line">             <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a>(value_list.size(), n_points));</div><div class="line"></div><div class="line">      <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p = 0; p &lt; n_points; ++p)</div><div class="line">        RightHandSide&lt;dim&gt;::vector_value(points[p], value_list[p]);</div><div class="line">    }</div><div class="line">  };</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  ElasticProblem&lt;dim&gt;::ElasticProblem()</div><div class="line">    : mpi_communicator(MPI_COMM_WORLD)</div><div class="line">    , <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(mpi_communicator))</div><div class="line">    , <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator))</div><div class="line">    , pcout(<a class="code" href="namespacestd.html">std</a>::cout, (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0))</div><div class="line">    , fe(<a class="code" href="classFE__Q.html">FE_Q</a>&lt;dim&gt;(1), dim)</div><div class="line">    , dof_handler(triangulation)</div><div class="line">  {}</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::setup_system()</div><div class="line">  {</div><div class="line">    <a class="code" href="namespaceGridTools.html#a99eba8e3b388258eda37a2724579dd1d">GridTools::partition_triangulation</a>(<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>, triangulation);</div><div class="line"></div><div class="line">    dof_handler.<a class="code" href="classDoFHandler.html#a553ca864aaf70330d9be86bc78f36d1e">distribute_dofs</a>(fe);</div><div class="line">    <a class="code" href="namespaceDoFRenumbering.html#a166d0082503ec2b8d9c560222b48f9d2">DoFRenumbering::subdomain_wise</a>(dof_handler);</div><div class="line"></div><div class="line">    hanging_node_constraints.clear();</div><div class="line">    <a class="code" href="group__constraints.html#ga3b4ea7dfd313e388d868c4e4aa685799">DoFTools::make_hanging_node_constraints</a>(dof_handler,</div><div class="line">                                            hanging_node_constraints);</div><div class="line">    hanging_node_constraints.close();</div><div class="line"></div><div class="line">    <a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>(), dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>());</div><div class="line">    <a class="code" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a>(dof_handler,</div><div class="line">                                    dsp,</div><div class="line">                                    hanging_node_constraints,</div><div class="line">                                    <span class="keyword">false</span>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> std::vector&lt;IndexSet&gt; locally_owned_dofs_per_proc =</div><div class="line">      <a class="code" href="namespaceDoFTools.html#a180d7118ecf27d72afbfecb7978c5e09">DoFTools::locally_owned_dofs_per_subdomain</a>(dof_handler);</div><div class="line">    <span class="keyword">const</span> <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs =</div><div class="line">      locally_owned_dofs_per_proc[<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>];</div><div class="line"></div><div class="line">    system_matrix.reinit(locally_owned_dofs,</div><div class="line">                         locally_owned_dofs,</div><div class="line">                         dsp,</div><div class="line">                         mpi_communicator);</div><div class="line"></div><div class="line">    solution.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line">    system_rhs.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::assemble_system()</div><div class="line">  {</div><div class="line">    <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a>   quadrature_formula(fe.<a class="code" href="classFiniteElementData.html#a2cbf5ad6b464871261dbd054bced18a8">degree</a> + 1);</div><div class="line">    <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values(fe,</div><div class="line">                            quadrature_formula,</div><div class="line">                            <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                              <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> dofs_per_cell = fe.<a class="code" href="classFiniteElementData.html#a33b522422da89e5c080e7405ad49d7c7">n_dofs_per_cell</a>();</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_q_points    = quadrature_formula.size();</div><div class="line"></div><div class="line">    <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a> <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(dofs_per_cell, dofs_per_cell);</div><div class="line">    Vector&lt;double&gt;     cell_rhs(dofs_per_cell);</div><div class="line"></div><div class="line">    std::vector&lt;types::global_dof_index&gt; local_dof_indices(dofs_per_cell);</div><div class="line"></div><div class="line">    std::vector&lt;double&gt; lambda_values(n_q_points);</div><div class="line">    std::vector&lt;double&gt; mu_values(n_q_points);</div><div class="line"></div><div class="line">    <a class="code" href="classFunctions_1_1ConstantFunction.html">Functions::ConstantFunction&lt;dim&gt;</a> <a class="code" href="namespaceDifferentiation_1_1SD.html#a841a7b84dc17bf2ba675522093a97e8ba945f3fc449518a73b9f5f32868db466c">lambda</a>(1.), <a class="code" href="namespacemu.html">mu</a>(1.);</div><div class="line"></div><div class="line">    RightHandSide&lt;dim&gt;          right_hand_side;</div><div class="line">    std::vector&lt;Vector&lt;double&gt;&gt; rhs_values(n_q_points, Vector&lt;double&gt;(dim));</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;cell : dof_handler.<a class="code" href="group__CPP11.html#gacdb6d3adec96a13a7079f6c893e1d3ff">active_cell_iterators</a>())</div><div class="line">      <span class="keywordflow">if</span> (cell-&gt;subdomain_id() == <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>)</div><div class="line">        {</div><div class="line">          cell_matrix = 0;</div><div class="line">          cell_rhs    = 0;</div><div class="line"></div><div class="line">          fe_values.<a class="code" href="classFEValues.html#a21f914e63d588e2652a9514620653d77">reinit</a>(cell);</div><div class="line"></div><div class="line">          <a class="code" href="namespaceDifferentiation_1_1SD.html#a841a7b84dc17bf2ba675522093a97e8ba945f3fc449518a73b9f5f32868db466c">lambda</a>.value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(), lambda_values);</div><div class="line">          <a class="code" href="namespacemu.html">mu</a>.value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(), mu_values);</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_i =</div><div class="line">                fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j = 0; j &lt; dofs_per_cell; ++j)</div><div class="line">                {</div><div class="line">                  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_j =</div><div class="line">                    fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(j).first;</div><div class="line"></div><div class="line">                  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points;</div><div class="line">                       ++q_point)</div><div class="line">                    {</div><div class="line">                      <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(i, j) +=</div><div class="line">                        ((fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)[component_i] *</div><div class="line">                          fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)[component_j] *</div><div class="line">                          lambda_values[q_point]) +</div><div class="line">                         (fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)[component_j] *</div><div class="line">                          fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)[component_i] *</div><div class="line">                          mu_values[q_point]) +</div><div class="line">                         ((component_i == component_j) ?</div><div class="line">                            (fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point) *</div><div class="line">                             fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point) *</div><div class="line">                             mu_values[q_point]) :</div><div class="line">                            0)) *</div><div class="line">                        fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">                    }</div><div class="line">                }</div><div class="line">            }</div><div class="line"></div><div class="line">          right_hand_side.vector_value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(),</div><div class="line">                                            rhs_values);</div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_i =</div><div class="line">                fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points; ++q_point)</div><div class="line">                cell_rhs(i) += fe_values.<a class="code" href="classFEValuesBase.html#a1dd48cb744013c448d57f8f77640c08d">shape_value</a>(i, q_point) *</div><div class="line">                               rhs_values[q_point](component_i) *</div><div class="line">                               fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">            }</div><div class="line"></div><div class="line">          cell-&gt;get_dof_indices(local_dof_indices);</div><div class="line">          hanging_node_constraints.distribute_local_to_global(cell_matrix,</div><div class="line">                                                              cell_rhs,</div><div class="line">                                                              local_dof_indices,</div><div class="line">                                                              system_matrix,</div><div class="line">                                                              system_rhs);</div><div class="line">        }</div><div class="line"></div><div class="line">    system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">    system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line"></div><div class="line">    std::map&lt;types::global_dof_index, double&gt; boundary_values;</div><div class="line">    <a class="code" href="namespaceVectorTools.html#af27ac28c698a9ed0199faed50a204538">VectorTools::interpolate_boundary_values</a>(dof_handler,</div><div class="line">                                             0,</div><div class="line">                                             <a class="code" href="classFunctions_1_1ZeroFunction.html">Functions::ZeroFunction&lt;dim&gt;</a>(dim),</div><div class="line">                                             boundary_values);</div><div class="line">    <a class="code" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values</a>(</div><div class="line">      boundary_values, system_matrix, solution, system_rhs, <span class="keyword">false</span>);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> ElasticProblem&lt;dim&gt;::solve()</div><div class="line">  {</div><div class="line">    <a class="code" href="classSolverControl.html">SolverControl</a> solver_control(solution.size(), 1<a class="code" href="namespacePhysics_1_1Elasticity_1_1Kinematics.html#a9587d5229555daa5b1fa1ba2f8a40adb">e</a>-8 * system_rhs.l2_norm());</div><div class="line">    <a class="code" href="classPETScWrappers_1_1SolverCG.html">PETScWrappers::SolverCG</a> cg(solver_control, mpi_communicator);</div><div class="line"></div><div class="line">    <a class="code" href="classPETScWrappers_1_1PreconditionBlockJacobi.html">PETScWrappers::PreconditionBlockJacobi</a> preconditioner(system_matrix);</div><div class="line"></div><div class="line">    cg.solve(system_matrix, solution, system_rhs, preconditioner);</div><div class="line"></div><div class="line">    Vector&lt;double&gt; localized_solution(solution);</div><div class="line"></div><div class="line">    hanging_node_constraints.distribute(localized_solution);</div><div class="line"></div><div class="line">    solution = localized_solution;</div><div class="line"></div><div class="line">    <span class="keywordflow">return</span> solver_control.last_step();</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::refine_grid()</div><div class="line">  {</div><div class="line">    <span class="keyword">const</span> Vector&lt;double&gt; localized_solution(solution);</div><div class="line"></div><div class="line">    Vector&lt;float&gt; local_error_per_cell(triangulation.n_active_cells());</div><div class="line">    <a class="code" href="classKellyErrorEstimator.html#ae2269e1c9903e9d863b7abd54948af00">KellyErrorEstimator&lt;dim&gt;::estimate</a>(dof_handler,</div><div class="line">                                       <a class="code" href="classQGauss.html">QGauss&lt;dim - 1&gt;</a>(fe.<a class="code" href="classFiniteElementData.html#a2cbf5ad6b464871261dbd054bced18a8">degree</a> + 1),</div><div class="line">                                       {},</div><div class="line">                                       localized_solution,</div><div class="line">                                       local_error_per_cell,</div><div class="line">                                       <a class="code" href="classComponentMask.html">ComponentMask</a>(),</div><div class="line">                                       <span class="keyword">nullptr</span>,</div><div class="line">                                       <a class="code" href="classMultithreadInfo.html#ad0b84ae105b385b88bdd4bfc0c530995">MultithreadInfo::n_threads</a>(),</div><div class="line">                                       <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_local_cells =</div><div class="line">      <a class="code" href="namespaceGridTools.html#a8c212a30784bec20b1ae13fad3fd579c">GridTools::count_cells_with_subdomain_association</a>(triangulation,</div><div class="line">                                                        <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>);</div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> distributed_all_errors(</div><div class="line">      mpi_communicator, triangulation.n_active_cells(), n_local_cells);</div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; local_error_per_cell.size(); ++i)</div><div class="line">      <span class="keywordflow">if</span> (local_error_per_cell(i) != 0)</div><div class="line">        distributed_all_errors(i) = local_error_per_cell(i);</div><div class="line">    distributed_all_errors.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae5042eefddc828c7c31e1e8e26da8b09">VectorOperation::insert</a>);</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keyword">const</span> Vector&lt;float&gt; localized_all_errors(distributed_all_errors);</div><div class="line"></div><div class="line">    <a class="code" href="namespaceGridRefinement.html#a48e5395381ed87155942a61a1edd134d">GridRefinement::refine_and_coarsen_fixed_number</a>(triangulation,</div><div class="line">                                                    localized_all_errors,</div><div class="line">                                                    0.3,</div><div class="line">                                                    0.03);</div><div class="line">    triangulation.execute_coarsening_and_refinement();</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <span class="keyword">const</span> Vector&lt;double&gt; localized_solution(solution);</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0)</div><div class="line">      {</div><div class="line">        std::ofstream output(<span class="stringliteral">&quot;solution-&quot;</span> + <a class="code" href="namespacePatterns_1_1Tools.html#a72743302dcb1a0fb1f2f8dc5122d299e">std::to_string</a>(cycle) + <span class="stringliteral">&quot;.vtk&quot;</span>);</div><div class="line"></div><div class="line">        <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">        data_out.<a class="code" href="classDataOut__DoFData.html#a6ed7c846331069f406b8c9933c37fda4">attach_dof_handler</a>(dof_handler);</div><div class="line"></div><div class="line">        std::vector&lt;std::string&gt; solution_names;</div><div class="line">        <span class="keywordflow">switch</span> (dim)</div><div class="line">          {</div><div class="line">            <span class="keywordflow">case</span> 1:</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;displacement&quot;</span>);</div><div class="line">              <span class="keywordflow">break</span>;</div><div class="line">            <span class="keywordflow">case</span> 2:</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">              <span class="keywordflow">break</span>;</div><div class="line">            <span class="keywordflow">case</span> 3:</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;z_displacement&quot;</span>);</div><div class="line">              <span class="keywordflow">break</span>;</div><div class="line">            <span class="keywordflow">default</span>:</div><div class="line">              <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(<span class="keyword">false</span>, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line">          }</div><div class="line"></div><div class="line">        data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(localized_solution, solution_names);</div><div class="line"></div><div class="line">        std::vector&lt;unsigned int&gt; partition_int(triangulation.n_active_cells());</div><div class="line">        <a class="code" href="namespaceGridTools.html#ac41d959ae1723a898b616c3320241ffe">GridTools::get_subdomain_association</a>(triangulation, partition_int);</div><div class="line"></div><div class="line">        <span class="keyword">const</span> Vector&lt;double&gt; partitioning(partition_int.begin(),</div><div class="line">                                          partition_int.end());</div><div class="line"></div><div class="line">        data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(partitioning, <span class="stringliteral">&quot;partitioning&quot;</span>);</div><div class="line"></div><div class="line">        data_out.<a class="code" href="classDataOut.html#a087f63e22f0614bca326dbdca288c646">build_patches</a>();</div><div class="line">        data_out.<a class="code" href="classDataOutInterface.html#acad99726038e4fca7f605fdffb3317e4">write_vtk</a>(output);</div><div class="line">      }</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">ElasticProblem&lt;dim&gt;::run</a>()</div><div class="line">  {</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle = 0; cycle &lt; 10; ++cycle)</div><div class="line">      {</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">          {</div><div class="line">            <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a>(triangulation, -1, 1);</div><div class="line">            triangulation.refine_global(3);</div><div class="line">          }</div><div class="line">        <span class="keywordflow">else</span></div><div class="line">          refine_grid();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">              &lt;&lt; triangulation.n_active_cells() &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        setup_system();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span> &lt;&lt; dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>()</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot; (by partition:&quot;</span>;</div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p = 0; p &lt; <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>; ++p)</div><div class="line">          pcout &lt;&lt; (p == 0 ? <span class="charliteral">&#39; &#39;</span> : <span class="charliteral">&#39;+&#39;</span>)</div><div class="line">                &lt;&lt; (<a class="code" href="namespaceDoFTools.html#ac704c6d311cd0f289d625427e03708ac">DoFTools::count_dofs_with_subdomain_association</a>(dof_handler,</div><div class="line">                                                                    p));</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;)&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        assemble_system();</div><div class="line">        <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_iterations = solve();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Solver converged in &quot;</span> &lt;&lt; n_iterations &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        output_results(cycle);</div><div class="line">      }</div><div class="line">  }</div><div class="line">} <span class="comment">// namespace Step17</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> **argv)</div><div class="line">{</div><div class="line">  <span class="keywordflow">try</span></div><div class="line">    {</div><div class="line">      <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">      <span class="keyword">using namespace </span>Step17;</div><div class="line"></div><div class="line">      <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">      ElasticProblem&lt;2&gt; elastic_problem;</div><div class="line">      elastic_problem.run();</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (...)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p>This tutorial depends on <a class="el" href="step_8.html">step-8</a> . <table class="tutorial"
 width="50%"> <tr><th colspan="2"><b><small>Table of
 contents</small></b><b><small>Table of contents</small></b></th></tr>
 <tr><td width="50%" valign="top">
 <ol>
 <li> <a href="#Intro" class=bold>Introduction</a><a href="#Intro"
 class=bold>Introduction</a>
 <ul>
 <li><a href="#Overview">Overview</a><a href="#Overview">Overview</a>
 <li><a href="#ParallelizingsoftwarewithMPI">Parallelizing software with
 MPI</a><a href="#ParallelizingsoftwarewithMPI">Parallelizing software with
 MPI</a>
 <li><a href="#Whatthisprogramdoes">What this program does</a><a
 href="#Whatthisprogramdoes">What this program does</a>
 </ul>
 <li> <a href="#CommProg" class=bold>The commented program</a><a
 href="#CommProg" class=bold>The commented program</a>
 <ul>
 <li><a href="#Includefiles">Include files</a><a
 href="#Includefiles">Include files</a>
 <li><a href="#ThecodeElasticProblemcodeclasstemplate">The
 <code>ElasticProblem</code> class template</a><a
 href="#ThecodeElasticProblemcodeclasstemplate">The
 <code>ElasticProblem</code> class template</a>
 <li><a href="#Righthandsidevalues">Right hand side values</a><a
 href="#Righthandsidevalues">Right hand side values</a>
 <li><a href="#ThecodeElasticProblemcodeclassimplementation">The
 <code>ElasticProblem</code> class implementation</a><a
 href="#ThecodeElasticProblemcodeclassimplementation">The
 <code>ElasticProblem</code> class implementation</a>
 <ul>
 <li><a
 href="#ElasticProblemElasticProblem">ElasticProblem::ElasticProblem</a><a
 href="#ElasticProblemElasticProblem">ElasticProblem::ElasticProblem</a>
 <li><a
 href="#ElasticProblemsetup_system">ElasticProblem::setup_system</a><a
 href="#ElasticProblemsetup_system">ElasticProblem::setup_system</a>
 <li><a
 href="#ElasticProblemassemble_system">ElasticProblem::assemble_system</a><a
 href="#ElasticProblemassemble_system">ElasticProblem::assemble_system</a>
 <li><a href="#ElasticProblemsolve">ElasticProblem::solve</a><a
 href="#ElasticProblemsolve">ElasticProblem::solve</a>
 <li><a href="#ElasticProblemrefine_grid">ElasticProblem::refine_grid</a><a
 href="#ElasticProblemrefine_grid">ElasticProblem::refine_grid</a>
 <li><a
 href="#ElasticProblemoutput_results">ElasticProblem::output_results</a><a
 href="#ElasticProblemoutput_results">ElasticProblem::output_results</a>
 <li><a href="#ElasticProblemrun">ElasticProblem::run</a><a
 href="#ElasticProblemrun">ElasticProblem::run</a>
 </ul>
 <li><a href="#Thecodemaincodefunction">The <code>main</code> function</a><a
 href="#Thecodemaincodefunction">The <code>main</code> function</a>
 </ul>
 </ol></td><td width="50%" valign="top"><ol>
 <li value="3"> <a href="#Results" class=bold>Results</a><a href="#Results"
 class=bold>Results</a>
 <ul>
 <li><a href="#Possibilitiesforextensions">Possibilities for
 extensions</a><a href="#Possibilitiesforextensions">Possibilities for
 extensions</a>
 </ul>
 <li> <a href="#PlainProg" class=bold>The plain program</a><a
 href="#PlainProg" class=bold>The plain program</a>
 </ol> </td> </tr> </table>
  <a class="anchor" id="Intro"></a><a class="anchor" id="Introduction"></a></p><h1>Introduction</h1>
<p><a class="anchor" id="Overview"></a></p><h3>Overview</h3>
<p>This program does not introduce any new mathematical ideas; in fact, all itdoes is to do the exact same computations that <a class="el" href="step_8.html">step-8</a> already does, but it does so in a different manner: instead of using deal.II'sown linear algebra classes, we build everything on top of classes deal.IIprovides that wrap around the linear algebra implementation of the <a href="http://www.mcs.anl.gov/petsc/" target="_top">PETSc</a> library. Andsince PETSc allows to distribute matrices and vectors across several computerswithin an MPI network, the resulting code will even be able to solve theproblem in parallel. If you don't know what PETSc is, then this would be agood time to take a quick glimpse at their homepage. As a prerequisite of this program, you need to have PETSc installed, and ifyou want to run in parallel on a cluster, you also need <a href="http://www-users.cs.umn.edu/~karypis/metis/index.html" target="_top">METIS</a> to partition meshes. The installation of deal.IItogether with these two additional libraries is described in the <a href="../../readme.html" target="body">README</a> file. The interface the classes from the PETScWrapper namespace provide is very similar to thatof the deal.II linear algebra classes, but instead of implementing thisfunctionality themselves, they simply pass on to their corresponding PETScfunctions. The wrappers are therefore only used to give PETSc a more modern,object oriented interface, and to make the use of PETSc and deal.II objects asinterchangeable as possible. The main point of using PETSc is that it can runin parallel. We will make use of this by partitioning the domain into as manyblocks ("subdomains") as there are processes in the MPI network. At the sametime, PETSc also provides dummy MPI stubs, so you can run this program on asingle machine if PETSc was configured without MPI.</p>
<p><a class="anchor" id="ParallelizingsoftwarewithMPI"></a></p><h3>Parallelizing software with MPI</h3>
<p>Developing software to run in parallel via MPI requires a bit of a change inmindset because one typically has to split up all data structures so thatevery processor only stores a piece of the entire problem. As a consequence,you can't typically access all components of a solution vector on eachprocessor</p>
<ul>
<li>each processor may simply not have enough memory to hold theentire solution vector. Because data is split up or "distributed" acrossprocessors, we call the programming model used by MPI "distributed memorycomputing" (as opposed to "shared memory computing", which would meanthat multiple processors can all access all data within one memoryspace, for example whenever multiple cores in a single machine workon a common task). Some of the fundamentals of distributed memorycomputing are discussed in the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using distributed memory</a>documentation module, which is itself a sub-module of the <a class="el" href="group__Parallel.html">Parallel computing</a> module. For larger problems, having to store the <em>entire</em> mesh on every processorwill clearly yield a bottleneck. Splitting up the mesh is slightly, though notmuch more complicated (from a user perspective, though it is <em>much</em> morecomplicated under the hood) to achieve andwe will show how to do this in <a class="el" href="step_40.html">step-40</a> and some other programs. There arenumerous occasions where, in the course of discussing how a function of thisprogram works, we will comment on the fact that it will not scale to largeproblems and why not. All of these issues will be addressed in <a class="el" href="step_18.html">step-18</a> andin particular <a class="el" href="step_40.html">step-40</a> , which scales to very large numbers of processes. Philosophically, the way MPI operates is as follows. You typically run aprogram via <div class="fragment"><div class="line">mpirun</div><div class="line"></div><div class="line">-np 32 ./step-17</div></div><!-- fragment --> which means to run it on (say) 32 processors. (If you are on a cluster system,you typically need to <em>schedule</em> the program to run whenever 32 processorsbecome available; this will be described in the documentation of yourcluster. But under the hood, whenever those processors become available,the same call as above will generally be executed.) What this does is thatthe MPI system will start 32 <em>copies</em> of the <code><a class="el" href="step_17.html">step-17</a></code> executable. (The MPI term for each of these running executables is that youhave 32 <a class="el" href="DEALGlossary.html#GlossMPIProcess">MPI processes</a>. )This may happen on different machines that can't even readfrom each others' memory spaces, or it may happen on the same machine, butthe end result is the same: each of these 32 copies will run with somememory allocated to it by the operating system, and it will not directlybe able to read the memory of the other 31 copies. In order to collaboratein a common task, these 32 copies then have to <em>communicate</em> witheach other. MPI, short for <em>Message Passing Interface</em>, makes thispossible by allowing programs to <em>send messages</em>. You can thinkof this as the mail service: you can put a letter to a specific addressinto the mail and it will be delivered. But that's the extent to whichyou can control things. If you want the receiver to do somethingwith the content of the letter, for example return to you data you wantfrom over there, then two things need to happen: (i) the receiver needsto actually go check whether there is anything in their mailbox, and (ii) ifthere is, react appropriately, for example by sending data back. If youwait for this return message but the original receiver was distractedand not paying attention, then you're out of luck: you'll simply have towait until your requested over there will be worked on. In some cases,bugs will lead the original receiver to never check your mail, and in thatcase you will wait forever</li>
<li>this is called a <em>deadlock</em>.( See also <a href="http://www.math.colostate.edu/~bangerth/videos.676.39.html">video lecture 39</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.html">video lecture 41</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.25.html">video lecture 41.25</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.5.html">video lecture 41.5</a>.) In practice, one does not usually program at the level of sending andreceiving individual messages, but uses higher level operations. Forexample, in the program we will use function calls that take a numberfrom each processor, add them all up, and return the sum to allprocessors. Internally, this is implemented using individual messages,but to the user this is transparent. We call such operations <em>collectives</em>because <em>all</em> processors participate in them. Collectives allow usto write programs where not every copy of the executable is doing somethingcompletely different (this would be incredibly difficult to program) butwhere in essence all copies are doing the same thing (though on differentdata) for themselves, running through the same blocks of code; then theycommunicate data through collectives; and then go back to doing somethingfor themselves again running through the same blocks of data. This is thekey piece to being able to write programs, and it is the key componentto making sure that programs can run on any number of processors,since we do not have to write different code for each of the participatingprocessors. (This is not to say that programs are never written in ways wheredifferent processors run through different blocks of code in theircopy of the executable. Programs internally also often communicatein other ways than through collectives. But in practice, parallel finiteelement codes almost always follow the scheme where every copyof the program runs through the same blocks of code at the same time,interspersed by phases where all processors communicate with each other.) In reality, even the level of calling MPI collective functions is toolow. Rather, the program below will not contain any directcalls to MPI at all, but only deal.II functions that hide thiscommunication from users of the deal.II. This has the advantage thatyou don't have to learn the details of MPI and its rather intricatefunction calls. That said, you do have to understand the generalphilosophy behind MPI as outlined above.</li>
</ul>
<p><a class="anchor" id="Whatthisprogramdoes"></a></p><h3>What this program does</h3>
<p>The techniques this program then demonstrates are:</p>
<ul>
<li>How to use the PETSc wrapper classes; this will already be visible in the declaration of the principal class of this program, <code>ElasticProblem</code> .</li>
<li>How to partition the mesh into subdomains; this happens in the <code>ElasticProblem::setup_system()</code> function.</li>
<li>How to parallelize operations for jobs running on an MPI network; here, this is something one has to pay attention to in a number of places, most notably in the <code>ElasticProblem::assemble_system()</code> function.</li>
<li>How to deal with vectors that store only a subset of vector entries and for which we have to ensure that they store what we need on the current processors. See for example the <code>ElasticProblem::solve()</code> and <code>ElasticProblem::refine_grid()</code> functions.</li>
<li>How to deal with status output from programs that run on multiple processors at the same time. This is done via the <code>pcout</code> variable in the program, initialized in the constructor. Since all this can only be demonstrated using actual code, let us go straight to thecode without much further ado.</li>
</ul>
<p><a class="anchor" id="CommProg"></a> </p><h1>The commented program</h1>
<p><a class="anchor" id="Includefiles"></a> </p><h3>Include files</h3>
<p>First the usual assortment of header files we have already used in previous example programs:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="quadrature__lib_8h.html">deal.II/base/quadrature_lib.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="function_8h.html">deal.II/base/function.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="logstream_8h.html">deal.II/base/logstream.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="multithread__info_8h.html">deal.II/base/multithread_info.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8h.html">deal.II/lac/vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="full__matrix_8h.html">deal.II/lac/full_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="affine__constraints_8h.html">deal.II/lac/affine_constraints.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dynamic__sparsity__pattern_8h.html">deal.II/lac/dynamic_sparsity_pattern.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="sparsity__tools_8h.html">deal.II/lac/sparsity_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid_2tria_8h.html">deal.II/grid/tria.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__generator_8h.html">deal.II/grid/grid_generator.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid_2grid__refinement_8h.html">deal.II/grid/grid_refinement.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dofs_2dof__handler_8h.html">deal.II/dofs/dof_handler.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__tools_8h.html">deal.II/dofs/dof_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe_2fe__values_8h.html">deal.II/fe/fe_values.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__system_8h.html">deal.II/fe/fe_system.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__q_8h.html">deal.II/fe/fe_q.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector__tools_8h.html">deal.II/numerics/vector_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="matrix__tools_8h.html">deal.II/numerics/matrix_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="numerics_2data__out_8h.html">deal.II/numerics/data_out.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="error__estimator_8h.html">deal.II/numerics/error_estimator.h</a>&gt;</span></div></div><!-- fragment --><p>And here come the things that we need particularly for this example program and that weren't in <a class="el" href="step_8.html">step-8</a> . First, we replace the standard output <code>std::cout</code> by a new stream <code>pcout</code> which is used in parallel computations for generating output only on one of the MPI processes.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="conditional__ostream_8h.html">deal.II/base/conditional_ostream.h</a>&gt;</span></div></div><!-- fragment --><p>We are going to query the number of processes and the number of the present process by calling the respective functions in the <a class="el" href="namespaceUtilities_1_1MPI.html">Utilities::MPI</a> namespace.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="mpi_8h.html">deal.II/base/mpi.h</a>&gt;</span></div></div><!-- fragment --><p>Then, we are going to replace all linear algebra components that involve the (global) linear system by classes that wrap interfaces similar to our own linear algebra classes around what PETSc offers (PETSc is a library written in C, and deal.II comes with wrapper classes that provide the PETSc functionality with an interface that is similar to the interface we already had for our own linear algebra classes). In particular, we need vectors and matrices that are distributed across several <a class="el" href="DEALGlossary.html#GlossMPIProcess">processes</a> in MPI programs (and simply map to sequential, local vectors and matrices if there is only a single process, i.e., if you are running on only one machine, and without MPI support):</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__vector_8h.html">deal.II/lac/petsc_vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__sparse__matrix_8h.html">deal.II/lac/petsc_sparse_matrix.h</a>&gt;</span></div></div><!-- fragment --><p>Then we also need interfaces for solvers and preconditioners that PETSc provides:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__solver_8h.html">deal.II/lac/petsc_solver.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__precondition_8h.html">deal.II/lac/petsc_precondition.h</a>&gt;</span></div></div><!-- fragment --><p>And in addition, we need some algorithms for partitioning our meshes so that they can be efficiently distributed across an MPI network. The partitioning algorithm is implemented in the <code><a class="el" href="namespaceGridTools.html">GridTools</a></code> namespace, and we need an additional include file for a function in <code><a class="el" href="namespaceDoFRenumbering.html">DoFRenumbering</a></code> that allows to sort the indices associated with degrees of freedom so that they are numbered according to the subdomain they are associated with:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__tools_8h.html">deal.II/grid/grid_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__renumbering_8h.html">deal.II/dofs/dof_renumbering.h</a>&gt;</span></div></div><!-- fragment --><p>And this is simply C++ again:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div></div><!-- fragment --><p>The last step is as in all previous programs:</p>
<div class="fragment"><div class="line"><span class="keyword">namespace </span>Step17</div><div class="line">{</div><div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div></div><!-- fragment --><p><a class="anchor" id="ThecodeElasticProblemcodeclasstemplate"></a> </p><h3>The <code>ElasticProblem</code> class template</h3>
<p>The first real part of the program is the declaration of the main class. As mentioned in the introduction, almost all of this has been copied verbatim from <a class="el" href="step_8.html">step-8</a> , so we only comment on the few differences between the two tutorials. There is one (cosmetic) change in that we let <code>solve</code> return a value, namely the number of iterations it took to converge, so that we can output this to the screen at the appropriate place.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keyword">class </span>ElasticProblem</div><div class="line">{</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  ElasticProblem();</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">run</a>();</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line">  <span class="keywordtype">void</span>         setup_system();</div><div class="line">  <span class="keywordtype">void</span>         assemble_system();</div><div class="line">  <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> solve();</div><div class="line">  <span class="keywordtype">void</span>         refine_grid();</div><div class="line">  <span class="keywordtype">void</span>         output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div></div><!-- fragment --><p>The first change is that we have to declare a variable that indicates the <a class="el" href="DEALGlossary.html#GlossMPICommunicator">MPI communicator</a> over which we are supposed to distribute our computations.</p>
<div class="fragment"><div class="line"><a class="code" href="classMPI__Comm.html">MPI_Comm</a> mpi_communicator;</div></div><!-- fragment --><p>Then we have two variables that tell us where in the parallel world we are. The first of the following variables, <code>n_mpi_processes</code> , tells us how many MPI processes there exist in total, while the second one, <code>this_mpi_process</code> , indicates which is the number of the present process within this space of processes (in MPI language, this corresponds to the <a class="el" href="DEALGlossary.html#GlossMPIRank">rank</a> of the process). The latter will have a unique value for each process between zero and (less than) <code>n_mpi_processes</code> . If this program is run on a single machine without MPI support, then their values are <code>1</code> and <code>0</code> , respectively.</p>
<div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>;</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>;</div></div><!-- fragment --><p>To make this simpler, the <a class="el" href="classConditionalOStream.html">ConditionalOStream</a> class does exactly this under the hood: it acts as if it were a stream, but only forwards to a real, underlying stream if a flag is set. By setting this condition to <code>this_mpi_process==0</code> (where <code>this_mpi_process</code> corresponds to the rank of an MPI process), we make sure that output is only generated from the first process and that we don't get the same lines of output over and over again, once per process. Thus, we can use <code>pcout</code> everywhere and in every process, but on all but one process nothing will ever happen to the information that is piped into the object via <code>operator&lt;&lt;</code> .</p>
<div class="fragment"><div class="line"><a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div></div><!-- fragment --><p>The remainder of the list of member variables is fundamentally the same as in <a class="el" href="step_8.html">step-8</a> . However, we change the declarations of matrix and vector types to use parallel PETSc objects instead. Note that we do not use a separate sparsity pattern, since PETSc manages this internally as part of its matrix data structures.</p>
<div class="fragment"><div class="line">  <a class="code" href="classTriangulation.html">Triangulation&lt;dim&gt;</a> <a class="code" href="p4est__wrappers_8cc.html#ace00f2f80d9780ef9aa1007e1c22c6a4">triangulation</a>;</div><div class="line">  <a class="code" href="classFESystem.html">FESystem&lt;dim&gt;</a>      fe;</div><div class="line">  <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a>    dof_handler;</div><div class="line"></div><div class="line">  <a class="code" href="classAffineConstraints.html">AffineConstraints&lt;double&gt;</a> hanging_node_constraints;</div><div class="line"></div><div class="line">  <a class="code" href="classPETScWrappers_1_1MPI_1_1SparseMatrix.html">PETScWrappers::MPI::SparseMatrix</a> system_matrix;</div><div class="line"></div><div class="line">  <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> solution;</div><div class="line">  <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> system_rhs;</div><div class="line">};</div></div><!-- fragment --><p><a class="anchor" id="Righthandsidevalues"></a> </p><h3>Right hand side values</h3>
<p>The following is taken from <a class="el" href="step_8.html">step-8</a> without change:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keyword">class </span>RightHandSide : <span class="keyword">public</span> <a class="code" href="classFunction.html">Function</a>&lt;dim&gt;</div><div class="line">{</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  <span class="keyword">virtual</span> <span class="keywordtype">void</span> <a class="code" href="classFunction.html#ae316ebc05d21989d573024f8a23c49cb">vector_value</a>(<span class="keyword">const</span> <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &amp;p,</div><div class="line">                            <a class="code" href="classVector.html">Vector&lt;double&gt;</a> &amp;  values)<span class="keyword"> const override</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(values.<a class="code" href="classVector.html#a81dcfa5c77bdd426603386c0844149ae">size</a>() == dim, <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a>(values.<a class="code" href="classVector.html#a81dcfa5c77bdd426603386c0844149ae">size</a>(), dim));</div><div class="line">    <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(dim &gt;= 2, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line"></div><div class="line">    <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> point_1, point_2;</div><div class="line">    point_1(0) = 0.5;</div><div class="line">    point_2(0) =</div><div class="line"></div><div class="line">-0.5;</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (((p</div><div class="line"></div><div class="line">- point_1).norm_square() &lt; 0.2 0.2) ||</div><div class="line">        ((p</div><div class="line"></div><div class="line">- point_2).norm_square() &lt; 0.2 0.2))</div><div class="line">      <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(0) = 1;</div><div class="line">    <span class="keywordflow">else</span></div><div class="line">      <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(0) = 0;</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (p.<a class="code" href="classPoint.html#a859ea7f3bf3e64be2e0f5ed1bfcc8550">square</a>() &lt; 0.2 0.2)</div><div class="line">      <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(1) = 1;</div><div class="line">    <span class="keywordflow">else</span></div><div class="line">      <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(1) = 0;</div><div class="line">  }</div><div class="line"></div><div class="line">  <span class="keyword">virtual</span> <span class="keywordtype">void</span></div><div class="line">  <a class="code" href="classFunction.html#aa041dde994d40c068e00661197ac75a6">vector_value_list</a>(<span class="keyword">const</span> std::vector&lt;<a class="code" href="classPoint.html">Point&lt;dim&gt;</a>&gt; &amp;points,</div><div class="line">                    std::vector&lt;<a class="code" href="classVector.html">Vector&lt;double&gt;</a>&gt; &amp;  value_list)<span class="keyword"> const override</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_points = points.size();</div><div class="line"></div><div class="line">    <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(value_list.size() == n_points,</div><div class="line">           <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a>(value_list.size(), n_points));</div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p = 0; p &lt; n_points; ++p)</div><div class="line">      RightHandSide&lt;dim&gt;::vector_value(points[p], value_list[p]);</div><div class="line">  }</div><div class="line">};</div></div><!-- fragment --><p><a class="anchor" id="ThecodeElasticProblemcodeclassimplementation"></a> </p><h3>The <code>ElasticProblem</code> class implementation</h3>
<p><a class="anchor" id="ElasticProblemElasticProblem"></a> </p><h4>ElasticProblem::ElasticProblem</h4>
<p>The first step in the actual implementation is the constructor of the main class. Apart from initializing the same member variables that we already had in <a class="el" href="step_8.html">step-8</a> , we here initialize the MPI communicator variable we shall use with the global MPI communicator linking all processes together (in more complex applications, one could here use a communicator object that only links a subset of all processes), and call the <a class="el" href="namespaceUtilities_1_1MPI.html">Utilities::MPI</a> helper functions to determine the number of processes and where the present one fits into this picture. In addition, we make sure that output is only generated by the (globally) first process. We do so by passing the stream we want to output to (<code>std::cout</code>) and a true/false flag as arguments where the latter is determined by testing whether the process currently executing the constructor call is the first in the MPI universe.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">ElasticProblem&lt;dim&gt;::ElasticProblem()</div><div class="line">  : mpi_communicator(MPI_COMM_WORLD)</div><div class="line">  , <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(mpi_communicator))</div><div class="line">  , <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator))</div><div class="line">  , pcout(<a class="code" href="namespacestd.html">std</a>::cout, (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0))</div><div class="line">  , fe(<a class="code" href="classFE__Q.html">FE_Q</a>&lt;dim&gt;(1), dim)</div><div class="line">  , dof_handler(triangulation)</div><div class="line">{}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemsetup_system"></a> </p><h4>ElasticProblem::setup_system</h4>
<p>Next, the function in which we set up the various variables for the global linear system to be solved needs to be implemented.</p>
<dl class="section note"><dt>Note</dt><dd>As mentioned in the introduction, we could avoid this manual partitioning step if we used the <a class="el" href="classparallel_1_1shared_1_1Triangulation.html">parallel::shared::Triangulation</a> class for the triangulation object instead (as we do in <a class="el" href="step_18.html">step-18</a> ). That class does, in essence, everything a regular triangulation does, but it then also automatically partitions the mesh after every mesh creation or refinement operation. Following partitioning, we need to enumerate all degrees of freedom as usual. However, we would like to enumerate the degrees of freedom in a way so that all degrees of freedom associated with cells in subdomain zero (which resides on process zero) come before all DoFs associated with cells on subdomain one, before those on cells on process two, and so on. We need this since we have to split the global vectors for right hand side and solution, as well as the matrix into contiguous chunks of rows that live on each of the processors, and we will want to do this in a way that requires minimal communication. This particular enumeration can be obtained by re-ordering degrees of freedom indices using <a class="el" href="namespaceDoFRenumbering.html#a166d0082503ec2b8d9c560222b48f9d2">DoFRenumbering::subdomain_wise()</a>. The final step of this initial setup is that we get ourselves an <a class="el" href="classIndexSet.html">IndexSet</a> that indicates the subset of the global number of unknowns this process is responsible for. (Note that a degree of freedom is not necessarily owned by the process that owns a cell just because the degree of freedom lives on this cell: some degrees of freedom live on interfaces between subdomains, and are consequently only owned by one of the processes adjacent to this interface.) Before we move on, let us recall a fact already discussed in the introduction: The triangulation we use here is replicated across all processes, and each process has a complete copy of the entire triangulation, with all cells. Partitioning only provides a way to identify which cells out of all each process "owns", but it knows everything about all of them. Likewise, the <a class="el" href="classDoFHandler.html">DoFHandler</a> object knows everything about every cell, in particular the degrees of freedom that live on each cell, whether it is one that the current process owns or not. This can not scale to large problems because eventually just storing the entire mesh, and everything that is associated with it, on every process will become infeasible if the problem is large enough. On the other hand, if we split the triangulation into parts so that every process stores only those cells it "owns" but nothing else (or, at least a sufficiently small fraction of everything else), then we can solve large problems if only we throw a large enough number of MPI processes at them. This is what we are going to in <a class="el" href="step_40.html">step-40</a> , for example, using the <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> class. On the other hand, most of the rest of what we demonstrate in the current program will actually continue to work whether we have the entire triangulation available, or only a piece of it.</dd></dl>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::setup_system()</div><div class="line">{</div><div class="line">  <a class="code" href="namespaceGridTools.html#a99eba8e3b388258eda37a2724579dd1d">GridTools::partition_triangulation</a>(<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>, triangulation);</div><div class="line"></div><div class="line">  dof_handler.<a class="code" href="classDoFHandler.html#a553ca864aaf70330d9be86bc78f36d1e">distribute_dofs</a>(fe);</div><div class="line">  <a class="code" href="namespaceDoFRenumbering.html#a166d0082503ec2b8d9c560222b48f9d2">DoFRenumbering::subdomain_wise</a>(dof_handler);</div></div><!-- fragment --><p>We need to initialize the objects denoting hanging node constraints for the present grid. As with the triangulation and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects, we will simply store <em>all</em> constraints on each process; again, this will not scale, but we show in <a class="el" href="step_40.html">step-40</a> how one can work around this by only storing on each MPI process the constraints for degrees of freedom that actually matter on this particular process.</p>
<div class="fragment"><div class="line">hanging_node_constraints.clear();</div><div class="line"><a class="code" href="group__constraints.html#ga3b4ea7dfd313e388d868c4e4aa685799">DoFTools::make_hanging_node_constraints</a>(dof_handler,</div><div class="line">                                        hanging_node_constraints);</div><div class="line">hanging_node_constraints.close();</div></div><!-- fragment --><p>Now we create the sparsity pattern for the system matrix. Note that we again compute and store all entries and not only the ones relevant to this process (see <a class="el" href="step_18.html">step-18</a> or <a class="el" href="step_40.html">step-40</a> for a more efficient way to handle this).</p>
<div class="fragment"><div class="line"><a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>(), dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>());</div><div class="line"><a class="code" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a>(dof_handler,</div><div class="line">                                dsp,</div><div class="line">                                hanging_node_constraints,</div><div class="line">                                <span class="keyword">false</span>);</div></div><!-- fragment --><p>Now we determine the set of locally owned DoFs and use that to initialize parallel vectors and matrix. Since the matrix and vectors need to work in parallel, we have to pass them an MPI communication object, as well as information about the partitioning contained in the <a class="el" href="classIndexSet.html">IndexSet</a> <code>locally_owned_dofs</code>. The <a class="el" href="classIndexSet.html">IndexSet</a> contains information about the global size (the <em>total</em> number of degrees of freedom) and also what subset of rows is to be stored locally. Note that the system matrix needs that partitioning information for the rows and columns. For square matrices, as it is the case here, the columns should be partitioned in the same way as the rows, but in the case of rectangular matrices one has to partition the columns in the same way as vectors are partitioned with which the matrix is multiplied, while rows have to partitioned in the same way as destination vectors of matrix-vector multiplications:</p>
<div class="fragment"><div class="line">  <span class="keyword">const</span> std::vector&lt;IndexSet&gt; locally_owned_dofs_per_proc =</div><div class="line">    <a class="code" href="namespaceDoFTools.html#a180d7118ecf27d72afbfecb7978c5e09">DoFTools::locally_owned_dofs_per_subdomain</a>(dof_handler);</div><div class="line">  <span class="keyword">const</span> <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs =</div><div class="line">    locally_owned_dofs_per_proc[<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>];</div><div class="line"></div><div class="line">  system_matrix.reinit(locally_owned_dofs,</div><div class="line">                       locally_owned_dofs,</div><div class="line">                       dsp,</div><div class="line">                       mpi_communicator);</div><div class="line"></div><div class="line">  solution.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line">  system_rhs.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemassemble_system"></a> </p><h4>ElasticProblem::assemble_system</h4>
<p>We now assemble the matrix and right hand side of the problem. There are some things worth mentioning before we go into detail. First, we will be assembling the system in parallel, i.e., each process will be responsible for assembling on cells that belong to this particular process. Note that the degrees of freedom are split in a way such that all DoFs in the interior of cells and between cells belonging to the same subdomain belong to the process that <code>owns</code> the cell. However, even then we sometimes need to assemble on a cell with a neighbor that belongs to a different process, and in these cases when we add up the local contributions into the global matrix or right hand side vector, we have to transfer these entries to the process that owns these elements. Fortunately, we don't have to do this by hand: PETSc does all this for us by caching these elements locally, and sending them to the other processes as necessary when we call the <code><a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a></code> functions on the matrix and vector at the end of this function. The second point is that once we have handed over matrix and vector contributions to PETSc, it is a) hard, and b) very inefficient to get them back for modifications. This is not only the fault of PETSc, it is also a consequence of the distributed nature of this program: if an entry resides on another processor, then it is necessarily expensive to get it. The consequence of this is that we should not try to first assemble the matrix and right hand side as if there were no hanging node constraints and boundary values, and then eliminate these in a second step (using, for example, <a class="el" href="classAffineConstraints.html#a5a1bc1bb2d705b582889ebaa24bcae5c">AffineConstraints::condense()</a>). Rather, we should try to eliminate hanging node constraints before handing these entries over to PETSc. This is easy: instead of copying elements by hand into the global matrix (as we do in <a class="el" href="step_4.html">step-4</a> ), we use the <a class="el" href="classAffineConstraints.html#a373fbdacd8c486e675b8d2bff8943192">AffineConstraints::distribute_local_to_global()</a> functions to take care of hanging nodes at the same time. We also already did this in <a class="el" href="step_6.html">step-6</a> . The second step, elimination of boundary nodes, could also be done this way by putting the boundary values into the same <a class="el" href="classAffineConstraints.html">AffineConstraints</a> object as hanging nodes (see the way it is done in <a class="el" href="step_6.html">step-6</a> , for example); however, it is not strictly necessary to do this here because eliminating boundary values can be done with only the data stored on each process itself, and consequently we use the approach used before in <a class="el" href="step_4.html">step-4</a> , i.e., via <a class="el" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values()</a>. All of this said, here is the actual implementation starting with the general setup of helper variables. (Note that we still use the deal.II full matrix and vector types for the local systems as these are small and need not be shared across processes.)</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::assemble_system()</div><div class="line">{</div><div class="line">  <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a>   quadrature_formula(fe.<a class="code" href="classFiniteElementData.html#a2cbf5ad6b464871261dbd054bced18a8">degree</a> + 1);</div><div class="line">  <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values(fe,</div><div class="line">                          quadrature_formula,</div><div class="line">                          <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                            <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> dofs_per_cell = fe.<a class="code" href="classFiniteElementData.html#a33b522422da89e5c080e7405ad49d7c7">n_dofs_per_cell</a>();</div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_q_points    = quadrature_formula.size();</div><div class="line"></div><div class="line">  <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a> <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(dofs_per_cell, dofs_per_cell);</div><div class="line">  <a class="code" href="classVector.html">Vector&lt;double&gt;</a>     cell_rhs(dofs_per_cell);</div><div class="line"></div><div class="line">  std::vector&lt;types::global_dof_index&gt; local_dof_indices(dofs_per_cell);</div><div class="line"></div><div class="line">  std::vector&lt;double&gt; lambda_values(n_q_points);</div><div class="line">  std::vector&lt;double&gt; mu_values(n_q_points);</div><div class="line"></div><div class="line">  <a class="code" href="classFunctions_1_1ConstantFunction.html">Functions::ConstantFunction&lt;dim&gt;</a> <a class="code" href="namespaceDifferentiation_1_1SD.html#a841a7b84dc17bf2ba675522093a97e8ba945f3fc449518a73b9f5f32868db466c">lambda</a>(1.), <a class="code" href="namespacemu.html">mu</a>(1.);</div><div class="line"></div><div class="line">  RightHandSide&lt;dim&gt;          right_hand_side;</div><div class="line">  std::vector&lt;Vector&lt;double&gt;&gt; rhs_values(n_q_points, <a class="code" href="classVector.html">Vector&lt;double&gt;</a>(dim));</div></div><!-- fragment --><p>The next thing is the loop over all elements. Note that we do not have to do <em>all</em> the work on every process: our job here is only to assemble the system on cells that actually belong to this MPI process, all other cells will be taken care of by other processes. This is what the if-clause immediately after the for-loop takes care of: it queries the subdomain identifier of each cell, which is a number associated with each cell that tells us about the owner process. In more generality, the subdomain id is used to split a domain into several parts (we do this above, at the beginning of <code>setup_system()</code> ), and which allows to identify which subdomain a cell is living on. In this application, we have each process handle exactly one subdomain, so we identify the terms <code>subdomain</code> and <code>MPI process</code> . Apart from this, assembling the local system is relatively uneventful if you have understood how this is done in <a class="el" href="step_8.html">step-8</a> . As mentioned above, distributing local contributions into the global matrix and right hand sides also takes care of hanging node constraints in the same way as is done in <a class="el" href="step_6.html">step-6</a> .</p>
<div class="fragment"><div class="line"><span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;cell : dof_handler.<a class="code" href="group__CPP11.html#gacdb6d3adec96a13a7079f6c893e1d3ff">active_cell_iterators</a>())</div><div class="line">  <span class="keywordflow">if</span> (cell-&gt;subdomain_id() == <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>)</div><div class="line">    {</div><div class="line">      cell_matrix = 0;</div><div class="line">      cell_rhs    = 0;</div><div class="line"></div><div class="line">      fe_values.<a class="code" href="classFEValues.html#a21f914e63d588e2652a9514620653d77">reinit</a>(cell);</div><div class="line"></div><div class="line">      <a class="code" href="namespaceDifferentiation_1_1SD.html#a841a7b84dc17bf2ba675522093a97e8ba945f3fc449518a73b9f5f32868db466c">lambda</a>.value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(), lambda_values);</div><div class="line">      <a class="code" href="namespacemu.html">mu</a>.value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(), mu_values);</div><div class="line"></div><div class="line">      <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">        {</div><div class="line">          <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_i =</div><div class="line">            fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j = 0; j &lt; dofs_per_cell; ++j)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_j =</div><div class="line">                fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(j).first;</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points;</div><div class="line">                   ++q_point)</div><div class="line">                {</div><div class="line">                  <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(i, j) +=</div><div class="line">                    ((fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)[component_i]</div><div class="line">                      fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)[component_j]</div><div class="line">                      lambda_values[q_point]) +</div><div class="line">                     (fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)[component_j]</div><div class="line">                      fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)[component_i]</div><div class="line">                      mu_values[q_point]) +</div><div class="line">                     ((component_i == component_j) ?</div><div class="line">                        (fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)</div><div class="line">                         fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)</div><div class="line">                         mu_values[q_point]) :</div><div class="line">                        0))</div><div class="line">                    fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">                }</div><div class="line">            }</div><div class="line">        }</div><div class="line"></div><div class="line">      right_hand_side.vector_value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(),</div><div class="line">                                        rhs_values);</div><div class="line">      <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">        {</div><div class="line">          <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_i =</div><div class="line">            fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points; ++q_point)</div><div class="line">            cell_rhs(i) += fe_values.<a class="code" href="classFEValuesBase.html#a1dd48cb744013c448d57f8f77640c08d">shape_value</a>(i, q_point)</div><div class="line">                           rhs_values[q_point](component_i)</div><div class="line">                           fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">        }</div><div class="line"></div><div class="line">      cell-&gt;get_dof_indices(local_dof_indices);</div><div class="line">      hanging_node_constraints.distribute_local_to_global(cell_matrix,</div><div class="line">                                                          cell_rhs,</div><div class="line">                                                          local_dof_indices,</div><div class="line">                                                          system_matrix,</div><div class="line">                                                          system_rhs);</div><div class="line">    }</div></div><!-- fragment --><p>The next step is to "compress" the vector and the system matrix. This means that each process sends the additions that were made to those entries of the matrix and vector that the process did not own itself to the process that owns them. After receiving these additions from other processes, each process then adds them to the values it already has. These additions are combining the integral contributions of shape functions living on several cells just as in a serial computation, with the difference that the cells are assigned to different processes.</p>
<div class="fragment"><div class="line">system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div></div><!-- fragment --><p>The global matrix and right hand side vectors have now been formed. We still have to apply boundary values, in the same way as we did, for example, in <a class="el" href="step_3.html">step-3</a> , <a class="el" href="step_4.html">step-4</a> , and a number of other programs. The last argument to the call to <a class="el" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values()</a> below allows for some optimizations. It controls whether we should also delete entries (i.e., set them to zero) in the matrix columns corresponding to boundary nodes, or to keep them (and passing <code>true</code> means: yes, do eliminate the columns). If we do eliminate columns, then the resulting matrix will be symmetric again if it was before; if we don't, then it won't. The solution of the resulting system should be the same, though. The only reason why we may want to make the system symmetric again is that we would like to use the CG method, which only works with symmetric matrices. The reason why we may <em>not</em> want to make the matrix symmetric is because this would require us to write into column entries that actually reside on other processes, i.e., it involves communicating data. This is always expensive. Experience tells us that CG also works (and works almost as well) if we don't remove the columns associated with boundary nodes, which can be explained by the special structure of this particular non-symmetry. To avoid the expense of communication, we therefore do not eliminate the entries in the affected columns.</p>
<div class="fragment"><div class="line">  std::map&lt;types::global_dof_index, double&gt; boundary_values;</div><div class="line">  <a class="code" href="namespaceVectorTools.html#af27ac28c698a9ed0199faed50a204538">VectorTools::interpolate_boundary_values</a>(dof_handler,</div><div class="line">                                           0,</div><div class="line">                                           <a class="code" href="classFunctions_1_1ZeroFunction.html">Functions::ZeroFunction&lt;dim&gt;</a>(dim),</div><div class="line">                                           boundary_values);</div><div class="line">  <a class="code" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values</a>(</div><div class="line">    boundary_values, system_matrix, solution, system_rhs, <span class="keyword">false</span>);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemsolve"></a> </p><h4>ElasticProblem::solve</h4>
<p>Having assembled the linear system, we next need to solve it. PETSc offers a variety of sequential and parallel solvers, for which we have written wrappers that have almost the same interface as is used for the deal.II solvers used in all previous example programs. The following code should therefore look rather familiar. At the top of the function, we set up a convergence monitor, and assign it the accuracy to which we would like to solve the linear system. Next, we create an actual solver object using PETSc's CG solver which also works with parallel (distributed) vectors and matrices. And finally a preconditioner; we choose to use a block Jacobi preconditioner which works by computing an incomplete LU decomposition on each diagonal block of the matrix. (In other words, each MPI process computes an ILU from the rows it stores by throwing away columns that correspond to row indices not stored locally; this yields a square matrix block from which we can compute an ILU. That means that if you run the program with only one process, then you will use an ILU(0) as a preconditioner, while if it is run on many processes, then we will have a number of blocks on the diagonal and the preconditioner is the ILU(0) of each of these blocks. In the extreme case of one degree of freedom per processor, this preconditioner is then simply a Jacobi preconditioner since the diagonal matrix blocks consist of only a single entry. Such a preconditioner is relatively easy to compute because it does not require any kind of communication between processors, but it is in general not very efficient for large numbers of processors.) Following this kind of setup, we then solve the linear system:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> ElasticProblem&lt;dim&gt;::solve()</div><div class="line">{</div><div class="line">  <a class="code" href="classSolverControl.html">SolverControl</a> solver_control(solution.size(), 1<a class="code" href="namespacePhysics_1_1Elasticity_1_1Kinematics.html#a9587d5229555daa5b1fa1ba2f8a40adb">e</a>-8 system_rhs.l2_norm());</div><div class="line">  <a class="code" href="classPETScWrappers_1_1SolverCG.html">PETScWrappers::SolverCG</a> cg(solver_control, mpi_communicator);</div><div class="line"></div><div class="line">  <a class="code" href="classPETScWrappers_1_1PreconditionBlockJacobi.html">PETScWrappers::PreconditionBlockJacobi</a> preconditioner(system_matrix);</div><div class="line"></div><div class="line">  cg.solve(system_matrix, solution, system_rhs, preconditioner);</div></div><!-- fragment --><p>The next step is to distribute hanging node constraints. This is a little tricky, since to fill in the value of a constrained node you need access to the values of the nodes to which it is constrained (for example, for a Q1 element in 2d, we need access to the two nodes on the big side of a hanging node face, to compute the value of the constrained node in the middle). The problem is that we have built our vectors (in <code>setup_system()</code> ) in such a way that every process is responsible for storing only those elements of the solution vector that correspond to the degrees of freedom this process "owns". There are, however, cases where in order to compute the value of the vector entry for a constrained degree of freedom on one process, we need to access vector entries that are stored on other processes. PETSc (and, for that matter, the MPI model on which it is built) does not allow to simply query vector entries stored on other processes, so what we do here is to get a copy of the "distributed" vector where we store all elements locally. This is simple, since the deal.II wrappers have a conversion constructor for the deal.II <a class="el" href="classVector.html">Vector</a> class. (This conversion of course requires communication, but in essence every process only needs to send its data to every other process once in bulk, rather than having to respond to queries for individual elements):</p>
<div class="fragment"><div class="line"><a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution(solution);</div></div><!-- fragment --><p>Of course, as in previous discussions, it is clear that such a step cannot scale very far if you wanted to solve large problems on large numbers of processes, because every process now stores <em>all elements</em> of the solution vector. (We will show how to do this better in <a class="el" href="step_40.html">step-40</a> .) On the other hand, distributing hanging node constraints is simple on this local copy, using the usual function AffineConstraints::distributed(). In particular, we can compute the values of <em>all</em> constrained degrees of freedom, whether the current process owns them or not:</p>
<div class="fragment"><div class="line">hanging_node_constraints.distribute(localized_solution);</div></div><!-- fragment --><p>Then transfer everything back into the global vector. The following operation copies those elements of the localized solution that we store locally in the distributed solution, and does not touch the other ones. Since we do the same operation on all processors, we end up with a distributed vector (i.e., a vector that on every process only stores the vector entries corresponding to degrees of freedom that are owned by this process) that has all the constrained nodes fixed. We end the function by returning the number of iterations it took to converge, to allow for some output.</p>
<div class="fragment"><div class="line">  solution = localized_solution;</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> solver_control.last_step();</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemrefine_grid"></a> </p><h4>ElasticProblem::refine_grid</h4>
<p>Using some kind of refinement indicator, the mesh can be refined. The problem is basically the same as with distributing hanging node constraints: in order to compute the error indicator (even if we were just interested in the indicator on the cells the current process owns), we need access to more elements of the solution vector than just those the current processor stores. To make this happen, we do essentially what we did in <code>solve()</code> already, namely get a <em>complete</em> copy of the solution vector onto every process, and use that to compute. This is in itself expensive as explained above and it is particular unnecessary since we had just created and then destroyed such a vector in <code>solve()</code> , but efficiency is not the point of this program and so let us opt for a design in which every function is as self-contained as possible. Once we have such a "localized" vector that contains <em>all</em> elements of the solution vector, we can compute the indicators for the cells that belong to the present process. In fact, we could of course compute <em>all</em> refinement indicators since our <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects store information about all cells, and since we have a complete copy of the solution vector. But in the interest in showing how to operate in parallel, let us demonstrate how one would operate if one were to only compute <em>some</em> error indicators and then exchange the remaining ones with the other processes. (Ultimately, each process needs a complete set of refinement indicators because every process needs to refine their mesh, and needs to refine it in exactly the same way as all of the other processes.) So, to do all of this, we need to:</p>
<ul>
<li>First, get a local copy of the distributed solution vector.</li>
<li>Second, create a vector to store the refinement indicators.</li>
<li>Third, let the <a class="el" href="classKellyErrorEstimator.html">KellyErrorEstimator</a> compute refinement indicators for all cells belonging to the present subdomain/process. The last argument of the call indicates which subdomain we are interested in. The three arguments before it are various other default arguments that one usually does not need (and does not state values for, but rather uses the defaults), but which we have to state here explicitly since we want to modify the value of a following argument (i.e., the one indicating the subdomain).</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::refine_grid()</div><div class="line">{</div><div class="line">  <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution(solution);</div><div class="line"></div><div class="line">  <a class="code" href="classVector.html">Vector&lt;float&gt;</a> local_error_per_cell(triangulation.n_active_cells());</div><div class="line">  <a class="code" href="classKellyErrorEstimator.html#ae2269e1c9903e9d863b7abd54948af00">KellyErrorEstimator&lt;dim&gt;::estimate</a>(dof_handler,</div><div class="line">                                     <a class="code" href="classQGauss.html">QGauss</a>&lt;dim</div><div class="line"></div><div class="line">- 1&gt;(fe.<a class="code" href="classFiniteElementData.html#a2cbf5ad6b464871261dbd054bced18a8">degree</a> + 1),</div><div class="line">                                     {},</div><div class="line">                                     localized_solution,</div><div class="line">                                     local_error_per_cell,</div><div class="line">                                     <a class="code" href="classComponentMask.html">ComponentMask</a>(),</div><div class="line">                                     <span class="keyword">nullptr</span>,</div><div class="line">                                     <a class="code" href="classMultithreadInfo.html#ad0b84ae105b385b88bdd4bfc0c530995">MultithreadInfo::n_threads</a>(),</div><div class="line">                                     <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>);</div></div><!-- fragment --><p>Now all processes have computed error indicators for their own cells and stored them in the respective elements of the <code>local_error_per_cell</code> vector. The elements of this vector for cells not owned by the present process are zero. However, since all processes have a copy of the entire triangulation and need to keep these copies in sync, they need the values of refinement indicators for all cells of the triangulation. Thus, we need to distribute our results. We do this by creating a distributed vector where each process has its share and sets the elements it has computed. Consequently, when you view this vector as one that lives across all processes, then every element of this vector has been set once. We can then assign this parallel vector to a local, non-parallel vector on each process, making <em>all</em> error indicators available on every process. So in the first step, we need to set up a parallel vector. For simplicity, every process will own a chunk with as many elements as this process owns cells, so that the first chunk of elements is stored with process zero, the next chunk with process one, and so on. It is important to remark, however, that these elements are not necessarily the ones we will write to. This is a consequence of the order in which cells are arranged, i.e., the order in which the elements of the vector correspond to cells is not ordered according to the subdomain these cells belong to. In other words, if on this process we compute indicators for cells of a certain subdomain, we may write the results to more or less random elements of the distributed vector; in particular, they may not necessarily lie within the chunk of vector we own on the present process. They will subsequently have to be copied into another process' memory space, an operation that PETSc does for us when we call the <code><a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a></code> function. This inefficiency could be avoided with some more code, but we refrain from it since it is not a major factor in the program's total runtime. So here is how we do it: count how many cells belong to this process, set up a distributed vector with that many elements to be stored locally, copy over the elements we computed locally, and finally compress the result. In fact, we really only copy the elements that are nonzero, so we may miss a few that we computed to zero, but this won't hurt since the original values of the vector are zero anyway.</p>
<div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_local_cells =</div><div class="line">  <a class="code" href="namespaceGridTools.html#a8c212a30784bec20b1ae13fad3fd579c">GridTools::count_cells_with_subdomain_association</a>(triangulation,</div><div class="line">                                                    <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>);</div><div class="line"><a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> distributed_all_errors(</div><div class="line">  mpi_communicator, triangulation.n_active_cells(), n_local_cells);</div><div class="line"></div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; local_error_per_cell.size(); ++i)</div><div class="line">  <span class="keywordflow">if</span> (local_error_per_cell(i) != 0)</div><div class="line">    distributed_all_errors(i) = local_error_per_cell(i);</div><div class="line">distributed_all_errors.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae5042eefddc828c7c31e1e8e26da8b09">VectorOperation::insert</a>);</div></div><!-- fragment --><p>So now we have this distributed vector that contains the refinement indicators for all cells. To use it, we need to obtain a local copy and then use it to mark cells for refinement or coarsening, and actually do the refinement and coarsening. It is important to recognize that <em>every</em> process does this to its own copy of the triangulation, and does it in exactly the same way.</p>
<div class="fragment"><div class="line">  <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;float&gt;</a> localized_all_errors(distributed_all_errors);</div><div class="line"></div><div class="line">  <a class="code" href="namespaceGridRefinement.html#a48e5395381ed87155942a61a1edd134d">GridRefinement::refine_and_coarsen_fixed_number</a>(triangulation,</div><div class="line">                                                  localized_all_errors,</div><div class="line">                                                  0.3,</div><div class="line">                                                  0.03);</div><div class="line">  triangulation.execute_coarsening_and_refinement();</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemoutput_results"></a> </p><h4>ElasticProblem::output_results</h4>
<p>The final function of significant interest is the one that creates graphical output. This works the same way as in <a class="el" href="step_8.html">step-8</a> , with two small differences. Before discussing these, let us state the general philosophy this function will work: we intend for all of the data to be generated on a single process, and subsequently written to a file. This is, as many other parts of this program already discussed, not something that will scale. Previously, we had argued that we will get into trouble with triangulations, DoFHandlers, and copies of the solution vector where every process has to store all of the data, and that there will come to be a point where each process simply doesn't have enough memory to store that much data. Here, the situation is different: it's not only the memory, but also the run time that's a problem. If one process is responsible for processing <em>all</em> of the data while all of the other processes do nothing, then this one function will eventually come to dominate the overall run time of the program. In particular, the time this function takes is going to be proportional to the overall size of the problem (counted in the number of cells, or the number of degrees of freedom), independent of the number of processes we throw at it. Such situations need to be avoided, and we will show in <a class="el" href="step_18.html">step-18</a> and <a class="el" href="step_40.html">step-40</a> how to address this issue. For the current problem, the solution is to have each process generate output data only for its own local cells, and write them to separate files, one file per process. This is how <a class="el" href="step_18.html">step-18</a> operates. Alternatively, one could simply leave everything in a set of independent files and let the visualization software read all of them (possibly also using multiple processors) and create a single visualization out of all of them; this is the path <a class="el" href="step_40.html">step-40</a> , <a class="el" href="step_32.html">step-32</a> , and all other parallel programs developed later on take. More specifically for the current function, all processes call this function, but not all of them need to do the work associated with generating output. In fact, they shouldn't, since we would try to write to the same file multiple times at once. So we let only the first process do this, and all the other ones idle around during this time (or start their work for the next iteration, or simply yield their CPUs to other jobs that happen to run at the same time). The second thing is that we not only output the solution vector, but also a vector that indicates which subdomain each cell belongs to. This will make for some nice pictures of partitioned domains. To implement this, process zero needs a complete set of solution components in a local vector. Just as with the previous function, the efficient way to do this would be to re-use the vector already created in the <code>solve()</code> function, but to keep things more self-contained, we simply re-create one here from the distributed solution vector. An important thing to realize is that we do this localization operation on all processes, not only the one that actually needs the data. This can't be avoided, however, with the simplified communication model of MPI we use for vectors in this tutorial program: MPI does not have a way to query data on another process, both sides have to initiate a communication at the same time. So even though most of the processes do not need the localized solution, we have to place the statement converting the distributed into a localized vector so that all processes execute it. (Part of this work could in fact be avoided. What we do is send the local parts of all processes to all other processes. What we would really need to do is to initiate an operation on all processes where each process simply sends its local chunk of data to process zero, since this is the only one that actually needs it, i.e., we need something like a gather operation. PETSc can do this, but for simplicity's sake we don't attempt to make use of this here. We don't, since what we do is not very expensive in the grand scheme of things: it is one vector communication among all processes, which has to be compared to the number of communications we have to do when solving the linear system, setting up the block-ILU for the preconditioner, and other operations.)</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword"></span>{</div><div class="line">  <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution(solution);</div></div><!-- fragment --><p>This being done, process zero goes ahead with setting up the output file as in <a class="el" href="step_8.html">step-8</a> , and attaching the (localized) solution vector to the output object.</p>
<div class="fragment"><div class="line"><span class="keywordflow">if</span> (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0)</div><div class="line">  {</div><div class="line">    std::ofstream output(<span class="stringliteral">&quot;solution-&quot;</span> + <a class="code" href="namespacePatterns_1_1Tools.html#a72743302dcb1a0fb1f2f8dc5122d299e">std::to_string</a>(cycle) + <span class="stringliteral">&quot;.vtk&quot;</span>);</div><div class="line"></div><div class="line">    <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a6ed7c846331069f406b8c9933c37fda4">attach_dof_handler</a>(dof_handler);</div><div class="line"></div><div class="line">    std::vector&lt;std::string&gt; solution_names;</div><div class="line">    <span class="keywordflow">switch</span> (dim)</div><div class="line">      {</div><div class="line">        <span class="keywordflow">case</span> 1:</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;displacement&quot;</span>);</div><div class="line">          <span class="keywordflow">break</span>;</div><div class="line">        <span class="keywordflow">case</span> 2:</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">          <span class="keywordflow">break</span>;</div><div class="line">        <span class="keywordflow">case</span> 3:</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;z_displacement&quot;</span>);</div><div class="line">          <span class="keywordflow">break</span>;</div><div class="line">        <span class="keywordflow">default</span>:</div><div class="line">          <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(<span class="keyword">false</span>, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line">      }</div><div class="line"></div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(localized_solution, solution_names);</div></div><!-- fragment --><p>The only other thing we do here is that we also output one value per cell indicating which subdomain (i.e., MPI process) it belongs to. This requires some conversion work, since the data the library provides us with is not the one the output class expects, but this is not difficult. First, set up a vector of integers, one per cell, that is then filled by the subdomain id of each cell. The elements of this vector are then converted to a floating point vector in a second step, and this vector is added to the <a class="el" href="classDataOut.html">DataOut</a> object, which then goes off creating output in VTK format:</p>
<div class="fragment"><div class="line">      std::vector&lt;unsigned int&gt; partition_int(triangulation.n_active_cells());</div><div class="line">      <a class="code" href="namespaceGridTools.html#ac41d959ae1723a898b616c3320241ffe">GridTools::get_subdomain_association</a>(triangulation, partition_int);</div><div class="line"></div><div class="line">      <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> partitioning(partition_int.begin(),</div><div class="line">                                        partition_int.end());</div><div class="line"></div><div class="line">      data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(partitioning, <span class="stringliteral">&quot;partitioning&quot;</span>);</div><div class="line"></div><div class="line">      data_out.<a class="code" href="classDataOut.html#a087f63e22f0614bca326dbdca288c646">build_patches</a>();</div><div class="line">      data_out.<a class="code" href="classDataOutInterface.html#acad99726038e4fca7f605fdffb3317e4">write_vtk</a>(output);</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemrun"></a> </p><h4>ElasticProblem::run</h4>
<p>Lastly, here is the driver function. It is almost completely unchanged from <a class="el" href="step_8.html">step-8</a> , with the exception that we replace <code>std::cout</code> by the <code>pcout</code> stream. Apart from this, the only other cosmetic change is that we output how many degrees of freedom there are per process, and how many iterations it took for the linear solver to converge:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">ElasticProblem&lt;dim&gt;::run</a>()</div><div class="line">{</div><div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle = 0; cycle &lt; 10; ++cycle)</div><div class="line">    {</div><div class="line">      pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">        {</div><div class="line">          <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a>(triangulation,</div><div class="line"></div><div class="line">-1, 1);</div><div class="line">          triangulation.refine_global(3);</div><div class="line">        }</div><div class="line">      <span class="keywordflow">else</span></div><div class="line">        refine_grid();</div><div class="line"></div><div class="line">      pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">            &lt;&lt; triangulation.n_active_cells() &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      setup_system();</div><div class="line"></div><div class="line">      pcout &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span> &lt;&lt; dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>()</div><div class="line">            &lt;&lt; <span class="stringliteral">&quot; (by partition:&quot;</span>;</div><div class="line">      <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p = 0; p &lt; <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>; ++p)</div><div class="line">        pcout &lt;&lt; (p == 0 ? <span class="charliteral">&#39; &#39;</span> : <span class="charliteral">&#39;+&#39;</span>)</div><div class="line">              &lt;&lt; (<a class="code" href="namespaceDoFTools.html#ac704c6d311cd0f289d625427e03708ac">DoFTools::count_dofs_with_subdomain_association</a>(dof_handler,</div><div class="line">                                                                  p));</div><div class="line">      pcout &lt;&lt; <span class="stringliteral">&quot;)&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      assemble_system();</div><div class="line">      <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_iterations = solve();</div><div class="line"></div><div class="line">      pcout &lt;&lt; <span class="stringliteral">&quot;   Solver converged in &quot;</span> &lt;&lt; n_iterations &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span></div><div class="line">            &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      output_results(cycle);</div><div class="line">    }</div><div class="line">}</div><div class="line">} <span class="comment">// namespace Step17</span></div></div><!-- fragment --><p><a class="anchor" id="Thecodemaincodefunction"></a> </p><h3>The <code>main</code> function</h3>
<p>The <code>main()</code> works the same way as most of the main functions in the other example programs, i.e., it delegates work to the <code>run</code> function of a managing object, and only wraps everything into some code to catch exceptions:</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span>*argv)</div><div class="line">{</div><div class="line"><span class="keywordflow">try</span></div><div class="line">  {</div><div class="line">    <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">    <span class="keyword">using namespace </span>Step17;</div></div><!-- fragment --><p>Here is the only real difference: MPI and PETSc both require that we initialize these libraries at the beginning of the program, and un-initialize them at the end. The class MPI_InitFinalize takes care of all of that. The trailing argument <code>1</code> means that we do want to run each MPI process with a single thread, a prerequisite with the PETSc parallel linear algebra.</p>
<div class="fragment"><div class="line">    <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">    ElasticProblem&lt;2&gt; elastic_problem;</div><div class="line">    elastic_problem.run();</div><div class="line">  }</div><div class="line"><span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">  {</div><div class="line">    std::cerr &lt;&lt; std::endl</div><div class="line">              &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line">    std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">              &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    <span class="keywordflow">return</span> 1;</div><div class="line">  }</div><div class="line"><span class="keywordflow">catch</span> (...)</div><div class="line">  {</div><div class="line">    std::cerr &lt;&lt; std::endl</div><div class="line">              &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line">    std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line">    <span class="keywordflow">return</span> 1;</div><div class="line">  }</div><div class="line"></div><div class="line"><span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p> <a class="anchor" id="Results"></a></p><h1>Results</h1>
<p>If the program above is compiled and run on a single processormachine, it should generate results that are very similar to thosethat we already got with <a class="el" href="step_8.html">step-8</a> . However, it becomes more interestingif we run it on a multicore machine or a cluster of computers. Themost basic way to run MPI programs is using a command line like </p><div class="fragment"><div class="line">mpirun</div><div class="line"></div><div class="line">-np 32 ./step-17</div></div><!-- fragment --><p> to run the <a class="el" href="step_17.html">step-17</a> executable with 32 processors. (If you work on a cluster, then there is typically a step in between where youneed to set up a job script and submit the script to a scheduler. The schedulerwill execute the script whenever it can allocate 32 unused processors for yourjob. How to write such jobscripts differs from cluster to cluster, and you should find the documentationof your cluster to see how to do this. On my system, I have to use the command <code>qsub</code> with a whole host of options to run a job in parallel.) Whether directly or through a scheduler, if you run this program on 8processors, you should get output like the following: </p><div class="fragment"><div class="line">Cycle 0:</div><div class="line">Number of active cells:       64</div><div class="line">Number of degrees of freedom: 162 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 22+22+20+20+18+16+20+24)</div><div class="line">Solver converged in 23 iterations.</div><div class="line">Cycle 1:</div><div class="line">Number of active cells:       124</div><div class="line">Number of degrees of freedom: 302 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 38+42+36+34+44+44+36+28)</div><div class="line">Solver converged in 35 iterations.</div><div class="line">Cycle 2:</div><div class="line">Number of active cells:       238</div><div class="line">Number of degrees of freedom: 570 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 68+80+66+74+58+68+78+78)</div><div class="line">Solver converged in 46 iterations.</div><div class="line">Cycle 3:</div><div class="line">Number of active cells:       454</div><div class="line">Number of degrees of freedom: 1046 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 120+134+124+130+154+138+122+124)</div><div class="line">Solver converged in 55 iterations.</div><div class="line">Cycle 4:</div><div class="line">Number of active cells:       868</div><div class="line">Number of degrees of freedom: 1926 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 232+276+214+248+230+224+234+268)</div><div class="line">Solver converged in 77 iterations.</div><div class="line">Cycle 5:</div><div class="line">Number of active cells:       1654</div><div class="line">Number of degrees of freedom: 3550 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 418+466+432+470+442+474+424+424)</div><div class="line">Solver converged in 93 iterations.</div><div class="line">Cycle 6:</div><div class="line">Number of active cells:       3136</div><div class="line">Number of degrees of freedom: 6702 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 838+796+828+892+866+798+878+806)</div><div class="line">Solver converged in 127 iterations.</div><div class="line">Cycle 7:</div><div class="line">Number of active cells:       5962</div><div class="line">Number of degrees of freedom: 12446 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 1586+1484+1652+1552+1556+1576+1560+1480)</div><div class="line">Solver converged in 158 iterations.</div><div class="line">Cycle 8:</div><div class="line">Number of active cells:       11320</div><div class="line">Number of degrees of freedom: 23586 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 2988+2924+2890+2868+2864+3042+2932+3078)</div><div class="line">Solver converged in 225 iterations.</div><div class="line">Cycle 9:</div><div class="line">Number of active cells:       21424</div><div class="line">Number of degrees of freedom: 43986 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 5470+5376+5642+5450+5630+5470+5416+5532)</div><div class="line">Solver converged in 282 iterations.</div><div class="line">Cycle 10:</div><div class="line">Number of active cells:       40696</div><div class="line">Number of degrees of freedom: 83754 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 10660+10606+10364+10258+10354+10322+10586+10604)</div><div class="line">Solver converged in 392 iterations.</div><div class="line">Cycle 11:</div><div class="line">Number of active cells:       76978</div><div class="line">Number of degrees of freedom: 156490 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 19516+20148+19390+19390+19336+19450+19730+19530)</div><div class="line">Solver converged in 509 iterations.</div><div class="line">Cycle 12:</div><div class="line">Number of active cells:       146206</div><div class="line">Number of degrees of freedom: 297994 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 37462+37780+37000+37060+37232+37328+36860+37272)</div><div class="line">Solver converged in 705 iterations.</div><div class="line">Cycle 13:</div><div class="line">Number of active cells:       276184</div><div class="line">Number of degrees of freedom: 558766 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 69206+69404+69882+71266+70348+69616+69796+69248)</div><div class="line">Solver converged in 945 iterations.</div><div class="line">Cycle 14:</div><div class="line">Number of active cells:       523000</div><div class="line">Number of degrees of freedom: 1060258 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 132928+132296+131626+132172+132170+133588+132252+133226)</div><div class="line">Solver converged in 1282 iterations.</div><div class="line">Cycle 15:</div><div class="line">Number of active cells:       987394</div><div class="line">Number of degrees of freedom: 1994226 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 253276+249068+247430+248402+248496+251380+248272+247902)</div><div class="line">Solver converged in 1760 iterations.</div><div class="line">Cycle 16:</div><div class="line">Number of active cells:       1867477</div><div class="line">Number of degrees of freedom: 3771884 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 468452+474204+470818+470884+469960+</div><div class="line">471186+470686+475694)</div><div class="line">Solver converged in 2251 iterations.</div></div><!-- fragment --><p> (This run uses a few more refinement cycles than the code available inthe examples/ directory. The run also used a version of METIS from2004 that generated different partitionings; consequently,the numbers you get today are slightly different.) As can be seen, we can easily get to almost four million unknowns. In fact, thecode's runtime with 8 processes was less than 7 minutes up to (and including)cycle 14, and 14 minutes including the second to last step. (These are numbersrelevant to when the code was initially written, in 2004.) I lost the timinginformation for the last step, though, but you get the idea. All this is afterrelease mode has been enabled by running <code>make release</code> , andwith the generation of graphical output switched off for the reasons stated inthe program comments above.( See also <a href="http://www.math.colostate.edu/~bangerth/videos.676.18.html">video lecture 18</a>.) The biggest 2d computations I did had roughly 7.1million unknowns, and were done on 32 processes. It took about 40 minutes.Not surprisingly, the limiting factor for how far one can go is how much memoryone has, since every process has to hold the entire mesh and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects,although matrices and vectors are split up. For the 7.1M computation, the memoryconsumption was about 600 bytes per unknown, which is not bad, but one has toconsider that this is for every unknown, whether we store the matrix and vectorentries locally or not.</p>
<p>Here is some output generated in the 12th cycle of the program, i.e. with roughly300,000 unknowns: </p><table align="center" style="width:80%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-ux.png" width="100%"/>
</div>
 </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-uy.png" width="100%"/>
</div>
  </td></tr>
</table>
<p>As one would hope for, the x- (left) and y-displacements (right) shown hereclosely match what we already saw in <a class="el" href="step_8.html">step-8</a> . As shownthere and in <a class="el" href="step_22.html">step-22</a> , we could as well have produced avector plot of the displacement field, rather than plotting it as twoseparate scalar fields. What may be more interesting,though, is to look at the mesh and partition at this step: </p><table align="center" width="80%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-grid.png" width="100%"/>
</div>
 </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-partition.png" width="100%"/>
</div>
  </td></tr>
</table>
<p>Again, the mesh (left) shows the same refinement pattern as seenpreviously. The right panel shows the partitioning of the domain across the 8processes, each indicated by a different color. The picture shows that thesubdomains are smaller where mesh cells are small, a fact that needs to beexpected given that the partitioning algorithm tries to equilibrate the numberof cells in each subdomain; this equilibration is also easily identified inthe output shown above, where the number of degrees per subdomain is roughlythe same.</p>
<p>It is worth noting that if we ran the same program with a different number ofprocesses, that we would likely get slightly different output: a differentmesh, different number of unknowns and iterations to convergence. The reasonfor this is that while the matrix and right hand side are the same independentof the number of processes used, the preconditioner is not: it performs anILU(0) on the chunk of the matrix of <em> each processor separately </em> . Thus,it's effectiveness as a preconditioner diminishes as the number of processesincreases, which makes the number of iterations increase. Since a differentpreconditioner leads to slight changes in the computed solution, this willthen lead to slightly different mesh cells tagged for refinement, and largerdifferences in subsequent steps. The solution will always look very similar,though.</p>
<p>Finally, here are some results for a 3d simulation. You can repeat these bychanging </p><div class="fragment"><div class="line">ElasticProblem&lt;2&gt; elastic_problem;</div></div><!-- fragment --><p> to </p><div class="fragment"><div class="line">ElasticProblem&lt;3&gt; elastic_problem;</div></div><!-- fragment --><p> in the main function. If you then run the program in parallel,you get something similar to this (this is for a job with 16 processes): </p><div class="fragment"><div class="line">Cycle 0:</div><div class="line">Number of active cells:       512</div><div class="line">Number of degrees of freedom: 2187 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 114+156+150+114+114+210+105+102+120+120+96+123+141+183+156+183)</div><div class="line">Solver converged in 27 iterations.</div><div class="line">Cycle 1:</div><div class="line">Number of active cells:       1604</div><div class="line">Number of degrees of freedom: 6549 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 393+291+342+354+414+417+570+366+444+288+543+525+345+387+489+381)</div><div class="line">Solver converged in 42 iterations.</div><div class="line">Cycle 2:</div><div class="line">Number of active cells:       4992</div><div class="line">Number of degrees of freedom: 19167 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 1428+1266+1095+1005+1455+1257+1410+1041+1320+1380+1080+1050+963+1005+1188+1224)</div><div class="line">Solver converged in 65 iterations.</div><div class="line">Cycle 3:</div><div class="line">Number of active cells:       15485</div><div class="line">Number of degrees of freedom: 56760 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 3099+3714+3384+3147+4332+3858+3615+3117+3027+3888+3942+3276+4149+3519+3030+3663)</div><div class="line">Solver converged in 96 iterations.</div><div class="line">Cycle 4:</div><div class="line">Number of active cells:       48014</div><div class="line">Number of degrees of freedom: 168762 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 11043+10752+9846+10752+9918+10584+10545+11433+12393+11289+10488+9885+10056+9771+11031+8976)</div><div class="line">Solver converged in 132 iterations.</div><div class="line">Cycle 5:</div><div class="line">Number of active cells:       148828</div><div class="line">Number of degrees of freedom: 492303 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 31359+30588+34638+32244+30984+28902+33297+31569+29778+29694+28482+28032+32283+30702+31491+28260)</div><div class="line">Solver converged in 179 iterations.</div><div class="line">Cycle 6:</div><div class="line">Number of active cells:       461392</div><div class="line">Number of degrees of freedom: 1497951 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 103587+100827+97611+93726+93429+88074+95892+88296+96882+93000+87864+90915+92232+86931+98091+90594)</div><div class="line">Solver converged in 261 iterations.</div></div><!-- fragment --><p>The last step, going up to 1.5 million unknowns, takes about 55 minutes with16 processes on 8 dual-processor machines (of the kind available in 2003). Thegraphical output generated bythis job is rather large (cycle 5 already prints around 82 MB of data), sowe contend ourselves with showing output from cycle 4: </p><table width="80%" align="center">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.4-3d-partition.png" width="100%"/>
</div>
 </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.4-3d-ux.png" width="100%"/>
</div>
  </td></tr>
</table>
<p>The left picture shows the partitioning of the cube into 16 processes, whereasthe right one shows the x-displacement along two cutplanes through the cube.</p>
<p><a class="anchor" id="extensions"></a><a class="anchor" id="Possibilitiesforextensions"></a></p><h3>Possibilities for extensions</h3>
<p>The program keeps a complete copy of the <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> objectson every processor. It also creates complete copies of the solution vector,and it creates output on only one processor. All of this is obviouslythe bottleneck as far as parallelization is concerned. Internally, within deal.II, parallelizing the datastructures used in hierarchic and unstructured triangulations is a hardproblem, and it took us a few more years to make this happen. The <a class="el" href="step_40.html">step-40</a> tutorial program and the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> documentation module talk about howto do these steps and what it takes from an application perspective. Anobvious extension of the current program would be to use this functionality tocompletely distribute computations to many more processors than used here.</p>
<p><a class="anchor" id="PlainProg"></a></p><h1>The plain program</h1>
<div class="fragment"><div class="line"><span class="comment">/* ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Copyright (C) 2000 - 2021 by the deal.II authors</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * This file is part of the deal.II library.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * The deal.II library is free software; you can use it, redistribute</span></div><div class="line"><span class="comment"> * it, and/or modify it under the terms of the GNU Lesser General</span></div><div class="line"><span class="comment"> * Public License as published by the Free Software Foundation; either</span></div><div class="line"><span class="comment"> * version 2.1 of the License, or (at your option) any later version.</span></div><div class="line"><span class="comment"> * The full text of the license can be found in the file LICENSE.md at</span></div><div class="line"><span class="comment"> * the top level directory of deal.II.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Author: Wolfgang Bangerth, University of Texas at Austin, 2000, 2004</span></div><div class="line"><span class="comment"> *         Wolfgang Bangerth, Texas A&amp;M University, 2016</span></div><div class="line"><span class="comment"> */</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="quadrature__lib_8h.html">deal.II/base/quadrature_lib.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="function_8h.html">deal.II/base/function.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="logstream_8h.html">deal.II/base/logstream.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="multithread__info_8h.html">deal.II/base/multithread_info.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8h.html">deal.II/lac/vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="full__matrix_8h.html">deal.II/lac/full_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="affine__constraints_8h.html">deal.II/lac/affine_constraints.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dynamic__sparsity__pattern_8h.html">deal.II/lac/dynamic_sparsity_pattern.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="sparsity__tools_8h.html">deal.II/lac/sparsity_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid_2tria_8h.html">deal.II/grid/tria.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__generator_8h.html">deal.II/grid/grid_generator.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid_2grid__refinement_8h.html">deal.II/grid/grid_refinement.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dofs_2dof__handler_8h.html">deal.II/dofs/dof_handler.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__tools_8h.html">deal.II/dofs/dof_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe_2fe__values_8h.html">deal.II/fe/fe_values.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__system_8h.html">deal.II/fe/fe_system.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__q_8h.html">deal.II/fe/fe_q.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector__tools_8h.html">deal.II/numerics/vector_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="matrix__tools_8h.html">deal.II/numerics/matrix_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="numerics_2data__out_8h.html">deal.II/numerics/data_out.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="error__estimator_8h.html">deal.II/numerics/error_estimator.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="conditional__ostream_8h.html">deal.II/base/conditional_ostream.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="mpi_8h.html">deal.II/base/mpi.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__vector_8h.html">deal.II/lac/petsc_vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__sparse__matrix_8h.html">deal.II/lac/petsc_sparse_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__solver_8h.html">deal.II/lac/petsc_solver.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__precondition_8h.html">deal.II/lac/petsc_precondition.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__tools_8h.html">deal.II/grid/grid_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__renumbering_8h.html">deal.II/dofs/dof_renumbering.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>Step17</div><div class="line">{</div><div class="line">  <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keyword">class </span>ElasticProblem</div><div class="line">  {</div><div class="line">  <span class="keyword">public</span>:</div><div class="line">    ElasticProblem();</div><div class="line">    <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">run</a>();</div><div class="line"></div><div class="line">  <span class="keyword">private</span>:</div><div class="line">    <span class="keywordtype">void</span>         setup_system();</div><div class="line">    <span class="keywordtype">void</span>         assemble_system();</div><div class="line">    <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> solve();</div><div class="line">    <span class="keywordtype">void</span>         refine_grid();</div><div class="line">    <span class="keywordtype">void</span>         output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div><div class="line"></div><div class="line">    <a class="code" href="classMPI__Comm.html">MPI_Comm</a> mpi_communicator;</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>;</div><div class="line"></div><div class="line">    <a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div><div class="line"></div><div class="line">    <a class="code" href="classTriangulation.html">Triangulation&lt;dim&gt;</a> <a class="code" href="p4est__wrappers_8cc.html#ace00f2f80d9780ef9aa1007e1c22c6a4">triangulation</a>;</div><div class="line">    <a class="code" href="classFESystem.html">FESystem&lt;dim&gt;</a>      fe;</div><div class="line">    <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a>    dof_handler;</div><div class="line"></div><div class="line">    <a class="code" href="classAffineConstraints.html">AffineConstraints&lt;double&gt;</a> hanging_node_constraints;</div><div class="line"></div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1SparseMatrix.html">PETScWrappers::MPI::SparseMatrix</a> system_matrix;</div><div class="line"></div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> solution;</div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> system_rhs;</div><div class="line">  };</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keyword">class </span>RightHandSide : <span class="keyword">public</span> <a class="code" href="classFunction.html">Function</a>&lt;dim&gt;</div><div class="line">  {</div><div class="line">  <span class="keyword">public</span>:</div><div class="line">    <span class="keyword">virtual</span> <span class="keywordtype">void</span> vector_value(<span class="keyword">const</span> <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &amp;p,</div><div class="line">                              Vector&lt;double&gt; &amp;  values)<span class="keyword"> const override</span></div><div class="line"><span class="keyword">    </span>{</div><div class="line">      <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(values.size() == dim, <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a>(values.size(), dim));</div><div class="line">      <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(dim &gt;= 2, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line"></div><div class="line">      <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> point_1, point_2;</div><div class="line">      point_1(0) = 0.5;</div><div class="line">      point_2(0) = -0.5;</div><div class="line"></div><div class="line">      <span class="keywordflow">if</span> (((p - point_1).norm_square() &lt; 0.2 * 0.2) ||</div><div class="line">          ((p - point_2).norm_square() &lt; 0.2 * 0.2))</div><div class="line">        <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(0) = 1;</div><div class="line">      <span class="keywordflow">else</span></div><div class="line">        <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(0) = 0;</div><div class="line"></div><div class="line">      <span class="keywordflow">if</span> (p.<a class="code" href="classPoint.html#a859ea7f3bf3e64be2e0f5ed1bfcc8550">square</a>() &lt; 0.2 * 0.2)</div><div class="line">        <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(1) = 1;</div><div class="line">      <span class="keywordflow">else</span></div><div class="line">        <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(1) = 0;</div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="keyword">virtual</span> <span class="keywordtype">void</span></div><div class="line">    vector_value_list(<span class="keyword">const</span> std::vector&lt;<a class="code" href="classPoint.html">Point&lt;dim&gt;</a>&gt; &amp;points,</div><div class="line">                      std::vector&lt;Vector&lt;double&gt;&gt; &amp;  value_list)<span class="keyword"> const override</span></div><div class="line"><span class="keyword">    </span>{</div><div class="line">      <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_points = points.size();</div><div class="line"></div><div class="line">      <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(value_list.size() == n_points,</div><div class="line">             <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a>(value_list.size(), n_points));</div><div class="line"></div><div class="line">      <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p = 0; p &lt; n_points; ++p)</div><div class="line">        RightHandSide&lt;dim&gt;::vector_value(points[p], value_list[p]);</div><div class="line">    }</div><div class="line">  };</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  ElasticProblem&lt;dim&gt;::ElasticProblem()</div><div class="line">    : mpi_communicator(MPI_COMM_WORLD)</div><div class="line">    , <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(mpi_communicator))</div><div class="line">    , <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator))</div><div class="line">    , pcout(<a class="code" href="namespacestd.html">std</a>::cout, (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0))</div><div class="line">    , fe(<a class="code" href="classFE__Q.html">FE_Q</a>&lt;dim&gt;(1), dim)</div><div class="line">    , dof_handler(triangulation)</div><div class="line">  {}</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::setup_system()</div><div class="line">  {</div><div class="line">    <a class="code" href="namespaceGridTools.html#a99eba8e3b388258eda37a2724579dd1d">GridTools::partition_triangulation</a>(<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>, triangulation);</div><div class="line"></div><div class="line">    dof_handler.<a class="code" href="classDoFHandler.html#a553ca864aaf70330d9be86bc78f36d1e">distribute_dofs</a>(fe);</div><div class="line">    <a class="code" href="namespaceDoFRenumbering.html#a166d0082503ec2b8d9c560222b48f9d2">DoFRenumbering::subdomain_wise</a>(dof_handler);</div><div class="line"></div><div class="line">    hanging_node_constraints.clear();</div><div class="line">    <a class="code" href="group__constraints.html#ga3b4ea7dfd313e388d868c4e4aa685799">DoFTools::make_hanging_node_constraints</a>(dof_handler,</div><div class="line">                                            hanging_node_constraints);</div><div class="line">    hanging_node_constraints.close();</div><div class="line"></div><div class="line">    <a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>(), dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>());</div><div class="line">    <a class="code" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a>(dof_handler,</div><div class="line">                                    dsp,</div><div class="line">                                    hanging_node_constraints,</div><div class="line">                                    <span class="keyword">false</span>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> std::vector&lt;IndexSet&gt; locally_owned_dofs_per_proc =</div><div class="line">      <a class="code" href="namespaceDoFTools.html#a180d7118ecf27d72afbfecb7978c5e09">DoFTools::locally_owned_dofs_per_subdomain</a>(dof_handler);</div><div class="line">    <span class="keyword">const</span> <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs =</div><div class="line">      locally_owned_dofs_per_proc[<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>];</div><div class="line"></div><div class="line">    system_matrix.reinit(locally_owned_dofs,</div><div class="line">                         locally_owned_dofs,</div><div class="line">                         dsp,</div><div class="line">                         mpi_communicator);</div><div class="line"></div><div class="line">    solution.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line">    system_rhs.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::assemble_system()</div><div class="line">  {</div><div class="line">    <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a>   quadrature_formula(fe.<a class="code" href="classFiniteElementData.html#a2cbf5ad6b464871261dbd054bced18a8">degree</a> + 1);</div><div class="line">    <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values(fe,</div><div class="line">                            quadrature_formula,</div><div class="line">                            <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                              <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> dofs_per_cell = fe.<a class="code" href="classFiniteElementData.html#a33b522422da89e5c080e7405ad49d7c7">n_dofs_per_cell</a>();</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_q_points    = quadrature_formula.size();</div><div class="line"></div><div class="line">    <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a> <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(dofs_per_cell, dofs_per_cell);</div><div class="line">    Vector&lt;double&gt;     cell_rhs(dofs_per_cell);</div><div class="line"></div><div class="line">    std::vector&lt;types::global_dof_index&gt; local_dof_indices(dofs_per_cell);</div><div class="line"></div><div class="line">    std::vector&lt;double&gt; lambda_values(n_q_points);</div><div class="line">    std::vector&lt;double&gt; mu_values(n_q_points);</div><div class="line"></div><div class="line">    <a class="code" href="classFunctions_1_1ConstantFunction.html">Functions::ConstantFunction&lt;dim&gt;</a> <a class="code" href="namespaceDifferentiation_1_1SD.html#a841a7b84dc17bf2ba675522093a97e8ba945f3fc449518a73b9f5f32868db466c">lambda</a>(1.), <a class="code" href="namespacemu.html">mu</a>(1.);</div><div class="line"></div><div class="line">    RightHandSide&lt;dim&gt;          right_hand_side;</div><div class="line">    std::vector&lt;Vector&lt;double&gt;&gt; rhs_values(n_q_points, Vector&lt;double&gt;(dim));</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;cell : dof_handler.<a class="code" href="group__CPP11.html#gacdb6d3adec96a13a7079f6c893e1d3ff">active_cell_iterators</a>())</div><div class="line">      <span class="keywordflow">if</span> (cell-&gt;subdomain_id() == <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>)</div><div class="line">        {</div><div class="line">          cell_matrix = 0;</div><div class="line">          cell_rhs    = 0;</div><div class="line"></div><div class="line">          fe_values.<a class="code" href="classFEValues.html#a21f914e63d588e2652a9514620653d77">reinit</a>(cell);</div><div class="line"></div><div class="line">          <a class="code" href="namespaceDifferentiation_1_1SD.html#a841a7b84dc17bf2ba675522093a97e8ba945f3fc449518a73b9f5f32868db466c">lambda</a>.value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(), lambda_values);</div><div class="line">          <a class="code" href="namespacemu.html">mu</a>.value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(), mu_values);</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_i =</div><div class="line">                fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j = 0; j &lt; dofs_per_cell; ++j)</div><div class="line">                {</div><div class="line">                  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_j =</div><div class="line">                    fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(j).first;</div><div class="line"></div><div class="line">                  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points;</div><div class="line">                       ++q_point)</div><div class="line">                    {</div><div class="line">                      <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(i, j) +=</div><div class="line">                        ((fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)[component_i] *</div><div class="line">                          fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)[component_j] *</div><div class="line">                          lambda_values[q_point]) +</div><div class="line">                         (fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)[component_j] *</div><div class="line">                          fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)[component_i] *</div><div class="line">                          mu_values[q_point]) +</div><div class="line">                         ((component_i == component_j) ?</div><div class="line">                            (fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point) *</div><div class="line">                             fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point) *</div><div class="line">                             mu_values[q_point]) :</div><div class="line">                            0)) *</div><div class="line">                        fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">                    }</div><div class="line">                }</div><div class="line">            }</div><div class="line"></div><div class="line">          right_hand_side.vector_value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(),</div><div class="line">                                            rhs_values);</div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_i =</div><div class="line">                fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points; ++q_point)</div><div class="line">                cell_rhs(i) += fe_values.<a class="code" href="classFEValuesBase.html#a1dd48cb744013c448d57f8f77640c08d">shape_value</a>(i, q_point) *</div><div class="line">                               rhs_values[q_point](component_i) *</div><div class="line">                               fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">            }</div><div class="line"></div><div class="line">          cell-&gt;get_dof_indices(local_dof_indices);</div><div class="line">          hanging_node_constraints.distribute_local_to_global(cell_matrix,</div><div class="line">                                                              cell_rhs,</div><div class="line">                                                              local_dof_indices,</div><div class="line">                                                              system_matrix,</div><div class="line">                                                              system_rhs);</div><div class="line">        }</div><div class="line"></div><div class="line">    system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">    system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line"></div><div class="line">    std::map&lt;types::global_dof_index, double&gt; boundary_values;</div><div class="line">    <a class="code" href="namespaceVectorTools.html#af27ac28c698a9ed0199faed50a204538">VectorTools::interpolate_boundary_values</a>(dof_handler,</div><div class="line">                                             0,</div><div class="line">                                             <a class="code" href="classFunctions_1_1ZeroFunction.html">Functions::ZeroFunction&lt;dim&gt;</a>(dim),</div><div class="line">                                             boundary_values);</div><div class="line">    <a class="code" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values</a>(</div><div class="line">      boundary_values, system_matrix, solution, system_rhs, <span class="keyword">false</span>);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> ElasticProblem&lt;dim&gt;::solve()</div><div class="line">  {</div><div class="line">    <a class="code" href="classSolverControl.html">SolverControl</a> solver_control(solution.size(), 1<a class="code" href="namespacePhysics_1_1Elasticity_1_1Kinematics.html#a9587d5229555daa5b1fa1ba2f8a40adb">e</a>-8 * system_rhs.l2_norm());</div><div class="line">    <a class="code" href="classPETScWrappers_1_1SolverCG.html">PETScWrappers::SolverCG</a> cg(solver_control, mpi_communicator);</div><div class="line"></div><div class="line">    <a class="code" href="classPETScWrappers_1_1PreconditionBlockJacobi.html">PETScWrappers::PreconditionBlockJacobi</a> preconditioner(system_matrix);</div><div class="line"></div><div class="line">    cg.solve(system_matrix, solution, system_rhs, preconditioner);</div><div class="line"></div><div class="line">    Vector&lt;double&gt; localized_solution(solution);</div><div class="line"></div><div class="line">    hanging_node_constraints.distribute(localized_solution);</div><div class="line"></div><div class="line">    solution = localized_solution;</div><div class="line"></div><div class="line">    <span class="keywordflow">return</span> solver_control.last_step();</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::refine_grid()</div><div class="line">  {</div><div class="line">    <span class="keyword">const</span> Vector&lt;double&gt; localized_solution(solution);</div><div class="line"></div><div class="line">    Vector&lt;float&gt; local_error_per_cell(triangulation.n_active_cells());</div><div class="line">    <a class="code" href="classKellyErrorEstimator.html#ae2269e1c9903e9d863b7abd54948af00">KellyErrorEstimator&lt;dim&gt;::estimate</a>(dof_handler,</div><div class="line">                                       <a class="code" href="classQGauss.html">QGauss&lt;dim - 1&gt;</a>(fe.<a class="code" href="classFiniteElementData.html#a2cbf5ad6b464871261dbd054bced18a8">degree</a> + 1),</div><div class="line">                                       {},</div><div class="line">                                       localized_solution,</div><div class="line">                                       local_error_per_cell,</div><div class="line">                                       <a class="code" href="classComponentMask.html">ComponentMask</a>(),</div><div class="line">                                       <span class="keyword">nullptr</span>,</div><div class="line">                                       <a class="code" href="classMultithreadInfo.html#ad0b84ae105b385b88bdd4bfc0c530995">MultithreadInfo::n_threads</a>(),</div><div class="line">                                       <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_local_cells =</div><div class="line">      <a class="code" href="namespaceGridTools.html#a8c212a30784bec20b1ae13fad3fd579c">GridTools::count_cells_with_subdomain_association</a>(triangulation,</div><div class="line">                                                        <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>);</div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> distributed_all_errors(</div><div class="line">      mpi_communicator, triangulation.n_active_cells(), n_local_cells);</div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; local_error_per_cell.size(); ++i)</div><div class="line">      <span class="keywordflow">if</span> (local_error_per_cell(i) != 0)</div><div class="line">        distributed_all_errors(i) = local_error_per_cell(i);</div><div class="line">    distributed_all_errors.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae5042eefddc828c7c31e1e8e26da8b09">VectorOperation::insert</a>);</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keyword">const</span> Vector&lt;float&gt; localized_all_errors(distributed_all_errors);</div><div class="line"></div><div class="line">    <a class="code" href="namespaceGridRefinement.html#a48e5395381ed87155942a61a1edd134d">GridRefinement::refine_and_coarsen_fixed_number</a>(triangulation,</div><div class="line">                                                    localized_all_errors,</div><div class="line">                                                    0.3,</div><div class="line">                                                    0.03);</div><div class="line">    triangulation.execute_coarsening_and_refinement();</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <span class="keyword">const</span> Vector&lt;double&gt; localized_solution(solution);</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0)</div><div class="line">      {</div><div class="line">        std::ofstream output(<span class="stringliteral">&quot;solution-&quot;</span> + <a class="code" href="namespacePatterns_1_1Tools.html#a72743302dcb1a0fb1f2f8dc5122d299e">std::to_string</a>(cycle) + <span class="stringliteral">&quot;.vtk&quot;</span>);</div><div class="line"></div><div class="line">        <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">        data_out.<a class="code" href="classDataOut__DoFData.html#a6ed7c846331069f406b8c9933c37fda4">attach_dof_handler</a>(dof_handler);</div><div class="line"></div><div class="line">        std::vector&lt;std::string&gt; solution_names;</div><div class="line">        <span class="keywordflow">switch</span> (dim)</div><div class="line">          {</div><div class="line">            <span class="keywordflow">case</span> 1:</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;displacement&quot;</span>);</div><div class="line">              <span class="keywordflow">break</span>;</div><div class="line">            <span class="keywordflow">case</span> 2:</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">              <span class="keywordflow">break</span>;</div><div class="line">            <span class="keywordflow">case</span> 3:</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;z_displacement&quot;</span>);</div><div class="line">              <span class="keywordflow">break</span>;</div><div class="line">            <span class="keywordflow">default</span>:</div><div class="line">              <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(<span class="keyword">false</span>, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line">          }</div><div class="line"></div><div class="line">        data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(localized_solution, solution_names);</div><div class="line"></div><div class="line">        std::vector&lt;unsigned int&gt; partition_int(triangulation.n_active_cells());</div><div class="line">        <a class="code" href="namespaceGridTools.html#ac41d959ae1723a898b616c3320241ffe">GridTools::get_subdomain_association</a>(triangulation, partition_int);</div><div class="line"></div><div class="line">        <span class="keyword">const</span> Vector&lt;double&gt; partitioning(partition_int.begin(),</div><div class="line">                                          partition_int.end());</div><div class="line"></div><div class="line">        data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(partitioning, <span class="stringliteral">&quot;partitioning&quot;</span>);</div><div class="line"></div><div class="line">        data_out.<a class="code" href="classDataOut.html#a087f63e22f0614bca326dbdca288c646">build_patches</a>();</div><div class="line">        data_out.<a class="code" href="classDataOutInterface.html#acad99726038e4fca7f605fdffb3317e4">write_vtk</a>(output);</div><div class="line">      }</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">ElasticProblem&lt;dim&gt;::run</a>()</div><div class="line">  {</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle = 0; cycle &lt; 10; ++cycle)</div><div class="line">      {</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">          {</div><div class="line">            <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a>(triangulation, -1, 1);</div><div class="line">            triangulation.refine_global(3);</div><div class="line">          }</div><div class="line">        <span class="keywordflow">else</span></div><div class="line">          refine_grid();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">              &lt;&lt; triangulation.n_active_cells() &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        setup_system();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span> &lt;&lt; dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>()</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot; (by partition:&quot;</span>;</div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p = 0; p &lt; <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>; ++p)</div><div class="line">          pcout &lt;&lt; (p == 0 ? <span class="charliteral">&#39; &#39;</span> : <span class="charliteral">&#39;+&#39;</span>)</div><div class="line">                &lt;&lt; (<a class="code" href="namespaceDoFTools.html#ac704c6d311cd0f289d625427e03708ac">DoFTools::count_dofs_with_subdomain_association</a>(dof_handler,</div><div class="line">                                                                    p));</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;)&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        assemble_system();</div><div class="line">        <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_iterations = solve();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Solver converged in &quot;</span> &lt;&lt; n_iterations &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        output_results(cycle);</div><div class="line">      }</div><div class="line">  }</div><div class="line">} <span class="comment">// namespace Step17</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> **argv)</div><div class="line">{</div><div class="line">  <span class="keywordflow">try</span></div><div class="line">    {</div><div class="line">      <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">      <span class="keyword">using namespace </span>Step17;</div><div class="line"></div><div class="line">      <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">      ElasticProblem&lt;2&gt; elastic_problem;</div><div class="line">      elastic_problem.run();</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (...)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p>This tutorial depends on <a class="el" href="step_8.html">step-8</a>.</p>
<p> 
<table class="tutorial" width="50%">
<tr><th colspan="2"><b><small>Table of contents</small></b></th></tr>
<tr><td width="50%" valign="top">
<ol>
  <li> <a href="#Intro" class=bold>Introduction</a>
    <ul>
        <li><a href="#Overview">Overview</a>
        <li><a href="#ParallelizingsoftwarewithMPI">Parallelizing software with MPI</a>
        <li><a href="#Whatthisprogramdoes">What this program does</a>
    </ul>
  <li> <a href="#CommProg" class=bold>The commented program</a>
    <ul>
        <li><a href="#Includefiles">Include files</a>
        <li><a href="#ThecodeElasticProblemcodeclasstemplate">The <code>ElasticProblem</code> class template</a>
        <li><a href="#Righthandsidevalues">Right hand side values</a>
        <li><a href="#ThecodeElasticProblemcodeclassimplementation">The <code>ElasticProblem</code> class implementation</a>
      <ul>
        <li><a href="#ElasticProblemElasticProblem">ElasticProblem::ElasticProblem</a>
        <li><a href="#ElasticProblemsetup_system">ElasticProblem::setup_system</a>
        <li><a href="#ElasticProblemassemble_system">ElasticProblem::assemble_system</a>
        <li><a href="#ElasticProblemsolve">ElasticProblem::solve</a>
        <li><a href="#ElasticProblemrefine_grid">ElasticProblem::refine_grid</a>
        <li><a href="#ElasticProblemoutput_results">ElasticProblem::output_results</a>
        <li><a href="#ElasticProblemrun">ElasticProblem::run</a>
      </ul>
        <li><a href="#Thecodemaincodefunction">The <code>main</code> function</a>
      </ul>
</ol></td><td width="50%" valign="top"><ol>
  <li value="3"> <a href="#Results" class=bold>Results</a>
    <ul>
        <li><a href="#Possibilitiesforextensions">Possibilities for extensions</a>
    </ul>
  <li> <a href="#PlainProg" class=bold>The plain program</a>
</ol> </td> </tr> </table>
 examples/step-17/doc/intro.dox</p>
<p><a class="anchor" id="Intro"></a></p>
<p><a class="anchor" id="Introduction"></a></p><h1>Introduction</h1>
<p><a class="anchor" id="Overview"></a></p><h3>Overview</h3>
<p>这个程序没有引入任何新的数学思想；事实上，它所做的只是做与<a class="el" href="step_8.html">step-8</a>已经做的完全相同的计算，但它以一种不同的方式来做：我们没有使用deal.II自己的线性代数类，而是在deal.II提供的类之上建立一切，这些类包裹着<a href="http://www.mcs.anl.gov/petsc/" target="_top">PETSc</a>库的线性代数实现。由于PETSc允许将矩阵和向量分布在MPI网络中的几台计算机上，因此产生的代码甚至能够以并行方式解决问题。如果你不知道PETSc是什么，那么这将是一个快速浏览其主页的好时机。</p>
<p>作为这个程序的先决条件，你需要安装PETSc，如果你想在一个集群上以并行方式运行，你还需要<a href="http://www-users.cs.umn.edu/~karypis/metis/index.html" target="_top">METIS</a>来划分网格。在<a href="../../readme.html" target="body">README</a>文件中描述了deal.II和这两个附加库的安装。</p>
<p>现在，关于细节：如前所述，该程序不计算任何新的东西，所以对有限元类等的使用与以前完全相同。与以前的程序不同的是，我们用几乎所有的类 <code><a class="el" href="classVector.html">Vector</a></code> and <code><a class="el" href="classSparseMatrix.html">SparseMatrix</a></code> 代替了它们的近似值 <code><a class="el" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a></code> 和 <code><a class="el" href="classPETScWrappers_1_1MPI_1_1SparseMatrix.html">PETScWrappers::MPI::SparseMatrix</a></code> ，它们存储数据的方式使MPI网络中的每个处理器只存储矩阵或矢量的一部分。更具体地说，每个处理器将只存储与它 "拥有 "的自由度相对应的矩阵的那些行。对于向量，它们要么只存储与处理器拥有的自由度相对应的元素（这是右手边所需要的），要么也存储一些额外的元素，以确保每个处理器都能访问处理器拥有的单元（所谓 <a class="el" href="DEALGlossary.html#GlossLocallyActiveDof">本地活动的自由度</a>）或邻近单元（所谓 <a class="el" href="DEALGlossary.html#GlossLocallyRelevantDof">本地相关自由度</a>）上的解组件。</p>
<p>来自PETScWrapper命名空间的类所提供的接口与deal.II线性代数类的接口非常相似，但它们不是自己实现这一功能，而是简单地传递给它们相应的PETSc函数。因此，包装器只是用来给PETSc一个更现代的、面向对象的接口，并使PETSc和deal.II对象的使用尽可能地互换。使用PETSc的主要意义在于它可以在并行状态下运行。我们将利用这一点，将域划分为与MPI网络中的进程一样多的块（"子域"）。同时，PETSc还提供了假的MPI存根，所以如果PETSc的配置中没有MPI，你可以在一台机器上运行这个程序。</p>
<p><a class="anchor" id="ParallelizingsoftwarewithMPI"></a></p><h3>Parallelizing software with MPI</h3>
<p>开发软件以通过MPI在parallel中运行，需要改变一下思维方式，因为我们通常必须分割所有的数据结构，使每个处理器只存储整个问题的一部分。因此，你通常不能在每个处理器上访问一个解决方案向量的所有组成部分 &ndash; 每个处理器可能根本没有足够的内存来容纳整个解决方案向量。由于数据被分割或 "分布 "在各个处理器上，我们把MPI使用的编程模型称为 "分布式内存计算"（与 "共享内存计算 "相反，后者意味着多个处理器可以访问一个内存空间中的所有数据，例如，当一台机器的多个核心在一个共同任务上工作时）。分布式内存计算的一些基本原理在 <a class="el" href="group__distributed.html">使用分布式内存的多处理器并行计算 </a>文档模块中讨论，该模块本身是 <a class="el" href="group__Parallel.html">并行计算 </a>模块的一个子模块。</p>
<p>一般来说，为了真正能够扩展到大量的处理器，我们需要在可用的处理器之间分割出<em>every</em>数据结构，其大小随着整个问题的大小而扩展。关于程序 "扩展 "的定义，见 <a class="el" href="DEALGlossary.html#GlossParallelScaling">本词汇表条目</a>）。这包括，例如，三角形、矩阵和所有全局向量（解决方案，右手边）。如果不拆分所有这些对象，其中一个对象将被复制到所有的处理器上，如果问题大小（和可用的处理器数量）变得很大，最终会简单地变得太大。另一方面，在每个处理器上保留大小与整个问题大小无关的对象是完全可以的。例如，可执行文件的每个副本将创建自己的有限元对象，或者我们在汇编中使用的局部矩阵）。)</p>
<p>在当前的程序中（以及相关的第18步），我们不会走得这么远，而是对MPI的使用做一个比较温和的介绍。更具体地说，我们要并行化的数据结构只有矩阵和向量。然而，我们并没有拆分Triangulation和DoFHandler类：每个进程仍然拥有这些对象的完整副本，而且所有进程都拥有其他进程所拥有的确切副本。然后，我们只需在每个处理器上的三角形的每个副本中，标记哪个处理器拥有哪些单元。这个过程被称为将网格 "分割 "为 <a class="el" href="DEALGlossary.html#GlossSubdomainId">子域</a>。</p>
<p>对于较大的问题，必须在每个处理器上存储<em>entire</em>网格，显然会产生一个瓶颈。分割网格是稍微的，虽然没有多复杂（从用户的角度来看，虽然它<em>much</em>下更复杂）来实现，我们将展示如何在<a class="el" href="step_40.html">step-40</a>和其他一些程序中这样做。在讨论这个程序的某个功能如何工作的过程中，我们会多次评论它不会扩展到大型问题，以及为什么不会。所有这些问题都将在第18步，特别是第40步中得到解决，它可以扩展到非常多的进程。</p>
<p>从哲学上讲，MPI的运作方式如下。你通常通过以下方式运行一个程序</p>
<div class="fragment"><div class="line">mpirun -np 32 ./step-17</div></div><!-- fragment --><p>这意味着在（比如）32个处理器上运行它。如果你是在一个集群系统上，你通常需要<em>schedule</em>程序在32个处理器可用时运行；这将在你的集群的文档中描述。但是在系统内部，每当这些处理器可用时，通常会执行上述相同的调用）。)这样做的目的是，MPI系统将启动32个<em>copies</em>的 <code><a class="el" href="step_17.html">step-17</a></code> 的可执行文件。(这些正在运行的可执行文件中的每一个的MPI术语是，你有32个 <a class="el" href="DEALGlossary.html#GlossMPIProcess">MPI进程</a>。)这可能发生在不同的机器上，甚至不能从对方的内存空间中读取，也可能发生在同一台机器上，但最终的结果是一样的：这32个副本中的每一个都将以操作系统分配给它的一些内存运行，而且它不能直接读取其他31个副本的内存。为了在一个共同的任务中进行协作，这32个副本就必须<em>communicate</em>相互协作。MPI是<em>Message Passing Interface</em>的缩写，通过允许程序<em>send messages</em>来实现这一点。你可以把它看作是邮件服务：你可以把一封写给特定地址的信放入邮件，它将被送达。但这是你能控制事物的程度。如果你想让收信人对信的内容做些什么，例如把你想要的数据从那边返回给你，那么需要发生两件事。(i)接收方需要实际去检查他们的邮箱里是否有东西，(ii)如果有的话，做出适当的反应，比如说发送数据回来。如果你等待这个返回信息，但原来的接收者却心不在焉，没有注意，那么你就不走运了：你只需要等待，直到你在那边的请求将被解决。在某些情况下，错误会导致原始接收者永远不检查你的邮件，在这种情况下，你将永远等待&ndash;这被称为<em>deadlock</em>。( See also <a href="http://www.math.colostate.edu/~bangerth/videos.676.39.html">video lecture 39</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.html">video lecture 41</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.25.html">video lecture 41.25</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.5.html">video lecture 41.5</a>.)</p>
<p>在实践中，人们通常不在发送和接收单个消息的层面上编程，而是使用更高层次的操作。例如，在程序中，我们将使用函数调用，从每个处理器获取一个数字，将它们全部相加，然后将总和返回给所有处理器。在内部，这是用单个消息实现的，但对用户来说，这是透明的。我们称这种操作为<em>collectives</em>，因为<em>all</em>处理器参与其中。集合体允许我们编写程序，其中不是每个可执行文件的副本都在做完全不同的事情（这将是难以置信的编程难度），但实质上所有副本都在为自己做同样的事情（尽管是在不同的数据上），通过相同的代码块运行；然后他们通过集合体进行数据通信；然后再回到为自己做事情，通过相同的数据块运行。这是能够编写程序的关键部分，也是确保程序能够在任何数量的处理器上运行的关键部分，因为我们不需要为每个参与的处理器编写不同的代码。</p>
<p>这并不是说程序从来都是以不同的处理器在其可执行文件的副本中运行不同的代码块的方式来编写的。程序内部也经常以其他方式而不是通过集合体进行通信。但是在实践中，并行有限元代码几乎总是遵循这样的方案：程序的每个副本在同一时间运行相同的代码块，中间穿插着所有处理器相互交流的阶段）。)</p>
<p>在现实中，即使是调用MPI集体函数的水平也太低了。相反，下面的程序根本不会包含对MPI的任何直接调用，而只包含对deal.II的用户隐藏这种通信的函数。这样做的好处是，你不需要学习MPI的细节和相当复杂的函数调用。也就是说，你确实必须理解上文所述的MPI背后的一般哲学。</p>
<p><a class="anchor" id="Whatthisprogramdoes"></a></p><h3>What this program does</h3>
<p>然后，这个程序演示的技术是。</p>
<ul>
<li>如何使用PETSc封装类；这在本程序的主类的声明中已经可以看到， <code>ElasticProblem</code> 。</li>
<li>如何将网格划分为子域；这发生在 <code>ElasticProblem::setup_system()</code> 函数。</li>
<li>如何对运行在MPI网络上的作业进行并行化操作；在这里，这是在很多地方都要注意的，最明显的是在 <code>ElasticProblem::assemble_system()</code> 函数中。</li>
<li>如何处理只存储向量项子集的向量，对于这些向量，我们必须确保它们在当前处理器上存储我们需要的东西。例如见 <code>ElasticProblem::solve()</code> and <code>ElasticProblem::refine_grid()</code> 函数。</li>
<li>如何处理同时在多个处理器上运行的程序的状态输出。这是通过程序中的 <code>pcout</code> 变量完成的，在构造函数中初始化。</li>
</ul>
<p>由于这一切只能用实际的代码来证明，让我们直接进入代码，不再多说。</p>
<p><a class="anchor" id="CommProg"></a> </p><h1>The commented program</h1>
<p><a class="anchor" id="Includefiles"></a> </p><h3>Include files</h3>
<p>First the usual assortment of header files we have already used in previous example programs:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="quadrature__lib_8h.html">deal.II/base/quadrature_lib.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="function_8h.html">deal.II/base/function.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="logstream_8h.html">deal.II/base/logstream.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="multithread__info_8h.html">deal.II/base/multithread_info.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8h.html">deal.II/lac/vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="full__matrix_8h.html">deal.II/lac/full_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="affine__constraints_8h.html">deal.II/lac/affine_constraints.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dynamic__sparsity__pattern_8h.html">deal.II/lac/dynamic_sparsity_pattern.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="sparsity__tools_8h.html">deal.II/lac/sparsity_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid_2tria_8h.html">deal.II/grid/tria.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__generator_8h.html">deal.II/grid/grid_generator.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid_2grid__refinement_8h.html">deal.II/grid/grid_refinement.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dofs_2dof__handler_8h.html">deal.II/dofs/dof_handler.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__tools_8h.html">deal.II/dofs/dof_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe_2fe__values_8h.html">deal.II/fe/fe_values.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__system_8h.html">deal.II/fe/fe_system.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__q_8h.html">deal.II/fe/fe_q.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector__tools_8h.html">deal.II/numerics/vector_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="matrix__tools_8h.html">deal.II/numerics/matrix_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="numerics_2data__out_8h.html">deal.II/numerics/data_out.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="error__estimator_8h.html">deal.II/numerics/error_estimator.h</a>&gt;</span></div></div><!-- fragment --><p>And here come the things that we need particularly for this example program and that weren't in <a class="el" href="step_8.html">step-8</a>. First, we replace the standard output <code>std::cout</code> by a new stream <code>pcout</code> which is used in parallel computations for generating output only on one of the MPI processes.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="conditional__ostream_8h.html">deal.II/base/conditional_ostream.h</a>&gt;</span></div></div><!-- fragment --><p>We are going to query the number of processes and the number of the present process by calling the respective functions in the <a class="el" href="namespaceUtilities_1_1MPI.html">Utilities::MPI</a> namespace.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="mpi_8h.html">deal.II/base/mpi.h</a>&gt;</span></div></div><!-- fragment --><p>Then, we are going to replace all linear algebra components that involve the (global) linear system by classes that wrap interfaces similar to our own linear algebra classes around what PETSc offers (PETSc is a library written in C, and deal.II comes with wrapper classes that provide the PETSc functionality with an interface that is similar to the interface we already had for our own linear algebra classes). In particular, we need vectors and matrices that are distributed across several <a class="el" href="DEALGlossary.html#GlossMPIProcess">processes</a> in MPI programs (and simply map to sequential, local vectors and matrices if there is only a single process, i.e., if you are running on only one machine, and without MPI support):</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__vector_8h.html">deal.II/lac/petsc_vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__sparse__matrix_8h.html">deal.II/lac/petsc_sparse_matrix.h</a>&gt;</span></div></div><!-- fragment --><p>Then we also need interfaces for solvers and preconditioners that PETSc provides:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__solver_8h.html">deal.II/lac/petsc_solver.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__precondition_8h.html">deal.II/lac/petsc_precondition.h</a>&gt;</span></div></div><!-- fragment --><p>And in addition, we need some algorithms for partitioning our meshes so that they can be efficiently distributed across an MPI network. The partitioning algorithm is implemented in the <code><a class="el" href="namespaceGridTools.html">GridTools</a></code> namespace, and we need an additional include file for a function in <code><a class="el" href="namespaceDoFRenumbering.html">DoFRenumbering</a></code> that allows to sort the indices associated with degrees of freedom so that they are numbered according to the subdomain they are associated with:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__tools_8h.html">deal.II/grid/grid_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__renumbering_8h.html">deal.II/dofs/dof_renumbering.h</a>&gt;</span></div></div><!-- fragment --><p>And this is simply C++ again:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div></div><!-- fragment --><p>The last step is as in all previous programs:</p>
<div class="fragment"><div class="line"><span class="keyword">namespace </span>Step17</div><div class="line">{</div><div class="line">  <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div></div><!-- fragment --><p><a class="anchor" id="ThecodeElasticProblemcodeclasstemplate"></a> </p><h3>The <code>ElasticProblem</code> class template</h3>
<p>The first real part of the program is the declaration of the main class. As mentioned in the introduction, almost all of this has been copied verbatim from <a class="el" href="step_8.html">step-8</a>, so we only comment on the few differences between the two tutorials. There is one (cosmetic) change in that we let <code>solve</code> return a value, namely the number of iterations it took to converge, so that we can output this to the screen at the appropriate place.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keyword">class </span>ElasticProblem</div><div class="line">{</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  ElasticProblem();</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">run</a>();</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line">  <span class="keywordtype">void</span>         setup_system();</div><div class="line">  <span class="keywordtype">void</span>         assemble_system();</div><div class="line">  <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> solve();</div><div class="line">  <span class="keywordtype">void</span>         refine_grid();</div><div class="line">  <span class="keywordtype">void</span>         output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div></div><!-- fragment --><p>The first change is that we have to declare a variable that indicates the <a class="el" href="DEALGlossary.html#GlossMPICommunicator">MPI communicator</a> over which we are supposed to distribute our computations.</p>
<div class="fragment"><div class="line"><a class="code" href="classMPI__Comm.html">MPI_Comm</a> mpi_communicator;</div></div><!-- fragment --><p>Then we have two variables that tell us where in the parallel world we are. The first of the following variables, <code>n_mpi_processes</code>, tells us how many MPI processes there exist in total, while the second one, <code>this_mpi_process</code>, indicates which is the number of the present process within this space of processes (in MPI language, this corresponds to the <a class="el" href="DEALGlossary.html#GlossMPIRank">rank</a> of the process). The latter will have a unique value for each process between zero and (less than) <code>n_mpi_processes</code>. If this program is run on a single machine without MPI support, then their values are <code>1</code> and <code>0</code>, respectively.</p>
<div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>;</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>;</div></div><!-- fragment --><p>Next up is a stream-like variable <code>pcout</code>. It is, in essence, just something we use for convenience: in a parallel program, if each process outputs status information, then there quickly is a lot of clutter. Rather, we would want to only have one <a class="el" href="DEALGlossary.html#GlossMPIProcess">process</a> output everything once, for example the one with <a class="el" href="DEALGlossary.html#GlossMPIRank">rank</a> zero. At the same time, it seems silly to prefix <em>every</em> place where we create output with an <code>if (my_rank==0)</code> condition.</p>
<p>To make this simpler, the <a class="el" href="classConditionalOStream.html">ConditionalOStream</a> class does exactly this under the hood: it acts as if it were a stream, but only forwards to a real, underlying stream if a flag is set. By setting this condition to <code>this_mpi_process==0</code> (where <code>this_mpi_process</code> corresponds to the rank of an MPI process), we make sure that output is only generated from the first process and that we don't get the same lines of output over and over again, once per process. Thus, we can use <code>pcout</code> everywhere and in every process, but on all but one process nothing will ever happen to the information that is piped into the object via <code>operator&lt;&lt;</code>.</p>
<div class="fragment"><div class="line"><a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div></div><!-- fragment --><p>The remainder of the list of member variables is fundamentally the same as in <a class="el" href="step_8.html">step-8</a>. However, we change the declarations of matrix and vector types to use parallel PETSc objects instead. Note that we do not use a separate sparsity pattern, since PETSc manages this internally as part of its matrix data structures.</p>
<div class="fragment"><div class="line">  <a class="code" href="classTriangulation.html">Triangulation&lt;dim&gt;</a> <a class="code" href="p4est__wrappers_8cc.html#ace00f2f80d9780ef9aa1007e1c22c6a4">triangulation</a>;</div><div class="line">  <a class="code" href="classFESystem.html">FESystem&lt;dim&gt;</a>      fe;</div><div class="line">  <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a>    dof_handler;</div><div class="line"></div><div class="line">  <a class="code" href="classAffineConstraints.html">AffineConstraints&lt;double&gt;</a> hanging_node_constraints;</div><div class="line"></div><div class="line">  <a class="code" href="classPETScWrappers_1_1MPI_1_1SparseMatrix.html">PETScWrappers::MPI::SparseMatrix</a> system_matrix;</div><div class="line"></div><div class="line">  <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> solution;</div><div class="line">  <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> system_rhs;</div><div class="line">};</div></div><!-- fragment --><p><a class="anchor" id="Righthandsidevalues"></a> </p><h3>Right hand side values</h3>
<p>The following is taken from <a class="el" href="step_8.html">step-8</a> without change:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keyword">class </span>RightHandSide : <span class="keyword">public</span> <a class="code" href="classFunction.html">Function</a>&lt;dim&gt;</div><div class="line">{</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  <span class="keyword">virtual</span> <span class="keywordtype">void</span> <a class="code" href="classFunction.html#ae316ebc05d21989d573024f8a23c49cb">vector_value</a>(<span class="keyword">const</span> <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &amp;p,</div><div class="line">                            <a class="code" href="classVector.html">Vector&lt;double&gt;</a> &amp;  values)<span class="keyword"> const override</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(values.<a class="code" href="classVector.html#a81dcfa5c77bdd426603386c0844149ae">size</a>() == dim, <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a>(values.<a class="code" href="classVector.html#a81dcfa5c77bdd426603386c0844149ae">size</a>(), dim));</div><div class="line">    <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(dim &gt;= 2, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line"></div><div class="line">    <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> point_1, point_2;</div><div class="line">    point_1(0) = 0.5;</div><div class="line">    point_2(0) = -0.5;</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (((p - point_1).norm_square() &lt; 0.2 * 0.2) ||</div><div class="line">        ((p - point_2).norm_square() &lt; 0.2 * 0.2))</div><div class="line">      <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(0) = 1;</div><div class="line">    <span class="keywordflow">else</span></div><div class="line">      <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(0) = 0;</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (p.<a class="code" href="classPoint.html#a859ea7f3bf3e64be2e0f5ed1bfcc8550">square</a>() &lt; 0.2 * 0.2)</div><div class="line">      <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(1) = 1;</div><div class="line">    <span class="keywordflow">else</span></div><div class="line">      <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(1) = 0;</div><div class="line">  }</div><div class="line"></div><div class="line">  <span class="keyword">virtual</span> <span class="keywordtype">void</span></div><div class="line">  <a class="code" href="classFunction.html#aa041dde994d40c068e00661197ac75a6">vector_value_list</a>(<span class="keyword">const</span> std::vector&lt;<a class="code" href="classPoint.html">Point&lt;dim&gt;</a>&gt; &amp;points,</div><div class="line">                    std::vector&lt;<a class="code" href="classVector.html">Vector&lt;double&gt;</a>&gt; &amp;  value_list)<span class="keyword"> const override</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_points = points.size();</div><div class="line"></div><div class="line">    <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(value_list.size() == n_points,</div><div class="line">           <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a>(value_list.size(), n_points));</div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p = 0; p &lt; n_points; ++p)</div><div class="line">      RightHandSide&lt;dim&gt;::vector_value(points[p], value_list[p]);</div><div class="line">  }</div><div class="line">};</div></div><!-- fragment --><p><a class="anchor" id="ThecodeElasticProblemcodeclassimplementation"></a> </p><h3>The <code>ElasticProblem</code> class implementation</h3>
<p><a class="anchor" id="ElasticProblemElasticProblem"></a> </p><h4>ElasticProblem::ElasticProblem</h4>
<p>The first step in the actual implementation is the constructor of the main class. Apart from initializing the same member variables that we already had in <a class="el" href="step_8.html">step-8</a>, we here initialize the MPI communicator variable we shall use with the global MPI communicator linking all processes together (in more complex applications, one could here use a communicator object that only links a subset of all processes), and call the <a class="el" href="namespaceUtilities_1_1MPI.html">Utilities::MPI</a> helper functions to determine the number of processes and where the present one fits into this picture. In addition, we make sure that output is only generated by the (globally) first process. We do so by passing the stream we want to output to (<code>std::cout</code>) and a true/false flag as arguments where the latter is determined by testing whether the process currently executing the constructor call is the first in the MPI universe.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">ElasticProblem&lt;dim&gt;::ElasticProblem()</div><div class="line">  : mpi_communicator(MPI_COMM_WORLD)</div><div class="line">  , <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(mpi_communicator))</div><div class="line">  , <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator))</div><div class="line">  , pcout(<a class="code" href="namespacestd.html">std</a>::cout, (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0))</div><div class="line">  , fe(<a class="code" href="classFE__Q.html">FE_Q</a>&lt;dim&gt;(1), dim)</div><div class="line">  , dof_handler(triangulation)</div><div class="line">{}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemsetup_system"></a> </p><h4>ElasticProblem::setup_system</h4>
<p>Next, the function in which we set up the various variables for the global linear system to be solved needs to be implemented.</p>
<p>However, before we proceed with this, there is one thing to do for a parallel program: we need to determine which MPI process is responsible for each of the cells. Splitting cells among processes, commonly called "partitioning the mesh", is done by assigning a <a class="el" href="DEALGlossary.html#GlossSubdomainId">subdomain id</a> to each cell. We do so by calling into the METIS library that does this in a very efficient way, trying to minimize the number of nodes on the interfaces between subdomains. Rather than trying to call METIS directly, we do this by calling the <a class="el" href="namespaceGridTools.html#a99eba8e3b388258eda37a2724579dd1d">GridTools::partition_triangulation()</a> function that does this at a much higher level of programming.</p>
<dl class="section note"><dt>Note</dt><dd>As mentioned in the introduction, we could avoid this manual partitioning step if we used the <a class="el" href="classparallel_1_1shared_1_1Triangulation.html">parallel::shared::Triangulation</a> class for the triangulation object instead (as we do in <a class="el" href="step_18.html">step-18</a>). That class does, in essence, everything a regular triangulation does, but it then also automatically partitions the mesh after every mesh creation or refinement operation.</dd></dl>
<p>Following partitioning, we need to enumerate all degrees of freedom as usual. However, we would like to enumerate the degrees of freedom in a way so that all degrees of freedom associated with cells in subdomain zero (which resides on process zero) come before all DoFs associated with cells on subdomain one, before those on cells on process two, and so on. We need this since we have to split the global vectors for right hand side and solution, as well as the matrix into contiguous chunks of rows that live on each of the processors, and we will want to do this in a way that requires minimal communication. This particular enumeration can be obtained by re-ordering degrees of freedom indices using <a class="el" href="namespaceDoFRenumbering.html#a166d0082503ec2b8d9c560222b48f9d2">DoFRenumbering::subdomain_wise()</a>.</p>
<p>The final step of this initial setup is that we get ourselves an <a class="el" href="classIndexSet.html">IndexSet</a> that indicates the subset of the global number of unknowns this process is responsible for. (Note that a degree of freedom is not necessarily owned by the process that owns a cell just because the degree of freedom lives on this cell: some degrees of freedom live on interfaces between subdomains, and are consequently only owned by one of the processes adjacent to this interface.)</p>
<p>Before we move on, let us recall a fact already discussed in the introduction: The triangulation we use here is replicated across all processes, and each process has a complete copy of the entire triangulation, with all cells. Partitioning only provides a way to identify which cells out of all each process "owns", but it knows everything about all of them. Likewise, the <a class="el" href="classDoFHandler.html">DoFHandler</a> object knows everything about every cell, in particular the degrees of freedom that live on each cell, whether it is one that the current process owns or not. This can not scale to large problems because eventually just storing the entire mesh, and everything that is associated with it, on every process will become infeasible if the problem is large enough. On the other hand, if we split the triangulation into parts so that every process stores only those cells it "owns" but nothing else (or, at least a sufficiently small fraction of everything else), then we can solve large problems if only we throw a large enough number of MPI processes at them. This is what we are going to in <a class="el" href="step_40.html">step-40</a>, for example, using the <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> class. On the other hand, most of the rest of what we demonstrate in the current program will actually continue to work whether we have the entire triangulation available, or only a piece of it.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::setup_system()</div><div class="line">{</div><div class="line">  <a class="code" href="namespaceGridTools.html#a99eba8e3b388258eda37a2724579dd1d">GridTools::partition_triangulation</a>(<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>, triangulation);</div><div class="line"></div><div class="line">  dof_handler.<a class="code" href="classDoFHandler.html#a553ca864aaf70330d9be86bc78f36d1e">distribute_dofs</a>(fe);</div><div class="line">  <a class="code" href="namespaceDoFRenumbering.html#a166d0082503ec2b8d9c560222b48f9d2">DoFRenumbering::subdomain_wise</a>(dof_handler);</div></div><!-- fragment --><p>We need to initialize the objects denoting hanging node constraints for the present grid. As with the triangulation and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects, we will simply store <em>all</em> constraints on each process; again, this will not scale, but we show in <a class="el" href="step_40.html">step-40</a> how one can work around this by only storing on each MPI process the constraints for degrees of freedom that actually matter on this particular process.</p>
<div class="fragment"><div class="line">hanging_node_constraints.clear();</div><div class="line"><a class="code" href="group__constraints.html#ga3b4ea7dfd313e388d868c4e4aa685799">DoFTools::make_hanging_node_constraints</a>(dof_handler,</div><div class="line">                                        hanging_node_constraints);</div><div class="line">hanging_node_constraints.close();</div></div><!-- fragment --><p>Now we create the sparsity pattern for the system matrix. Note that we again compute and store all entries and not only the ones relevant to this process (see <a class="el" href="step_18.html">step-18</a> or <a class="el" href="step_40.html">step-40</a> for a more efficient way to handle this).</p>
<div class="fragment"><div class="line"><a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>(), dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>());</div><div class="line"><a class="code" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a>(dof_handler,</div><div class="line">                                dsp,</div><div class="line">                                hanging_node_constraints,</div><div class="line">                                <span class="keyword">false</span>);</div></div><!-- fragment --><p>Now we determine the set of locally owned DoFs and use that to initialize parallel vectors and matrix. Since the matrix and vectors need to work in parallel, we have to pass them an MPI communication object, as well as information about the partitioning contained in the <a class="el" href="classIndexSet.html">IndexSet</a> <code>locally_owned_dofs</code>. The <a class="el" href="classIndexSet.html">IndexSet</a> contains information about the global size (the <em>total</em> number of degrees of freedom) and also what subset of rows is to be stored locally. Note that the system matrix needs that partitioning information for the rows and columns. For square matrices, as it is the case here, the columns should be partitioned in the same way as the rows, but in the case of rectangular matrices one has to partition the columns in the same way as vectors are partitioned with which the matrix is multiplied, while rows have to partitioned in the same way as destination vectors of matrix-vector multiplications:</p>
<div class="fragment"><div class="line">  <span class="keyword">const</span> std::vector&lt;IndexSet&gt; locally_owned_dofs_per_proc =</div><div class="line">    <a class="code" href="namespaceDoFTools.html#a180d7118ecf27d72afbfecb7978c5e09">DoFTools::locally_owned_dofs_per_subdomain</a>(dof_handler);</div><div class="line">  <span class="keyword">const</span> <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs =</div><div class="line">    locally_owned_dofs_per_proc[<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>];</div><div class="line"></div><div class="line">  system_matrix.reinit(locally_owned_dofs,</div><div class="line">                       locally_owned_dofs,</div><div class="line">                       dsp,</div><div class="line">                       mpi_communicator);</div><div class="line"></div><div class="line">  solution.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line">  system_rhs.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemassemble_system"></a> </p><h4>ElasticProblem::assemble_system</h4>
<p>We now assemble the matrix and right hand side of the problem. There are some things worth mentioning before we go into detail. First, we will be assembling the system in parallel, i.e., each process will be responsible for assembling on cells that belong to this particular process. Note that the degrees of freedom are split in a way such that all DoFs in the interior of cells and between cells belonging to the same subdomain belong to the process that <code>owns</code> the cell. However, even then we sometimes need to assemble on a cell with a neighbor that belongs to a different process, and in these cases when we add up the local contributions into the global matrix or right hand side vector, we have to transfer these entries to the process that owns these elements. Fortunately, we don't have to do this by hand: PETSc does all this for us by caching these elements locally, and sending them to the other processes as necessary when we call the <code><a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a></code> functions on the matrix and vector at the end of this function.</p>
<p>The second point is that once we have handed over matrix and vector contributions to PETSc, it is a) hard, and b) very inefficient to get them back for modifications. This is not only the fault of PETSc, it is also a consequence of the distributed nature of this program: if an entry resides on another processor, then it is necessarily expensive to get it. The consequence of this is that we should not try to first assemble the matrix and right hand side as if there were no hanging node constraints and boundary values, and then eliminate these in a second step (using, for example, <a class="el" href="classAffineConstraints.html#a5a1bc1bb2d705b582889ebaa24bcae5c">AffineConstraints::condense()</a>). Rather, we should try to eliminate hanging node constraints before handing these entries over to PETSc. This is easy: instead of copying elements by hand into the global matrix (as we do in <a class="el" href="step_4.html">step-4</a>), we use the <a class="el" href="classAffineConstraints.html#a373fbdacd8c486e675b8d2bff8943192">AffineConstraints::distribute_local_to_global()</a> functions to take care of hanging nodes at the same time. We also already did this in <a class="el" href="step_6.html">step-6</a>. The second step, elimination of boundary nodes, could also be done this way by putting the boundary values into the same <a class="el" href="classAffineConstraints.html">AffineConstraints</a> object as hanging nodes (see the way it is done in <a class="el" href="step_6.html">step-6</a>, for example); however, it is not strictly necessary to do this here because eliminating boundary values can be done with only the data stored on each process itself, and consequently we use the approach used before in <a class="el" href="step_4.html">step-4</a>, i.e., via <a class="el" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values()</a>.</p>
<p>All of this said, here is the actual implementation starting with the general setup of helper variables. (Note that we still use the deal.II full matrix and vector types for the local systems as these are small and need not be shared across processes.)</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::assemble_system()</div><div class="line">{</div><div class="line">  <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a>   quadrature_formula(fe.<a class="code" href="classFiniteElementData.html#a2cbf5ad6b464871261dbd054bced18a8">degree</a> + 1);</div><div class="line">  <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values(fe,</div><div class="line">                          quadrature_formula,</div><div class="line">                          <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                            <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> dofs_per_cell = fe.<a class="code" href="classFiniteElementData.html#a33b522422da89e5c080e7405ad49d7c7">n_dofs_per_cell</a>();</div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_q_points    = quadrature_formula.size();</div><div class="line"></div><div class="line">  <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a> <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(dofs_per_cell, dofs_per_cell);</div><div class="line">  <a class="code" href="classVector.html">Vector&lt;double&gt;</a>     cell_rhs(dofs_per_cell);</div><div class="line"></div><div class="line">  std::vector&lt;types::global_dof_index&gt; local_dof_indices(dofs_per_cell);</div><div class="line"></div><div class="line">  std::vector&lt;double&gt; lambda_values(n_q_points);</div><div class="line">  std::vector&lt;double&gt; mu_values(n_q_points);</div><div class="line"></div><div class="line">  <a class="code" href="classFunctions_1_1ConstantFunction.html">Functions::ConstantFunction&lt;dim&gt;</a> <a class="code" href="namespaceDifferentiation_1_1SD.html#a841a7b84dc17bf2ba675522093a97e8ba945f3fc449518a73b9f5f32868db466c">lambda</a>(1.), <a class="code" href="namespacemu.html">mu</a>(1.);</div><div class="line"></div><div class="line">  RightHandSide&lt;dim&gt;          right_hand_side;</div><div class="line">  std::vector&lt;Vector&lt;double&gt;&gt; rhs_values(n_q_points, <a class="code" href="classVector.html">Vector&lt;double&gt;</a>(dim));</div></div><!-- fragment --><p>The next thing is the loop over all elements. Note that we do not have to do <em>all</em> the work on every process: our job here is only to assemble the system on cells that actually belong to this MPI process, all other cells will be taken care of by other processes. This is what the if-clause immediately after the for-loop takes care of: it queries the subdomain identifier of each cell, which is a number associated with each cell that tells us about the owner process. In more generality, the subdomain id is used to split a domain into several parts (we do this above, at the beginning of <code>setup_system()</code>), and which allows to identify which subdomain a cell is living on. In this application, we have each process handle exactly one subdomain, so we identify the terms <code>subdomain</code> and <code>MPI process</code>.</p>
<p>Apart from this, assembling the local system is relatively uneventful if you have understood how this is done in <a class="el" href="step_8.html">step-8</a>. As mentioned above, distributing local contributions into the global matrix and right hand sides also takes care of hanging node constraints in the same way as is done in <a class="el" href="step_6.html">step-6</a>.</p>
<div class="fragment"><div class="line"><span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;cell : dof_handler.<a class="code" href="group__CPP11.html#gacdb6d3adec96a13a7079f6c893e1d3ff">active_cell_iterators</a>())</div><div class="line">  <span class="keywordflow">if</span> (cell-&gt;subdomain_id() == <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>)</div><div class="line">    {</div><div class="line">      cell_matrix = 0;</div><div class="line">      cell_rhs    = 0;</div><div class="line"></div><div class="line">      fe_values.<a class="code" href="classFEValues.html#a21f914e63d588e2652a9514620653d77">reinit</a>(cell);</div><div class="line"></div><div class="line">      <a class="code" href="namespaceDifferentiation_1_1SD.html#a841a7b84dc17bf2ba675522093a97e8ba945f3fc449518a73b9f5f32868db466c">lambda</a>.value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(), lambda_values);</div><div class="line">      <a class="code" href="namespacemu.html">mu</a>.value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(), mu_values);</div><div class="line"></div><div class="line">      <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">        {</div><div class="line">          <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_i =</div><div class="line">            fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j = 0; j &lt; dofs_per_cell; ++j)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_j =</div><div class="line">                fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(j).first;</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points;</div><div class="line">                   ++q_point)</div><div class="line">                {</div><div class="line">                  <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(i, j) +=</div><div class="line">                    ((fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)[component_i] *</div><div class="line">                      fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)[component_j] *</div><div class="line">                      lambda_values[q_point]) +</div><div class="line">                     (fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)[component_j] *</div><div class="line">                      fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)[component_i] *</div><div class="line">                      mu_values[q_point]) +</div><div class="line">                     ((component_i == component_j) ?</div><div class="line">                        (fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point) *</div><div class="line">                         fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point) *</div><div class="line">                         mu_values[q_point]) :</div><div class="line">                        0)) *</div><div class="line">                    fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">                }</div><div class="line">            }</div><div class="line">        }</div><div class="line"></div><div class="line">      right_hand_side.vector_value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(),</div><div class="line">                                        rhs_values);</div><div class="line">      <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">        {</div><div class="line">          <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_i =</div><div class="line">            fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points; ++q_point)</div><div class="line">            cell_rhs(i) += fe_values.<a class="code" href="classFEValuesBase.html#a1dd48cb744013c448d57f8f77640c08d">shape_value</a>(i, q_point) *</div><div class="line">                           rhs_values[q_point](component_i) *</div><div class="line">                           fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">        }</div><div class="line"></div><div class="line">      cell-&gt;get_dof_indices(local_dof_indices);</div><div class="line">      hanging_node_constraints.distribute_local_to_global(cell_matrix,</div><div class="line">                                                          cell_rhs,</div><div class="line">                                                          local_dof_indices,</div><div class="line">                                                          system_matrix,</div><div class="line">                                                          system_rhs);</div><div class="line">    }</div></div><!-- fragment --><p>The next step is to "compress" the vector and the system matrix. This means that each process sends the additions that were made to those entries of the matrix and vector that the process did not own itself to the process that owns them. After receiving these additions from other processes, each process then adds them to the values it already has. These additions are combining the integral contributions of shape functions living on several cells just as in a serial computation, with the difference that the cells are assigned to different processes.</p>
<div class="fragment"><div class="line">system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div></div><!-- fragment --><p>The global matrix and right hand side vectors have now been formed. We still have to apply boundary values, in the same way as we did, for example, in <a class="el" href="step_3.html">step-3</a>, <a class="el" href="step_4.html">step-4</a>, and a number of other programs.</p>
<p>The last argument to the call to <a class="el" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values()</a> below allows for some optimizations. It controls whether we should also delete entries (i.e., set them to zero) in the matrix columns corresponding to boundary nodes, or to keep them (and passing <code>true</code> means: yes, do eliminate the columns). If we do eliminate columns, then the resulting matrix will be symmetric again if it was before; if we don't, then it won't. The solution of the resulting system should be the same, though. The only reason why we may want to make the system symmetric again is that we would like to use the CG method, which only works with symmetric matrices. The reason why we may <em>not</em> want to make the matrix symmetric is because this would require us to write into column entries that actually reside on other processes, i.e., it involves communicating data. This is always expensive.</p>
<p>Experience tells us that CG also works (and works almost as well) if we don't remove the columns associated with boundary nodes, which can be explained by the special structure of this particular non-symmetry. To avoid the expense of communication, we therefore do not eliminate the entries in the affected columns.</p>
<div class="fragment"><div class="line">  std::map&lt;types::global_dof_index, double&gt; boundary_values;</div><div class="line">  <a class="code" href="namespaceVectorTools.html#af27ac28c698a9ed0199faed50a204538">VectorTools::interpolate_boundary_values</a>(dof_handler,</div><div class="line">                                           0,</div><div class="line">                                           <a class="code" href="classFunctions_1_1ZeroFunction.html">Functions::ZeroFunction&lt;dim&gt;</a>(dim),</div><div class="line">                                           boundary_values);</div><div class="line">  <a class="code" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values</a>(</div><div class="line">    boundary_values, system_matrix, solution, system_rhs, <span class="keyword">false</span>);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemsolve"></a> </p><h4>ElasticProblem::solve</h4>
<p>Having assembled the linear system, we next need to solve it. PETSc offers a variety of sequential and parallel solvers, for which we have written wrappers that have almost the same interface as is used for the deal.II solvers used in all previous example programs. The following code should therefore look rather familiar.</p>
<p>At the top of the function, we set up a convergence monitor, and assign it the accuracy to which we would like to solve the linear system. Next, we create an actual solver object using PETSc's CG solver which also works with parallel (distributed) vectors and matrices. And finally a preconditioner; we choose to use a block Jacobi preconditioner which works by computing an incomplete LU decomposition on each diagonal block of the matrix. (In other words, each MPI process computes an ILU from the rows it stores by throwing away columns that correspond to row indices not stored locally; this yields a square matrix block from which we can compute an ILU. That means that if you run the program with only one process, then you will use an ILU(0) as a preconditioner, while if it is run on many processes, then we will have a number of blocks on the diagonal and the preconditioner is the ILU(0) of each of these blocks. In the extreme case of one degree of freedom per processor, this preconditioner is then simply a Jacobi preconditioner since the diagonal matrix blocks consist of only a single entry. Such a preconditioner is relatively easy to compute because it does not require any kind of communication between processors, but it is in general not very efficient for large numbers of processors.)</p>
<p>Following this kind of setup, we then solve the linear system:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> ElasticProblem&lt;dim&gt;::solve()</div><div class="line">{</div><div class="line">  <a class="code" href="classSolverControl.html">SolverControl</a> solver_control(solution.size(), 1<a class="code" href="namespacePhysics_1_1Elasticity_1_1Kinematics.html#a9587d5229555daa5b1fa1ba2f8a40adb">e</a>-8 * system_rhs.l2_norm());</div><div class="line">  <a class="code" href="classPETScWrappers_1_1SolverCG.html">PETScWrappers::SolverCG</a> cg(solver_control, mpi_communicator);</div><div class="line"></div><div class="line">  <a class="code" href="classPETScWrappers_1_1PreconditionBlockJacobi.html">PETScWrappers::PreconditionBlockJacobi</a> preconditioner(system_matrix);</div><div class="line"></div><div class="line">  cg.solve(system_matrix, solution, system_rhs, preconditioner);</div></div><!-- fragment --><p>The next step is to distribute hanging node constraints. This is a little tricky, since to fill in the value of a constrained node you need access to the values of the nodes to which it is constrained (for example, for a Q1 element in 2d, we need access to the two nodes on the big side of a hanging node face, to compute the value of the constrained node in the middle).</p>
<p>The problem is that we have built our vectors (in <code>setup_system()</code>) in such a way that every process is responsible for storing only those elements of the solution vector that correspond to the degrees of freedom this process "owns". There are, however, cases where in order to compute the value of the vector entry for a constrained degree of freedom on one process, we need to access vector entries that are stored on other processes. PETSc (and, for that matter, the MPI model on which it is built) does not allow to simply query vector entries stored on other processes, so what we do here is to get a copy of the "distributed" vector where we store all elements locally. This is simple, since the deal.II wrappers have a conversion constructor for the deal.II <a class="el" href="classVector.html">Vector</a> class. (This conversion of course requires communication, but in essence every process only needs to send its data to every other process once in bulk, rather than having to respond to queries for individual elements):</p>
<div class="fragment"><div class="line"><a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution(solution);</div></div><!-- fragment --><p>Of course, as in previous discussions, it is clear that such a step cannot scale very far if you wanted to solve large problems on large numbers of processes, because every process now stores <em>all elements</em> of the solution vector. (We will show how to do this better in <a class="el" href="step_40.html">step-40</a>.) On the other hand, distributing hanging node constraints is simple on this local copy, using the usual function AffineConstraints::distributed(). In particular, we can compute the values of <em>all</em> constrained degrees of freedom, whether the current process owns them or not:</p>
<div class="fragment"><div class="line">hanging_node_constraints.distribute(localized_solution);</div></div><!-- fragment --><p>Then transfer everything back into the global vector. The following operation copies those elements of the localized solution that we store locally in the distributed solution, and does not touch the other ones. Since we do the same operation on all processors, we end up with a distributed vector (i.e., a vector that on every process only stores the vector entries corresponding to degrees of freedom that are owned by this process) that has all the constrained nodes fixed.</p>
<p>We end the function by returning the number of iterations it took to converge, to allow for some output.</p>
<div class="fragment"><div class="line">  solution = localized_solution;</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> solver_control.last_step();</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemrefine_grid"></a> </p><h4>ElasticProblem::refine_grid</h4>
<p>Using some kind of refinement indicator, the mesh can be refined. The problem is basically the same as with distributing hanging node constraints: in order to compute the error indicator (even if we were just interested in the indicator on the cells the current process owns), we need access to more elements of the solution vector than just those the current processor stores. To make this happen, we do essentially what we did in <code>solve()</code> already, namely get a <em>complete</em> copy of the solution vector onto every process, and use that to compute. This is in itself expensive as explained above and it is particular unnecessary since we had just created and then destroyed such a vector in <code>solve()</code>, but efficiency is not the point of this program and so let us opt for a design in which every function is as self-contained as possible.</p>
<p>Once we have such a "localized" vector that contains <em>all</em> elements of the solution vector, we can compute the indicators for the cells that belong to the present process. In fact, we could of course compute <em>all</em> refinement indicators since our <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects store information about all cells, and since we have a complete copy of the solution vector. But in the interest in showing how to operate in parallel, let us demonstrate how one would operate if one were to only compute <em>some</em> error indicators and then exchange the remaining ones with the other processes. (Ultimately, each process needs a complete set of refinement indicators because every process needs to refine their mesh, and needs to refine it in exactly the same way as all of the other processes.)</p>
<p>So, to do all of this, we need to:</p><ul>
<li>First, get a local copy of the distributed solution vector.</li>
<li>Second, create a vector to store the refinement indicators.</li>
<li>Third, let the <a class="el" href="classKellyErrorEstimator.html">KellyErrorEstimator</a> compute refinement indicators for all cells belonging to the present subdomain/process. The last argument of the call indicates which subdomain we are interested in. The three arguments before it are various other default arguments that one usually does not need (and does not state values for, but rather uses the defaults), but which we have to state here explicitly since we want to modify the value of a following argument (i.e., the one indicating the subdomain).</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::refine_grid()</div><div class="line">{</div><div class="line">  <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution(solution);</div><div class="line"></div><div class="line">  <a class="code" href="classVector.html">Vector&lt;float&gt;</a> local_error_per_cell(triangulation.n_active_cells());</div><div class="line">  <a class="code" href="classKellyErrorEstimator.html#ae2269e1c9903e9d863b7abd54948af00">KellyErrorEstimator&lt;dim&gt;::estimate</a>(dof_handler,</div><div class="line">                                     <a class="code" href="classQGauss.html">QGauss&lt;dim - 1&gt;</a>(fe.<a class="code" href="classFiniteElementData.html#a2cbf5ad6b464871261dbd054bced18a8">degree</a> + 1),</div><div class="line">                                     {},</div><div class="line">                                     localized_solution,</div><div class="line">                                     local_error_per_cell,</div><div class="line">                                     <a class="code" href="classComponentMask.html">ComponentMask</a>(),</div><div class="line">                                     <span class="keyword">nullptr</span>,</div><div class="line">                                     <a class="code" href="classMultithreadInfo.html#ad0b84ae105b385b88bdd4bfc0c530995">MultithreadInfo::n_threads</a>(),</div><div class="line">                                     <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>);</div></div><!-- fragment --><p>Now all processes have computed error indicators for their own cells and stored them in the respective elements of the <code>local_error_per_cell</code> vector. The elements of this vector for cells not owned by the present process are zero. However, since all processes have a copy of the entire triangulation and need to keep these copies in sync, they need the values of refinement indicators for all cells of the triangulation. Thus, we need to distribute our results. We do this by creating a distributed vector where each process has its share and sets the elements it has computed. Consequently, when you view this vector as one that lives across all processes, then every element of this vector has been set once. We can then assign this parallel vector to a local, non-parallel vector on each process, making <em>all</em> error indicators available on every process.</p>
<p>So in the first step, we need to set up a parallel vector. For simplicity, every process will own a chunk with as many elements as this process owns cells, so that the first chunk of elements is stored with process zero, the next chunk with process one, and so on. It is important to remark, however, that these elements are not necessarily the ones we will write to. This is a consequence of the order in which cells are arranged, i.e., the order in which the elements of the vector correspond to cells is not ordered according to the subdomain these cells belong to. In other words, if on this process we compute indicators for cells of a certain subdomain, we may write the results to more or less random elements of the distributed vector; in particular, they may not necessarily lie within the chunk of vector we own on the present process. They will subsequently have to be copied into another process' memory space, an operation that PETSc does for us when we call the <code><a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a></code> function. This inefficiency could be avoided with some more code, but we refrain from it since it is not a major factor in the program's total runtime.</p>
<p>So here is how we do it: count how many cells belong to this process, set up a distributed vector with that many elements to be stored locally, copy over the elements we computed locally, and finally compress the result. In fact, we really only copy the elements that are nonzero, so we may miss a few that we computed to zero, but this won't hurt since the original values of the vector are zero anyway.</p>
<div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_local_cells =</div><div class="line">  <a class="code" href="namespaceGridTools.html#a8c212a30784bec20b1ae13fad3fd579c">GridTools::count_cells_with_subdomain_association</a>(triangulation,</div><div class="line">                                                    <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>);</div><div class="line"><a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> distributed_all_errors(</div><div class="line">  mpi_communicator, triangulation.n_active_cells(), n_local_cells);</div><div class="line"></div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; local_error_per_cell.size(); ++i)</div><div class="line">  <span class="keywordflow">if</span> (local_error_per_cell(i) != 0)</div><div class="line">    distributed_all_errors(i) = local_error_per_cell(i);</div><div class="line">distributed_all_errors.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae5042eefddc828c7c31e1e8e26da8b09">VectorOperation::insert</a>);</div></div><!-- fragment --><p>So now we have this distributed vector that contains the refinement indicators for all cells. To use it, we need to obtain a local copy and then use it to mark cells for refinement or coarsening, and actually do the refinement and coarsening. It is important to recognize that <em>every</em> process does this to its own copy of the triangulation, and does it in exactly the same way.</p>
<div class="fragment"><div class="line">  <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;float&gt;</a> localized_all_errors(distributed_all_errors);</div><div class="line"></div><div class="line">  <a class="code" href="namespaceGridRefinement.html#a48e5395381ed87155942a61a1edd134d">GridRefinement::refine_and_coarsen_fixed_number</a>(triangulation,</div><div class="line">                                                  localized_all_errors,</div><div class="line">                                                  0.3,</div><div class="line">                                                  0.03);</div><div class="line">  triangulation.execute_coarsening_and_refinement();</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemoutput_results"></a> </p><h4>ElasticProblem::output_results</h4>
<p>The final function of significant interest is the one that creates graphical output. This works the same way as in <a class="el" href="step_8.html">step-8</a>, with two small differences. Before discussing these, let us state the general philosophy this function will work: we intend for all of the data to be generated on a single process, and subsequently written to a file. This is, as many other parts of this program already discussed, not something that will scale. Previously, we had argued that we will get into trouble with triangulations, DoFHandlers, and copies of the solution vector where every process has to store all of the data, and that there will come to be a point where each process simply doesn't have enough memory to store that much data. Here, the situation is different: it's not only the memory, but also the run time that's a problem. If one process is responsible for processing <em>all</em> of the data while all of the other processes do nothing, then this one function will eventually come to dominate the overall run time of the program. In particular, the time this function takes is going to be proportional to the overall size of the problem (counted in the number of cells, or the number of degrees of freedom), independent of the number of processes we throw at it.</p>
<p>Such situations need to be avoided, and we will show in <a class="el" href="step_18.html">step-18</a> and <a class="el" href="step_40.html">step-40</a> how to address this issue. For the current problem, the solution is to have each process generate output data only for its own local cells, and write them to separate files, one file per process. This is how <a class="el" href="step_18.html">step-18</a> operates. Alternatively, one could simply leave everything in a set of independent files and let the visualization software read all of them (possibly also using multiple processors) and create a single visualization out of all of them; this is the path <a class="el" href="step_40.html">step-40</a>, <a class="el" href="step_32.html">step-32</a>, and all other parallel programs developed later on take.</p>
<p>More specifically for the current function, all processes call this function, but not all of them need to do the work associated with generating output. In fact, they shouldn't, since we would try to write to the same file multiple times at once. So we let only the first process do this, and all the other ones idle around during this time (or start their work for the next iteration, or simply yield their CPUs to other jobs that happen to run at the same time). The second thing is that we not only output the solution vector, but also a vector that indicates which subdomain each cell belongs to. This will make for some nice pictures of partitioned domains.</p>
<p>To implement this, process zero needs a complete set of solution components in a local vector. Just as with the previous function, the efficient way to do this would be to re-use the vector already created in the <code>solve()</code> function, but to keep things more self-contained, we simply re-create one here from the distributed solution vector.</p>
<p>An important thing to realize is that we do this localization operation on all processes, not only the one that actually needs the data. This can't be avoided, however, with the simplified communication model of MPI we use for vectors in this tutorial program: MPI does not have a way to query data on another process, both sides have to initiate a communication at the same time. So even though most of the processes do not need the localized solution, we have to place the statement converting the distributed into a localized vector so that all processes execute it.</p>
<p>(Part of this work could in fact be avoided. What we do is send the local parts of all processes to all other processes. What we would really need to do is to initiate an operation on all processes where each process simply sends its local chunk of data to process zero, since this is the only one that actually needs it, i.e., we need something like a gather operation. PETSc can do this, but for simplicity's sake we don't attempt to make use of this here. We don't, since what we do is not very expensive in the grand scheme of things: it is one vector communication among all processes, which has to be compared to the number of communications we have to do when solving the linear system, setting up the block-ILU for the preconditioner, and other operations.)</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword"></span>{</div><div class="line">  <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution(solution);</div></div><!-- fragment --><p>This being done, process zero goes ahead with setting up the output file as in <a class="el" href="step_8.html">step-8</a>, and attaching the (localized) solution vector to the output object.</p>
<div class="fragment"><div class="line"><span class="keywordflow">if</span> (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0)</div><div class="line">  {</div><div class="line">    std::ofstream output(<span class="stringliteral">&quot;solution-&quot;</span> + <a class="code" href="namespacePatterns_1_1Tools.html#a72743302dcb1a0fb1f2f8dc5122d299e">std::to_string</a>(cycle) + <span class="stringliteral">&quot;.vtk&quot;</span>);</div><div class="line"></div><div class="line">    <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a6ed7c846331069f406b8c9933c37fda4">attach_dof_handler</a>(dof_handler);</div><div class="line"></div><div class="line">    std::vector&lt;std::string&gt; solution_names;</div><div class="line">    <span class="keywordflow">switch</span> (dim)</div><div class="line">      {</div><div class="line">        <span class="keywordflow">case</span> 1:</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;displacement&quot;</span>);</div><div class="line">          <span class="keywordflow">break</span>;</div><div class="line">        <span class="keywordflow">case</span> 2:</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">          <span class="keywordflow">break</span>;</div><div class="line">        <span class="keywordflow">case</span> 3:</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">          solution_names.emplace_back(<span class="stringliteral">&quot;z_displacement&quot;</span>);</div><div class="line">          <span class="keywordflow">break</span>;</div><div class="line">        <span class="keywordflow">default</span>:</div><div class="line">          <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(<span class="keyword">false</span>, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line">      }</div><div class="line"></div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(localized_solution, solution_names);</div></div><!-- fragment --><p>The only other thing we do here is that we also output one value per cell indicating which subdomain (i.e., MPI process) it belongs to. This requires some conversion work, since the data the library provides us with is not the one the output class expects, but this is not difficult. First, set up a vector of integers, one per cell, that is then filled by the subdomain id of each cell.</p>
<p>The elements of this vector are then converted to a floating point vector in a second step, and this vector is added to the <a class="el" href="classDataOut.html">DataOut</a> object, which then goes off creating output in VTK format:</p>
<div class="fragment"><div class="line">      std::vector&lt;unsigned int&gt; partition_int(triangulation.n_active_cells());</div><div class="line">      <a class="code" href="namespaceGridTools.html#ac41d959ae1723a898b616c3320241ffe">GridTools::get_subdomain_association</a>(triangulation, partition_int);</div><div class="line"></div><div class="line">      <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> partitioning(partition_int.begin(),</div><div class="line">                                        partition_int.end());</div><div class="line"></div><div class="line">      data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(partitioning, <span class="stringliteral">&quot;partitioning&quot;</span>);</div><div class="line"></div><div class="line">      data_out.<a class="code" href="classDataOut.html#a087f63e22f0614bca326dbdca288c646">build_patches</a>();</div><div class="line">      data_out.<a class="code" href="classDataOutInterface.html#acad99726038e4fca7f605fdffb3317e4">write_vtk</a>(output);</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemrun"></a> </p><h4>ElasticProblem::run</h4>
<p>Lastly, here is the driver function. It is almost completely unchanged from <a class="el" href="step_8.html">step-8</a>, with the exception that we replace <code>std::cout</code> by the <code>pcout</code> stream. Apart from this, the only other cosmetic change is that we output how many degrees of freedom there are per process, and how many iterations it took for the linear solver to converge:</p>
<div class="fragment"><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">ElasticProblem&lt;dim&gt;::run</a>()</div><div class="line">  {</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle = 0; cycle &lt; 10; ++cycle)</div><div class="line">      {</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">          {</div><div class="line">            <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a>(triangulation, -1, 1);</div><div class="line">            triangulation.refine_global(3);</div><div class="line">          }</div><div class="line">        <span class="keywordflow">else</span></div><div class="line">          refine_grid();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">              &lt;&lt; triangulation.n_active_cells() &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        setup_system();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span> &lt;&lt; dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>()</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot; (by partition:&quot;</span>;</div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p = 0; p &lt; <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>; ++p)</div><div class="line">          pcout &lt;&lt; (p == 0 ? <span class="charliteral">&#39; &#39;</span> : <span class="charliteral">&#39;+&#39;</span>)</div><div class="line">                &lt;&lt; (<a class="code" href="namespaceDoFTools.html#ac704c6d311cd0f289d625427e03708ac">DoFTools::count_dofs_with_subdomain_association</a>(dof_handler,</div><div class="line">                                                                    p));</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;)&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        assemble_system();</div><div class="line">        <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_iterations = solve();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Solver converged in &quot;</span> &lt;&lt; n_iterations &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        output_results(cycle);</div><div class="line">      }</div><div class="line">  }</div><div class="line">} <span class="comment">// namespace Step17</span></div></div><!-- fragment --><p><a class="anchor" id="Thecodemaincodefunction"></a> </p><h3>The <code>main</code> function</h3>
<p>The <code>main()</code> works the same way as most of the main functions in the other example programs, i.e., it delegates work to the <code>run</code> function of a managing object, and only wraps everything into some code to catch exceptions:</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> **argv)</div><div class="line">{</div><div class="line">  <span class="keywordflow">try</span></div><div class="line">    {</div><div class="line">      <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">      <span class="keyword">using namespace </span>Step17;</div></div><!-- fragment --><p>Here is the only real difference: MPI and PETSc both require that we initialize these libraries at the beginning of the program, and un-initialize them at the end. The class MPI_InitFinalize takes care of all of that. The trailing argument <code>1</code> means that we do want to run each MPI process with a single thread, a prerequisite with the PETSc parallel linear algebra.</p>
<div class="fragment"><div class="line">      <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">      ElasticProblem&lt;2&gt; elastic_problem;</div><div class="line">      elastic_problem.run();</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (...)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p> examples/step-17/doc/results.dox</p>
<p><a class="anchor" id="Results"></a></p><h1>Results</h1>
<p>如果上述程序在单处理器机器上编译和运行，它产生的结果应该与我们已经通过步骤8得到的结果非常相似。然而，如果我们在多核机器或计算机集群上运行它，就会变得更加有趣。运行MPI程序的最基本方法是使用一个命令行，如</p>
<div class="fragment"><div class="line">mpirun -np 32 ./step-17</div></div><!-- fragment --><p>以32个处理器运行<a class="el" href="step_17.html">step-17</a>可执行文件。</p>
<p>如果你在一个集群上工作，那么中间通常有一个步骤，你需要设置一个作业脚本，并将该脚本提交给调度器。只要调度器能够为你的工作分配32个未使用的处理器，它就会执行这个脚本。如何编写这样的作业脚本因集群而异，你应该找到你的集群的文档来看看如何做。在我的系统上，我必须使用 <code>qsub</code> 这个命令，加上一大堆的选项来并行运行一个作业）。)</p>
<p>无论是直接还是通过调度器，如果你在8个处理器上运行这个程序，你应该得到如下输出。</p>
<div class="fragment"><div class="line">Cycle 0:</div><div class="line">   Number of active cells:       64</div><div class="line">   Number of degrees of freedom: 162 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 22+22+20+20+18+16+20+24)</div><div class="line">   Solver converged in 23 iterations.</div><div class="line">Cycle 1:</div><div class="line">   Number of active cells:       124</div><div class="line">   Number of degrees of freedom: 302 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 38+42+36+34+44+44+36+28)</div><div class="line">   Solver converged in 35 iterations.</div><div class="line">Cycle 2:</div><div class="line">   Number of active cells:       238</div><div class="line">   Number of degrees of freedom: 570 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 68+80+66+74+58+68+78+78)</div><div class="line">   Solver converged in 46 iterations.</div><div class="line">Cycle 3:</div><div class="line">   Number of active cells:       454</div><div class="line">   Number of degrees of freedom: 1046 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 120+134+124+130+154+138+122+124)</div><div class="line">   Solver converged in 55 iterations.</div><div class="line">Cycle 4:</div><div class="line">   Number of active cells:       868</div><div class="line">   Number of degrees of freedom: 1926 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 232+276+214+248+230+224+234+268)</div><div class="line">   Solver converged in 77 iterations.</div><div class="line">Cycle 5:</div><div class="line">   Number of active cells:       1654</div><div class="line">   Number of degrees of freedom: 3550 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 418+466+432+470+442+474+424+424)</div><div class="line">   Solver converged in 93 iterations.</div><div class="line">Cycle 6:</div><div class="line">   Number of active cells:       3136</div><div class="line">   Number of degrees of freedom: 6702 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 838+796+828+892+866+798+878+806)</div><div class="line">   Solver converged in 127 iterations.</div><div class="line">Cycle 7:</div><div class="line">   Number of active cells:       5962</div><div class="line">   Number of degrees of freedom: 12446 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 1586+1484+1652+1552+1556+1576+1560+1480)</div><div class="line">   Solver converged in 158 iterations.</div><div class="line">Cycle 8:</div><div class="line">   Number of active cells:       11320</div><div class="line">   Number of degrees of freedom: 23586 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 2988+2924+2890+2868+2864+3042+2932+3078)</div><div class="line">   Solver converged in 225 iterations.</div><div class="line">Cycle 9:</div><div class="line">   Number of active cells:       21424</div><div class="line">   Number of degrees of freedom: 43986 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 5470+5376+5642+5450+5630+5470+5416+5532)</div><div class="line">   Solver converged in 282 iterations.</div><div class="line">Cycle 10:</div><div class="line">   Number of active cells:       40696</div><div class="line">   Number of degrees of freedom: 83754 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 10660+10606+10364+10258+10354+10322+10586+10604)</div><div class="line">   Solver converged in 392 iterations.</div><div class="line">Cycle 11:</div><div class="line">   Number of active cells:       76978</div><div class="line">   Number of degrees of freedom: 156490 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 19516+20148+19390+19390+19336+19450+19730+19530)</div><div class="line">   Solver converged in 509 iterations.</div><div class="line">Cycle 12:</div><div class="line">   Number of active cells:       146206</div><div class="line">   Number of degrees of freedom: 297994 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 37462+37780+37000+37060+37232+37328+36860+37272)</div><div class="line">   Solver converged in 705 iterations.</div><div class="line">Cycle 13:</div><div class="line">   Number of active cells:       276184</div><div class="line">   Number of degrees of freedom: 558766 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 69206+69404+69882+71266+70348+69616+69796+69248)</div><div class="line">   Solver converged in 945 iterations.</div><div class="line">Cycle 14:</div><div class="line">   Number of active cells:       523000</div><div class="line">   Number of degrees of freedom: 1060258 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 132928+132296+131626+132172+132170+133588+132252+133226)</div><div class="line">   Solver converged in 1282 iterations.</div><div class="line">Cycle 15:</div><div class="line">   Number of active cells:       987394</div><div class="line">   Number of degrees of freedom: 1994226 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 253276+249068+247430+248402+248496+251380+248272+247902)</div><div class="line">   Solver converged in 1760 iterations.</div><div class="line">Cycle 16:</div><div class="line">   Number of active cells:       1867477</div><div class="line">   Number of degrees of freedom: 3771884 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 468452+474204+470818+470884+469960+</div><div class="line">471186+470686+475694)</div><div class="line">   Solver converged in 2251 iterations.</div></div><!-- fragment --><p>(这次运行比examples/目录中的代码多用了几个细化周期。该运行还使用了2004年的METIS版本，产生了不同的分区；因此，你今天得到的数字略有不同）。)</p>
<p>可以看出，我们可以很容易地达到近400万个未知数。事实上，这段代码在8个进程中的运行时间不到7分钟，直到（包括）第14个周期，14分钟包括倒数第二步。(这些数字与该代码最初编写的时间有关，即2004年。)虽然我失去了最后一步的时间信息，但你会明白的。所有这些都是在通过运行 <code>make release</code> 启用发布模式之后，并且由于上述程序注释中所述的原因，关闭了图形输出的生成。( See also <a href="http://www.math.colostate.edu/~bangerth/videos.676.18.html">video lecture 18</a>.) 我做的最大的2D计算大约有710万个未知数，在32个进程上完成。花了大约40分钟。毫不奇怪，一个人能够走多远的限制因素是他有多少内存，因为每个进程都必须持有整个网格和DoFHandler对象，尽管矩阵和向量被分割开来。对于7.1M的计算，每个未知数的内存消耗约为600字节，这并不坏，但我们必须考虑到这是针对每个未知数的，无论我们是否在本地存储矩阵和向量条目。</p>
<p>下面是在程序的第12个周期中产生的一些输出，即大约有30万个未知数。</p>
<table align="center" style="width:80%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-ux.png" width="100%"/>
</div>
 </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-uy.png" width="100%"/>
</div>
  </td></tr>
</table>
<p>正如人们所希望的那样，这里显示的X位移（左）和Y位移（右）与我们在第8步中已经看到的密切相关。正如第22步所示，我们也可以制作一个位移场的矢量图，而不是把它绘制成两个独立的标量场。不过，可能更有趣的是，在这一步看一下网格和分区。</p>
<table align="center" width="80%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-grid.png" width="100%"/>
</div>
 </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-partition.png" width="100%"/>
</div>
  </td></tr>
</table>
<p>同样，网格（左边）显示了与之前看到的相同的细化模式。右图显示了8个过程中的域的划分，每个过程用不同的颜色表示。图片显示，在网格单元较小的地方，子域较小，考虑到分区算法试图平衡每个子域的单元数，这一事实是需要预期的；这种平衡在上图所示的输出中也很容易识别，每个子域的度数大致相同。</p>
<p>值得注意的是，如果我们用不同的进程数来运行同一个程序，我们可能会得到稍微不同的输出：不同的网格，不同的未知数和迭代收敛的次数。其原因是，虽然矩阵和右手边是相同的，与使用的进程数无关，但预处理程序不是：它对每个处理器的 <em> 矩阵块分别执行ILU(0) </em> 。因此，随着进程数的增加，它作为预处理程序的有效性会降低，这使得迭代次数增加。由于不同的预处理程序会导致计算出的解有细微的变化，这将导致细化时标记的网格单元略有不同，在后续步骤中的差异也更大。不过，解决方案看起来总是非常相似的。</p>
<p>最后，这里是3D模拟的一些结果。你可以通过改变以下内容来重复这些结果</p>
<div class="fragment"><div class="line">ElasticProblem&lt;2&gt; elastic_problem;</div></div><!-- fragment --><p>至</p>
<div class="fragment"><div class="line">ElasticProblem&lt;3&gt; elastic_problem;</div></div><!-- fragment --><p>在主函数中。如果你再并行运行该程序，你会得到与此类似的东西（这是针对一个有16个进程的工作）。</p>
<div class="fragment"><div class="line">Cycle 0:</div><div class="line">   Number of active cells:       512</div><div class="line">   Number of degrees of freedom: 2187 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 114+156+150+114+114+210+105+102+120+120+96+123+141+183+156+183)</div><div class="line">   Solver converged in 27 iterations.</div><div class="line">Cycle 1:</div><div class="line">   Number of active cells:       1604</div><div class="line">   Number of degrees of freedom: 6549 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 393+291+342+354+414+417+570+366+444+288+543+525+345+387+489+381)</div><div class="line">   Solver converged in 42 iterations.</div><div class="line">Cycle 2:</div><div class="line">   Number of active cells:       4992</div><div class="line">   Number of degrees of freedom: 19167 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 1428+1266+1095+1005+1455+1257+1410+1041+1320+1380+1080+1050+963+1005+1188+1224)</div><div class="line">   Solver converged in 65 iterations.</div><div class="line">Cycle 3:</div><div class="line">   Number of active cells:       15485</div><div class="line">   Number of degrees of freedom: 56760 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 3099+3714+3384+3147+4332+3858+3615+3117+3027+3888+3942+3276+4149+3519+3030+3663)</div><div class="line">   Solver converged in 96 iterations.</div><div class="line">Cycle 4:</div><div class="line">   Number of active cells:       48014</div><div class="line">   Number of degrees of freedom: 168762 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 11043+10752+9846+10752+9918+10584+10545+11433+12393+11289+10488+9885+10056+9771+11031+8976)</div><div class="line">   Solver converged in 132 iterations.</div><div class="line">Cycle 5:</div><div class="line">   Number of active cells:       148828</div><div class="line">   Number of degrees of freedom: 492303 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 31359+30588+34638+32244+30984+28902+33297+31569+29778+29694+28482+28032+32283+30702+31491+28260)</div><div class="line">   Solver converged in 179 iterations.</div><div class="line">Cycle 6:</div><div class="line">   Number of active cells:       461392</div><div class="line">   Number of degrees of freedom: 1497951 (by <a class="code" href="namespaceSparsityTools.html#a452753b6ffdf31b33f2bcd792b05df93">partition</a>: 103587+100827+97611+93726+93429+88074+95892+88296+96882+93000+87864+90915+92232+86931+98091+90594)</div><div class="line">   Solver converged in 261 iterations.</div></div><!-- fragment --><p>最后一步，达到150万个未知数，在8台双处理器机器（2003年可用的那种）上进行16个进程，需要大约55分钟。这个工作产生的图形输出相当大（第5周期已经打印了大约82MB的数据），所以我们要显示第4周期的输出。</p>
<table width="80%" align="center">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.4-3d-partition.png" width="100%"/>
</div>
 </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.4-3d-ux.png" width="100%"/>
</div>
  </td></tr>
</table>
<p>左图显示的是将立方体划分为16个过程，而右图显示的是沿两个切面通过立方体的X位移。</p>
<p><a class="anchor" id="extensions"></a></p>
<p><a class="anchor" id="Possibilitiesforextensions"></a></p><h3>Possibilities for extensions</h3>
<p>该程序在每个处理器上都保留一份三角形和DoFHandler对象的完整副本。它还创建了解决方案矢量的完整副本，并且只在一个处理器上创建输出。就并行化而言，所有这些显然是瓶颈。</p>
<p>在内部，在deal.II中，将分层和非结构化三角计算中使用的数据结构并行化是一个难点，我们又花了几年时间才实现了这一点。<a class="el" href="step_40.html">step-40</a>教程程序和 <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> 文档模块谈到了如何做这些步骤，以及从应用的角度来看需要什么。当前程序的一个明显的扩展是使用这个功能将计算完全分布到比这里使用的更多的处理器。</p>
<p><a class="anchor" id="PlainProg"></a> </p><h1>The plain program</h1>
<div class="fragment"><div class="line"><span class="comment">/* ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Copyright (C) 2000 - 2021 by the deal.II authors</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * This file is part of the deal.II library.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * The deal.II library is free software; you can use it, redistribute</span></div><div class="line"><span class="comment"> * it, and/or modify it under the terms of the GNU Lesser General</span></div><div class="line"><span class="comment"> * Public License as published by the Free Software Foundation; either</span></div><div class="line"><span class="comment"> * version 2.1 of the License, or (at your option) any later version.</span></div><div class="line"><span class="comment"> * The full text of the license can be found in the file LICENSE.md at</span></div><div class="line"><span class="comment"> * the top level directory of deal.II.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Author: Wolfgang Bangerth, University of Texas at Austin, 2000, 2004</span></div><div class="line"><span class="comment"> *         Wolfgang Bangerth, Texas A&amp;M University, 2016</span></div><div class="line"><span class="comment"> */</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="quadrature__lib_8h.html">deal.II/base/quadrature_lib.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="function_8h.html">deal.II/base/function.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="logstream_8h.html">deal.II/base/logstream.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="multithread__info_8h.html">deal.II/base/multithread_info.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8h.html">deal.II/lac/vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="full__matrix_8h.html">deal.II/lac/full_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="affine__constraints_8h.html">deal.II/lac/affine_constraints.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dynamic__sparsity__pattern_8h.html">deal.II/lac/dynamic_sparsity_pattern.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="sparsity__tools_8h.html">deal.II/lac/sparsity_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid_2tria_8h.html">deal.II/grid/tria.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__generator_8h.html">deal.II/grid/grid_generator.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid_2grid__refinement_8h.html">deal.II/grid/grid_refinement.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dofs_2dof__handler_8h.html">deal.II/dofs/dof_handler.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__tools_8h.html">deal.II/dofs/dof_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe_2fe__values_8h.html">deal.II/fe/fe_values.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__system_8h.html">deal.II/fe/fe_system.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__q_8h.html">deal.II/fe/fe_q.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector__tools_8h.html">deal.II/numerics/vector_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="matrix__tools_8h.html">deal.II/numerics/matrix_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="numerics_2data__out_8h.html">deal.II/numerics/data_out.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="error__estimator_8h.html">deal.II/numerics/error_estimator.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="conditional__ostream_8h.html">deal.II/base/conditional_ostream.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="mpi_8h.html">deal.II/base/mpi.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__vector_8h.html">deal.II/lac/petsc_vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__sparse__matrix_8h.html">deal.II/lac/petsc_sparse_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__solver_8h.html">deal.II/lac/petsc_solver.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="petsc__precondition_8h.html">deal.II/lac/petsc_precondition.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__tools_8h.html">deal.II/grid/grid_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__renumbering_8h.html">deal.II/dofs/dof_renumbering.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>Step17</div><div class="line">{</div><div class="line">  <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keyword">class </span>ElasticProblem</div><div class="line">  {</div><div class="line">  <span class="keyword">public</span>:</div><div class="line">    ElasticProblem();</div><div class="line">    <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">run</a>();</div><div class="line"></div><div class="line">  <span class="keyword">private</span>:</div><div class="line">    <span class="keywordtype">void</span>         setup_system();</div><div class="line">    <span class="keywordtype">void</span>         assemble_system();</div><div class="line">    <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> solve();</div><div class="line">    <span class="keywordtype">void</span>         refine_grid();</div><div class="line">    <span class="keywordtype">void</span>         output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div><div class="line"></div><div class="line">    <a class="code" href="classMPI__Comm.html">MPI_Comm</a> mpi_communicator;</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>;</div><div class="line"></div><div class="line">    <a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div><div class="line"></div><div class="line">    <a class="code" href="classTriangulation.html">Triangulation&lt;dim&gt;</a> <a class="code" href="p4est__wrappers_8cc.html#ace00f2f80d9780ef9aa1007e1c22c6a4">triangulation</a>;</div><div class="line">    <a class="code" href="classFESystem.html">FESystem&lt;dim&gt;</a>      fe;</div><div class="line">    <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a>    dof_handler;</div><div class="line"></div><div class="line">    <a class="code" href="classAffineConstraints.html">AffineConstraints&lt;double&gt;</a> hanging_node_constraints;</div><div class="line"></div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1SparseMatrix.html">PETScWrappers::MPI::SparseMatrix</a> system_matrix;</div><div class="line"></div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> solution;</div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> system_rhs;</div><div class="line">  };</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keyword">class </span>RightHandSide : <span class="keyword">public</span> <a class="code" href="classFunction.html">Function</a>&lt;dim&gt;</div><div class="line">  {</div><div class="line">  <span class="keyword">public</span>:</div><div class="line">    <span class="keyword">virtual</span> <span class="keywordtype">void</span> vector_value(<span class="keyword">const</span> <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &amp;p,</div><div class="line">                              Vector&lt;double&gt; &amp;  values)<span class="keyword"> const override</span></div><div class="line"><span class="keyword">    </span>{</div><div class="line">      <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(values.size() == dim, <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a>(values.size(), dim));</div><div class="line">      <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(dim &gt;= 2, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line"></div><div class="line">      <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> point_1, point_2;</div><div class="line">      point_1(0) = 0.5;</div><div class="line">      point_2(0) = -0.5;</div><div class="line"></div><div class="line">      <span class="keywordflow">if</span> (((p - point_1).norm_square() &lt; 0.2 * 0.2) ||</div><div class="line">          ((p - point_2).norm_square() &lt; 0.2 * 0.2))</div><div class="line">        <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(0) = 1;</div><div class="line">      <span class="keywordflow">else</span></div><div class="line">        <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(0) = 0;</div><div class="line"></div><div class="line">      <span class="keywordflow">if</span> (p.<a class="code" href="classPoint.html#a859ea7f3bf3e64be2e0f5ed1bfcc8550">square</a>() &lt; 0.2 * 0.2)</div><div class="line">        <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(1) = 1;</div><div class="line">      <span class="keywordflow">else</span></div><div class="line">        <a class="code" href="namespaceEvaluationFlags.html#a9b7c6d689cb76386839d0d13640f59aeaf9825c682f693a6a200094641a0d6a58">values</a>(1) = 0;</div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="keyword">virtual</span> <span class="keywordtype">void</span></div><div class="line">    vector_value_list(<span class="keyword">const</span> std::vector&lt;<a class="code" href="classPoint.html">Point&lt;dim&gt;</a>&gt; &amp;points,</div><div class="line">                      std::vector&lt;Vector&lt;double&gt;&gt; &amp;  value_list)<span class="keyword"> const override</span></div><div class="line"><span class="keyword">    </span>{</div><div class="line">      <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_points = points.size();</div><div class="line"></div><div class="line">      <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(value_list.size() == n_points,</div><div class="line">             <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a>(value_list.size(), n_points));</div><div class="line"></div><div class="line">      <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p = 0; p &lt; n_points; ++p)</div><div class="line">        RightHandSide&lt;dim&gt;::vector_value(points[p], value_list[p]);</div><div class="line">    }</div><div class="line">  };</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  ElasticProblem&lt;dim&gt;::ElasticProblem()</div><div class="line">    : mpi_communicator(MPI_COMM_WORLD)</div><div class="line">    , <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(mpi_communicator))</div><div class="line">    , <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator))</div><div class="line">    , pcout(<a class="code" href="namespacestd.html">std</a>::cout, (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0))</div><div class="line">    , fe(<a class="code" href="classFE__Q.html">FE_Q</a>&lt;dim&gt;(1), dim)</div><div class="line">    , dof_handler(triangulation)</div><div class="line">  {}</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::setup_system()</div><div class="line">  {</div><div class="line">    <a class="code" href="namespaceGridTools.html#a99eba8e3b388258eda37a2724579dd1d">GridTools::partition_triangulation</a>(<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>, triangulation);</div><div class="line"></div><div class="line">    dof_handler.<a class="code" href="classDoFHandler.html#a553ca864aaf70330d9be86bc78f36d1e">distribute_dofs</a>(fe);</div><div class="line">    <a class="code" href="namespaceDoFRenumbering.html#a166d0082503ec2b8d9c560222b48f9d2">DoFRenumbering::subdomain_wise</a>(dof_handler);</div><div class="line"></div><div class="line">    hanging_node_constraints.clear();</div><div class="line">    <a class="code" href="group__constraints.html#ga3b4ea7dfd313e388d868c4e4aa685799">DoFTools::make_hanging_node_constraints</a>(dof_handler,</div><div class="line">                                            hanging_node_constraints);</div><div class="line">    hanging_node_constraints.close();</div><div class="line"></div><div class="line">    <a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>(), dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>());</div><div class="line">    <a class="code" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a>(dof_handler,</div><div class="line">                                    dsp,</div><div class="line">                                    hanging_node_constraints,</div><div class="line">                                    <span class="keyword">false</span>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> std::vector&lt;IndexSet&gt; locally_owned_dofs_per_proc =</div><div class="line">      <a class="code" href="namespaceDoFTools.html#a180d7118ecf27d72afbfecb7978c5e09">DoFTools::locally_owned_dofs_per_subdomain</a>(dof_handler);</div><div class="line">    <span class="keyword">const</span> <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs =</div><div class="line">      locally_owned_dofs_per_proc[<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>];</div><div class="line"></div><div class="line">    system_matrix.reinit(locally_owned_dofs,</div><div class="line">                         locally_owned_dofs,</div><div class="line">                         dsp,</div><div class="line">                         mpi_communicator);</div><div class="line"></div><div class="line">    solution.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line">    system_rhs.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::assemble_system()</div><div class="line">  {</div><div class="line">    <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a>   quadrature_formula(fe.<a class="code" href="classFiniteElementData.html#a2cbf5ad6b464871261dbd054bced18a8">degree</a> + 1);</div><div class="line">    <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values(fe,</div><div class="line">                            quadrature_formula,</div><div class="line">                            <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                              <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> dofs_per_cell = fe.<a class="code" href="classFiniteElementData.html#a33b522422da89e5c080e7405ad49d7c7">n_dofs_per_cell</a>();</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_q_points    = quadrature_formula.size();</div><div class="line"></div><div class="line">    <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a> <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(dofs_per_cell, dofs_per_cell);</div><div class="line">    Vector&lt;double&gt;     cell_rhs(dofs_per_cell);</div><div class="line"></div><div class="line">    std::vector&lt;types::global_dof_index&gt; local_dof_indices(dofs_per_cell);</div><div class="line"></div><div class="line">    std::vector&lt;double&gt; lambda_values(n_q_points);</div><div class="line">    std::vector&lt;double&gt; mu_values(n_q_points);</div><div class="line"></div><div class="line">    <a class="code" href="classFunctions_1_1ConstantFunction.html">Functions::ConstantFunction&lt;dim&gt;</a> <a class="code" href="namespaceDifferentiation_1_1SD.html#a841a7b84dc17bf2ba675522093a97e8ba945f3fc449518a73b9f5f32868db466c">lambda</a>(1.), <a class="code" href="namespacemu.html">mu</a>(1.);</div><div class="line"></div><div class="line">    RightHandSide&lt;dim&gt;          right_hand_side;</div><div class="line">    std::vector&lt;Vector&lt;double&gt;&gt; rhs_values(n_q_points, Vector&lt;double&gt;(dim));</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;cell : dof_handler.<a class="code" href="group__CPP11.html#gacdb6d3adec96a13a7079f6c893e1d3ff">active_cell_iterators</a>())</div><div class="line">      <span class="keywordflow">if</span> (cell-&gt;subdomain_id() == <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>)</div><div class="line">        {</div><div class="line">          cell_matrix = 0;</div><div class="line">          cell_rhs    = 0;</div><div class="line"></div><div class="line">          fe_values.<a class="code" href="classFEValues.html#a21f914e63d588e2652a9514620653d77">reinit</a>(cell);</div><div class="line"></div><div class="line">          <a class="code" href="namespaceDifferentiation_1_1SD.html#a841a7b84dc17bf2ba675522093a97e8ba945f3fc449518a73b9f5f32868db466c">lambda</a>.value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(), lambda_values);</div><div class="line">          <a class="code" href="namespacemu.html">mu</a>.value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(), mu_values);</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_i =</div><div class="line">                fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j = 0; j &lt; dofs_per_cell; ++j)</div><div class="line">                {</div><div class="line">                  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_j =</div><div class="line">                    fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(j).first;</div><div class="line"></div><div class="line">                  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points;</div><div class="line">                       ++q_point)</div><div class="line">                    {</div><div class="line">                      <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(i, j) +=</div><div class="line">                        ((fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)[component_i] *</div><div class="line">                          fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)[component_j] *</div><div class="line">                          lambda_values[q_point]) +</div><div class="line">                         (fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)[component_j] *</div><div class="line">                          fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)[component_i] *</div><div class="line">                          mu_values[q_point]) +</div><div class="line">                         ((component_i == component_j) ?</div><div class="line">                            (fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point) *</div><div class="line">                             fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point) *</div><div class="line">                             mu_values[q_point]) :</div><div class="line">                            0)) *</div><div class="line">                        fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">                    }</div><div class="line">                }</div><div class="line">            }</div><div class="line"></div><div class="line">          right_hand_side.vector_value_list(fe_values.<a class="code" href="classFEValuesBase.html#ae41b67cfd48e02f6035e39c84f0fb47a">get_quadrature_points</a>(),</div><div class="line">                                            rhs_values);</div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> component_i =</div><div class="line">                fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points; ++q_point)</div><div class="line">                cell_rhs(i) += fe_values.<a class="code" href="classFEValuesBase.html#a1dd48cb744013c448d57f8f77640c08d">shape_value</a>(i, q_point) *</div><div class="line">                               rhs_values[q_point](component_i) *</div><div class="line">                               fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">            }</div><div class="line"></div><div class="line">          cell-&gt;get_dof_indices(local_dof_indices);</div><div class="line">          hanging_node_constraints.distribute_local_to_global(cell_matrix,</div><div class="line">                                                              cell_rhs,</div><div class="line">                                                              local_dof_indices,</div><div class="line">                                                              system_matrix,</div><div class="line">                                                              system_rhs);</div><div class="line">        }</div><div class="line"></div><div class="line">    system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">    system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line"></div><div class="line">    std::map&lt;types::global_dof_index, double&gt; boundary_values;</div><div class="line">    <a class="code" href="namespaceVectorTools.html#af27ac28c698a9ed0199faed50a204538">VectorTools::interpolate_boundary_values</a>(dof_handler,</div><div class="line">                                             0,</div><div class="line">                                             <a class="code" href="classFunctions_1_1ZeroFunction.html">Functions::ZeroFunction&lt;dim&gt;</a>(dim),</div><div class="line">                                             boundary_values);</div><div class="line">    <a class="code" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values</a>(</div><div class="line">      boundary_values, system_matrix, solution, system_rhs, <span class="keyword">false</span>);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> ElasticProblem&lt;dim&gt;::solve()</div><div class="line">  {</div><div class="line">    <a class="code" href="classSolverControl.html">SolverControl</a> solver_control(solution.size(), 1<a class="code" href="namespacePhysics_1_1Elasticity_1_1Kinematics.html#a9587d5229555daa5b1fa1ba2f8a40adb">e</a>-8 * system_rhs.l2_norm());</div><div class="line">    <a class="code" href="classPETScWrappers_1_1SolverCG.html">PETScWrappers::SolverCG</a> cg(solver_control, mpi_communicator);</div><div class="line"></div><div class="line">    <a class="code" href="classPETScWrappers_1_1PreconditionBlockJacobi.html">PETScWrappers::PreconditionBlockJacobi</a> preconditioner(system_matrix);</div><div class="line"></div><div class="line">    cg.solve(system_matrix, solution, system_rhs, preconditioner);</div><div class="line"></div><div class="line">    Vector&lt;double&gt; localized_solution(solution);</div><div class="line"></div><div class="line">    hanging_node_constraints.distribute(localized_solution);</div><div class="line"></div><div class="line">    solution = localized_solution;</div><div class="line"></div><div class="line">    <span class="keywordflow">return</span> solver_control.last_step();</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::refine_grid()</div><div class="line">  {</div><div class="line">    <span class="keyword">const</span> Vector&lt;double&gt; localized_solution(solution);</div><div class="line"></div><div class="line">    Vector&lt;float&gt; local_error_per_cell(triangulation.n_active_cells());</div><div class="line">    <a class="code" href="classKellyErrorEstimator.html#ae2269e1c9903e9d863b7abd54948af00">KellyErrorEstimator&lt;dim&gt;::estimate</a>(dof_handler,</div><div class="line">                                       <a class="code" href="classQGauss.html">QGauss&lt;dim - 1&gt;</a>(fe.<a class="code" href="classFiniteElementData.html#a2cbf5ad6b464871261dbd054bced18a8">degree</a> + 1),</div><div class="line">                                       {},</div><div class="line">                                       localized_solution,</div><div class="line">                                       local_error_per_cell,</div><div class="line">                                       <a class="code" href="classComponentMask.html">ComponentMask</a>(),</div><div class="line">                                       <span class="keyword">nullptr</span>,</div><div class="line">                                       <a class="code" href="classMultithreadInfo.html#ad0b84ae105b385b88bdd4bfc0c530995">MultithreadInfo::n_threads</a>(),</div><div class="line">                                       <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_local_cells =</div><div class="line">      <a class="code" href="namespaceGridTools.html#a8c212a30784bec20b1ae13fad3fd579c">GridTools::count_cells_with_subdomain_association</a>(triangulation,</div><div class="line">                                                        <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>);</div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a> distributed_all_errors(</div><div class="line">      mpi_communicator, triangulation.n_active_cells(), n_local_cells);</div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; local_error_per_cell.size(); ++i)</div><div class="line">      <span class="keywordflow">if</span> (local_error_per_cell(i) != 0)</div><div class="line">        distributed_all_errors(i) = local_error_per_cell(i);</div><div class="line">    distributed_all_errors.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae5042eefddc828c7c31e1e8e26da8b09">VectorOperation::insert</a>);</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keyword">const</span> Vector&lt;float&gt; localized_all_errors(distributed_all_errors);</div><div class="line"></div><div class="line">    <a class="code" href="namespaceGridRefinement.html#a48e5395381ed87155942a61a1edd134d">GridRefinement::refine_and_coarsen_fixed_number</a>(triangulation,</div><div class="line">                                                    localized_all_errors,</div><div class="line">                                                    0.3,</div><div class="line">                                                    0.03);</div><div class="line">    triangulation.execute_coarsening_and_refinement();</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <span class="keyword">const</span> Vector&lt;double&gt; localized_solution(solution);</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0)</div><div class="line">      {</div><div class="line">        std::ofstream output(<span class="stringliteral">&quot;solution-&quot;</span> + <a class="code" href="namespacePatterns_1_1Tools.html#a72743302dcb1a0fb1f2f8dc5122d299e">std::to_string</a>(cycle) + <span class="stringliteral">&quot;.vtk&quot;</span>);</div><div class="line"></div><div class="line">        <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">        data_out.<a class="code" href="classDataOut__DoFData.html#a6ed7c846331069f406b8c9933c37fda4">attach_dof_handler</a>(dof_handler);</div><div class="line"></div><div class="line">        std::vector&lt;std::string&gt; solution_names;</div><div class="line">        <span class="keywordflow">switch</span> (dim)</div><div class="line">          {</div><div class="line">            <span class="keywordflow">case</span> 1:</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;displacement&quot;</span>);</div><div class="line">              <span class="keywordflow">break</span>;</div><div class="line">            <span class="keywordflow">case</span> 2:</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">              <span class="keywordflow">break</span>;</div><div class="line">            <span class="keywordflow">case</span> 3:</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">              solution_names.emplace_back(<span class="stringliteral">&quot;z_displacement&quot;</span>);</div><div class="line">              <span class="keywordflow">break</span>;</div><div class="line">            <span class="keywordflow">default</span>:</div><div class="line">              <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a>(<span class="keyword">false</span>, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line">          }</div><div class="line"></div><div class="line">        data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(localized_solution, solution_names);</div><div class="line"></div><div class="line">        std::vector&lt;unsigned int&gt; partition_int(triangulation.n_active_cells());</div><div class="line">        <a class="code" href="namespaceGridTools.html#ac41d959ae1723a898b616c3320241ffe">GridTools::get_subdomain_association</a>(triangulation, partition_int);</div><div class="line"></div><div class="line">        <span class="keyword">const</span> Vector&lt;double&gt; partitioning(partition_int.begin(),</div><div class="line">                                          partition_int.end());</div><div class="line"></div><div class="line">        data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(partitioning, <span class="stringliteral">&quot;partitioning&quot;</span>);</div><div class="line"></div><div class="line">        data_out.<a class="code" href="classDataOut.html#a087f63e22f0614bca326dbdca288c646">build_patches</a>();</div><div class="line">        data_out.<a class="code" href="classDataOutInterface.html#acad99726038e4fca7f605fdffb3317e4">write_vtk</a>(output);</div><div class="line">      }</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">ElasticProblem&lt;dim&gt;::run</a>()</div><div class="line">  {</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle = 0; cycle &lt; 10; ++cycle)</div><div class="line">      {</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">          {</div><div class="line">            <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a>(triangulation, -1, 1);</div><div class="line">            triangulation.refine_global(3);</div><div class="line">          }</div><div class="line">        <span class="keywordflow">else</span></div><div class="line">          refine_grid();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">              &lt;&lt; triangulation.n_active_cells() &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        setup_system();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span> &lt;&lt; dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>()</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot; (by partition:&quot;</span>;</div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p = 0; p &lt; <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>; ++p)</div><div class="line">          pcout &lt;&lt; (p == 0 ? <span class="charliteral">&#39; &#39;</span> : <span class="charliteral">&#39;+&#39;</span>)</div><div class="line">                &lt;&lt; (<a class="code" href="namespaceDoFTools.html#ac704c6d311cd0f289d625427e03708ac">DoFTools::count_dofs_with_subdomain_association</a>(dof_handler,</div><div class="line">                                                                    p));</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;)&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        assemble_system();</div><div class="line">        <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_iterations = solve();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Solver converged in &quot;</span> &lt;&lt; n_iterations &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        output_results(cycle);</div><div class="line">      }</div><div class="line">  }</div><div class="line">} <span class="comment">// namespace Step17</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> **argv)</div><div class="line">{</div><div class="line">  <span class="keywordflow">try</span></div><div class="line">    {</div><div class="line">      <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">      <span class="keyword">using namespace </span>Step17;</div><div class="line"></div><div class="line">      <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">      ElasticProblem&lt;2&gt; elastic_problem;</div><div class="line">      elastic_problem.run();</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (...)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --> </div></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.13-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
