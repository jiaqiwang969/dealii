<!-- HTML header for doxygen 1.8.13-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="canonical" href="https://www.dealii.org/current/doxygen/deal.II/group__distributed.html" />
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>The deal.II Library: Parallel computing with multiple processors using</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link rel="SHORTCUT ICON" href="deal.ico"></link>
<script type="text/javascript" src="custom.js"></script>
<meta name="author" content="The deal.II Authors <authors@dealii.org>"></meta>
<meta name="copyright" content="Copyright (C) 1998 - 2021 by the deal.II authors"></meta>
<meta name="deal.II-version" content="10.0.0-pre"></meta>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="logo200.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">
   &#160;<span id="projectnumber">Reference documentation for deal.II version 10.0.0-pre</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!--Extra macros for MathJax:-->
<div style="display:none">
\(\newcommand{\dealvcentcolon}{\mathrel{\mathop{:}}}\)
\(\newcommand{\dealcoloneq}{\dealvcentcolon\mathrel{\mkern-1.2mu}=}\)
\(\newcommand{\jump}[1]{\left[\!\left[ #1 \right]\!\right]}\)
\(\newcommand{\average}[1]{\left\{\!\left\{ #1 \right\}\!\right\}}\)
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#namespaces">Namespaces</a> &#124;
<a href="#nested-classes">Classes</a>  </div>
  <div class="headertitle">
<div class="title">Parallel computing with multiple processors using<div class="ingroups"><a class="el" href="group__Parallel.html">Parallel computing</a></div></div>  </div>
</div><!--header-->
<div class="contents">

<p>A module discussing the use of parallelism on distributed memory 集群。  
<a href="#details">More...</a></p>
<div class="dynheader">
Collaboration diagram for Parallel computing with multiple processors using:</div>
<div class="dyncontent">
<center><table><tr><td><div class="center"><iframe scrolling="no" frameborder="0" src="group__distributed.svg" width="427" height="52"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe>
</div>
</td></tr></table></center>
</div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="namespaces"></a>
Namespaces</h2></td></tr>
<tr class="memitem:namespaceparallel_1_1distributed"><td class="memItemLeft" align="right" valign="top"> &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel_1_1distributed.html">parallel::distributed</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallel_1_1distributed_1_1CellDataTransfer.html">parallel::distributed::CellDataTransfer&lt; dim, spacedim, VectorType &gt;</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallel_1_1CellWeights.html">parallel::CellWeights&lt; dim, spacedim &gt;</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallel_1_1shared_1_1Triangulation.html">parallel::shared::Triangulation&lt; dim, spacedim &gt;</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallel_1_1distributed_1_1SolutionTransfer.html">parallel::distributed::SolutionTransfer&lt; dim, VectorType, spacedim &gt;</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation&lt; dim, spacedim &gt;</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p>A module discussing the use of parallelism on distributed memory 集群。 </p>
<p>A module discussing the use of parallelism on distributed memory clusters.</p>
<p>distributed memory</p>
<dl class="section note"><dt>Note</dt><dd>The material presented here is also discussed in <a href="http://www.math.colostate.edu/~bangerth/videos.676.39.html">video lecture 39</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.html">video lecture 41</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.25.html">video lecture 41.25</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.5.html">video lecture 41.5</a>. (All video lectures are also available <a href="http://www.math.colostate.edu/~bangerth/videos.html">here</a>.) <h3>Overview</h3>
</dd></dl>
<p>deal.II除了在 <a class="el" href="group__threads.html">Parallel computing with multiple processors accessing</a> 模块中讨论的共享内存机器内的并行化之外，还可以使用通过MPI连接的多台机器来并行化计算。基本上有两种方法可以利用多部机器。</p>
<ul>
<li>每台机器都在本地保存整个网格和DoF处理程序，但每台机器上只保存全局矩阵、稀疏模式和解向量的一部分。</li>
<li>网格和自由度处理程序也是分布式的，也就是说，每个处理器只存储一部分单元和自由度。没有一个处理器知道整个网格、矩阵或解决方案，事实上，在这种模式下解决的问题通常非常大（比如，一亿到几十亿自由度），没有一个处理器可以或应该存储哪怕一个解决方案矢量。 这两个选项中的第一个相对简单，因为人们在有限元程序中想做的大部分事情仍然以基本相同的方式工作，而且处理分布式矩阵、向量和线性求解器是很好的外部库，如Trilinos或PETSc，可以使事情看起来与在本地提供的一切几乎完全一样。这种并行化模式的使用在教程程序 <a class="el" href="step_17.html">step-17</a> 和 <a class="el" href="step_18.html">step-18</a> 中作了解释，这里不再详细讨论。 真正的分布式网格的使用要复杂得多，因为它改变了一些可以用deal.II三角计算、DoF处理程序等完成的事情，或者使之成为不可能。本模块以离地面50000英尺的有利位置记录了这些问题，而不涉及太多的细节。下面描述的所有算法都在命名空间 <a class="el" href="namespaceparallel_1_1distributed.html">parallel::distributed</a>. 的类和函数中实现。 在使用MPI的并行计算中，一个重要的方面是，对矩阵和向量元素的写访问需要在操作结束后和使用对象前（例如从读取）调用compress（）。也请参见 <a class="el" href="DEALGlossary.html#GlossCompress">GlossCompress</a> 。 <h4>Other resources</h4>
</li>
</ul>
<p>本命名空间中使用的算法的完整讨论，以及这里使用的许多术语的彻底描述，可以在 <a class="el" href="DEALGlossary.html#distributed_paper">分布式计算论文 </a> 中找到。特别是，该论文表明，本模块讨论的方法可以扩展到数千个处理器和远远超过10亿个自由度。这篇论文还给出了许多术语的简明定义，这些术语在这里和图书馆的其他地方使用，与分布式计算有关。 <a class="el" href="step_40.html">step-40</a> 教程程序展示了该命名空间的类和方法在拉普拉斯方程上的应用，而 <a class="el" href="step_55.html">step-55</a> 则是针对一个矢量值问题。 <a class="el" href="step_32.html">step-32</a> 将 <a class="el" href="step_31.html">step-31</a> 程序扩展到大规模并行计算，从而解释了这里讨论的主题在更复杂应用中的使用。</p>
<h4>Distributed triangulations</h4>
<table align="center">
<tr>
<td><div class="image">
<img src="distributed_mesh_0.png" alt="distributed_mesh_0.png"/>
</div>
 </td><td><div class="image">
<img src="distributed_mesh_1.png" alt="distributed_mesh_1.png"/>
</div>
  </td></tr>
<tr>
<td><div class="image">
<img src="distributed_mesh_2.png" alt="distributed_mesh_2.png"/>
</div>
 </td><td><div class="image">
<img src="distributed_mesh_3.png" alt="distributed_mesh_3.png"/>
</div>
  </td></tr>
</table>
<p>单元的颜色是基于 <a class="el" href="DEALGlossary.html#GlossSubdomainId">GlossSubdomainId</a> 的 "子域ID" ，它标识了哪个处理器拥有一个单元：绿松石色代表处理器0，绿色代表处理器1，黄色代表处理器2，红色代表处理器3。可以看出，每个进程在自己的单元格周围都有一层幽灵单元格，这些单元格被正确地用子域ID着色，子域ID标识了拥有这些单元格的处理器。还要注意每个处理器是如何存储一些人工单元的，用蓝色表示，它们的存在只是为了确保每个处理器知道所有的粗网格单元，并且网格具有2:1的细化属性；然而，在这些人工单元所占据的区域，处理器不知道那里的网格到底有多细，因为这些区域是由其他处理器所拥有的。因此，我们将开发的所有算法只能在本地拥有的单元上运行，如果有必要，也可以在幽灵单元上运行；试图访问任何人工单元上的数据很可能是一个错误。注意，我们可以通过测试<code>cell- &gt;<a class="el" href="namespacetypes.html#a3d8ea8a4c6a58127f8c2fcc9ec74af6e">subdomain_id()</a> == triangulation.local_owned_subdomain()</code>来确定我们是否拥有一个单元。 这里需要考虑的 "真正的
"网格是由每个进程所拥有的单元组成的联合体，即由绿松石、绿色、黄色和红色区域的重叠所产生的网格，不考虑蓝色区域。</p>
<dl class="section note"><dt>Note</dt><dd>这个 "真实
"的网格被分解成由每个进程存储的碎片，由<a href="http://www.p4est.org">p4est</a>库提供。 p4est将完整的网格存储在一个叫做平行森林的分布式数据结构中（因此得名）。平行森林由四叉树（2D）或八叉树（3D）组成，这些树起源于每个粗略的网格单元，代表了从父单元到其四个（2D）或八个（3D）子单元的细化结构。在内部，这个平行森林由一个单元的（分布式）线性阵列表示，对应于每个树的深度优先遍历，然后每个进程存储这个单元的线性阵列的一个连续部分。这就导致了如上图所示的分区，从这个意义上说，它们不是最佳的，因为它们不能使子域之间的接口长度最小化（因此也不能使通信量最小化），但在实践中却非常好，可以用超快的算法进行操作。 因此，以这种方式存储和操作单元的效率往往超过了通信的优化损失。 这种划分方法产生的各个子域有时也可能由不相连的部分组成，如右上图所示）。然而，可以证明每个子域最多包括两个不相连的部分；见C. Burstedde, J. Holke, T. Isaac: "Bounds on the number of discontinuities of
Morton-type space-filling curves", <a href="http://arxiv.org/abs/1505.05055">arXiv 1505.05055</a>, 2017.) <h4>Distributed degree of freedom handler</h4>
</dd></dl>
<p>DoFHandler类建立在Triangulation类的基础上，但它可以检测到每当我们实际使用 <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> 类型的对象作为三角形。在这种情况下，它为存在于全局网格上的所有自由度分配全局数，但每个处理器将只知道那些定义在本地相关单元上的自由度（即本地拥有的单元或者是幽灵单元）。在内部，该算法的工作原理是：循环浏览我们本地拥有的所有单元，并为定义在这些单元上的自由度分配DoF指数，在不同处理器拥有的子域界面上的自由度，不属于邻近的处理器。然后，所有处理器交换他们本地拥有的自由度，并以这样的方式转移他们自己的指数，即所有子域上的每个自由度都由一个介于0和 <a class="el" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">DoFHandler::n_dofs()</a> 之间的指数唯一识别（这个函数返回全局自由度的数量，在所有处理器上累积）。请注意，在这一步之后，每个进程拥有的自由度形成了一个连续的范围，例如，可以通过 <a class="el" href="classDoFHandler.html#ad39fd2189568f2f6b7d557237e3372e3">DoFHandler::locally_owned_dofs()</a>. 返回的连续索引集得到。 在为所有自由度分配了唯一的索引之后， <a class="el" href="classDoFHandler.html#a553ca864aaf70330d9be86bc78f36d1e">DoFHandler::distribute_dofs()</a> 函数就会在所有幽灵单元上循环，并与邻近的处理器进行通信，以确保这些幽灵单元上的自由度的全局索引与邻居分配给它们的索引一致。 通过这个方案，我们可以确保我们本地拥有的每个单元以及所有的幽灵单元都可以被要求产生定义在它们身上的自由度的全局正确指数。然而，要求人造单元上的自由度很可能不会有什么好结果，因为这些单元没有任何信息（事实上，甚至不知道这些单元是否在全局网格上是活跃的，或被进一步细化）。 像往常一样，自由度在被列举后可以被重新编号，使用命名空间DoFRenumbering中的函数。</p>
<h4>Linear systems for distributed computations</h4>
<p>在处理非常多的处理器时，人们很快就会了解到一件事，那就是不能在每个处理器上存储每个自由度的信息，即使这些信息是 "这个自由度不在这里"。这方面的一个例子是，我们可以为一个有 <a class="el" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">DoFHandler::n_dofs()</a> 行的（压缩）稀疏模式创建一个对象，但我们只填充那些对应于 <a class="el" href="classDoFHandler.html#af16dc39b7ff25aaf361bc6ab440aeee7">DoFHandler::n_locally_owned_dofs()</a> 本地拥有的自由度的行。原因很简单：为了举例，我们假设我们有10亿个自由度，分布在100个处理器上；如果我们甚至在这个稀疏模式中每行只持有16个字节（无论我们是否拥有相应的自由度），即使每一行都是空的，我们也需要16GB的对象。当然，只有1000万行是不空的，为此我们需要160MB，再加上存储非零条目的实际列索引所需的东西。假设我们有一个中等复杂的问题，每行有50个条目，我们为每个条目存储价值4个字节的列索引，那么我们需要为1000万行中的每一行存储216个字节，以对应我们拥有的自由度，总共2.16GB。而我们不拥有的9.9亿行，每行需要16字节，共计15.840GB。很明显，如果我们使用更多的处理器，这个比例也不会变得更好。 解决这个问题的方法是只对线性系统中我们拥有的部分使用任何内存，或者出于其他原因需要。对于所有其他部分，我们必须知道它们的存在，但我们不能设置我们数据结构的任何部分。为此，存在一个叫做IndexSet的类，它表示我们所关心的一组索引，我们可能要为其分配内存。稀疏模式、约束矩阵、矩阵和向量的数据结构可以用这些IndexSet对象进行初始化，以真正只关心那些与索引集中的索引相对应的行或条目，而不关心所有其他的索引。然后这些对象会询问集合中存在多少个索引，为每个索引分配内存（例如初始化稀疏模式的一行的数据结构），当你想访问全局自由度 <code>i</code> 的数据时，你会被重定向到用索引 <code>i</code> 调用 <a class="el" href="classIndexSet.html#a4d924bea58d98feebf99fc714b14b7d0">IndexSet::index_within_set()</a> 的结果，而不是。访问 <a class="el" href="classIndexSet.html#a66c79fc7f17b2eeff0f0fb757e77e0c3">IndexSet::is_element()</a> 为假的元素 <code>i</code> 的数据将产生一个错误。 剩下的问题是如何确定与我们在每个处理器上需要担心的自由度相对应的指数集。为此，你可以使用 <a class="el" href="classDoFHandler.html#ad39fd2189568f2f6b7d557237e3372e3">DoFHandler::locally_owned_dofs()</a> 函数来获取一个处理器拥有的所有指数。注意，这是定义在本地拥有的单元上的自由度的一个子集（因为两个不同子域之间的界面上的一些自由度可能被邻居拥有）。这个定义在我们拥有的单元上的自由度集合可以通过函数 <a class="el" href="namespaceDoFTools.html#a5f745d25d2397a91dc65b9158b8d77a6">DoFTools::extract_locally_active_dofs()</a>. 得到。 最后，有时我们需要本地拥有的子域以及相邻的幽灵单元上所有自由度的集合。这个信息由 <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> 函数提供。</p>
<h5>Vectors with ghost elements</h5>
<p>一个典型的并行应用要处理两种不同类型的并行向量：带有鬼魂元素的向量（也叫鬼魂向量）和没有鬼魂元素的向量。 这两种类型通常可以由同一数据类型表示，但当然也有不同的向量类型可以分别表示这两种类型：例如 <a class="el" href="classTrilinosWrappers_1_1MPI_1_1Vector.html">TrilinosWrappers::MPI::Vector</a>, <a class="el" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a>, 和建立在这些之上的BlockVector对象）。你可以在 <a class="el" href="DEALGlossary.html#GlossGhostedVector">关于重影向量的词汇表条目 </a> 中找到关于区分这些类型的向量的讨论。 另一方面，没有鬼魂项的向量在其他所有地方都可以使用，如组装、求解或任何其他形式的操作。这些通常是只写的操作，因此不需要对可能被另一个处理器拥有的向量元素进行读取访问。 你可以使用operator=在有鬼魂元素和无鬼魂元素的向量之间进行复制（你可以在 <a class="el" href="step_40.html">step-40</a> , <a class="el" href="step_55.html">step-55</a> , 和 <a class="el" href="step_32.html">step-32</a> 中看到）。</p>
<h5>Sparsity patterns</h5>
<p>在写这篇文章的时候，唯一能够处理刚才解释的情况的类是DynamicSparsityPattern。该函数的一个版本 <a class="el" href="classDynamicSparsityPattern.html#aa32f9f3ebad084d001349cd3ddb4074e">DynamicSparsityPattern::reinit()</a> 存在，它接受一个IndexSet参数，指示要为疏散模式的哪些行分配内存。换句话说，创建这样一个对象是安全的，它将报告其大小为10亿，但实际上只存储了索引集有多少个元素的行。然后你可以使用通常的函数 <a class="el" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a> 来建立疏散模式，该模式是在网格的本地拥有的部分进行装配而产生的。产生的对象可以用来初始化PETSc或Trilinos矩阵，这些矩阵通过完全分布式存储支持非常大的对象尺寸。然后，该矩阵可以通过只在当前处理器所拥有的单元上进行循环来进行组装。 唯一需要注意的是，稀疏性需要存储哪些自由度的条目。从本质上讲，这些是我们在组装时可能在矩阵中存储的值。很明显，这些肯定是本地活动的自由度（它们生活在我们本地拥有的单元上），但是通过约束，也有可能写到位于幽灵单元上的条目。因此，你需要在初始化稀疏模式时传递来自 <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> 的索引集。</p>
<h4>Constraints on degrees of freedom</h4>
<p>在创建稀疏模式以及组装线性系统时，我们需要知道自由度的约束，例如由悬挂节点或边界条件导致的约束。像动态稀疏模式类一样，AffineConstraints容器在构造时也可以接受一个IndexSet，这个IndexSet指示在可能非常多的自由度中它应该实际存储哪些约束。与稀疏模式不同的是，这些自由度现在只是我们在组装时本地处理的自由度，即那些由 <a class="el" href="namespaceDoFTools.html#a5f745d25d2397a91dc65b9158b8d77a6">DoFTools::extract_locally_active_dofs()</a> 返回的自由度（本地拥有的自由度的超集）。 然而，在有些情况下，更复杂的约束会出现在有限元程序中。一个例子是在 \(hp\) 的适应性计算中，自由度可以针对其他自由度进行约束，而这些自由度本身也被约束。在这样的情况下，为了完全解决这个约束链，只存储局部活动自由度的约束可能是不够的，可能还需要有局部相关自由度的约束可用。在这种情况下，AffineConstraints对象需要用由 <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> 产生的IndexSet进行初始化。 一般来说，如果你碰巧没有在每个处理器上存储所有必要的约束条件，你的程序将继续做一些事情：你将只是生成错误的矩阵条目，但程序不会中止。这与稀疏模式的情况相反：在那里，如果传递给DynamicSparsityPattern的IndexSet表明它应该存储太少的矩阵行，那么当你试图向不存在的矩阵条目写入时，程序会中止，或者矩阵类会默默地分配更多的内存来容纳它们。因此，在指明存储哪些约束条件时，谨慎行事是很有用的，使用 <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> 的结果而不是 <a class="el" href="namespaceDoFTools.html#a5f745d25d2397a91dc65b9158b8d77a6">DoFTools::extract_locally_active_dofs()</a> 。这也是可以承受的，因为局部相关自由度的集合只比局部活动自由度的集合稍大一些。我们在 <a class="el" href="step_32.html">step-32</a> 、 <a class="el" href="step_40.html">step-40</a> 和 <a class="el" href="step_55.html">step-55</a> 中选择了这种策略。</p>
<h4>Postprocessing</h4>
<p>和其他一切一样，你只能对本地处理器拥有的单元格进行后处理。DataOut和KellyErrorEstimator类自动做到了这一点：它们只对本地拥有的单元格进行操作，不需要做任何特别的事情。至少对于大型计算来说，也没有办法在一台机器上合并所有这些本地计算的结果，也就是说，每个处理器必须是自给的。例如，每个处理器必须生成自己的并行输出文件，而这些文件必须由一个能够处理多个输入文件的程序进行可视化处理，而不是在生成一个单一的输出文件之前将调用DataOut的结果合并到一个处理器。后者可以实现，例如，使用 <a class="el" href="namespaceDataOutBase.html#a3aec479936b78bd0ec2ecc3674e24584">DataOutBase::write_vtu()</a> 和 <a class="el" href="namespaceDataOutBase.html#ac8c87832129884d0603e3ab9ae132741">DataOutBase::write_pvtu_record()</a> 函数。 这些考虑同样适用于所有其他的后处理动作：例如，虽然有可能通过在本地进行计算并将产生的单个数字处理器累积为整个通信的单个数字来计算全局能量耗散率，但如果每个处理器产生的数据量很大，一般来说是不可能做到这一点。 然而，对于后处理有一个特别的考虑：无论你在一个处理器拥有的每个单元上做什么，你至少需要访问在这些单元上活跃的所有那些解向量的值（即访问所有<em>locally active degrees of freedom</em>的集合，用 <a class="el" href="DEALGlossary.html#distributed_paper">distributed_paper</a> 《分布式计算论文》的语言），这是这个处理器实际拥有的自由度的超集（因为它可能不拥有自己的单元和其他处理器拥有的那些单元之间界面上的所有自由度）。然而，有时你需要更多的信息：例如，为了计算KellyErrorIndicator的结果，我们需要评估当前和邻近单元的界面上的梯度；后者可能为其他处理器所拥有，所以我们也需要这些自由度。因此，一般来说，人们需要获得所有自由度为<em>locally relevant</em>的解值。另一方面，我们可以用于并行线性代数的两个包（PETSc和Trilinos）以及 parallel::distributed::Vector 都将向量细分为每个处理器拥有的块和存储在其他处理器的块。因此，要对东西进行后处理意味着我们必须告诉PETSc或Trilinos，它也应该导入<em>ghost elements</em>，即除了我们本地拥有的向量之外，还应该导入解向量的其他向量元素。对于重影向量，这可以通过使用以分布式向量为参数的operator=来实现。</p>
<dl class="section note"><dt>Note</dt><dd>The material presented here is also discussed in <a href="http://www.math.colostate.edu/~bangerth/videos.676.39.html">video lecture 39</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.html">video lecture 41</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.25.html">video lecture 41.25</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.5.html">video lecture 41.5</a>. (All video lectures are also available <a href="http://www.math.colostate.edu/~bangerth/videos.html">here</a>.)</dd></dl>
<h3>Overview</h3>
<p>deal.II can use multiple machines connected via MPI to parallelize computations, in addition to the parallelization within a shared memory machine discussed in the <a class="el" href="group__threads.html">Parallel computing with multiple processors accessing</a> module. There are essentially two ways to utilize multiple machines:</p>
<ul>
<li>Each machine keeps the entire mesh and DoF handler locally, but only a share of the global matrix, sparsity pattern, and solution vector is stored on each machine.</li>
<li>The mesh and DoF handler are also distributed, i.e. each processor stores only a share of the cells and degrees of freedom. No processor has knowledge of the entire mesh, matrix, or solution, and in fact problems solved in this mode are usually so large (say, 100s of millions to billions of degrees of freedom) that no processor can or should store even a single solution vector.</li>
</ul>
<p>The first of these two options is relatively straightforward because most of the things one wants to do in a finite element program still work in essentially the same way, and handling distributed matrices, vectors, and linear solvers is something for which good external libraries such as Trilinos or PETSc exist that can make things look almost exactly the same as they would if everything was available locally. The use of this mode of parallelization is explained in the tutorial programs <a class="el" href="step_17.html">step-17</a>, and <a class="el" href="step_18.html">step-18</a> and will not be discussed here in more detail.</p>
<p>The use of truly distributed meshes is somewhat more complex because it changes or makes impossible some of the things that can otherwise be done with deal.II triangulations, DoF handlers, etc. This module documents these issues with a vantage point at 50,000 ft above ground without going into too many details. All the algorithms described below are implement in classes and functions in namespace <a class="el" href="namespaceparallel_1_1distributed.html">parallel::distributed</a>.</p>
<p>One important aspect in parallel computations using MPI is that write access to matrix and vector elements requires a call to <a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a> after the operation is finished and before the object is used (for example read from). Also see <a class="el" href="DEALGlossary.html#GlossCompress">GlossCompress</a>.</p>
<h4>Other resources</h4>
<p>A complete discussion of the algorithms used in this namespace, as well as a thorough description of many of the terms used here, can be found in the <a class="el" href="DEALGlossary.html#distributed_paper">Distributed Computing paper</a>. In particular, the paper shows that the methods discussed in this module scale to thousands of processors and well over a billion degrees of freedom. The paper also gives a concise definition of many of the terms that are used here and in other places of the library related to distributed computing. The <a class="el" href="step_40.html">step-40</a> tutorial program shows an application of the classes and methods of this namespace to the Laplace equation, while <a class="el" href="step_55.html">step-55</a> does so for a vector-valued problem. <a class="el" href="step_32.html">step-32</a> extends the <a class="el" href="step_31.html">step-31</a> program to massively parallel computations and thereby explains the use of the topic discussed here to more complicated applications.</p>
<p>For a discussion of what we consider "scalable" programs, see <a class="el" href="DEALGlossary.html#GlossParallelScaling">this glossary entry</a>.</p>
<h4>Distributed triangulations</h4>
<p>In parallel distributed mode, objects of type <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> on each processor only store a subset of cells. In particular, the global mesh can be thought of as decomposed so that each MPI process "owns" a number of cells. The mesh each process then stores locally consists of exactly those cells that it owns, as well as one layer of <a class="el" href="DEALGlossary.html#GlossGhostCell">ghost cells</a> around the ones it locally owns, and a number of cells we call <a class="el" href="DEALGlossary.html#GlossArtificialCell">artificial</a>. The latter are cells that ensure that each processor has a mesh that has all the coarse level cells and that respects the invariant that neighboring cells can not differ by more than one level of refinement. The following pictures show such a mesh, distributed across four processors, and the collection of cells each of these processors stores locally:</p>
<table align="center">
<tr>
<td><div class="image">
<img src="distributed_mesh_0.png" alt="distributed_mesh_0.png"/>
</div>
 </td><td><div class="image">
<img src="distributed_mesh_1.png" alt="distributed_mesh_1.png"/>
</div>
  </td></tr>
<tr>
<td><div class="image">
<img src="distributed_mesh_2.png" alt="distributed_mesh_2.png"/>
</div>
 </td><td><div class="image">
<img src="distributed_mesh_3.png" alt="distributed_mesh_3.png"/>
</div>
  </td></tr>
</table>
<p>The cells are colored based on the <a class="el" href="DEALGlossary.html#GlossSubdomainId">subdomain id</a>, which identifies which processor owns a cell: turquoise for processor 0, green for processor 1, yellow for processor 2, and red for processor 3. As can be seen, each process has one layer of ghost cells around its own cells, which are correctly colored by the subdomain id that identifies the processor that owns each of these cells. Note also how each processor stores a number of artificial cells, indicated in blue, that only exist to ensure that each processor knows about all coarse grid cells and that the meshes have the 2:1 refinement property; however, in the area occupied by these artificial cells, a processor has no knowledge how refined the mesh there really is, as these are areas that are owned by other processors. As a consequence, all algorithms we will develop can only run over the locally owned cells and if necessary the ghost cells; trying to access data on any of the artificial cells is most likely an error. Note that we can determine whether we own a cell by testing that <code>cell-&gt;<a class="el" href="namespacetypes.html#a3d8ea8a4c6a58127f8c2fcc9ec74af6e">subdomain_id()</a> == triangulation.locally_owned_subdomain()</code>.</p>
<p>The "real" mesh one has to think of here is the one that would result from forming the union of cells each of the processes own, i.e. from the overlap of the turquoise, green, yellow and red areas, disregarding the blue areas.</p>
<dl class="section note"><dt>Note</dt><dd>The decomposition of this "real" mesh into the pieces stored by each processes is provided by the <a href="http://www.p4est.org">p4est</a> library. p4est stores the complete mesh in a distributed data structure called a parallel forest (thus the name). A parallel forest consists of quad-trees (in 2d) or oct-trees (in 3d) originating in each coarse mesh cell and representing the refinement structure from parent cells to their four (in 2d) or eight (in 3d) children. Internally, this parallel forest is represented by a (distributed) linear array of cells that corresponds to a depth-first traverse of each tree, and each process then stores a contiguous section of this linear array of cells. This results in partitions such as the one shown above that are not optimal in the sense that they do not minimize the length of the interface between subdomains (and consequently do not minimize the amount of communication) but that in practice are very good and can be manipulated with exceedingly fast algorithms. The efficiency of storing and manipulating cells in this way therefore often outweighs the loss in optimality of communication. (The individual subdomains resulting from this method of partitioning may also sometimes consist of disconnected parts, such as shown at the top right. However, it can be proven that each subdomain consists of at most two disconnected pieces; see C. Burstedde, J. Holke, T. Isaac: "Bounds on the number of
  discontinuities of Morton-type space-filling curves", <a href="http://arxiv.org/abs/1505.05055">arXiv 1505.05055</a>, 2017.)</dd></dl>
<h4>Distributed degree of freedom handler</h4>
<p>The <a class="el" href="classDoFHandler.html">DoFHandler</a> class builds on the <a class="el" href="classTriangulation.html">Triangulation</a> class, but it can detect whenever we actually use an object of type <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> as triangulation. In that case, it assigns global numbers for all degrees of freedom that exist, given a finite element, on the global mesh, but each processor will only know about those that are defined on locally relevant cells (i.e. cells either locally owned or that are ghost cells). Internally, the algorithm essentially works by just looping over all cells we own locally and assigning DoF indices to the degrees of freedom defined on them and, in the case of degrees of freedom at the interface between subdomains owned by different processors, that are not owned by the neighboring processor. All processors then exchange how many degrees of freedom they locally own and shift their own indices in such a way that every degree of freedom on all subdomains are uniquely identified by an index between zero and <a class="el" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">DoFHandler::n_dofs()</a> (this function returns the global number of degrees of freedom, accumulated over all processors). Note that after this step, the degrees of freedom owned by each process form a contiguous range that can, for example, be obtained by the contiguous index set returned by <a class="el" href="classDoFHandler.html#ad39fd2189568f2f6b7d557237e3372e3">DoFHandler::locally_owned_dofs()</a>. After assigning unique indices to all degrees of freedom, the <a class="el" href="classDoFHandler.html#a553ca864aaf70330d9be86bc78f36d1e">DoFHandler::distribute_dofs()</a> function then loops over all ghost cells and communicates with neighboring processors to ensure that the global indices of degrees of freedom on these ghost cells match the ones that the neighbor has assigned to them.</p>
<p>Through this scheme, we can make sure that each cell we locally own as well as all the ghost cells can be asked to yield the globally correct indices for the degrees of freedom defined on them. However, asking for degrees of freedom on artificial cells is likely going to lead to nothing good, as no information is available for these cells (in fact, it isn't even known whether these cells are active on the global mesh, or are further refined).</p>
<p>As usual, degrees of freedom can be renumbered after being enumerated, using the functions in namespace <a class="el" href="namespaceDoFRenumbering.html">DoFRenumbering</a>.</p>
<h4>Linear systems for distributed computations</h4>
<p>One thing one learns very quickly when working with very large numbers of processors is that one can not store information about every degree of freedom on each processor, even if this information is "this degree of freedom doesn't live here". An example for this is that we can create an object for a (compressed) sparsity pattern that has <a class="el" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">DoFHandler::n_dofs()</a> rows, but for which we fill only those rows that correspond to the <a class="el" href="classDoFHandler.html#af16dc39b7ff25aaf361bc6ab440aeee7">DoFHandler::n_locally_owned_dofs()</a> locally owned degrees of freedom. The reason is simple: for the sake of example, let's assume we have 1 billion degrees of freedom distributed across 100 processors; if we even only hold 16 bytes per line in this sparsity pattern (whether we own the corresponding DoF or not), we'll need 16 GB for this object even if every single line is empty. Of course, only 10 million lines will be non-empty, for which we need 160 MB plus whatever is necessary to store the actual column indices of nonzero entries. Let's say we have a moderately complex problem with 50 entries per row, for each of which we store the column index worth 4 bytes, then we'll need 216 bytes for each of the 10 million lines that correspond to the degrees of freedom we own, for a total of 2.16 GB. And we'll need 16 bytes for each of the 990 million lines that we don't own, for a total of 15.840 GB. It is clear that this ratio doesn't become any better if we go to even higher numbers of processors.</p>
<p>The solution to this problem is to really only use any memory at all for those parts of the linear system that we own, or need for some other reason. For all other parts, we must know that they exist, but we can not set up any part of our data structure. To this end, there exists a class called <a class="el" href="classIndexSet.html">IndexSet</a> that denotes a set of indices which we care for, and for which we may have to allocate memory. The data structures for sparsity patterns, constraint matrices, matrices and vector can be initialized with these <a class="el" href="classIndexSet.html">IndexSet</a> objects to really only care for those rows or entries that correspond to indices in the index set, and not care about all others. These objects will then ask how many indices exist in the set, allocate memory for each one of them (e.g. initialize the data structures for a line of a sparsity pattern), and when you want to access data for global degree of freedom <code>i</code> you will be redirected to the result of calling <a class="el" href="classIndexSet.html#a4d924bea58d98feebf99fc714b14b7d0">IndexSet::index_within_set()</a> with index <code>i</code> instead. Accessing data for elements <code>i</code> for which <a class="el" href="classIndexSet.html#a66c79fc7f17b2eeff0f0fb757e77e0c3">IndexSet::is_element()</a> is false will yield an error.</p>
<p>The remaining question is how to identify the set of indices that correspond to degrees of freedom we need to worry about on each processor. To this end, you can use the <a class="el" href="classDoFHandler.html#ad39fd2189568f2f6b7d557237e3372e3">DoFHandler::locally_owned_dofs()</a> function to get at all the indices a processor owns. Note that this is a subset of the degrees of freedom that are defined on the locally owned cells (since some of the degrees of freedom at the interface between two different subdomains may be owned by the neighbor). This set of degrees of freedom defined on cells we own can be obtained using the function <a class="el" href="namespaceDoFTools.html#a5f745d25d2397a91dc65b9158b8d77a6">DoFTools::extract_locally_active_dofs()</a>. Finally, one sometimes needs the set of all degrees of freedom on the locally owned subdomain as well as the adjacent ghost cells. This information is provided by the <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> function.</p>
<h5>Vectors with ghost elements</h5>
<p>A typical parallel application is dealing with two different kinds of parallel vectors: vectors with ghost elements (also called ghosted vectors) and vectors without ghost elements. (Both kinds can typically be represented by the same data type, but there are of course different vector types that can each represent both flavors: for example <a class="el" href="classTrilinosWrappers_1_1MPI_1_1Vector.html">TrilinosWrappers::MPI::Vector</a>, <a class="el" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a>, and <a class="el" href="classBlockVector.html">BlockVector</a> objects built on these). You can find a discussion of what distinguishes these kinds of vectors in the <a class="el" href="DEALGlossary.html#GlossGhostedVector">glossary entry on ghosted vectors</a>.</p>
<p>From a usage point of view, ghosted vectors are typically used for data output, postprocessing, error estimation, input in integration. This is because in these operations, one typically needs access not only to <a class="el" href="DEALGlossary.html#GlossLocallyOwnedDof">locally owned dofs</a> but also to <a class="el" href="DEALGlossary.html#GlossLocallyActiveDof">locally active dofs</a> and sometimes to <a class="el" href="DEALGlossary.html#GlossLocallyRelevantDof">locally relevant dofs</a>, and their values may not be stored in non-ghosted vectors on the processor that needs them. The operations listed above also only require read-only access to vectors, and ghosted vectors are therefore usable in these contexts.</p>
<p>On the other hand, vectors without ghost entries are used in all other places like assembling, solving, or any other form of manipulation. These are typically write-only operations and therefore need not have read access to vector elements that may be owned by another processor.</p>
<p>You can copy between vectors with and without ghost elements (you can see this in <a class="el" href="step_40.html">step-40</a>, <a class="el" href="step_55.html">step-55</a>, and <a class="el" href="step_32.html">step-32</a>) using operator=.</p>
<h5>Sparsity patterns</h5>
<p>At the time of writing this, the only class equipped to deal with the situation just explained is <a class="el" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a>. A version of the function <a class="el" href="classDynamicSparsityPattern.html#aa32f9f3ebad084d001349cd3ddb4074e">DynamicSparsityPattern::reinit()</a> exists that takes an <a class="el" href="classIndexSet.html">IndexSet</a> argument that indicates which lines of the sparsity pattern to allocate memory for. In other words, it is safe to create such an object that will report as its size 1 billion, but in fact only stores only as many rows as the index set has elements. You can then use the usual function <a class="el" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a> to build the sparsity pattern that results from assembling on the locally owned portion of the mesh. The resulting object can be used to initialize a PETSc or Trilinos matrix which support very large object sizes through completely distributed storage. The matrix can then be assembled by only looping over those cells owned by the current processor.</p>
<p>The only thing to pay attention to is for which degrees of freedom the sparsity needs to store entries. These are, in essence, the ones we could possibly store values to in the matrix upon assembly. It is clear that these are certainly the locally active degrees of freedom (which live on the cells we locally own) but through constraints, it may also be possible to write to entries that are located on ghost cells. Consequently, you need to pass the index set that results from <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> upon initializing the sparsity pattern.</p>
<h4>Constraints on degrees of freedom</h4>
<p>When creating the sparsity pattern as well as when assembling the linear system, we need to know about constraints on degrees of freedom, for example resulting from hanging nodes or boundary conditions. Like the <a class="el" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> class, the <a class="el" href="classAffineConstraints.html">AffineConstraints</a> container can also take an <a class="el" href="classIndexSet.html">IndexSet</a> upon construction that indicates for which of the possibly very large number of degrees of freedom it should actually store constraints. Unlike for the sparsity pattern, these are now only those degrees of freedom which we work on locally when assembling, namely those returned by <a class="el" href="namespaceDoFTools.html#a5f745d25d2397a91dc65b9158b8d77a6">DoFTools::extract_locally_active_dofs()</a> (a superset of the locally owned ones).</p>
<p>There are, however, situations where more complicated constraints appear in finite element programs. An example is in \(hp\) adaptive computations where degrees of freedom can be constrained against other degrees of freedom that are themselves constrained. In a case like this, in order to fully resolve this chain of constraints, it may not be sufficient to only store constraints on locally active degrees of freedom but one may also need to have constraints available on locally relevant ones. In that case, the <a class="el" href="classAffineConstraints.html">AffineConstraints</a> object needs to be initialized with the <a class="el" href="classIndexSet.html">IndexSet</a> produced by <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> .</p>
<p>In general, your program will continue to do something if you happen to not store all necessary constraints on each processor: you will just generate wrong matrix entries, but the program will not abort. This is opposed to the situation of the sparsity pattern: there, if the <a class="el" href="classIndexSet.html">IndexSet</a> passed to the <a class="el" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> indicates that it should store too few rows of the matrix, the program will either abort when you attempt to write into matrix entries that do not exist or the matrix class will silently allocate more memory to accommodate them. As a consequence, it is useful to err on the side of caution when indicating which constraints to store and use the result of <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> rather than <a class="el" href="namespaceDoFTools.html#a5f745d25d2397a91dc65b9158b8d77a6">DoFTools::extract_locally_active_dofs()</a> . This is also affordable since the set of locally relevant degrees of freedom is only marginally larger than the set of locally active degrees of freedom. We choose this strategy in <a class="el" href="step_32.html">step-32</a>, <a class="el" href="step_40.html">step-40</a>, and <a class="el" href="step_55.html">step-55</a>.</p>
<h4>Postprocessing</h4>
<p>Like everything else, you can only do postprocessing on cells a local processor owns. The <a class="el" href="classDataOut.html">DataOut</a> and <a class="el" href="classKellyErrorEstimator.html">KellyErrorEstimator</a> classes do this automatically: they only operate on locally owned cells without the need to do anything in particular. At least for large computations, there is also no way to merge the results of all these local computations on a single machine, i.e. each processor has to be self-sufficient. For example, each processor has to generate its own parallel output files that have to be visualized by a program that can deal with multiple input files rather than merging the results of calling <a class="el" href="classDataOut.html">DataOut</a> to a single processor before generating a single output file. The latter can be achieved, for example, using the <a class="el" href="namespaceDataOutBase.html#a3aec479936b78bd0ec2ecc3674e24584">DataOutBase::write_vtu()</a> and <a class="el" href="namespaceDataOutBase.html#ac8c87832129884d0603e3ab9ae132741">DataOutBase::write_pvtu_record()</a> functions.</p>
<p>These same considerations hold for all other postprocessing actions as well: while it is, for example, possible to compute a global energy dissipation rate by doing the computations locally and accumulating the resulting single number processor to a single number for the entire communication, it is in general not possible to do the same if the volume of data produced by every processor is significant.</p>
<p>There is one particular consideration for postprocessing, however: whatever you do on each cell a processor owns, you need access to at least all those values of the solution vector that are active on these cells (i.e. to the set of all <em>locally active degrees of freedom</em>, in the language of the <a class="el" href="DEALGlossary.html#distributed_paper">Distributed Computing paper</a>), which is a superset of the degrees of freedom this processor actually owns (because it may not own all degrees of freedom on the interface between its own cells and those cells owned by other processors). Sometimes, however, you need even more information: for example, to compute the KellyErrorIndicator results, one needs to evaluate the gradient at the interface on the current as well as its neighbor cell; the latter may be owned by another processor, so we need those degrees of freedom as well. In general, therefore, one needs access to the solution values for all degrees of freedom that are <em>locally relevant</em>. On the other hand, both of the packages we can use for parallel linear algebra (PETSc and Trilinos) as well as parallel::distributed::Vector subdivide vectors into chunks each processor owns and chunks stored on other processors. To postprocess stuff therefore means that we have to tell PETSc or Trilinos that it should also import <em>ghost elements</em>, i.e. additional vector elements of the solution vector other than the ones we own locally. For ghosted vectors, this can be achieved by using operator= with a distributed vector as argument.</p>
<p>distributed memory</p>
<dl class="section note"><dt>Note</dt><dd>The material presented here is also discussed in <a href="http://www.math.colostate.edu/~bangerth/videos.676.39.html">video lecture 39</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.html">video lecture 41</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.25.html">video lecture 41.25</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.5.html">video lecture 41.5</a>. (All video lectures are also available <a href="http://www.math.colostate.edu/~bangerth/videos.html">here</a>.) <h3>Overview</h3>
</dd></dl>
<p>deal.II can use multiple machines connected via MPI to parallelize computations, in addition to the parallelization within a shared memory machine discussed in the <a class="el" href="group__threads.html">Parallel computing with multiple processors accessing</a> module. There are essentially two ways to utilize multiple machines:</p>
<ul>
<li>Each machine keeps the entire mesh and DoF handler locally, but only a share of the global matrix, sparsity pattern, and solution vector is stored on each machine.</li>
<li>The mesh and DoF handler are also distributed, i.e. each processor stores only a share of the cells and degrees of freedom. No processor has knowledge of the entire mesh, matrix, or solution, and in fact problems solved in this mode are usually so large (say, 100s of millions to billions of degrees of freedom) that no processor can or should store even a single solution vector. The first of these two options is relatively straightforward because most of the things one wants to do in a finite element program still work in essentially the same way, and handling distributed matrices, vectors, and linear solvers is something for which good external libraries such as Trilinos or PETSc exist that can make things look almost exactly the same as they would if everything was available locally. The use of this mode of parallelization is explained in the tutorial programs <a class="el" href="step_17.html">step-17</a> , and <a class="el" href="step_18.html">step-18</a> and will not be discussed here in more detail. The use of truly distributed meshes is somewhat more complex because it changes or makes impossible some of the things that can otherwise be done with deal.II triangulations, DoF handlers, etc. This module documents these issues with a vantage point at 50,000 ft above ground without going into too many details. All the algorithms described below are implement in classes and functions in namespace <a class="el" href="namespaceparallel_1_1distributed.html">parallel::distributed</a>. One important aspect in parallel computations using MPI is that write access to matrix and vector elements requires a call to <a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a> after the operation is finished and before the object is used (for example read from). Also see <a class="el" href="DEALGlossary.html#GlossCompress">GlossCompress</a> . <h4>Other resources</h4>
</li>
</ul>
<p>A complete discussion of the algorithms used in this namespace, as well as a thorough description of many of the terms used here, can be found in the <a class="el" href="DEALGlossary.html#distributed_paper">Distributed Computing paper</a>. In particular, the paper shows that the methods discussed in this module scale to thousands of processors and well over a billion degrees of freedom. The paper also gives a concise definition of many of the terms that are used here and in other places of the library related to distributed computing. The <a class="el" href="step_40.html">step-40</a> tutorial program shows an application of the classes and methods of this namespace to the Laplace equation, while <a class="el" href="step_55.html">step-55</a> does so for a vector-valued problem. <a class="el" href="step_32.html">step-32</a> extends the <a class="el" href="step_31.html">step-31</a> program to massively parallel computations and thereby explains the use of the topic discussed here to more complicated applications.</p>
<h4>Distributed triangulations</h4>
<table align="center">
<tr>
<td><div class="image">
<img src="distributed_mesh_0.png" alt="distributed_mesh_0.png"/>
</div>
 </td><td><div class="image">
<img src="distributed_mesh_1.png" alt="distributed_mesh_1.png"/>
</div>
  </td></tr>
<tr>
<td><div class="image">
<img src="distributed_mesh_2.png" alt="distributed_mesh_2.png"/>
</div>
 </td><td><div class="image">
<img src="distributed_mesh_3.png" alt="distributed_mesh_3.png"/>
</div>
  </td></tr>
</table>
<p>The cells are colored based on the <a class="el" href="DEALGlossary.html#GlossSubdomainId">subdomain id</a>, which identifies which processor owns a cell: turquoise for processor 0, green for processor 1, yellow for processor 2, and red for processor 3. As can be seen, each process has one layer of ghost cells around its own cells, which are correctly colored by the subdomain id that identifies the processor that owns each of these cells. Note also how each processor stores a number of artificial cells, indicated in blue, that only exist to ensure that each processor knows about all coarse grid cells and that the meshes have the 2:1 refinement property; however, in the area occupied by these artificial cells, a processor has no knowledge how refined the mesh there really is, as these are areas that are owned by other processors. As a consequence, all algorithms we will develop can only run over the locally owned cells and if necessary the ghost cells; trying to access data on any of the artificial cells is most likely an error. Note that we can determine whether we own a cell by testing that <code>cell- &gt;<a class="el" href="namespacetypes.html#a3d8ea8a4c6a58127f8c2fcc9ec74af6e">subdomain_id()</a> == triangulation.locally_owned_subdomain()</code>. The "real" mesh one has to think of here is the one that would result from forming the union of cells each of the processes own, i.e. from the overlap of the turquoise, green, yellow and red areas, disregarding the blue areas.</p>
<dl class="section note"><dt>Note</dt><dd>The decomposition of this "real" mesh into the pieces stored by each processes is provided by the <a href="http://www.p4est.org">p4est</a> library. p4est stores the complete mesh in a distributed data structure called a parallel forest (thus the name). A parallel forest consists of quad-trees (in 2d) or oct-trees (in 3d) originating in each coarse mesh cell and representing the refinement structure from parent cells to their four (in 2d) or eight (in 3d) children. Internally, this parallel forest is represented by a (distributed) linear array of cells that corresponds to a depth-first traverse of each tree, and each process then stores a contiguous section of this linear array of cells. This results in partitions such as the one shown above that are not optimal in the sense that they do not minimize the length of the interface between subdomains (and consequently do not minimize the amount of communication) but that in practice are very good and can be manipulated with exceedingly fast algorithms. The efficiency of storing and manipulating cells in this way therefore often outweighs the loss in optimality of communication. (The individual subdomains resulting from this method of partitioning may also sometimes consist of disconnected parts, such as shown at the top right. However, it can be proven that each subdomain consists of at most two disconnected pieces; see C. Burstedde, J. Holke, T. Isaac: "Bounds on the
number of   discontinuities of Morton-type space-filling curves", <a href="http://arxiv.org/abs/1505.05055">arXiv 1505.05055</a>, 2017.)</dd></dl>
<h4>Distributed degree of freedom handler</h4>
<p>The <a class="el" href="classDoFHandler.html">DoFHandler</a> class builds on the <a class="el" href="classTriangulation.html">Triangulation</a> class, but it can detect whenever we actually use an object of type <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> as triangulation. In that case, it assigns global numbers for all degrees of freedom that exist, given a finite element, on the global mesh, but each processor will only know about those that are defined on locally relevant cells (i.e. cells either locally owned or that are ghost cells). Internally, the algorithm essentially works by just looping over all cells we own locally and assigning DoF indices to the degrees of freedom defined on them and, in the case of degrees of freedom at the interface between subdomains owned by different processors, that are not owned by the neighboring processor. All processors then exchange how many degrees of freedom they locally own and shift their own indices in such a way that every degree of freedom on all subdomains are uniquely identified by an index between zero and <a class="el" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">DoFHandler::n_dofs()</a> (this function returns the global number of degrees of freedom, accumulated over all processors). Note that after this step, the degrees of freedom owned by each process form a contiguous range that can, for example, be obtained by the contiguous index set returned by <a class="el" href="classDoFHandler.html#ad39fd2189568f2f6b7d557237e3372e3">DoFHandler::locally_owned_dofs()</a>. After assigning unique indices to all degrees of freedom, the <a class="el" href="classDoFHandler.html#a553ca864aaf70330d9be86bc78f36d1e">DoFHandler::distribute_dofs()</a> function then loops over all ghost cells and communicates with neighboring processors to ensure that the global indices of degrees of freedom on these ghost cells match the ones that the neighbor has assigned to them. Through this scheme, we can make sure that each cell we locally own as well as all the ghost cells can be asked to yield the globally correct indices for the degrees of freedom defined on them. However, asking for degrees of freedom on artificial cells is likely going to lead to nothing good, as no information is available for these cells (in fact, it isn't even known whether these cells are active on the global mesh, or are further refined). As usual, degrees of freedom can be renumbered after being enumerated, using the functions in namespace <a class="el" href="namespaceDoFRenumbering.html">DoFRenumbering</a>.</p>
<h4>Linear systems for distributed computations</h4>
<p>One thing one learns very quickly when working with very large numbers of processors is that one can not store information about every degree of freedom on each processor, even if this information is "this degree of freedom doesn't live here". An example for this is that we can create an object for a (compressed) sparsity pattern that has <a class="el" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">DoFHandler::n_dofs()</a> rows, but for which we fill only those rows that correspond to the <a class="el" href="classDoFHandler.html#af16dc39b7ff25aaf361bc6ab440aeee7">DoFHandler::n_locally_owned_dofs()</a> locally owned degrees of freedom. The reason is simple: for the sake of example, let's assume we have 1 billion degrees of freedom distributed across 100 processors; if we even only hold 16 bytes per line in this sparsity pattern (whether we own the corresponding DoF or not), we'll need 16 GB for this object even if every single line is empty. Of course, only 10 million lines will be non-empty, for which we need 160 MB plus whatever is necessary to store the actual column indices of nonzero entries. Let's say we have a moderately complex problem with 50 entries per row, for each of which we store the column index worth 4 bytes, then we'll need 216 bytes for each of the 10 million lines that correspond to the degrees of freedom we own, for a total of 2.16 GB. And we'll need 16 bytes for each of the 990 million lines that we don't own, for a total of 15.840 GB. It is clear that this ratio doesn't become any better if we go to even higher numbers of processors. The solution to this problem is to really only use any memory at all for those parts of the linear system that we own, or need for some other reason. For all other parts, we must know that they exist, but we can not set up any part of our data structure. To this end, there exists a class called <a class="el" href="classIndexSet.html">IndexSet</a> that denotes a set of indices which we care for, and for which we may have to allocate memory. The data structures for sparsity patterns, constraint matrices, matrices and vector can be initialized with these <a class="el" href="classIndexSet.html">IndexSet</a> objects to really only care for those rows or entries that correspond to indices in the index set, and not care about all others. These objects will then ask how many indices exist in the set, allocate memory for each one of them (e.g. initialize the data structures for a line of a sparsity pattern), and when you want to access data for global degree of freedom <code>i</code> you will be redirected to the result of calling <a class="el" href="classIndexSet.html#a4d924bea58d98feebf99fc714b14b7d0">IndexSet::index_within_set()</a> with index <code>i</code> instead. Accessing data for elements <code>i</code> for which <a class="el" href="classIndexSet.html#a66c79fc7f17b2eeff0f0fb757e77e0c3">IndexSet::is_element()</a> is false will yield an error. The remaining question is how to identify the set of indices that correspond to degrees of freedom we need to worry about on each processor. To this end, you can use the <a class="el" href="classDoFHandler.html#ad39fd2189568f2f6b7d557237e3372e3">DoFHandler::locally_owned_dofs()</a> function to get at all the indices a processor owns. Note that this is a subset of the degrees of freedom that are defined on the locally owned cells (since some of the degrees of freedom at the interface between two different subdomains may be owned by the neighbor). This set of degrees of freedom defined on cells we own can be obtained using the function <a class="el" href="namespaceDoFTools.html#a5f745d25d2397a91dc65b9158b8d77a6">DoFTools::extract_locally_active_dofs()</a>. Finally, one sometimes needs the set of all degrees of freedom on the locally owned subdomain as well as the adjacent ghost cells. This information is provided by the <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> function.</p>
<h5>Vectors with ghost elements</h5>
<p>A typical parallel application is dealing with two different kinds of parallel vectors: vectors with ghost elements (also called ghosted vectors) and vectors without ghost elements. (Both kinds can typically be represented by the same data type, but there are of course different vector types that can each represent both flavors: for example <a class="el" href="classTrilinosWrappers_1_1MPI_1_1Vector.html">TrilinosWrappers::MPI::Vector</a>, <a class="el" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a>, and <a class="el" href="classBlockVector.html">BlockVector</a> objects built on these). You can find a discussion of what distinguishes these kinds of vectors in the <a class="el" href="DEALGlossary.html#GlossGhostedVector">glossary entry on ghosted vectors</a>. On the other hand, vectors without ghost entries are used in all other places like assembling, solving, or any other form of manipulation. These are typically write-only operations and therefore need not have read access to vector elements that may be owned by another processor. You can copy between vectors with and without ghost elements (you can see this in <a class="el" href="step_40.html">step-40</a> , <a class="el" href="step_55.html">step-55</a> , and <a class="el" href="step_32.html">step-32</a> ) using operator=.</p>
<h5>Sparsity patterns</h5>
<p>At the time of writing this, the only class equipped to deal with the situation just explained is <a class="el" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a>. A version of the function <a class="el" href="classDynamicSparsityPattern.html#aa32f9f3ebad084d001349cd3ddb4074e">DynamicSparsityPattern::reinit()</a> exists that takes an <a class="el" href="classIndexSet.html">IndexSet</a> argument that indicates which lines of the sparsity pattern to allocate memory for. In other words, it is safe to create such an object that will report as its size 1 billion, but in fact only stores only as many rows as the index set has elements. You can then use the usual function <a class="el" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a> to build the sparsity pattern that results from assembling on the locally owned portion of the mesh. The resulting object can be used to initialize a PETSc or Trilinos matrix which support very large object sizes through completely distributed storage. The matrix can then be assembled by only looping over those cells owned by the current processor. The only thing to pay attention to is for which degrees of freedom the sparsity needs to store entries. These are, in essence, the ones we could possibly store values to in the matrix upon assembly. It is clear that these are certainly the locally active degrees of freedom (which live on the cells we locally own) but through constraints, it may also be possible to write to entries that are located on ghost cells. Consequently, you need to pass the index set that results from <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> upon initializing the sparsity pattern.</p>
<h4>Constraints on degrees of freedom</h4>
<p>When creating the sparsity pattern as well as when assembling the linear system, we need to know about constraints on degrees of freedom, for example resulting from hanging nodes or boundary conditions. Like the <a class="el" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> class, the <a class="el" href="classAffineConstraints.html">AffineConstraints</a> container can also take an <a class="el" href="classIndexSet.html">IndexSet</a> upon construction that indicates for which of the possibly very large number of degrees of freedom it should actually store constraints. Unlike for the sparsity pattern, these are now only those degrees of freedom which we work on locally when assembling, namely those returned by <a class="el" href="namespaceDoFTools.html#a5f745d25d2397a91dc65b9158b8d77a6">DoFTools::extract_locally_active_dofs()</a> (a superset of the locally owned ones). There are, however, situations where more complicated constraints appear in finite element programs. An example is in \(hp\) adaptive computations where degrees of freedom can be constrained against other degrees of freedom that are themselves constrained. In a case like this, in order to fully resolve this chain of constraints, it may not be sufficient to only store constraints on locally active degrees of freedom but one may also need to have constraints available on locally relevant ones. In that case, the <a class="el" href="classAffineConstraints.html">AffineConstraints</a> object needs to be initialized with the <a class="el" href="classIndexSet.html">IndexSet</a> produced by <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> . In general, your program will continue to do something if you happen to not store all necessary constraints on each processor: you will just generate wrong matrix entries, but the program will not abort. This is opposed to the situation of the sparsity pattern: there, if the <a class="el" href="classIndexSet.html">IndexSet</a> passed to the <a class="el" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> indicates that it should store too few rows of the matrix, the program will either abort when you attempt to write into matrix entries that do not exist or the matrix class will silently allocate more memory to accommodate them. As a consequence, it is useful to err on the side of caution when indicating which constraints to store and use the result of <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> rather than <a class="el" href="namespaceDoFTools.html#a5f745d25d2397a91dc65b9158b8d77a6">DoFTools::extract_locally_active_dofs()</a> . This is also affordable since the set of locally relevant degrees of freedom is only marginally larger than the set of locally active degrees of freedom. We choose this strategy in <a class="el" href="step_32.html">step-32</a> , <a class="el" href="step_40.html">step-40</a> , and <a class="el" href="step_55.html">step-55</a> .</p>
<h4>Postprocessing</h4>
<p>Like everything else, you can only do postprocessing on cells a local processor owns. The <a class="el" href="classDataOut.html">DataOut</a> and <a class="el" href="classKellyErrorEstimator.html">KellyErrorEstimator</a> classes do this automatically: they only operate on locally owned cells without the need to do anything in particular. At least for large computations, there is also no way to merge the results of all these local computations on a single machine, i.e. each processor has to be self-sufficient. For example, each processor has to generate its own parallel output files that have to be visualized by a program that can deal with multiple input files rather than merging the results of calling <a class="el" href="classDataOut.html">DataOut</a> to a single processor before generating a single output file. The latter can be achieved, for example, using the <a class="el" href="namespaceDataOutBase.html#a3aec479936b78bd0ec2ecc3674e24584">DataOutBase::write_vtu()</a> and <a class="el" href="namespaceDataOutBase.html#ac8c87832129884d0603e3ab9ae132741">DataOutBase::write_pvtu_record()</a> functions. These same considerations hold for all other postprocessing actions as well: while it is, for example, possible to compute a global energy dissipation rate by doing the computations locally and accumulating the resulting single number processor to a single number for the entire communication, it is in general not possible to do the same if the volume of data produced by every processor is significant. There is one particular consideration for postprocessing, however: whatever you do on each cell a processor owns, you need access to at least all those values of the solution vector that are active on these cells (i.e. to the set of all <em>locally active degrees of freedom</em>, in the language of the <a class="el" href="DEALGlossary.html#distributed_paper">Distributed Computing paper</a>), which is a superset of the degrees of freedom this processor actually owns (because it may not own all degrees of freedom on the interface between its own cells and those cells owned by other processors). Sometimes, however, you need even more information: for example, to compute the KellyErrorIndicator results, one needs to evaluate the gradient at the interface on the current as well as its neighbor cell; the latter may be owned by another processor, so we need those degrees of freedom as well. In general, therefore, one needs access to the solution values for all degrees of freedom that are <em>locally relevant</em>. On the other hand, both of the packages we can use for parallel linear algebra (PETSc and Trilinos) as well as parallel::distributed::Vector subdivide vectors into chunks each processor owns and chunks stored on other processors. To postprocess stuff therefore means that we have to tell PETSc or Trilinos that it should also import <em>ghost elements</em>, i.e. additional vector elements of the solution vector other than the ones we own locally. For ghosted vectors, this can be achieved by using operator= with a distributed vector as argument. </p>
</div><!-- contents -->
<!-- HTML footer for doxygen 1.8.13-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
