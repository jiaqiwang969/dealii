<!-- HTML header for doxygen 1.8.13-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="canonical" href="https://www.dealii.org/current/doxygen/deal.II/step_40.html" />
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>The deal.II Library: The step-40 tutorial program</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link rel="SHORTCUT ICON" href="deal.ico"></link>
<script type="text/javascript" src="custom.js"></script>
<meta name="author" content="The deal.II Authors <authors@dealii.org>"></meta>
<meta name="copyright" content="Copyright (C) 1998 - 2021 by the deal.II authors"></meta>
<meta name="deal.II-version" content="10.0.0-pre"></meta>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="logo200.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">
   &#160;<span id="projectnumber">Reference documentation for deal.II version 10.0.0-pre</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!--Extra macros for MathJax:-->
<div style="display:none">
\(\newcommand{\dealvcentcolon}{\mathrel{\mathop{:}}}\)
\(\newcommand{\dealcoloneq}{\dealvcentcolon\mathrel{\mkern-1.2mu}=}\)
\(\newcommand{\jump}[1]{\left[\!\left[ #1 \right]\!\right]}\)
\(\newcommand{\average}[1]{\left\{\!\left\{ #1 \right\}\!\right\}}\)
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">The step-40 tutorial program </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This tutorial depends on <a class="el" href="step_6.html">step-6</a>.</p>
<p> 
<table class="tutorial" width="50%">
<tr><th colspan="2"><b><small>Table of contents</small></b></th></tr>
<tr><td width="50%" valign="top">
<ol>
  <li> <a href="#Intro" class=bold>Introduction</a>
    <ul>
        <li><a href="#Thetestcase">The testcase</a>
    </ul>
  <li> <a href="#CommProg" class=bold>The commented program</a>
    <ul>
        <li><a href="#Includefiles">Include files</a>
        <li><a href="#ThecodeLaplaceProblemcodeclasstemplate">The <code>LaplaceProblem</code> class template</a>
        <li><a href="#ThecodeLaplaceProblemcodeclassimplementation">The <code>LaplaceProblem</code> class implementation</a>
      <ul>
        <li><a href="#Constructor">Constructor</a>
        <li><a href="#LaplaceProblemsetup_system">LaplaceProblem::setup_system</a>
        <li><a href="#LaplaceProblemassemble_system">LaplaceProblem::assemble_system</a>
        <li><a href="#LaplaceProblemsolve">LaplaceProblem::solve</a>
        <li><a href="#LaplaceProblemrefine_grid">LaplaceProblem::refine_grid</a>
        <li><a href="#LaplaceProblemoutput_results">LaplaceProblem::output_results</a>
        <li><a href="#LaplaceProblemrun">LaplaceProblem::run</a>
        <li><a href="#main">main()</a>
      </ul>
      </ul>
</ol></td><td width="50%" valign="top"><ol>
  <li value="3"> <a href="#Results" class=bold>Results</a>
    <ul>
        <li><a href="#Possibilitiesforextensions">Possibilities for extensions</a>
    </ul>
  <li> <a href="#PlainProg" class=bold>The plain program</a>
</ol> </td> </tr> </table>
 <br />
</p>
<p><em>This program was contributed by Timo Heister, Martin Kronbichler and Wolfgang Bangerth. <br />
 This material is based upon work partly supported by the National Science Foundation under Award No. EAR-0426271 and The California Institute of Technology. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author and do not necessarily reflect the views of the National Science Foundation or of The California Institute of Technology. </em></p>
<dl class="section note"><dt>Note</dt><dd>As a prerequisite of this program, you need to have both PETSc and the p4est library installed. The installation of deal.II together with these two additional libraries is described in the <a href="../../readme.html" target="body">README</a> file. Note also that to work properly, this program needs access to the Hypre preconditioner package implementing algebraic multigrid; it can be installed as part of PETSc but has to be explicitly enabled during PETSc configuration; see the page linked to from the installation instructions for PETSc.</dd></dl>
<p><a class="anchor" id="Intro"></a> <a class="anchor" id="Introduction"></a></p><h1>Introduction</h1>
<dl class="section note"><dt>Note</dt><dd>The material presented here is also discussed in <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.5.html">video lecture 41.5</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.75.html">video lecture 41.75</a>. (All video lectures are also available <a href="http://www.math.colostate.edu/~bangerth/videos.html">here</a>.)</dd></dl>
<p>Given today's computers, most finite element computations can be done on a single machine. The majority of previous tutorial programs therefore shows only this, possibly splitting up work among a number of processors that, however, can all access the same, shared memory space. That said, there are problems that are simply too big for a single machine and in that case the problem has to be split up in a suitable way among multiple machines each of which contributes its part to the whole. A simple way to do that was shown in <a class="el" href="step_17.html">step-17</a> and <a class="el" href="step_18.html">step-18</a>, where we show how a program can use <a href="http://www.mpi-forum.org/" target="_top">MPI</a> to parallelize assembling the linear system, storing it, solving it, and computing error estimators. All of these operations scale relatively trivially (for a definition of what it means for an operation to "scale", see <a class="el" href="DEALGlossary.html#GlossParallelScaling">this glossary entry</a>), but there was one significant drawback: for this to be moderately simple to implement, each MPI processor had to keep its own copy of the entire <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects. Consequently, while we can suspect (with good reasons) that the operations listed above can scale to thousands of computers and problem sizes of billions of cells and billions of degrees of freedom, building the one big mesh for the entire problem these thousands of computers are solving on every last processor is clearly not going to scale: it is going to take forever, and maybe more importantly no single machine will have enough memory to store a mesh that has a billion cells (at least not at the time of writing this). In reality, programs like <a class="el" href="step_17.html">step-17</a> and <a class="el" href="step_18.html">step-18</a> can therefore not be run on more than maybe 100 or 200 processors and even there storing the <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects consumes the vast majority of memory on each machine.</p>
<p>Consequently, we need to approach the problem differently: to scale to very large problems each processor can only store its own little piece of the <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects. deal.II implements such a scheme in the <a class="el" href="namespaceparallel_1_1distributed.html">parallel::distributed</a> namespace and the classes therein. It builds on an external library, <a href="http://www.p4est.org/">p4est</a> (a play on the expression <em>parallel forest</em> that describes the parallel storage of a hierarchically constructed mesh as a forest of quad- or oct-trees). You need to <a href="../../external-libs/p4est.html">install and configure p4est</a> but apart from that all of its workings are hidden under the surface of deal.II.</p>
<p>In essence, what the <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> class and code inside the <a class="el" href="classDoFHandler.html">DoFHandler</a> class do is to split the global mesh so that every processor only stores a small bit it "owns" along with one layer of "ghost" cells that surround the ones it owns. What happens in the rest of the domain on which we want to solve the partial differential equation is unknown to each processor and can only be inferred through communication with other machines if such information is needed. This implies that we also have to think about problems in a different way than we did in, for example, <a class="el" href="step_17.html">step-17</a> and <a class="el" href="step_18.html">step-18</a>: no processor can have the entire solution vector for postprocessing, for example, and every part of a program has to be parallelized because no processor has all the information necessary for sequential operations.</p>
<p>A general overview of how this parallelization happens is described in the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> documentation module. You should read it for a top-level overview before reading through the source code of this program. A concise discussion of many terms we will use in the program is also provided in the <a class="el" href="DEALGlossary.html#distributed_paper">Distributed Computing paper</a>. It is probably worthwhile reading it for background information on how things work internally in this program.</p>
<p><a class="anchor" id="Thetestcase"></a></p><h3>The testcase</h3>
<p>This program essentially re-solves what we already do in <a class="el" href="step_6.html">step-6</a>, i.e. it solves the Laplace equation </p><p class="formulaDsp">
\begin{align*} -\Delta u &amp;= f \qquad &amp;&amp;\text{in}\ \Omega=[0,1]^2, \\ u &amp;= 0 \qquad &amp;&amp;\text{on}\ \partial\Omega. \end{align*}
</p>
<p> The difference of course is now that we want to do so on a mesh that may have a billion cells, with a billion or so degrees of freedom. There is no doubt that doing so is completely silly for such a simple problem, but the point of a tutorial program is, after all, not to do something useful but to show how useful programs can be implemented using deal.II. Be that as it may, to make things at least a tiny bit interesting, we choose the right hand side as a discontinuous function, </p><p class="formulaDsp">
\begin{align*} f(x,y) = \left\{ \begin{array}{ll} 1 &amp; \text{if}\ y &gt; \frac 12 + \frac 14 \sin(4\pi x), \\ -1 &amp; \text{otherwise}, \end{array} \right. \end{align*}
</p>
<p> so that the solution has a singularity along the sinusoidal line snaking its way through the domain. As a consequence, mesh refinement will be concentrated along this line. You can see this in the mesh picture shown below in the results section.</p>
<p>Rather than continuing here and giving a long introduction, let us go straight to the program code. If you have read through <a class="el" href="step_6.html">step-6</a> and the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> documentation module, most of things that are going to happen should be familiar to you already. In fact, comparing the two programs you will notice that the additional effort necessary to make things work in parallel is almost insignificant: the two programs have about the same number of lines of code (though <a class="el" href="step_6.html">step-6</a> spends more space on dealing with coefficients and output). In either case, the comments below will only be on the things that set <a class="el" href="step_40.html">step-40</a> apart from <a class="el" href="step_6.html">step-6</a> and that aren't already covered in the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> documentation module.</p>
<dl class="section note"><dt>Note</dt><dd>This program will be able to compute on as many processors as you want to throw at it, and for as large a problem as you have the memory and patience to solve. However, there <em>is</em> a limit: the number of unknowns can not exceed the largest number that can be stored with an object of type <a class="el" href="namespacetypes.html#a3543786f7dc7c57385fc923a6afd5917">types::global_dof_index</a>. By default, this is an alias for <code>unsigned int</code>, which on most machines today is a 32-bit integer, limiting you to some 4 billion (in reality, since this program uses PETSc, you will be limited to half that as PETSc uses signed integers). However, this can be changed during configuration to use 64-bit integers, see the ReadMe file. This will give problem sizes you are unlikely to exceed anytime soon.</dd></dl>
<p><a class="anchor" id="CommProg"></a> </p><h1>The commented program</h1>
<p><a class="anchor" id="Includefiles"></a> </p><h3>Include files</h3>
<p>Most of the include files we need for this program have already been discussed in previous programs. In particular, all of the following should already be familiar friends:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="quadrature__lib_8h.html">deal.II/base/quadrature_lib.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="function_8h.html">deal.II/base/function.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="timer_8h.html">deal.II/base/timer.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="generic__linear__algebra_8h.html">deal.II/lac/generic_linear_algebra.h</a>&gt;</span></div></div><!-- fragment --><p>This program can use either PETSc or Trilinos for its parallel algebra needs. By default, if deal.II has been configured with PETSc, it will use PETSc. Otherwise, the following few lines will check that deal.II has been configured with Trilinos and take that.</p>
<p>But there may be cases where you want to use Trilinos, even though deal.II has <em>also</em> been configured with PETSc, for example to compare the performance of these two libraries. To do this, add the following #define to the source code: </p><div class="CodeFragmentInTutorialComment"> <div class="fragment"><div class="line"><span class="preprocessor">#define FORCE_USE_OF_TRILINOS</span></div></div><!-- fragment --> </div><p>Using this logic, the following lines will then import either the PETSc or Trilinos wrappers into the namespace <code>LA</code> (for "linear algebra). In the former case, we are also defining the macro <code>USE_PETSC_LA</code> so that we can detect if we are using PETSc (see solve() for an example where this is necessary).</p>
<div class="fragment"><div class="line"><span class="keyword">namespace </span>LA</div><div class="line">{</div><div class="line"><span class="preprocessor">#if defined(DEAL_II_WITH_PETSC) &amp;&amp; !defined(DEAL_II_PETSC_WITH_COMPLEX) &amp;&amp; \</span></div><div class="line"><span class="preprocessor">  !(defined(DEAL_II_WITH_TRILINOS) &amp;&amp; defined(FORCE_USE_OF_TRILINOS))</span></div><div class="line">  <span class="keyword">using namespace </span>dealii::LinearAlgebraPETSc;</div><div class="line"><span class="preprocessor">#  define USE_PETSC_LA</span></div><div class="line"><span class="preprocessor">#elif defined(DEAL_II_WITH_TRILINOS)</span></div><div class="line">  <span class="keyword">using namespace </span>dealii::LinearAlgebraTrilinos;</div><div class="line"><span class="preprocessor">#else</span></div><div class="line"><span class="preprocessor">#  error DEAL_II_WITH_PETSC or DEAL_II_WITH_TRILINOS required</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">} <span class="comment">// namespace LA</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8h.html">deal.II/lac/vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="full__matrix_8h.html">deal.II/lac/full_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="solver__cg_8h.html">deal.II/lac/solver_cg.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="affine__constraints_8h.html">deal.II/lac/affine_constraints.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dynamic__sparsity__pattern_8h.html">deal.II/lac/dynamic_sparsity_pattern.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__generator_8h.html">deal.II/grid/grid_generator.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dofs_2dof__handler_8h.html">deal.II/dofs/dof_handler.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__tools_8h.html">deal.II/dofs/dof_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe_2fe__values_8h.html">deal.II/fe/fe_values.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__q_8h.html">deal.II/fe/fe_q.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector__tools_8h.html">deal.II/numerics/vector_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="numerics_2data__out_8h.html">deal.II/numerics/data_out.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="error__estimator_8h.html">deal.II/numerics/error_estimator.h</a>&gt;</span></div></div><!-- fragment --><p>The following, however, will be new or be used in new roles. Let's walk through them. The first of these will provide the tools of the <a class="el" href="namespaceUtilities_1_1System.html">Utilities::System</a> namespace that we will use to query things like the number of processors associated with the current MPI universe, or the number within this universe the processor this job runs on is:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="include_2deal_8II_2base_2utilities_8h.html">deal.II/base/utilities.h</a>&gt;</span></div></div><!-- fragment --><p>The next one provides a class, ConditionOStream that allows us to write code that would output things to a stream (such as <code>std::cout</code> on every processor but throws the text away on all but one of them. We could achieve the same by simply putting an <code>if</code> statement in front of each place where we may generate output, but this doesn't make the code any prettier. In addition, the condition whether this processor should or should not produce output to the screen is the same every time &ndash; and consequently it should be simple enough to put it into the statements that generate output itself.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="conditional__ostream_8h.html">deal.II/base/conditional_ostream.h</a>&gt;</span></div></div><!-- fragment --><p>After these preliminaries, here is where it becomes more interesting. As mentioned in the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> module, one of the fundamental truths of solving problems on large numbers of processors is that there is no way for any processor to store everything (e.g. information about all cells in the mesh, all degrees of freedom, or the values of all elements of the solution vector). Rather, every processor will <em>own</em> a few of each of these and, if necessary, may <em>know</em> about a few more, for example the ones that are located on cells adjacent to the ones this processor owns itself. We typically call the latter <em>ghost cells</em>, <em>ghost nodes</em> or <em>ghost elements of a vector</em>. The point of this discussion here is that we need to have a way to indicate which elements a particular processor owns or need to know of. This is the realm of the <a class="el" href="classIndexSet.html">IndexSet</a> class: if there are a total of \(N\) cells, degrees of freedom, or vector elements, associated with (non-negative) integral indices \([0,N)\), then both the set of elements the current processor owns as well as the (possibly larger) set of indices it needs to know about are subsets of the set \([0,N)\). <a class="el" href="classIndexSet.html">IndexSet</a> is a class that stores subsets of this set in an efficient format:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="index__set_8h.html">deal.II/base/index_set.h</a>&gt;</span></div></div><!-- fragment --><p>The next header file is necessary for a single function, <a class="el" href="namespaceSparsityTools.html#afbc0c7a206ced91b154666215ea3c218">SparsityTools::distribute_sparsity_pattern</a>. The role of this function will be explained below.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="sparsity__tools_8h.html">deal.II/lac/sparsity_tools.h</a>&gt;</span></div></div><!-- fragment --><p>The final two, new header files provide the class <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> that provides meshes distributed across a potentially very large number of processors, while the second provides the namespace <a class="el" href="namespaceparallel_1_1distributed_1_1GridRefinement.html">parallel::distributed::GridRefinement</a> that offers functions that can adaptively refine such distributed meshes:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="distributed_2tria_8h.html">deal.II/distributed/tria.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="distributed_2grid__refinement_8h.html">deal.II/distributed/grid_refinement.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>Step40</div><div class="line">{</div><div class="line">  <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div></div><!-- fragment --><p><a class="anchor" id="ThecodeLaplaceProblemcodeclasstemplate"></a> </p><h3>The <code>LaplaceProblem</code> class template</h3>
<p>Next let's declare the main class of this program. Its structure is almost exactly that of the <a class="el" href="step_6.html">step-6</a> tutorial program. The only significant differences are:</p><ul>
<li>The <code>mpi_communicator</code> variable that describes the set of processors we want this code to run on. In practice, this will be MPI_COMM_WORLD, i.e. all processors the batch scheduling system has assigned to this particular job.</li>
<li>The presence of the <code>pcout</code> variable of type ConditionOStream.</li>
<li>The obvious use of <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> instead of <a class="el" href="classTriangulation.html">Triangulation</a>.</li>
<li>The presence of two <a class="el" href="classIndexSet.html">IndexSet</a> objects that denote which sets of degrees of freedom (and associated elements of solution and right hand side vectors) we own on the current processor and which we need (as ghost elements) for the algorithms in this program to work.</li>
<li>The fact that all matrices and vectors are now distributed. We use either the PETSc or Trilinos wrapper classes so that we can use one of the sophisticated preconditioners offered by Hypre (with PETSc) or ML (with Trilinos). Note that as part of this class, we store a solution vector that does not only contain the degrees of freedom the current processor owns, but also (as ghost elements) all those vector elements that correspond to "locally relevant" degrees of freedom (i.e. all those that live on locally owned cells or the layer of ghost cells that surround it).</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keyword">class </span>LaplaceProblem</div><div class="line">{</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  LaplaceProblem();</div><div class="line"></div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">run</a>();</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line">  <span class="keywordtype">void</span> setup_system();</div><div class="line">  <span class="keywordtype">void</span> assemble_system();</div><div class="line">  <span class="keywordtype">void</span> solve();</div><div class="line">  <span class="keywordtype">void</span> refine_grid();</div><div class="line">  <span class="keywordtype">void</span> output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div><div class="line"></div><div class="line">  <a class="code" href="classMPI__Comm.html">MPI_Comm</a> mpi_communicator;</div><div class="line"></div><div class="line">  <a class="code" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation&lt;dim&gt;</a> <a class="code" href="p4est__wrappers_8cc.html#ace00f2f80d9780ef9aa1007e1c22c6a4">triangulation</a>;</div><div class="line"></div><div class="line">  <a class="code" href="classFE__Q.html">FE_Q&lt;dim&gt;</a>       fe;</div><div class="line">  <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a> dof_handler;</div><div class="line"></div><div class="line">  <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs;</div><div class="line">  <a class="code" href="classIndexSet.html">IndexSet</a> locally_relevant_dofs;</div><div class="line"></div><div class="line">  <a class="code" href="classAffineConstraints.html">AffineConstraints&lt;double&gt;</a> constraints;</div><div class="line"></div><div class="line">  <a class="code" href="namespaceLinearAlgebraDealII.html#a912abe2208022aec6753876bcc72f6bf">LA::MPI::SparseMatrix</a> system_matrix;</div><div class="line">  <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>       locally_relevant_solution;</div><div class="line">  <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>       system_rhs;</div><div class="line"></div><div class="line">  <a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div><div class="line">  <a class="code" href="classTimerOutput.html">TimerOutput</a>        computing_timer;</div><div class="line">};</div></div><!-- fragment --><p><a class="anchor" id="ThecodeLaplaceProblemcodeclassimplementation"></a> </p><h3>The <code>LaplaceProblem</code> class implementation</h3>
<p><a class="anchor" id="Constructor"></a> </p><h4>Constructor</h4>
<p>Constructors and destructors are rather trivial. In addition to what we do in <a class="el" href="step_6.html">step-6</a>, we set the set of processors we want to work on to all machines available (MPI_COMM_WORLD); ask the triangulation to ensure that the mesh remains smooth and free to refined islands, for example; and initialize the <code>pcout</code> variable to only allow processor zero to output anything. The final piece is to initialize a timer that we use to determine how much compute time the different parts of the program take:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">LaplaceProblem&lt;dim&gt;::LaplaceProblem()</div><div class="line">  : mpi_communicator(MPI_COMM_WORLD)</div><div class="line">  , triangulation(mpi_communicator,</div><div class="line">                  typename <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::MeshSmoothing(</div><div class="line">                    <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::smoothing_on_refinement |</div><div class="line">                    <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::smoothing_on_coarsening))</div><div class="line">  , fe(2)</div><div class="line">  , dof_handler(triangulation)</div><div class="line">  , pcout(<a class="code" href="namespacestd.html">std</a>::cout,</div><div class="line">          (<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator) == 0))</div><div class="line">  , computing_timer(mpi_communicator,</div><div class="line">                    pcout,</div><div class="line">                    <a class="code" href="classTimerOutput.html">TimerOutput</a>::summary,</div><div class="line">                    <a class="code" href="classTimerOutput.html">TimerOutput</a>::wall_times)</div><div class="line">{}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemsetup_system"></a> </p><h4>LaplaceProblem::setup_system</h4>
<p>The following function is, arguably, the most interesting one in the entire program since it goes to the heart of what distinguishes parallel <a class="el" href="step_40.html">step-40</a> from sequential <a class="el" href="step_6.html">step-6</a>.</p>
<p>At the top we do what we always do: tell the <a class="el" href="classDoFHandler.html">DoFHandler</a> object to distribute degrees of freedom. Since the triangulation we use here is distributed, the <a class="el" href="classDoFHandler.html">DoFHandler</a> object is smart enough to recognize that on each processor it can only distribute degrees of freedom on cells it owns; this is followed by an exchange step in which processors tell each other about degrees of freedom on ghost cell. The result is a <a class="el" href="classDoFHandler.html">DoFHandler</a> that knows about the degrees of freedom on locally owned cells and ghost cells (i.e. cells adjacent to locally owned cells) but nothing about cells that are further away, consistent with the basic philosophy of distributed computing that no processor can know everything.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::setup_system()</div><div class="line">{</div><div class="line">  <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;setup&quot;</span>);</div><div class="line"></div><div class="line">  dof_handler.distribute_dofs(fe);</div></div><!-- fragment --><p>The next two lines extract some information we will need later on, namely two index sets that provide information about which degrees of freedom are owned by the current processor (this information will be used to initialize solution and right hand side vectors, and the system matrix, indicating which elements to store on the current processor and which to expect to be stored somewhere else); and an index set that indicates which degrees of freedom are locally relevant (i.e. live on cells that the current processor owns or on the layer of ghost cells around the locally owned cells; we need all of these degrees of freedom, for example, to estimate the error on the local cells).</p>
<div class="fragment"><div class="line">locally_owned_dofs = dof_handler.locally_owned_dofs();</div><div class="line"><a class="code" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs</a>(dof_handler, locally_relevant_dofs);</div></div><!-- fragment --><p>Next, let us initialize the solution and right hand side vectors. As mentioned above, the solution vector we seek does not only store elements we own, but also ghost entries; on the other hand, the right hand side vector only needs to have the entries the current processor owns since all we will ever do is write into it, never read from it on locally owned cells (of course the linear solvers will read from it, but they do not care about the geometric location of degrees of freedom).</p>
<div class="fragment"><div class="line">locally_relevant_solution.reinit(locally_owned_dofs,</div><div class="line">                                 locally_relevant_dofs,</div><div class="line">                                 mpi_communicator);</div><div class="line">system_rhs.reinit(locally_owned_dofs, mpi_communicator);</div></div><!-- fragment --><p>The next step is to compute hanging node and boundary value constraints, which we combine into a single object storing all constraints.</p>
<p>As with all other things in parallel, the mantra must be that no processor can store all information about the entire universe. As a consequence, we need to tell the <a class="el" href="classAffineConstraints.html">AffineConstraints</a> object for which degrees of freedom it can store constraints and for which it may not expect any information to store. In our case, as explained in the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> module, the degrees of freedom we need to care about on each processor are the locally relevant ones, so we pass this to the <a class="el" href="classAffineConstraints.html#a2c9d71b5b7e8851c25a411ccf34de986">AffineConstraints::reinit</a> function. As a side note, if you forget to pass this argument, the <a class="el" href="classAffineConstraints.html">AffineConstraints</a> class will allocate an array with length equal to the largest DoF index it has seen so far. For processors with high MPI process number, this may be very large &ndash; maybe on the order of billions. The program would then allocate more memory than for likely all other operations combined for this single array.</p>
<div class="fragment"><div class="line">constraints.<a class="code" href="classAffineConstraints.html#addd15bc409c61d6f795f0132c574335b">clear</a>();</div><div class="line">constraints.<a class="code" href="classAffineConstraints.html#a2c9d71b5b7e8851c25a411ccf34de986">reinit</a>(locally_relevant_dofs);</div><div class="line"><a class="code" href="group__constraints.html#ga3b4ea7dfd313e388d868c4e4aa685799">DoFTools::make_hanging_node_constraints</a>(dof_handler, constraints);</div><div class="line"><a class="code" href="namespaceVectorTools.html#af27ac28c698a9ed0199faed50a204538">VectorTools::interpolate_boundary_values</a>(dof_handler,</div><div class="line">                                         0,</div><div class="line">                                         <a class="code" href="classFunctions_1_1ZeroFunction.html">Functions::ZeroFunction&lt;dim&gt;</a>(),</div><div class="line">                                         constraints);</div><div class="line">constraints.<a class="code" href="classAffineConstraints.html#a1611aa37f754086388ca76bcd421cce5">close</a>();</div></div><!-- fragment --><p>The last part of this function deals with initializing the matrix with accompanying sparsity pattern. As in previous tutorial programs, we use the <a class="el" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> as an intermediate with which we then initialize the system matrix. To do so we have to tell the sparsity pattern its size but as above there is no way the resulting object will be able to store even a single pointer for each global degree of freedom; the best we can hope for is that it stores information about each locally relevant degree of freedom, i.e. all those that we may ever touch in the process of assembling the matrix (the <a class="el" href="DEALGlossary.html#distributed_paper">distributed computing paper</a> has a long discussion why one really needs the locally relevant, and not the small set of locally active degrees of freedom in this context).</p>
<p>So we tell the sparsity pattern its size and what DoFs to store anything for and then ask <a class="el" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a> to fill it (this function ignores all cells that are not locally owned, mimicking what we will do below in the assembly process). After this, we call a function that exchanges entries in these sparsity pattern between processors so that in the end each processor really knows about all the entries that will exist in that part of the finite element matrix that it will own. The final step is to initialize the matrix with the sparsity pattern.</p>
<div class="fragment"><div class="line">  <a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(locally_relevant_dofs);</div><div class="line"></div><div class="line">  <a class="code" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a>(dof_handler, dsp, constraints, <span class="keyword">false</span>);</div><div class="line">  <a class="code" href="namespaceSparsityTools.html#afbc0c7a206ced91b154666215ea3c218">SparsityTools::distribute_sparsity_pattern</a>(dsp,</div><div class="line">                                             dof_handler.locally_owned_dofs(),</div><div class="line">                                             mpi_communicator,</div><div class="line">                                             locally_relevant_dofs);</div><div class="line"></div><div class="line">  system_matrix.reinit(locally_owned_dofs,</div><div class="line">                       locally_owned_dofs,</div><div class="line">                       dsp,</div><div class="line">                       mpi_communicator);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemassemble_system"></a> </p><h4>LaplaceProblem::assemble_system</h4>
<p>The function that then assembles the linear system is comparatively boring, being almost exactly what we've seen before. The points to watch out for are:</p><ul>
<li>Assembly must only loop over locally owned cells. There are multiple ways to test that; for example, we could compare a cell's subdomain_id against information from the triangulation as in <code>cell-&gt;<a class="el" href="namespacetypes.html#a3d8ea8a4c6a58127f8c2fcc9ec74af6e">subdomain_id()</a> == triangulation.locally_owned_subdomain()</code>, or skip all cells for which the condition <code>cell-&gt;is_ghost() || cell-&gt;is_artificial()</code> is true. The simplest way, however, is to simply ask the cell whether it is owned by the local processor.</li>
<li>Copying local contributions into the global matrix must include distributing constraints and boundary values. In other words, we cannot (as we did in <a class="el" href="step_6.html">step-6</a>) first copy every local contribution into the global matrix and only in a later step take care of hanging node constraints and boundary values. The reason is, as discussed in <a class="el" href="step_17.html">step-17</a>, that the parallel vector classes do not provide access to arbitrary elements of the matrix once they have been assembled into it &ndash; in parts because they may simply no longer reside on the current processor but have instead been shipped to a different machine.</li>
<li>The way we compute the right hand side (given the formula stated in the introduction) may not be the most elegant but will do for a program whose focus lies somewhere entirely different.</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::assemble_system()</div><div class="line">{</div><div class="line">  <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;assembly&quot;</span>);</div><div class="line"></div><div class="line">  <span class="keyword">const</span> <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a> quadrature_formula(fe.degree + 1);</div><div class="line"></div><div class="line">  <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values(fe,</div><div class="line">                          quadrature_formula,</div><div class="line">                          <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                            <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> dofs_per_cell = fe.n_dofs_per_cell();</div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_q_points    = quadrature_formula.<a class="code" href="classQuadrature.html#af9f7d82770fa8126e19113f3e3db755b">size</a>();</div><div class="line"></div><div class="line">  <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a> <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(dofs_per_cell, dofs_per_cell);</div><div class="line">  <a class="code" href="classVector.html">Vector&lt;double&gt;</a>     cell_rhs(dofs_per_cell);</div><div class="line"></div><div class="line">  std::vector&lt;types::global_dof_index&gt; local_dof_indices(dofs_per_cell);</div><div class="line"></div><div class="line">  <span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;cell : dof_handler.<a class="code" href="group__CPP11.html#gacdb6d3adec96a13a7079f6c893e1d3ff">active_cell_iterators</a>())</div><div class="line">    <span class="keywordflow">if</span> (cell-&gt;is_locally_owned())</div><div class="line">      {</div><div class="line">        cell_matrix = 0.;</div><div class="line">        cell_rhs    = 0.;</div><div class="line"></div><div class="line">        fe_values.<a class="code" href="classFEValues.html#a21f914e63d588e2652a9514620653d77">reinit</a>(cell);</div><div class="line"></div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points; ++q_point)</div><div class="line">          {</div><div class="line">            <span class="keyword">const</span> <span class="keywordtype">double</span> rhs_value =</div><div class="line">              (fe_values.<a class="code" href="classFEValuesBase.html#ab123e5da03736be4977c76fbcb6a2e37">quadrature_point</a>(q_point)[1] &gt;</div><div class="line">                   0.5 +</div><div class="line">                     0.25 * <a class="code" href="vectorization_8h.html#ad9b7aa5c50bf9ce988a0f756a3f2baa5">std::sin</a>(4.0 * <a class="code" href="namespacenumbers.html#a3e24f194a9cb9b6ff4442b8a7a877d4a">numbers::PI</a> *</div><div class="line">                                     fe_values.<a class="code" href="classFEValuesBase.html#ab123e5da03736be4977c76fbcb6a2e37">quadrature_point</a>(q_point)[0]) ?</div><div class="line">                 1. :</div><div class="line">                 -1.);</div><div class="line"></div><div class="line">            <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">              {</div><div class="line">                <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j = 0; j &lt; dofs_per_cell; ++j)</div><div class="line">                  <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(i, j) += fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point) *</div><div class="line">                                       fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point) *</div><div class="line">                                       fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line"></div><div class="line">                cell_rhs(i) += rhs_value *                         </div><div class="line">                               fe_values.<a class="code" href="classFEValuesBase.html#a1dd48cb744013c448d57f8f77640c08d">shape_value</a>(i, q_point) * </div><div class="line">                               fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">              }</div><div class="line">          }</div><div class="line"></div><div class="line">        cell-&gt;get_dof_indices(local_dof_indices);</div><div class="line">        constraints.<a class="code" href="classAffineConstraints.html#a373fbdacd8c486e675b8d2bff8943192">distribute_local_to_global</a>(cell_matrix,</div><div class="line">                                               cell_rhs,</div><div class="line">                                               local_dof_indices,</div><div class="line">                                               system_matrix,</div><div class="line">                                               system_rhs);</div><div class="line">      }</div></div><!-- fragment --><p>Notice that the assembling above is just a local operation. So, to form the "global" linear system, a synchronization between all processors is needed. This could be done by invoking the function <a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a>. See <a class="el" href="DEALGlossary.html#GlossCompress">Compressing distributed objects</a> for more information on what is <a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a> designed to do.</p>
<div class="fragment"><div class="line">  system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">  system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemsolve"></a> </p><h4>LaplaceProblem::solve</h4>
<p>Even though solving linear systems on potentially tens of thousands of processors is by far not a trivial job, the function that does this is &ndash; at least at the outside &ndash; relatively simple. Most of the parts you've seen before. There are really only two things worth mentioning:</p><ul>
<li>Solvers and preconditioners are built on the deal.II wrappers of PETSc and Trilinos functionality. It is relatively well known that the primary bottleneck of massively parallel linear solvers is not actually the communication between processors, but the fact that it is difficult to produce preconditioners that scale well to large numbers of processors. Over the second half of the first decade of the 21st century, it has become clear that algebraic multigrid (AMG) methods turn out to be extremely efficient in this context, and we will use one of them &ndash; either the BoomerAMG implementation of the Hypre package that can be interfaced to through PETSc, or a preconditioner provided by ML, which is part of Trilinos &ndash; for the current program. The rest of the solver itself is boilerplate and has been shown before. Since the linear system is symmetric and positive definite, we can use the CG method as the outer solver.</li>
<li>Ultimately, we want a vector that stores not only the elements of the solution for degrees of freedom the current processor owns, but also all other locally relevant degrees of freedom. On the other hand, the solver itself needs a vector that is uniquely split between processors, without any overlap. We therefore create a vector at the beginning of this function that has these properties, use it to solve the linear system, and only assign it to the vector we want at the very end. This last step ensures that all ghost elements are also copied as necessary.</li>
</ul>
<div class="fragment"><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::solve()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;solve&quot;</span>);</div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>    completely_distributed_solution(locally_owned_dofs,</div><div class="line">                                                    mpi_communicator);</div><div class="line"></div><div class="line">    <a class="code" href="classSolverControl.html">SolverControl</a> solver_control(dof_handler.n_dofs(), 1e-12);</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc.html#a9d2c129d77c89b40badfb65d94c5b955">LA::SolverCG</a> solver(solver_control, mpi_communicator);</div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc.html#a9d2c129d77c89b40badfb65d94c5b955">LA::SolverCG</a> solver(solver_control);</div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc_1_1MPI.html#a41f11f7a1992c6d6aa9367b12c68f791">LA::MPI::PreconditionAMG</a> preconditioner;</div><div class="line"></div><div class="line">    LA::MPI::PreconditionAMG::AdditionalData data;</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">    data.symmetric_operator = <span class="keyword">true</span>;</div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">// Trilinos defaults are good </span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">    preconditioner.initialize(system_matrix, data);</div><div class="line"></div><div class="line">    solver.solve(system_matrix,</div><div class="line">                 completely_distributed_solution,</div><div class="line">                 system_rhs,</div><div class="line">                 preconditioner);</div><div class="line"></div><div class="line">    pcout &lt;&lt; <span class="stringliteral">&quot;   Solved in &quot;</span> &lt;&lt; solver_control.last_step() &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span></div><div class="line">          &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#a7b3d3f295bb56d6cd6856bdc6cbe8a01">distribute</a>(completely_distributed_solution);</div><div class="line"></div><div class="line">    locally_relevant_solution = completely_distributed_solution;</div><div class="line">  }</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemrefine_grid"></a> </p><h4>LaplaceProblem::refine_grid</h4>
<p>The function that estimates the error and refines the grid is again almost exactly like the one in <a class="el" href="step_6.html">step-6</a>. The only difference is that the function that flags cells to be refined is now in namespace <a class="el" href="namespaceparallel_1_1distributed_1_1GridRefinement.html">parallel::distributed::GridRefinement</a> &ndash; a namespace that has functions that can communicate between all involved processors and determine global thresholds to use in deciding which cells to refine and which to coarsen.</p>
<p>Note that we didn't have to do anything special about the <a class="el" href="classKellyErrorEstimator.html">KellyErrorEstimator</a> class: we just give it a vector with as many elements as the local triangulation has cells (locally owned cells, ghost cells, and artificial ones), but it only fills those entries that correspond to cells that are locally owned.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::refine_grid()</div><div class="line">{</div><div class="line">  <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;refine&quot;</span>);</div><div class="line"></div><div class="line">  <a class="code" href="classVector.html">Vector&lt;float&gt;</a> estimated_error_per_cell(triangulation.<a class="code" href="classTriangulation.html#a5ea5c9957dbb566a562bbe2c0f3971e9">n_active_cells</a>());</div><div class="line">  <a class="code" href="classKellyErrorEstimator.html#ae2269e1c9903e9d863b7abd54948af00">KellyErrorEstimator&lt;dim&gt;::estimate</a>(</div><div class="line">    dof_handler,</div><div class="line">    <a class="code" href="classQGauss.html">QGauss&lt;dim - 1&gt;</a>(fe.degree + 1),</div><div class="line">    std::map&lt;<a class="code" href="classunsigned_01int.html">types::boundary_id</a>, <span class="keyword">const</span> <a class="code" href="classFunction.html">Function&lt;dim&gt;</a> *&gt;(),</div><div class="line">    locally_relevant_solution,</div><div class="line">    estimated_error_per_cell);</div><div class="line">  <a class="code" href="namespaceparallel_1_1distributed_1_1GridRefinement.html#aa2ffb707a796ae6dedb75036606ef2e6">parallel::distributed::GridRefinement::refine_and_coarsen_fixed_number</a>(</div><div class="line">    triangulation, estimated_error_per_cell, 0.3, 0.03);</div><div class="line">  triangulation.<a class="code" href="classTriangulation.html#ac8b4fbb207303ec7f5ef758821ecd8cb">execute_coarsening_and_refinement</a>();</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemoutput_results"></a> </p><h4>LaplaceProblem::output_results</h4>
<p>Compared to the corresponding function in <a class="el" href="step_6.html">step-6</a>, the one here is a tad more complicated. There are two reasons: the first one is that we do not just want to output the solution but also for each cell which processor owns it (i.e. which "subdomain" it is in). Secondly, as discussed at length in <a class="el" href="step_17.html">step-17</a> and <a class="el" href="step_18.html">step-18</a>, generating graphical data can be a bottleneck in parallelizing. In <a class="el" href="step_18.html">step-18</a>, we have moved this step out of the actual computation but shifted it into a separate program that later combined the output from various processors into a single file. But this doesn't scale: if the number of processors is large, this may mean that the step of combining data on a single processor later becomes the longest running part of the program, or it may produce a file that's so large that it can't be visualized any more. We here follow a more sensible approach, namely creating individual files for each MPI process and leaving it to the visualization program to make sense of that.</p>
<p>To start, the top of the function looks like it usually does. In addition to attaching the solution vector (the one that has entries for all locally relevant, not only the locally owned, elements), we attach a data vector that stores, for each cell, the subdomain the cell belongs to. This is slightly tricky, because of course not every processor knows about every cell. The vector we attach therefore has an entry for every cell that the current processor has in its mesh (locally owned ones, ghost cells, and artificial cells), but the <a class="el" href="classDataOut.html">DataOut</a> class will ignore all entries that correspond to cells that are not owned by the current processor. As a consequence, it doesn't actually matter what values we write into these vector entries: we simply fill the entire vector with the number of the current MPI process (i.e. the subdomain_id of the current process); this correctly sets the values we care for, i.e. the entries that correspond to locally owned cells, while providing the wrong value for all other elements &ndash; but these are then ignored anyway.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword"></span>{</div><div class="line">  <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">  data_out.<a class="code" href="classDataOut__DoFData.html#a6ed7c846331069f406b8c9933c37fda4">attach_dof_handler</a>(dof_handler);</div><div class="line">  data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(locally_relevant_solution, <span class="stringliteral">&quot;u&quot;</span>);</div><div class="line"></div><div class="line">  <a class="code" href="classVector.html">Vector&lt;float&gt;</a> subdomain(triangulation.<a class="code" href="classTriangulation.html#a5ea5c9957dbb566a562bbe2c0f3971e9">n_active_cells</a>());</div><div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; subdomain.size(); ++i)</div><div class="line">    subdomain(i) = triangulation.<a class="code" href="classTriangulation.html#a44ea82a097d8317c98fa422307aff874">locally_owned_subdomain</a>();</div><div class="line">  data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(subdomain, <span class="stringliteral">&quot;subdomain&quot;</span>);</div><div class="line"></div><div class="line">  data_out.<a class="code" href="classDataOut.html#a087f63e22f0614bca326dbdca288c646">build_patches</a>();</div></div><!-- fragment --><p>The next step is to write this data to disk. We write up to 8 VTU files in parallel with the help of MPI-IO. Additionally a PVTU record is generated, which groups the written VTU files.</p>
<div class="fragment"><div class="line">  data_out.<a class="code" href="classDataOutInterface.html#a0864e51eb173c87e2a3edc9391ea8009">write_vtu_with_pvtu_record</a>(</div><div class="line">    <span class="stringliteral">&quot;./&quot;</span>, <span class="stringliteral">&quot;solution&quot;</span>, cycle, mpi_communicator, 2, 8);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemrun"></a> </p><h4>LaplaceProblem::run</h4>
<p>The function that controls the overall behavior of the program is again like the one in <a class="el" href="step_6.html">step-6</a>. The minor difference are the use of <code>pcout</code> instead of <code>std::cout</code> for output to the console (see also <a class="el" href="step_17.html">step-17</a>) and that we only generate graphical output if at most 32 processors are involved. Without this limit, it would be just too easy for people carelessly running this program without reading it first to bring down the cluster interconnect and fill any file system available :-)</p>
<p>A functional difference to <a class="el" href="step_6.html">step-6</a> is the use of a square domain and that we start with a slightly finer mesh (5 global refinement cycles) &ndash; there just isn't much of a point showing a massively parallel program starting on 4 cells (although admittedly the point is only slightly stronger starting on 1024).</p>
<div class="fragment"><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">LaplaceProblem&lt;dim&gt;::run</a>()</div><div class="line">  {</div><div class="line">    pcout &lt;&lt; <span class="stringliteral">&quot;Running with &quot;</span></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot;PETSc&quot;</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot;Trilinos&quot;</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot; on &quot;</span> &lt;&lt; <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">Utilities::MPI::n_mpi_processes</a>(mpi_communicator)</div><div class="line">          &lt;&lt; <span class="stringliteral">&quot; MPI rank(s)...&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_cycles = 8;</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle = 0; cycle &lt; n_cycles; ++cycle)</div><div class="line">      {</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">          {</div><div class="line">            <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a>(triangulation);</div><div class="line">            triangulation.<a class="code" href="classTriangulation.html#a6ad0b3fb24aae17f4668427a433dea19">refine_global</a>(5);</div><div class="line">          }</div><div class="line">        <span class="keywordflow">else</span></div><div class="line">          refine_grid();</div><div class="line"></div><div class="line">        setup_system();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">              &lt;&lt; triangulation.<a class="code" href="classTriangulation.html#a584733c8499dbd140694bfe04e0963ca">n_global_active_cells</a>() &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span> &lt;&lt; dof_handler.n_dofs()</div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        assemble_system();</div><div class="line">        solve();</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">Utilities::MPI::n_mpi_processes</a>(mpi_communicator) &lt;= 32)</div><div class="line">          {</div><div class="line">            <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;output&quot;</span>);</div><div class="line">            output_results(cycle);</div><div class="line">          }</div><div class="line"></div><div class="line">        computing_timer.print_summary();</div><div class="line">        computing_timer.reset();</div><div class="line"></div><div class="line">        pcout &lt;&lt; std::endl;</div><div class="line">      }</div><div class="line">  }</div><div class="line">} <span class="comment">// namespace Step40</span></div></div><!-- fragment --><p><a class="anchor" id="main"></a> </p><h4>main()</h4>
<p>The final function, <code>main()</code>, again has the same structure as in all other programs, in particular <a class="el" href="step_6.html">step-6</a>. Like the other programs that use MPI, we have to initialize and finalize MPI, which is done using the helper object <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a>. The constructor of that class also initializes libraries that depend on MPI, such as p4est, PETSc, SLEPc, and Zoltan (though the last two are not used in this tutorial). The order here is important: we cannot use any of these libraries until they are initialized, so it does not make sense to do anything before creating an instance of <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a>.</p>
<p>After the solver finishes, the LaplaceProblem destructor will run followed by <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html#a812d3e3369e2c86ba94beb13368d92d7">Utilities::MPI::MPI_InitFinalize::~MPI_InitFinalize()</a>. This order is also important: <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html#a812d3e3369e2c86ba94beb13368d92d7">Utilities::MPI::MPI_InitFinalize::~MPI_InitFinalize()</a> calls <code>PetscFinalize</code> (and finalization functions for other libraries), which will delete any in-use PETSc objects. This must be done after we destruct the Laplace solver to avoid double deletion errors. Fortunately, due to the order of destructor call rules of C++, we do not need to worry about any of this: everything happens in the correct order (i.e., the reverse of the order of construction). The last function called by <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html#a812d3e3369e2c86ba94beb13368d92d7">Utilities::MPI::MPI_InitFinalize::~MPI_InitFinalize()</a> is <code>MPI_Finalize</code>: i.e., once this object is destructed the program should exit since MPI will no longer be available.</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> *argv[])</div><div class="line">{</div><div class="line">  <span class="keywordflow">try</span></div><div class="line">    {</div><div class="line">      <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">      <span class="keyword">using namespace </span>Step40;</div><div class="line"></div><div class="line">      <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">      LaplaceProblem&lt;2&gt; laplace_problem_2d;</div><div class="line">      laplace_problem_2d.run();</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (...)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p> <a class="anchor" id="Results"></a></p><h1>Results</h1>
<p>When you run the program, on a single processor or with your local MPI installation on a few, you should get output like this: </p><div class="fragment"><div class="line">Cycle 0:</div><div class="line">   Number of active cells:       1024</div><div class="line">   Number of degrees of freedom: 4225</div><div class="line">   Solved in 10 iterations.</div><div class="line"></div><div class="line"></div><div class="line">+---------------------------------------------+------------+------------+</div><div class="line">| Total wallclock time elapsed since start    |     0.176s |            |</div><div class="line">|                                             |            |            |</div><div class="line">| Section                         | no. calls |  wall time | % of total |</div><div class="line">+---------------------------------+-----------+------------+------------+</div><div class="line">| assembly                        |         1 |    0.0209s |        12% |</div><div class="line">| output                          |         1 |    0.0189s |        11% |</div><div class="line">| setup                           |         1 |    0.0299s |        17% |</div><div class="line">| solve                           |         1 |    0.0419s |        24% |</div><div class="line">+---------------------------------+-----------+------------+------------+</div><div class="line"></div><div class="line"></div><div class="line">Cycle 1:</div><div class="line">   Number of active cells:       1954</div><div class="line">   Number of degrees of freedom: 8399</div><div class="line">   Solved in 10 iterations.</div><div class="line"></div><div class="line"></div><div class="line">+---------------------------------------------+------------+------------+</div><div class="line">| Total wallclock time elapsed since start    |     0.327s |            |</div><div class="line">|                                             |            |            |</div><div class="line">| Section                         | no. calls |  wall time | % of total |</div><div class="line">+---------------------------------+-----------+------------+------------+</div><div class="line">| assembly                        |         1 |    0.0368s |        11% |</div><div class="line">| output                          |         1 |    0.0208s |       6.4% |</div><div class="line">| <a class="code" href="namespaceGridRefinement.html#a1cf30058b31ce7f9b389e8310bb9fc54">refine</a>                          |         1 |     0.157s |        48% |</div><div class="line">| setup                           |         1 |    0.0452s |        14% |</div><div class="line">| solve                           |         1 |    0.0668s |        20% |</div><div class="line">+---------------------------------+-----------+------------+------------+</div><div class="line"></div><div class="line"></div><div class="line">Cycle 2:</div><div class="line">   Number of active cells:       3664</div><div class="line">   Number of degrees of freedom: 16183</div><div class="line">   Solved in 11 iterations.</div><div class="line"></div><div class="line">...</div></div><!-- fragment --><p>The exact numbers differ, depending on how many processors we use; this is due to the fact that the preconditioner depends on the partitioning of the problem, the solution then differs in the last few digits, and consequently the mesh refinement differs slightly. The primary thing to notice here, though, is that the number of iterations does not increase with the size of the problem. This guarantees that we can efficiently solve even the largest problems.</p>
<p>When run on a sufficiently large number of machines (say a few thousand), this program can relatively easily solve problems with well over one billion unknowns in less than a minute. On the other hand, such big problems can no longer be visualized, so we also ran the program on only 16 processors. Here are a mesh, along with its partitioning onto the 16 processors, and the corresponding solution:</p>
<table width="100%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.mesh.png"/>
</div>
  </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.solution.png"/>
</div>
   </td></tr>
</table>
<p>The mesh on the left has a mere 7,069 cells. This is of course a problem we would easily have been able to solve already on a single processor using <a class="el" href="step_6.html">step-6</a>, but the point of the program was to show how to write a program that scales to many more machines. For example, here are two graphs that show how the run time of a large number of parts of the program scales on problems with around 52 and 375 million degrees of freedom if we take more and more processors (these and the next couple of graphs are taken from an earlier version of the <a class="el" href="DEALGlossary.html#distributed_paper">Distributed Computing paper</a>; updated graphs showing data of runs on even larger numbers of processors, and a lot more interpretation can be found in the final version of the paper):</p>
<table width="100%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.strong2.png"/>
</div>
  </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.strong.png"/>
</div>
   </td></tr>
</table>
<p>As can clearly be seen, the program scales nicely to very large numbers of processors. (For a discussion of what we consider "scalable" programs, see <a class="el" href="DEALGlossary.html#GlossParallelScaling">this glossary entry</a>.) The curves, in particular the linear solver, become a bit wobbly at the right end of the graphs since each processor has too little to do to offset the cost of communication (the part of the whole problem each processor has to solve in the above two examples is only 13,000 and 90,000 degrees of freedom when 4,096 processors are used; a good rule of thumb is that parallel programs work well if each processor has at least 100,000 unknowns).</p>
<p>While the strong scaling graphs above show that we can solve a problem of fixed size faster and faster if we take more and more processors, the more interesting question may be how big problems can become so that they can still be solved within a reasonable time on a machine of a particular size. We show this in the following two graphs for 256 and 4096 processors:</p>
<table width="100%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.256.png"/>
</div>
  </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.4096.png"/>
</div>
   </td></tr>
</table>
<p>What these graphs show is that all parts of the program scale linearly with the number of degrees of freedom. This time, lines are wobbly at the left as the size of local problems is too small. For more discussions of these results we refer to the <a class="el" href="DEALGlossary.html#distributed_paper">Distributed Computing paper</a>.</p>
<p>So how large are the largest problems one can solve? At the time of writing this problem, the limiting factor is that the program uses the BoomerAMG algebraic multigrid method from the <a href="http://acts.nersc.gov/hypre/" target="_top">Hypre package</a> as a preconditioner, which unfortunately uses signed 32-bit integers to index the elements of a distributed matrix. This limits the size of problems to \(2^{31}-1=2,147,483,647\) degrees of freedom. From the graphs above it is obvious that the scalability would extend beyond this number, and one could expect that given more than the 4,096 machines shown above would also further reduce the compute time. That said, one can certainly expect that this limit will eventually be lifted by the hypre developers.</p>
<p>On the other hand, this does not mean that deal.II cannot solve bigger problems. Indeed, <a class="el" href="step_37.html">step-37</a> shows how one can solve problems that are not just a little, but very substantially larger than anything we have shown here.</p>
<p><a class="anchor" id="extensions"></a> <a class="anchor" id="Possibilitiesforextensions"></a></p><h3>Possibilities for extensions</h3>
<p>In a sense, this program is the ultimate solver for the Laplace equation: it can essentially solve the equation to whatever accuracy you want, if only you have enough processors available. Since the Laplace equation by itself is not terribly interesting at this level of accuracy, the more interesting possibilities for extension therefore concern not so much this program but what comes beyond it. For example, several of the other programs in this tutorial have significant run times, especially in 3d. It would therefore be interesting to use the techniques explained here to extend other programs to support parallel distributed computations. We have done this for <a class="el" href="step_31.html">step-31</a> in the <a class="el" href="step_32.html">step-32</a> tutorial program, but the same would apply to, for example, <a class="el" href="step_23.html">step-23</a> and <a class="el" href="step_25.html">step-25</a> for hyperbolic time dependent problems, <a class="el" href="step_33.html">step-33</a> for gas dynamics, or <a class="el" href="step_35.html">step-35</a> for the Navier-Stokes equations.</p>
<p>Maybe equally interesting is the problem of postprocessing. As mentioned above, we only show pictures of the solution and the mesh for 16 processors because 4,096 processors solving 1 billion unknowns would produce graphical output on the order of several 10 gigabyte. Currently, no program is able to visualize this amount of data in any reasonable way unless it also runs on at least several hundred processors. There are, however, approaches where visualization programs directly communicate with solvers on each processor with each visualization process rendering the part of the scene computed by the solver on this processor. Implementing such an interface would allow to quickly visualize things that are otherwise not amenable to graphical display.</p>
<p><a class="anchor" id="PlainProg"></a> </p><h1>The plain program</h1>
<div class="fragment"><div class="line"><span class="comment">/* ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Copyright (C) 2009 - 2021 by the deal.II authors</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * This file is part of the deal.II library.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * The deal.II library is free software; you can use it, redistribute</span></div><div class="line"><span class="comment"> * it, and/or modify it under the terms of the GNU Lesser General</span></div><div class="line"><span class="comment"> * Public License as published by the Free Software Foundation; either</span></div><div class="line"><span class="comment"> * version 2.1 of the License, or (at your option) any later version.</span></div><div class="line"><span class="comment"> * The full text of the license can be found in the file LICENSE.md at</span></div><div class="line"><span class="comment"> * the top level directory of deal.II.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Author: Wolfgang Bangerth, Texas A&amp;M University, 2009, 2010</span></div><div class="line"><span class="comment"> *         Timo Heister, University of Goettingen, 2009, 2010</span></div><div class="line"><span class="comment"> */</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="quadrature__lib_8h.html">deal.II/base/quadrature_lib.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="function_8h.html">deal.II/base/function.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="timer_8h.html">deal.II/base/timer.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="generic__linear__algebra_8h.html">deal.II/lac/generic_linear_algebra.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>LA</div><div class="line">{</div><div class="line"><span class="preprocessor">#if defined(DEAL_II_WITH_PETSC) &amp;&amp; !defined(DEAL_II_PETSC_WITH_COMPLEX) &amp;&amp; \</span></div><div class="line"><span class="preprocessor">  !(defined(DEAL_II_WITH_TRILINOS) &amp;&amp; defined(FORCE_USE_OF_TRILINOS))</span></div><div class="line">  <span class="keyword">using namespace </span>dealii::LinearAlgebraPETSc;</div><div class="line"><span class="preprocessor">#  define USE_PETSC_LA</span></div><div class="line"><span class="preprocessor">#elif defined(DEAL_II_WITH_TRILINOS)</span></div><div class="line">  <span class="keyword">using namespace </span>dealii::LinearAlgebraTrilinos;</div><div class="line"><span class="preprocessor">#else</span></div><div class="line"><span class="preprocessor">#  error DEAL_II_WITH_PETSC or DEAL_II_WITH_TRILINOS required</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">} <span class="comment">// namespace LA</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8h.html">deal.II/lac/vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="full__matrix_8h.html">deal.II/lac/full_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="solver__cg_8h.html">deal.II/lac/solver_cg.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="affine__constraints_8h.html">deal.II/lac/affine_constraints.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dynamic__sparsity__pattern_8h.html">deal.II/lac/dynamic_sparsity_pattern.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__generator_8h.html">deal.II/grid/grid_generator.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dofs_2dof__handler_8h.html">deal.II/dofs/dof_handler.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__tools_8h.html">deal.II/dofs/dof_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe_2fe__values_8h.html">deal.II/fe/fe_values.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__q_8h.html">deal.II/fe/fe_q.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector__tools_8h.html">deal.II/numerics/vector_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="numerics_2data__out_8h.html">deal.II/numerics/data_out.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="error__estimator_8h.html">deal.II/numerics/error_estimator.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="include_2deal_8II_2base_2utilities_8h.html">deal.II/base/utilities.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="conditional__ostream_8h.html">deal.II/base/conditional_ostream.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="index__set_8h.html">deal.II/base/index_set.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="sparsity__tools_8h.html">deal.II/lac/sparsity_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="distributed_2tria_8h.html">deal.II/distributed/tria.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="distributed_2grid__refinement_8h.html">deal.II/distributed/grid_refinement.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>Step40</div><div class="line">{</div><div class="line">  <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keyword">class </span>LaplaceProblem</div><div class="line">  {</div><div class="line">  <span class="keyword">public</span>:</div><div class="line">    LaplaceProblem();</div><div class="line"></div><div class="line">    <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">run</a>();</div><div class="line"></div><div class="line">  <span class="keyword">private</span>:</div><div class="line">    <span class="keywordtype">void</span> setup_system();</div><div class="line">    <span class="keywordtype">void</span> assemble_system();</div><div class="line">    <span class="keywordtype">void</span> solve();</div><div class="line">    <span class="keywordtype">void</span> refine_grid();</div><div class="line">    <span class="keywordtype">void</span> output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div><div class="line"></div><div class="line">    <a class="code" href="classMPI__Comm.html">MPI_Comm</a> mpi_communicator;</div><div class="line"></div><div class="line">    <a class="code" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation&lt;dim&gt;</a> <a class="code" href="p4est__wrappers_8cc.html#ace00f2f80d9780ef9aa1007e1c22c6a4">triangulation</a>;</div><div class="line"></div><div class="line">    <a class="code" href="classFE__Q.html">FE_Q&lt;dim&gt;</a>       fe;</div><div class="line">    <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a> dof_handler;</div><div class="line"></div><div class="line">    <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs;</div><div class="line">    <a class="code" href="classIndexSet.html">IndexSet</a> locally_relevant_dofs;</div><div class="line"></div><div class="line">    <a class="code" href="classAffineConstraints.html">AffineConstraints&lt;double&gt;</a> constraints;</div><div class="line"></div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a912abe2208022aec6753876bcc72f6bf">LA::MPI::SparseMatrix</a> system_matrix;</div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>       locally_relevant_solution;</div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>       system_rhs;</div><div class="line"></div><div class="line">    <a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div><div class="line">    <a class="code" href="classTimerOutput.html">TimerOutput</a>        computing_timer;</div><div class="line">  };</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  LaplaceProblem&lt;dim&gt;::LaplaceProblem()</div><div class="line">    : mpi_communicator(MPI_COMM_WORLD)</div><div class="line">    , triangulation(mpi_communicator,</div><div class="line">                    typename <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::MeshSmoothing(</div><div class="line">                      <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::smoothing_on_refinement |</div><div class="line">                      <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::smoothing_on_coarsening))</div><div class="line">    , fe(2)</div><div class="line">    , dof_handler(triangulation)</div><div class="line">    , pcout(<a class="code" href="namespacestd.html">std</a>::cout,</div><div class="line">            (<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator) == 0))</div><div class="line">    , computing_timer(mpi_communicator,</div><div class="line">                      pcout,</div><div class="line">                      <a class="code" href="classTimerOutput.html">TimerOutput</a>::summary,</div><div class="line">                      <a class="code" href="classTimerOutput.html">TimerOutput</a>::wall_times)</div><div class="line">  {}</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::setup_system()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;setup&quot;</span>);</div><div class="line"></div><div class="line">    dof_handler.distribute_dofs(fe);</div><div class="line"></div><div class="line">    locally_owned_dofs = dof_handler.locally_owned_dofs();</div><div class="line">    <a class="code" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs</a>(dof_handler, locally_relevant_dofs);</div><div class="line"></div><div class="line">    locally_relevant_solution.reinit(locally_owned_dofs,</div><div class="line">                                     locally_relevant_dofs,</div><div class="line">                                     mpi_communicator);</div><div class="line">    system_rhs.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line"></div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#addd15bc409c61d6f795f0132c574335b">clear</a>();</div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#a2c9d71b5b7e8851c25a411ccf34de986">reinit</a>(locally_relevant_dofs);</div><div class="line">    <a class="code" href="group__constraints.html#ga3b4ea7dfd313e388d868c4e4aa685799">DoFTools::make_hanging_node_constraints</a>(dof_handler, constraints);</div><div class="line">    <a class="code" href="namespaceVectorTools.html#af27ac28c698a9ed0199faed50a204538">VectorTools::interpolate_boundary_values</a>(dof_handler,</div><div class="line">                                             0,</div><div class="line">                                             <a class="code" href="classFunctions_1_1ZeroFunction.html">Functions::ZeroFunction&lt;dim&gt;</a>(),</div><div class="line">                                             constraints);</div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#a1611aa37f754086388ca76bcd421cce5">close</a>();</div><div class="line"></div><div class="line">    <a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(locally_relevant_dofs);</div><div class="line"></div><div class="line">    <a class="code" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a>(dof_handler, dsp, constraints, <span class="keyword">false</span>);</div><div class="line">    <a class="code" href="namespaceSparsityTools.html#afbc0c7a206ced91b154666215ea3c218">SparsityTools::distribute_sparsity_pattern</a>(dsp,</div><div class="line">                                               dof_handler.locally_owned_dofs(),</div><div class="line">                                               mpi_communicator,</div><div class="line">                                               locally_relevant_dofs);</div><div class="line"></div><div class="line">    system_matrix.reinit(locally_owned_dofs,</div><div class="line">                         locally_owned_dofs,</div><div class="line">                         dsp,</div><div class="line">                         mpi_communicator);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::assemble_system()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;assembly&quot;</span>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a> quadrature_formula(fe.degree + 1);</div><div class="line"></div><div class="line">    <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values(fe,</div><div class="line">                            quadrature_formula,</div><div class="line">                            <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                              <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> dofs_per_cell = fe.n_dofs_per_cell();</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_q_points    = quadrature_formula.<a class="code" href="classQuadrature.html#af9f7d82770fa8126e19113f3e3db755b">size</a>();</div><div class="line"></div><div class="line">    <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a> <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(dofs_per_cell, dofs_per_cell);</div><div class="line">    Vector&lt;double&gt;     cell_rhs(dofs_per_cell);</div><div class="line"></div><div class="line">    std::vector&lt;types::global_dof_index&gt; local_dof_indices(dofs_per_cell);</div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;cell : dof_handler.<a class="code" href="group__CPP11.html#gacdb6d3adec96a13a7079f6c893e1d3ff">active_cell_iterators</a>())</div><div class="line">      <span class="keywordflow">if</span> (cell-&gt;is_locally_owned())</div><div class="line">        {</div><div class="line">          cell_matrix = 0.;</div><div class="line">          cell_rhs    = 0.;</div><div class="line"></div><div class="line">          fe_values.<a class="code" href="classFEValues.html#a21f914e63d588e2652a9514620653d77">reinit</a>(cell);</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points; ++q_point)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">double</span> rhs_value =</div><div class="line">                (fe_values.<a class="code" href="classFEValuesBase.html#ab123e5da03736be4977c76fbcb6a2e37">quadrature_point</a>(q_point)[1] &gt;</div><div class="line">                     0.5 +</div><div class="line">                       0.25 * <a class="code" href="vectorization_8h.html#ad9b7aa5c50bf9ce988a0f756a3f2baa5">std::sin</a>(4.0 * <a class="code" href="namespacenumbers.html#a3e24f194a9cb9b6ff4442b8a7a877d4a">numbers::PI</a> *</div><div class="line">                                       fe_values.<a class="code" href="classFEValuesBase.html#ab123e5da03736be4977c76fbcb6a2e37">quadrature_point</a>(q_point)[0]) ?</div><div class="line">                   1. :</div><div class="line">                   -1.);</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">                {</div><div class="line">                  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j = 0; j &lt; dofs_per_cell; ++j)</div><div class="line">                    <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(i, j) += fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point) *</div><div class="line">                                         fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point) *</div><div class="line">                                         fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line"></div><div class="line">                  cell_rhs(i) += rhs_value *                         </div><div class="line">                                 fe_values.<a class="code" href="classFEValuesBase.html#a1dd48cb744013c448d57f8f77640c08d">shape_value</a>(i, q_point) * </div><div class="line">                                 fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">                }</div><div class="line">            }</div><div class="line"></div><div class="line">          cell-&gt;get_dof_indices(local_dof_indices);</div><div class="line">          constraints.<a class="code" href="classAffineConstraints.html#a373fbdacd8c486e675b8d2bff8943192">distribute_local_to_global</a>(cell_matrix,</div><div class="line">                                                 cell_rhs,</div><div class="line">                                                 local_dof_indices,</div><div class="line">                                                 system_matrix,</div><div class="line">                                                 system_rhs);</div><div class="line">        }</div><div class="line"></div><div class="line">    system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">    system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::solve()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;solve&quot;</span>);</div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>    completely_distributed_solution(locally_owned_dofs,</div><div class="line">                                                    mpi_communicator);</div><div class="line"></div><div class="line">    <a class="code" href="classSolverControl.html">SolverControl</a> solver_control(dof_handler.n_dofs(), 1e-12);</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc.html#a9d2c129d77c89b40badfb65d94c5b955">LA::SolverCG</a> solver(solver_control, mpi_communicator);</div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc.html#a9d2c129d77c89b40badfb65d94c5b955">LA::SolverCG</a> solver(solver_control);</div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc_1_1MPI.html#a41f11f7a1992c6d6aa9367b12c68f791">LA::MPI::PreconditionAMG</a> preconditioner;</div><div class="line"></div><div class="line">    LA::MPI::PreconditionAMG::AdditionalData data;</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">    data.symmetric_operator = <span class="keyword">true</span>;</div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">/* Trilinos defaults are good */</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">    preconditioner.<a class="code" href="classPETScWrappers_1_1PreconditionBoomerAMG.html#a3c82672eac786d7f369b296439859ff2">initialize</a>(system_matrix, data);</div><div class="line"></div><div class="line">    solver.solve(system_matrix,</div><div class="line">                 completely_distributed_solution,</div><div class="line">                 system_rhs,</div><div class="line">                 preconditioner);</div><div class="line"></div><div class="line">    pcout &lt;&lt; <span class="stringliteral">&quot;   Solved in &quot;</span> &lt;&lt; solver_control.last_step() &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span></div><div class="line">          &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#a7b3d3f295bb56d6cd6856bdc6cbe8a01">distribute</a>(completely_distributed_solution);</div><div class="line"></div><div class="line">    locally_relevant_solution = completely_distributed_solution;</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::refine_grid()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;refine&quot;</span>);</div><div class="line"></div><div class="line">    Vector&lt;float&gt; estimated_error_per_cell(triangulation.<a class="code" href="classTriangulation.html#a5ea5c9957dbb566a562bbe2c0f3971e9">n_active_cells</a>());</div><div class="line">    <a class="code" href="classKellyErrorEstimator.html#ae2269e1c9903e9d863b7abd54948af00">KellyErrorEstimator&lt;dim&gt;::estimate</a>(</div><div class="line">      dof_handler,</div><div class="line">      <a class="code" href="classQGauss.html">QGauss&lt;dim - 1&gt;</a>(fe.degree + 1),</div><div class="line">      std::map&lt;<a class="code" href="classunsigned_01int.html">types::boundary_id</a>, <span class="keyword">const</span> <a class="code" href="classFunction.html">Function&lt;dim&gt;</a> *&gt;(),</div><div class="line">      locally_relevant_solution,</div><div class="line">      estimated_error_per_cell);</div><div class="line">    <a class="code" href="namespaceparallel_1_1distributed_1_1GridRefinement.html#aa2ffb707a796ae6dedb75036606ef2e6">parallel::distributed::GridRefinement::refine_and_coarsen_fixed_number</a>(</div><div class="line">      triangulation, estimated_error_per_cell, 0.3, 0.03);</div><div class="line">    triangulation.<a class="code" href="classTriangulation.html#ac8b4fbb207303ec7f5ef758821ecd8cb">execute_coarsening_and_refinement</a>();</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a6ed7c846331069f406b8c9933c37fda4">attach_dof_handler</a>(dof_handler);</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(locally_relevant_solution, <span class="stringliteral">&quot;u&quot;</span>);</div><div class="line"></div><div class="line">    Vector&lt;float&gt; subdomain(triangulation.<a class="code" href="classTriangulation.html#a5ea5c9957dbb566a562bbe2c0f3971e9">n_active_cells</a>());</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; subdomain.size(); ++i)</div><div class="line">      subdomain(i) = triangulation.<a class="code" href="classTriangulation.html#a44ea82a097d8317c98fa422307aff874">locally_owned_subdomain</a>();</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(subdomain, <span class="stringliteral">&quot;subdomain&quot;</span>);</div><div class="line"></div><div class="line">    data_out.<a class="code" href="classDataOut.html#a087f63e22f0614bca326dbdca288c646">build_patches</a>();</div><div class="line"></div><div class="line">    data_out.<a class="code" href="classDataOutInterface.html#a0864e51eb173c87e2a3edc9391ea8009">write_vtu_with_pvtu_record</a>(</div><div class="line">      <span class="stringliteral">&quot;./&quot;</span>, <span class="stringliteral">&quot;solution&quot;</span>, cycle, mpi_communicator, 2, 8);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">LaplaceProblem&lt;dim&gt;::run</a>()</div><div class="line">  {</div><div class="line">    pcout &lt;&lt; <span class="stringliteral">&quot;Running with &quot;</span></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot;PETSc&quot;</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot;Trilinos&quot;</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot; on &quot;</span> &lt;&lt; <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">Utilities::MPI::n_mpi_processes</a>(mpi_communicator)</div><div class="line">          &lt;&lt; <span class="stringliteral">&quot; MPI rank(s)...&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_cycles = 8;</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle = 0; cycle &lt; n_cycles; ++cycle)</div><div class="line">      {</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">          {</div><div class="line">            <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a>(triangulation);</div><div class="line">            triangulation.<a class="code" href="classTriangulation.html#a6ad0b3fb24aae17f4668427a433dea19">refine_global</a>(5);</div><div class="line">          }</div><div class="line">        <span class="keywordflow">else</span></div><div class="line">          refine_grid();</div><div class="line"></div><div class="line">        setup_system();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">              &lt;&lt; triangulation.<a class="code" href="classTriangulation.html#a584733c8499dbd140694bfe04e0963ca">n_global_active_cells</a>() &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span> &lt;&lt; dof_handler.n_dofs()</div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        assemble_system();</div><div class="line">        solve();</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">Utilities::MPI::n_mpi_processes</a>(mpi_communicator) &lt;= 32)</div><div class="line">          {</div><div class="line">            <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;output&quot;</span>);</div><div class="line">            output_results(cycle);</div><div class="line">          }</div><div class="line"></div><div class="line">        computing_timer.print_summary();</div><div class="line">        computing_timer.reset();</div><div class="line"></div><div class="line">        pcout &lt;&lt; std::endl;</div><div class="line">      }</div><div class="line">  }</div><div class="line">} <span class="comment">// namespace Step40</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> *argv[])</div><div class="line">{</div><div class="line">  <span class="keywordflow">try</span></div><div class="line">    {</div><div class="line">      <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">      <span class="keyword">using namespace </span>Step40;</div><div class="line"></div><div class="line">      <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">      LaplaceProblem&lt;2&gt; laplace_problem_2d;</div><div class="line">      laplace_problem_2d.run();</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (...)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p>This tutorial depends on <a class="el" href="step_6.html">step-6</a> . <table class="tutorial"
 width="50%"> <tr><th colspan="2"><b><small>Table of
 contents</small></b><b><small>Table of contents</small></b></th></tr>
 <tr><td width="50%" valign="top">
 <ol>
 <li> <a href="#Intro" class=bold>Introduction</a><a href="#Intro"
 class=bold>Introduction</a>
 <ul>
 <li><a href="#Thetestcase">The testcase</a><a href="#Thetestcase">The
 testcase</a>
 </ul>
 <li> <a href="#CommProg" class=bold>The commented program</a><a
 href="#CommProg" class=bold>The commented program</a>
 <ul>
 <li><a href="#Includefiles">Include files</a><a
 href="#Includefiles">Include files</a>
 <li><a href="#ThecodeLaplaceProblemcodeclasstemplate">The
 <code>LaplaceProblem</code> class template</a><a
 href="#ThecodeLaplaceProblemcodeclasstemplate">The
 <code>LaplaceProblem</code> class template</a>
 <li><a href="#ThecodeLaplaceProblemcodeclassimplementation">The
 <code>LaplaceProblem</code> class implementation</a><a
 href="#ThecodeLaplaceProblemcodeclassimplementation">The
 <code>LaplaceProblem</code> class implementation</a>
 <ul>
 <li><a href="#Constructor">Constructor</a><a
 href="#Constructor">Constructor</a>
 <li><a
 href="#LaplaceProblemsetup_system">LaplaceProblem::setup_system</a><a
 href="#LaplaceProblemsetup_system">LaplaceProblem::setup_system</a>
 <li><a
 href="#LaplaceProblemassemble_system">LaplaceProblem::assemble_system</a><a
 href="#LaplaceProblemassemble_system">LaplaceProblem::assemble_system</a>
 <li><a href="#LaplaceProblemsolve">LaplaceProblem::solve</a><a
 href="#LaplaceProblemsolve">LaplaceProblem::solve</a>
 <li><a href="#LaplaceProblemrefine_grid">LaplaceProblem::refine_grid</a><a
 href="#LaplaceProblemrefine_grid">LaplaceProblem::refine_grid</a>
 <li><a
 href="#LaplaceProblemoutput_results">LaplaceProblem::output_results</a><a
 href="#LaplaceProblemoutput_results">LaplaceProblem::output_results</a>
 <li><a href="#LaplaceProblemrun">LaplaceProblem::run</a><a
 href="#LaplaceProblemrun">LaplaceProblem::run</a>
 <li><a href="#main">main()</a><a href="#main">main()</a>
 </ul>
 </ul>
 </ol></td><td width="50%" valign="top"><ol>
 <li value="3"> <a href="#Results" class=bold>Results</a><a href="#Results"
 class=bold>Results</a>
 <ul>
 <li><a href="#Possibilitiesforextensions">Possibilities for
 extensions</a><a href="#Possibilitiesforextensions">Possibilities for
 extensions</a>
 </ul>
 <li> <a href="#PlainProg" class=bold>The plain program</a><a
 href="#PlainProg" class=bold>The plain program</a>
 </ol> </td> </tr> </table>
  <br />
 <em>This program was contributed by Timo Heister, Martin Kronbichler and Wolfgang Bangerth. <br />
 This material is based upon work partly supported by the National Science Foundation under Award No. EAR-0426271 and The California Institute of Technology. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author and do not necessarily reflect the views of the National Science Foundation or of The California Institute of Technology. </em></p>
<dl class="section note"><dt>Note</dt><dd>As a prerequisite of this program, you need to have both PETSc and thep4est library installed. The installation of deal.IItogether with these two additional libraries is described in the <a href="../../readme.html" target="body">README</a> file. Note also thatto work properly, this program needs access to the Hyprepreconditioner package implementing algebraic multigrid; it can beinstalled as part of PETSc but has to be explicitly enabled duringPETSc configuration; see the page linked to from the installationinstructions for PETSc.</dd></dl>
<p><a class="anchor" id="Intro"></a><a class="anchor" id="Introduction"></a></p><h1>Introduction</h1>
<dl class="section note"><dt>Note</dt><dd>The material presented here is also discussed in <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.5.html">video lecture 41.5</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.75.html">video lecture 41.75</a>. (All video lectures are also available <a href="http://www.math.colostate.edu/~bangerth/videos.html">here</a>.) Given today's computers, most finite element computations can be done ona single machine. The majority of previous tutorial programs thereforeshows only this, possibly splitting up work among a number ofprocessors that, however, can all access the same, shared memoryspace. That said, there are problems that are simply too big for asingle machine and in that case the problem has to be split up in asuitable way among multiple machines each of which contributes itspart to the whole. A simple way to do that was shown in <a class="el" href="step_17.html">step-17</a> and <a class="el" href="step_18.html">step-18</a> , where we show how a program can use &lt;a Consequently, we need to approach the problem differently: to scale tovery large problems each processor can only store its own little pieceof the <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects. deal.II implements such ascheme in the <a class="el" href="namespaceparallel_1_1distributed.html">parallel::distributed</a> namespace and the classestherein. It builds on an external library, <a href="http://www.p4est.org/">p4est</a> (a play on the expression<em>parallel forest</em> that describes the parallel storage of ahierarchically constructed mesh as a forest of quad- oroct-trees). You need to <a href="../../external-libs/p4est.html">install and configure p4est</a>but apart from that all of its workings are hidden under the surfaceof deal.II. In essence, what the <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> class andcode inside the <a class="el" href="classDoFHandler.html">DoFHandler</a> class do is to splitthe global mesh so that every processor only stores a small bit it"owns" along with one layer of "ghost" cells that surround the ones itowns. What happens in the rest of the domain on which we want to solvethe partial differential equation is unknown to each processor and canonly be inferred through communication with other machines if suchinformation is needed. This implies that we also have to think aboutproblems in a different way than we did in, for example, <a class="el" href="step_17.html">step-17</a> and <a class="el" href="step_18.html">step-18</a> : no processor can have the entire solution vector forpostprocessing, for example, and every part of a program has to beparallelized because no processor has all the information necessaryfor sequential operations. A general overview of how this parallelization happens is described inthe <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> documentation module. You should read it for atop-level overview before reading through the source code of thisprogram. A concise discussion of many terms we will use in the programis also provided in the <a class="el" href="DEALGlossary.html#distributed_paper">Distributed Computing paper</a>. It is probably worthwhile reading it for background information on howthings work internally in this program.</dd></dl>
<p><a class="anchor" id="Thetestcase"></a></p><h3>The testcase</h3>
<p>This program essentially re-solves what we already do in <a class="el" href="step_6.html">step-6</a> , i.e. it solves the Laplace equation </p><p class="formulaDsp">
\begin{align*} -\Delta u &amp;= f \qquad &amp;&amp;\text{in}\ \Omega=[0,1]^2, \\ u &amp;= 0 \qquad &amp;&amp;\text{on}\ \partial\Omega. \end{align*}
</p>
<p> The difference of course is now that we want to do so on a mesh thatmay have a billion cells, with a billion or so degrees offreedom. There is no doubt that doing so is completely silly for sucha simple problem, but the point of a tutorial program is, after all,not to do something useful but to show how useful programs can beimplemented using deal.II. Be that as it may, to make things at leasta tiny bit interesting, we choose the right hand side as adiscontinuous function, </p><p class="formulaDsp">
\begin{align*} f(x,y) = \left\{ \begin{array}{ll} 1 &amp; \text{if}\ y &gt; \frac 12 + \frac 14 \sin(4\pi x), \\ -1 &amp; \text{otherwise}, \end{array} \right. \end{align*}
</p>
<p> so that the solution has a singularity along the sinusoidal linesnaking its way through the domain. As a consequence, mesh refinementwill be concentrated along this line. You can see this in the meshpicture shown below in the results section. Rather than continuing here and giving a long introduction, let us gostraight to the program code. If you have read through <a class="el" href="step_6.html">step-6</a> and the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> documentation module, most of things that are goingto happen should be familiar to you already. In fact, comparing the twoprograms you will notice that the additional effort necessary to make thingswork in parallel is almost insignificant: the two programs have about thesame number of lines of code (though <a class="el" href="step_6.html">step-6</a> spends more space on dealing withcoefficients and output). In either case, the comments below will only be onthe things that set <a class="el" href="step_40.html">step-40</a> apart from <a class="el" href="step_6.html">step-6</a> and that aren't already coveredin the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> documentation module.</p>
<dl class="section note"><dt>Note</dt><dd>This program will be able to compute on as many processors as you wantto throw at it, and for as large a problem as you have the memory and patienceto solve. However, there <em>is</em> a limit: the number of unknowns can notexceed the largest number that can be stored with an object of type <a class="el" href="namespacetypes.html#a3543786f7dc7c57385fc923a6afd5917">types::global_dof_index</a>. By default, this is an alias for <code>unsignedint</code>, which on most machines today is a 32-bit integer, limiting you tosome 4 billion (in reality, since this program uses PETSc, you will be limitedto half that as PETSc uses signed integers). However, this can be changedduring configuration to use 64-bit integers, see the ReadMe file. This willgive problem sizes you are unlikely to exceed anytime soon.</dd></dl>
<p><a class="anchor" id="CommProg"></a> </p><h1>The commented program</h1>
<p><a class="anchor" id="Includefiles"></a> </p><h3>Include files</h3>
<p>Most of the include files we need for this program have already been discussed in previous programs. In particular, all of the following should already be familiar friends:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="quadrature__lib_8h.html">deal.II/base/quadrature_lib.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="function_8h.html">deal.II/base/function.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="timer_8h.html">deal.II/base/timer.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="generic__linear__algebra_8h.html">deal.II/lac/generic_linear_algebra.h</a>&gt;</span></div></div><!-- fragment --><p>This program can use either PETSc or Trilinos for its parallel algebra needs. By default, if deal.II has been configured with PETSc, it will use PETSc. Otherwise, the following few lines will check that deal.II has been configured with Trilinos and take that.</p>
<p>But there may be cases where you want to use Trilinos, even though deal.II hasalso* been configured with PETSc, for example to compare the performance of these two libraries. To do this, add the following #define to the source code: </p><div class="CodeFragmentInTutorialComment"></div><div class="CodeFragmentInTutorialComment"><div class="fragment"><div class="line"><span class="preprocessor">#define FORCE_USE_OF_TRILINOS</span></div></div><!-- fragment --></div><div class="CodeFragmentInTutorialComment"> </div><p>Using this logic, the following lines will then import either the PETSc or Trilinos wrappers into the namespace <code>LA</code> (for "linear algebra). In the former case, we are also defining the macro <code>USE_PETSC_LA</code> so that we can detect if we are using PETSc (see solve() for an example where this is necessary).</p>
<div class="fragment"><div class="line"><span class="keyword">namespace </span>LA</div><div class="line">{</div><div class="line"><span class="preprocessor">#if defined(DEAL_II_WITH_PETSC) &amp;&amp; !defined(DEAL_II_PETSC_WITH_COMPLEX) &amp;&amp; \</span></div><div class="line"><span class="preprocessor">!(defined(DEAL_II_WITH_TRILINOS) &amp;&amp; defined(FORCE_USE_OF_TRILINOS))</span></div><div class="line"><span class="keyword">using namespace </span>dealii::LinearAlgebraPETSc;</div><div class="line"><span class="preprocessor">#  define USE_PETSC_LA</span></div><div class="line"><span class="preprocessor">#elif defined(DEAL_II_WITH_TRILINOS)</span></div><div class="line"><span class="keyword">using namespace </span>dealii::LinearAlgebraTrilinos;</div><div class="line"><span class="preprocessor">#else</span></div><div class="line"><span class="preprocessor">#  error DEAL_II_WITH_PETSC or DEAL_II_WITH_TRILINOS required</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">} <span class="comment">// namespace LA</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8h.html">deal.II/lac/vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="full__matrix_8h.html">deal.II/lac/full_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="solver__cg_8h.html">deal.II/lac/solver_cg.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="affine__constraints_8h.html">deal.II/lac/affine_constraints.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dynamic__sparsity__pattern_8h.html">deal.II/lac/dynamic_sparsity_pattern.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__generator_8h.html">deal.II/grid/grid_generator.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dofs_2dof__handler_8h.html">deal.II/dofs/dof_handler.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__tools_8h.html">deal.II/dofs/dof_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe_2fe__values_8h.html">deal.II/fe/fe_values.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__q_8h.html">deal.II/fe/fe_q.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector__tools_8h.html">deal.II/numerics/vector_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="numerics_2data__out_8h.html">deal.II/numerics/data_out.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="error__estimator_8h.html">deal.II/numerics/error_estimator.h</a>&gt;</span></div></div><!-- fragment --><p>The following, however, will be new or be used in new roles. Let's walk through them. The first of these will provide the tools of the <a class="el" href="namespaceUtilities_1_1System.html">Utilities::System</a> namespace that we will use to query things like the number of processors associated with the current MPI universe, or the number within this universe the processor this job runs on is:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="include_2deal_8II_2base_2utilities_8h.html">deal.II/base/utilities.h</a>&gt;</span></div></div><!-- fragment --><p>The next one provides a class, ConditionOStream that allows us to write code that would output things to a stream (such as <code>std::cout</code> on every processor but throws the text away on all but one of them. We could achieve the same by simply putting an <code>if</code> statement in front of each place where we may generate output, but this doesn't make the code any prettier. In addition, the condition whether this processor should or should not produce output to the screen is the same every time</p>
<ul>
<li>and consequently it should be simple enough to put it into the statements that generate output itself.</li>
</ul>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="conditional__ostream_8h.html">deal.II/base/conditional_ostream.h</a>&gt;</span></div></div><!-- fragment --><p>After these preliminaries, here is where it becomes more interesting. As mentioned in the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> module, one of the fundamental truths of solving problems on large numbers of processors is that there is no way for any processor to store everything (e.g. information about all cells in the mesh, all degrees of freedom, or the values of all elements of the solution vector). Rather, every processor will <em>own</em> a few of each of these and, if necessary, may <em>know</em> about a few more, for example the ones that are located on cells adjacent to the ones this processor owns itself. We typically call the latter <em>ghost cells</em>, <em>ghost nodes</em> or <em>ghost elements of a vector</em>. The point of this discussion here is that we need to have a way to indicate which elements a particular processor owns or need to know of. This is the realm of the <a class="el" href="classIndexSet.html">IndexSet</a> class: if there are a total of \(N\) cells, degrees of freedom, or vector elements, associated with (non-negative) integral indices \([0,N)\) , then both the set of elements the current processor owns as well as the (possibly larger) set of indices it needs to know about are subsets of the set \([0,N)\) . <a class="el" href="classIndexSet.html">IndexSet</a> is a class that stores subsets of this set in an efficient format:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="index__set_8h.html">deal.II/base/index_set.h</a>&gt;</span></div></div><!-- fragment --><p>The next header file is necessary for a single function, <a class="el" href="namespaceSparsityTools.html#afbc0c7a206ced91b154666215ea3c218">SparsityTools::distribute_sparsity_pattern</a>. The role of this function will be explained below.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="sparsity__tools_8h.html">deal.II/lac/sparsity_tools.h</a>&gt;</span></div></div><!-- fragment --><p>The final two, new header files provide the class <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> that provides meshes distributed across a potentially very large number of processors, while the second provides the namespace <a class="el" href="namespaceparallel_1_1distributed_1_1GridRefinement.html">parallel::distributed::GridRefinement</a> that offers functions that can adaptively refine such distributed meshes:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="distributed_2tria_8h.html">deal.II/distributed/tria.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="distributed_2grid__refinement_8h.html">deal.II/distributed/grid_refinement.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>Step40</div><div class="line">{</div><div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div></div><!-- fragment --><p><a class="anchor" id="ThecodeLaplaceProblemcodeclasstemplate"></a> </p><h3>The <code>LaplaceProblem</code> class template</h3>
<p>Next let's declare the main class of this program. Its structure is almost exactly that of the <a class="el" href="step_6.html">step-6</a> tutorial program. The only significant differences are:</p>
<ul>
<li>The <code>mpi_communicator</code> variable that describes the set of processors we want this code to run on. In practice, this will be MPI_COMM_WORLD, i.e. all processors the batch scheduling system has assigned to this particular job.</li>
<li>The presence of the <code>pcout</code> variable of type ConditionOStream.</li>
<li>The obvious use of <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> instead of <a class="el" href="classTriangulation.html">Triangulation</a>.</li>
<li>The presence of two <a class="el" href="classIndexSet.html">IndexSet</a> objects that denote which sets of degrees of freedom (and associated elements of solution and right hand side vectors) we own on the current processor and which we need (as ghost elements) for the algorithms in this program to work.</li>
<li>The fact that all matrices and vectors are now distributed. We use either the PETSc or Trilinos wrapper classes so that we can use one of the sophisticated preconditioners offered by Hypre (with PETSc) or ML (with Trilinos). Note that as part of this class, we store a solution vector that does not only contain the degrees of freedom the current processor owns, but also (as ghost elements) all those vector elements that correspond to "locally relevant" degrees of freedom (i.e. all those that live on locally owned cells or the layer of ghost cells that surround it).</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keyword">class </span>LaplaceProblem</div><div class="line">{</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  LaplaceProblem();</div><div class="line"></div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">run</a>();</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line">  <span class="keywordtype">void</span> setup_system();</div><div class="line">  <span class="keywordtype">void</span> assemble_system();</div><div class="line">  <span class="keywordtype">void</span> solve();</div><div class="line">  <span class="keywordtype">void</span> refine_grid();</div><div class="line">  <span class="keywordtype">void</span> output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div><div class="line"></div><div class="line">  <a class="code" href="classMPI__Comm.html">MPI_Comm</a> mpi_communicator;</div><div class="line"></div><div class="line">  <a class="code" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation&lt;dim&gt;</a> <a class="code" href="p4est__wrappers_8cc.html#ace00f2f80d9780ef9aa1007e1c22c6a4">triangulation</a>;</div><div class="line"></div><div class="line">  <a class="code" href="classFE__Q.html">FE_Q&lt;dim&gt;</a>       fe;</div><div class="line">  <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a> dof_handler;</div><div class="line"></div><div class="line">  <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs;</div><div class="line">  <a class="code" href="classIndexSet.html">IndexSet</a> locally_relevant_dofs;</div><div class="line"></div><div class="line">  <a class="code" href="classAffineConstraints.html">AffineConstraints&lt;double&gt;</a> constraints;</div><div class="line"></div><div class="line">  <a class="code" href="namespaceLinearAlgebraDealII.html#a912abe2208022aec6753876bcc72f6bf">LA::MPI::SparseMatrix</a> system_matrix;</div><div class="line">  <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>       locally_relevant_solution;</div><div class="line">  <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>       system_rhs;</div><div class="line"></div><div class="line">  <a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div><div class="line">  <a class="code" href="classTimerOutput.html">TimerOutput</a>        computing_timer;</div><div class="line">};</div></div><!-- fragment --><p><a class="anchor" id="ThecodeLaplaceProblemcodeclassimplementation"></a> </p><h3>The <code>LaplaceProblem</code> class implementation</h3>
<p><a class="anchor" id="Constructor"></a> </p><h4>Constructor</h4>
<p>Constructors and destructors are rather trivial. In addition to what we do in <a class="el" href="step_6.html">step-6</a> , we set the set of processors we want to work on to all machines available (MPI_COMM_WORLD); ask the triangulation to ensure that the mesh remains smooth and free to refined islands, for example; and initialize the <code>pcout</code> variable to only allow processor zero to output anything. The final piece is to initialize a timer that we use to determine how much compute time the different parts of the program take:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">LaplaceProblem&lt;dim&gt;::LaplaceProblem()</div><div class="line">  : mpi_communicator(MPI_COMM_WORLD)</div><div class="line">  , triangulation(mpi_communicator,</div><div class="line">                  typename <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::MeshSmoothing(</div><div class="line">                    <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::smoothing_on_refinement |</div><div class="line">                    <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::smoothing_on_coarsening))</div><div class="line">  , fe(2)</div><div class="line">  , dof_handler(triangulation)</div><div class="line">  , pcout(<a class="code" href="namespacestd.html">std</a>::cout,</div><div class="line">          (<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator) == 0))</div><div class="line">  , computing_timer(mpi_communicator,</div><div class="line">                    pcout,</div><div class="line">                    <a class="code" href="classTimerOutput.html">TimerOutput</a>::summary,</div><div class="line">                    <a class="code" href="classTimerOutput.html">TimerOutput</a>::wall_times)</div><div class="line">{}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemsetup_system"></a> </p><h4>LaplaceProblem::setup_system</h4>
<p>The following function is, arguably, the most interesting one in the entire program since it goes to the heart of what distinguishes parallel <a class="el" href="step_40.html">step-40</a> from sequential <a class="el" href="step_6.html">step-6</a> . At the top we do what we always do: tell the <a class="el" href="classDoFHandler.html">DoFHandler</a> object to distribute degrees of freedom. Since the triangulation we use here is distributed, the <a class="el" href="classDoFHandler.html">DoFHandler</a> object is smart enough to recognize that on each processor it can only distribute degrees of freedom on cells it owns; this is followed by an exchange step in which processors tell each other about degrees of freedom on ghost cell. The result is a <a class="el" href="classDoFHandler.html">DoFHandler</a> that knows about the degrees of freedom on locally owned cells and ghost cells (i.e. cells adjacent to locally owned cells) but nothing about cells that are further away, consistent with the basic philosophy of distributed computing that no processor can know everything.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::setup_system()</div><div class="line">{</div><div class="line">  <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;setup&quot;</span>);</div><div class="line"></div><div class="line">  dof_handler.distribute_dofs(fe);</div></div><!-- fragment --><p>The next two lines extract some information we will need later on, namely two index sets that provide information about which degrees of freedom are owned by the current processor (this information will be used to initialize solution and right hand side vectors, and the system matrix, indicating which elements to store on the current processor and which to expect to be stored somewhere else); and an index set that indicates which degrees of freedom are locally relevant (i.e. live on cells that the current processor owns or on the layer of ghost cells around the locally owned cells; we need all of these degrees of freedom, for example, to estimate the error on the local cells).</p>
<div class="fragment"><div class="line">locally_owned_dofs = dof_handler.locally_owned_dofs();</div><div class="line"><a class="code" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs</a>(dof_handler, locally_relevant_dofs);</div></div><!-- fragment --><p>Next, let us initialize the solution and right hand side vectors. As mentioned above, the solution vector we seek does not only store elements we own, but also ghost entries; on the other hand, the right hand side vector only needs to have the entries the current processor owns since all we will ever do is write into it, never read from it on locally owned cells (of course the linear solvers will read from it, but they do not care about the geometric location of degrees of freedom).</p>
<div class="fragment"><div class="line">locally_relevant_solution.reinit(locally_owned_dofs,</div><div class="line">                                 locally_relevant_dofs,</div><div class="line">                                 mpi_communicator);</div><div class="line">system_rhs.reinit(locally_owned_dofs, mpi_communicator);</div></div><!-- fragment --><p>The next step is to compute hanging node and boundary value constraints, which we combine into a single object storing all constraints. As with all other things in parallel, the mantra must be that no processor can store all information about the entire universe. As a consequence, we need to tell the <a class="el" href="classAffineConstraints.html">AffineConstraints</a> object for which degrees of freedom it can store constraints and for which it may not expect any information to store. In our case, as explained in the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> module, the degrees of freedom we need to care about on each processor are the locally relevant ones, so we pass this to the <a class="el" href="classAffineConstraints.html#a2c9d71b5b7e8851c25a411ccf34de986">AffineConstraints::reinit</a> function. As a side note, if you forget to pass this argument, the <a class="el" href="classAffineConstraints.html">AffineConstraints</a> class will allocate an array with length equal to the largest DoF index it has seen so far. For processors with high MPI process number, this may be very large</p>
<ul>
<li>maybe on the order of billions. The program would then allocate more memory than for likely all other operations combined for this single array.</li>
</ul>
<div class="fragment"><div class="line">constraints.<a class="code" href="classAffineConstraints.html#addd15bc409c61d6f795f0132c574335b">clear</a>();</div><div class="line">constraints.<a class="code" href="classAffineConstraints.html#a2c9d71b5b7e8851c25a411ccf34de986">reinit</a>(locally_relevant_dofs);</div><div class="line"><a class="code" href="group__constraints.html#ga3b4ea7dfd313e388d868c4e4aa685799">DoFTools::make_hanging_node_constraints</a>(dof_handler, constraints);</div><div class="line"><a class="code" href="namespaceVectorTools.html#af27ac28c698a9ed0199faed50a204538">VectorTools::interpolate_boundary_values</a>(dof_handler,</div><div class="line">                                         0,</div><div class="line">                                         <a class="code" href="classFunctions_1_1ZeroFunction.html">Functions::ZeroFunction&lt;dim&gt;</a>(),</div><div class="line">                                         constraints);</div><div class="line">constraints.<a class="code" href="classAffineConstraints.html#a1611aa37f754086388ca76bcd421cce5">close</a>();</div></div><!-- fragment --><p>The last part of this function deals with initializing the matrix with accompanying sparsity pattern. As in previous tutorial programs, we use the <a class="el" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> as an intermediate with which we then initialize the system matrix. To do so we have to tell the sparsity pattern its size but as above there is no way the resulting object will be able to store even a single pointer for each global degree of freedom; the best we can hope for is that it stores information about each locally relevant degree of freedom, i.e. all those that we may ever touch in the process of assembling the matrix (the <a class="el" href="DEALGlossary.html#distributed_paper">distributed computing paper</a> has a long discussion why one really needs the locally relevant, and not the small set of locally active degrees of freedom in this context). So we tell the sparsity pattern its size and what DoFs to store anything for and then ask <a class="el" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a> to fill it (this function ignores all cells that are not locally owned, mimicking what we will do below in the assembly process). After this, we call a function that exchanges entries in these sparsity pattern between processors so that in the end each processor really knows about all the entries that will exist in that part of the finite element matrix that it will own. The final step is to initialize the matrix with the sparsity pattern.</p>
<div class="fragment"><div class="line">  <a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(locally_relevant_dofs);</div><div class="line"></div><div class="line">  <a class="code" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a>(dof_handler, dsp, constraints, <span class="keyword">false</span>);</div><div class="line">  <a class="code" href="namespaceSparsityTools.html#afbc0c7a206ced91b154666215ea3c218">SparsityTools::distribute_sparsity_pattern</a>(dsp,</div><div class="line">                                             dof_handler.locally_owned_dofs(),</div><div class="line">                                             mpi_communicator,</div><div class="line">                                             locally_relevant_dofs);</div><div class="line"></div><div class="line">  system_matrix.reinit(locally_owned_dofs,</div><div class="line">                       locally_owned_dofs,</div><div class="line">                       dsp,</div><div class="line">                       mpi_communicator);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemassemble_system"></a> </p><h4>LaplaceProblem::assemble_system</h4>
<p>The function that then assembles the linear system is comparatively boring, being almost exactly what we've seen before. The points to watch out for are:</p>
<ul>
<li>Assembly must only loop over locally owned cells. There are multiple ways to test that; for example, we could compare a cell's subdomain_id against information from the triangulation as in <code>cell-&gt;<a class="el" href="namespacetypes.html#a3d8ea8a4c6a58127f8c2fcc9ec74af6e">subdomain_id()</a> == triangulation.locally_owned_subdomain()</code>, or skip all cells for which the condition <code>cell-&gt;is_ghost() || cell-&gt;is_artificial()</code> is true. The simplest way, however, is to simply ask the cell whether it is owned by the local processor.</li>
<li>Copying local contributions into the global matrix must include distributing constraints and boundary values. In other words, we cannot (as we did in <a class="el" href="step_6.html">step-6</a> ) first copy every local contribution into the global matrix and only in a later step take care of hanging node constraints and boundary values. The reason is, as discussed in <a class="el" href="step_17.html">step-17</a> , that the parallel vector classes do not provide access to arbitrary elements of the matrix once they have been assembled into it</li>
<li>in parts because they may simply no longer reside on the current processor but have instead been shipped to a different machine.</li>
<li>The way we compute the right hand side (given the formula stated in the introduction) may not be the most elegant but will do for a program whose focus lies somewhere entirely different.</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::assemble_system()</div><div class="line">{</div><div class="line">  <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;assembly&quot;</span>);</div><div class="line"></div><div class="line">  <span class="keyword">const</span> <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a> quadrature_formula(fe.degree + 1);</div><div class="line"></div><div class="line">  <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values(fe,</div><div class="line">                          quadrature_formula,</div><div class="line">                          <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                            <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> dofs_per_cell = fe.n_dofs_per_cell();</div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_q_points    = quadrature_formula.<a class="code" href="classQuadrature.html#af9f7d82770fa8126e19113f3e3db755b">size</a>();</div><div class="line"></div><div class="line">  <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a> <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(dofs_per_cell, dofs_per_cell);</div><div class="line">  <a class="code" href="classVector.html">Vector&lt;double&gt;</a>     cell_rhs(dofs_per_cell);</div><div class="line"></div><div class="line">  std::vector&lt;types::global_dof_index&gt; local_dof_indices(dofs_per_cell);</div><div class="line"></div><div class="line">  <span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;cell : dof_handler.<a class="code" href="group__CPP11.html#gacdb6d3adec96a13a7079f6c893e1d3ff">active_cell_iterators</a>())</div><div class="line">    <span class="keywordflow">if</span> (cell-&gt;is_locally_owned())</div><div class="line">      {</div><div class="line">        cell_matrix = 0.;</div><div class="line">        cell_rhs    = 0.;</div><div class="line"></div><div class="line">        fe_values.<a class="code" href="classFEValues.html#a21f914e63d588e2652a9514620653d77">reinit</a>(cell);</div><div class="line"></div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points; ++q_point)</div><div class="line">          {</div><div class="line">            <span class="keyword">const</span> <span class="keywordtype">double</span> rhs_value =</div><div class="line">              (fe_values.<a class="code" href="classFEValuesBase.html#ab123e5da03736be4977c76fbcb6a2e37">quadrature_point</a>(q_point)[1] &gt;</div><div class="line">                   0.5 +</div><div class="line">                     0.25 <a class="code" href="vectorization_8h.html#ad9b7aa5c50bf9ce988a0f756a3f2baa5">std::sin</a>(4.0 <a class="code" href="namespacenumbers.html#a3e24f194a9cb9b6ff4442b8a7a877d4a">numbers::PI</a></div><div class="line">                                     fe_values.<a class="code" href="classFEValuesBase.html#ab123e5da03736be4977c76fbcb6a2e37">quadrature_point</a>(q_point)[0]) ?</div><div class="line">                 1. :</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">-1.);</div><div class="line"></div><div class="line">            <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">              {</div><div class="line">                <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j = 0; j &lt; dofs_per_cell; ++j)</div><div class="line">                  <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(i, j) += fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point)</div><div class="line">                                       fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point)</div><div class="line">                                       fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line"></div><div class="line">                cell_rhs(i) += rhs_value</div><div class="line">                               fe_values.<a class="code" href="classFEValuesBase.html#a1dd48cb744013c448d57f8f77640c08d">shape_value</a>(i, q_point)</div><div class="line">                               fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">              }</div><div class="line">          }</div><div class="line"></div><div class="line">        cell-&gt;get_dof_indices(local_dof_indices);</div><div class="line">        constraints.<a class="code" href="classAffineConstraints.html#a373fbdacd8c486e675b8d2bff8943192">distribute_local_to_global</a>(cell_matrix,</div><div class="line">                                               cell_rhs,</div><div class="line">                                               local_dof_indices,</div><div class="line">                                               system_matrix,</div><div class="line">                                               system_rhs);</div><div class="line">      }</div></div><!-- fragment --><div class="fragment"><div class="line">  system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">  system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemsolve"></a> </p><h4>LaplaceProblem::solve</h4>
<p>Even though solving linear systems on potentially tens of thousands of processors is by far not a trivial job, the function that does this is</p>
<ul>
<li>at least at the outside</li>
<li>relatively simple. Most of the parts you've seen before. There are really only two things worth mentioning:</li>
<li>Solvers and preconditioners are built on the deal.II wrappers of PETSc and Trilinos functionality. It is relatively well known that the primary bottleneck of massively parallel linear solvers is not actually the communication between processors, but the fact that it is difficult to produce preconditioners that scale well to large numbers of processors. Over the second half of the first decade of the 21st century, it has become clear that algebraic multigrid (AMG) methods turn out to be extremely efficient in this context, and we will use one of them</li>
<li>either the BoomerAMG implementation of the Hypre package that can be interfaced to through PETSc, or a preconditioner provided by ML, which is part of Trilinos</li>
<li>for the current program. The rest of the solver itself is boilerplate and has been shown before. Since the linear system is symmetric and positive definite, we can use the CG method as the outer solver.</li>
<li>Ultimately, we want a vector that stores not only the elements of the solution for degrees of freedom the current processor owns, but also all other locally relevant degrees of freedom. On the other hand, the solver itself needs a vector that is uniquely split between processors, without any overlap. We therefore create a vector at the beginning of this function that has these properties, use it to solve the linear system, and only assign it to the vector we want at the very end. This last step ensures that all ghost elements are also copied as necessary.</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::solve()</div><div class="line">{</div><div class="line">  <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;solve&quot;</span>);</div><div class="line">  <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>    completely_distributed_solution(locally_owned_dofs,</div><div class="line">                                                  mpi_communicator);</div><div class="line"></div><div class="line">  <a class="code" href="classSolverControl.html">SolverControl</a> solver_control(dof_handler.n_dofs(), 1e-12);</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">  <a class="code" href="namespaceLinearAlgebraPETSc.html#a9d2c129d77c89b40badfb65d94c5b955">LA::SolverCG</a> solver(solver_control, mpi_communicator);</div><div class="line"><span class="preprocessor">#else</span></div><div class="line">  <a class="code" href="namespaceLinearAlgebraPETSc.html#a9d2c129d77c89b40badfb65d94c5b955">LA::SolverCG</a> solver(solver_control);</div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line">  <a class="code" href="namespaceLinearAlgebraPETSc_1_1MPI.html#a41f11f7a1992c6d6aa9367b12c68f791">LA::MPI::PreconditionAMG</a> preconditioner;</div><div class="line"></div><div class="line">  LA::MPI::PreconditionAMG::AdditionalData data;</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">  data.symmetric_operator = <span class="keyword">true</span>;</div><div class="line"><span class="preprocessor">#else</span></div><div class="line">  <span class="comment">// Trilinos defaults are good</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">  preconditioner.<a class="code" href="classPETScWrappers_1_1PreconditionBoomerAMG.html#a3c82672eac786d7f369b296439859ff2">initialize</a>(system_matrix, data);</div><div class="line"></div><div class="line">  solver.solve(system_matrix,</div><div class="line">               completely_distributed_solution,</div><div class="line">               system_rhs,</div><div class="line">               preconditioner);</div><div class="line"></div><div class="line">  pcout &lt;&lt; <span class="stringliteral">&quot;   Solved in &quot;</span> &lt;&lt; solver_control.last_step() &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span></div><div class="line">        &lt;&lt; std::endl;</div><div class="line"></div><div class="line">  constraints.<a class="code" href="classAffineConstraints.html#a7b3d3f295bb56d6cd6856bdc6cbe8a01">distribute</a>(completely_distributed_solution);</div><div class="line"></div><div class="line">  locally_relevant_solution = completely_distributed_solution;</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemrefine_grid"></a> </p><h4>LaplaceProblem::refine_grid</h4>
<p>The function that estimates the error and refines the grid is again almost exactly like the one in <a class="el" href="step_6.html">step-6</a> . The only difference is that the function that flags cells to be refined is now in namespace <a class="el" href="namespaceparallel_1_1distributed_1_1GridRefinement.html">parallel::distributed::GridRefinement</a></p>
<ul>
<li>a namespace that has functions that can communicate between all involved processors and determine global thresholds to use in deciding which cells to refine and which to coarsen. Note that we didn't have to do anything special about the <a class="el" href="classKellyErrorEstimator.html">KellyErrorEstimator</a> class: we just give it a vector with as many elements as the local triangulation has cells (locally owned cells, ghost cells, and artificial ones), but it only fills those entries that correspond to cells that are locally owned.</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::refine_grid()</div><div class="line">{</div><div class="line">  <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;refine&quot;</span>);</div><div class="line"></div><div class="line">  <a class="code" href="classVector.html">Vector&lt;float&gt;</a> estimated_error_per_cell(triangulation.<a class="code" href="classTriangulation.html#a5ea5c9957dbb566a562bbe2c0f3971e9">n_active_cells</a>());</div><div class="line">  <a class="code" href="classKellyErrorEstimator.html#ae2269e1c9903e9d863b7abd54948af00">KellyErrorEstimator&lt;dim&gt;::estimate</a>(</div><div class="line">    dof_handler,</div><div class="line">    <a class="code" href="classQGauss.html">QGauss</a>&lt;dim</div><div class="line"></div><div class="line">- 1&gt;(fe.degree + 1),</div><div class="line">    std::map&lt;<a class="code" href="classunsigned_01int.html">types::boundary_id</a>, <span class="keyword">const</span> <a class="code" href="classFunction.html">Function&lt;dim&gt;</a>&gt;(),</div><div class="line">    locally_relevant_solution,</div><div class="line">    estimated_error_per_cell);</div><div class="line">  <a class="code" href="namespaceparallel_1_1distributed_1_1GridRefinement.html#aa2ffb707a796ae6dedb75036606ef2e6">parallel::distributed::GridRefinement::refine_and_coarsen_fixed_number</a>(</div><div class="line">    triangulation, estimated_error_per_cell, 0.3, 0.03);</div><div class="line">  triangulation.<a class="code" href="classTriangulation.html#ac8b4fbb207303ec7f5ef758821ecd8cb">execute_coarsening_and_refinement</a>();</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemoutput_results"></a> </p><h4>LaplaceProblem::output_results</h4>
<p>Compared to the corresponding function in <a class="el" href="step_6.html">step-6</a> , the one here is a tad more complicated. There are two reasons: the first one is that we do not just want to output the solution but also for each cell which processor owns it (i.e. which "subdomain" it is in). Secondly, as discussed at length in <a class="el" href="step_17.html">step-17</a> and <a class="el" href="step_18.html">step-18</a> , generating graphical data can be a bottleneck in parallelizing. In <a class="el" href="step_18.html">step-18</a> , we have moved this step out of the actual computation but shifted it into a separate program that later combined the output from various processors into a single file. But this doesn't scale: if the number of processors is large, this may mean that the step of combining data on a single processor later becomes the longest running part of the program, or it may produce a file that's so large that it can't be visualized any more. We here follow a more sensible approach, namely creating individual files for each MPI process and leaving it to the visualization program to make sense of that. To start, the top of the function looks like it usually does. In addition to attaching the solution vector (the one that has entries for all locally relevant, not only the locally owned, elements), we attach a data vector that stores, for each cell, the subdomain the cell belongs to. This is slightly tricky, because of course not every processor knows about every cell. The vector we attach therefore has an entry for every cell that the current processor has in its mesh (locally owned ones, ghost cells, and artificial cells), but the <a class="el" href="classDataOut.html">DataOut</a> class will ignore all entries that correspond to cells that are not owned by the current processor. As a consequence, it doesn't actually matter what values we write into these vector entries: we simply fill the entire vector with the number of the current MPI process (i.e. the subdomain_id of the current process); this correctly sets the values we care for, i.e. the entries that correspond to locally owned cells, while providing the wrong value for all other elements</p>
<ul>
<li>but these are then ignored anyway.</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword"></span>{</div><div class="line">  <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">  data_out.<a class="code" href="classDataOut__DoFData.html#a6ed7c846331069f406b8c9933c37fda4">attach_dof_handler</a>(dof_handler);</div><div class="line">  data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(locally_relevant_solution, <span class="stringliteral">&quot;u&quot;</span>);</div><div class="line"></div><div class="line">  <a class="code" href="classVector.html">Vector&lt;float&gt;</a> subdomain(triangulation.<a class="code" href="classTriangulation.html#a5ea5c9957dbb566a562bbe2c0f3971e9">n_active_cells</a>());</div><div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; subdomain.size(); ++i)</div><div class="line">    subdomain(i) = triangulation.<a class="code" href="classTriangulation.html#a44ea82a097d8317c98fa422307aff874">locally_owned_subdomain</a>();</div><div class="line">  data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(subdomain, <span class="stringliteral">&quot;subdomain&quot;</span>);</div><div class="line"></div><div class="line">  data_out.<a class="code" href="classDataOut.html#a087f63e22f0614bca326dbdca288c646">build_patches</a>();</div></div><!-- fragment --><p>The next step is to write this data to disk. We write up to 8 VTU files in parallel with the help of MPI-IO. Additionally a PVTU record is generated, which groups the written VTU files.</p>
<div class="fragment"><div class="line">  data_out.<a class="code" href="classDataOutInterface.html#a0864e51eb173c87e2a3edc9391ea8009">write_vtu_with_pvtu_record</a>(</div><div class="line">    <span class="stringliteral">&quot;./&quot;</span>, <span class="stringliteral">&quot;solution&quot;</span>, cycle, mpi_communicator, 2, 8);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemrun"></a> </p><h4>LaplaceProblem::run</h4>
<p>The function that controls the overall behavior of the program is again like the one in <a class="el" href="step_6.html">step-6</a> . The minor difference are the use of <code>pcout</code> instead of <code>std::cout</code> for output to the console (see also <a class="el" href="step_17.html">step-17</a> ) and that we only generate graphical output if at most 32 processors are involved. Without this limit, it would be just too easy for people carelessly running this program without reading it first to bring down the cluster interconnect and fill any file system available :-) A functional difference to <a class="el" href="step_6.html">step-6</a> is the use of a square domain and that we start with a slightly finer mesh (5 global refinement cycles)</p>
<ul>
<li>there just isn't much of a point showing a massively parallel program starting on 4 cells (although admittedly the point is only slightly stronger starting on 1024).</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">LaplaceProblem&lt;dim&gt;::run</a>()</div><div class="line">{</div><div class="line">  pcout &lt;&lt; <span class="stringliteral">&quot;Running with &quot;</span></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">        &lt;&lt; <span class="stringliteral">&quot;PETSc&quot;</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">        &lt;&lt; <span class="stringliteral">&quot;Trilinos&quot;</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">        &lt;&lt; <span class="stringliteral">&quot; on &quot;</span> &lt;&lt; <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">Utilities::MPI::n_mpi_processes</a>(mpi_communicator)</div><div class="line">        &lt;&lt; <span class="stringliteral">&quot; MPI rank(s)...&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_cycles = 8;</div><div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle = 0; cycle &lt; n_cycles; ++cycle)</div><div class="line">    {</div><div class="line">      pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">        {</div><div class="line">          <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a>(triangulation);</div><div class="line">          triangulation.<a class="code" href="classTriangulation.html#a6ad0b3fb24aae17f4668427a433dea19">refine_global</a>(5);</div><div class="line">        }</div><div class="line">      <span class="keywordflow">else</span></div><div class="line">        refine_grid();</div><div class="line"></div><div class="line">      setup_system();</div><div class="line"></div><div class="line">      pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">            &lt;&lt; triangulation.<a class="code" href="classTriangulation.html#a584733c8499dbd140694bfe04e0963ca">n_global_active_cells</a>() &lt;&lt; std::endl</div><div class="line">            &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span> &lt;&lt; dof_handler.n_dofs()</div><div class="line">            &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      assemble_system();</div><div class="line">      solve();</div><div class="line"></div><div class="line">      <span class="keywordflow">if</span> (<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">Utilities::MPI::n_mpi_processes</a>(mpi_communicator) &lt;= 32)</div><div class="line">        {</div><div class="line">          <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;output&quot;</span>);</div><div class="line">          output_results(cycle);</div><div class="line">        }</div><div class="line"></div><div class="line">      computing_timer.print_summary();</div><div class="line">      computing_timer.reset();</div><div class="line"></div><div class="line">      pcout &lt;&lt; std::endl;</div><div class="line">    }</div><div class="line">}</div><div class="line">} <span class="comment">// namespace Step40</span></div></div><!-- fragment --><p><a class="anchor" id="main"></a> </p><h4>main()</h4>
<p>The final function, <code>main()</code> , again has the same structure as in all other programs, in particular <a class="el" href="step_6.html">step-6</a> . Like the other programs that use MPI, we have to initialize and finalize MPI, which is done using the helper object <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a>. The constructor of that class also initializes libraries that depend on MPI, such as p4est, PETSc, SLEPc, and Zoltan (though the last two are not used in this tutorial). The order here is important: we cannot use any of these libraries until they are initialized, so it does not make sense to do anything before creating an instance of <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a>.</p>
<p>After the solver finishes, the LaplaceProblem destructor will run followed by <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html#a812d3e3369e2c86ba94beb13368d92d7">Utilities::MPI::MPI_InitFinalize::~MPI_InitFinalize()</a>. This order is also important: <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html#a812d3e3369e2c86ba94beb13368d92d7">Utilities::MPI::MPI_InitFinalize::~MPI_InitFinalize()</a> calls <code>PetscFinalize</code> (and finalization functions for other libraries), which will delete any in-use PETSc objects. This must be done after we destruct the Laplace solver to avoid double deletion errors. Fortunately, due to the order of destructor call rules of C++, we do not need to worry about any of this: everything happens in the correct order (i.e., the reverse of the order of construction). The last function called by <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html#a812d3e3369e2c86ba94beb13368d92d7">Utilities::MPI::MPI_InitFinalize::~MPI_InitFinalize()</a> is <code>MPI_Finalize</code> : i.e., once this object is destructed the program should exit since MPI will no longer be available.</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, charargv[])</div><div class="line">{</div><div class="line"><span class="keywordflow">try</span></div><div class="line">  {</div><div class="line">    <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">    <span class="keyword">using namespace </span>Step40;</div><div class="line"></div><div class="line">    <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">    LaplaceProblem&lt;2&gt; laplace_problem_2d;</div><div class="line">    laplace_problem_2d.run();</div><div class="line">  }</div><div class="line"><span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">  {</div><div class="line">    std::cerr &lt;&lt; std::endl</div><div class="line">              &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line">    std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">              &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    <span class="keywordflow">return</span> 1;</div><div class="line">  }</div><div class="line"><span class="keywordflow">catch</span> (...)</div><div class="line">  {</div><div class="line">    std::cerr &lt;&lt; std::endl</div><div class="line">              &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line">    std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">              &lt;&lt; std::endl;</div><div class="line">    <span class="keywordflow">return</span> 1;</div><div class="line">  }</div><div class="line"></div><div class="line"><span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p> <a class="anchor" id="Results"></a></p><h1>Results</h1>
<p>When you run the program, on a single processor or with your local MPIinstallation on a few, you should get output like this: </p><div class="fragment"><div class="line">Cycle 0:</div><div class="line">Number of active cells:       1024</div><div class="line">Number of degrees of freedom: 4225</div><div class="line">Solved in 10 iterations.</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">+---------------------------------------------+------------+------------+</div><div class="line">| Total wallclock time elapsed since start    |     0.176s |            |</div><div class="line">|                                             |            |            |</div><div class="line">| Section                         | no. calls |  wall time | % of total |</div><div class="line">+---------------------------------+-----------+------------+------------+</div><div class="line">| assembly                        |         1 |    0.0209s |        12% |</div><div class="line">| output                          |         1 |    0.0189s |        11% |</div><div class="line">| setup                           |         1 |    0.0299s |        17% |</div><div class="line">| solve                           |         1 |    0.0419s |        24% |</div><div class="line">+---------------------------------+-----------+------------+------------+</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">Cycle 1:</div><div class="line">Number of active cells:       1954</div><div class="line">Number of degrees of freedom: 8399</div><div class="line">Solved in 10 iterations.</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">+---------------------------------------------+------------+------------+</div><div class="line">| Total wallclock time elapsed since start    |     0.327s |            |</div><div class="line">|                                             |            |            |</div><div class="line">| Section                         | no. calls |  wall time | % of total |</div><div class="line">+---------------------------------+-----------+------------+------------+</div><div class="line">| assembly                        |         1 |    0.0368s |        11% |</div><div class="line">| output                          |         1 |    0.0208s |       6.4% |</div><div class="line">| <a class="code" href="namespaceGridRefinement.html#a1cf30058b31ce7f9b389e8310bb9fc54">refine</a>                          |         1 |     0.157s |        48% |</div><div class="line">| setup                           |         1 |    0.0452s |        14% |</div><div class="line">| solve                           |         1 |    0.0668s |        20% |</div><div class="line">+---------------------------------+-----------+------------+------------+</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">Cycle 2:</div><div class="line">Number of active cells:       3664</div><div class="line">Number of degrees of freedom: 16183</div><div class="line">Solved in 11 iterations.</div><div class="line"></div><div class="line">...</div></div><!-- fragment --><p>The exact numbers differ, depending on how many processors we use;this is due to the fact that the preconditioner depends on thepartitioning of the problem, the solution then differs in the last fewdigits, and consequently the mesh refinement differs slightly.The primary thing to notice here, though, is that the number ofiterations does not increase with the size of the problem. Thisguarantees that we can efficiently solve even the largest problems. When run on a sufficiently large number of machines (say a fewthousand), this program can relatively easily solve problems with wellover one billion unknowns in less than a minute. On the other hand,such big problems can no longer be visualized, so we also ran theprogram on only 16 processors. Here are a mesh, along with itspartitioning onto the 16 processors, and the corresponding solution: </p><table width="100%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.mesh.png"/>
</div>
  </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.solution.png"/>
</div>
   </td></tr>
</table>
<p>The mesh on the left has a mere 7,069 cells. This is of course aproblem we would easily have been able to solve already on a singleprocessor using <a class="el" href="step_6.html">step-6</a> , but the point of the program was to show howto write a program that scales to many more machines. For example,here are two graphs that show how the run time of a large number of partsof the program scales on problems with around 52 and 375 million degrees offreedom if we take more and more processors (these and the next couple ofgraphs are taken from an earlier version of the <a class="el" href="DEALGlossary.html#distributed_paper">Distributed Computing paper</a> ; updated graphs showingdata of runs on even larger numbers of processors, and a lotmore interpretation can be found in the final version of the paper): </p><table width="100%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.strong2.png"/>
</div>
  </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.strong.png"/>
</div>
   </td></tr>
</table>
<p>While the strong scaling graphs above show that we can solve a problem offixed size faster and faster if we take more and more processors, the moreinteresting question may be how big problems can become so that they can stillbe solved within a reasonable time on a machine of a particular size. We showthis in the following two graphs for 256 and 4096 processors: </p><table width="100%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.256.png"/>
</div>
  </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.4096.png"/>
</div>
   </td></tr>
</table>
<p>What these graphs show is that all parts of the program scale linearly withthe number of degrees of freedom. This time, lines are wobbly at the left asthe size of local problems is too small. For more discussions of these resultswe refer to the <a class="el" href="DEALGlossary.html#distributed_paper">Distributed Computing paper</a>. So how large are the largest problems one can solve? At the time of writingthis problem, thelimiting factor is that the program uses the BoomerAMG algebraicmultigrid method from the <a href="http://acts.nersc.gov/hypre/" target="_top">Hypre package</a> asa preconditioner, which unfortunately uses signed 32-bit integers toindex the elements of a distributed matrix. This limits the size ofproblems to \(2^{31}-1=2,147,483,647\) degrees of freedom. From the graphsabove it is obvious that the scalability would extend beyond thisnumber, and one could expect that given more than the 4,096 machinesshown above would also further reduce the compute time. That said, onecan certainly expect that this limit will eventually be lifted by thehypre developers. On the other hand, this does not mean that deal.II cannot solve biggerproblems. Indeed, <a class="el" href="step_37.html">step-37</a> shows how one can solve problems that are notjust a little, but very substantially larger than anything we have shownhere.</p>
<p><a class="anchor" id="extensions"></a><a class="anchor" id="Possibilitiesforextensions"></a></p><h3>Possibilities for extensions</h3>
<p>In a sense, this program is the ultimate solver for the Laplaceequation: it can essentially solve the equation to whatever accuracyyou want, if only you have enough processors available. Since theLaplace equation by itself is not terribly interesting at this levelof accuracy, the more interesting possibilities for extensiontherefore concern not so much this program but what comes beyondit. For example, several of the other programs in this tutorial havesignificant run times, especially in 3d. It would therefore beinteresting to use the techniques explained here to extend otherprograms to support parallel distributed computations. We have donethis for <a class="el" href="step_31.html">step-31</a> in the <a class="el" href="step_32.html">step-32</a> tutorial program, but the same wouldapply to, for example, <a class="el" href="step_23.html">step-23</a> and <a class="el" href="step_25.html">step-25</a> for hyperbolic timedependent problems, <a class="el" href="step_33.html">step-33</a> for gas dynamics, or <a class="el" href="step_35.html">step-35</a> for theNavier-Stokes equations. Maybe equally interesting is the problem of postprocessing. Asmentioned above, we only show pictures of the solution and the meshfor 16 processors because 4,096 processors solving 1 billion unknownswould produce graphical output on the order of several 10gigabyte. Currently, no program is able to visualize this amount ofdata in any reasonable way unless it also runs on at least severalhundred processors. There are, however, approaches where visualizationprograms directly communicate with solvers on each processor with eachvisualization process rendering the part of the scene computed by thesolver on this processor. Implementing such an interface would allowto quickly visualize things that are otherwise not amenable tographical display.</p>
<p><a class="anchor" id="PlainProg"></a></p><h1>The plain program</h1>
<div class="fragment"><div class="line"><span class="comment">/* ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Copyright (C) 2009 - 2021 by the deal.II authors</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * This file is part of the deal.II library.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * The deal.II library is free software; you can use it, redistribute</span></div><div class="line"><span class="comment"> * it, and/or modify it under the terms of the GNU Lesser General</span></div><div class="line"><span class="comment"> * Public License as published by the Free Software Foundation; either</span></div><div class="line"><span class="comment"> * version 2.1 of the License, or (at your option) any later version.</span></div><div class="line"><span class="comment"> * The full text of the license can be found in the file LICENSE.md at</span></div><div class="line"><span class="comment"> * the top level directory of deal.II.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Author: Wolfgang Bangerth, Texas A&amp;M University, 2009, 2010</span></div><div class="line"><span class="comment"> *         Timo Heister, University of Goettingen, 2009, 2010</span></div><div class="line"><span class="comment"> */</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="quadrature__lib_8h.html">deal.II/base/quadrature_lib.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="function_8h.html">deal.II/base/function.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="timer_8h.html">deal.II/base/timer.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="generic__linear__algebra_8h.html">deal.II/lac/generic_linear_algebra.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>LA</div><div class="line">{</div><div class="line"><span class="preprocessor">#if defined(DEAL_II_WITH_PETSC) &amp;&amp; !defined(DEAL_II_PETSC_WITH_COMPLEX) &amp;&amp; \</span></div><div class="line"><span class="preprocessor">  !(defined(DEAL_II_WITH_TRILINOS) &amp;&amp; defined(FORCE_USE_OF_TRILINOS))</span></div><div class="line">  <span class="keyword">using namespace </span>dealii::LinearAlgebraPETSc;</div><div class="line"><span class="preprocessor">#  define USE_PETSC_LA</span></div><div class="line"><span class="preprocessor">#elif defined(DEAL_II_WITH_TRILINOS)</span></div><div class="line">  <span class="keyword">using namespace </span>dealii::LinearAlgebraTrilinos;</div><div class="line"><span class="preprocessor">#else</span></div><div class="line"><span class="preprocessor">#  error DEAL_II_WITH_PETSC or DEAL_II_WITH_TRILINOS required</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">} <span class="comment">// namespace LA</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8h.html">deal.II/lac/vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="full__matrix_8h.html">deal.II/lac/full_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="solver__cg_8h.html">deal.II/lac/solver_cg.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="affine__constraints_8h.html">deal.II/lac/affine_constraints.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dynamic__sparsity__pattern_8h.html">deal.II/lac/dynamic_sparsity_pattern.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__generator_8h.html">deal.II/grid/grid_generator.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dofs_2dof__handler_8h.html">deal.II/dofs/dof_handler.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__tools_8h.html">deal.II/dofs/dof_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe_2fe__values_8h.html">deal.II/fe/fe_values.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__q_8h.html">deal.II/fe/fe_q.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector__tools_8h.html">deal.II/numerics/vector_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="numerics_2data__out_8h.html">deal.II/numerics/data_out.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="error__estimator_8h.html">deal.II/numerics/error_estimator.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="include_2deal_8II_2base_2utilities_8h.html">deal.II/base/utilities.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="conditional__ostream_8h.html">deal.II/base/conditional_ostream.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="index__set_8h.html">deal.II/base/index_set.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="sparsity__tools_8h.html">deal.II/lac/sparsity_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="distributed_2tria_8h.html">deal.II/distributed/tria.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="distributed_2grid__refinement_8h.html">deal.II/distributed/grid_refinement.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>Step40</div><div class="line">{</div><div class="line">  <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keyword">class </span>LaplaceProblem</div><div class="line">  {</div><div class="line">  <span class="keyword">public</span>:</div><div class="line">    LaplaceProblem();</div><div class="line"></div><div class="line">    <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">run</a>();</div><div class="line"></div><div class="line">  <span class="keyword">private</span>:</div><div class="line">    <span class="keywordtype">void</span> setup_system();</div><div class="line">    <span class="keywordtype">void</span> assemble_system();</div><div class="line">    <span class="keywordtype">void</span> solve();</div><div class="line">    <span class="keywordtype">void</span> refine_grid();</div><div class="line">    <span class="keywordtype">void</span> output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div><div class="line"></div><div class="line">    <a class="code" href="classMPI__Comm.html">MPI_Comm</a> mpi_communicator;</div><div class="line"></div><div class="line">    <a class="code" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation&lt;dim&gt;</a> <a class="code" href="p4est__wrappers_8cc.html#ace00f2f80d9780ef9aa1007e1c22c6a4">triangulation</a>;</div><div class="line"></div><div class="line">    <a class="code" href="classFE__Q.html">FE_Q&lt;dim&gt;</a>       fe;</div><div class="line">    <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a> dof_handler;</div><div class="line"></div><div class="line">    <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs;</div><div class="line">    <a class="code" href="classIndexSet.html">IndexSet</a> locally_relevant_dofs;</div><div class="line"></div><div class="line">    <a class="code" href="classAffineConstraints.html">AffineConstraints&lt;double&gt;</a> constraints;</div><div class="line"></div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a912abe2208022aec6753876bcc72f6bf">LA::MPI::SparseMatrix</a> system_matrix;</div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>       locally_relevant_solution;</div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>       system_rhs;</div><div class="line"></div><div class="line">    <a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div><div class="line">    <a class="code" href="classTimerOutput.html">TimerOutput</a>        computing_timer;</div><div class="line">  };</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  LaplaceProblem&lt;dim&gt;::LaplaceProblem()</div><div class="line">    : mpi_communicator(MPI_COMM_WORLD)</div><div class="line">    , triangulation(mpi_communicator,</div><div class="line">                    typename <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::MeshSmoothing(</div><div class="line">                      <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::smoothing_on_refinement |</div><div class="line">                      <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::smoothing_on_coarsening))</div><div class="line">    , fe(2)</div><div class="line">    , dof_handler(triangulation)</div><div class="line">    , pcout(<a class="code" href="namespacestd.html">std</a>::cout,</div><div class="line">            (<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator) == 0))</div><div class="line">    , computing_timer(mpi_communicator,</div><div class="line">                      pcout,</div><div class="line">                      <a class="code" href="classTimerOutput.html">TimerOutput</a>::summary,</div><div class="line">                      <a class="code" href="classTimerOutput.html">TimerOutput</a>::wall_times)</div><div class="line">  {}</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::setup_system()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;setup&quot;</span>);</div><div class="line"></div><div class="line">    dof_handler.distribute_dofs(fe);</div><div class="line"></div><div class="line">    locally_owned_dofs = dof_handler.locally_owned_dofs();</div><div class="line">    <a class="code" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs</a>(dof_handler, locally_relevant_dofs);</div><div class="line"></div><div class="line">    locally_relevant_solution.reinit(locally_owned_dofs,</div><div class="line">                                     locally_relevant_dofs,</div><div class="line">                                     mpi_communicator);</div><div class="line">    system_rhs.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line"></div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#addd15bc409c61d6f795f0132c574335b">clear</a>();</div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#a2c9d71b5b7e8851c25a411ccf34de986">reinit</a>(locally_relevant_dofs);</div><div class="line">    <a class="code" href="group__constraints.html#ga3b4ea7dfd313e388d868c4e4aa685799">DoFTools::make_hanging_node_constraints</a>(dof_handler, constraints);</div><div class="line">    <a class="code" href="namespaceVectorTools.html#af27ac28c698a9ed0199faed50a204538">VectorTools::interpolate_boundary_values</a>(dof_handler,</div><div class="line">                                             0,</div><div class="line">                                             <a class="code" href="classFunctions_1_1ZeroFunction.html">Functions::ZeroFunction&lt;dim&gt;</a>(),</div><div class="line">                                             constraints);</div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#a1611aa37f754086388ca76bcd421cce5">close</a>();</div><div class="line"></div><div class="line">    <a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(locally_relevant_dofs);</div><div class="line"></div><div class="line">    <a class="code" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a>(dof_handler, dsp, constraints, <span class="keyword">false</span>);</div><div class="line">    <a class="code" href="namespaceSparsityTools.html#afbc0c7a206ced91b154666215ea3c218">SparsityTools::distribute_sparsity_pattern</a>(dsp,</div><div class="line">                                               dof_handler.locally_owned_dofs(),</div><div class="line">                                               mpi_communicator,</div><div class="line">                                               locally_relevant_dofs);</div><div class="line"></div><div class="line">    system_matrix.reinit(locally_owned_dofs,</div><div class="line">                         locally_owned_dofs,</div><div class="line">                         dsp,</div><div class="line">                         mpi_communicator);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::assemble_system()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;assembly&quot;</span>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a> quadrature_formula(fe.degree + 1);</div><div class="line"></div><div class="line">    <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values(fe,</div><div class="line">                            quadrature_formula,</div><div class="line">                            <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                              <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> dofs_per_cell = fe.n_dofs_per_cell();</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_q_points    = quadrature_formula.<a class="code" href="classQuadrature.html#af9f7d82770fa8126e19113f3e3db755b">size</a>();</div><div class="line"></div><div class="line">    <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a> <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(dofs_per_cell, dofs_per_cell);</div><div class="line">    Vector&lt;double&gt;     cell_rhs(dofs_per_cell);</div><div class="line"></div><div class="line">    std::vector&lt;types::global_dof_index&gt; local_dof_indices(dofs_per_cell);</div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;cell : dof_handler.<a class="code" href="group__CPP11.html#gacdb6d3adec96a13a7079f6c893e1d3ff">active_cell_iterators</a>())</div><div class="line">      <span class="keywordflow">if</span> (cell-&gt;is_locally_owned())</div><div class="line">        {</div><div class="line">          cell_matrix = 0.;</div><div class="line">          cell_rhs    = 0.;</div><div class="line"></div><div class="line">          fe_values.<a class="code" href="classFEValues.html#a21f914e63d588e2652a9514620653d77">reinit</a>(cell);</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points; ++q_point)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">double</span> rhs_value =</div><div class="line">                (fe_values.<a class="code" href="classFEValuesBase.html#ab123e5da03736be4977c76fbcb6a2e37">quadrature_point</a>(q_point)[1] &gt;</div><div class="line">                     0.5 +</div><div class="line">                       0.25 * <a class="code" href="vectorization_8h.html#ad9b7aa5c50bf9ce988a0f756a3f2baa5">std::sin</a>(4.0 * <a class="code" href="namespacenumbers.html#a3e24f194a9cb9b6ff4442b8a7a877d4a">numbers::PI</a> *</div><div class="line">                                       fe_values.<a class="code" href="classFEValuesBase.html#ab123e5da03736be4977c76fbcb6a2e37">quadrature_point</a>(q_point)[0]) ?</div><div class="line">                   1. :</div><div class="line">                   -1.);</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">                {</div><div class="line">                  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j = 0; j &lt; dofs_per_cell; ++j)</div><div class="line">                    <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(i, j) += fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point) *</div><div class="line">                                         fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point) *</div><div class="line">                                         fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line"></div><div class="line">                  cell_rhs(i) += rhs_value *                         </div><div class="line">                                 fe_values.<a class="code" href="classFEValuesBase.html#a1dd48cb744013c448d57f8f77640c08d">shape_value</a>(i, q_point) * </div><div class="line">                                 fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">                }</div><div class="line">            }</div><div class="line"></div><div class="line">          cell-&gt;get_dof_indices(local_dof_indices);</div><div class="line">          constraints.<a class="code" href="classAffineConstraints.html#a373fbdacd8c486e675b8d2bff8943192">distribute_local_to_global</a>(cell_matrix,</div><div class="line">                                                 cell_rhs,</div><div class="line">                                                 local_dof_indices,</div><div class="line">                                                 system_matrix,</div><div class="line">                                                 system_rhs);</div><div class="line">        }</div><div class="line"></div><div class="line">    system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">    system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::solve()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;solve&quot;</span>);</div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>    completely_distributed_solution(locally_owned_dofs,</div><div class="line">                                                    mpi_communicator);</div><div class="line"></div><div class="line">    <a class="code" href="classSolverControl.html">SolverControl</a> solver_control(dof_handler.n_dofs(), 1e-12);</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc.html#a9d2c129d77c89b40badfb65d94c5b955">LA::SolverCG</a> solver(solver_control, mpi_communicator);</div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc.html#a9d2c129d77c89b40badfb65d94c5b955">LA::SolverCG</a> solver(solver_control);</div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc_1_1MPI.html#a41f11f7a1992c6d6aa9367b12c68f791">LA::MPI::PreconditionAMG</a> preconditioner;</div><div class="line"></div><div class="line">    LA::MPI::PreconditionAMG::AdditionalData data;</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">    data.symmetric_operator = <span class="keyword">true</span>;</div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">/* Trilinos defaults are good */</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">    preconditioner.<a class="code" href="classPETScWrappers_1_1PreconditionBoomerAMG.html#a3c82672eac786d7f369b296439859ff2">initialize</a>(system_matrix, data);</div><div class="line"></div><div class="line">    solver.solve(system_matrix,</div><div class="line">                 completely_distributed_solution,</div><div class="line">                 system_rhs,</div><div class="line">                 preconditioner);</div><div class="line"></div><div class="line">    pcout &lt;&lt; <span class="stringliteral">&quot;   Solved in &quot;</span> &lt;&lt; solver_control.last_step() &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span></div><div class="line">          &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#a7b3d3f295bb56d6cd6856bdc6cbe8a01">distribute</a>(completely_distributed_solution);</div><div class="line"></div><div class="line">    locally_relevant_solution = completely_distributed_solution;</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::refine_grid()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;refine&quot;</span>);</div><div class="line"></div><div class="line">    Vector&lt;float&gt; estimated_error_per_cell(triangulation.<a class="code" href="classTriangulation.html#a5ea5c9957dbb566a562bbe2c0f3971e9">n_active_cells</a>());</div><div class="line">    <a class="code" href="classKellyErrorEstimator.html#ae2269e1c9903e9d863b7abd54948af00">KellyErrorEstimator&lt;dim&gt;::estimate</a>(</div><div class="line">      dof_handler,</div><div class="line">      <a class="code" href="classQGauss.html">QGauss&lt;dim - 1&gt;</a>(fe.degree + 1),</div><div class="line">      std::map&lt;<a class="code" href="classunsigned_01int.html">types::boundary_id</a>, <span class="keyword">const</span> <a class="code" href="classFunction.html">Function&lt;dim&gt;</a> *&gt;(),</div><div class="line">      locally_relevant_solution,</div><div class="line">      estimated_error_per_cell);</div><div class="line">    <a class="code" href="namespaceparallel_1_1distributed_1_1GridRefinement.html#aa2ffb707a796ae6dedb75036606ef2e6">parallel::distributed::GridRefinement::refine_and_coarsen_fixed_number</a>(</div><div class="line">      triangulation, estimated_error_per_cell, 0.3, 0.03);</div><div class="line">    triangulation.<a class="code" href="classTriangulation.html#ac8b4fbb207303ec7f5ef758821ecd8cb">execute_coarsening_and_refinement</a>();</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a6ed7c846331069f406b8c9933c37fda4">attach_dof_handler</a>(dof_handler);</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(locally_relevant_solution, <span class="stringliteral">&quot;u&quot;</span>);</div><div class="line"></div><div class="line">    Vector&lt;float&gt; subdomain(triangulation.<a class="code" href="classTriangulation.html#a5ea5c9957dbb566a562bbe2c0f3971e9">n_active_cells</a>());</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; subdomain.size(); ++i)</div><div class="line">      subdomain(i) = triangulation.<a class="code" href="classTriangulation.html#a44ea82a097d8317c98fa422307aff874">locally_owned_subdomain</a>();</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(subdomain, <span class="stringliteral">&quot;subdomain&quot;</span>);</div><div class="line"></div><div class="line">    data_out.<a class="code" href="classDataOut.html#a087f63e22f0614bca326dbdca288c646">build_patches</a>();</div><div class="line"></div><div class="line">    data_out.<a class="code" href="classDataOutInterface.html#a0864e51eb173c87e2a3edc9391ea8009">write_vtu_with_pvtu_record</a>(</div><div class="line">      <span class="stringliteral">&quot;./&quot;</span>, <span class="stringliteral">&quot;solution&quot;</span>, cycle, mpi_communicator, 2, 8);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">LaplaceProblem&lt;dim&gt;::run</a>()</div><div class="line">  {</div><div class="line">    pcout &lt;&lt; <span class="stringliteral">&quot;Running with &quot;</span></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot;PETSc&quot;</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot;Trilinos&quot;</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot; on &quot;</span> &lt;&lt; <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">Utilities::MPI::n_mpi_processes</a>(mpi_communicator)</div><div class="line">          &lt;&lt; <span class="stringliteral">&quot; MPI rank(s)...&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_cycles = 8;</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle = 0; cycle &lt; n_cycles; ++cycle)</div><div class="line">      {</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">          {</div><div class="line">            <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a>(triangulation);</div><div class="line">            triangulation.<a class="code" href="classTriangulation.html#a6ad0b3fb24aae17f4668427a433dea19">refine_global</a>(5);</div><div class="line">          }</div><div class="line">        <span class="keywordflow">else</span></div><div class="line">          refine_grid();</div><div class="line"></div><div class="line">        setup_system();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">              &lt;&lt; triangulation.<a class="code" href="classTriangulation.html#a584733c8499dbd140694bfe04e0963ca">n_global_active_cells</a>() &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span> &lt;&lt; dof_handler.n_dofs()</div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        assemble_system();</div><div class="line">        solve();</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">Utilities::MPI::n_mpi_processes</a>(mpi_communicator) &lt;= 32)</div><div class="line">          {</div><div class="line">            <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;output&quot;</span>);</div><div class="line">            output_results(cycle);</div><div class="line">          }</div><div class="line"></div><div class="line">        computing_timer.print_summary();</div><div class="line">        computing_timer.reset();</div><div class="line"></div><div class="line">        pcout &lt;&lt; std::endl;</div><div class="line">      }</div><div class="line">  }</div><div class="line">} <span class="comment">// namespace Step40</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> *argv[])</div><div class="line">{</div><div class="line">  <span class="keywordflow">try</span></div><div class="line">    {</div><div class="line">      <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">      <span class="keyword">using namespace </span>Step40;</div><div class="line"></div><div class="line">      <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">      LaplaceProblem&lt;2&gt; laplace_problem_2d;</div><div class="line">      laplace_problem_2d.run();</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (...)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p>This tutorial depends on <a class="el" href="step_6.html">step-6</a>.</p>
<p> 
<table class="tutorial" width="50%">
<tr><th colspan="2"><b><small>Table of contents</small></b></th></tr>
<tr><td width="50%" valign="top">
<ol>
  <li> <a href="#Intro" class=bold>Introduction</a>
    <ul>
        <li><a href="#Thetestcase">The testcase</a>
    </ul>
  <li> <a href="#CommProg" class=bold>The commented program</a>
    <ul>
        <li><a href="#Includefiles">Include files</a>
        <li><a href="#ThecodeLaplaceProblemcodeclasstemplate">The <code>LaplaceProblem</code> class template</a>
        <li><a href="#ThecodeLaplaceProblemcodeclassimplementation">The <code>LaplaceProblem</code> class implementation</a>
      <ul>
        <li><a href="#Constructor">Constructor</a>
        <li><a href="#LaplaceProblemsetup_system">LaplaceProblem::setup_system</a>
        <li><a href="#LaplaceProblemassemble_system">LaplaceProblem::assemble_system</a>
        <li><a href="#LaplaceProblemsolve">LaplaceProblem::solve</a>
        <li><a href="#LaplaceProblemrefine_grid">LaplaceProblem::refine_grid</a>
        <li><a href="#LaplaceProblemoutput_results">LaplaceProblem::output_results</a>
        <li><a href="#LaplaceProblemrun">LaplaceProblem::run</a>
        <li><a href="#main">main()</a>
      </ul>
      </ul>
</ol></td><td width="50%" valign="top"><ol>
  <li value="3"> <a href="#Results" class=bold>Results</a>
    <ul>
        <li><a href="#Possibilitiesforextensions">Possibilities for extensions</a>
    </ul>
  <li> <a href="#PlainProg" class=bold>The plain program</a>
</ol> </td> </tr> </table>
 examples/step-40/doc/intro.dox</p>
<p><br />
</p>
<p><em>This program was contributed by Timo Heister, Martin Kronbichler and Wolfgang Bangerth. <br />
 This material is based upon work partly supported by the National Science Foundation under Award No. EAR-0426271 and The California Institute of Technology. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author and do not necessarily reflect the views of the National Science Foundation or of The California Institute of Technology. </em></p>
<dl class="section note"><dt>Note</dt><dd>作为这个程序的前提条件，你需要同时安装PETSc和p4est库。在<a href="../../readme.html" target="body">README</a>文件中描述了deal.II与这两个附加库的安装。还要注意的是，为了正常工作，本程序需要访问实现代数多网格的Hypre预处理程序包；它可以作为PETSc的一部分安装，但必须在配置PETSc时明确启用；参见PETSc安装说明中的链接页面。</dd></dl>
<p><a class="anchor" id="Intro"></a></p>
<p><a class="anchor" id="Introduction"></a></p><h1>Introduction</h1>
<dl class="section note"><dt>Note</dt><dd>The material presented here is also discussed in <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.5.html">video lecture 41.5</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.75.html">video lecture 41.75</a>. (All video lectures are also available <a href="http://www.math.colostate.edu/~bangerth/videos.html">here</a>.)</dd></dl>
<p>鉴于今天的计算机，大多数有限元计算可以在一台机器上完成。因此，以前的大多数教程程序只显示了这一点，可能是在一些处理器之间进行分工，但这些处理器都可以访问相同的共享内存空间。也就是说，有些问题对于单台机器来说实在是太大了，在这种情况下，必须以适当的方式将问题分割给多台机器，每台机器都为整体贡献自己的一部分。在第17步和第18步中展示了一个简单的方法，我们展示了一个程序如何使用<a href="http://www.mpi-forum.org/" target="_top">MPI</a>来并行组装线性系统，存储它，解决它，并计算误差估计。所有这些操作的扩展都是相对微不足道的（关于操作 "扩展 "的定义，见 <a class="el" href="DEALGlossary.html#GlossParallelScaling">本词汇表条目</a>），但是有一个明显的缺点：为了使这个实现适度简单，每个MPI处理器都必须保留自己的整个Triangulation和DoFHandler对象的副本。因此，虽然我们可以怀疑（有充分的理由）上面列出的操作可以扩展到成千上万的计算机和数十亿个单元和数十亿个自由度的问题规模，但在每一个最后的处理器上为这成千上万的计算机所解决的整个问题建立一个大的网格显然是不能扩展的：这将需要永远，也许更重要的是没有一台机器会有足够的内存来存储一个有十亿个单元的网格（至少在写这篇文章时没有）。在现实中，像第17步和第18步这样的程序不可能在超过100或200个处理器上运行，即使在那里，存储Triangulation和DoFHandler对象也会消耗每台机器上的绝大部分内存。</p>
<p>因此，我们需要以不同的方式来处理这个问题：为了扩展到非常大的问题，每个处理器只能存储自己的一小块三角形和DoFHandler对象。deal.II在 <a class="el" href="namespaceparallel_1_1distributed.html">parallel::distributed</a> 命名空间和其中的类中实现了这样一个方案。它建立在一个外部库上，<a href="http://www.p4est.org/">p4est</a>（对表达式<em>parallel forest</em>的发挥，描述了将分层构造的网格作为四叉树或八叉树的森林进行并行存储）。你需要<a href="../../external-libs/p4est.html">install and configure p4est</a>，但除此之外，它的所有工作原理都隐藏在deal.II的表面之下。</p>
<p>本质上， <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> 类和DoFHandler类中的代码所做的是分割全局网格，使每个处理器只存储其 "拥有 "的一小部分，以及围绕其拥有的单元的一层 "幽灵 "单元。在我们想要解决偏微分方程的领域的其余部分发生了什么，对每个处理器来说都是未知的，如果需要这些信息，只能通过与其他机器的交流来推断。这意味着我们还必须以不同于例如第17步和第18步的方式来思考问题：例如，没有一个处理器可以拥有用于后处理的整个解矢量，程序的每一部分都必须被并行化，因为没有一个处理器拥有顺序操作所需的所有信息。</p>
<p>在 <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> 文档模块中描述了这种并行化如何发生的一般概述。在阅读本程序的源代码之前，你应该先阅读它，以获得一个顶层的概述。在 <a class="el" href="DEALGlossary.html#distributed_paper">分布式计算论文 </a>中也提供了关于我们将在程序中使用的许多术语的简明讨论。也许值得一读，以了解本程序内部如何工作的背景信息。</p>
<p><a class="anchor" id="Thetestcase"></a></p><h3>The testcase</h3>
<p>这个程序基本上重新解决了我们在步骤6中已经做的事情，即它解决了拉普拉斯方程</p>
<p class="formulaDsp">
\begin{align*} -\Delta u &amp;= f \qquad &amp;&amp;\text{in}\ \Omega=[0,1]^2, \\ u &amp;= 0 \qquad &amp;&amp;\text{on}\ \partial\Omega. \end{align*}
</p>
<p>当然不同的是，现在我们要在一个可能有十亿个单元，有十亿个左右自由度的网格上这样做。毫无疑问，对于这样一个简单的问题，这样做是完全愚蠢的，但毕竟一个教程程序的重点不是做一些有用的东西，而是展示如何使用deal.II来实现有用的程序。尽管如此，为了使事情至少有一点点有趣，我们选择右侧为一个不连续的函数。</p>
<p class="formulaDsp">
\begin{align*} f(x,y) = \left\{ \begin{array}{ll} 1 &amp; \text{if}\ y &gt; \frac 12 + \frac 14 \sin(4\pi x), \\ -1 &amp; \text{otherwise}, \end{array} \right. \end{align*}
</p>
<p>使得解沿着蜿蜒穿过域的正弦线有一个奇点。因此，网格的细化将集中在这条线上。你可以在下面结果部分的网格图中看到这一点。</p>
<p>与其在这里继续做冗长的介绍，不如让我们直接进入程序代码。如果你已经读完了步骤6和 <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> 文档模块，大部分将要发生的事情你应该已经熟悉了。事实上，比较这两个程序，你会发现在parallel中工作所需的额外努力几乎是微不足道的：这两个程序的代码行数差不多（尽管步骤6在处理系数和输出方面花费了更多的空间）。在任何情况下，下面的评论将只针对使<a class="el" href="step_40.html">step-40</a>与<a class="el" href="step_6.html">step-6</a>不同的事情，而且在 <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> 文档模块中还没有涵盖。</p>
<dl class="section note"><dt>Note</dt><dd>这个程序将能够在你想扔给它的多少个处理器上进行计算，以及你有多少内存和耐心来解决多大的问题。然而，<em>is</em>有一个限制：未知数的数量不能超过可以用类型 <a class="el" href="namespacetypes.html#a3543786f7dc7c57385fc923a6afd5917">types::global_dof_index</a>. 的对象存储的最大数量。默认情况下，这是<code>unsigned int</code>的别名，在今天大多数机器上是一个32位的整数，限制了你大约40亿（实际上，由于这个程序使用PETSc，你将被限制在一半，因为PETSc使用有符号整数）。然而，这可以在配置过程中改变为使用64位整数，见ReadMe文件。这将使问题的大小在短期内不太可能超过。</dd></dl>
<p><a class="anchor" id="CommProg"></a> </p><h1>The commented program</h1>
<p><a class="anchor" id="Includefiles"></a> </p><h3>Include files</h3>
<p>Most of the include files we need for this program have already been discussed in previous programs. In particular, all of the following should already be familiar friends:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="quadrature__lib_8h.html">deal.II/base/quadrature_lib.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="function_8h.html">deal.II/base/function.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="timer_8h.html">deal.II/base/timer.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="generic__linear__algebra_8h.html">deal.II/lac/generic_linear_algebra.h</a>&gt;</span></div></div><!-- fragment --><p>This program can use either PETSc or Trilinos for its parallel algebra needs. By default, if deal.II has been configured with PETSc, it will use PETSc. Otherwise, the following few lines will check that deal.II has been configured with Trilinos and take that.</p>
<p>But there may be cases where you want to use Trilinos, even though deal.II has <em>also</em> been configured with PETSc, for example to compare the performance of these two libraries. To do this, add the following #define to the source code: </p><div class="CodeFragmentInTutorialComment"> <div class="fragment"><div class="line"><span class="preprocessor">#define FORCE_USE_OF_TRILINOS</span></div></div><!-- fragment --> </div><p>Using this logic, the following lines will then import either the PETSc or Trilinos wrappers into the namespace <code>LA</code> (for "linear algebra). In the former case, we are also defining the macro <code>USE_PETSC_LA</code> so that we can detect if we are using PETSc (see solve() for an example where this is necessary).</p>
<div class="fragment"><div class="line"><span class="keyword">namespace </span>LA</div><div class="line">{</div><div class="line"><span class="preprocessor">#if defined(DEAL_II_WITH_PETSC) &amp;&amp; !defined(DEAL_II_PETSC_WITH_COMPLEX) &amp;&amp; \</span></div><div class="line"><span class="preprocessor">  !(defined(DEAL_II_WITH_TRILINOS) &amp;&amp; defined(FORCE_USE_OF_TRILINOS))</span></div><div class="line">  <span class="keyword">using namespace </span>dealii::LinearAlgebraPETSc;</div><div class="line"><span class="preprocessor">#  define USE_PETSC_LA</span></div><div class="line"><span class="preprocessor">#elif defined(DEAL_II_WITH_TRILINOS)</span></div><div class="line">  <span class="keyword">using namespace </span>dealii::LinearAlgebraTrilinos;</div><div class="line"><span class="preprocessor">#else</span></div><div class="line"><span class="preprocessor">#  error DEAL_II_WITH_PETSC or DEAL_II_WITH_TRILINOS required</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">} <span class="comment">// namespace LA</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8h.html">deal.II/lac/vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="full__matrix_8h.html">deal.II/lac/full_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="solver__cg_8h.html">deal.II/lac/solver_cg.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="affine__constraints_8h.html">deal.II/lac/affine_constraints.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dynamic__sparsity__pattern_8h.html">deal.II/lac/dynamic_sparsity_pattern.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__generator_8h.html">deal.II/grid/grid_generator.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dofs_2dof__handler_8h.html">deal.II/dofs/dof_handler.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__tools_8h.html">deal.II/dofs/dof_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe_2fe__values_8h.html">deal.II/fe/fe_values.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__q_8h.html">deal.II/fe/fe_q.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector__tools_8h.html">deal.II/numerics/vector_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="numerics_2data__out_8h.html">deal.II/numerics/data_out.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="error__estimator_8h.html">deal.II/numerics/error_estimator.h</a>&gt;</span></div></div><!-- fragment --><p>The following, however, will be new or be used in new roles. Let's walk through them. The first of these will provide the tools of the <a class="el" href="namespaceUtilities_1_1System.html">Utilities::System</a> namespace that we will use to query things like the number of processors associated with the current MPI universe, or the number within this universe the processor this job runs on is:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="include_2deal_8II_2base_2utilities_8h.html">deal.II/base/utilities.h</a>&gt;</span></div></div><!-- fragment --><p>The next one provides a class, ConditionOStream that allows us to write code that would output things to a stream (such as <code>std::cout</code> on every processor but throws the text away on all but one of them. We could achieve the same by simply putting an <code>if</code> statement in front of each place where we may generate output, but this doesn't make the code any prettier. In addition, the condition whether this processor should or should not produce output to the screen is the same every time &ndash; and consequently it should be simple enough to put it into the statements that generate output itself.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="conditional__ostream_8h.html">deal.II/base/conditional_ostream.h</a>&gt;</span></div></div><!-- fragment --><p>After these preliminaries, here is where it becomes more interesting. As mentioned in the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> module, one of the fundamental truths of solving problems on large numbers of processors is that there is no way for any processor to store everything (e.g. information about all cells in the mesh, all degrees of freedom, or the values of all elements of the solution vector). Rather, every processor will <em>own</em> a few of each of these and, if necessary, may <em>know</em> about a few more, for example the ones that are located on cells adjacent to the ones this processor owns itself. We typically call the latter <em>ghost cells</em>, <em>ghost nodes</em> or <em>ghost elements of a vector</em>. The point of this discussion here is that we need to have a way to indicate which elements a particular processor owns or need to know of. This is the realm of the <a class="el" href="classIndexSet.html">IndexSet</a> class: if there are a total of \(N\) cells, degrees of freedom, or vector elements, associated with (non-negative) integral indices \([0,N)\), then both the set of elements the current processor owns as well as the (possibly larger) set of indices it needs to know about are subsets of the set \([0,N)\). <a class="el" href="classIndexSet.html">IndexSet</a> is a class that stores subsets of this set in an efficient format:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="index__set_8h.html">deal.II/base/index_set.h</a>&gt;</span></div></div><!-- fragment --><p>The next header file is necessary for a single function, <a class="el" href="namespaceSparsityTools.html#afbc0c7a206ced91b154666215ea3c218">SparsityTools::distribute_sparsity_pattern</a>. The role of this function will be explained below.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="sparsity__tools_8h.html">deal.II/lac/sparsity_tools.h</a>&gt;</span></div></div><!-- fragment --><p>The final two, new header files provide the class <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> that provides meshes distributed across a potentially very large number of processors, while the second provides the namespace <a class="el" href="namespaceparallel_1_1distributed_1_1GridRefinement.html">parallel::distributed::GridRefinement</a> that offers functions that can adaptively refine such distributed meshes:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="distributed_2tria_8h.html">deal.II/distributed/tria.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="distributed_2grid__refinement_8h.html">deal.II/distributed/grid_refinement.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>Step40</div><div class="line">{</div><div class="line">  <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div></div><!-- fragment --><p><a class="anchor" id="ThecodeLaplaceProblemcodeclasstemplate"></a> </p><h3>The <code>LaplaceProblem</code> class template</h3>
<p>Next let's declare the main class of this program. Its structure is almost exactly that of the <a class="el" href="step_6.html">step-6</a> tutorial program. The only significant differences are:</p><ul>
<li>The <code>mpi_communicator</code> variable that describes the set of processors we want this code to run on. In practice, this will be MPI_COMM_WORLD, i.e. all processors the batch scheduling system has assigned to this particular job.</li>
<li>The presence of the <code>pcout</code> variable of type ConditionOStream.</li>
<li>The obvious use of <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> instead of <a class="el" href="classTriangulation.html">Triangulation</a>.</li>
<li>The presence of two <a class="el" href="classIndexSet.html">IndexSet</a> objects that denote which sets of degrees of freedom (and associated elements of solution and right hand side vectors) we own on the current processor and which we need (as ghost elements) for the algorithms in this program to work.</li>
<li>The fact that all matrices and vectors are now distributed. We use either the PETSc or Trilinos wrapper classes so that we can use one of the sophisticated preconditioners offered by Hypre (with PETSc) or ML (with Trilinos). Note that as part of this class, we store a solution vector that does not only contain the degrees of freedom the current processor owns, but also (as ghost elements) all those vector elements that correspond to "locally relevant" degrees of freedom (i.e. all those that live on locally owned cells or the layer of ghost cells that surround it).</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keyword">class </span>LaplaceProblem</div><div class="line">{</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  LaplaceProblem();</div><div class="line"></div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">run</a>();</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line">  <span class="keywordtype">void</span> setup_system();</div><div class="line">  <span class="keywordtype">void</span> assemble_system();</div><div class="line">  <span class="keywordtype">void</span> solve();</div><div class="line">  <span class="keywordtype">void</span> refine_grid();</div><div class="line">  <span class="keywordtype">void</span> output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div><div class="line"></div><div class="line">  <a class="code" href="classMPI__Comm.html">MPI_Comm</a> mpi_communicator;</div><div class="line"></div><div class="line">  <a class="code" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation&lt;dim&gt;</a> <a class="code" href="p4est__wrappers_8cc.html#ace00f2f80d9780ef9aa1007e1c22c6a4">triangulation</a>;</div><div class="line"></div><div class="line">  <a class="code" href="classFE__Q.html">FE_Q&lt;dim&gt;</a>       fe;</div><div class="line">  <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a> dof_handler;</div><div class="line"></div><div class="line">  <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs;</div><div class="line">  <a class="code" href="classIndexSet.html">IndexSet</a> locally_relevant_dofs;</div><div class="line"></div><div class="line">  <a class="code" href="classAffineConstraints.html">AffineConstraints&lt;double&gt;</a> constraints;</div><div class="line"></div><div class="line">  <a class="code" href="namespaceLinearAlgebraDealII.html#a912abe2208022aec6753876bcc72f6bf">LA::MPI::SparseMatrix</a> system_matrix;</div><div class="line">  <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>       locally_relevant_solution;</div><div class="line">  <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>       system_rhs;</div><div class="line"></div><div class="line">  <a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div><div class="line">  <a class="code" href="classTimerOutput.html">TimerOutput</a>        computing_timer;</div><div class="line">};</div></div><!-- fragment --><p><a class="anchor" id="ThecodeLaplaceProblemcodeclassimplementation"></a> </p><h3>The <code>LaplaceProblem</code> class implementation</h3>
<p><a class="anchor" id="Constructor"></a> </p><h4>Constructor</h4>
<p>Constructors and destructors are rather trivial. In addition to what we do in <a class="el" href="step_6.html">step-6</a>, we set the set of processors we want to work on to all machines available (MPI_COMM_WORLD); ask the triangulation to ensure that the mesh remains smooth and free to refined islands, for example; and initialize the <code>pcout</code> variable to only allow processor zero to output anything. The final piece is to initialize a timer that we use to determine how much compute time the different parts of the program take:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">LaplaceProblem&lt;dim&gt;::LaplaceProblem()</div><div class="line">  : mpi_communicator(MPI_COMM_WORLD)</div><div class="line">  , triangulation(mpi_communicator,</div><div class="line">                  typename <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::MeshSmoothing(</div><div class="line">                    <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::smoothing_on_refinement |</div><div class="line">                    <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::smoothing_on_coarsening))</div><div class="line">  , fe(2)</div><div class="line">  , dof_handler(triangulation)</div><div class="line">  , pcout(<a class="code" href="namespacestd.html">std</a>::cout,</div><div class="line">          (<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator) == 0))</div><div class="line">  , computing_timer(mpi_communicator,</div><div class="line">                    pcout,</div><div class="line">                    <a class="code" href="classTimerOutput.html">TimerOutput</a>::summary,</div><div class="line">                    <a class="code" href="classTimerOutput.html">TimerOutput</a>::wall_times)</div><div class="line">{}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemsetup_system"></a> </p><h4>LaplaceProblem::setup_system</h4>
<p>The following function is, arguably, the most interesting one in the entire program since it goes to the heart of what distinguishes parallel <a class="el" href="step_40.html">step-40</a> from sequential <a class="el" href="step_6.html">step-6</a>.</p>
<p>At the top we do what we always do: tell the <a class="el" href="classDoFHandler.html">DoFHandler</a> object to distribute degrees of freedom. Since the triangulation we use here is distributed, the <a class="el" href="classDoFHandler.html">DoFHandler</a> object is smart enough to recognize that on each processor it can only distribute degrees of freedom on cells it owns; this is followed by an exchange step in which processors tell each other about degrees of freedom on ghost cell. The result is a <a class="el" href="classDoFHandler.html">DoFHandler</a> that knows about the degrees of freedom on locally owned cells and ghost cells (i.e. cells adjacent to locally owned cells) but nothing about cells that are further away, consistent with the basic philosophy of distributed computing that no processor can know everything.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::setup_system()</div><div class="line">{</div><div class="line">  <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;setup&quot;</span>);</div><div class="line"></div><div class="line">  dof_handler.distribute_dofs(fe);</div></div><!-- fragment --><p>The next two lines extract some information we will need later on, namely two index sets that provide information about which degrees of freedom are owned by the current processor (this information will be used to initialize solution and right hand side vectors, and the system matrix, indicating which elements to store on the current processor and which to expect to be stored somewhere else); and an index set that indicates which degrees of freedom are locally relevant (i.e. live on cells that the current processor owns or on the layer of ghost cells around the locally owned cells; we need all of these degrees of freedom, for example, to estimate the error on the local cells).</p>
<div class="fragment"><div class="line">locally_owned_dofs = dof_handler.locally_owned_dofs();</div><div class="line"><a class="code" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs</a>(dof_handler, locally_relevant_dofs);</div></div><!-- fragment --><p>Next, let us initialize the solution and right hand side vectors. As mentioned above, the solution vector we seek does not only store elements we own, but also ghost entries; on the other hand, the right hand side vector only needs to have the entries the current processor owns since all we will ever do is write into it, never read from it on locally owned cells (of course the linear solvers will read from it, but they do not care about the geometric location of degrees of freedom).</p>
<div class="fragment"><div class="line">locally_relevant_solution.reinit(locally_owned_dofs,</div><div class="line">                                 locally_relevant_dofs,</div><div class="line">                                 mpi_communicator);</div><div class="line">system_rhs.reinit(locally_owned_dofs, mpi_communicator);</div></div><!-- fragment --><p>The next step is to compute hanging node and boundary value constraints, which we combine into a single object storing all constraints.</p>
<p>As with all other things in parallel, the mantra must be that no processor can store all information about the entire universe. As a consequence, we need to tell the <a class="el" href="classAffineConstraints.html">AffineConstraints</a> object for which degrees of freedom it can store constraints and for which it may not expect any information to store. In our case, as explained in the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using</a> module, the degrees of freedom we need to care about on each processor are the locally relevant ones, so we pass this to the <a class="el" href="classAffineConstraints.html#a2c9d71b5b7e8851c25a411ccf34de986">AffineConstraints::reinit</a> function. As a side note, if you forget to pass this argument, the <a class="el" href="classAffineConstraints.html">AffineConstraints</a> class will allocate an array with length equal to the largest DoF index it has seen so far. For processors with high MPI process number, this may be very large &ndash; maybe on the order of billions. The program would then allocate more memory than for likely all other operations combined for this single array.</p>
<div class="fragment"><div class="line">constraints.<a class="code" href="classAffineConstraints.html#addd15bc409c61d6f795f0132c574335b">clear</a>();</div><div class="line">constraints.<a class="code" href="classAffineConstraints.html#a2c9d71b5b7e8851c25a411ccf34de986">reinit</a>(locally_relevant_dofs);</div><div class="line"><a class="code" href="group__constraints.html#ga3b4ea7dfd313e388d868c4e4aa685799">DoFTools::make_hanging_node_constraints</a>(dof_handler, constraints);</div><div class="line"><a class="code" href="namespaceVectorTools.html#af27ac28c698a9ed0199faed50a204538">VectorTools::interpolate_boundary_values</a>(dof_handler,</div><div class="line">                                         0,</div><div class="line">                                         <a class="code" href="classFunctions_1_1ZeroFunction.html">Functions::ZeroFunction&lt;dim&gt;</a>(),</div><div class="line">                                         constraints);</div><div class="line">constraints.<a class="code" href="classAffineConstraints.html#a1611aa37f754086388ca76bcd421cce5">close</a>();</div></div><!-- fragment --><p>The last part of this function deals with initializing the matrix with accompanying sparsity pattern. As in previous tutorial programs, we use the <a class="el" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> as an intermediate with which we then initialize the system matrix. To do so we have to tell the sparsity pattern its size but as above there is no way the resulting object will be able to store even a single pointer for each global degree of freedom; the best we can hope for is that it stores information about each locally relevant degree of freedom, i.e. all those that we may ever touch in the process of assembling the matrix (the <a class="el" href="DEALGlossary.html#distributed_paper">distributed computing paper</a> has a long discussion why one really needs the locally relevant, and not the small set of locally active degrees of freedom in this context).</p>
<p>So we tell the sparsity pattern its size and what DoFs to store anything for and then ask <a class="el" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a> to fill it (this function ignores all cells that are not locally owned, mimicking what we will do below in the assembly process). After this, we call a function that exchanges entries in these sparsity pattern between processors so that in the end each processor really knows about all the entries that will exist in that part of the finite element matrix that it will own. The final step is to initialize the matrix with the sparsity pattern.</p>
<div class="fragment"><div class="line">  <a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(locally_relevant_dofs);</div><div class="line"></div><div class="line">  <a class="code" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a>(dof_handler, dsp, constraints, <span class="keyword">false</span>);</div><div class="line">  <a class="code" href="namespaceSparsityTools.html#afbc0c7a206ced91b154666215ea3c218">SparsityTools::distribute_sparsity_pattern</a>(dsp,</div><div class="line">                                             dof_handler.locally_owned_dofs(),</div><div class="line">                                             mpi_communicator,</div><div class="line">                                             locally_relevant_dofs);</div><div class="line"></div><div class="line">  system_matrix.reinit(locally_owned_dofs,</div><div class="line">                       locally_owned_dofs,</div><div class="line">                       dsp,</div><div class="line">                       mpi_communicator);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemassemble_system"></a> </p><h4>LaplaceProblem::assemble_system</h4>
<p>The function that then assembles the linear system is comparatively boring, being almost exactly what we've seen before. The points to watch out for are:</p><ul>
<li>Assembly must only loop over locally owned cells. There are multiple ways to test that; for example, we could compare a cell's subdomain_id against information from the triangulation as in <code>cell-&gt;<a class="el" href="namespacetypes.html#a3d8ea8a4c6a58127f8c2fcc9ec74af6e">subdomain_id()</a> == triangulation.locally_owned_subdomain()</code>, or skip all cells for which the condition <code>cell-&gt;is_ghost() || cell-&gt;is_artificial()</code> is true. The simplest way, however, is to simply ask the cell whether it is owned by the local processor.</li>
<li>Copying local contributions into the global matrix must include distributing constraints and boundary values. In other words, we cannot (as we did in <a class="el" href="step_6.html">step-6</a>) first copy every local contribution into the global matrix and only in a later step take care of hanging node constraints and boundary values. The reason is, as discussed in <a class="el" href="step_17.html">step-17</a>, that the parallel vector classes do not provide access to arbitrary elements of the matrix once they have been assembled into it &ndash; in parts because they may simply no longer reside on the current processor but have instead been shipped to a different machine.</li>
<li>The way we compute the right hand side (given the formula stated in the introduction) may not be the most elegant but will do for a program whose focus lies somewhere entirely different.</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::assemble_system()</div><div class="line">{</div><div class="line">  <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;assembly&quot;</span>);</div><div class="line"></div><div class="line">  <span class="keyword">const</span> <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a> quadrature_formula(fe.degree + 1);</div><div class="line"></div><div class="line">  <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values(fe,</div><div class="line">                          quadrature_formula,</div><div class="line">                          <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                            <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> dofs_per_cell = fe.n_dofs_per_cell();</div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_q_points    = quadrature_formula.<a class="code" href="classQuadrature.html#af9f7d82770fa8126e19113f3e3db755b">size</a>();</div><div class="line"></div><div class="line">  <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a> <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(dofs_per_cell, dofs_per_cell);</div><div class="line">  <a class="code" href="classVector.html">Vector&lt;double&gt;</a>     cell_rhs(dofs_per_cell);</div><div class="line"></div><div class="line">  std::vector&lt;types::global_dof_index&gt; local_dof_indices(dofs_per_cell);</div><div class="line"></div><div class="line">  <span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;cell : dof_handler.<a class="code" href="group__CPP11.html#gacdb6d3adec96a13a7079f6c893e1d3ff">active_cell_iterators</a>())</div><div class="line">    <span class="keywordflow">if</span> (cell-&gt;is_locally_owned())</div><div class="line">      {</div><div class="line">        cell_matrix = 0.;</div><div class="line">        cell_rhs    = 0.;</div><div class="line"></div><div class="line">        fe_values.<a class="code" href="classFEValues.html#a21f914e63d588e2652a9514620653d77">reinit</a>(cell);</div><div class="line"></div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points; ++q_point)</div><div class="line">          {</div><div class="line">            <span class="keyword">const</span> <span class="keywordtype">double</span> rhs_value =</div><div class="line">              (fe_values.<a class="code" href="classFEValuesBase.html#ab123e5da03736be4977c76fbcb6a2e37">quadrature_point</a>(q_point)[1] &gt;</div><div class="line">                   0.5 +</div><div class="line">                     0.25 * <a class="code" href="vectorization_8h.html#ad9b7aa5c50bf9ce988a0f756a3f2baa5">std::sin</a>(4.0 * <a class="code" href="namespacenumbers.html#a3e24f194a9cb9b6ff4442b8a7a877d4a">numbers::PI</a> *</div><div class="line">                                     fe_values.<a class="code" href="classFEValuesBase.html#ab123e5da03736be4977c76fbcb6a2e37">quadrature_point</a>(q_point)[0]) ?</div><div class="line">                 1. :</div><div class="line">                 -1.);</div><div class="line"></div><div class="line">            <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">              {</div><div class="line">                <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j = 0; j &lt; dofs_per_cell; ++j)</div><div class="line">                  <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(i, j) += fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point) *</div><div class="line">                                       fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point) *</div><div class="line">                                       fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line"></div><div class="line">                cell_rhs(i) += rhs_value *                         </div><div class="line">                               fe_values.<a class="code" href="classFEValuesBase.html#a1dd48cb744013c448d57f8f77640c08d">shape_value</a>(i, q_point) * </div><div class="line">                               fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">              }</div><div class="line">          }</div><div class="line"></div><div class="line">        cell-&gt;get_dof_indices(local_dof_indices);</div><div class="line">        constraints.<a class="code" href="classAffineConstraints.html#a373fbdacd8c486e675b8d2bff8943192">distribute_local_to_global</a>(cell_matrix,</div><div class="line">                                               cell_rhs,</div><div class="line">                                               local_dof_indices,</div><div class="line">                                               system_matrix,</div><div class="line">                                               system_rhs);</div><div class="line">      }</div></div><!-- fragment --><p>Notice that the assembling above is just a local operation. So, to form the "global" linear system, a synchronization between all processors is needed. This could be done by invoking the function <a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a>. See <a class="el" href="DEALGlossary.html#GlossCompress">Compressing distributed objects</a> for more information on what is <a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a> designed to do.</p>
<div class="fragment"><div class="line">  system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">  system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemsolve"></a> </p><h4>LaplaceProblem::solve</h4>
<p>Even though solving linear systems on potentially tens of thousands of processors is by far not a trivial job, the function that does this is &ndash; at least at the outside &ndash; relatively simple. Most of the parts you've seen before. There are really only two things worth mentioning:</p><ul>
<li>Solvers and preconditioners are built on the deal.II wrappers of PETSc and Trilinos functionality. It is relatively well known that the primary bottleneck of massively parallel linear solvers is not actually the communication between processors, but the fact that it is difficult to produce preconditioners that scale well to large numbers of processors. Over the second half of the first decade of the 21st century, it has become clear that algebraic multigrid (AMG) methods turn out to be extremely efficient in this context, and we will use one of them &ndash; either the BoomerAMG implementation of the Hypre package that can be interfaced to through PETSc, or a preconditioner provided by ML, which is part of Trilinos &ndash; for the current program. The rest of the solver itself is boilerplate and has been shown before. Since the linear system is symmetric and positive definite, we can use the CG method as the outer solver.</li>
<li>Ultimately, we want a vector that stores not only the elements of the solution for degrees of freedom the current processor owns, but also all other locally relevant degrees of freedom. On the other hand, the solver itself needs a vector that is uniquely split between processors, without any overlap. We therefore create a vector at the beginning of this function that has these properties, use it to solve the linear system, and only assign it to the vector we want at the very end. This last step ensures that all ghost elements are also copied as necessary.</li>
</ul>
<div class="fragment"><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::solve()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;solve&quot;</span>);</div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>    completely_distributed_solution(locally_owned_dofs,</div><div class="line">                                                    mpi_communicator);</div><div class="line"></div><div class="line">    <a class="code" href="classSolverControl.html">SolverControl</a> solver_control(dof_handler.n_dofs(), 1e-12);</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc.html#a9d2c129d77c89b40badfb65d94c5b955">LA::SolverCG</a> solver(solver_control, mpi_communicator);</div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc.html#a9d2c129d77c89b40badfb65d94c5b955">LA::SolverCG</a> solver(solver_control);</div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc_1_1MPI.html#a41f11f7a1992c6d6aa9367b12c68f791">LA::MPI::PreconditionAMG</a> preconditioner;</div><div class="line"></div><div class="line">    LA::MPI::PreconditionAMG::AdditionalData data;</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">    data.symmetric_operator = <span class="keyword">true</span>;</div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">/* Trilinos defaults are good */</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">    preconditioner.<a class="code" href="classPETScWrappers_1_1PreconditionBoomerAMG.html#a3c82672eac786d7f369b296439859ff2">initialize</a>(system_matrix, data);</div><div class="line"></div><div class="line">    solver.solve(system_matrix,</div><div class="line">                 completely_distributed_solution,</div><div class="line">                 system_rhs,</div><div class="line">                 preconditioner);</div><div class="line"></div><div class="line">    pcout &lt;&lt; <span class="stringliteral">&quot;   Solved in &quot;</span> &lt;&lt; solver_control.last_step() &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span></div><div class="line">          &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#a7b3d3f295bb56d6cd6856bdc6cbe8a01">distribute</a>(completely_distributed_solution);</div><div class="line"></div><div class="line">    locally_relevant_solution = completely_distributed_solution;</div><div class="line">  }</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemrefine_grid"></a> </p><h4>LaplaceProblem::refine_grid</h4>
<p>The function that estimates the error and refines the grid is again almost exactly like the one in <a class="el" href="step_6.html">step-6</a>. The only difference is that the function that flags cells to be refined is now in namespace <a class="el" href="namespaceparallel_1_1distributed_1_1GridRefinement.html">parallel::distributed::GridRefinement</a> &ndash; a namespace that has functions that can communicate between all involved processors and determine global thresholds to use in deciding which cells to refine and which to coarsen.</p>
<p>Note that we didn't have to do anything special about the <a class="el" href="classKellyErrorEstimator.html">KellyErrorEstimator</a> class: we just give it a vector with as many elements as the local triangulation has cells (locally owned cells, ghost cells, and artificial ones), but it only fills those entries that correspond to cells that are locally owned.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::refine_grid()</div><div class="line">{</div><div class="line">  <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;refine&quot;</span>);</div><div class="line"></div><div class="line">  <a class="code" href="classVector.html">Vector&lt;float&gt;</a> estimated_error_per_cell(triangulation.<a class="code" href="classTriangulation.html#a5ea5c9957dbb566a562bbe2c0f3971e9">n_active_cells</a>());</div><div class="line">  <a class="code" href="classKellyErrorEstimator.html#ae2269e1c9903e9d863b7abd54948af00">KellyErrorEstimator&lt;dim&gt;::estimate</a>(</div><div class="line">    dof_handler,</div><div class="line">    <a class="code" href="classQGauss.html">QGauss&lt;dim - 1&gt;</a>(fe.degree + 1),</div><div class="line">    std::map&lt;<a class="code" href="classunsigned_01int.html">types::boundary_id</a>, <span class="keyword">const</span> <a class="code" href="classFunction.html">Function&lt;dim&gt;</a> *&gt;(),</div><div class="line">    locally_relevant_solution,</div><div class="line">    estimated_error_per_cell);</div><div class="line">  <a class="code" href="namespaceparallel_1_1distributed_1_1GridRefinement.html#aa2ffb707a796ae6dedb75036606ef2e6">parallel::distributed::GridRefinement::refine_and_coarsen_fixed_number</a>(</div><div class="line">    triangulation, estimated_error_per_cell, 0.3, 0.03);</div><div class="line">  triangulation.<a class="code" href="classTriangulation.html#ac8b4fbb207303ec7f5ef758821ecd8cb">execute_coarsening_and_refinement</a>();</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemoutput_results"></a> </p><h4>LaplaceProblem::output_results</h4>
<p>Compared to the corresponding function in <a class="el" href="step_6.html">step-6</a>, the one here is a tad more complicated. There are two reasons: the first one is that we do not just want to output the solution but also for each cell which processor owns it (i.e. which "subdomain" it is in). Secondly, as discussed at length in <a class="el" href="step_17.html">step-17</a> and <a class="el" href="step_18.html">step-18</a>, generating graphical data can be a bottleneck in parallelizing. In <a class="el" href="step_18.html">step-18</a>, we have moved this step out of the actual computation but shifted it into a separate program that later combined the output from various processors into a single file. But this doesn't scale: if the number of processors is large, this may mean that the step of combining data on a single processor later becomes the longest running part of the program, or it may produce a file that's so large that it can't be visualized any more. We here follow a more sensible approach, namely creating individual files for each MPI process and leaving it to the visualization program to make sense of that.</p>
<p>To start, the top of the function looks like it usually does. In addition to attaching the solution vector (the one that has entries for all locally relevant, not only the locally owned, elements), we attach a data vector that stores, for each cell, the subdomain the cell belongs to. This is slightly tricky, because of course not every processor knows about every cell. The vector we attach therefore has an entry for every cell that the current processor has in its mesh (locally owned ones, ghost cells, and artificial cells), but the <a class="el" href="classDataOut.html">DataOut</a> class will ignore all entries that correspond to cells that are not owned by the current processor. As a consequence, it doesn't actually matter what values we write into these vector entries: we simply fill the entire vector with the number of the current MPI process (i.e. the subdomain_id of the current process); this correctly sets the values we care for, i.e. the entries that correspond to locally owned cells, while providing the wrong value for all other elements &ndash; but these are then ignored anyway.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword"></span>{</div><div class="line">  <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">  data_out.<a class="code" href="classDataOut__DoFData.html#a6ed7c846331069f406b8c9933c37fda4">attach_dof_handler</a>(dof_handler);</div><div class="line">  data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(locally_relevant_solution, <span class="stringliteral">&quot;u&quot;</span>);</div><div class="line"></div><div class="line">  <a class="code" href="classVector.html">Vector&lt;float&gt;</a> subdomain(triangulation.<a class="code" href="classTriangulation.html#a5ea5c9957dbb566a562bbe2c0f3971e9">n_active_cells</a>());</div><div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; subdomain.size(); ++i)</div><div class="line">    subdomain(i) = triangulation.<a class="code" href="classTriangulation.html#a44ea82a097d8317c98fa422307aff874">locally_owned_subdomain</a>();</div><div class="line">  data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(subdomain, <span class="stringliteral">&quot;subdomain&quot;</span>);</div><div class="line"></div><div class="line">  data_out.<a class="code" href="classDataOut.html#a087f63e22f0614bca326dbdca288c646">build_patches</a>();</div></div><!-- fragment --><p>The next step is to write this data to disk. We write up to 8 VTU files in parallel with the help of MPI-IO. Additionally a PVTU record is generated, which groups the written VTU files.</p>
<div class="fragment"><div class="line">  data_out.<a class="code" href="classDataOutInterface.html#a0864e51eb173c87e2a3edc9391ea8009">write_vtu_with_pvtu_record</a>(</div><div class="line">    <span class="stringliteral">&quot;./&quot;</span>, <span class="stringliteral">&quot;solution&quot;</span>, cycle, mpi_communicator, 2, 8);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="LaplaceProblemrun"></a> </p><h4>LaplaceProblem::run</h4>
<p>The function that controls the overall behavior of the program is again like the one in <a class="el" href="step_6.html">step-6</a>. The minor difference are the use of <code>pcout</code> instead of <code>std::cout</code> for output to the console (see also <a class="el" href="step_17.html">step-17</a>) and that we only generate graphical output if at most 32 processors are involved. Without this limit, it would be just too easy for people carelessly running this program without reading it first to bring down the cluster interconnect and fill any file system available :-)</p>
<p>A functional difference to <a class="el" href="step_6.html">step-6</a> is the use of a square domain and that we start with a slightly finer mesh (5 global refinement cycles) &ndash; there just isn't much of a point showing a massively parallel program starting on 4 cells (although admittedly the point is only slightly stronger starting on 1024).</p>
<div class="fragment"><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">LaplaceProblem&lt;dim&gt;::run</a>()</div><div class="line">  {</div><div class="line">    pcout &lt;&lt; <span class="stringliteral">&quot;Running with &quot;</span></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot;PETSc&quot;</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot;Trilinos&quot;</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot; on &quot;</span> &lt;&lt; <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">Utilities::MPI::n_mpi_processes</a>(mpi_communicator)</div><div class="line">          &lt;&lt; <span class="stringliteral">&quot; MPI rank(s)...&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_cycles = 8;</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle = 0; cycle &lt; n_cycles; ++cycle)</div><div class="line">      {</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">          {</div><div class="line">            <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a>(triangulation);</div><div class="line">            triangulation.<a class="code" href="classTriangulation.html#a6ad0b3fb24aae17f4668427a433dea19">refine_global</a>(5);</div><div class="line">          }</div><div class="line">        <span class="keywordflow">else</span></div><div class="line">          refine_grid();</div><div class="line"></div><div class="line">        setup_system();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">              &lt;&lt; triangulation.<a class="code" href="classTriangulation.html#a584733c8499dbd140694bfe04e0963ca">n_global_active_cells</a>() &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span> &lt;&lt; dof_handler.n_dofs()</div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        assemble_system();</div><div class="line">        solve();</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">Utilities::MPI::n_mpi_processes</a>(mpi_communicator) &lt;= 32)</div><div class="line">          {</div><div class="line">            <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;output&quot;</span>);</div><div class="line">            output_results(cycle);</div><div class="line">          }</div><div class="line"></div><div class="line">        computing_timer.print_summary();</div><div class="line">        computing_timer.reset();</div><div class="line"></div><div class="line">        pcout &lt;&lt; std::endl;</div><div class="line">      }</div><div class="line">  }</div><div class="line">} <span class="comment">// namespace Step40</span></div></div><!-- fragment --><p><a class="anchor" id="main"></a> </p><h4>main()</h4>
<p>The final function, <code>main()</code>, again has the same structure as in all other programs, in particular <a class="el" href="step_6.html">step-6</a>. Like the other programs that use MPI, we have to initialize and finalize MPI, which is done using the helper object <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a>. The constructor of that class also initializes libraries that depend on MPI, such as p4est, PETSc, SLEPc, and Zoltan (though the last two are not used in this tutorial). The order here is important: we cannot use any of these libraries until they are initialized, so it does not make sense to do anything before creating an instance of <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a>.</p>
<p>After the solver finishes, the LaplaceProblem destructor will run followed by <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html#a812d3e3369e2c86ba94beb13368d92d7">Utilities::MPI::MPI_InitFinalize::~MPI_InitFinalize()</a>. This order is also important: <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html#a812d3e3369e2c86ba94beb13368d92d7">Utilities::MPI::MPI_InitFinalize::~MPI_InitFinalize()</a> calls <code>PetscFinalize</code> (and finalization functions for other libraries), which will delete any in-use PETSc objects. This must be done after we destruct the Laplace solver to avoid double deletion errors. Fortunately, due to the order of destructor call rules of C++, we do not need to worry about any of this: everything happens in the correct order (i.e., the reverse of the order of construction). The last function called by <a class="el" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html#a812d3e3369e2c86ba94beb13368d92d7">Utilities::MPI::MPI_InitFinalize::~MPI_InitFinalize()</a> is <code>MPI_Finalize</code>: i.e., once this object is destructed the program should exit since MPI will no longer be available.</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> *argv[])</div><div class="line">{</div><div class="line">  <span class="keywordflow">try</span></div><div class="line">    {</div><div class="line">      <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">      <span class="keyword">using namespace </span>Step40;</div><div class="line"></div><div class="line">      <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">      LaplaceProblem&lt;2&gt; laplace_problem_2d;</div><div class="line">      laplace_problem_2d.run();</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (...)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p> examples/step-40/doc/results.dox</p>
<p><a class="anchor" id="Results"></a></p><h1>Results</h1>
<p>当你在单个处理器上或在几个本地MPI安装上运行该程序时，你应该得到这样的输出。</p>
<div class="fragment"><div class="line">Cycle 0:</div><div class="line">   Number of active cells:       1024</div><div class="line">   Number of degrees of freedom: 4225</div><div class="line">   Solved in 10 iterations.</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">+---------------------------------------------+------------+------------+</div><div class="line">| Total wallclock time elapsed since start    |     0.176s |            |</div><div class="line">|                                             |            |            |</div><div class="line">| Section                         | no. calls |  wall time | % of total |</div><div class="line">+---------------------------------+-----------+------------+------------+</div><div class="line">| assembly                        |         1 |    0.0209s |        12% |</div><div class="line">| output                          |         1 |    0.0189s |        11% |</div><div class="line">| setup                           |         1 |    0.0299s |        17% |</div><div class="line">| solve                           |         1 |    0.0419s |        24% |</div><div class="line">+---------------------------------+-----------+------------+------------+</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">Cycle 1:</div><div class="line">   Number of active cells:       1954</div><div class="line">   Number of degrees of freedom: 8399</div><div class="line">   Solved in 10 iterations.</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">+---------------------------------------------+------------+------------+</div><div class="line">| Total wallclock time elapsed since start    |     0.327s |            |</div><div class="line">|                                             |            |            |</div><div class="line">| Section                         | no. calls |  wall time | % of total |</div><div class="line">+---------------------------------+-----------+------------+------------+</div><div class="line">| assembly                        |         1 |    0.0368s |        11% |</div><div class="line">| output                          |         1 |    0.0208s |       6.4% |</div><div class="line">| <a class="code" href="namespaceGridRefinement.html#a1cf30058b31ce7f9b389e8310bb9fc54">refine</a>                          |         1 |     0.157s |        48% |</div><div class="line">| setup                           |         1 |    0.0452s |        14% |</div><div class="line">| solve                           |         1 |    0.0668s |        20% |</div><div class="line">+---------------------------------+-----------+------------+------------+</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">Cycle 2:</div><div class="line">   Number of active cells:       3664</div><div class="line">   Number of degrees of freedom: 16183</div><div class="line">   Solved in 11 iterations.</div><div class="line"></div><div class="line"></div><div class="line">...</div></div><!-- fragment --><p>确切的数字是不同的，这取决于我们使用多少个处理器；这是由于预处理程序取决于问题的分区，然后解决方案在最后几位上有所不同，因此，网格细化也略有不同。不过，这里最值得注意的是，迭代次数并不随问题的大小而增加。这保证了我们甚至可以有效地解决最大的问题。</p>
<p>当在足够多的机器上运行时（比如说几千台），这个程序可以相对容易地在不到一分钟的时间内解决有远超过10亿个未知数的问题。另一方面，这样的大问题已经不能被视觉化，所以我们也只在16个处理器上运行该程序。下面是一个网格，以及它在16个处理器上的划分，还有相应的解决方案。</p>
<table width="100%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.mesh.png"/>
</div>
  </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.solution.png"/>
</div>
   </td></tr>
</table>
<p>左边的网格仅有7,069个单元。当然，这个问题我们在单台处理器上使用<a class="el" href="step_6.html">step-6</a>就已经很容易解决了，但是这个程序的重点是展示如何编写一个可以扩展到更多机器的程序。例如，这里有两张图，显示了如果我们采取越来越多的处理器，程序的大量部分的运行时间是如何在大约5200万和37500万自由度的问题上扩展的（这些和接下来的几张图取自 <a class="el" href="DEALGlossary.html#distributed_paper">分布式计算论文 </a>的早期版本；显示在更大数量的处理器上运行数据的更新图，以及更多的解释可以在该论文的最终版本中找到）。</p>
<table width="100%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.strong2.png"/>
</div>
  </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.strong.png"/>
</div>
   </td></tr>
</table>
<p>可以清楚地看到，这个程序可以很好地扩展到非常多的处理器。关于我们认为的 "可扩展 "程序的讨论，见 <a class="el" href="DEALGlossary.html#GlossParallelScaling">本词汇表条目</a>）。曲线，特别是线性求解器，在图形的右端变得有点摇摆不定，因为每个处理器要做的事情太少，无法抵消通信成本（在上面两个例子中，每个处理器要解决的整个问题的部分，在使用4,096个处理器时，只有13,000和90,000个自由度；一个好的经验法则是，如果每个处理器至少有100,000个未知数，并行程序就会运行良好）。</p>
<p>虽然上面的强扩展图显示，如果我们采取越来越多的处理器，我们可以越来越快地解决一个固定大小的问题，但更有趣的问题可能是，问题可以变得多大，以便在一个特定大小的机器上仍然可以在合理的时间内解决它们。我们在下面两张256和4096处理器的图中展示了这一点。</p>
<table width="100%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.256.png"/>
</div>
  </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-40.4096.png"/>
</div>
   </td></tr>
</table>
<p>这些图显示的是，程序的所有部分都随着自由度数的增加而线性扩展。这一次，由于局部问题的规模太小，线条在左边摇摆不定。关于这些结果的更多讨论，我们参考了 <a class="el" href="DEALGlossary.html#distributed_paper">分布式计算论文</a>。</p>
<p>那么，一个人能够解决的最大问题是多大？在写这个问题的时候，限制因素是程序使用<a href="http://acts.nersc.gov/hypre/" target="_top">Hypre package</a>中的BoomerAMG代数多网格方法作为预处理程序，不幸的是，它使用有符号的32位整数来索引分布式矩阵的元素。这将问题的大小限制在 \(2^{31}-1=2,147,483,647\) 个自由度。从上面的图中可以看出，可扩展性会超过这个数字，而且可以预期，给定超过上面显示的4096台机器也会进一步减少计算时间。也就是说，人们当然可以期待，这个限制最终会被hybre的开发者解除。</p>
<p>另一方面，这并不意味着deal.II不能解决更大的问题。事实上，<a class="el" href="step_37.html">step-37</a>展示了如何解决不仅仅是一点点，而是大大超过我们在这里所展示的任何问题的问题。</p>
<p><a class="anchor" id="extensions"></a></p>
<p><a class="anchor" id="Possibilitiesforextensions"></a></p><h3>Possibilities for extensions</h3>
<p>从某种意义上说，这个程序是拉普拉斯方程的终极解算器：只要你有足够的处理器，它基本上可以把方程解到你想要的精度。由于拉普拉斯方程本身在这种精度水平上并不十分有趣，因此，更有趣的扩展可能性不在于这个程序，而在于它之后的内容。例如，本教程中的其他几个程序都有相当长的运行时间，特别是在3D中。因此，使用这里解释的技术来扩展其他程序以支持并行的分布式计算将是有趣的。我们在<a class="el" href="step_32.html">step-32</a>教程程序中对<a class="el" href="step_31.html">step-31</a>做了这样的处理，但同样的做法也适用于，例如，用于双曲时间相关问题的<a class="el" href="step_23.html">step-23</a>和<a class="el" href="step_25.html">step-25</a>，用于气体动力学的<a class="el" href="step_33.html">step-33</a>，或用于纳维-斯托克斯方程的<a class="el" href="step_35.html">step-35</a>。</p>
<p>也许同样有趣的是后处理的问题。如上所述，我们只展示了16个处理器的解决方案和网格的图片，因为4,096个处理器解决10亿个未知数会产生几10G的图形输出。目前，除非在至少几百个处理器上运行，否则没有任何程序能够以任何合理的方式将如此大量的数据可视化。然而，有一些方法，可视化程序直接与每个处理器上的求解器进行通信，每个可视化进程渲染这个处理器上的求解器所计算的场景部分。实现这样的接口将允许快速可视化那些在其他方面不适合用图形显示的东西。</p>
<p><a class="anchor" id="PlainProg"></a> </p><h1>The plain program</h1>
<div class="fragment"><div class="line"><span class="comment">/* ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Copyright (C) 2009 - 2021 by the deal.II authors</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * This file is part of the deal.II library.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * The deal.II library is free software; you can use it, redistribute</span></div><div class="line"><span class="comment"> * it, and/or modify it under the terms of the GNU Lesser General</span></div><div class="line"><span class="comment"> * Public License as published by the Free Software Foundation; either</span></div><div class="line"><span class="comment"> * version 2.1 of the License, or (at your option) any later version.</span></div><div class="line"><span class="comment"> * The full text of the license can be found in the file LICENSE.md at</span></div><div class="line"><span class="comment"> * the top level directory of deal.II.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Author: Wolfgang Bangerth, Texas A&amp;M University, 2009, 2010</span></div><div class="line"><span class="comment"> *         Timo Heister, University of Goettingen, 2009, 2010</span></div><div class="line"><span class="comment"> */</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="quadrature__lib_8h.html">deal.II/base/quadrature_lib.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="function_8h.html">deal.II/base/function.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="timer_8h.html">deal.II/base/timer.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="generic__linear__algebra_8h.html">deal.II/lac/generic_linear_algebra.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>LA</div><div class="line">{</div><div class="line"><span class="preprocessor">#if defined(DEAL_II_WITH_PETSC) &amp;&amp; !defined(DEAL_II_PETSC_WITH_COMPLEX) &amp;&amp; \</span></div><div class="line"><span class="preprocessor">  !(defined(DEAL_II_WITH_TRILINOS) &amp;&amp; defined(FORCE_USE_OF_TRILINOS))</span></div><div class="line">  <span class="keyword">using namespace </span>dealii::LinearAlgebraPETSc;</div><div class="line"><span class="preprocessor">#  define USE_PETSC_LA</span></div><div class="line"><span class="preprocessor">#elif defined(DEAL_II_WITH_TRILINOS)</span></div><div class="line">  <span class="keyword">using namespace </span>dealii::LinearAlgebraTrilinos;</div><div class="line"><span class="preprocessor">#else</span></div><div class="line"><span class="preprocessor">#  error DEAL_II_WITH_PETSC or DEAL_II_WITH_TRILINOS required</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">} <span class="comment">// namespace LA</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8h.html">deal.II/lac/vector.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="full__matrix_8h.html">deal.II/lac/full_matrix.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="solver__cg_8h.html">deal.II/lac/solver_cg.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="affine__constraints_8h.html">deal.II/lac/affine_constraints.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dynamic__sparsity__pattern_8h.html">deal.II/lac/dynamic_sparsity_pattern.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="grid__generator_8h.html">deal.II/grid/grid_generator.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dofs_2dof__handler_8h.html">deal.II/dofs/dof_handler.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="dof__tools_8h.html">deal.II/dofs/dof_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe_2fe__values_8h.html">deal.II/fe/fe_values.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="fe__q_8h.html">deal.II/fe/fe_q.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector__tools_8h.html">deal.II/numerics/vector_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="numerics_2data__out_8h.html">deal.II/numerics/data_out.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="error__estimator_8h.html">deal.II/numerics/error_estimator.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="include_2deal_8II_2base_2utilities_8h.html">deal.II/base/utilities.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="conditional__ostream_8h.html">deal.II/base/conditional_ostream.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="index__set_8h.html">deal.II/base/index_set.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="sparsity__tools_8h.html">deal.II/lac/sparsity_tools.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="distributed_2tria_8h.html">deal.II/distributed/tria.h</a>&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="distributed_2grid__refinement_8h.html">deal.II/distributed/grid_refinement.h</a>&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>Step40</div><div class="line">{</div><div class="line">  <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keyword">class </span>LaplaceProblem</div><div class="line">  {</div><div class="line">  <span class="keyword">public</span>:</div><div class="line">    LaplaceProblem();</div><div class="line"></div><div class="line">    <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">run</a>();</div><div class="line"></div><div class="line">  <span class="keyword">private</span>:</div><div class="line">    <span class="keywordtype">void</span> setup_system();</div><div class="line">    <span class="keywordtype">void</span> assemble_system();</div><div class="line">    <span class="keywordtype">void</span> solve();</div><div class="line">    <span class="keywordtype">void</span> refine_grid();</div><div class="line">    <span class="keywordtype">void</span> output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div><div class="line"></div><div class="line">    <a class="code" href="classMPI__Comm.html">MPI_Comm</a> mpi_communicator;</div><div class="line"></div><div class="line">    <a class="code" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation&lt;dim&gt;</a> <a class="code" href="p4est__wrappers_8cc.html#ace00f2f80d9780ef9aa1007e1c22c6a4">triangulation</a>;</div><div class="line"></div><div class="line">    <a class="code" href="classFE__Q.html">FE_Q&lt;dim&gt;</a>       fe;</div><div class="line">    <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a> dof_handler;</div><div class="line"></div><div class="line">    <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs;</div><div class="line">    <a class="code" href="classIndexSet.html">IndexSet</a> locally_relevant_dofs;</div><div class="line"></div><div class="line">    <a class="code" href="classAffineConstraints.html">AffineConstraints&lt;double&gt;</a> constraints;</div><div class="line"></div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a912abe2208022aec6753876bcc72f6bf">LA::MPI::SparseMatrix</a> system_matrix;</div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>       locally_relevant_solution;</div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>       system_rhs;</div><div class="line"></div><div class="line">    <a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div><div class="line">    <a class="code" href="classTimerOutput.html">TimerOutput</a>        computing_timer;</div><div class="line">  };</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  LaplaceProblem&lt;dim&gt;::LaplaceProblem()</div><div class="line">    : mpi_communicator(MPI_COMM_WORLD)</div><div class="line">    , triangulation(mpi_communicator,</div><div class="line">                    typename <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::MeshSmoothing(</div><div class="line">                      <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::smoothing_on_refinement |</div><div class="line">                      <a class="code" href="classTriangulation.html">Triangulation</a>&lt;dim&gt;::smoothing_on_coarsening))</div><div class="line">    , fe(2)</div><div class="line">    , dof_handler(triangulation)</div><div class="line">    , pcout(<a class="code" href="namespacestd.html">std</a>::cout,</div><div class="line">            (<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator) == 0))</div><div class="line">    , computing_timer(mpi_communicator,</div><div class="line">                      pcout,</div><div class="line">                      <a class="code" href="classTimerOutput.html">TimerOutput</a>::summary,</div><div class="line">                      <a class="code" href="classTimerOutput.html">TimerOutput</a>::wall_times)</div><div class="line">  {}</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::setup_system()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;setup&quot;</span>);</div><div class="line"></div><div class="line">    dof_handler.distribute_dofs(fe);</div><div class="line"></div><div class="line">    locally_owned_dofs = dof_handler.locally_owned_dofs();</div><div class="line">    <a class="code" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs</a>(dof_handler, locally_relevant_dofs);</div><div class="line"></div><div class="line">    locally_relevant_solution.reinit(locally_owned_dofs,</div><div class="line">                                     locally_relevant_dofs,</div><div class="line">                                     mpi_communicator);</div><div class="line">    system_rhs.reinit(locally_owned_dofs, mpi_communicator);</div><div class="line"></div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#addd15bc409c61d6f795f0132c574335b">clear</a>();</div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#a2c9d71b5b7e8851c25a411ccf34de986">reinit</a>(locally_relevant_dofs);</div><div class="line">    <a class="code" href="group__constraints.html#ga3b4ea7dfd313e388d868c4e4aa685799">DoFTools::make_hanging_node_constraints</a>(dof_handler, constraints);</div><div class="line">    <a class="code" href="namespaceVectorTools.html#af27ac28c698a9ed0199faed50a204538">VectorTools::interpolate_boundary_values</a>(dof_handler,</div><div class="line">                                             0,</div><div class="line">                                             <a class="code" href="classFunctions_1_1ZeroFunction.html">Functions::ZeroFunction&lt;dim&gt;</a>(),</div><div class="line">                                             constraints);</div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#a1611aa37f754086388ca76bcd421cce5">close</a>();</div><div class="line"></div><div class="line">    <a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(locally_relevant_dofs);</div><div class="line"></div><div class="line">    <a class="code" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a>(dof_handler, dsp, constraints, <span class="keyword">false</span>);</div><div class="line">    <a class="code" href="namespaceSparsityTools.html#afbc0c7a206ced91b154666215ea3c218">SparsityTools::distribute_sparsity_pattern</a>(dsp,</div><div class="line">                                               dof_handler.locally_owned_dofs(),</div><div class="line">                                               mpi_communicator,</div><div class="line">                                               locally_relevant_dofs);</div><div class="line"></div><div class="line">    system_matrix.reinit(locally_owned_dofs,</div><div class="line">                         locally_owned_dofs,</div><div class="line">                         dsp,</div><div class="line">                         mpi_communicator);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::assemble_system()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;assembly&quot;</span>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a> quadrature_formula(fe.degree + 1);</div><div class="line"></div><div class="line">    <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values(fe,</div><div class="line">                            quadrature_formula,</div><div class="line">                            <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                              <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> dofs_per_cell = fe.n_dofs_per_cell();</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_q_points    = quadrature_formula.<a class="code" href="classQuadrature.html#af9f7d82770fa8126e19113f3e3db755b">size</a>();</div><div class="line"></div><div class="line">    <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a> <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(dofs_per_cell, dofs_per_cell);</div><div class="line">    Vector&lt;double&gt;     cell_rhs(dofs_per_cell);</div><div class="line"></div><div class="line">    std::vector&lt;types::global_dof_index&gt; local_dof_indices(dofs_per_cell);</div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;cell : dof_handler.<a class="code" href="group__CPP11.html#gacdb6d3adec96a13a7079f6c893e1d3ff">active_cell_iterators</a>())</div><div class="line">      <span class="keywordflow">if</span> (cell-&gt;is_locally_owned())</div><div class="line">        {</div><div class="line">          cell_matrix = 0.;</div><div class="line">          cell_rhs    = 0.;</div><div class="line"></div><div class="line">          fe_values.<a class="code" href="classFEValues.html#a21f914e63d588e2652a9514620653d77">reinit</a>(cell);</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point = 0; q_point &lt; n_q_points; ++q_point)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">double</span> rhs_value =</div><div class="line">                (fe_values.<a class="code" href="classFEValuesBase.html#ab123e5da03736be4977c76fbcb6a2e37">quadrature_point</a>(q_point)[1] &gt;</div><div class="line">                     0.5 +</div><div class="line">                       0.25 * <a class="code" href="vectorization_8h.html#ad9b7aa5c50bf9ce988a0f756a3f2baa5">std::sin</a>(4.0 * <a class="code" href="namespacenumbers.html#a3e24f194a9cb9b6ff4442b8a7a877d4a">numbers::PI</a> *</div><div class="line">                                       fe_values.<a class="code" href="classFEValuesBase.html#ab123e5da03736be4977c76fbcb6a2e37">quadrature_point</a>(q_point)[0]) ?</div><div class="line">                   1. :</div><div class="line">                   -1.);</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; dofs_per_cell; ++i)</div><div class="line">                {</div><div class="line">                  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j = 0; j &lt; dofs_per_cell; ++j)</div><div class="line">                    <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#a8bc7b8136646134f73a4193adefe15f8">cell_matrix</a>(i, j) += fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(i, q_point) *</div><div class="line">                                         fe_values.<a class="code" href="classFEValuesBase.html#a46aefdb527125dafb59dcba92a0f256e">shape_grad</a>(j, q_point) *</div><div class="line">                                         fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line"></div><div class="line">                  cell_rhs(i) += rhs_value *                         </div><div class="line">                                 fe_values.<a class="code" href="classFEValuesBase.html#a1dd48cb744013c448d57f8f77640c08d">shape_value</a>(i, q_point) * </div><div class="line">                                 fe_values.<a class="code" href="classFEValuesBase.html#abade89efb068b71b7ced7082012a2441">JxW</a>(q_point);</div><div class="line">                }</div><div class="line">            }</div><div class="line"></div><div class="line">          cell-&gt;get_dof_indices(local_dof_indices);</div><div class="line">          constraints.<a class="code" href="classAffineConstraints.html#a373fbdacd8c486e675b8d2bff8943192">distribute_local_to_global</a>(cell_matrix,</div><div class="line">                                                 cell_rhs,</div><div class="line">                                                 local_dof_indices,</div><div class="line">                                                 system_matrix,</div><div class="line">                                                 system_rhs);</div><div class="line">        }</div><div class="line"></div><div class="line">    system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">    system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::solve()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;solve&quot;</span>);</div><div class="line">    <a class="code" href="namespaceLinearAlgebraDealII.html#a9fe13d579422411556954ab3f28a59be">LA::MPI::Vector</a>    completely_distributed_solution(locally_owned_dofs,</div><div class="line">                                                    mpi_communicator);</div><div class="line"></div><div class="line">    <a class="code" href="classSolverControl.html">SolverControl</a> solver_control(dof_handler.n_dofs(), 1e-12);</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc.html#a9d2c129d77c89b40badfb65d94c5b955">LA::SolverCG</a> solver(solver_control, mpi_communicator);</div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc.html#a9d2c129d77c89b40badfb65d94c5b955">LA::SolverCG</a> solver(solver_control);</div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line">    <a class="code" href="namespaceLinearAlgebraPETSc_1_1MPI.html#a41f11f7a1992c6d6aa9367b12c68f791">LA::MPI::PreconditionAMG</a> preconditioner;</div><div class="line"></div><div class="line">    LA::MPI::PreconditionAMG::AdditionalData data;</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">    data.symmetric_operator = <span class="keyword">true</span>;</div><div class="line"><span class="preprocessor">#else</span></div><div class="line">    <span class="comment">/* Trilinos defaults are good */</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">    preconditioner.<a class="code" href="classPETScWrappers_1_1PreconditionBoomerAMG.html#a3c82672eac786d7f369b296439859ff2">initialize</a>(system_matrix, data);</div><div class="line"></div><div class="line">    solver.solve(system_matrix,</div><div class="line">                 completely_distributed_solution,</div><div class="line">                 system_rhs,</div><div class="line">                 preconditioner);</div><div class="line"></div><div class="line">    pcout &lt;&lt; <span class="stringliteral">&quot;   Solved in &quot;</span> &lt;&lt; solver_control.last_step() &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span></div><div class="line">          &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    constraints.<a class="code" href="classAffineConstraints.html#a7b3d3f295bb56d6cd6856bdc6cbe8a01">distribute</a>(completely_distributed_solution);</div><div class="line"></div><div class="line">    locally_relevant_solution = completely_distributed_solution;</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::refine_grid()</div><div class="line">  {</div><div class="line">    <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;refine&quot;</span>);</div><div class="line"></div><div class="line">    Vector&lt;float&gt; estimated_error_per_cell(triangulation.<a class="code" href="classTriangulation.html#a5ea5c9957dbb566a562bbe2c0f3971e9">n_active_cells</a>());</div><div class="line">    <a class="code" href="classKellyErrorEstimator.html#ae2269e1c9903e9d863b7abd54948af00">KellyErrorEstimator&lt;dim&gt;::estimate</a>(</div><div class="line">      dof_handler,</div><div class="line">      <a class="code" href="classQGauss.html">QGauss&lt;dim - 1&gt;</a>(fe.degree + 1),</div><div class="line">      std::map&lt;<a class="code" href="classunsigned_01int.html">types::boundary_id</a>, <span class="keyword">const</span> <a class="code" href="classFunction.html">Function&lt;dim&gt;</a> *&gt;(),</div><div class="line">      locally_relevant_solution,</div><div class="line">      estimated_error_per_cell);</div><div class="line">    <a class="code" href="namespaceparallel_1_1distributed_1_1GridRefinement.html#aa2ffb707a796ae6dedb75036606ef2e6">parallel::distributed::GridRefinement::refine_and_coarsen_fixed_number</a>(</div><div class="line">      triangulation, estimated_error_per_cell, 0.3, 0.03);</div><div class="line">    triangulation.<a class="code" href="classTriangulation.html#ac8b4fbb207303ec7f5ef758821ecd8cb">execute_coarsening_and_refinement</a>();</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> LaplaceProblem&lt;dim&gt;::output_results(<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a6ed7c846331069f406b8c9933c37fda4">attach_dof_handler</a>(dof_handler);</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(locally_relevant_solution, <span class="stringliteral">&quot;u&quot;</span>);</div><div class="line"></div><div class="line">    Vector&lt;float&gt; subdomain(triangulation.<a class="code" href="classTriangulation.html#a5ea5c9957dbb566a562bbe2c0f3971e9">n_active_cells</a>());</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i = 0; i &lt; subdomain.size(); ++i)</div><div class="line">      subdomain(i) = triangulation.<a class="code" href="classTriangulation.html#a44ea82a097d8317c98fa422307aff874">locally_owned_subdomain</a>();</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#a79cbe2f02f8dfb85026c71d783dbb703">add_data_vector</a>(subdomain, <span class="stringliteral">&quot;subdomain&quot;</span>);</div><div class="line"></div><div class="line">    data_out.<a class="code" href="classDataOut.html#a087f63e22f0614bca326dbdca288c646">build_patches</a>();</div><div class="line"></div><div class="line">    data_out.<a class="code" href="classDataOutInterface.html#a0864e51eb173c87e2a3edc9391ea8009">write_vtu_with_pvtu_record</a>(</div><div class="line">      <span class="stringliteral">&quot;./&quot;</span>, <span class="stringliteral">&quot;solution&quot;</span>, cycle, mpi_communicator, 2, 8);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream_1_1internal_1_1tbb__no__coloring.html#a8673698a405bf47aa24002aeb6d76d70">LaplaceProblem&lt;dim&gt;::run</a>()</div><div class="line">  {</div><div class="line">    pcout &lt;&lt; <span class="stringliteral">&quot;Running with &quot;</span></div><div class="line"><span class="preprocessor">#ifdef USE_PETSC_LA</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot;PETSc&quot;</span></div><div class="line"><span class="preprocessor">#else</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot;Trilinos&quot;</span></div><div class="line"><span class="preprocessor">#endif</span></div><div class="line">          &lt;&lt; <span class="stringliteral">&quot; on &quot;</span> &lt;&lt; <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">Utilities::MPI::n_mpi_processes</a>(mpi_communicator)</div><div class="line">          &lt;&lt; <span class="stringliteral">&quot; MPI rank(s)...&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_cycles = 8;</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle = 0; cycle &lt; n_cycles; ++cycle)</div><div class="line">      {</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">          {</div><div class="line">            <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a>(triangulation);</div><div class="line">            triangulation.<a class="code" href="classTriangulation.html#a6ad0b3fb24aae17f4668427a433dea19">refine_global</a>(5);</div><div class="line">          }</div><div class="line">        <span class="keywordflow">else</span></div><div class="line">          refine_grid();</div><div class="line"></div><div class="line">        setup_system();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">              &lt;&lt; triangulation.<a class="code" href="classTriangulation.html#a584733c8499dbd140694bfe04e0963ca">n_global_active_cells</a>() &lt;&lt; std::endl</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span> &lt;&lt; dof_handler.n_dofs()</div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        assemble_system();</div><div class="line">        solve();</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">Utilities::MPI::n_mpi_processes</a>(mpi_communicator) &lt;= 32)</div><div class="line">          {</div><div class="line">            <a class="code" href="classTimerOutput_1_1Scope.html">TimerOutput::Scope</a> t(computing_timer, <span class="stringliteral">&quot;output&quot;</span>);</div><div class="line">            output_results(cycle);</div><div class="line">          }</div><div class="line"></div><div class="line">        computing_timer.print_summary();</div><div class="line">        computing_timer.reset();</div><div class="line"></div><div class="line">        pcout &lt;&lt; std::endl;</div><div class="line">      }</div><div class="line">  }</div><div class="line">} <span class="comment">// namespace Step40</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> *argv[])</div><div class="line">{</div><div class="line">  <span class="keywordflow">try</span></div><div class="line">    {</div><div class="line">      <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">      <span class="keyword">using namespace </span>Step40;</div><div class="line"></div><div class="line">      <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">      LaplaceProblem&lt;2&gt; laplace_problem_2d;</div><div class="line">      laplace_problem_2d.run();</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (...)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl</div><div class="line">                &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --> </div></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.13-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
