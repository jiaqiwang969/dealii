ely independent. This technique is particularly efficient for the simulation of fluid-structure interaction problems, where the configuration of the embedded structure is part of the problem itself, and one solves a (possibly non-linear) elastic problem to determine the (time dependent) configuration of  [2.x.5814] , and a (possibly non-linear) flow problem in  [2.x.5815] , plus coupling conditions on the interface between the fluid and the solid. 

In this tutorial program we keep things a little simpler, and we assume that the configuration of the embedded domain is given in one of two possible ways: 

- as a deformation mapping  [2.x.5816] , defined on a continuous finite dimensional space on  [2.x.5817]  and representing, for any point  [2.x.5818] , its coordinate  [2.x.5819]  in  [2.x.5820] ; 

- as a displacement mapping  [2.x.5821]  for  [2.x.5822] , representing for any point  [2.x.5823]  the displacement vector applied in order to deform  [2.x.5824]  to its actual configuration  [2.x.5825] . 

We define the embedded reference domain  [2.x.5826]  `embedded_grid`: on this triangulation we construct a finite dimensional space (`embedded_configuration_dh`) to describe either the deformation or the displacement through a FiniteElement system of FE_Q objects (`embedded_configuration_fe`). This finite dimensional space is used only to interpolate a user supplied function (`embedded_configuration_function`) representing either  [2.x.5827]  (if the parameter `use_displacement` is set to  [2.x.5828]  or  [2.x.5829]  (if the parameter `use_displacement` is set to  [2.x.5830]  

The Lagrange multiplier  [2.x.5831]  and the user supplied function  [2.x.5832]  are defined through another finite dimensional space `embedded_dh`, and through another FiniteElement `embedded_fe`, using the same reference domain. In order to take into account the deformation of the domain, either a MappingFEField or a MappingQEulerian object are initialized with the `embedded_configuration` vector. 

In the embedding space, a standard finite dimensional space `space_dh` is constructed on the embedding grid `space_grid`, using the FiniteElement `space_fe`, following almost verbatim the approach taken in step-6. 

We represent the discretizations of the spaces  [2.x.5833]  and  [2.x.5834]  with 

[1.x.2248] and 

[1.x.2249] respectively, where  [2.x.5835]  is the dimension of `space_dh`, and  [2.x.5836]  the dimension of `embedded_dh`. 

Once all the finite dimensional spaces are defined, the variational formulation of the problem above leaves us with the following finite dimensional system of equations: 

[1.x.2250] 

where 

[1.x.2251] 



While the matrix  [2.x.5837]  is the standard stiffness matrix for the Poisson problem on  [2.x.5838] , and the vector  [2.x.5839]  is a standard right-hand-side vector for a finite element problem with forcing term  [2.x.5840]  on  [2.x.5841] , (see, for example, step-3), the matrix  [2.x.5842]  or its transpose  [2.x.5843]  are non-standard since they couple information on two non-matching grids. 

In particular, the integral that appears in the computation of a single entry of  [2.x.5844] , is computed on  [2.x.5845] . As usual in finite elements we split this integral into contributions from all cells of the triangulation used to discretize  [2.x.5846] , we transform the integral on  [2.x.5847]  to an integral on the reference element  [2.x.5848] , where  [2.x.5849]  is the mapping from  [2.x.5850]  to  [2.x.5851] , and compute the integral on  [2.x.5852]  using a quadrature formula: 

[1.x.2252] 

Computing this sum is non-trivial because we have to evaluate  [2.x.5853] . In general, if  [2.x.5854]  and  [2.x.5855]  are not aligned, the point  [2.x.5856]  is completely arbitrary with respect to  [2.x.5857] , and unless we figure out a way to interpolate all basis functions of  [2.x.5858]  on an arbitrary point on  [2.x.5859] , we cannot compute the integral needed for an entry of the matrix  [2.x.5860] . 

To evaluate  [2.x.5861]  the following steps needs to be taken (as shown in the picture below): 

- For a given cell  [2.x.5862]  in  [2.x.5863]  compute the real point  [2.x.5864] , where  [2.x.5865]  is one of the quadrature points used for the integral on  [2.x.5866] . 

- Find the cell of  [2.x.5867]  in which  [2.x.5868]  lies. We shall call this element  [2.x.5869] . 

- To evaluate the basis function use the inverse of the mapping  [2.x.5870]  that transforms the reference element  [2.x.5871]  into the element  [2.x.5872] :  [2.x.5873] . 

<p align="center"> <img   src="https://www.dealii.org/images/steps/developer/step-60.C_interpolation.png"   alt="">  [2.x.5874]  

The three steps above can be computed by calling, in turn, 

-  [2.x.5875]  followed by 

-  [2.x.5876]  We then 

- construct a custom Quadrature formula, containing the point in the reference  cell and then 

- construct an FEValues object, with the given quadrature formula, and  initialized with the cell obtained in the first step. 

This is what the deal.II function  [2.x.5877]  does when evaluating a finite element field (not just a single shape function) at an arbitrary point; but this would be inefficient in this case. 

A better solution is to use a convenient wrapper to perform the first three steps on a collection of points:  [2.x.5878]  If one is actually interested in computing the full coupling matrix, then it is possible to call the method  [2.x.5879]  that performs the above steps in an efficient way, reusing all possible data structures, and gathering expensive steps together. This is the function we'll be using later in this tutorial. 

We solve the final saddle point problem by an iterative solver, applied to the Schur complement  [2.x.5880]  (whose construction is described, for example, in step-20), and we construct  [2.x.5881]  using LinearOperator classes. 




[1.x.2253] 

The problem we solve here is identical to step-4, with the difference that we impose some constraints on an embedded domain  [2.x.5882] . The tutorial is written in a dimension independent way, and in the results section we show how to vary both `dim` and `spacedim`. 

The tutorial is compiled for `dim` equal to one and `spacedim` equal to two. If you want to run the program in embedding dimension `spacedim` equal to three, you will most likely want to change the reference domain for  [2.x.5883]  to be, for example, something you read from file, or a closed sphere that you later deform to something more interesting. 

In the default scenario,  [2.x.5884]  has co-dimension one, and this tutorial program implements the Fictitious Boundary Method. As it turns out, the same techniques are used in the Variational Immersed Finite Element Method, and the coupling operator  [2.x.5885]  defined above is the same in almost all of these non-matching methods. 

The embedded domain is assumed to be included in  [2.x.5886] , which we take as the unit square  [2.x.5887] . The definition of the fictitious domain  [2.x.5888]  can be modified through the parameter file, and can be given as a mapping from the reference interval  [2.x.5889]  to a curve in  [2.x.5890] . 

If the curve is closed, then the results will be similar to running the same problem on a grid whose boundary is  [2.x.5891] . The program will happily run also with a non-closed  [2.x.5892] , although in those cases the mathematical formulation of the problem is more difficult, since  [2.x.5893]  will have a boundary by itself that has co-dimension two with respect to the domain  [2.x.5894] . 




[1.x.2254] 

 [2.x.5895]   [2.x.5896]  Glowinski, R., T.-W. Pan, T.I. Hesla, and D.D. Joseph. 1999. “A Distributed   Lagrange Multiplier/fictitious Domain Method for Particulate Flows.”   International Journal of Multiphase Flow 25 (5). Pergamon: 755–94. 

 [2.x.5897]  Boffi, D., L. Gastaldi, L. Heltai, and C.S. Peskin. 2008. “On the   Hyper-Elastic Formulation of the Immersed Boundary Method.” Computer Methods   in Applied Mechanics and Engineering 197 (25–28). 

 [2.x.5898]  Heltai, L., and F. Costanzo. 2012. “Variational Implementation of Immersed   Finite Element Methods.” Computer Methods in Applied Mechanics and Engineering   229–232.  [2.x.5899]  


examples/step-60/doc/results.dox 



[1.x.2255] 

The directory in which this program is run does not contain a parameter file by default. On the other hand, this program wants to read its parameters from a file called parameters.prm -- and so, when you execute it the first time, you will get an exception that no such file can be found: 

[1.x.2256] 



However, as the error message already states, the code that triggers the exception will also generate a parameters.prm file that simply contains the default values for all parameters this program cares about. By inspection of the parameter file, we see the following: 

[1.x.2257] 



If you now run the program, you will get a file called `used_parameters.prm`, containing a shorter version of the above parameters (without comments and documentation), documenting all parameters that were used to run your program: 

[1.x.2258] 



The rationale behind creating first `parameters.prm` file (the first time the program is run) and then a `used_parameters.prm` (every other times you run the program), is because you may want to leave most parameters to their default values, and only modify a handful of them. 

For example, you could use the following (perfectly valid) parameter file with this tutorial program: 

[1.x.2259] 



and you would obtain exactly the same results as in test case 1 below. 

[1.x.2260] 

For the default problem the value of  [2.x.5900]  on  [2.x.5901]  is set to the constant  [2.x.5902] : this is like imposing a constant Dirichlet boundary condition on  [2.x.5903] , seen as boundary of the portion of  [2.x.5904]  inside  [2.x.5905] . Similarly on  [2.x.5906]  we have zero Dirichlet boundary conditions. 


<div class="twocolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-60.1_no_grid.png"            alt = ""            width="500">     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-60.1_grid.png"            alt = ""            width="500">     </div>   </div> </div> 

The output of the program will look like the following: 

[1.x.2261] 



You may notice that, in terms of CPU time, assembling the coupling system is twice as expensive as assembling the standard Poisson system, even though the matrix is smaller. This is due to the non-matching nature of the discretization. Whether this is acceptable or not, depends on the applications. 

If the problem was set in a three-dimensional setting, and the immersed mesh was time dependent, it would be much more expensive to recreate the mesh at each step rather than use the technique we present here. Moreover, you may be able to create a very fast and optimized solver on a uniformly refined square or cubic grid, and embed the domain where you want to perform your computation using the technique presented here. This would require you to only have a surface representatio of your domain (a much cheaper and easier mesh to produce). 

To play around a little bit, we are going to complicate a little the fictitious domain as well as the boundary conditions we impose on it. 

[1.x.2262] 

If we use the following parameter file: 

[1.x.2263] 



We get a "flowery" looking domain, where we impose a linear boundary condition  [2.x.5907] . This test shows that the method is actually quite accurate in recovering an exactly linear function from its boundary conditions, and even though the meshes are not aligned, we obtain a pretty good result. 

Replacing  [2.x.5908]  with  [2.x.5909] , i.e., modifying the parameter file such that we have 

[1.x.2264] 

produces the saddle on the right. 

<div class="twocolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-60.3_grid.png"            alt = ""            width="500">     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-60.4_grid.png"            alt = ""            width="500">     </div>   </div> </div> 

[1.x.2265] 

[1.x.2266] 

[1.x.2267] 

While the current tutorial program is written for `spacedim` equal to two, there are only minor changes you have to do in order for the program to run in different combinations of dimensions. 

If you want to run with `spacedim` equal to three and `dim` equal to two, then you will almost certainly want to perform the following changes: 

- use a different reference domain for the embedded grid, maybe reading it from   a file. It is not possible to construct a smooth closed surface with one   single parametrization of a square domain, therefore you'll most likely want   to use a reference domain that is topologically equivalent to a the boundary   of a sphere. 

- use a displacement instead of the deformation to map  [2.x.5910]  into  [2.x.5911]  

[1.x.2268] 

We have seen in other tutorials (for example in step-5 and step-54) how to read grids from input files. A nice generalization for this tutorial program would be to allow the user to select a grid to read from the parameter file itself, instead of hardcoding the mesh type in the tutorial program itself. 

[1.x.2269] 

At the moment, we have no preconditioner on the Schur complement. This is ok for two dimensional problems, where a few hundred iterations bring the residual down to the machine precision, but it's not going to work in three dimensions. 

It is not obvious what a good preconditioner would be here. The physical problem we are solving with the Schur complement, is to associate to the Dirichlet data  [2.x.5912] , the value of the Lagrange multiplier  [2.x.5913] .  [2.x.5914]  can be interpreted as the *jump* in the normal gradient that needs to be imposed on  [2.x.5915]  across  [2.x.5916] , in order to obtain the Dirichlet data  [2.x.5917] . 

So  [2.x.5918]  is some sort of Neumann to Dirichlet map, and we would like to have a good approximation for the Dirichlet to Neumann map. A possibility would be to use a Boundary Element approximation of the problem on  [2.x.5919] , and construct a rough approximation of the hyper-singular operator for the Poisson problem associated to  [2.x.5920] , which is precisely a Dirichlet to Neumann map. 

[1.x.2270] 

The simple code proposed here can serve as a starting point for more complex problems which, to be solved, need to be run on parallel code, possibly using distributed meshes (see step-17, step-40, and the documentation for  [2.x.5921]  and  [2.x.5922]  

When using non-matching grids in parallel a problem arises: to compute the matrix  [2.x.5923]  a process needs information about both meshes on the same portion of real space but, when working with distributed meshes, this information may not be available, because the locally owned part of the  [2.x.5924]  triangulation stored on a given processor may not be physically co-located with the locally owned part of the  [2.x.5925]  triangulation stored on the same processor. 

Various strategies can be implemented to tackle this problem: 

- distribute the two meshes so that this constraint is satisfied; 

- use communication for the parts of real space where the constraint is not   satisfied; 

- use a distributed triangulation for the embedding space, and a shared   triangulation for the emdedded configuration. 

The latter strategy is clearly the easiest to implement, as most of the functions used in this tutorial program will work unchanged also in the parallel case. Of course one could use the reversal strategy (that is, have a distributed embedded Triangulation and a shared embedding Triangulation). 

However, this strategy is most likely going to be more expensive, since by definition the embedding grid is larger than the embedded grid, and it makes more sense to distribute the largest of the two grids, maintaining the smallest one shared among all processors. 


examples/step-61/doc/intro.dox 

 [2.x.5926]  

[1.x.2271] 

[1.x.2272] 

[1.x.2273] 

This tutorial program presents an implementation of the "weak Galerkin" finite element method for the Poisson equation. In some sense, the motivation for considering this method starts from the same point as in step-51: We would like to consider discontinuous shape functions, but then need to address the fact that the resulting problem has a much larger number of degrees of freedom compared to the usual continuous Galerkin method (because, for example, each vertex carries as many degrees of freedom as there are adjacent cells). We also have to address the fact that, unlike in the continuous Galerkin method, [1.x.2274] degree of freedom on one cell couples with all of the degrees of freedom on each of its face neighbor cells. Consequently, the matrix one gets from the "traditional" discontinuous Galerkin methods are both large and relatively dense. 

Both the hybridized discontinuous Galerkin method (HDG) in step-51 and the weak Galerkin (WG) method in this tutorial address the issue of coupling by introducing additional degrees of freedom whose shape functions only live on a face between cells (i.e., on the "skeleton" of the mesh), and which therefore "insulate" the degrees of freedom on the adjacent cells from each other: cell degrees of freedom only couple with other cell degrees of freedom on the same cell, as well as face degrees of freedom, but not with cell degrees of freedom on neighboring cells. Consequently, the coupling of shape functions for these cell degrees of freedom indeed couple on exactly one cell and the degrees of freedom defined on its faces. 

For a given equation, say the second order Poisson equation, the difference between the HDG and the WG method is how precisely one formulates the problem that connects all of these different shape functions. (Indeed, for some WG and HDG formulation, it is possible to show that they are equivalent.) The HDG does things by reformulating second order problems in terms of a system of first order equations and then conceptually considers the face degrees of freedom to be "fluxes" of this first order system. In contrast, the WG method keeps things in second order form and considers the face degrees of freedom as of the same type as the primary solution variable, just restricted to the lower-dimensional faces. For the purposes of the equation, one then needs to somehow "extend" these shape functions into the interior of the cell when defining what it means to apply a differential operator to them. Compared to the HDG, the method has the advantage that it does not lead to a proliferation of unknowns due to rewriting the equation as a first-order system, but it is also not quite as easy to implement. However, as we will see in the following, this additional effort is not prohibitive. 




[1.x.2275] 

Weak Galerkin Finite Element Methods (WGFEMs) use discrete weak functions to approximate scalar unknowns, and discrete weak gradients to approximate classical gradients. The method was originally introduced by Junping Wang and Xiu Ye in the paper [1.x.2276][1.x.2277]. Compared to the continuous Galerkin method, the weak Galerkin method satisfies important physical properties, namely local mass conservation and bulk normal flux continuity. It results in a SPD linear system, and optimal convergence rates can be obtained with mesh refinement. 




[1.x.2278] This program solves the Poisson equation using the weak Galerkin finite element method: 

[1.x.2279] 

where  [2.x.5927]  is a bounded domain. In the context of the flow of a fluid through a porous medium,  [2.x.5928]  is the pressure,  [2.x.5929]  is a permeability tensor,  [2.x.5930]  is the source term, and  [2.x.5931]  represent Dirichlet and Neumann boundary conditions. We can introduce a flux,  [2.x.5932] , that corresponds to the Darcy velocity (in the way we did in step-20) and this variable will be important in the considerations below. 

In this program, we will consider a test case where the exact pressure is  [2.x.5933]  on the unit square domain, with homogeneous Dirichelet boundary conditions and  [2.x.5934]  the identity matrix. Then we will calculate  [2.x.5935]  errors of pressure, velocity, and flux. 




[1.x.2280] 

The Poisson equation above has a solution  [2.x.5936]  that needs to satisfy the weak formulation of the problem, 

[1.x.2281] 

for all test functions  [2.x.5937] , where 

[1.x.2282] 

and 

[1.x.2283] 

Here, we have integrated by parts in the bilinear form, and we are evaluating the gradient of  [2.x.5938]  in the interior and the values of  [2.x.5939]  on the boundary of the domain. All of this is well defined because we assume that the solution is in  [2.x.5940]  for which taking the gradient and evaluating boundary values are valid operations. 

The idea of the weak Galerkin method is now to approximate the exact  [2.x.5941]  solution with a [1.x.2284]  [2.x.5942] . This function may only be discontinuous along interfaces between cells, and because we will want to evaluate this function also along interfaces, we have to prescribe not only what values it is supposed to have in the cell interiors but also its values along interfaces. We do this by saying that  [2.x.5943]  is actually a tuple,  [2.x.5944] , though it's really just a single function that is either equal to  [2.x.5945]  or  [2.x.5946] , depending on whether it is evaluated at a point  [2.x.5947]  that lies in the cell interior or on cell interfaces. 

We would then like to simply stick this approximation into the bilinear form above. This works for the case where we have to evaluate the test function  [2.x.5948]  on the boundary (where we would simply take its interface part  [2.x.5949] ) but we have to be careful with the gradient because that is only defined in cell interiors. Consequently, the weak Galerkin scheme for the Poisson equation is defined by 

[1.x.2285] 

for all discrete test functions  [2.x.5950] , where 

[1.x.2286] 

and 

[1.x.2287] 

The key point is that here, we have replaced the gradient  [2.x.5951]  by the [1.x.2288] operator  [2.x.5952]  that makes sense for our peculiarly defined approximation  [2.x.5953] . 

The question is then how that operator works. For this, let us first say how we think of the discrete approximation  [2.x.5954]  of the pressure. As mentioned above, the "function"  [2.x.5955]  actually consists of two parts: the values  [2.x.5956]  in the interior of cells, and  [2.x.5957]  on the interfaces. We have to define discrete (finite-dimensional) function spaces for both of these; in this program, we will use FE_DGQ for  [2.x.5958]  as the space in the interior of cells (defined on each cell, but in general discontinuous along interfaces), and FE_FaceQ for  [2.x.5959]  as the space on the interfaces. 

Then let us consider just a single cell (because the integrals above are all defined cell-wise, and because the weak discrete gradient is defined cell-by-cell). The restriction of  [2.x.5960]  to cell  [2.x.5961] ,  [2.x.5962]  then consists of the pair  [2.x.5963] . In essence, we can think of  [2.x.5964]  of some function defined on  [2.x.5965]  that approximates the gradient; in particular, if  [2.x.5966]  was the restriction of a differentiable function (to the interior and boundary of  [2.x.5967]  -- which would make it continuous between the interior and boundary), then  [2.x.5968]  would simply be the exact gradient  [2.x.5969] . But, since  [2.x.5970]  is not continuous between interior and boundary of  [2.x.5971] , we need a more general definition; furthermore, we can not deal with arbitrary functions, and so require that  [2.x.5972]  is also in a finite element space (which, since the gradient is a vector, has to be vector-valued, and because the weak gradient is defined on each cell separately, will also be discontinuous between cells). 

The way this is done is to define this weak gradient operator  [2.x.5973]  (where  [2.x.5974]  is the vector-valued Raviart-Thomas space of order  [2.x.5975]  on cell  [2.x.5976] ) in the following way: 

[1.x.2289] 

for all test functions  [2.x.5977] . This is, in essence, simply an application of the integration-by-parts formula. In other words, for a given  [2.x.5978] , we need to think of  [2.x.5979]  as that Raviart-Thomas function of degree  [2.x.5980]  for which the left hand side and right hand side are equal for all test functions. 

A key point to make is then the following: While the usual gradient  [2.x.5981]  is a *local* operator that computes derivatives based simply on the value of a function at a point and its (infinitesimal) neighborhood, the weak discrete gradient  [2.x.5982]  does not have this property: It depends on the values of the function it is applied to on the entire cell, including the cell's boundary. Both are, however, linear operators as is clear from the definition of  [2.x.5983]  above, and that will allow us to represent  [2.x.5984]  via a matrix in the discussion below. 

 [2.x.5985]  It may be worth pointing out that while the weak discrete   gradient is an element of the Raviart-Thomas space  [2.x.5986]  on each   cell  [2.x.5987] , it is discontinuous between cells. On the other hand, the   Raviart-Thomas space  [2.x.5988]  defined on the entire   mesh and implemented by the FE_RaviartThomas class represents   functions that have continuous normal components at interfaces   between cells. This means that [1.x.2290],  [2.x.5989]    is not in  [2.x.5990] , even though it is on every cell  [2.x.5991]  in  [2.x.5992] .   Rather, it is in a "broken" Raviart-Thomas space that below we will   represent by the symbol  [2.x.5993] . (The term "broken" here refers to   the process of "breaking something apart", and not to the synonym to   the expression "not functional".) One might therefore (rightfully) argue that   the notation used in the weak Galerkin literature is a bit misleading,   but as so often it all depends on the context in which a certain   notation is used -- in the current context, references to the   Raviart-Thomas space or element are always understood to be to the   "broken" spaces. 

 [2.x.5994]  deal.II happens to have an implementation of this broken Raviart-Thomas   space: The FE_DGRT class. As a consequence, in this tutorial we will simply   always use the FE_DGRT class, even though in all of those places where   we have to compute cell-local matrices and vectors, it makes no difference. 




[1.x.2291] 

Since  [2.x.5995]  is an element of a finite element space, we can expand it in a basis as we always do, i.e., we can write 

[1.x.2292] 

Here, since  [2.x.5996]  has two components (the interior and the interface components), the same must hold true for the basis functions  [2.x.5997] , which we can write as  [2.x.5998] . If you've followed the descriptions in step-8, step-20, and the  [2.x.5999]  "documentation module on vector-valued problems", it will be no surprise that for some values of  [2.x.6000] ,  [2.x.6001]  will be zero, whereas for other values of  [2.x.6002] ,  [2.x.6003]  will be zero -- i.e., shape functions will be of either one or the other kind. That is not important, here, however. What is important is that we need to wonder how we can represent  [2.x.6004]  because that is clearly what will appear in the problem when we want to implement the bilinear form 

[1.x.2293] 



The key point is that  [2.x.6005]  is known to be a member of the "broken" Raviart-Thomas space  [2.x.6006] . What this means is that we can represent (on each cell  [2.x.6007]  separately) 

[1.x.2294] 

where the functions  [2.x.6008] , and where  [2.x.6009]  is a matrix of dimension 

[1.x.2295] 

(That the weak discrete gradient can be represented as a matrix should not come as a surprise: It is a linear operator from one finite dimensional space to another finite dimensional space. If one chooses bases for both of these spaces, then [1.x.2296] can of course be written as a matrix mapping the vector of expansion coefficients with regards to the basis of the domain space of the operator, to the vector of expansion coefficients with regards to the basis in the image space.) 

Using this expansion, we can easily use the definition of the weak discrete gradient above to define what the matrix is going to be: 

[1.x.2297] 

for all test functions  [2.x.6010] . 

This clearly leads to a linear system of the form 

[1.x.2298] 

with 

[1.x.2299] 

and consequently 

[1.x.2300] 

(In this last step, we have assumed that the indices  [2.x.6011]  only range over those degrees of freedom active on cell  [2.x.6012] , thereby ensuring that the mass matrix on the space  [2.x.6013]  is invertible.) Equivalently, using the symmetry of the matrix  [2.x.6014] , we have that 

[1.x.2301] 

Also worth pointing out is that the matrices  [2.x.6015]  and  [2.x.6016]  are of course not square but rectangular. 




[1.x.2302] 

Having explained how the weak discrete gradient is defined, we can now come back to the question of how the linear system for the equation in question should be assembled. Specifically, using the definition of the bilinear form  [2.x.6017]  shown above, we then need to compute the elements of the local contribution to the global matrix, 

[1.x.2303] 

As explained above, we can expand  [2.x.6018]  in terms of the Raviart-Thomas basis on each cell, and similarly for  [2.x.6019] : 

[1.x.2304] 

By re-arranging sums, this yields the following expression: 

[1.x.2305] 

So, if we have the matrix  [2.x.6020]  for each cell  [2.x.6021] , then we can easily compute the contribution  [2.x.6022]  for cell  [2.x.6023]  to the matrix  [2.x.6024]  as follows: 

[1.x.2306] 

Here, 

[1.x.2307] 

which is really just the mass matrix on cell  [2.x.6025]  using the Raviart-Thomas basis and weighting by the permeability tensor  [2.x.6026] . The derivation here then shows that the weak Galerkin method really just requires us to compute these  [2.x.6027]  and  [2.x.6028]  matrices on each cell  [2.x.6029] , and then  [2.x.6030] , which is easily computed. The code to be shown below does exactly this. 

Having so computed the contribution  [2.x.6031]  of cell  [2.x.6032]  to the global matrix, all we have to do is to "distribute" these local contributions into the global matrix. How this is done is first shown in step-3 and step-4. In the current program, this will be facilitated by calling  [2.x.6033]  

A linear system of course also needs a right hand side. There is no difficulty associated with computing the right hand side here other than the fact that we only need to use the cell-interior part  [2.x.6034]  for each shape function  [2.x.6035] . 




[1.x.2308][1.x.2309] 

The discussions in the previous sections have given us a linear system that we can solve for the numerical pressure  [2.x.6036] . We can use this to compute an approximation to the variable  [2.x.6037]  that corresponds to the velocity with which the medium flows in a porous medium if this is the model we are trying to solve. This kind of step -- computing a derived quantity from the solution of the discrete problem -- is typically called "post-processing". 

Here, instead of using the exact gradient of  [2.x.6038] , let us instead use the discrete weak gradient of  [2.x.6039]  to calculate the velocity on each element. As discussed above, on each element the gradient of the numerical pressure  [2.x.6040]  can be approximated by discrete weak gradients   [2.x.6041] : 

[1.x.2310] 



On cell  [2.x.6042] , the numerical velocity  [2.x.6043]  can be written as 

[1.x.2311] 

where  [2.x.6044]  is the expansion matrix from above, and  [2.x.6045]  is the basis function of the  [2.x.6046]  space on a cell. 

Unfortunately,  [2.x.6047]  may not be in the  [2.x.6048]  space (unless, of course, if  [2.x.6049]  is constant times the identity matrix). So, in order to represent it in a finite element program, we need to project it back into a finite dimensional space we can work with. Here, we will use the  [2.x.6050] -projection to project it back to the (broken)  [2.x.6051]  space. 

We define the projection as  [2.x.6052]  on each cell  [2.x.6053] . For any  [2.x.6054] ,  [2.x.6055]  So, rather than the formula shown above, the numerical velocity on cell  [2.x.6056]  instead becomes 

[1.x.2312] 

and we have the following system to solve for the coefficients  [2.x.6057] : 

[1.x.2313] 

In the implementation below, the matrix with elements  [2.x.6058]  is called  [2.x.6059] , whereas the matrix with elements  [2.x.6060]  is called  [2.x.6061] . 

Then the elementwise velocity is 

[1.x.2314] 

where  [2.x.6062]  is called `cell_velocity` in the code. 

Using this velocity obtained by "postprocessing" the solution, we can define the  [2.x.6063] -errors of pressure, velocity, and flux by the following formulas: 

[1.x.2315] 

where  [2.x.6064]  is the area of the element,  [2.x.6065]  are faces of the element,  [2.x.6066]  are unit normal vectors of each face. The last of these norms measures the accuracy of the normal component of the velocity vectors over the interfaces between the cells of the mesh. The scaling factor  [2.x.6067]  is chosen so as to scale out the difference in the length (or area) of the collection of interfaces as the mesh size changes. 

The first of these errors above is easily computed using  [2.x.6068]  The others require a bit more work and are implemented in the code below. 


examples/step-61/doc/results.dox 



[1.x.2316] 

We run the program with a right hand side that will produce the solution  [2.x.6069]  and with homogeneous Dirichlet boundary conditions in the domain  [2.x.6070] . In addition, we choose the coefficient matrix in the differential operator  [2.x.6071]  as the identity matrix. We test this setup using  [2.x.6072] ,  [2.x.6073]  and  [2.x.6074]  element combinations, which one can select by using the appropriate constructor argument for the `WGDarcyEquation` object in `main()`. We will then visualize pressure values in interiors of cells and on faces. We want to see that the pressure maximum is around 1 and the minimum is around 0. With mesh refinement, the convergence rates of pressure, velocity and flux should then be around 1 for  [2.x.6075]  , 2 for  [2.x.6076] , and 3 for  [2.x.6077] . 




[1.x.2317][1.x.2318] 

The following figures show interior pressures and face pressures using the  [2.x.6078]  element. The mesh is refined 2 times (top) and 4 times (bottom), respectively. (This number can be adjusted in the `make_grid()` function.) When the mesh is coarse, one can see the face pressures  [2.x.6079]  neatly between the values of the interior pressures  [2.x.6080]  on the two adjacent cells. 

 [2.x.6081]  

From the figures, we can see that with the mesh refinement, the maximum and minimum pressure values are approaching the values we expect. Since the mesh is a rectangular mesh and numbers of cells in each direction is even, we have symmetric solutions. From the 3d figures on the right, we can see that on  [2.x.6082] , the pressure is a constant in the interior of the cell, as expected. 

[1.x.2319][1.x.2320] 

We run the code with differently refined meshes (chosen in the `make_grid()` function) and get the following convergence rates of pressure, velocity, and flux (as defined in the introduction). 

 [2.x.6083]  

We can see that the convergence rates of  [2.x.6084]  are around 1. This, of course, matches our theoretical expectations. 




[1.x.2321][1.x.2322] 

We can repeat the experiment from above using the next higher polynomial degree: The following figures are interior pressures and face pressures implemented using  [2.x.6085] . The mesh is refined 4 times.  Compared to the previous figures using  [2.x.6086] , on each cell, the solution is no longer constant on each cell, as we now use bilinear polynomials to do the approximation. Consequently, there are 4 pressure values in one interior, 2 pressure values on each face. 

 [2.x.6087]  

Compared to the corresponding image for the  [2.x.6088]  combination, the solution is now substantially more accurate and, in particular so close to being continuous at the interfaces that we can no longer distinguish the interface pressures  [2.x.6089]  from the interior pressures  [2.x.6090]  on the adjacent cells. 

[1.x.2323][1.x.2324] 

The following are the convergence rates of pressure, velocity, and flux we obtain from using the  [2.x.6091]  element combination: 

 [2.x.6092]  

The convergence rates of  [2.x.6093]  are around 2, as expected. 




[1.x.2325][1.x.2326] 

Let us go one polynomial degree higher. The following are interior pressures and face pressures implemented using  [2.x.6094] , with mesh size  [2.x.6095]  (i.e., 5 global mesh refinement steps). In the program, we use `data_out_face.build_patches(fe.degree)` when generating graphical output (see the documentation of  [2.x.6096]  which here implies that we divide each 2d cell interior into 4 subcells in order to provide a better visualization of the quadratic polynomials.  [2.x.6097]  




[1.x.2327][1.x.2328] 

As before, we can generate convergence data for the  [2.x.6098]  errors of pressure, velocity, and flux using the  [2.x.6099]  combination: 

 [2.x.6100]  

Once more, the convergence rates of  [2.x.6101]  is as expected, with values around 3. 


examples/step-62/doc/intro.dox 

 [2.x.6102]  

[1.x.2329]  [2.x.6103]  




 [2.x.6104]  As a prerequisite of this program, you need to have HDF5, complex PETSc, and the p4est libraries installed. The installation of deal.II together with these additional libraries is described in the [1.x.2330] file. 

[1.x.2331] A phononic crystal is a periodic nanostructure that modifies the motion of mechanical vibrations or [phonons](https://en.wikipedia.org/wiki/Phonon). Phononic structures can be used to disperse, route and confine mechanical vibrations. These structures have potential applications in [quantum information](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.86.1391) and have been used to study [macroscopic quantum phenomena](https://science.sciencemag.org/content/358/6360/203). Phononic crystals are usually fabricated in [cleanrooms](https://en.wikipedia.org/wiki/Cleanroom). 

In this tutorial we show how to a design a [phononic superlattice cavity](https://doi.org/10.1103/PhysRevA.94.033813) which is a particular type of phononic crystal that can be used to confine mechanical vibrations. A phononic superlattice cavity is formed by two [Distributed Bragg Reflector](https://en.wikipedia.org/wiki/Distributed_Bragg_reflector), mirrors and a  [2.x.6105]  cavity where  [2.x.6106]  is the acoustic wavelength. Acoustic DBRs are  periodic structures where a set of bilayer stacks with contrasting physical properties (sound velocity index) is repeated  [2.x.6107]  times. Superlattice cavities are usually grown on a [Gallium Arsenide](https://en.wikipedia.org/wiki/Gallium_arsenide) wafer by [Molecular Beam Epitaxy](https://en.wikipedia.org/wiki/Molecular-beam_epitaxy). The bilayers correspond to GaAs/AlAs mirror pairs. As shown below, the thickness of the mirror layers (brown and green) is  [2.x.6108]  and the thickness of the cavity (blue) is  [2.x.6109] . 

 [2.x.6110]  

In this tutorial we calculate the [band gap](https://en.wikipedia.org/wiki/Band_gap) and the mechanical resonance of a phononic superlattice cavity but the code presented here can be easily used to design and calculate other types of [phononic crystals](https://science.sciencemag.org/content/358/6360/203). 

The device is a waveguide in which the wave goes from left to right. The simulations of this tutorial are done in 2D, but the code is dimension independent and can be easily used with 3D simulations. The waveguide width is equal to the  [2.x.6111]  dimension of the domain and the waveguide length is equal to the  [2.x.6112]  dimension of the domain. There are two regimes that depend on the waveguide width: 

- Single mode: In this case the width of the structure is much   smaller than the wavelength.   This case can be solved either with FEM (the approach that we take here) or with   a simple semi-analytical   [1D transfer matrix formalism](https://en.wikipedia.org/wiki/Transfer_matrix). 

- Multimode: In this case the width of the structure is larger than the wavelength.   This case can be solved using FEM   or with a [scattering matrix formalism](https://doi.org/10.1103/PhysRevA.94.033813).   Although we do not study this case in this tutorial, it is very easy to reach the multimode   regime by increasing the parameter waveguide width (`dimension_y` in the jupyter notebook). 

The simulations of this tutorial are performed in the frequency domain. To calculate the transmission spectrum, we use a [procedure](https://meep.readthedocs.io/en/latest/Python_Tutorials/Resonant_Modes_and_Transmission_in_a_Waveguide_Cavity/) that is commonly used in time domain [FDTD](https://en.wikipedia.org/wiki/Finite-difference_time-domain_method) simulations. A pulse at a certain frequency is generated on the left side of the structure and the transmitted energy is measured on the right side of the structure. The simulation is run twice. First, we run the simulation with the phononic structure and measure the transmitted energy: 

 [2.x.6113]  

Then, we run the simulation without the phononic structure and measure the transmitted energy. We use the simulation without the structure for the calibration: 

 [2.x.6114]  

The transmission coefficient corresponds to the energy of the first simulation divided by the calibration energy. We repeat this procedure for each frequency step. 




[1.x.2332] What we want to simulate here is the transmission of elastic waves. Consequently, the right description of the problem uses the elastic equations, which in the time domain are given by 

[1.x.2333] 

where the stiffness tensor  [2.x.6115]  depends on the spatial coordinates and the strain is the symmetrized gradient of the displacement, given by 

[1.x.2334] 



[A perfectly matched layer (PML)](https://en.wikipedia.org/wiki/Perfectly_matched_layer) can be used to truncate the solution at the boundaries. A PML is a transformation that results in a complex coordinate stretching. 

Instead of a time domain approach, this tutorial program converts the equations above into the frequency domain by performing a Fourier transform with regard to the time variable. The elastic equations in the frequency domain then read as follows 

[1.x.2335] 

where the coefficients  [2.x.6116]  account for the absorption. There are 3  [2.x.6117]  coefficients in 3D and 2 in 2D. The imaginary par of  [2.x.6118]  is equal to zero outside the PML. The PMLs are reflectionless only for the exact wave equations. When the set of equations is discretized the PML is no longer reflectionless. The reflections can be made arbitrarily small as long as the medium is slowly varying, see [the adiabatic theorem](https://doi.org/10.1103/PhysRevE.66.066608). In the code a quadratic turn-on of the PML has been used. A linear and cubic turn-on is also [known to work](https://doi.org/10.1364/OE.16.011376). These equations can be expanded into 

[1.x.2336] 



[1.x.2337] 

where summation over repeated indices (here  [2.x.6119] , as well as  [2.x.6120]  and  [2.x.6121] ) is as always implied. Note that the strain is no longer symmetric after applying the complex coordinate stretching of the PML. This set of equations can be written as 

[1.x.2338] 



The same as the strain, the stress tensor is not symmetric inside the PML ( [2.x.6122] ). Indeed the fields inside the PML are not physical. It is useful to introduce the tensors  [2.x.6123]  and  [2.x.6124] . 

[1.x.2339] 



We can multiply by  [2.x.6125]  and integrate over the domain  [2.x.6126]  and integrate by parts. 

[1.x.2340] 

It is this set of equations we want to solve for a set of frequencies  [2.x.6127]  in order to compute the transmission coefficient as function of frequency. The linear system becomes 

[1.x.2341] 



[1.x.2342] In this tutorial we use a python [jupyter notebook](https://github.com/dealii/dealii/blob/master/examples/step-62/step-62.ipynb) to set up the parameters and run the simulation. First we create a HDF5 file where we store the parameters and the results of the simulation. 

Each of the simulations (displacement and calibration) is stored in a separate HDF5 group: 

[1.x.2343] 




examples/step-62/doc/results.dox 



[1.x.2344] 

[1.x.2345] 

The results are analyzed in the [jupyter notebook](https://github.com/dealii/dealii/blob/master/examples/step-62/step-62.ipynb) with the following code 

[1.x.2346] 



A phononic cavity is characterized by the [resonance frequency](https://en.wikipedia.org/wiki/Resonance) and the [the quality factor](https://en.wikipedia.org/wiki/Q_factor). The quality factor is equal to the ratio between the stored energy in the resonator and the energy dissipated energy per cycle, which is approximately equivalent to the ratio between the resonance frequency and the [full width at half maximum (FWHM)](https://en.wikipedia.org/wiki/Full_width_at_half_maximum). The FWHM is equal to the bandwidth over which the power of vibration is greater than half the power at the resonant frequency. 

[1.x.2347] 



The square of the amplitude of the mechanical resonance  [2.x.6128]  as a function of the frequency has a gaussian shape 

[1.x.2348] 

where  [2.x.6129]  is the resonance frequency and  [2.x.6130]  is the dissipation rate. We used the previous equation in the jupyter notebook to fit the mechanical resonance. 

Given the values we have chosen for the parameters, one could estimate the resonance frequency analytically. Indeed, this is then confirmed by what we get in this program: the phononic superlattice cavity exhibits a mechanical resonance at 20GHz and a quality factor of 5046. The following images show the transmission amplitude and phase as a function of frequency in the vicinity of the resonance frequency: 

 [2.x.6131]   [2.x.6132]  

The images above suggest that the periodic structure has its intended effect: It really only lets waves of a very specific frequency pass through, whereas all other waves are reflected. This is of course precisely what one builds these sorts of devices for. But it is not quite this easy. In practice, there is really only a "band gap", i.e., the device blocks waves other than the desired one at 20GHz only within a certain frequency range. Indeed, to find out how large this "gap" is within which waves are blocked, we can extend the frequency range to 16 GHz through the appropriate parameters in the input file. We then obtain the following image: 

 [2.x.6133]  

What this image suggests is that in the range of around 18 to around 22 GHz, really only the waves with a frequency of 20 GHz are allowed to pass through, but beyond this range, there are plenty of other frequencies that can pass through the device. 

[1.x.2349] 

We can inspect the mode profile with Paraview or VisIt. As we have discussed, at resonance all the mechanical energy is transmitted and the amplitude of motion is amplified inside the cavity. It can be observed that the PMLs are quite effective to truncate the solution. The following image shows the mode profile at resonance: 

 [2.x.6134]  

On the other hand,  out of resonance all the mechanical energy is reflected. The following image shows the profile at 19.75 GHz. Note the interference between the force pulse and the reflected wave at the position  [2.x.6135] . 

 [2.x.6136]  

[1.x.2350] 

Phononic superlattice cavities find application in [quantum optomechanics](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.86.1391). Here we have presented the simulation of a 2D superlattice cavity, but this code can be used as well to simulate "real world" 3D devices such as [micropillar superlattice cavities](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.060101), which are promising candidates to study macroscopic quantum phenomena. The 20GHz mode of a micropillar superlattice cavity is essentially a mechanical harmonic oscillator that is very well isolated from the environment. If the device is cooled down to 20mK in a dilution fridge, the mode would then become a macroscopic quantum harmonic oscillator. 




[1.x.2351] 

Instead of setting the parameters in the C++ file we could set the parameters using a python script and save them in the HDF5 file that we will use for the simulations. Then the deal.II program will read the parameters from the HDF5 file. 

[1.x.2352] 



In order to read the HDF5 parameters we have to use the  [2.x.6137]  flag. 

[1.x.2353] 




examples/step-63/doc/intro.dox 

 [2.x.6138]  

[1.x.2354] 

 [2.x.6139]  

[1.x.2355] 

[1.x.2356] 

This program solves an advection-diffusion problem using a geometric multigrid (GMG) preconditioner. The basics of this preconditioner are discussed in step-16; here we discuss the necessary changes needed for a non-symmetric PDE. Additionally, we introduce the idea of block smoothing (as compared to point smoothing in step-16), and examine the effects of DoF renumbering for additive and multiplicative smoothers. 

[1.x.2357] The advection-diffusion equation is given by 

[1.x.2358] 

where  [2.x.6140] ,  [2.x.6141]  is the [1.x.2359], and  [2.x.6142]  is a source. A few notes: 

1. If  [2.x.6143] , this is the Laplace equation solved in step-16 (and many other places). 

2. If  [2.x.6144]  then this is the stationary advection equation solved in step-9. 

3. One can define a dimensionless number for this problem, called the [1.x.2360]:  [2.x.6145] , where  [2.x.6146]  is the length scale of the domain. It characterizes the kind of equation we are considering: If  [2.x.6147] , we say the problem is [1.x.2361], else if  [2.x.6148]  we will say the problem is [1.x.2362]. 

For the discussion in this tutorial we will be concerned with advection-dominated flow. This is the complicated case: We know that for diffusion-dominated problems, the standard Galerkin method works just fine, and we also know that simple multigrid methods such as those defined in step-16 are very efficient. On the other hand, for advection-dominated problems, the standard Galerkin approach leads to oscillatory and unstable discretizations, and simple solvers are often not very efficient. This tutorial program is therefore intended to address both of these issues. 




[1.x.2363] 

Using the standard Galerkin finite element method, for suitable test functions  [2.x.6149] , a discrete weak form of the PDE would read 

[1.x.2364] 

where 

[1.x.2365] 



Unfortunately, one typically gets oscillatory solutions with this approach. Indeed, the following error estimate can be shown for this formulation: 

[1.x.2366] 

The infimum on the right can be estimated as follows if the exact solution is sufficiently smooth: 

[1.x.2367] 

where  [2.x.6150]  is the polynomial degree of the finite elements used. As a consequence, we obtain the estimate 

[1.x.2368] 

In other words, the numerical solution will converge. On the other hand, given the definition of  [2.x.6151]  above, we have to expect poor numerical solutions with a large error when  [2.x.6152] , i.e., if the problem has only a small amount of diffusion. 

To combat this, we will consider the new weak form 

[1.x.2369] 

where the sum is done over all cells  [2.x.6153]  with the inner product taken for each cell, and  [2.x.6154]  is a cell-wise constant stabilization parameter defined in  [2.x.6155] . 

Essentially, adding in the discrete strong form residual enhances the coercivity of the bilinear form  [2.x.6156]  which increases the stability of the discrete solution. This method is commonly referred to as [1.x.2370] or [1.x.2371] (streamline upwind/Petrov-Galerkin). 




[1.x.2372] 

One of the goals of this tutorial is to expand from using a simple (point-wise) Gauss-Seidel (SOR) smoother that is used in step-16 (class PreconditionSOR) on each level of the multigrid hierarchy. The term "point-wise" is traditionally used in solvers to indicate that one solves at one "grid point" at a time; for scalar problems, this means to use a solver that updates one unknown of the linear system at a time, keeping all of the others fixed; one would then iterate over all unknowns in the problem and, once done, start over again from the first unknown until these "sweeps" converge. Jacobi, Gauss-Seidel, and SOR iterations can all be interpreted in this way. In the context of multigrid, one does not think of these methods as "solvers", but as "smoothers". As such, one is not interested in actually solving the linear system. It is enough to remove the high-frequency part of the residual for the multigrid method to work, because that allows restricting the solution to a coarser mesh.  Therefore, one only does a few, fixed number of "sweeps" over all unknowns. In the code in this tutorial this is controlled by the "Smoothing steps" parameter. 

But these methods are known to converge rather slowly when used as solvers. While as multigrid smoothers, they are surprisingly good, they can also be improved upon. In particular, we consider "cell-based" smoothers here as well. These methods solve for all unknowns on a cell at once, keeping all other unknowns fixed; they then move on to the next cell, and so on and so forth. One can think of them as "block" versions of Jacobi, Gauss-Seidel, or SOR, but because degrees of freedom are shared among multiple cells, these blocks overlap and the methods are in fact best be explained within the framework of additive and multiplicative Schwarz methods. 

In contrast to step-16, our test problem contains an advective term. Especially with a small diffusion constant  [2.x.6157] , information is transported along streamlines in the given advection direction. This means that smoothers are likely to be more effective if they allow information to travel in downstream direction within a single smoother application. If we want to solve one unknown (or block of unknowns) at a time in the order in which these unknowns (or blocks) are enumerated, then this information propagation property requires reordering degrees of freedom or cells (for the cell-based smoothers) accordingly so that the ones further upstream are treated earlier (have lower indices) and those further downstream are treated later (have larger indices). The influence of the ordering will be visible in the results section. 

Let us now briefly define the smoothers used in this tutorial. For a more detailed introduction, we refer to  [2.x.6158]  and the books  [2.x.6159]  and  [2.x.6160] . A Schwarz preconditioner requires a decomposition 

[1.x.2373] 

of our finite element space  [2.x.6161] . Each subproblem  [2.x.6162]  also has a Ritz projection  [2.x.6163]  based on the bilinear form  [2.x.6164] . This projection induces a local operator  [2.x.6165]  for each subproblem  [2.x.6166] . If  [2.x.6167]  is the orthogonal projector onto  [2.x.6168] , one can show  [2.x.6169] . 

With this we can define an [1.x.2374] for the operator  [2.x.6170]  as 

[1.x.2375] 

In other words, we project our solution into each subproblem, apply the inverse of the subproblem  [2.x.6171] , and sum the contributions up over all  [2.x.6172] . 

Note that one can interpret the point-wise (one unknown at a time) Jacobi method as an additive Schwarz method by defining a subproblem  [2.x.6173]  for each degree of freedom. Then,  [2.x.6174]  becomes a multiplication with the inverse of a diagonal entry of  [2.x.6175] . 

For the "Block Jacobi" method used in this tutorial, we define a subproblem  [2.x.6176]  for each cell of the mesh on the current level. Note that we use a continuous finite element, so these blocks are overlapping, as degrees of freedom on an interface between two cells belong to both subproblems. The logic for the Schwarz operator operating on the subproblems (in deal.II they are called "blocks") is implemented in the class RelaxationBlock. The "Block Jacobi" method is implemented in the class RelaxationBlockJacobi. Many aspects of the class (for example how the blocks are defined and how to invert the local subproblems  [2.x.6177] ) can be configured in the smoother data, see  [2.x.6178]  and  [2.x.6179]  for details. 

So far, we discussed additive smoothers where the updates can be applied independently and there is no information flowing within a single smoother application. A [1.x.2376] addresses this and is defined by 

[1.x.2377] 

In contrast to above, the updates on the subproblems  [2.x.6180]  are applied sequentially. This means that the update obtained when inverting the subproblem  [2.x.6181]  is immediately used in  [2.x.6182] . This becomes visible when writing out the project: 

[1.x.2378] 



When defining the sub-spaces  [2.x.6183]  as whole blocks of degrees of freedom, this method is implemented in the class RelaxationBlockSOR and used when you select "Block SOR" in this tutorial. The class RelaxationBlockSOR is also derived from RelaxationBlock. As such, both additive and multiplicative Schwarz methods are implemented in a unified framework. 

Finally, let us note that the standard Gauss-Seidel (or SOR) method can be seen as a multiplicative Schwarz method with a subproblem for each DoF. 




[1.x.2379] 

We will be considering the following test problem:  [2.x.6184] , i.e., a square with a circle of radius 0.3 centered at the origin removed. In addition, we use  [2.x.6185] ,  [2.x.6186] ,  [2.x.6187] , and Dirichlet boundary values 

[1.x.2380] 



The following figures depict the solutions with (left) and without (right) streamline diffusion. Without streamline diffusion we see large oscillations around the boundary layer, demonstrating the instability of the standard Galerkin finite element method for this problem. 

 [2.x.6188]  


examples/step-63/doc/results.dox 



[1.x.2381] 

[1.x.2382] 

The major advantage for GMG is that it is an  [2.x.6189]  method, that is, the complexity of the problem increases linearly with the problem size. To show then that the linear solver presented in this tutorial is in fact  [2.x.6190] , all one needs to do is show that the iteration counts for the GMRES solve stay roughly constant as we refine the mesh. 

Each of the following tables gives the GMRES iteration counts to reduce the initial residual by a factor of  [2.x.6191] . We selected a sufficient number of smoothing steps (based on the method) to get iteration numbers independent of mesh size. As can be seen from the tables below, the method is indeed  [2.x.6192] . 

[1.x.2383] 

The point-wise smoothers ("Jacobi" and "SOR") get applied in the order the DoFs are numbered on each level. We can influence this using the DoFRenumbering namespace. The block smoothers are applied based on the ordering we set in `setup_smoother()`. We can visualize this numbering. The following pictures show the cell numbering of the active cells in downstream, random, and upstream numbering (left to right): 

 [2.x.6193]  

Let us start with the additive smoothers. The following table shows the number of iterations necessary to obtain convergence from GMRES: 

 [2.x.6194]  

We see that renumbering the DoFs/cells has no effect on convergence speed. This is because these smoothers compute operations on each DoF (point-smoother) or cell (block-smoother) independently and add up the results. Since we can define these smoothers as an application of a sum of matrices, and matrix addition is commutative, the order at which we sum the different components will not affect the end result. 

On the other hand, the situation is different for multiplicative smoothers: 

 [2.x.6195]  

Here, we can speed up convergence by renumbering the DoFs/cells in the advection direction, and similarly, we can slow down convergence if we do the renumbering in the opposite direction. This is because advection-dominated problems have a directional flow of information (in the advection direction) which, given the right renumbering of DoFs/cells, multiplicative methods are able to capture. 

This feature of multiplicative methods is, however, dependent on the value of  [2.x.6196] . As we increase  [2.x.6197]  and the problem becomes more diffusion-dominated, we have a more uniform propagation of information over the mesh and there is a diminished advantage for renumbering in the advection direction. On the opposite end, in the extreme case of  [2.x.6198]  (advection-only), we have a 1st-order PDE and multiplicative methods with the right renumbering become effective solvers: A correct downstream numbering may lead to methods that require only a single iteration because information can be propagated from the inflow boundary downstream, with no information transport in the opposite direction. (Note, however, that in the case of  [2.x.6199] , special care must be taken for the boundary conditions in this case). 




[1.x.2384] 

We will limit the results to runs using the downstream renumbering. Here is a cross comparison of all four smoothers for both  [2.x.6200]  and  [2.x.6201]  elements: 

 [2.x.6202]  

We see that for  [2.x.6203] , both multiplicative smoothers require a smaller combination of smoothing steps and iteration counts than either additive smoother. However, when we increase the degree to a  [2.x.6204]  element, there is a clear advantage for the block smoothers in terms of the number of smoothing steps and iterations required to solve. Specifically, the block SOR smoother gives constant iteration counts over the degree, and the block Jacobi smoother only sees about a 38% increase in iterations compared to 75% and 183% for Jacobi and SOR respectively. 

[1.x.2385] 

Iteration counts do not tell the full story in the optimality of a one smoother over another. Obviously we must examine the cost of an iteration. Block smoothers here are at a disadvantage as they are having to construct and invert a cell matrix for each cell. Here is a comparison of solve times for a  [2.x.6205]  element with 74,496 DoFs: 

 [2.x.6206]  

The smoother that requires the most iterations (Jacobi) actually takes the shortest time (roughly 2/3 the time of the next fastest method). This is because all that is required to apply a Jacobi smoothing step is multiplication by a diagonal matrix which is very cheap. On the other hand, while SOR requires over 3x more iterations (each with 3x more smoothing steps) than block SOR, the times are roughly equivalent, implying that a smoothing step of block SOR is roughly 9x slower than a smoothing step of SOR. Lastly, block Jacobi is almost 6x more expensive than block SOR, which intuitively makes sense from the fact that 1 step of each method has the same cost (inverting the cell matrices and either adding or multiply them together), and block Jacobi has 3 times the number of smoothing steps per iteration with 2 times the iterations. 




[1.x.2386] 

There are a few more important points to mention: 

<ol>  [2.x.6207]  For a mesh distributed in parallel, multiplicative methods cannot be executed over the entire domain. This is because they operate one cell at a time, and downstream cells can only be handled once upstream cells have already been done. This is fine on a single processor: The processor just goes through the list of cells one after the other. However, in parallel, it would imply that some processors are idle because upstream processors have not finished doing the work on cells upstream from the ones owned by the current processor. Once the upstream processors are done, the downstream ones can start, but by that time the upstream processors have no work left. In other words, most of the time during these smoother steps, most processors are in fact idle. This is not how one obtains good parallel scalability! 

One can use a hybrid method where a multiplicative smoother is applied on each subdomain, but as you increase the number of subdomains, the method approaches the behavior of an additive method. This is a major disadvantage to these methods.  [2.x.6208]  

 [2.x.6209]  Current research into block smoothers suggest that soon we will be able to compute the inverse of the cell matrices much cheaper than what is currently being done inside deal.II. This research is based on the fast diagonalization method (dating back to the 1960s) and has been used in the spectral community for around 20 years (see, e.g., [1.x.2387]). There are currently efforts to generalize these methods to DG and make them more robust. Also, it seems that one should be able to take advantage of matrix-free implementations and the fact that, in the interior of the domain, cell matrices tend to look very similar, allowing fewer matrix inverse computations.  [2.x.6210]   [2.x.6211]  

Combining 1. and 2. gives a good reason for expecting that a method like block Jacobi could become very powerful in the future, even though currently for these examples it is quite slow. 




[1.x.2388] 

[1.x.2389] 

Change the number of smoothing steps and the smoother relaxation parameter (set in  [2.x.6212]  inside  [2.x.6213] , only necessary for point smoothers) so that we maintain a constant number of iterations for a  [2.x.6214]  element. 

[1.x.2390] 

Increase/decrease the parameter "Epsilon" in the `.prm` files of the multiplicative methods and observe for which values renumbering no longer influences convergence speed. 

[1.x.2391] 

The code is set up to work correctly with an adaptively refined mesh (the interface matrices are created and set). Devise a suitable refinement criterium or try the KellyErrorEstimator class. 


examples/step-64/doc/intro.dox 

 [2.x.6215]  

[1.x.2392] 




[1.x.2393] 

This example shows how to implement a matrix-free method on the GPU using CUDA for the Helmholtz equation with variable coefficients on a hypercube. The linear system will be solved using the conjugate gradient method and is parallelized  with MPI. 

In the last few years, heterogeneous computing in general and GPUs in particular have gained a lot of popularity. This is because GPUs offer better computing capabilities and memory bandwidth than CPUs for a given power budget. Among the architectures available in early 2019, GPUs are about 2x-3x as power efficient than server CPUs with wide [1.x.2394] for PDE-related tasks. GPUs are also the most popular architecture for machine learning. On the other hand, GPUs are not easy to program. This program explores the deal.II capabilities to see how efficiently such a program can be implemented. 

While we have tried for the interface of the matrix-free classes for the CPU and the GPU to be as close as possible, there are a few differences. When using the matrix-free framework on a GPU, one must write some CUDA code. However, the amount is fairly small and the use of CUDA is limited to a few keywords. 




[1.x.2395] 

In this example, we consider the Helmholtz problem [1.x.2396] 

where  [2.x.6216]  is a variable coefficient. 

We choose as domain  [2.x.6217]  and  [2.x.6218] . Since the coefficient is symmetric around the origin but the domain is not, we will end up with a non-symmetric solution. 

If you've made it this far into the tutorial, you will know how the weak formulation of this problem looks like and how, in principle, one assembles linear systems for it. Of course, in this program we will in fact not actually form the matrix, but rather only represent its action when one multiplies with it. 




[1.x.2397] 

GPUs (we will use the term "device" from now on to refer to the GPU) have their own memory that is separate from the memory accessible to the CPU (we will use the term "host" from now on). A normal calculation on the device can be divided in three separate steps: 

 -# the data is moved from the host to the device, 

 -# the computation is done on the device, 

 -# the result is moved back from the device to the host 

The data movements can either be done explicitly by the user code or done automatically using UVM (Unified Virtual Memory). In deal.II, only the first method is supported. While it means an extra burden for the user, this allows for better control of data movement and more importantly it avoids to mistakenly run important kernels on the host instead of the device. 

The data movement in deal.II is done using  [2.x.6219]  These vectors can be seen as buffers on the host that are used to either store data received from the device or to send data to the device. There are two types of vectors that can be used on the device: 

-  [2.x.6220]  which is similar to the more common Vector<Number>, and 

-  [2.x.6221]   [2.x.6222]  which is a regular  [2.x.6223]  where we have specified which memory space to use. 

If no memory space is specified, the default is  [2.x.6224]  

Next, we show how to move data to/from the device using  [2.x.6225]  

[1.x.2398] 

Both of the vector classes used here only work on a single machine, i.e., one memory space on a host and one on a device. 

But there are cases where one wants to run a parallel computation between multiple MPI processes on a number of machines, each of which is equipped with GPUs. In that case, one wants to use  [2.x.6226]  which is similar but the `import()` stage may involve MPI communication: 

[1.x.2399] 

The `relevant_rw_vector` is an object that stores a subset of all elements of the vector. Typically, these are the  [2.x.6227]  "locally relevant DoFs", which implies that they overlap between different MPI processes. Consequently, the elements stored in that vector on one machine may not coincide with the ones stored by the GPU on that machine, requiring MPI communication to import them. 

In all of these cases, while importing a vector, values can either be inserted (using  [2.x.6228]  or added to prior content of the vector (using  [2.x.6229]  




[1.x.2400] 

The code necessary to evaluate the matrix-free operator on the device is very similar to the one on the host. However, there are a few differences, the main ones being that the `local_apply()` function in Step-37 and the loop over quadrature points both need to be encapsulated in their own functors. 


examples/step-64/doc/results.dox 



[1.x.2401] 

Since the main purpose of this tutorial is to demonstrate how to use the  [2.x.6230]  interface, not to compute anything useful in itself, we just show the expected output here: 

[1.x.2402] 



One can make two observations here: First, the norm of the numerical solution converges, presumably to the norm of the exact (but unknown) solution. And second, the number of iterations roughly doubles with each refinement of the mesh. (This is in keeping with the expectation that the number of CG iterations grows with the square root of the condition number of the matrix; and that we know that the condition number of the matrix of a second-order differential operation grows like  [2.x.6231] .) This is of course rather inefficient, as an optimal solver would have a number of iterations that is independent of the size of the problem. But having such a solver would require using a better preconditioner than the identity matrix we have used here. 


[1.x.2403] 

[1.x.2404] 

Currently, this program uses no preconditioner at all. This is mainly since constructing an efficient matrix-free preconditioner is non-trivial.  However, simple choices just requiring the diagonal of the corresponding matrix are good candidates and these can be computed in a matrix-free way as well. Alternatively, and maybe even better, one could extend the tutorial to use multigrid with Chebyshev smoothers similar to step-37. 


examples/step-65/doc/intro.dox 



 [2.x.6232]  

[1.x.2405] 

[1.x.2406] 

[1.x.2407] 

This tutorial program presents an advanced manifold class, TransfiniteInterpolationManifold, and how to work around its main disadvantage, the relatively high cost. 

[1.x.2408] 

[1.x.2409] 

In many applications, the finite element mesh must be able to represent a relatively complex geometry. In the step-1, step-49, and step-53 tutorial programs, some techniques to generate grids available within the deal.II library have been introduced. Given a base mesh, deal.II is then able to create a finer mesh by subdividing the cells into children, either uniformly or only in selected parts of the computational domain. Besides the basic meshing capabilities collected in the GridGenerator namespace, deal.II also comes with interfaces to read in meshes generated by (quad- and hex-only) mesh generators using the functions of namespace GridIn, as for example demonstrated in step-5. A fundamental limitation of externally generated meshes is that the information provided by the generated cells in the mesh only consists of the position of the vertices and their connectivity, without the context of the underlying geometry that used to be available in the mesh generator that originally created this mesh. This becomes problematic once the mesh is refined within deal.II and additional points need to be placed. The step-54 tutorial program shows how to overcome this limitation by using CAD surfaces in terms of the OpenCASCADE library, and step-53 by providing the same kind of information programmatically from within the source code. 

Within deal.II, the placement of new points during mesh refinement or for the definition of higher order mappings is controlled by manifold objects, see the  [2.x.6233]  "manifold module" for details. To give an example, consider the following situation of a two-dimensional annulus (with pictures taken from the manifold module). If we start with an initial mesh of 10 cells and refine the mesh three times globally without attaching any manifolds, we would obtain the following mesh: 

 [2.x.6234]  

The picture looks like this because, by default, deal.II only knows where to put the vertices of child cells by averaging the locations of the vertices of the parent cell. This yields a polygonal domain whose faces are the edges of the original (coarse mesh) cells. Obviously, we must attach a curved description to the boundary faces of the triangulation to reproduce the circular shape upon mesh refinement, like in the following picture: 

 [2.x.6235]  

This is better: At least the inner and outer boundaries are now approaching real circles if we continue to refine the mesh. However, the mesh in this picture is still not optimal for an annulus in the sense that the [1.x.2410] lines from one cell to the next have kinks at certain vertices, and one would rather like to use the following mesh: 

 [2.x.6236]  

In this last (optimal) case, which is also the default produced by  [2.x.6237]  the curved manifold description (in this case a polar manifold description) is applied not only to the boundary faces, but to the whole domain. Whenever the triangulation requests a new point, e.g., the mid point of the edges or the cells when it refines a cell into four children, it will place them along the respective mid points in the polar coordinate system. By contrast, the case above where only the boundary was subject to the polar manifold, only mid points along the boundary would be placed along the curved description, whereas mid points in the interior would be computed by suitable averages of the surrounding points in the Cartesian coordinate system (see the  [2.x.6238]  "manifold module" for more details). 

At this point, one might assume that curved volume descriptions are the way to go. This is generally not wrong, though it is sometimes not so easy to describe how exactly this should work. Here are a couple of examples: 

- Imagine that the mesh above had actually been a disk, not just a ring.   In that case the polar manifold degenerates at the origin and   would not produce reasonable new points. In fact, defining a   manifold description for things that are supposed "to look round"   but might have points at or close to the origin is surprisingly very   difficult. 

- A similar thing happens at the origin   of the three-dimensional ball when one tries to attach a spherical manifold to   the whole volume &ndash; in this case, the computation of new manifold points   would abort with an exception. 

- CAD geometries often only describe the boundary of the domain, in a   similar way to how we only attached a manifold to the boundary in   the second picture above. Similarly, step-54 only uses the CAD   geometry to generate a surface mesh (maybe because that is what is   needed to solve the problem in question), but if one wanted to solve   a problem in the water or the air around the ship described there,   we would need to have a volume mesh. The question is then how   exactly we should describe what is supposed to happen in the   interior of the domain. 

These simple examples make it clear that for many interesting cases we must step back from the desire to have an analytic curved description for the full volume: There will need to be [1.x.2411] kind of information that leads to curvature also in the interior, but it must be possible to do this without actually writing down an explicit formula that describes the kind of geometry. 

So what happens if we don't do anything at all in the interior and only describe the surface as a manifold? Sometimes, as in the ring shown above, the result is not terrible. But sometimes it is. Consider the case of a torus (e.g. generated with  [2.x.6239]  with a TorusManifold object attached to the surface only, no additional manifolds on the interior cells and faces, and with six cells in toroidal direction before refinement. If the mesh is refined once, we would obtain the following mesh, shown with the upper half of the mesh clipped away: 

 [2.x.6240]  

This is clearly sub-optimal. Indeed, if we had started with fewer than the six cells shown above in toroidal direction, the mapping actually inverts in some regions because the new points placed along interior cells intersect with the boundary as they are not following the circular shape along the toroidal direction. The simple case of a torus can still be fixed because we know that the toroidal direction follows a cylindrical coordinate system, so attaching a TorusManifold to the surface combined with CylindricalManifold with appropriate periodicity in toroidal direction applied to all interior entities would produce a high-quality mesh as follows, now shown with two top cells hidden: 

 [2.x.6241]  

This mesh is pretty good, but obviously it is linked to a good description of the volume, which we lack in other cases. Actually, there is an imperfection also in this case, as we can see some unnatural kinks of two adjacent cells in the interior of the domain which are hidden by the top two boundary cells, as opposed to the following setup (the default manifolds applied by  [2.x.6242]  and using the TransfiniteInterpolationManifold): 

 [2.x.6243]  

[1.x.2412] 

In order to find a better strategy, let us look at the two-dimensional disk again (that is also the base entity rotated along the toroidal direction in the torus). As we learned above, we can only apply the curved polar description to the boundary (or a rim of cells sufficiently far away from the origin) but must eventually transition to a straight description towards the disk's center. If we use a flat manifold in the interior of the cells (i.e., one in which new vertices are created by averaging of the adjacent existing ones) and a polar manifold only for the boundary of the disk, we get the following mesh upon four global refinements: 

 [2.x.6244]  

That's not a terrible mesh. At the same time, if you know that the original coarse mesh consisted of a single square in the middle, with four caps around it, then it's not hard to see every refinement step that happened to this mesh to get the picture above. 

While the triangulation class of deal.II tries to propagate information from the boundary into the interior when creating new points, the reach of this algorithm is limited: 

 [2.x.6245]  

The picture above highlights those cells on the disk that are touching the boundary and where boundary information could in principle be taken into account when only looking at a single cell at the time. Clearly, the area where some curvature can be taken into account gets more limited as the mesh is refined, thus creating the seemingly irregular spots in the mesh: When computing the center of any one of the boundary cells in the leftmost picture, the ideal position is the mid point between the outer circle and the cell in the middle. This is exactly what is used for the first refinement step in the Triangulation class. However, for the second refinement all interior edges as well as the interior cell layers can only add points according to a flat manifold description. 

At this point, we realize what would be needed to create a better mesh: For [1.x.2413] new points in [1.x.2414] child cell that is created within the red shaded layer on the leftmost picture, we want to compute the interpolation with respect to the curvature in the area covered by the respective coarse cell. This is achieved by adding the class TransfiniteInterpolationManifold to the highlighted cells of the coarse grid in the leftmost panel of the figure above. This class adheres to the general manifold interfaces, i.e., given any set of points within its domain of definition, it can compute weighted averages conforming to the manifold (using a formula that will be given in a minute). These weighted averages are used whenever the mesh is refined, or when a higher order mapping (such as MappingQGeneric or MappingC1) is evaluated on a given cell subject to this manifold. Using this manifold on the shaded cells of the coarse grid of the disk (i.e., not only in the outer-most layer of cells) produces the following mesh upon four global steps of refinement: 

 [2.x.6246]  

There are still some kinks in the lines of this mesh, but they are restricted to the faces between coarse mesh cells, whereas the rest of the mesh is about as smooth as one would like. Indeed, given a straight-sided central cell, this representation is the best possible one as all mesh cells follow a smooth transition from the straight sides in the square block in the interior to the circular shape on the boundary. (One could possibly do a bit better by allowing some curvature also in the central square block, that eventually vanishes as the center is approached.) 




[1.x.2415] 

In the simple case of a disk with one curved and three straight edges, we can explicitly write down how to achieve the blending of the shapes. For this, it is useful to map the physical cell, like the top one, back to the reference coordinate system  [2.x.6247]  where we compute averages between certain points. If we were to use a simple bilinear map spanned by four vertices  [2.x.6248] , the image of a point  [2.x.6249]  would be 

[1.x.2416] 



For the case of the curved surface, we want to modify this formula. For the top cell of the coarse mesh of the disk, we can assume that the points  [2.x.6250]  and  [2.x.6251]  sit along the straight line at the lower end and the points  [2.x.6252]  and  [2.x.6253]  are connected by a quarter circle along the top. We would then map a point  [2.x.6254]  as 

[1.x.2417] 

where  [2.x.6255]  is a curve that describes the  [2.x.6256]  coordinates of the quarter circle in terms of an arclength parameter  [2.x.6257] . This represents a linear interpolation between the straight lower edge and the curved upper edge of the cell, and is the basis for the picture shown above. 

This formula is easily generalized to the case where all four edges are described by a curve rather than a straight line. We call the four functions, parameterized by a single coordinate  [2.x.6258]  or  [2.x.6259]  in the horizontal and vertical directions,  [2.x.6260]  for the left, right, lower, and upper edge of a quadrilateral, respectively. The interpolation then reads 

[1.x.2418] 



This formula assumes that the boundary curves match and coincide with the vertices  [2.x.6261] , e.g.  [2.x.6262]  or  [2.x.6263] . The subtraction of the bilinear interpolation in the second line of the formula makes sure that the prescribed curves are followed exactly on the boundary: Along each of the four edges, we need to subtract the contribution of the two adjacent edges evaluated in the corners, which is then simply a vertex position. It is easy to check that the formula for the circle above is reproduced if three of the four curves  [2.x.6264]  are straight and thus coincide with the bilinear interpolation. 

This formula, called transfinite interpolation, was introduced in 1973 by [1.x.2419]. Even though transfinite interpolation essentially only represents a linear blending of the bounding curves, the interpolation exactly follows the boundary curves for each real number  [2.x.6265]  or  [2.x.6266] , i.e., it interpolates in an infinite number of points, which was the original motivation to label this variant of interpolation a transfinite one by Gordon and Hall. Another interpretation is that the transfinite interpolation interpolates from the left and right and the top and bottom linearly, from which we need to subtract the bilinear interpolation to ensure a unit weight in the interior of the domain. 

The transfinite interpolation is easily generalized to three spatial dimensions. In that case, the interpolation allows to blend 6 different surface descriptions for any of the quads of a three-dimensional cell and 12 edge descriptions for the lines of a cell. Again, to ensure a consistent map, it is necessary to subtract the contribution of edges and add the contribution of vertices again to make the curves follow the prescribed surface or edge description. In the three-dimensional case, it is also possible to use a transfinite interpolation from a curved edge both into the adjacent faces and the adjacent cells. 

The interpolation of the transfinite interpolation in deal.II is general in the sense that it can deal with arbitrary curves. It will evaluate the curves in terms of their original coordinates of the  [2.x.6267] -dimensional space but with one (or two, in the case of edges in 3D) coordinate held fixed at  [2.x.6268]  or  [2.x.6269]  to ensure that any other manifold class, including CAD files if desired, can be applied out of the box. Transfinite interpolation is a standard ingredient in mesh generators, so the main strength of the integration of this feature within the deal.II library is to enable it during adaptive refinement and coarsening of the mesh, and for creating higher-degree mappings that use manifolds to insert additional points beyond the mesh vertices. 

As a final remark on transfinite interpolation, we mention that the mesh refinement strategies in deal.II in absence of a volume manifold description are also based on the weights of the transfinite interpolation and optimal in that sense. The difference is that the default algorithm sees only one cell at a time, and so will apply the optimal algorithm only on those cells touching the curved manifolds. In contrast, using the transfinite mapping on entire [1.x.2420] of cells (originating from one coarser cell) allows to use the transfinite interpolation method in a way that propagates information from the boundary to cells far away. 




[1.x.2421] 

A mesh with a transfinite manifold description is typically set up in two steps. The first step is to create a coarse mesh (or read it in from a file) and to attach a curved manifold to some of the mesh entities. For the above example of the disk, we attach a polar manifold to the faces along the outer circle (this is done automatically by  [2.x.6270]  Before we start refining the mesh, we then assign a TransfiniteInterpolationManifold to all interior cells and edges of the mesh, which of course needs to be based on some manifold id that we have assigned to those entities (everything except the circle on the boundary). It does not matter whether we also assign a TransfiniteInterpolationManifold to the inner square of the disk or not because the transfinite interpolation on a coarse cell with straight edges (or flat faces in 3d) simply yields subdivided children with straight edges (flat faces). 

Later, when the mesh is refined or when a higher-order mapping is set up based on this mesh, the cells will query the underlying manifold object for new points. This process takes a set of surrounding points, for example the four vertices of a two-dimensional cell, and a set of weights to each of these points, for definition a new point. For the mid point of a cell, each of the four vertices would get weight 0.25. For the transfinite interpolation manifold, the process of building weighted sums requires some serious work. By construction, we want to combine the points in terms of the reference coordinates  [2.x.6271]  and  [2.x.6272]  (or  [2.x.6273]  in 3D) of the surrounding points. However, the interface of the manifold classes in deal.II does not get the reference coordinates of the surrounding points (as they are not stored globally) but rather the physical coordinates only. Thus, the first step the transfinite interpolation manifold has to do is to invert the mapping and find the reference coordinates within one of the coarse cells of the transfinite interpolation (e.g. one of the four shaded coarse-grid cells of the disk mesh above). This inversion is done by a Newton iteration (or rather, finite-difference based Newton scheme combined with Broyden's method) and queries the transfinite interpolation according to the formula above several times. Each of these queries in turn might call an expensive manifold, e.g. a spherical description of a ball, and be expensive on its own. Since the Manifold interface class of deal.II only provides a set of points, the transfinite interpolation initially does not even know to which coarse grid cell the set of surrounding points belong to and needs to search among several cells based on some heuristics. In terms of [1.x.2422], one could describe the implementation of the transfinite interpolation as an [1.x.2423]-based implementation: Each cell of the initial coarse grid of the triangulation represents a chart with its own reference space, and the surrounding manifolds provide a way to transform from the chart space (i.e., the reference cell) to the physical space. The collection of the charts of the coarse grid cells is an atlas, and as usual, the first thing one does when looking up something in an atlas is to find the right chart. 

Once the reference coordinates of the surrounding points have been found, a new point in the reference coordinate system is computed by a simple weighted sum. Finally, the reference point is inserted into the formula for the transfinite interpolation, which gives the desired new point. 

In a number of cases, the curved manifold is not only used during mesh refinement, but also to ensure a curved representation of boundaries within the cells of the computational domain. This is a necessity to guarantee high-order convergence for high-order polynomials on complex geometries anyway, but sometimes an accurate geometry is also desired with linear shape functions. This is often done by polynomial descriptions of the cells and called the isoparametric concept if the polynomial degree to represent the curved mesh elements is the same as the degree of the polynomials for the numerical solution. If the degree of the geometry is higher or lower than the solution, one calls that a super- or sub-parametric geometry representation, respectively. In deal.II, the standard class for polynomial representation is MappingQGeneric. If, for example, this class is used with polynomial degree  [2.x.6274]  in 3D, a total of 125 (i.e.,  [2.x.6275] ) points are needed for the interpolation. Among these points, 8 are the cell's vertices and already available from the mesh, but the other 117 need to be provided by the manifold. In case the transfinite interpolation manifold is used, we can imagine that going through the pull-back into reference coordinates of some yet to be determined coarse cell, followed by subsequent push-forward on each of the 117 points, is a lot of work and can be very time consuming. 

What makes things worse is that the structure of many programs is such that the mapping is queried several times independently for the same cell. Its primary use is in the assembly of the linear system, i.e., the computation of the system matrix and the right hand side, via the `mapping` argument of the FEValues object. However, also the interpolation of boundary values, the computation of numerical errors, writing the output, and evaluation of error estimators must involve the same mapping to ensure a consistent interpretation of the solution vectors. Thus, even a linear stationary problem that is solved once will evaluate the points of the mapping several times. For the cubic case in 3D mentioned above, this means computing 117 points per cell by an expensive algorithm many times. The situation is more pressing for nonlinear or time-dependent problems where those operations are done over and over again. 

As the manifold description via a transfinite interpolation can easily be hundreds of times more expensive than a similar query on a flat manifold, it makes sense to compute the additional points only once and use them in all subsequent calls. The deal.II library provides the class MappingQCache for exactly this purpose. The cache is typically not overly big compared to the memory consumed by a system matrix, as will become clear when looking at the results of this tutorial program. The usage of MappingQCache is simple: Once the mesh has been set up (or changed during refinement), we call  [2.x.6276]  with the desired triangulation as well as a desired mapping as arguments. The initialization then goes through all cells of the mesh and queries the given mapping for its additional points. Those get stored for an identifier of the cell so that they can later be returned whenever the mapping computes some quantities related to the cell (like the Jacobians of the map between the reference and physical coordinates). 

As a final note, we mention that the TransfiniteInterpolationManifold also makes the refinement of the mesh more expensive. In this case, the MappingQCache does not help because it would compute points that can subsequently not be re-used; there currently does not exist a more efficient mechanism in deal.II. However, the mesh refinement contains many other expensive steps as well, so it is not as big as an issue compared to the rest of the computation. It also only happens at most once per time step or nonlinear iteration. 

[1.x.2424] 

In this tutorial program, the usage of TransfiniteInterpolationManifold is exemplified in combination with MappingQCache. The test case is relatively simple and takes up the solution stages involved in many typical programs, e.g., the step-6 tutorial program. As a geometry, we select one prototype use of TransfiniteInterpolationManifold, namely a setup involving a spherical ball that is in turn surrounded by a cube. Such a setup would be used, for example, for a spherical inclusion embedded in a background medium, and if that inclusion has different material properties that require that the interface between the two materials needs to be tracked by element interfaces. A visualization of the grid is given here: 

 [2.x.6277]  

For this case, we want to attach a spherical description to the surface inside the domain and use the transfinite interpolation to smoothly switch to the straight lines of the outer cube and the cube at the center of the ball. 

Within the program, we will follow a typical flow in finite element programs, starting from the setup of DoFHandler and sparsity patterns, the assembly of a linear system for solving the Poisson equation with a jumping coefficient, its solution with a simple iterative method, computation of some numerical error with  [2.x.6278]  as well as an error estimator. We record timings for each section and run the code twice. In the first run, we hand a MappingQGeneric object to each stage of the program separately, where points get re-computed over and over again. In the second run, we use MappingQCache instead. 


examples/step-65/doc/results.dox 



[1.x.2425] 

[1.x.2426] 

If we run the three-dimensional version of this program with polynomials of degree three, we get the following program output: 

[1.x.2427] 



Before discussing the timings, we look at the memory consumption for the MappingQCache object: Our program prints that it utilizes 23 MB of memory. If we relate this number to the memory consumption of a single (solution or right hand side) vector, which is 1.5 MB (namely, 181,609 elements times 8 bytes per entry in double precision), or to the memory consumed by the system matrix and the sparsity pattern (which is 274 MB), we realize that it is not an overly heavy data structure, given its benefits. 

With respect to the timers, we see a clear improvement in the overall run time of the program by a factor of 2.7. If we disregard the iterative solver, which is the same in both cases (and not optimal, given the simple preconditioner we use, and the fact that sparse matrix-vector products waste operations for cubic polynomials), the advantage is a factor of almost 5. This is pretty impressive for a linear stationary problem, and cost savings would indeed be much more prominent for time-dependent and nonlinear problems where assembly is called several times. If we look into the individual components, we get a clearer picture of what is going on and why the cache is so efficient: In the MappingQGeneric case, essentially every operation that involves a mapping take at least 5 seconds to run. The norm computation runs two  [2.x.6279]  functions, which each take almost 5 seconds. (The computation of constraints is cheaper because it only evaluates the mapping in cells at the boundary for the interpolation of boundary conditions.) If we compare these 5 seconds to the time it takes to fill the MappingQCache, which is 5.2 seconds (for all cells, not just the active ones), it becomes obvious that the computation of the mapping support points dominates over everything else in the MappingQGeneric case. Perhaps the most striking result is the time for the error estimator, labeled "Compute error estimator", where the MappingQGeneric implementation takes 17.3 seconds and the MappingQCache variant less than 0.5 seconds. The reason why the former is so expensive (three times more expensive than the assembly, for instance) is that the error estimation involves evaluation of quantities over faces, where each face in the mesh requests additional points of the mapping that in turn go through the very expensive TransfiniteInterpolationManifold class. As there are six faces per cell, this happens much more often than in assembly. Again, MappingQCache nicely eliminates the repeated evaluation, aggregating all the expensive steps involving the manifold in a single initialization call that gets repeatedly used. 


examples/step-66/doc/intro.dox 

 [2.x.6280]  

[1.x.2428] 


[1.x.2429] 

[1.x.2430] 

The aim of this tutorial program is to demonstrate how to solve a nonlinear problem using Newton's method within the matrix-free framework. This tutorial combines several techniques already introduced in step-15, step-16, step-37, step-48 and others. 




[1.x.2431] On the unit circle  [2.x.6281]  we consider the following nonlinear elliptic boundary value problem subject to a homogeneous Dirichlet boundary condition: Find a function  [2.x.6282]  such that it holds: 

[1.x.2432] 

This problem is also called the [1.x.2433] and is a typical example for problems from combustion theory, see for example  [2.x.6283] . 




[1.x.2434] As usual, we first derive the weak formulation for this problem by multiplying with a smooth test function  [2.x.6284]  respecting the boundary condition and integrating over the domain  [2.x.6285] . Integration by parts and putting the term from the right hand side to the left yields the weak formulation: Find a function  [2.x.6286]  such that for all test functions  [2.x.6287]  it holds: 

[1.x.2435] 



Choosing the Lagrangian finite element space  [2.x.6288] , which directly incorporates the homogeneous Dirichlet boundary condition, we can define a basis  [2.x.6289]  and thus it suffices to test only with those basis functions. So the discrete problem reads as follows: Find  [2.x.6290]  such that for all  [2.x.6291]  it holds: 

[1.x.2436] 

As each finite element function is a linear combination of the basis functions  [2.x.6292] , we can identify the finite element solution by a vector from  [2.x.6293]  consisting of the unknown values in each degree of freedom (DOF). Thus, we define the nonlinear function  [2.x.6294]  representing the discrete nonlinear problem. 

To solve this nonlinear problem we use Newton's method. So given an initial guess  [2.x.6295] , which already fulfills the Dirichlet boundary condition, we determine a sequence of Newton steps  [2.x.6296]  by successively applying the following scheme: 

[1.x.2437] 

So in each Newton step we have to solve a linear problem  [2.x.6297] , where the system matrix  [2.x.6298]  is represented by the Jacobian  [2.x.6299]  and the right hand side  [2.x.6300]  by the negative residual  [2.x.6301] . The solution vector  [2.x.6302]  is in that case the Newton update of the  [2.x.6303] -th Newton step. Note, that we assume an initial guess  [2.x.6304] , which already fulfills the Dirichlet boundary conditions of the problem formulation (in fact this could also be an inhomogeneous Dirichlet boundary condition) and thus the Newton updates  [2.x.6305]  satisfy a homogeneous Dirichlet condition. 

Until now we only tested with the basis functions, however, we can also represent any function of  [2.x.6306]  as linear combination of basis functions. More mathematically this means, that every element of  [2.x.6307]  can be identified with a vector  [2.x.6308]  via the representation formula:  [2.x.6309] . So using this we can give an expression for the discrete Jacobian and the residual: 

[1.x.2438] 

Compared to step-15 we could also have formed the Frech{\'e}t derivative of the nonlinear function corresponding to the strong formulation of the problem and discretized it afterwards. However, in the end we would get the same set of discrete equations. 




[1.x.2439] Note, how the system matrix, actually the Jacobian, depends on the previous Newton step  [2.x.6310] . Hence we need to tell the function that computes the system matrix about the solution at the last Newton step. In an implementation with a classical  [2.x.6311]  function we would gather this information from the last Newton step during assembly by the use of the member functions  [2.x.6312]  and  [2.x.6313]  The  [2.x.6314]  function would then looks like: 

[1.x.2440] 



Since we want to solve this problem without storing a matrix, we need to tell the matrix-free operator this information before we use it. Therefore in the derived class  [2.x.6315]  we will implement a function called  [2.x.6316] , which will process the information of the last Newton step prior to the usage of the matrix-vector implementation. Furthermore we want to use a geometric multigrid (GMG) preconditioner for the linear solver, so in order to apply the multilevel operators we need to pass the last Newton step also to these operators. This is kind of a tricky task, since the vector containing the last Newton step has to be interpolated to all levels of the triangulation. In the code this task will be done by the function  [2.x.6317]  Note, a fundamental difference to the previous cases, where we set up and used a geometric multigrid preconditioner, is the fact, that we can reuse the MGTransferMatrixFree object for the computation of all Newton steps. So we can save some work here by defining a class variable and using an already set up MGTransferMatrixFree object  [2.x.6318]  that was initialized in the  [2.x.6319]  function. 

[1.x.2441] 



The function evaluating the nonlinearity works basically in the same way as the function  [2.x.6320]  from step-37 evaluating a coefficient function. The idea is to use an FEEvaluation object to evaluate the Newton step and store the expression in a table for all cells and all quadrature points: 

[1.x.2442] 






[1.x.2443] As said in step-37 the matrix-free method gets more efficient if we choose a higher order finite element space. Since we want to solve the problem on the  [2.x.6321] -dimensional unit ball, it would be good to have an appropriate boundary approximation to overcome convergence issues. For this reason we use an isoparametric approach with the MappingQGeneric class to recover the smooth boundary as well as the mapping for inner cells. In addition, to get a good triangulation in total we make use of the TransfiniteInterpolationManifold. 


examples/step-66/doc/results.dox 



[1.x.2444] 

The aim of this tutorial step was to demonstrate the solution of a nonlinear PDE with the matrix-free framework. 




[1.x.2445] Running the program on two processes in release mode via 

[1.x.2446] 

gives the following output on the console 

[1.x.2447] 



We show the solution for the two- and three-dimensional problem in the following figure. 

<div class="twocolumn" style="width: 80%; text-align: center;">   <div>     <img src = "https://www.dealii.org/images/steps/developer/step-66.solution-2d.png"      alt     = "Solution of the two-dimensional Gelfand problem."      width   = "100%">   </div>   <div>     <img src = "https://www.dealii.org/images/steps/developer/step-66.solution-3d.png"      alt     = "Solution of the three-dimensional Gelfand problem."      width   = "100%">   </div> </div> 




[1.x.2448] In the program output above we find some interesting information about the Newton iterations. The terminal output in each refinement cycle presents detailed diagnostics of the Newton method, which show first of all the number of Newton steps and for each step the norm of the residual  [2.x.6322] , the norm of the Newton update  [2.x.6323] , and the number of CG iterations  [2.x.6324] . 

We observe that for all cases the Newton method converges in approximately three to four steps, which shows the quadratic convergence of the Newton method with a full step length  [2.x.6325] . However, be aware that for a badly chosen initial guess  [2.x.6326] , the Newton method will also diverge quadratically. Usually if you do not have an appropriate initial guess, you try a few damped Newton steps with a reduced step length  [2.x.6327]  until the Newton step is again in the quadratic convergence domain. This damping and relaxation of the Newton step length truly requires a more sophisticated implementation of the Newton method, which we designate to you as a possible extension of the tutorial. 

Furthermore, we see that the number of CG iterations is approximately constant with successive mesh refinements and an increasing number of DoFs. This is of course due to the geometric multigrid preconditioner and similar to the observations made in other tutorials that use this method, e.g., step-16 and step-37. Just to give an example, in the three-dimensional case after five refinements, we have approximately 14.7 million distributed DoFs with fourth-order Lagrangian finite elements, but the number of CG iterations is still less than ten. 

In addition, there is one more very useful optimization that we applied and that should be mentioned here. In the  [2.x.6328]  function we explicitly reset the vector holding the Newton update before passing it as the output vector to the solver. In that case we use a starting value of zero for the CG method, which is more suitable than the previous Newton update, the actual content of the  [2.x.6329]  before resetting, and thus reduces the number of CG iterations by a few steps. 




[1.x.2449] A couple of possible extensions are available concerning minor updates fo the present code as well as a deeper numerical investigation of the Gelfand problem. 

[1.x.2450] Beside a step size controlled version of the Newton iteration as mentioned already in step-15, one could also implement a more flexible stopping criterion for the Newton iteration. For example one could replace the fixed tolerances for the residual  [2.x.6330]  and implement a mixed error control with a given absolute and relative tolerance, such that the Newton iteration exists with success as, e.g., 

[1.x.2451] 

For more advanced applications with many nonlinear systems to solve, for example at each time step for a time-dependent problem, it turns out that it is not necessary to set up and assemble the Jacobian anew at every single Newton step or even for each time step. Instead, the existing Jacobian from a previous step can be used for the Newton iteration. The Jacobian is then only rebuilt if, for example, the Newton iteration converges too slowly. Such an idea yields a [1.x.2452]. Admittedly, when using the matrix-free framework, the assembly of the Jacobian is omitted anyway, but with in this way one can try to optimize the reassembly of the geometric multigrid preconditioner. Remember that each time the solution from the old Newton step must be distributed to all levels and the mutligrid preconditioner must be reinitialized. 

[1.x.2453] In the results section of step-37 and others, the parallel scalability of the matrix-free framework on a large number of processors has already been demonstrated very impressively. In the nonlinear case we consider here, we note that one of the bottlenecks could become the transfer and evaluation of the matrix-free Jacobi operator and its multistage operators in the previous Newton step, since we need to transfer the old solution at all stages in each step. A first parallel scalability analysis in  [2.x.6331]  shows quite good strong scalability when the problem size is large enough. However, a more detailed analysis needs to be performed for reliable results. Moreover, the problem has been solved only with MPI so far, without using the possibilities of shared memory parallelization with threads. Therefore, for this example, you could try hybrid parallelization with MPI and threads, such as described in step-48. 

[1.x.2454] Analogously to step-50 and the mentioned possible extension of step-75, you can convince yourself which method is faster. 

[1.x.2455] One can consider the corresponding eigenvalue problem, which is called Bratu problem. For example, if we define a fixed eigenvalue  [2.x.6332] , we can compute the corresponding discrete eigenfunction. You will notice that the number of Newton steps will increase with increasing  [2.x.6333] . To reduce the number of Newton steps you can use the following trick: start from a certain  [2.x.6334] , compute the eigenfunction, increase  [2.x.6335] , and then use the previous solution as an initial guess for the Newton iteration. In the end you can plot the  [2.x.6336] -norm over the eigenvalue  [2.x.6337] . What do you observe for further increasing  [2.x.6338] ? 


examples/step-67/doc/intro.dox 



 [2.x.6339]  

[1.x.2456] 

[1.x.2457] 

[1.x.2458] 

This tutorial program solves the Euler equations of fluid dynamics using an explicit time integrator with the matrix-free framework applied to a high-order discontinuous Galerkin discretization in space. For details about the Euler system and an alternative implicit approach, we also refer to the step-33 tutorial program. You might also want to look at step-69 for an alternative approach to solving these equations. 




[1.x.2459] 

The Euler equations are a conservation law, describing the motion of a compressible inviscid gas, 

[1.x.2460] 

where the  [2.x.6340]  components of the solution vector are  [2.x.6341] . Here,  [2.x.6342]  denotes the fluid density,  [2.x.6343]  the fluid velocity, and  [2.x.6344]  the energy density of the gas. The velocity is not directly solved for, but rather the variable  [2.x.6345] , the linear momentum (since this is the conserved quantity). 

The Euler flux function, a  [2.x.6346]  matrix, is defined as 

[1.x.2461] 

with  [2.x.6347]  the  [2.x.6348]  identity matrix and  [2.x.6349]  the outer product; its components denote the mass, momentum, and energy fluxes, respectively. The right hand side forcing is given by 

[1.x.2462] 

where the vector  [2.x.6350]  denotes the direction and magnitude of gravity. It could, however, also denote any other external force per unit mass that is acting on the fluid. (Think, for example, of the electrostatic forces exerted by an external electric field on charged particles.) 

The three blocks of equations, the second involving  [2.x.6351]  components, describe the conservation of mass, momentum, and energy. The pressure is not a solution variable but needs to be expressed through a "closure relationship" by the other variables; we here choose the relationship appropriate for a gas with molecules composed of two atoms, which at moderate temperatures is given by  [2.x.6352]  with the constant  [2.x.6353] . 




[1.x.2463] 

For spatial discretization, we use a high-order discontinuous Galerkin (DG) discretization, using a solution expansion of the form 

[1.x.2464] 

Here,  [2.x.6354]  denotes the  [2.x.6355] th basis function, written in vector form with separate shape functions for the different components and letting  [2.x.6356]  go through the density, momentum, and energy variables, respectively. In this form, the space dependence is contained in the shape functions and the time dependence in the unknown coefficients  [2.x.6357] . As opposed to the continuous finite element method where some shape functions span across element boundaries, the shape functions are local to a single element in DG methods, with a discontinuity from one element to the next. The connection of the solution from one cell to its neighbors is instead imposed by the numerical fluxes specified below. This allows for some additional flexibility, for example to introduce directionality in the numerical method by, e.g., upwinding. 

DG methods are popular methods for solving problems of transport character because they combine low dispersion errors with controllable dissipation on barely resolved scales. This makes them particularly attractive for simulation in the field of fluid dynamics where a wide range of active scales needs to be represented and inadequately resolved features are prone to disturb the important well-resolved features. Furthermore, high-order DG methods are well-suited for modern hardware with the right implementation. At the same time, DG methods are no silver bullet. In particular when the solution develops discontinuities (shocks), as is typical for the Euler equations in some flow regimes, high-order DG methods tend to oscillatory solutions, like all high-order methods when not using flux- or slope-limiters. This is a consequence of [1.x.2465] that states that any total variation limited (TVD) scheme that is linear (like a basic DG discretization) can at most be first-order accurate. Put differently, since DG methods aim for higher order accuracy, they cannot be TVD on solutions that develop shocks. Even though some communities claim that the numerical flux in DG methods can control dissipation, this is of limited value unless [1.x.2466] shocks in a problem align with cell boundaries. Any shock that passes through the interior of cells will again produce oscillatory components due to the high-order polynomials. In the finite element and DG communities, there exist a number of different approaches to deal with shocks, for example the introduction of artificial diffusion on troubled cells (using a troubled-cell indicator based e.g. on a modal decomposition of the solution), a switch to dissipative low-order finite volume methods on a subgrid, or the addition of some limiting procedures. Given the ample possibilities in this context, combined with the considerable implementation effort, we here refrain from the regime of the Euler equations with pronounced shocks, and rather concentrate on the regime of subsonic flows with wave-like phenomena. For a method that works well with shocks (but is more expensive per unknown), we refer to the step-69 tutorial program. 

For the derivation of the DG formulation, we multiply the Euler equations with test functions  [2.x.6358]  and integrate over an individual cell  [2.x.6359] , which gives 

[1.x.2467] 



We then integrate the second term by parts, moving the divergence from the solution slot to the test function slot, and producing an integral over the element boundary: 

[1.x.2468] 

In the surface integral, we have replaced the term  [2.x.6360]  by the term  [2.x.6361] , the numerical flux. The role of the numerical flux is to connect the solution on neighboring elements and weakly impose continuity of the solution. This ensures that the global coupling of the PDE is reflected in the discretization, despite independent basis functions on the cells. The connectivity to the neighbor is included by defining the numerical flux as a function  [2.x.6362]  of the solution from both sides of an interior face,  [2.x.6363]  and  [2.x.6364] . A basic property we require is that the numerical flux needs to be [1.x.2469]. That is, we want all information (i.e., mass, momentum, and energy) that leaves a cell over a face to enter the neighboring cell in its entirety and vice versa. This can be expressed as  [2.x.6365] , meaning that the numerical flux evaluates to the same result from either side. Combined with the fact that the numerical flux is multiplied by the unit outer normal vector on the face under consideration, which points in opposite direction from the two sides, we see that the conservation is fulfilled. An alternative point of view of the numerical flux is as a single-valued intermediate state that links the solution weakly from both sides. 

There is a large number of numerical flux functions available, also called Riemann solvers. For the Euler equations, there exist so-called exact Riemann solvers -- meaning that the states from both sides are combined in a way that is consistent with the Euler equations along a discontinuity -- and approximate Riemann solvers, which violate some physical properties and rely on other mechanisms to render the scheme accurate overall. Approximate Riemann solvers have the advantage of beging cheaper to compute. Most flux functions have their origin in the finite volume community, which are similar to DG methods with polynomial degree 0 within the cells (called volumes). As the volume integral of the Euler operator  [2.x.6366]  would disappear for constant solution and test functions, the numerical flux must fully represent the physical operator, explaining why there has been a large body of research in that community. For DG methods, consistency is guaranteed by higher order polynomials within the cells, making the numerical flux less of an issue and usually affecting only the convergence rate, e.g., whether the solution converges as  [2.x.6367] ,  [2.x.6368]  or  [2.x.6369]  in the  [2.x.6370]  norm for polynomials of degree  [2.x.6371] . The numerical flux can thus be seen as a mechanism to select more advantageous dissipation/dispersion properties or regarding the extremal eigenvalue of the discretized and linearized operator, which affect the maximal admissible time step size in explicit time integrators. 

In this tutorial program, we implement two variants of fluxes that can be controlled via a switch in the program (of course, it would be easy to make them a run time parameter controlled via an input file). The first flux is the local Lax--Friedrichs flux 

[1.x.2470] 



In the original definition of the Lax--Friedrichs flux, a factor  [2.x.6372]  is used (corresponding to the maximal speed at which information is moving on the two sides of the interface), stating that the difference between the two states,  [2.x.6373]  is penalized by the largest eigenvalue in the Euler flux, which is  [2.x.6374] , where  [2.x.6375]  is the speed of sound. In the implementation below, we modify the penalty term somewhat, given that the penalty is of approximate nature anyway. We use 

[1.x.2471] 

The additional factor  [2.x.6376]  reduces the penalty strength (which results in a reduced negative real part of the eigenvalues, and thus increases the admissible time step size). Using the squares within the sums allows us to reduce the number of expensive square root operations, which is 4 for the original Lax--Friedrichs definition, to a single one. This simplification leads to at most a factor of 2 in the reduction of the parameter  [2.x.6377] , since  [2.x.6378] , with the last inequality following from Young's inequality. 

The second numerical flux is one proposed by Harten, Lax and van Leer, called the HLL flux. It takes the different directions of propagation of the Euler equations into account, depending on the speed of sound. It utilizes some intermediate states  [2.x.6379]  and  [2.x.6380]  to define the two branches  [2.x.6381]  and  [2.x.6382] . From these branches, one then defines the flux 

[1.x.2472] 

Regarding the definition of the intermediate state  [2.x.6383]  and  [2.x.6384] , several variants have been proposed. The variant originally proposed uses a density-averaged definition of the velocity,  [2.x.6385] . Since we consider the Euler equations without shocks, we simply use arithmetic means,  [2.x.6386]  and  [2.x.6387] , with  [2.x.6388] , in this tutorial program, and leave other variants to a possible extension. We also note that the HLL flux has been extended in the literature to the so-called HLLC flux, where C stands for the ability to represent contact discontinuities. 

At the boundaries with no neighboring state  [2.x.6389]  available, it is common practice to deduce suitable exterior values from the boundary conditions (see the general literature on DG methods for details). In this tutorial program, we consider three types of boundary conditions, namely [1.x.2473] where all components are prescribed, 

[1.x.2474] 

[1.x.2475], where we do not prescribe exterior solutions as the flow field is leaving the domain and use the interior values instead; we still need to prescribe the energy as there is one incoming characteristic left in the Euler flux, 

[1.x.2476] 

and [1.x.2477] which describe a no-penetration configuration: 

[1.x.2478] 



The polynomial expansion of the solution is finally inserted to the weak form and test functions are replaced by the basis functions. This gives a discrete in space, continuous in time nonlinear system with a finite number of unknown coefficient values  [2.x.6390] ,  [2.x.6391] . Regarding the choice of the polynomial degree in the DG method, there is no consensus in literature as of 2019 as to what polynomial degrees are most efficient and the decision is problem-dependent. Higher order polynomials ensure better convergence rates and are thus superior for moderate to high accuracy requirements for [1.x.2479] solutions. At the same time, the volume-to-surface ratio of where degrees of freedom are located, increases with higher degrees, and this makes the effect of the numerical flux weaker, typically reducing dissipation. However, in most of the cases the solution is not smooth, at least not compared to the resolution that can be afforded. This is true for example in incompressible fluid dynamics, compressible fluid dynamics, and the related topic of wave propagation. In this pre-asymptotic regime, the error is approximately proportional to the numerical resolution, and other factors such as dispersion errors or the dissipative behavior become more important. Very high order methods are often ruled out because they come with more restrictive CFL conditions measured against the number of unknowns, and they are also not as flexible when it comes to representing complex geometries. Therefore, polynomial degrees between two and six are most popular in practice, see e.g. the efficiency evaluation in  [2.x.6392]  and references cited therein. 

[1.x.2480] 

To discretize in time, we slightly rearrange the weak form and sum over all cells: 

[1.x.2481] 

where  [2.x.6393]  runs through all basis functions with from 1 to  [2.x.6394] . 

We now denote by  [2.x.6395]  the mass matrix with entries  [2.x.6396] , and by 

[1.x.2482] 

the operator evaluating the right-hand side of the Euler operator, given a function  [2.x.6397]  associated with a global vector of unknowns and the finite element in use. This function  [2.x.6398]  is explicitly time-dependent as the numerical flux evaluated at the boundary will involve time-dependent data  [2.x.6399] ,  [2.x.6400] , and  [2.x.6401]  on some parts of the boundary, depending on the assignment of boundary conditions. With this notation, we can write the discrete in space, continuous in time system compactly as 

[1.x.2483] 

where we have taken the liberty to also denote the global solution vector by  [2.x.6402]  (in addition to the the corresponding finite element function). Equivalently, the system above has the form 

[1.x.2484] 



For hyperbolic systems discretized by high-order discontinuous Galerkin methods, explicit time integration of this system is very popular. This is due to the fact that the mass matrix  [2.x.6403]  is block-diagonal (with each block corresponding to only variables of the same kind defined on the same cell) and thus easily inverted. In each time step -- or stage of a Runge--Kutta scheme -- one only needs to evaluate the differential operator once using the given data and subsequently apply the inverse of the mass matrix. For implicit time stepping, on the other hand, one would first have to linearize the equations and then iteratively solve the linear system, which involves several residual evaluations and at least a dozen applications of the linearized operator, as has been demonstrated in the step-33 tutorial program. 

Of course, the simplicity of explicit time stepping comes with a price, namely conditional stability due to the so-called Courant--Friedrichs--Lewy (CFL) condition. It states that the time step cannot be larger than the fastest propagation of information by the discretized differential operator. In more modern terms, the speed of propagation corresponds to the largest eigenvalue in the discretized operator, and in turn depends on the mesh size, the polynomial degree  [2.x.6404]  and the physics of the Euler operator, i.e., the eigenvalues of the linearization of  [2.x.6405]  with respect to  [2.x.6406] . In this program, we set the time step as follows: 

[1.x.2485] 



with the maximum taken over all quadrature points and all cells. The dimensionless number  [2.x.6407]  denotes the Courant number and can be chosen up to a maximally stable number  [2.x.6408] , whose value depends on the selected time stepping method and its stability properties. The power  [2.x.6409]  used for the polynomial scaling is heuristic and represents the closest fit for polynomial degrees between 1 and 8, see e.g.  [2.x.6410] . In the limit of higher degrees,  [2.x.6411] , a scaling of  [2.x.6412]  is more accurate, related to the inverse estimates typically used for interior penalty methods. Regarding the [1.x.2486] mesh sizes  [2.x.6413]  and  [2.x.6414]  used in the formula, we note that the convective transport is directional. Thus an appropriate scaling is to use the element length in the direction of the velocity  [2.x.6415] . The code below derives this scaling from the inverse of the Jacobian from the reference to real cell, i.e., we approximate  [2.x.6416] . The acoustic waves, instead, are isotropic in character, which is why we use the smallest feature size, represented by the smallest singular value of  [2.x.6417] , for the acoustic scaling  [2.x.6418] . Finally, we need to add the convective and acoustic limits, as the Euler equations can transport information with speed  [2.x.6419] . 

In this tutorial program, we use a specific variant of [1.x.2487], which in general use the following update procedure from the state  [2.x.6420]  at time  [2.x.6421]  to the new time  [2.x.6422]  with  [2.x.6423] : 

[1.x.2488] 

The vectors  [2.x.6424] ,  [2.x.6425] , in an  [2.x.6426] -stage scheme are evaluations of the operator at some intermediate state and used to define the end-of-step value  [2.x.6427]  via some linear combination. The scalar coefficients in this scheme,  [2.x.6428] ,  [2.x.6429] , and  [2.x.6430] , are defined such that certain conditions are satisfied for higher order schemes, the most basic one being  [2.x.6431] . The parameters are typically collected in the form of a so-called [1.x.2489] that collects all of the coefficients that define the scheme. For a five-stage scheme, it would look like this: 

[1.x.2490] 



In this tutorial program, we use a subset of explicit Runge--Kutta methods, so-called low-storage Runge--Kutta methods (LSRK), which assume additional structure in the coefficients. In the variant used by reference  [2.x.6432] , the assumption is to use Butcher tableaus of the form 

[1.x.2491] 

With such a definition, the update to  [2.x.6433]  shares the storage with the information for the intermediate values  [2.x.6434] . Starting with  [2.x.6435]  and  [2.x.6436] , the update in each of the  [2.x.6437]  stages simplifies to 

[1.x.2492] 

Besides the vector  [2.x.6438]  that is successively updated, this scheme only needs two auxiliary vectors, namely the vector  [2.x.6439]  to hold the evaluation of the differential operator, and the vector  [2.x.6440]  that holds the right-hand side for the differential operator application. In subsequent stages  [2.x.6441] , the values  [2.x.6442]  and  [2.x.6443]  can use the same storage. 

The main advantages of low-storage variants are the reduced memory consumption on the one hand (if a very large number of unknowns must be fit in memory, holding all  [2.x.6444]  to compute subsequent updates can be a limit already for  [2.x.6445]  in between five and eight -- recall that we are using an explicit scheme, so we do not need to store any matrices that are typically much larger than a few vectors), and the reduced memory access on the other. In this program, we are particularly interested in the latter aspect. Since cost of operator evaluation is only a small multiple of the cost of simply streaming the input and output vector from memory with the optimized matrix-free methods of deal.II, we must consider the cost of vector updates, and low-storage variants can deliver up to twice the throughput of conventional explicit Runge--Kutta methods for this reason, see e.g. the analysis in  [2.x.6446] . 

Besides three variants for third, fourth and fifth order accuracy from the reference  [2.x.6447] , we also use a fourth-order accurate variant with seven stages that was optimized for acoustics setups from  [2.x.6448] . Acoustic problems are one of the interesting aspects of the subsonic regime of the Euler equations where compressibility leads to the transmission of sound waves; often, one uses further simplifications of the linearized Euler equations around a background state or the acoustic wave equation around a fixed frame. 




[1.x.2493] 

The major ingredients used in this program are the fast matrix-free techniques we use to evaluate the operator  [2.x.6449]  and the inverse mass matrix  [2.x.6450] . Actually, the term [1.x.2494] is a slight misnomer, because we are working with a nonlinear operator and do not linearize the operator that in turn could be represented by a matrix. However, fast evaluation of integrals has become popular as a replacement of sparse matrix-vector products, as shown in step-37 and step-59, and we have coined this infrastructure [1.x.2495] in deal.II for this reason. Furthermore, the inverse mass matrix is indeed applied in a matrix-free way, as detailed below. 

The matrix-free infrastructure allows us to quickly evaluate the integrals in the weak forms. The ingredients are the fast interpolation from solution coefficients into values and derivatives at quadrature points, point-wise operations at quadrature points (where we implement the differential operator as derived above), as well as multiplication by all test functions and summation over quadrature points. The first and third component make use of sum factorization and have been extensively discussed in the step-37 tutorial program for the cell integrals and step-59 for the face integrals. The only difference is that we now deal with a system of  [2.x.6451]  components, rather than the scalar systems in previous tutorial programs. In the code, all that changes is a template argument of the FEEvaluation and FEFaceEvaluation classes, the one to set the number of components. The access to the vector is the same as before, all handled transparently by the evaluator. We also note that the variant with a single evaluator chosen in the code below is not the only choice -- we could also have used separate evalators for the separate components  [2.x.6452] ,  [2.x.6453] , and  [2.x.6454] ; given that we treat all components similarly (also reflected in the way we state the equation as a vector system), this would be more complicated here. As before, the FEEvaluation class provides explicit vectorization by combining the operations on several cells (and faces), involving data types called VectorizedArray. Since the arithmetic operations are overloaded for this type, we do not have to bother with it all that much, except for the evaluation of functions through the Function interface, where we need to provide particular [1.x.2496] evaluations for several quadrature point locations at once. 

A more substantial change in this program is the operation at quadrature points: Here, the multi-component evaluators provide us with return types not discussed before. Whereas  [2.x.6455]  would return a scalar (more precisely, a VectorizedArray type due to vectorization across cells) for the Laplacian of step-37, it now returns a type that is `Tensor<1,dim+2,VectorizedArray<Number>>`. Likewise, the gradient type is now `Tensor<1,dim+2,Tensor<1,dim,VectorizedArray<Number>>>`, where the outer tensor collects the `dim+2` components of the Euler system, and the inner tensor the partial derivatives in the various directions. For example, the flux  [2.x.6456]  of the Euler system is of this type. In order to reduce the amount of code we have to write for spelling out these types, we use the C++ `auto` keyword where possible. 

From an implementation point of view, the nonlinearity is not a big difficulty: It is introduced naturally as we express the terms of the Euler weak form, for example in the form of the momentum term  [2.x.6457] . To obtain this expression, we first deduce the velocity  [2.x.6458]  from the momentum variable  [2.x.6459] . Given that  [2.x.6460]  is represented as a  [2.x.6461] -degree polynomial, as is  [2.x.6462] , the velocity  [2.x.6463]  is a rational expression in terms of the reference coordinates  [2.x.6464] . As we perform the multiplication  [2.x.6465] , we obtain an expression that is the ratio of two polynomials, with polynomial degree  [2.x.6466]  in the numerator and polynomial degree  [2.x.6467]  in the denominator. Combined with the gradient of the test function, the integrand is of degree  [2.x.6468]  in the numerator and  [2.x.6469]  in the denominator already for affine cells, i.e., for parallelograms/ parallelepipeds. For curved cells, additional polynomial and rational expressions appear when multiplying the integrand by the determinant of the Jacobian of the mapping. At this point, one usually needs to give up on insisting on exact integration, and take whatever accuracy the Gaussian (more precisely, Gauss--Legrende) quadrature provides. The situation is then similar to the one for the Laplace equation, where the integrand contains rational expressions on non-affince cells and is also only integrated approximately. As these formulas only integrate polynomials exactly, we have to live with the [1.x.2497] in the form of an integration error. 

While inaccurate integration is usually tolerable for elliptic problems, for hyperbolic problems inexact integration causes some headache in the form of an effect called [1.x.2498]. The term comes from signal processing and expresses the situation of inappropriate, too coarse sampling. In terms of quadrature, the inappropriate sampling means that we use too few quadrature points compared to what would be required to accurately sample the variable-coefficient integrand. It has been shown in the DG literature that aliasing errors can introduce unphysical oscillations in the numerical solution for [1.x.2499] resolved simulations. The fact that aliasing mostly affects coarse resolutions -- whereas finer meshes with the same scheme work fine -- is not surprising because well-resolved simulations tend to be smooth on length-scales of a cell (i.e., they have small coefficients in the higher polynomial degrees that are missed by too few quadrature points, whereas the main solution contribution in the lower polynomial degrees is still well-captured -- this is simply a consequence of Taylor's theorem). To address this topic, various approaches have been proposed in the DG literature. One technique is filtering which damps the solution components pertaining to higher polynomial degrees. As the chosen nodal basis is not hierarchical, this would mean to transform from the nodal basis into a hierarchical one (e.g., a modal one based on Legendre polynomials) where the contributions within a cell are split by polynomial degrees. In that basis, one could then multiply the solution coefficients associated with higher degrees by a small number, keep the lower ones intact (to not destroy consistency), and then transform back to the nodal basis. However, filters reduce the accuracy of the method. Another, in some sense simpler, strategy is to use more quadrature points to capture non-linear terms more accurately. Using more than  [2.x.6470]  quadrature points per coordinate directions is sometimes called over-integration or consistent integration. The latter name is most common in the context of the incompressible Navier-Stokes equations, where the  [2.x.6471]  nonlinearity results in polynomial integrands of degree  [2.x.6472]  (when also considering the test function), which can be integrated exactly with  [2.x.6473]  quadrature points per direction as long as the element geometry is affine. In the context of the Euler equations with non-polynomial integrands, the choice is less clear. Depending on the variation in the various variables both  [2.x.6474]  or  [2.x.6475]  points (integrating exactly polynomials of degree  [2.x.6476]  or  [2.x.6477] , respectively) are common. 

To reflect this variability in the choice of quadrature in the program, we keep the number of quadrature points a variable to be specified just as the polynomial degree, and note that one would make different choices depending also on the flow configuration. The default choice is  [2.x.6478]  points -- a bit more than the minimum possible of  [2.x.6479]  points. The FEEvaluation and FEFaceEvaluation classes allow to seamlessly change the number of points by a template parameter, such that the program does not get more complicated because of that. 




[1.x.2500] 

The last ingredient is the evaluation of the inverse mass matrix  [2.x.6480] . In DG methods with explicit time integration, mass matrices are block-diagonal and thus easily inverted -- one only needs to invert the diagonal blocks. However, given the fact that matrix-free evaluation of integrals is closer in cost to the access of the vectors only, even the application of a block-diagonal matrix (e.g. via an array of LU factors) would be several times more expensive than evaluation of  [2.x.6481]  simply because just storing and loading matrices of size `dofs_per_cell` times `dofs_per_cell` for higher order finite elements repeatedly is expensive. As this is clearly undesirable, part of the community has moved to bases where the mass matrix is diagonal, for example the [1.x.2501]-orthogonal Legendre basis using hierarchical polynomials or Lagrange polynomials on the points of the Gaussian quadrature (which is just another way of utilizing Legendre information). While the diagonal property breaks down for deformed elements, the error made by taking a diagonal mass matrix and ignoring the rest (a variant of mass lumping, though not the one with an additional integration error as utilized in step-48) has been shown to not alter discretization accuracy. The Lagrange basis in the points of Gaussian quadrature is sometimes also referred to as a collocation setup, as the nodal points of the polynomials coincide (= are "co-located") with the points of quadrature, obviating some interpolation operations. Given the fact that we want to use more quadrature points for nonlinear terms in  [2.x.6482] , however, the collocation property is lost. (More precisely, it is still used in FEEvaluation and FEFaceEvaluation after a change of basis, see the matrix-free paper  [2.x.6483] .) 

In this tutorial program, we use the collocation idea for the application of the inverse mass matrix, but with a slight twist. Rather than using the collocation via Lagrange polynomials in the points of Gaussian quadrature, we prefer a conventional Lagrange basis in Gauss-Lobatto points as those make the evaluation of face integrals cheap. This is because for Gauss-Lobatto points, some of the node points are located on the faces of the cell and it is not difficult to show that on any given face, the only shape functions with non-zero values are exactly the ones whose node points are in fact located on that face. One could of course also use the Gauss-Lobatto quadrature (with some additional integration error) as was done in step-48, but we do not want to sacrifice accuracy as these quadrature formulas are generally of lower order than the general Gauss quadrature formulas. Instead, we use an idea described in the reference  [2.x.6484]  where it was proposed to change the basis for the sake of applying the inverse mass matrix. Let us denote by  [2.x.6485]  the matrix of shape functions evaluated at quadrature points, with shape functions in the row of the matrix and quadrature points in columns. Then, the mass matrix on a cell  [2.x.6486]  is given by 

[1.x.2502] 

Here,  [2.x.6487]  is the diagonal matrix with the determinant of the Jacobian times the quadrature weight (JxW) as entries. The matrix  [2.x.6488]  is constructed as the Kronecker product (tensor product) of one-dimensional matrices, e.g. in 3D as 

[1.x.2503] 

which is the result of the basis functions being a tensor product of one-dimensional shape functions and the quadrature formula being the tensor product of 1D quadrature formulas. For the case that the number of polynomials equals the number of quadrature points, all matrices in  [2.x.6489]  are square, and also the ingredients to  [2.x.6490]  in the Kronecker product are square. Thus, one can invert each matrix to form the overall inverse, 

[1.x.2504] 

This formula is of exactly the same structure as the steps in the forward evaluation of integrals with sum factorization techniques (i.e., the FEEvaluation and MatrixFree framework of deal.II). Hence, we can utilize the same code paths with a different interpolation matrix,  [2.x.6491]  rather than  [2.x.6492] . 

The class  [2.x.6493]  implements this operation: It changes from the basis contained in the finite element (in this case, FE_DGQ) to the Lagrange basis in Gaussian quadrature points. Here, the inverse of a diagonal mass matrix can be evaluated, which is simply the inverse of the `JxW` factors (i.e., the quadrature weight times the determinant of the Jacobian from reference to real coordinates). Once this is done, we can change back to the standard nodal Gauss-Lobatto basis. 

The advantage of this particular way of applying the inverse mass matrix is a cost similar to the forward application of a mass matrix, which is cheaper than the evaluation of the spatial operator  [2.x.6494]  with over-integration and face integrals. (We will demonstrate this with detailed timing information in the [1.x.2505].) In fact, it is so cheap that it is limited by the bandwidth of reading the source vector, reading the diagonal, and writing into the destination vector on most modern architectures. The hardware used for the result section allows to do the computations at least twice as fast as the streaming of the vectors from memory. 




[1.x.2506] 

In this tutorial program, we implement two test cases. The first case is a convergence test limited to two space dimensions. It runs a so-called isentropic vortex which is transported via a background flow field. The second case uses a more exciting setup: We start with a cylinder immersed in a channel, using the  [2.x.6495]  function. Here, we impose a subsonic initial field at Mach number of  [2.x.6496]  with a constant velocity in  [2.x.6497]  direction. At the top and bottom walls as well as at the cylinder, we impose a no-penetration (i.e., tangential flow) condition. This setup forces the flow to re-orient as compared to the initial condition, which results in a big sound wave propagating away from the cylinder. In upstream direction, the wave travels more slowly (as it has to move against the oncoming gas), including a discontinuity in density and pressure. In downstream direction, the transport is faster as sound propagation and fluid flow go in the same direction, which smears out the discontinuity somewhat. Once the sound wave hits the upper and lower walls, the sound is reflected back, creating some nice shapes as illustrated in the [1.x.2507] below. 


examples/step-67/doc/results.dox 



[1.x.2508] 

[1.x.2509] 

Running the program with the default settings on a machine with 40 processes produces the following output: 

[1.x.2510] 



The program output shows that all errors are small. This is due to the fact that we use a relatively fine mesh of  [2.x.6498]  cells with polynomials of degree 5 for a solution that is smooth. An interesting pattern shows for the time step size: whereas it is 0.0069 up to time 5, it increases to 0.0096 for later times. The step size increases once the vortex with some motion on top of the speed of sound (and thus faster propagation) leaves the computational domain between times 5 and 6.5. After that point, the flow is simply uniform in the same direction, and the maximum velocity of the gas is reduced compared to the previous state where the uniform velocity was overlaid by the vortex. Our time step formula recognizes this effect. 

The final block of output shows detailed information about the timing of individual parts of the programs; it breaks this down by showing the time taken by the fastest and the slowest processor, and the average time -- this is often useful in very large computations to find whether there are processors that are consistently overheated (and consequently are throttling their clock speed) or consistently slow for other reasons. The summary shows that 1283 time steps have been performed in 1.02 seconds (looking at the average time among all MPI processes), while the output of 11 files has taken additional 0.96 seconds. Broken down per time step and into the five Runge--Kutta stages, the compute time per evaluation is 0.16 milliseconds. This high performance is typical of matrix-free evaluators and a reason why explicit time integration is very competitive against implicit solvers, especially for large-scale simulations. The breakdown of computational times at the end of the program run shows that the evaluation of integrals in  [2.x.6499]  contributes with around 0.92 seconds and the application of the inverse mass matrix with 0.06 seconds. Furthermore, the estimation of the transport speed for the time step size computation contributes with another 0.05 seconds of compute time. 

If we use three more levels of global refinement and 9.4 million DoFs in total, the final statistics are as follows (for the modified Lax--Friedrichs flux,  [2.x.6500] , and the same system of 40 cores of dual-socket Intel Xeon Gold 6230): 

[1.x.2511] 



Per time step, the solver now takes 0.02 seconds, about 25 times as long as for the small problem with 147k unknowns. Given that the problem involves 64 times as many unknowns, the increase in computing time is not surprising. Since we also do 8 times as many time steps, the compute time should in theory increase by a factor of 512. The actual increase is 205 s / 1.02 s = 202. This is because the small problem size cannot fully utilize the 40 cores due to communication overhead. This becomes clear if we look into the details of the operations done per time step. The evaluation of the differential operator  [2.x.6501]  with nearest neighbor communication goes from 0.92 seconds to 127 seconds, i.e., it increases with a factor of 138. On the other hand, the cost for application of the inverse mass matrix and the vector updates, which do not need to communicate between the MPI processes at all, has increased by a factor of 1195. The increase is more than the theoretical factor of 512 because the operation is limited by the bandwidth from RAM memory for the larger size while for the smaller size, all vectors fit into the caches of the CPU. The numbers show that the mass matrix evaluation and vector update part consume almost 40% of the time spent by the Runge--Kutta stages -- despite using a low-storage Runge--Kutta integrator and merging of vector operations! And despite using over-integration for the  [2.x.6502]  operator. For simpler differential operators and more expensive time integrators, the proportion spent in the mass matrix and vector update part can also reach 70%. If we compute a throughput number in terms of DoFs processed per second and Runge--Kutta stage, we obtain [1.x.2512] This throughput number is very high, given that simply copying one vector to another one runs at only around 10,000 MDoFs/s. 

If we go to the next-larger size with 37.7 million DoFs, the overall simulation time is 2196 seconds, with 1978 seconds spent in the time stepping. The increase in run time is a factor of 9.3 for the L_h operator (1179 versus 127 seconds) and a factor of 10.3 for the inverse mass matrix and vector updates (797 vs 77.5 seconds). The reason for this non-optimal increase in run time can be traced back to cache effects on the given hardware (with 40 MB of L2 cache and 55 MB of L3 cache): While not all of the relevant data fits into caches for 9.4 million DoFs (one vector takes 75 MB and we have three vectors plus some additional data in MatrixFree), there is capacity for one and a half vector nonetheless. Given that modern caches are more sophisticated than the naive least-recently-used strategy (where we would have little re-use as the data is used in a streaming-like fashion), we can assume that a sizeable fraction of data can indeed be delivered from caches for the 9.4 million DoFs case. For the larger case, even with optimal caching less than 10 percent of data would fit into caches, with an associated loss in performance. 




[1.x.2513] 

For the modified Lax--Friedrichs flux and measuring the error in the momentum variable, we obtain the following convergence table (the rates are very similar for the density and energy variables): 

 [2.x.6503]  

If we switch to the Harten-Lax-van Leer flux, the results are as follows:  [2.x.6504]  

The tables show that we get optimal  [2.x.6505]  convergence rates for both numerical fluxes. The errors are slightly smaller for the Lax--Friedrichs flux for  [2.x.6506] , but the picture is reversed for  [2.x.6507] ; in any case, the differences on this testcase are relatively small. 

For  [2.x.6508] , we reach the roundoff accuracy of  [2.x.6509]  with both fluxes on the finest grids. Also note that the errors are absolute with a domain length of  [2.x.6510] , so relative errors are below  [2.x.6511] . The HLL flux is somewhat better for the highest degree, which is due to a slight inaccuracy of the Lax--Friedrichs flux: The Lax--Friedrichs flux sets a Dirichlet condition on the solution that leaves the domain, which results in a small artificial reflection, which is accentuated for the Lax--Friedrichs flux. Apart from that, we see that the influence of the numerical flux is minor, as the polynomial part inside elements is the main driver of the accucary. The limited influence of the flux also has consequences when trying to approach more challenging setups with the higher-order DG setup: Taking for example the parameters and grid of step-33, we get oscillations (which in turn make density negative and make the solution explode) with both fluxes once the high-mass part comes near the boundary, as opposed to the low-order finite volume case ( [2.x.6512] ). Thus, any case that leads to shocks in the solution necessitates some form of limiting or artificial dissipation. For another alternative, see the step-69 tutorial program. 




[1.x.2520] 

For the test case of the flow around a cylinder in a channel, we need to change the first code line to 

[1.x.2521] 

This test case starts with a background field of a constant velocity of Mach number 0.31 and a constant initial density; the flow will have to go around an obstacle in the form of a cylinder. Since we impose a no-penetration condition on the cylinder walls, the flow that initially impinges head-on onto to cylinder has to rearrange, which creates a big sound wave. The following pictures show the pressure at times 0.1, 0.25, 0.5, and 1.0 (top left to bottom right) for the 2D case with 5 levels of global refinement, using 102,400 cells with polynomial degree of 5 and 14.7 million degrees of freedom over all 4 solution variables. We clearly see the discontinuity that propagates slowly in the upstream direction and more quickly in downstream direction in the first snapshot at time 0.1. At time 0.25, the sound wave has reached the top and bottom walls and reflected back to the interior. From the different distances of the reflected waves from lower and upper walls we can see the slight asymmetry of the Sch&auml;fer-Turek test case represented by  [2.x.6513]  with somewhat more space above the cylinder compared to below. At later times, the picture is more chaotic with many sound waves all over the place. 

 [2.x.6514]  

The next picture shows an elevation plot of the pressure at time 1.0 looking from the channel inlet towards the outlet at the same resolution -- here, we can see the large number of reflections. In the figure, two types of waves are visible. The larger-amplitude waves correspond to various reflections that happened as the initial discontinuity hit the walls, whereas the small-amplitude waves of size similar to the elements correspond to numerical artifacts. They have their origin in the finite resolution of the scheme and appear as the discontinuity travels through elements with high-order polynomials. This effect can be cured by increasing resolution. Apart from this effect, the rich wave structure is the result of the transport accuracy of the high-order DG method. 

 [2.x.6515]  

With 2 levels of global refinement with 1,600 cells, the mesh and its partitioning on 40 MPI processes looks as follows: 

 [2.x.6516]  

When we run the code with 4 levels of global refinements on 40 cores, we get the following output: 

[1.x.2522] 



The norms shown here for the various quantities are the deviations  [2.x.6517] ,  [2.x.6518] , and  [2.x.6519]  against the background field (namely, the initial condition). The distribution of run time is overall similar as in the previous test case. The only slight difference is the larger proportion of time spent in  [2.x.6520]  as compared to the inverse mass matrix and vector updates. This is because the geometry is deformed and the matrix-free framework needs to load additional arrays for the geometry from memory that are compressed in the affine mesh case. 

Increasing the number of global refinements to 5, the output becomes: 

[1.x.2523] 



The effect on performance is similar to the analytical test case -- in theory, computation times should increase by a factor of 8, but we actually see an increase by a factor of 11 for the time steps (219.5 seconds versus 2450 seconds). This can be traced back to caches, with the small case mostly fitting in caches. An interesting effect, typical of programs with a mix of local communication (integrals  [2.x.6521] ) and global communication (computation of transport speed) with some load imbalance, can be observed by looking at the MPI ranks that encounter the minimal and maximal time of different phases, respectively. Rank 0 reports the fastest throughput for the "rk time stepping total" part. At the same time, it appears to be slowest for the "compute transport speed" part, almost a factor of 2 slower than the average and almost a factor of 4 compared to the faster rank. Since the latter involves global communication, we can attribute the slowness in this part to the fact that the local Runge--Kutta stages have advanced more quickly on this rank and need to wait until the other processors catch up. At this point, one can wonder about the reason for this imbalance: The number of cells is almost the same on all MPI processes. However, the matrix-free framework is faster on affine and Cartesian cells located towards the outlet of the channel, to which the lower MPI ranks are assigned. On the other hand, rank 32, which reports the highest run time for the Runga--Kutta stages, owns the curved cells near the cylinder, for which no data compression is possible. To improve throughput, we could assign different weights to different cell types when partitioning the  [2.x.6522]  object, or even measure the run time for a few time steps and try to rebalance then. 

The throughput per Runge--Kutta stage can be computed to 2085 MDoFs/s for the 14.7 million DoFs test case over the 346,000 Runge--Kutta stages, slightly slower than the Cartesian mesh throughput of 2360 MDoFs/s reported above. 

Finally, if we add one additional refinement, we record the following output: 

[1.x.2524] 



The "rk time stepping total" part corresponds to a throughput of 2010 MDoFs/s. The overall run time to perform 139k time steps is 20k seconds (5.7 hours) or 7 time steps per second -- not so bad for having nearly 60 million unknowns. More throughput can be achieved by adding more cores to the computation. 




[1.x.2525] 

Switching the channel test case to 3D with 3 global refinements, the output is 

[1.x.2526] 



The physics are similar to the 2D case, with a slight motion in the z direction due to the gravitational force. The throughput per Runge--Kutta stage in this case is 

[1.x.2527] 



The throughput is lower than in 2D because the computation of the  [2.x.6523]  term is more expensive. This is due to over-integration with `degree+2` points and the larger fraction of face integrals (worse volume-to-surface ratio) with more expensive flux computations. If we only consider the inverse mass matrix and vector update part, we record a throughput of 4857 MDoFs/s for the 2D case of the isentropic vortex with 37.7 million unknowns, whereas the 3D case runs with 4535 MDoFs/s. The performance is similar because both cases are in fact limited by the memory bandwidth. 

If we go to four levels of global refinement, we need to increase the number of processes to fit everything in memory -- the computation needs around 350 GB of RAM memory in this case. Also, the time it takes to complete 35k time steps becomes more tolerable by adding additional resources. We therefore use 6 nodes with 40 cores each, resulting in a computation with 240 MPI processes: 

[1.x.2528] 

This simulation had nearly 2 billion unknowns -- quite a large computation indeed, and still only needed around 1.5 seconds per time step. 




[1.x.2529] 

The code presented here straight-forwardly extends to adaptive meshes, given appropriate indicators for setting the refinement flags. Large-scale adaptivity of a similar solver in the context of the acoustic wave equation has been achieved by the [1.x.2530]. However, in the present context, the benefits of adaptivity are often limited to early times and effects close to the origin of sound waves, as the waves eventually reflect and diffract. This leads to steep gradients all over the place, similar to turbulent flow, and a more or less globally refined mesh. 

Another topic that we did not discuss in the results section is a comparison of different time integration schemes. The program provides four variants of low-storage Runga--Kutta integrators that each have slightly different accuracy and stability behavior. Among the schemes implemented here, the higher-order ones provide additional accuracy but come with slightly lower efficiency in terms of step size per stage before they violate the CFL condition. An interesting extension would be to compare the low-storage variants proposed here with standard Runge--Kutta integrators or to use vector operations that are run separate from the mass matrix operation and compare performance. 




[1.x.2531] 

As mentioned in the introduction, the modified Lax--Friedrichs flux and the HLL flux employed in this program are only two variants of a large body of numerical fluxes available in the literature on the Euler equations. One example is the HLLC flux (Harten-Lax-van Leer-Contact) flux which adds the effect of rarefaction waves missing in the HLL flux, or the Roe flux. As mentioned in the introduction, the effect of numerical fluxes on high-order DG schemes is debatable (unlike for the case of low-order discretizations). 

A related improvement to increase the stability of the solver is to also consider the spatial integral terms. A shortcoming in the rather naive implementation used above is the fact that the energy conservation of the original Euler equations (in the absence of shocks) only holds up to a discretization error. If the solution is under-resolved, the discretization error can give rise to an increase in the numerical energy and eventually render the discretization unstable. This is because of the inexact numerical integration of the terms in the Euler equations, which both contain rational nonlinearities and higher-degree content from curved cells. A way out of this dilemma are so-called skew-symmetric formulations, see  [2.x.6524]  for a simple variant. Skew symmetry means that switching the role of the solution  [2.x.6525]  and test functions  [2.x.6526]  in the weak form produces the exact negative of the original quantity, apart from some boundary terms. In the discrete setting, the challenge is to keep this skew symmetry also when the integrals are only computed approximately (in the continuous case, skew-symmetry is a consequence of integration by parts). Skew-symmetric numerical schemes balance spatial derivatives in the conservative form  [2.x.6527]  with contributions in the convective form  [2.x.6528]  for some  [2.x.6529] . The precise terms depend on the equation and the integration formula, and can in some cases by understood by special skew-symmetric finite difference schemes. 

To get started, interested readers could take a look at https://github.com/kronbichler/advection_miniapp, where a skew-symmetric DG formulation is implemented with deal.II for a simple advection equation. 

[1.x.2532] 

As mentioned in the introduction, the solution to the Euler equations develops shocks as the Mach number increases, which require additional mechanisms to stabilize the scheme, e.g. in the form of limiters. The main challenge besides actually implementing the limiter or artificial viscosity approach would be to load-balance the computations, as the additional computations involved for limiting the oscillations in troubled cells would make them more expensive than the plain DG cells without limiting. Furthermore, additional numerical fluxes that better cope with the discontinuities would also be an option. 

One ingredient also necessary for supersonic flows are appropriate boundary conditions. As opposed to the subsonic outflow boundaries discussed in the introduction and implemented in the program, all characteristics are outgoing for supersonic outflow boundaries, so we do not want to prescribe any external data, 

[1.x.2533] 



In the code, we would simply add the additional statement 

[1.x.2534] 

in the `local_apply_boundary_face()` function. 

[1.x.2535] 

When the interest with an Euler solution is mostly in the propagation of sound waves, it often makes sense to linearize the Euler equations around a background state, i.e., a given density, velocity and energy (or pressure) field, and only compute the change against these fields. This is the setting of the wide field of aeroacoustics. Even though the resolution requirements are sometimes considerably reduced, implementation gets somewhat more complicated as the linearization gives rise to additional terms. From a code perspective, in the operator evaluation we also need to equip the code with the state to linearize against. This information can be provided either by analytical functions (that are evaluated in terms of the position of the quadrature points) or by a vector similar to the solution. Based on that vector, we would create an additional FEEvaluation object to read from it and provide the values of the field at quadrature points. If the background velocity is zero and the density is constant, the linearized Euler equations further simplify and can equivalently be written in the form of the acoustic wave equation. 

A challenge in the context of sound propagation is often the definition of boundary conditions, as the computational domain needs to be of finite size, whereas the actual simulation often spans an infinite (or at least much larger) physical domain. Conventional Dirichlet or Neumann boundary conditions give rise to reflections of the sound waves that eventually propagate back to the region of interest and spoil the solution. Therefore, various variants of non-reflecting boundary conditions or sponge layers, often in the form of [1.x.2536] -- where the solution is damped without reflection 

-- are common. 




[1.x.2537] 

The solver presented in this tutorial program can also be extended to the compressible Navier--Stokes equations by adding viscous terms, as described in  [2.x.6530] . To keep as much of the performance obtained here despite the additional cost of elliptic terms, e.g. via an interior penalty method, one can switch the basis from FE_DGQ to FE_DGQHermite like in the step-59 tutorial program. 




[1.x.2538] 

In this tutorial, we used face-centric loops. Here, cell and face integrals are treated in separate loops, resulting in multiple writing accesses into the result vector, which is relatively expensive on modern hardware since writing operations generally result also in an implicit read operation. Element-centric loops, on the other hand, are processing a cell and in direct succession processing all its 2d faces. Although this kind of loop implies that fluxes have to be computed twice (for each side of an interior face), the fact that the result vector has to accessed only once might - and the fact that the resulting algorithm is free of race-conditions and as such perfectly suitable for shared memory - already give a performance boost. If you are interested in these advanced topics, you can take a look at step-76 where we take the present tutorial and modify it so that we can use these features. 


examples/step-68/doc/intro.dox 

 [2.x.6531]  

[1.x.2539] 

[1.x.2540] 

[1.x.2541] 

Particles play an important part in numerical models for a large  number of applications. Particles are routinely used  as massless tracers to visualize the dynamic of a transient flow. They  can also play an intrinsic role as part of a more complex finite element  model, as is the case for the Particle-In-Cell (PIC) method  [2.x.6532]   or they can even be used to simulate the motion of granular matter, as in  the Discrete Element Method (DEM)  [2.x.6533] . In the case  of DEM, the resulting model is not related to the finite element method anymore,  but just leads to a system of ordinary differential equation which describes  the motion of the particles and the dynamic of their collisions. All of  these models can be built using deal.II's particle handling capabilities. 

In the present step, we use particles as massless tracers to illustrate the dynamic of a vortical flow. Since the particles are massless tracers, the position of each particle  [2.x.6534]  is described by the following ordinary differential equation (ODE): 

[1.x.2542] 



where  [2.x.6535]  is the position of particle  [2.x.6536]  and  [2.x.6537]  the flow velocity at its position. In the present step, this ODE is solved using the explicit Euler method. The resulting scheme is: 

[1.x.2543] 



where  [2.x.6538]  and  [2.x.6539]  are the position of particle  [2.x.6540]  at time  [2.x.6541]  and  [2.x.6542] , respectively and where  [2.x.6543]  is the time step. In the present step, the velocity at the location of particles is obtained in two different fashions: 

- By evaluating the velocity function at the location of the particles; 

- By evaluating the velocity function on a background triangulation and, using a  finite element support, interpolating at the position of the particle. 

The first approach is not practical, since the velocity profile is generally not known analytically. The second approach, based on interpolating a solution at the position of the particles, mimics exactly what would be done in a realistic computational fluid dynamic simulation, and this follows the way we have also evaluated the finite element solution at particle locations in step-19. In this step, we illustrate both strategies. 

We note that much greater accuracy could be obtained by using a fourth order Runge-Kutta method or another appropriate scheme for the time integration of the motion of the particles.  Implementing a more advanced time-integration scheme would be a straightforward extension of this step. 

[1.x.2544] 

In deal.II,  [2.x.6544]  are very simple and flexible entities that can be used to build PIC, DEM or any type of particle-based models. Particles have a location in real space, a location in the reference space of the element in which they are located and a unique ID. In the majority of cases, simulations that include particles require a significant number of them. Thus, it becomes interesting to handle all particles through an entity which agglomerates all particles. In deal.II, this is achieved through the use of the  [2.x.6545]  class. 

By default, particles do not have a diameter, a mass or any other physical properties which we would generally expect of physical particles. However, through a ParticleHandler, particles have access to a  [2.x.6546]  This PropertyPool is an array which can be used to store an arbitrary number of properties associated with the particles. Consequently, users can build their own particle solver and attribute the desired properties to the particles (e.g., mass, charge, diameter, temperature, etc.). In the present tutorial, this is used to store the value of the fluid velocity and the process id to which the particles belong. 

[1.x.2545] 

Although the present step is not computationally intensive, simulations that include many particles can be computationally demanding and require parallelization. The present step showcases the distributed parallel capabilities of deal.II for particles. In general, there are three main challenges that specifically arise in parallel distributed simulations that include particles: 

- Generating the particles on the distributed triangulation; 

- Exchanging the particles that leave local domains between the processors; 

- Load balancing the simulation so that every processor has a similar computational load. These challenges and their solution in deal.II have been discussed in more detail in  [2.x.6547] , but we will summarize them below. 

There are of course also questions on simply setting up a code that uses particles. These have largely already been addressed in step-19. Some more advanced techniques will also be discussed in step-70. 

[1.x.2546] 

Generating distributed particles in a scalable way is not straightforward since the processor to which they belong must first be identified before the cell in which they are located is found.  deal.II provides numerous capabilities to generate particles through the  [2.x.6548]  namespace.  Some of these particle generators create particles only on the locally owned subdomain. For example,  [2.x.6549]  creates particles at the same reference locations within each cell of the local subdomain and  [2.x.6550]  uses a globally defined probability density function to determine how many and where to generate particles locally. 

In other situations, such as the present step, particles must be generated at specific locations on cells that may be owned only by a subset of the processors. In  most of these situations, the insertion of the particles is done for a very limited number of time-steps and, consequently, does not constitute a large portion of the computational cost. For these occasions, deal.II provides convenient  [2.x.6551]  that can globally insert the particles even if the particle is not located in a cell owned by the parallel process on which the call to create the particle is initiated. The generators first locate on which subdomain the particles are situated, identify in which cell they are located and exchange the necessary information among the processors to ensure that the particle is generated with the right properties. Consequently, this type of particle generation can be communication intensive. The  [2.x.6552]  and the  [2.x.6553]  generate particles using a triangulation and the points of an associated DoFHandler or quadrature respectively. The triangulation that is used to generate the particles can be the same triangulation that is used for the background mesh, in which case these functions are very similar to the  [2.x.6554]  function described in the previous paragraph. However, the triangulation used to generate particles can also be different (non-matching) from the triangulation of the background grid, which is useful to generate particles in particular shapes (as in this example), or to transfer information between two different computational grids (as in step-70).  Furthermore, the  [2.x.6555]  class provides the  [2.x.6556]  function which enables the global insertion of particles from a vector of arbitrary points and a global vector of bounding boxes. In the present step, we use the  [2.x.6557]  function on a non-matching triangulation to insert particles located at positions in the shape of a disk. 

[1.x.2547] 

As particles move around in parallel distributed computations they may leave the locally owned subdomain and need to be transferred to their new owner processes. This situation can arise in two very different ways: First, if the previous owning process knows the new owner of the particles that were lost (for example because the particles moved from the locally owned cell of one processor into an adjacent ghost cells of a distributed triangulation) then the transfer can be handled efficiently as a point-to-point communication between each process and the new owners. This transfer happens automatically whenever particles are sorted into their new cells. Secondly, the previous owner may not know to which process the particle has moved. In this case the particle is discarded by default, as a global search for the owner can be expensive. step-19 shows how such a discarded particle can still be collected, interpreted, and potentially reinserted by the user. In the present example we prevent the second case by imposing a CFL criterion on the timestep to ensure particles will at most move into the ghost layer of the local process and can therefore be send to neighboring processes automatically. 

[1.x.2548] 

The last challenge that arises in parallel distributed computations using particles is to balance the computational load between work that is done on the grid, for example solving the finite-element problem, and the work that is done on the particles, for example advecting the particles or computing the forces between particles or between particles and grid. By default, for example in step-40, deal.II distributes the background mesh as evenly as possible between the available processes, that is it balances the number of cells on each process. However, if some cells own many more particles than other cells, or if the particles of one cell are much more computationally expensive than the particles in other cells, then this problem no longer scales efficiently (for a discussion of what we consider "scalable" programs, see  [2.x.6558]  "this glossary entry"). Thus, we have to apply a form of "load balancing", which means we estimate the computational load that is associated with each cell and its particles. Repartitioning the mesh then accounts for this combined computational load instead of the simplified assumption of the number of cells  [2.x.6559] . 

In this section we only discussed the particle-specific challenges in distributed computation. Parallel challenges that particles share with finite-element solutions (parallel output, data transfer during mesh refinement) can be addressed with the solutions found for finite-element problems already discussed in other examples. 

[1.x.2549] 

In the present step, we use particles as massless tracers to illustrate the dynamics of a particular vortical flow: the Rayleigh--Kothe vortex. This flow pattern is generally used as a complex test case for interface tracking methods (e.g., volume-of-fluid and level set approaches) since it leads to strong rotation and elongation of the fluid  [2.x.6560] . 

The stream function  [2.x.6561]  of this Rayleigh-Kothe vortex is defined as: 

[1.x.2550] 

where  [2.x.6562]  is half the period of the flow. The velocity profile in 2D ( [2.x.6563] ) is : 

[1.x.2551] 



The velocity profile is illustrated in the following animation: 

[1.x.2552] 



It can be seen that this velocity reverses periodically due to the term  [2.x.6564]  and that material will end up at its starting position after every period of length  [2.x.6565] . We will run this tutorial program for exactly one period and compare the final particle location to the initial location to illustrate this flow property. This example uses the testcase to produce two models that handle the particles slightly differently. The first model prescribes the exact analytical velocity solution as the velocity for each particle. Therefore in this model there is no error in the assigned velocity to the particles, and any deviation of particle positions from the analytical position at a given time results from the error in solving the equation of motion for the particle inexactly, using a time stepping method. In the second model the analytical velocity field is first interpolated to a finite-element vector space (to simulate the case that the velocity was obtained from solving a finite-element problem, in the same way as the ODE for each particle in step-19 depends on a finite element solution). This finite-element "solution" is then evaluated at the locations of the particles to solve their equation of motion. The difference between the two cases allows to assess whether the chosen finite-element space is sufficiently accurate to advect the particles with the optimal convergence rate of the chosen particle advection scheme, a question that is important in practice to determine the accuracy of the combined algorithm (see e.g.  [2.x.6566] ). 


examples/step-68/doc/results.dox 



[1.x.2553] 

The directory in which this program is run contains an example parameter file by default. If you do not specify a parameter file as an argument on the command line, the program will try to read the file "parameters.prm" by default, and will execute the code. 

On any number of cores, the simulation output will look like: 

[1.x.2554] 



We note that, by default, the simulation runs the particle tracking with an analytical velocity for 2000 iterations, then restarts from the beginning and runs the particle tracking with velocity interpolation for the same duration. The results are written every 10th iteration. 

[1.x.2555] 

The following animation displays the trajectory of the particles as they are advected by the flow field. We see that after the complete duration of the flow, the particle go back to their initial configuration as is expected. 

[1.x.2556] 



[1.x.2557] 

The following animation shows the impact of dynamic load balancing. We clearly see that the subdomains adapt themselves to balance the number of particles per subdomain. However, a perfect load balancing is not reached, in part due to the coarseness of the background mesh. 

[1.x.2558] 






[1.x.2559] 

This program highlights some of the main capabilities for handling particles in deal.II, notably their capacity to be used in distributed parallel simulations. However, this step could be extended in numerous manners: 

- High-order time integration (for example using a Runge-Kutta 4 method) could be used to increase the accuracy or allow for larger time-step sizes with the same accuracy. 

- The full equation of motion (with inertia) could be solved for the particles. In this case the particles would need to have additional properties such as their mass, as in step-19, and if one wanted to also consider interactions with the fluid, their diameter. 

- Coupling to a flow solver. This step could be straightforwardly coupled to any parallel program in which the Stokes (step-32, step-70) or the Navier-Stokes equations are solved (e.g., step-57). 

- Computing the difference in final particle positions between the two models would allow to quantify the influence of the interpolation error on particle motion. 


examples/step-69/doc/intro.dox 

[1.x.2560] 

 [2.x.6567]  [2.x.6568] Sandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA0003525. This document describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government. [2.x.6569]  

 [2.x.6570]  This tutorial step implements a first-order accurate [1.x.2561] based on a first-order [1.x.2562] for solving Euler's equations of gas dynamics  [2.x.6571] . As such it is presented primarily for educational purposes. For actual research computations you might want to consider exploring a corresponding [1.x.2563] that uses [1.x.2564] techniques, and strong stability-preserving (SSP) time integration, see  [2.x.6572]  ([1.x.2565]). 

 [2.x.6573]  

[1.x.2566] 

[1.x.2567] 

This tutorial presents a first-order scheme for solving compressible Euler's equations that is based on three ingredients: a [1.x.2568]-type discretization of Euler's equations in the context of finite elements; a graph-viscosity stabilization based on a [1.x.2569] upper bound of the local wave speed; and explicit time-stepping. As such, the ideas and techniques presented in this tutorial step are drastically different from those used in step-33, which focuses on the use of automatic differentiation. From a programming perspective this tutorial will focus on a number of techniques found in large-scale computations: hybrid thread-MPI parallelization; efficient local numbering of degrees of freedom; concurrent post-processing and write-out of results using worker threads; as well as checkpointing and restart. 

It should be noted that first-order schemes in the context of hyperbolic conservation laws require prohibitively many degrees of freedom to resolve certain key features of the simulated fluid, and thus, typically only serve as elementary building blocks in higher-order schemes  [2.x.6574] . However, we hope that the reader still finds the tutorial step to be a good starting point (in particular with respect to the programming techniques) before jumping into full research codes such as the second-order scheme discussed in  [2.x.6575] . 


[1.x.2570] 

[1.x.2571] 

The compressible Euler's equations of gas dynamics are written in conservative form as follows: 

[1.x.2572] 

where  [2.x.6576] , and  [2.x.6577] , and  [2.x.6578]  is the space dimension. We say that  [2.x.6579]  is the state and  [2.x.6580]  is the flux of the system. In the case of Euler's equations the state is given by  [2.x.6581] : where  [2.x.6582]  denotes the density,  [2.x.6583]  is the momentum, and  [2.x.6584]  is the total energy of the system. The flux of the system  [2.x.6585]  is defined as 

[1.x.2573] 

where  [2.x.6586]  is the identity matrix and  [2.x.6587]  denotes the tensor product. Here, we have introduced the pressure  [2.x.6588]  that, in general, is defined by a closed-form equation of state. In this tutorial we limit the discussion to the class of polytropic ideal gases for which the pressure is given by 

[1.x.2574] 

where the factor  [2.x.6589]  denotes the [1.x.2575]. 




[1.x.2576] 

Hyperbolic conservation laws, such as 

[1.x.2577] 

pose a significant challenge with respect to solution theory. An evident observation is that rewriting the equation in variational form and testing with the solution itself does not lead to an energy estimate because the pairing  [2.x.6590]  (understood as the  [2.x.6591]  inner product or duality pairing) is not guaranteed to be non-negative. Notions such as energy-stability or  [2.x.6592] -stability are (in general) meaningless in this context. 

Historically, the most fruitful step taken in order to deepen the understanding of hyperbolic conservation laws was to assume that the solution is formally defined as  [2.x.6593]  where  [2.x.6594]  is the solution of the parabolic regularization 

[1.x.2578] 

Such solutions, which are understood as the solution recovered in the zero-viscosity limit, are often referred to as [1.x.2579]. (This is, because physically  [2.x.6595]  can be understood as related to the viscosity of the fluid, i.e., a quantity that indicates the amount of friction neighboring gas particles moving at different speeds exert on each other. The Euler equations themselves are derived under the assumption of no friction, but can physically be expected to describe the limiting case of vanishing friction or viscosity.) Global existence and uniqueness of such solutions is an open issue. However, we know at least that if such viscosity solutions exists they have to satisfy the constraint  [2.x.6596]  for all  [2.x.6597]  and  [2.x.6598]  where 

[1.x.2580] 

Here,  [2.x.6599]  denotes the specific entropy 

[1.x.2581] 

We will refer to  [2.x.6600]  as the invariant set of Euler's equations. In other words, a state  [2.x.6601]  obeys positivity of the density, positivity of the internal energy, and a local minimum principle on the specific entropy. This condition is a simplified version of a class of pointwise stability constraints satisfied by the exact (viscosity) solution. By pointwise we mean that the constraint has to be satisfied at every point of the domain, not just in an averaged (integral, or high order moments) sense. 

In context of a numerical approximation, a violation of such a constraint has dire consequences: it almost surely leads to catastrophic failure of the numerical scheme, loss of hyperbolicity, and overall, loss of well-posedness of the (discrete) problem. It would also mean that we have computed something that can not be interpreted physically. (For example, what are we to make of a computed solution with a negative density?) In the following we will formulate a scheme that ensures that the discrete approximation of  [2.x.6602]  remains in  [2.x.6603] . 




[1.x.2582] 

Following Step-9, Step-12, Step-33, and Step-67, at this point it might look tempting to base a discretization of Euler's equations on a (semi-discrete) variational formulation: 

[1.x.2583] 

Here,  [2.x.6604]  is an appropriate finite element space, and  [2.x.6605]  is some linear stabilization method (possibly complemented with some ad-hoc shock-capturing technique, see for instance Chapter 5 of  [2.x.6606]  and references therein). Most time-dependent discretization approaches described in the deal.II tutorials are based on such a (semi-discrete) variational approach. Fundamentally, from an analysis perspective, variational discretizations are conceived to provide some notion of global (integral) stability, meaning an estimate of the form 

[1.x.2584] 

holds true, where  [2.x.6607]  could represent the  [2.x.6608] -norm or, more generally, some discrete (possibly mesh dependent) energy-norm. Variational discretizations of hyperbolic conservation laws have been very popular since the mid eighties, in particular combined with SUPG-type stabilization and/or upwinding techniques (see the early work of  [2.x.6609]  and  [2.x.6610] ). They have proven to be some of the best approaches for simulations in the subsonic shockless regime and similarly benign situations. 

<!-- In particular, tutorial Step-67 focuses on Euler's equation of gas dynamics in the subsonic regime using dG techniques. --> 

However, in the transonic and supersonic regimes, and shock-hydrodynamics applications the use of variational schemes might be questionable. In fact, at the time of this writing, most shock-hydrodynamics codes are still firmly grounded on finite volume methods. The main reason for failure of variational schemes in such extreme regimes is the lack of pointwise stability. This stems from the fact that [1.x.2585] bounds on integrated quantities (e.g. integrals of moments) have in general no implications on pointwise properties of the solution. While some of these problems might be alleviated by the (perpetual) chase of the right shock capturing scheme, finite difference-like and finite volume schemes still have an edge in many regards. 

In this tutorial step we therefore depart from variational schemes. We will present a completely algebraic formulation (with the flavor of a collocation-type scheme) that preserves constraints pointwise, i.e., 

[1.x.2586] 

Contrary to finite difference/volume schemes, the scheme implemented in this step maximizes the use of finite element software infrastructure, works on any mesh, in any space dimension, and is theoretically guaranteed to always work, all the time, no exception. This illustrates that deal.II can be used far beyond the context of variational schemes in Hilbert spaces and that a large number of classes, modules and namespaces from deal.II can be adapted for such a purpose. 




[1.x.2587] 

Let  [2.x.6611]  be scalar-valued finite dimensional space spanned by a basis  [2.x.6612]  where:  [2.x.6613]  and  [2.x.6614]  is the set of all indices (nonnegative integers) identifying each scalar Degree of Freedom (DOF) in the mesh. Therefore a scalar finite element functional  [2.x.6615]  can be written as  [2.x.6616]  with  [2.x.6617] . We introduce the notation for vector-valued approximation spaces  [2.x.6618] . Let  [2.x.6619] , then it can be written as  [2.x.6620]  where  [2.x.6621]  and  [2.x.6622]  is a scalar-valued shape function. 

 [2.x.6623]  We purposely refrain from using vector-valued finite element spaces in our notation. Vector-valued finite element spaces are natural for variational formulations of PDE systems (e.g. Navier-Stokes). In such context, the interactions that have to be computed describe [1.x.2588]: with proper renumbering of the vector-valued DoFHandler (i.e. initialized with an FESystem) it is possible to compute the block-matrices (required in order to advance the solution) with relative ease. However, the interactions that have to be computed in the context of time-explicit collocation-type schemes (such as finite differences and/or the scheme presented in this tutorial) can be better described as [1.x.2589] (not between DOFs). In addition, in our case we do not solve a linear equation in order to advance the solution. This leaves very little reason to use vector-valued finite element spaces both in theory and/or practice. 

We will use the usual Lagrange finite elements: let  [2.x.6624]  denote the set of all support points (see  [2.x.6625]  "this glossary entry"), where  [2.x.6626] . Then each index  [2.x.6627]  uniquely identifies a support point  [2.x.6628] , as well as a scalar-valued shape function  [2.x.6629] . With this notation at hand we can define the (explicit time stepping) scheme as: 

[1.x.2590] 

where 

  -  [2.x.6630]      is the lumped mass matrix 

  -  [2.x.6631]  is the time step size 

  -  [2.x.6632]  (note that  [2.x.6633] )     is a vector-valued matrix that was used to approximate the divergence     of the flux in a weak sense. 

  -  [2.x.6634]  is the adjacency list     containing all degrees of freedom coupling to the index  [2.x.6635] . In other     words  [2.x.6636]  contains all nonzero column indices for row     index i.  [2.x.6637]  will also be called a "stencil". 

  -  [2.x.6638]  is the flux  [2.x.6639]  of the     hyperbolic system evaluated for the state  [2.x.6640]  associated     with support point  [2.x.6641] . 

  -  [2.x.6642]  if  [2.x.6643]  is the so     called [1.x.2591]. The graph viscosity serves as a     stabilization term, it is somewhat the discrete counterpart of      [2.x.6644]  that appears in the notion of viscosity     solution described above. We will base our construction of  [2.x.6645]  on     an estimate of the maximal local wavespeed  [2.x.6646]  that     will be explained in detail in a moment. 

  - the diagonal entries of the viscosity matrix are defined as      [2.x.6647] . 

  -  [2.x.6648]  is a     normalization of the  [2.x.6649]  matrix that enters the     approximate Riemann solver with which we compute an the approximations      [2.x.6650]  on the local wavespeed. (This will be explained     further down below). 

The definition of  [2.x.6651]  is far from trivial and we will postpone the precise definition in order to focus first on some algorithmic and implementation questions. We note that 

  -  [2.x.6652]  and  [2.x.6653]  do not evolve in time (provided we keep the     discretization fixed). It thus makes sense to assemble these     matrices/vectors once in a so called [1.x.2592] and reuse     them in every time step. They are part of what we are going to call     off-line data. 

  - At every time step we have to evaluate  [2.x.6654]  and      [2.x.6655] , which will     constitute the bulk of the computational cost. 

Consider the following pseudo-code, illustrating a possible straight forward strategy for computing the solution  [2.x.6656]  at a new time  [2.x.6657]  given a known state  [2.x.6658]  at time  [2.x.6659] : 

[1.x.2593] 



We note here that: 

- This "assembly" does not require any form of quadrature or cell-loops. 

- Here  [2.x.6660]  and  [2.x.6661]  are a global matrix and a global vector containing all the vectors  [2.x.6662]  and all the states  [2.x.6663]  respectively. 

-  [2.x.6664] ,  [2.x.6665] , and  [2.x.6666]  are hypothetical implementations that either collect (from) or write (into) global matrices and vectors. 

- If we assume a Cartesian mesh in two space dimensions, first-order polynomial space  [2.x.6667] , and that  [2.x.6668]  is an interior node (i.e.  [2.x.6669]  is not on the boundary of the domain) then:  [2.x.6670]  should contain nine state vector elements (i.e. all the states in the patch/macro element associated to the shape function  [2.x.6671] ). This is one of the major differences with the usual cell-based loop where the gather functionality (encoded in FEValuesBase<dim, spacedim>.get_function_values() in the case of deal.II) only collects values for the local cell (just a subset of the patch). 

The actual implementation will deviate from above code in one key aspect: the time-step size  [2.x.6672]  has to be chosen subject to a CFL condition 

[1.x.2594] 

where  [2.x.6673]  is a chosen constant. This will require to compute all  [2.x.6674]  in a separate step prior to actually performing above update. The core principle remains unchanged, though: we do not loop over cells but rather over all edges of the sparsity graph. 

 [2.x.6675]  It is not uncommon to encounter such fully-algebraic schemes (i.e. no bilinear forms, no cell loops, and no quadrature) outside of the finite element community in the wider CFD community. There is a rich history of application of this kind of schemes, also called [1.x.2595] or [1.x.2596] finite element schemes (see for instance  [2.x.6676]  for a historical overview). However, it is important to highlight that the algebraic structure of the scheme (presented in this tutorial) and the node-loops are not just a performance gimmick. Actually, the structure of this scheme was born out of theoretical necessity: the proof of pointwise stability of the scheme hinges on the specific algebraic structure of the scheme. In addition, it is not possible to compute the algebraic viscosities  [2.x.6677]  using cell-loops since they depend nonlinearly on information that spans more than one cell (superposition does not hold: adding contributions from separate cells does not lead to the right result). 

[1.x.2597] 

In the example considered in this tutorial step we use three different types of boundary conditions: essential-like boundary conditions (we prescribe a state at the left boundary of our domain), outflow boundary conditions (also called "do-nothing" boundary conditions) at the right boundary of the domain, and "reflecting" boundary conditions  [2.x.6678]  (also called "slip" boundary conditions) at the top, bottom, and surface of the obstacle. We will not discuss much about essential and "do-nothing" boundary conditions since their implementation is relatively easy and the reader will be able to pick-up the implementation directly from the (documented) source code. In this portion of the introduction we will focus only on the "reflecting" boundary conditions which are somewhat more tricky. 

 [2.x.6679]  At the time of this writing (early 2020) it is not unreasonable to say that both analysis and implementation of stable boundary conditions for hyperbolic systems of conservation laws is an open issue. For the case of variational formulations, stable boundary conditions are those leading to a well-posed (coercive) bilinear form. But for general hyperbolic systems of conservation laws (and for the algebraic formulation used in this tutorial) coercivity has no applicability and/or meaning as a notion of stability. In this tutorial step we will use preservation of the invariant set as our main notion of stability which (at the very least) guarantees well-posedness of the discrete problem. 

For the case of the reflecting boundary conditions we will proceed as follows: 

- For every time step advance in time satisfying no boundary condition at all. 

- Let  [2.x.6680]  be the portion of the boundary where we want to   enforce reflecting boundary conditions. At the end of the time step we enforce   reflecting boundary conditions strongly in a post-processing step where we   execute the projection     [1.x.2598] 

  that removes the normal component of  [2.x.6681] . This is a somewhat   naive idea that preserves a few fundamental properties of the PDE as we   explain below. 

This is approach is usually called "explicit treatment of boundary conditions". The well seasoned finite element person might find this approach questionable. No doubt, when solving parabolic, or elliptic equations, we typically enforce essential (Dirichlet-like) boundary conditions by making them part of the approximation space  [2.x.6682] , and treat natural (e.g. Neumann) boundary conditions as part of the variational formulation. We also know that explicit treatment of boundary conditions (in the context of parabolic PDEs) almost surely leads to catastrophic consequences. However, in the context of nonlinear hyperbolic equations we have that: 

- It is relatively easy to prove that (for the case of reflecting boundary conditions) explicit treatment of boundary conditions is not only conservative but also guarantees preservation of the property  [2.x.6683]  for all  [2.x.6684]  (well-posedness). This is perhaps the most important reason to use explicit enforcement of boundary conditions. 

- To the best of our knowledge: we are not aware of any mathematical result proving that it is possible to guarantee the property  [2.x.6685]  for all  [2.x.6686]  when using either direct enforcement of boundary conditions into the approximation space, or weak enforcement using the Nitsche penalty method (which is for example widely used in discontinuous Galerkin schemes). In addition, some of these traditional ideas lead to quite restrictive time step constraints. 

- There is enough numerical evidence suggesting that explicit treatment of Dirichlet-like boundary conditions is stable under CFL conditions and does not introduce any loss in accuracy. 

If  [2.x.6687]  represents Euler's equation with reflecting boundary conditions on the entirety of the boundary (i.e.  [2.x.6688] ) and we integrate in space and time  [2.x.6689]  we would obtain 

[1.x.2599] 

Note that momentum is NOT a conserved quantity (interaction with walls leads to momentum gain/loss): however  [2.x.6690]  has to satisfy a momentum balance. Even though we will not use reflecting boundary conditions in the entirety of the domain, we would like to know that our implementation of reflecting boundary conditions is consistent with the conservation properties mentioned above. In particular, if we use the projection  [2.x.6691]  in the entirety of the domain the following discrete mass-balance can be guaranteed: 

[1.x.2600] 

where  [2.x.6692]  is the pressure at the nodes that lie at the boundary. Clearly  [2.x.6693]  is the discrete counterpart of  [2.x.6694] . The proof of identity  [2.x.6695]  is omitted, but we briefly mention that it hinges on the definition of the [1.x.2601]  [2.x.6696]  provided in  [2.x.6697] . We also note that this enforcement of reflecting boundary conditions is different from the one originally advanced in  [2.x.6698] . 


examples/step-69/doc/results.dox 

[1.x.2602] 

[1.x.2603] 

Running the program with default parameters in release mode takes about 1 minute on a 4 core machine (with hyperthreading): 

[1.x.2604] 



One thing that becomes evident is the fact that the program spends two thirds of the execution time computing the graph viscosity d_ij and about a third of the execution time in performing the update, where computing the flux  [2.x.6699]  is the expensive operation. The preset default resolution is about 37k gridpoints, which amounts to about 148k spatial degrees of freedom in 2D. An animated schlieren plot of the solution looks as follows: 

 [2.x.6700]  

It is evident that 37k gridpoints for the first-order method is nowhere near the resolution needed to resolve any flow features. For comparison, here is a "reference" computation with a second-order method and about 9.5M gridpoints ([1.x.2605]): 

 [2.x.6701]  

So, we give the first-order method a second chance and run it with about 2.4M gridpoints on a small compute server: 

[1.x.2606] 



And with the following result: 

 [2.x.6702]  

That's substantially better, although of course at the price of having run the code for roughly 2 hours on 16 cores. 




[1.x.2607] 

[1.x.2608] 

The program showcased here is really only first-order accurate, as discussed above. The pictures above illustrate how much diffusion that introduces and how far the solution is from one that actually resolves the features we care about. 

This can be fixed, but it would exceed what a *tutorial* is about. Nevertheless, it is worth showing what one can achieve by adding a second-order scheme. For example, here is a video computed with [1.x.2609] that shows (with a different color scheme) a 2d simulation that corresponds to the cases shown above: 

[1.x.2610] 



This simulation was done with 38 million degrees of freedom (continuous  [2.x.6703]  finite elements) per component of the solution vector. The exquisite detail of the solution is remarkable for these kinds of simulations, including in the sub-sonic region behind the obstacle. 

One can also with relative ease further extend this to the 3d case: 

[1.x.2611] 



Solving this becomes expensive, however: The simulation was done with 1,817 million degrees of freedom (continuous  [2.x.6704]  finite elements) per component (for a total of 9.09 billion spatial degrees of freedom) and ran on 30,720 MPI ranks. The code achieved an average througput of 969M grid points per second (0.04M gridpoints per second per CPU). The front and back wall show a "Schlieren plot": the magnitude of the gradient of the density on an exponential scale from white (low) to black (high). All other cutplanes and the surface of the obstacle show the magnitude of the vorticity on a white (low) - yellow (medium) - red (high) scale. (The scales of the individual cutplanes have been adjusted for a nicer visualization.) 


examples/step-7/doc/intro.dox 

[1.x.2612] 

[1.x.2613] 

In this program, we will mainly consider two aspects: <ol>    [2.x.6705]  Verification of correctness of the program and generation of convergence   tables;    [2.x.6706]  Non-homogeneous Neumann boundary conditions for the Helmholtz equation.  [2.x.6707]  Besides these topics, again a variety of improvements and tricks will be shown. 




[1.x.2614] 

There has probably never been a non-trivial finite element program that worked right from the start. It is therefore necessary to find ways to verify whether a computed solution is correct or not. Usually, this is done by choosing the set-up of a simulation in such a way that we know the exact continuous solution and evaluate the difference between continuous and computed discrete solution. If this difference converges to zero with the right order of convergence, this is already a good indication of correctness, although there may be other sources of error persisting which have only a small contribution to the total error or are of higher order. In the context of finite element simulations, this technique of picking the solution by choosing appropriate right hand sides and boundary conditions is often called the [1.x.2615]. 

In this example, we will not go into the theories of systematic software verification which is a very complicated problem. Rather we will demonstrate the tools which deal.II can offer in this respect. This is basically centered around the functionality of a single function,  [2.x.6708]  This function computes the difference between a given continuous function and a finite element field in various norms on each cell. Of course, like with any other integral, we can only evaluate these norms using quadrature formulas; the choice of the right quadrature formula is therefore crucial to the accurate evaluation of the error. This holds in particular for the  [2.x.6709]  norm, where we evaluate the maximal deviation of numerical and exact solution only at the quadrature points; one should then not try to use a quadrature rule whose evaluation occurs only at points where [super-convergence](https://en.wikipedia.org/wiki/Superconvergence) might occur, such as the Gauss points of the lowest-order Gauss quadrature formula for which the integrals in the assembly of the matrix is correct (e.g., for linear elements, do not use the QGauss(2) quadrature formula). In fact, this is generally good advice also for the other norms: if your quadrature points are fortuitously chosen at locations where the error happens to be particularly small due to superconvergence, the computed error will look like it is much smaller than it really is and may even suggest a higher convergence order. Consequently, we will choose a different quadrature formula for the integration of these error norms than for the assembly of the linear system. 

The function  [2.x.6710]  evaluates the desired norm on each cell  [2.x.6711]  of the triangulation and returns a vector which holds these values for each cell. From the local values, we can then obtain the global error. For example, if the vector  [2.x.6712]  with element  [2.x.6713]  for all cells  [2.x.6714]  contains the local  [2.x.6715]  norms  [2.x.6716] , then 

[1.x.2616] 

is the global  [2.x.6717]  error  [2.x.6718] . 

In the program, we will show how to evaluate and use these quantities, and we will monitor their values under mesh refinement. Of course, we have to choose the problem at hand such that we can explicitly state the solution and its derivatives, but since we want to evaluate the correctness of the program, this is only reasonable. If we know that the program produces the correct solution for one (or, if one wants to be really sure: many) specifically chosen right hand sides, we can be rather confident that it will also compute the correct solution for problems where we don't know the exact values. 

In addition to simply computing these quantities, we will show how to generate nicely formatted tables from the data generated by this program that automatically computes convergence rates etc. In addition, we will compare different strategies for mesh refinement. 




[1.x.2617] 

The second, totally unrelated, subject of this example program is the use of non-homogeneous boundary conditions. These are included into the variational form using boundary integrals which we have to evaluate numerically when assembling the right hand side vector. 

Before we go into programming, let's have a brief look at the mathematical formulation. The equation that we want to solve here is the Helmholtz equation "with the nice sign": 

[1.x.2618] 

on the square  [2.x.6719]  with  [2.x.6720] , augmented by Dirichlet boundary conditions 

[1.x.2619] 

on some part  [2.x.6721]  of the boundary  [2.x.6722] , and Neumann conditions 

[1.x.2620] 

on the rest  [2.x.6723] . In our particular testcase, we will use  [2.x.6724] . (We say that this equation has the "nice sign" because the operator  [2.x.6725]  with the identity  [2.x.6726]  and  [2.x.6727]  is a positive definite operator; the [1.x.2621] is  [2.x.6728]  and results from modeling time-harmonic processes. The operator is not positive definite if  [2.x.6729]  is large, and this leads to all sorts of issues we need not discuss here. The operator may also not be invertible -- i.e., the equation does not have a unique solution -- if  [2.x.6730]  happens to be one of the eigenvalues of  [2.x.6731] .) 

Because we want to verify the convergence of our numerical solution  [2.x.6732] , we want a setup so that we know the exact solution  [2.x.6733] . This is where the Method of Manufactured Solutions comes in. To this end, let us choose a function 

[1.x.2622] 

where the centers  [2.x.6734]  of the exponentials are    [2.x.6735] ,    [2.x.6736] , and    [2.x.6737] , and the half width is set to  [2.x.6738] . The method of manufactured solution then says: choose 

[1.x.2623] 

With this particular choice, we infer that of course the solution of the original problem happens to be  [2.x.6739] . In other words, by choosing the right hand sides of the equation and the boundary conditions in a particular way, we have manufactured ourselves a problem to which we know the solution. This allows us then to compute the error of our numerical solution. In the code below, we represent  [2.x.6740]  by the  [2.x.6741]  class, and other classes will be used to denote  [2.x.6742]  and  [2.x.6743] . 

Using the above definitions, we can state the weak formulation of the equation, which reads: find  [2.x.6744]  such that 

[1.x.2624] 

for all test functions  [2.x.6745] . The boundary term  [2.x.6746]  has appeared by integration by parts and using  [2.x.6747]  on  [2.x.6748]  and  [2.x.6749]  on  [2.x.6750] . The cell matrices and vectors which we use to build the global matrices and right hand side vectors in the discrete formulation therefore look like this: 

[1.x.2625] 

Since the generation of the domain integrals has been shown in previous examples several times, only the generation of the contour integral is of interest here. It basically works along the following lines: for domain integrals we have the  [2.x.6751]  class that provides values and gradients of the shape values, as well as Jacobian determinants and other information and specified quadrature points in the cell; likewise, there is a class  [2.x.6752]  that performs these tasks for integrations on faces of cells. One provides it with a quadrature formula for a manifold with dimension one less than the dimension of the domain is, and the cell and the number of its face on which we want to perform the integration. The class will then compute the values, gradients, normal vectors, weights, etc. at the quadrature points on this face, which we can then use in the same way as for the domain integrals. The details of how this is done are shown in the following program. 




[1.x.2626] 

Besides the mathematical topics outlined above, we also want to use this program to illustrate one aspect of good programming practice, namely the use of namespaces. In programming the deal.II library, we have take great care not to use names for classes and global functions that are overly generic, say  [2.x.6753]  etc. Furthermore, we have put everything into namespace  [2.x.6754] . But when one writes application programs that aren't meant for others to use, one doesn't always pay this much attention. If you follow the programming style of step-1 through step-6, these functions then end up in the global namespace where, unfortunately, a lot of other stuff also lives (basically everything the C language provides, along with everything you get from the operating system through header files). To make things a bit worse, the designers of the C language were also not always careful in avoiding generic names; for example, the symbols <code>j1, jn</code> are defined in C header files (they denote Bessel functions). 

To avoid the problems that result if names of different functions or variables collide (often with confusing error messages), it is good practice to put everything you do into a [1.x.2627]. Following this style, we will open a namespace  [2.x.6755]  at the top of the program, import the deal.II namespace into it, put everything that's specific to this program (with the exception of  [2.x.6756] , which must be in the global namespace) into it, and only close it at the bottom of the file. In other words, the structure of the program is of the kind 

[1.x.2628] 

We will follow this scheme throughout the remainder of the deal.II tutorial. 


examples/step-7/doc/results.dox 



[1.x.2629] 


The program generates two kinds of output. The first are the output files  [2.x.6757] ,  [2.x.6758] , and  [2.x.6759] . We show the latter in a 3d view here: 


 [2.x.6760]  





Secondly, the program writes tables not only to disk, but also to the screen while running. The output looks like the following (recall that columns labeled as " [2.x.6761] " actually show the  [2.x.6762]  [1.x.2630]norm of the error, not the full  [2.x.6763]  norm): 




[1.x.2631] 




One can see the error reduction upon grid refinement, and for the cases where global refinement was performed, also the convergence rates can be seen. The linear and quadratic convergence rates of Q1 and Q2 elements in the  [2.x.6764]  semi-norm can clearly be seen, as are the quadratic and cubic rates in the  [2.x.6765]  norm. 





Finally, the program also generated LaTeX versions of the tables (not shown here) that is written into a file in a way so that it could be copy-pasted into a LaTeX document. 




[1.x.2632] 

What we showed above is how to determine the size of the error  [2.x.6766]  in a number of different norms. We did this primarily because we were interested in testing that our solutions *converge*. But from an engineering perspective, the question is often more practical: How fine do I have to make my mesh so that the error is "small enough"? In other words, if in the table above the  [2.x.6767]  semi-norm has been reduced to `4.121e-03`, is this good enough for me to sign the blueprint and declare that our numerical simulation showed that the bridge is strong enough? 

In practice, we are rarely in this situation because I can not typically compare the numerical solution  [2.x.6768]  against the exact solution  [2.x.6769]  in situations that matter -- if I knew  [2.x.6770] , I would not have to compute  [2.x.6771] . But even if I could, the question to ask in general is then: `4.121e-03` *what*? The solution will have physical units, say kg-times-meter-squared, and I'm integrating a function with units square of the above over the domain, and then take the square root. So if the domain is two-dimensional, the units of  [2.x.6772]  are kg-times-meter-cubed. The question is then: Is  [2.x.6773]  kg-times-meter-cubed small? That depends on what you're trying to simulate: If you're an astronomer used to masses measured in solar masses and distances in light years, then yes, this is a fantastically small number. But if you're doing atomic physics, then no: That's not small, and your error is most certainly not sufficiently small; you need a finer mesh. 

In other words, when we look at these sorts of numbers, we generally need to compare against a "scale". One way to do that is to not look at the *absolute* error  [2.x.6774]  in whatever norm, but at the *relative* error  [2.x.6775] . If this ratio is  [2.x.6776] , then you know that *on average*, the difference between  [2.x.6777]  and  [2.x.6778]  is 0.001 per cent -- probably small enough for engineering purposes. 

How do we compute  [2.x.6779] ? We just need to do an integration loop over all cells, quadrature points on these cells, and then sum things up and take the square root at the end. But there is a simpler way often used: You can call 

[1.x.2633] 

which computes  [2.x.6780] . Alternatively, if you're particularly lazy and don't feel like creating the `zero_vector`, you could use that if the mesh is not too coarse, then  [2.x.6781] , and we can compute  [2.x.6782]  by calling 

[1.x.2634] 

In both cases, one then only has to combine the vector of cellwise norms into one global norm as we already do in the program, by calling 

[1.x.2635] 






[1.x.2636] 

[1.x.2637] 

Go ahead and run the program with higher order elements ( [2.x.6783] ,  [2.x.6784] , ...). You will notice that assertions in several parts of the code will trigger (for example in the generation of the filename for the data output). You might have to address these, but it should not be very hard to get the program to work! 

[1.x.2638] 

Is Q1 or Q2 better? What about adaptive versus global refinement? A (somewhat unfair but typical) metric to compare them, is to look at the error as a function of the number of unknowns. 

To see this, create a plot in log-log style with the number of unknowns on the  [2.x.6785]  axis and the  [2.x.6786]  error on the  [2.x.6787]  axis. You can add reference lines for  [2.x.6788]  and  [2.x.6789]  and check that global and adaptive refinement follow those. If one makes the (not completely unreasonable) assumption that with a good linear solver, the computational effort is proportional to the number of unknowns  [2.x.6790] , then it is clear that an error reduction of  [2.x.6791]  is substantially better than a reduction of the form  [2.x.6792] : That is, that adaptive refinement gives us the desired error level with less computational work than if we used global refinement. This is not a particularly surprising conclusion, but it's worth checking these sorts of assumptions in practice. 

Of course, a fairer comparison would be to plot runtime (switch to release mode first!) instead of number of unknowns on the  [2.x.6793]  axis. If you plotted run time against the number of unknowns by timing each refinement step (e.g., using the Timer class), you will notice that the linear solver is not perfect -- its run time grows faster than proportional to the linear system size -- and picking a better linear solver might be appropriate for this kind of comparison. 


examples/step-70/doc/intro.dox 

 [2.x.6794]  

[1.x.2639] 

 [2.x.6795]  




[1.x.2640] 

[1.x.2641] 

In this tutorial we consider a mixing problem in the laminar flow regime. Such problems occur in a wide range of applications ranging from chemical engineering to power generation (e.g. turbomachinery). Mixing problems are particularly hard to solve numerically, because they often involve a container (with fixed boundaries, and possibly complex geometries such as baffles), represented by the domain  [2.x.6796] , and one (or more) immersed and rotating impellers (represented by the domain  [2.x.6797] ). The domain in which we would like to solve the flow equations is the (time dependent) difference between the two domains, namely:  [2.x.6798] . 

For rotating impellers, the use of Arbitrary Lagrangian Eulerian formulations (in which the fluid domain -- along with the mesh! -- is smoothly deformed to follow the deformations of the immersed solid) is not possible, unless only small times (i.e., small fluid domain deformations) are considered. If one wants to track the evolution of the flow across multiple rotations of the impellers, the resulting deformed grid would simply be too distorted to be useful. 

In this case, a viable alternative strategy would be to use non-matching methods (similarly to what we have done in step-60), where a background fixed grid (that may or may not be locally refined in time to better capture the solid motion) is coupled with a rotating, independent, grid. 

In order to maintain the same notations used in step-60, we use  [2.x.6799]  to denote the domain in  [2.x.6800]  representing the container of both the fluid and the impeller, and we use  [2.x.6801]  in  [2.x.6802]  to denote either the full impeller (when its `spacedim` measure is non-negligible, i.e., when we can represent it as a grid of dimension `dim` equal to `spacedim`), a co-dimension one representation of a thin impeller, or just the boundary of the full impeller. 

The domain  [2.x.6803]  is embedded in  [2.x.6804]  ( [2.x.6805] ) and it is non-matching: It does not, in general, align with any of the features of the volume mesh. We solve a partial differential equation on  [2.x.6806] , enforcing some conditions on the solution of the problem on the embedded domain  [2.x.6807]  by some penalization techniques. In the current case, the condition is that the velocity of the fluid at points on  [2.x.6808]  equal the velocity of the solid impeller at that point. 

The technique we describe here is presented in the literature using one of many names: the [1.x.2642] and the [1.x.2643] among others.  The main principle is that the discretization of the two grids are kept completely independent. In the present tutorial, this approach is used to solve for the motion of a viscous fluid, described by the Stokes equation, that is agitated by a rigid non-deformable impeller. 

Thus, the equations solved in  [2.x.6809]  are the Stokes equations for a creeping flow (i.e. a flow where  [2.x.6810] ) and a no-slip boundary condition is applied on the moving *embedded domain*  [2.x.6811]  associated with the impeller. However, this tutorial could be readily extended to other equations (e.g. the Navier-Stokes equations, linear elasticity equation, etc.). It can be seen as a natural extension of step-60 that enables the solution of large problems using a distributed parallel computing architecture via MPI. 

However, contrary to step-60, the Dirichlet boundary conditions on  [2.x.6812]  are imposed weakly instead of through the use of Lagrange multipliers, and we concentrate on dealing with the coupling of two fully distributed triangulations (a combination that was not possible in the implementation of step-60). 

There are two interesting scenarios that occur when one wants to enforce conditions on the embedded domain  [2.x.6813] : 

- The geometrical dimension `dim` of the embedded domain  [2.x.6814]  is the same of the domain  [2.x.6815]  (`spacedim`), that is, the spacedim-dimensional measure of  [2.x.6816]  is not zero. In this case, the imposition of the Dirichlet boundary boundary condition on  [2.x.6817]  is done through a volumetric penalization. If the applied penalization only depends on the velocity, this is often referred to as  [2.x.6818]  penalization whereas if the penalization depends on both the velocity and its gradient, it is an  [2.x.6819]  penalization. The case of the  [2.x.6820]  penalization is very similar to a Darcy-type approach. Both  [2.x.6821]  and  [2.x.6822]  penalizations have been analyzed extensively (see, for example,  [2.x.6823] ). 

- The embedded domain  [2.x.6824]  has an intrinsic dimension `dim` which is smaller than that of  [2.x.6825]  (`spacedim`), thus its spacedim-dimensional measure is zero; for example it is a curve embedded in a two dimensional domain, or a surface embedded in a three-dimensional domain. This is of course physically impossible, but one may consider very thin sheets of metal moving in a fluid as essentially lower-dimensional if the thickness of the sheet is negligible. In this case, the boundary condition is imposed weakly on  [2.x.6826]  by applying the [1.x.2644] method (see  [2.x.6827] ). 

Both approaches have very similar requirements and result in highly similar formulations. Thus, we treat them almost in the same way. 

In this tutorial program we are not interested in further details on  [2.x.6828] : we assume that the dimension of the embedded domain (`dim`) is always smaller by one or equal with respect to the dimension of the embedding domain  [2.x.6829]  (`spacedim`). 

We are going to solve the following differential problem: given a sufficiently regular function  [2.x.6830]  on  [2.x.6831] , find the solution  [2.x.6832]  to 

[1.x.2645] 



This equation, which we have normalized by scaling the time units in such a way that the viscosity has a numerical value of 1, describes slow, viscous flow such as honey or lava. The main goal of this tutorial is to show how to impose the velocity field condition  [2.x.6833]  on a non-matching  [2.x.6834]  in a weak way, using a penalization method. A more extensive discussion of the Stokes problem including body forces, different boundary conditions, and solution strategies can be found in step-22. 

Let us start by considering the Stokes problem alone, in the entire domain  [2.x.6835] . We look for a velocity field  [2.x.6836]  and a pressure field  [2.x.6837]  that satisfy the Stokes equations with homogeneous boundary conditions on  [2.x.6838] . 

The weak form of the Stokes equations is obtained by first writing it in vector form as 

[1.x.2646] 

forming the dot product from the left with a vector-valued test function  [2.x.6839] , and integrating over the domain  [2.x.6840] , yielding the following set of equations: 

[1.x.2647] 

which has to hold for all test functions  [2.x.6841] . 


Integrating by parts and exploiting the boundary conditions on  [2.x.6842] , we obtain the following variational problem: 

[1.x.2648] 



where  [2.x.6843]  represents the  [2.x.6844]  scalar product. This is the same variational form used in step-22. 

This variational formulation does not take into account the embedded domain. Contrary to step-60, we do not enforce strongly the constraints of  [2.x.6845]  on  [2.x.6846] , but enforce them weakly via a penalization term. 

The analysis of this weak imposition of the boundary condition depends on the spacedim-dimensional measure of  [2.x.6847]  as either positive (if `dim` is equal to `spacedim`) or zero (if `dim` is smaller than `spacedim`). We discuss both scenarios. 




[1.x.2649] 

In this case, we assume that  [2.x.6848]  is the boundary of the actual impeller, that is, a closed curve embedded in a two-dimensional domain or a closed surface in a three-dimensional domain. The idea of this method starts by considering a weak imposition of the Dirichlet boundary condition on  [2.x.6849] , following the Nitsche method. This is achieved by using the following modified formulation on the fluid domain, where no strong conditions on the test functions on  [2.x.6850]  are imposed: 

[1.x.2650] 



The integrals over  [2.x.6851]  are lower-dimensional integrals. It can be shown (see  [2.x.6852] ) that there exists a positive constant  [2.x.6853]  so that if  [2.x.6854] , the weak imposition of the boundary will be consistent and stable. The first two additional integrals on  [2.x.6855]  (the second line in the equation above) appear naturally after integrating by parts, when one does not assume that  [2.x.6856]  is zero on  [2.x.6857] . 

The third line in the equation above contains two terms that are added to ensure consistency of the weak form, and a stabilization term, that is there to enforce the boundary condition with an error which is consistent with the approximation error. The consistency terms and the stabilization term are added to the right hand side with the actual boundary data  [2.x.6858] . 

When  [2.x.6859]  satisfies the condition  [2.x.6860]  on  [2.x.6861] , all the consistency and stability integrals on  [2.x.6862]  cancel out, and one is left with the usual weak form of Stokes flow, that is, the above formulation is consistent. 

We note that an alternative (non-symmetric) formulation can be used : 

[1.x.2651] 

Note the different sign of the first terms on the third and fourth lines. In this case, the stability and consistency conditions become  [2.x.6863] . In the symmetric case, the value of  [2.x.6864]  is dependent on  [2.x.6865] , and it is in general chosen such that  [2.x.6866]  with  [2.x.6867]  a measure of size of the face being integrated and  [2.x.6868]  a constant such that  [2.x.6869] . This is as one usually does with the Nitsche penalty method to enforcing Dirichlet boundary conditions. 

The non-symmetric approach, on the other hand, is related to how one enforced continuity for the non-symmetric interior penalty method for discontinuous Galerkin methods (the "NIPG" method  [2.x.6870] ). Even if the non-symmetric case seems advantageous w.r.t. possible choices of stabilization parameters, we opt for the symmetric discretization, since in this case it can be shown that the dual problem is also consistent, leading to a solution where not only the energy norm of the solution converges with the correct order, but also its  [2.x.6871]  norm. Furthermore, the resulting matrix remains symmetric. 

The above formulation works under the assumption that the domain is discretized exactly. However, if the deformation of the impeller is a rigid body motion, it is possible to artificially extend the solution of the Stokes problem inside the propeller itself, since a rigid body motion is also a solution to the Stokes problem. The idea is then to solve the same problem, inside  [2.x.6872] , imposing the same boundary conditions on  [2.x.6873] , using the same penalization technique, and testing with test functions  [2.x.6874]  which are globally continuous over  [2.x.6875] . 

This results in the following (intermediate) formulation: 

[1.x.2652] 

where the jump terms, denoted with  [2.x.6876] , are computed with respect to a fixed orientation of the normal vector  [2.x.6877] . The factor of 2 appears in front of  [2.x.6878]  since we see every part of  [2.x.6879]  twice, once from within the fluid and once from within the obstacle moving around in it. (For all of the other integrals over  [2.x.6880] , we visit each part of  [2.x.6881]  twice, but with opposite signs, and consequently get the jump terms.) 

Here we notice that, unlike in discontinuous Galerkin methods, the test and trial functions are continuous across  [2.x.6882] . Moreover, if  [2.x.6883]  is not aligned with cell boundaries, all the jump terms are also zero, since, in general, finite element function spaces are smooth inside each cell, and if  [2.x.6884]  cuts through an element intersecting its boundary only at a finite number of points, all the contributions on  [2.x.6885] , with the exception of the stabilization ones, can be neglected from the formulation, resulting in the following final form of the variational formulation: 

[1.x.2653] 



In step-60, the imposition of the constraint required the addition of new variables in the form of Lagrange multipliers. This is not the case for this tutorial program. The imposition of the boundary condition using Nitsche's method only modifies the system matrix and the right-hand side without adding additional unknowns. However, the velocity vector  [2.x.6886]  on the embedded domain will not match exactly the prescribed velocity  [2.x.6887] , but only up to a numerical error which is in the same order as the interpolation error of the finite element method. Furthermore, as in step-60, we still need to integrate over the non-matching embedded grid in order to construct the boundary term necessary to impose the boundary condition over  [2.x.6888] . 




[1.x.2654] 

In this case,  [2.x.6889]  has the same dimension, but is embedded into  [2.x.6890] . We can think of this as a thick object moving around in the fluid. In the case of  [2.x.6891]  penalization, the additional penalization term can be interpreted as a Darcy term within  [2.x.6892] , resulting in: 

[1.x.2655] 



Here, integrals over  [2.x.6893]  are simply integrals over a part of the volume. The  [2.x.6894]  penalization thus consists in adding a volumetric term that constrains the velocity of the fluid to adhere to the velocity of the rigid body within  [2.x.6895] . Also in this case,  [2.x.6896]  must be chosen sufficiently large in order to ensure that the Dirichlet boundary condition in  [2.x.6897]  is sufficiently respected, but not too high in order to maintain the proper conditioning of the system matrix. 

A  [2.x.6898]  penalization may be constructed in a similar manner, with the addition of a viscous component to the penalization that dampens the velocity gradient within  [2.x.6899] : 

[1.x.2656] 



Notice that the  [2.x.6900]  penalization (`dim` equal to `spacedim`) and the Nitsche penalization (`dim` equal to `spacedim-1`) result in the exact same numerical implementation, thanks to the dimension independent capabilities of deal.II. 




[1.x.2657] 

In this tutorial, both the embedded grid  [2.x.6901]  and the embedding grid are described using a  [2.x.6902]  These two triangulations can be built from functions in the GridGenerator namespace or by reading a mesh file produced with another application (e.g. GMSH, see the discussion in step-49). This is slightly more general than what was previously done in step-60. 

The addition of the immersed boundary method, whether it is in the `dim=spacedim` or `dim<spacedim` case, only introduces additional terms in the system matrix and the right-hand side of the system which result from the integration over  [2.x.6903] . This does not modify the number of variables for which the problem must be solved. The challenge is thus related to the integrals that must be carried over  [2.x.6904] . 

As usual in finite elements we split this integral into contributions from all cells of the triangulation used to discretize  [2.x.6905] , we transform the integral on  [2.x.6906]  to an integral on the reference element  [2.x.6907] , where  [2.x.6908]  is the mapping from  [2.x.6909]  to  [2.x.6910] , and compute the integral on  [2.x.6911]  using a quadrature formula. For example: 

[1.x.2658] 

Computing this sum is non-trivial because we have to evaluate  [2.x.6912] . In general, if  [2.x.6913]  and  [2.x.6914]  are not aligned, the point  [2.x.6915]  is completely arbitrary with respect to  [2.x.6916] , and unless we figure out a way to interpolate all basis functions of  [2.x.6917]  on an arbitrary point on  [2.x.6918] , we cannot compute the integral needed. 


To evaluate  [2.x.6919]  the following steps needs to be taken (as shown in the picture below): 

- For a given cell  [2.x.6920]  in  [2.x.6921]  compute the real point  [2.x.6922] , where  [2.x.6923]  is one of the quadrature points used for the integral on  [2.x.6924] . This is the easy part:  [2.x.6925]  gives us the real-space locations of all quadrature points. 

- Find the cell of  [2.x.6926]  in which  [2.x.6927]  lies. We shall call this element  [2.x.6928] . 

- Find the reference coordinates within  [2.x.6929]  of  [2.x.6930] . For this, we need the inverse of the mapping  [2.x.6931]  that transforms the reference element  [2.x.6932]  into the element  [2.x.6933] :  [2.x.6934] . 

- Evaluate the basis function  [2.x.6935]  of the  [2.x.6936]  mesh at this   point  [2.x.6937] . This is, again, relatively simple using FEValues. 


<p align="center"> <img   src="https://www.dealii.org/images/steps/developer/step-60.C_interpolation.png"   alt="">  [2.x.6938]  

In step-60, the second through fourth steps above were computed by calling, in turn, 

-  [2.x.6939]  followed by 

-  [2.x.6940]  We then 

- construct a custom Quadrature formula, containing the point in the reference  cell and then 

- construct an FEValues object, with the given quadrature formula, and  initialized with the cell obtained in the first step. 

Although this approach could work for the present case, it does not lends itself readily to parallel simulations using distributed triangulations. Indeed, since the position of the quadrature points on the cells of the embedded domain  [2.x.6941]  do not match that of the embedding triangulation and since  [2.x.6942]  is constantly moving, this would require that the triangulation representing  [2.x.6943]  be stored in it's entirety for all of the processors. As the number of processor and the number of cells in  [2.x.6944]  increases, this leads to a severe bottleneck in terms of memory. Consequently, an alternative strategy is sought in this step. 




[1.x.2659] 

Remember that for both the penalization approach ( [2.x.6945]  or  [2.x.6946] ) and the Nitsche method, we want to compute integrals that are approximated by the quadrature. That is, we need to compute 

[1.x.2660] If you followed the discussion above, then you will recall that  [2.x.6947]  and  [2.x.6948]  are shape functions defined on the fluid mesh. The only things defined on the solid mesh are:  [2.x.6949] , which is the location of a quadrature point on a solid cell that is part of  [2.x.6950] ,  [2.x.6951]  is the determinant of its Jacobian, and  [2.x.6952]  the corresponding quadrature weight. 

The important part to realize is now this:  [2.x.6953]  is a property of the quadrature formula and does not change with time. Furthermore, the Jacobian matrix of  [2.x.6954]  itself changes as the solid obstacle moves around in the fluid, but because the solid is considered non-deforming (it only translates and rotates, but doesn't dilate), the determinant of the Jacobian remains constant. As a consequence, the product  [2.x.6955]  (which we typically denote by `JxW`) remains constant for each quadrature point. So the only thing we need keep track of are the positions  [2.x.6956]  -- but these move with the velocity of the solid domain. 

In other words, we don't actually need to keep the solid mesh at all. All we need is the positions  [2.x.6957]  and corresponding `JxW` values. Since both of these properties are point-properties (or point-vectors) that are attached to the solid material, they can be idealized as a set of disconnected infinitesimally small "particles", which carry the required `JxW` information with the movement of the solid. deal.II has the ability to distribute and store such a set of particles in large-scale parallel computations in the form of the ParticleHandler class (for details on the implementation see  [2.x.6958] ), and we will make use of this functionality in this tutorial. 

Thus, the approach taken in this step is as follows: 

- Create a  [2.x.6959]  for the domain  [2.x.6960] ; 

- Create  [2.x.6961]  at the positions of the quadrature points on  [2.x.6962] ; 

- Call the  [2.x.6963]  function,   to distribute the particles across processors, *following the solid   triangulation*; 

- Attach the `JxW` values as a "property" to each  [2.x.6964]  object. 

This structure is relatively expensive to generate, but must only be generated once per simulation. Once the  [2.x.6965]  is generated and the required information is attached to the particle, the integrals over  [2.x.6966]  can be carried out by exploiting the fact that particles are grouped cellwise inside ParticleHandler, allowing us to: 

- Looping over all cells of  [2.x.6967]  that contain at least one particle 

- Looping over all particles in the given cell 

- Compute the integrals and fill the global matrix. 

Since the  [2.x.6968]  can manage the exchange of particles from one processor to the other, the embedded triangulation can be moved or deformed by displacing the particles. The only constraint associated with this displacement is that particles should be displaced by a distance that is no larger than the size of one cell. That's because that is the limit to which  [2.x.6969]  can track which cell a particle that leaves its current cell now resides in. 

Once the entire problem (the Stokes problem and the immersed boundary imposition) is assembled, the final saddle point problem is solved by an iterative solver, applied to the Schur complement  [2.x.6970]  (whose construction is described, for example, in step-22), and we construct  [2.x.6971]  using LinearOperator classes. 




[1.x.2661] 

The problem we solve here is a demonstration of the time-reversibility of Stokes flow. This is often illustrated in science education experiments with a Taylor-Couette flow and dye droplets that revert back to their original shape after the fluid has been displaced in a periodic manner. 

[1.x.2662] 



In the present problem, a very viscous fluid is agitated by the rotation of an impeller, which, in 2D, is modeled by a rectangular grid. The impeller rotates for a given number of revolutions, after which the flow is reversed such that the same number of revolutions is carried out in the opposite direction. We recall that since the Stokes equations are self-adjoint, creeping flows are reversible. Consequently, if the impeller motion is reversed in the opposite direction, the fluid should return to its original position. In the present case, this is illustrated by inserting a circle of passive tracer particles that are advected by the fluid and which return to their original position, thus demonstrating the time-reversibility of the flow. 




[1.x.2663] 

This tutorial program uses a number of techniques on imposing velocity conditions on non-matching interfaces in the interior of the fluid. For more background material, you may want to look up the following references:  [2.x.6972] ,  [2.x.6973] ,  [2.x.6974] ,  [2.x.6975] ,  [2.x.6976] . 


examples/step-70/doc/results.dox 



[1.x.2664] 

The directory in which this program is run contains a number of sample parameter files that you can use to reproduce the results presented in this section. If you do not specify a parameter file as an argument on the command line, the program will try to read the file "`parameters.prm`" by default, and will execute the two dimensional version of the code. As explained in the discussion of the source code, if your file name contains the string "23", then the program will run a three dimensional problem, with immersed solid of co-dimension one. If it contains the string "3", it will run a three dimensional problem, with immersed solid of co-dimension zero, otherwise it will run a two dimensional problem with immersed solid of co-dimension zero. 

Regardless of the specific parameter file name, if the specified file does not exist, when you execute the program you will get an exception that no such file can be found: 

[1.x.2665] 



However, as the error message already states, the code that triggers the exception will also generate the specified file ("`parameters.prm`" in this case) that simply contains the default values for all parameters this program cares about (for the correct dimension and co-dimension, according to the whether a string "23" or "3" is contained in the file name). By inspection of the default parameter file, we see the following: 

[1.x.2666] 



If you now run the program, you will get a file called `parameters_22.prm` in the directory specified by the parameter `Output directory` (which defaults to the current directory) containing a shorter version of the above parameters (without comments and documentation), documenting all parameters that were used to run your program: 

[1.x.2667] 



The rationale behind creating first `parameters.prm` file (the first time the program is run) and then a `output/parameters_22.prm` (every time you run the program with an existing input file), is because you may want to leave most parameters to their default values, and only modify a handful of them, while still beeing able to reproduce the results and inspect what parameters were used for a specific simulation. It is generally good scientific practice to store the parameter file you used for a simulation along with the simulation output so that you can repeat the exact same run at a later time if necessary. 

Another reason is because the input file may only contain those parameters that differ from their defaults. For example, you could use the following (perfectly valid) parameter file with this tutorial program: 

[1.x.2668] 

and you would run the program with Q3/Q2 Taylor-Hood finite elements, for 101 steps, using a Nitsche penalty of `10`, and leaving all the other parameters to their default value. The output directory then contains a record of not just these parameters, but indeed all parameters used in the simulation. You can inspect all the other parameters in the produced file `parameters_22.prm`. 




[1.x.2669] 

The default problem generates a co-dimension zero impeller, consisting of a rotating rectangular grid, where the rotation is for half a time unit in one direction, and half a time unit in the opposite direction, with constant angular velocity equal to  [2.x.6977] . Consequently, the impeller does half a rotation and returns to its original position. The following animation displays the velocity magnitude, the motion of the solid impeller and of the tracer particles. 


<p align="center">    <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-70.2d_tracing.gif"            alt = ""            width="500">     </div>  [2.x.6978]  

On one core, the output of the program will look like the following: 

[1.x.2670] 



You may notice that assembling the coupling system is more expensive than assembling the Stokes part. This depends highly on the number of Gauss points (solid particles) that are used to apply the Nitsche restriction. In the present case, a relatively low number of tracer particles are used. Consequently, tracking their motion is relatively cheap. 

The following movie shows the evolution of the solution over time: 

[1.x.2671] 



The movie shows the rotating obstacle in gray (actually a superposition of the solid particles plotted with large enough dots that they overlap), [1.x.2672] in light colors (including the corner vertices that form at specific times during the simulation), and the tracer particles in bluish tones. 

The simulation shows that at the end time, the tracer particles have somewhat returned to their original position, although they have been distorted by the flow field. The following image compares the initial and the final position of the particles after one time unit of flow. 

<p align="center">    <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-70.tracer_comparison.png"            alt = ""            width="500">     </div>  [2.x.6979]  

In this case, we see that the tracer particles that were outside of the swept volume of the impeller have returned very close to their initial position, whereas those in the swept volume were slightly more deformed. This deformation is non-physical. It is caused by the numerical error induced by the explicit Euler scheme used to advect the particles, by the loss of accuracy due to the fictitious domain and, finally, by the discretization error on the Stokes equations. The first two errors are the leading cause of this deformation and they could be alleviated by the use of a finer mesh and a lower time step. 




[1.x.2673] 

To play around a little bit, we complicate the fictitious domain (taken from https://grabcad.com/library/lungstors-blower-1), and run a co-dimension one simulation in three space dimensions, using the following "`parameters_23.prm`" file: 

[1.x.2674] 



In this case, the timing outputs are a bit different: 

[1.x.2675] 



Now, the solver is taking most of the solution time in three dimensions, and the particle motion and Nitsche assembly remain relatively unimportant as far as run time is concerned. 




[1.x.2676] 




[1.x.2677] 

[1.x.2678] 

The current tutorial program shows a one-way coupling between the fluid and the solid, where the solid motion is imposed (and not solved for), and read in the solid domain by exploiting the location and the weights of the solid quadrature points. 

The structure of the code already allows one to implement a two-way coupling, by exploiting the possibility to read values of the fluid velocity on the quadrature points of the solid grid. For this to be more efficient in terms of MPI communication patterns, one should maintain ownership of the quadrature points on the solid processor that owns the cells where they have been created. In the current code, it is sufficient to define the IndexSet of the vectors used to exchange information of the quadrature points by using the solid partition instead of the initial fluid partition. 

This allows the combination of the technique used in this tutorial program with those presented in the tutorial step-60 to solve a fluid structure interaction problem with distributed Lagrange multipliers, on  [2.x.6980]  objects. 

The timings above show that the current preconditioning strategy does not work well for Nitsche penalization, and we should come up with a better preconditioner if we want to aim at larger problems. Moreover, a checkpoint restart strategy should be implemented to allow for longer simulations to be interrupted and restored, as it is done for example in the step-69 tutorial. 


examples/step-71/doc/intro.dox 

 [2.x.6981]  

[1.x.2679] 




[1.x.2680] 

The aim of this tutorial is, quite simply, to introduce the fundamentals of both [automatic](https://en.wikipedia.org/wiki/Automatic_differentiation) and [symbolic differentiation](https://en.wikipedia.org/wiki/Computer_algebra) (respectively abbreviated as AD and SD): Ways in which one can, in source code, describe a function  [2.x.6982]  and automatically also obtain a representation of derivatives  [2.x.6983]  (the "Jacobian"),  [2.x.6984]  (the "Hessian"), etc., without having to write additional lines of code. Doing this is quite helpful in solving nonlinear or optimization problems where one would like to only describe the nonlinear equation or the objective function in the code, without having to also provide their derivatives (which are necessary for a Newton method for solving a nonlinear problem, or for finding a minimizer). 

Since AD and SD tools are somewhat independent of finite elements and boundary value problems, this tutorial is going to be different to the others that you may have read beforehand. It will focus specifically on how these frameworks work and the principles and thinking behind them, and will forgo looking at them in the direct context of a finite element simulation. 

We will, in fact, look at two different sets of problems that have greatly different levels of complexity, but when framed properly hold sufficient similarity that the same AD and SD frameworks can be leveraged. With these examples the aim is to build up an understanding of the steps that are required to use the AD and SD tools, the differences between them, and hopefully identify where they could be immediately be used in order to improve or simplify existing code. 

It's plausible that you're wondering what AD and SD are, in the first place. Well, that question is easy to answer but without context is not very insightful. So we're not going to cover that in this introduction, but will rather defer this until the first introductory example where we lay out the key points as this example unfolds. To complement this, we should mention that the core theory for both frameworks is extensively discussed in the  [2.x.6985]  module, so it bears little repeating here. 

Since we have to pick *some* sufficiently interesting topic to investigate and identify where AD and SD can be used effectively, the main problem that's implemented in the second half of the tutorial is one of modeling a coupled constitutive law, specifically a magneto-active material (with hysteretic effects). As a means of an introduction to that, later in the introduction some grounding theory for that class of materials will be presented. Naturally, this is not a field (or even a class of materials) that is of interest to a wide audience. Therefore, the author wishes to express up front that this theory and any subsequent derivations mustn't be considered the focus of this tutorial. Instead, keep in mind the complexity of the problem that arises from the relatively innocuous description of the constitutive law, and what we might (in the context of a boundary value problem) need to derive from that. We will perform some computations with these constitutive laws at the level of a representative continuum point (so, remaining in the  realm of continuum mechanics), and will produce some benchmark results around which we can frame a final discussion on the topic of computational performance. 

Once we have the foundation upon which we can build further concepts, we will see how AD in particular can be exploited at a finite element (rather than continuum) level: this is a topic that is covered in step-72, as well as step-33. But before then, let's take a moment to think about why we might want to consider using these sorts of tools, and what benefits they can potentially offer you. 




[1.x.2681] 

The primary driver for using AD or SD is typically that there is some situation that requires differentiation to be performed, and that doing so is sufficiently challenging to make the prospect of using an external tool to perform that specific task appealing. A broad categorization for the circumstances under which AD or SD can be rendered most useful include (but are probably not limited to) the following: 

- [1.x.2682] For a new class of problems where you're trying to   implement a solution quickly, and want to remove some of the intricate details   (in terms of both the mathematics as well as the organizational structure of   the code itself). You might be willing to justify any additional computational   cost, which would be offset by an increased agility in restructuring your code   or modifying the part of the problem that is introducing some complex nonlinearity   with minimal effort. 

- [1.x.2683] It could very well be that some problems just happen to have   a nonlinearity that is incredibly challenging to linearize or formulate by hand.   Having this challenge taken care of for you by a tool that is, for the most part,   robust, reliable, and accurate may alleviate some of the pains in implementing   certain problems. Examples of this include step-15, where the   derivative of the nonlinear PDE we solve is not incredibly difficult   to derive, but sufficiently cumbersome that one has to pay attention   in doing so by hand, and where implementing the corresponding finite   element formulation of the Newton step takes more than just the few   lines that it generally takes to implement the bilinear form;   step-33 (where we actually use AD) is an even more extreme example. 

- [1.x.2684] For materials and simulations that exhibit nonlinear response,   an accurate rather than only approximate material tangent (the term mechanical engineers use for   the derivative of a material law) can be the difference between convergent and   divergent behavior, especially at high external (or coupling) loads.   As the complexity of the problem increases, so do the opportunities to introduce   subtle (or, perhaps, not-so-subtle) errors that produce predictably negative   results.   Additionally, there is a lot to be gained by verifying that the implementation is   completely correct. For example, certain categories of problems are known to exhibit   instabilities, and therefore when you start to lose quadratic convergence in a   nonlinear solver (e.g., Newton's method) then this may not be a huge surprise to   the investigator. However, it is hard (if not impossible) to distinguish between   convergence behavior that is produced as you near an unstable solution and when   you simply have an error in the material or finite element linearization, and   start to drift off the optimal convergence path due to that. Having a   method of verifying the correctness of the implementation of a constitutive law   linearization, for example, is perhaps the only meaningful way that you can   use to catch such errors, assuming that you've got nobody else to scrutinize your code.   Thankfully, with some tactical programming it is quite straight-forward to structure   a code for reuse, such that you can use the same classes in production code and   directly verify them in, for instance, a unit-test framework. 

This tutorial program will have two parts: One where we just introduce the basic ideas of automatic and symbolic differentiation support in deal.II using a simple set of examples; and one where we apply this to a realistic but much more complicated case. For that second half, the next section will provide some background on magneto-mechanical materials -- you can skip this section if all you want to learn about is what AD and SD actually are, but you probably want to read over this section if you are interested in how to apply AD and SD for concrete situations. 




[1.x.2685] 

[1.x.2686] 

As a prelude to introducing the coupled magneto-mechanical material law that we'll use to model a magneto-active polymer, we'll start with a very concise summary of the salient thermodynamics to which these constitutive laws must subscribe. The basis for the theory, as summarized here, is described in copious detail by Truesdell and Toupin  [2.x.6986]  and Coleman and Noll  [2.x.6987] , and follows the logic laid out by Holzapfel  [2.x.6988] . 

Starting from the first law of thermodynamics, and following a few technical assumptions, it can be shown the the balance between the kinetic plus internal energy rates and the power supplied to the system from external sources is given by the following relationship that equates the rate of change of the energy in an (arbitrary) volume  [2.x.6989]  on the left, and the sum of forces acting on that volume on the right: 

[1.x.2687] 

Here  [2.x.6990]  represents the total time derivative,  [2.x.6991]  is the material density as measured in the Lagrangian reference frame,  [2.x.6992]  is the material velocity and  [2.x.6993]  its acceleration,  [2.x.6994]  is the internal energy per unit reference volume,  [2.x.6995]  is the total Piola stress tensor and  [2.x.6996]  is the time rate of the deformation gradient tensor,  [2.x.6997]  and  [2.x.6998]  are, respectively, the magnetic field vector and the magnetic induction (or magnetic flux density) vector,  [2.x.6999]  and  [2.x.7000]  are the electric field vector and electric displacement vector, and  [2.x.7001]  and  [2.x.7002]  represent the referential thermal flux vector and thermal source. The material differential operator  [2.x.7003]  where  [2.x.7004]  is the material position vector. With some rearrangement of terms, invoking the arbitrariness of the integration volume  [2.x.7005] , the total internal energy density rate  [2.x.7006]  can be identified as 

[1.x.2688] 

The total internal energy includes contributions that arise not only due to mechanical deformation (the first term), and thermal fluxes and sources (the fourth and fifth terms), but also due to the intrinsic energy stored in the magnetic and electric fields themselves (the second and third terms, respectively). 

The second law of thermodynamics, known also as the entropy inequality principle, informs us that certain thermodynamic processes are irreversible. After accounting for the total entropy and rate of entropy input, the Clausius-Duhem inequality can be derived. In local form (and in the material configuration), this reads 

[1.x.2689] 

The quantity  [2.x.7007]  is the absolute temperature, and  [2.x.7008]  represents the entropy per unit reference volume. 

Using this to replace  [2.x.7009]  in the result stemming from the first law of thermodynamics, we now have the relation 

[1.x.2690] 

On the basis of Fourier's law, which informs us that heat flows from regions of high temperature to low temperature, the last term is always positive and can be ignored. This renders the local dissipation inequality 

[1.x.2691] 

It is postulated  [2.x.7010]  that the Legendre transformation 

[1.x.2692] 

from which we may define the free energy density function  [2.x.7011]  with the stated parameterization, exists and is valid. Taking the material rate of this equation and substituting it into the local dissipation inequality results in the generic expression 

[1.x.2693] 

Under the assumption of isothermal conditions, and that the electric field does not excite the material in a manner that is considered non-negligible, then this dissipation inequality reduces to 

[1.x.2694] 



[1.x.2695] 

When considering materials that exhibit mechanically dissipative behavior, it can be shown that this can be captured within the dissipation inequality through the augmentation of the material free energy density function with additional parameters that represent internal variables  [2.x.7012] . Consequently, we write it as 

[1.x.2696] 

where  [2.x.7013]  represents the internal variable (which acts like a measure of the deformation gradient) associated with the `i`th mechanical dissipative (viscous) mechanism. As can be inferred from its parameterization, each of these internal parameters is considered to evolve in time. Currently the free energy density function  [2.x.7014]  is parameterized in terms of the magnetic induction  [2.x.7015] . This is the natural parameterization that comes as a consequence of the considered balance laws. Should such a class of materials to be incorporated within a finite-element model, it would be ascertained that a certain formulation of the magnetic problem, known as the magnetic vector potential formulation, would need to be adopted. This has its own set of challenges, so where possible the more simple magnetic scalar potential formulation may be preferred. In that case, the magnetic problem needs to be parameterized in terms of the magnetic field  [2.x.7016] . To make this re-parameterization, we execute a final Legendre transformation 

[1.x.2697] 

At the same time, we may take advantage of the principle of material frame indifference in order to express the energy density function in terms of symmetric deformation measures: 

[1.x.2698] 

The upshot of these two transformations (leaving out considerable explicit and hidden details) renders the final expression for the reduced dissipation inequality as 

[1.x.2699] 

(Notice the sign change on the second term on the right hand side, and the transfer of the time derivative to the magnetic induction vector.) The stress quantity  [2.x.7017]  is known as the total Piola-Kirchhoff stress tensor and its energy conjugate  [2.x.7018]  is the right Cauchy-Green deformation tensor, and  [2.x.7019]  is the re-parameterized internal variable associated with the `i`th mechanical dissipative (viscous) mechanism. 

Expansion of the material rate of the energy density function, and rearrangement of the various terms, results in the expression 

[1.x.2700] 

At this point, its worth noting the use of the [partial derivatives](https://en.wikipedia.org/wiki/Partial_derivative)  [2.x.7020] . This is an important detail that will be fundamental to a certain design choice made within the tutorial. As brief reminder of what this signifies, the partial derivative of a multi-variate function returns the derivative of that function with respect to one of those variables while holding the others constant: 

[1.x.2701] 

More specific to what's encoded in the dissipation inequality (with the very general free energy density function  [2.x.7021]  with its parameterization yet to be formalized), if one of the input variables is a function of another, it is also held constant and the chain rule does not propagate any further, while the computing total derivative would imply judicious use of the chain rule. This can be better understood by comparing the following two statements: 

[1.x.2702] 



Returning to the thermodynamics of the problem, we next exploit the arbitrariness of the quantities  [2.x.7022]  and  [2.x.7023] , by application of the Coleman-Noll procedure  [2.x.7024] ,  [2.x.7025] . This leads to the identification of the kinetic conjugate quantities 

[1.x.2703] 

(Again, note the use of the partial derivatives to define the stress and magnetic induction in this generalized setting.) From what terms remain in the dissipative power (namely those related to the mechanical dissipative mechanisms), if they are assumed to be independent of one another then, for each mechanism `i`, 

[1.x.2704] 

This constraint must be satisfied through the appropriate choice of free energy function, as well as a carefully considered evolution law for the internal variables. 

In the case that there are no dissipative mechanisms to be captured within the constitutive model (e.g., if the material to be modelled is magneto-hyperelastic) then the free energy density function  [2.x.7026]  reduces to a stored energy density function, and the total stress and magnetic induction can be simplified 

[1.x.2705] 

where the operator  [2.x.7027]  denotes the total derivative operation. 

For completeness, the linearization of the stress tensor and magnetic induction are captured within the fourth-order total referential elastic tangent tensor  [2.x.7028] , the second-order magnetostatic tangent tensor  [2.x.7029]  and the third-order total referential magnetoelastic coupling tensor  [2.x.7030] . Irrespective of the parameterization of  [2.x.7031]  and  [2.x.7032] , these quantities may be computed by 

[1.x.2706] 

For the case of rate-dependent materials, this expands to 

[1.x.2707] 

while for rate-independent materials the linearizations are 

[1.x.2708] 

The subtle difference between them is the application of a partial derivative during the calculation of the first derivatives. We'll see later how this affects the choice of AD versus SD for this specific application. For now, we'll simply introduce the two specific materials that are implemented within this tutorial. 

[1.x.2709] 

The first material that we'll consider is one that is governed by a magneto-hyperelastic constitutive law. This material responds to both deformation as well as immersion in a magnetic field, but exhibits no time- or history-dependent behavior (such as dissipation through viscous damping or magnetic hysteresis, etc.). The *stored energy density function* for such a material is only parameterized in terms of the (current) field variables, but not their time derivatives or past values. 

We'll choose the energy density function, which captures both the energy stored in the material due to deformation and magnetization, as well as the energy stored in the magnetic field itself, to be 

[1.x.2710] 

with 

[1.x.2711] 

and for which the variable  [2.x.7033]  ( [2.x.7034]  being the rank-2 identity tensor) represents the spatial dimension and  [2.x.7035]  is the deformation gradient tensor. To give some brief background to the various components of  [2.x.7036] , the first two terms bear a great resemblance to the stored energy density function for a (hyperelastic) Neohookean material. The only difference between what's used here and the Neohookean material is the scaling of the elastic shear modulus by the magnetic field-sensitive saturation function  [2.x.7037]  (see  [2.x.7038] , equation 29). This function will, in effect, cause the material to stiffen in the presence of a strong magnetic field. As it is governed by a sigmoid-type function, the shear modulus will asymptotically converge on the specified saturation shear modulus. It can also be shown that the last term in  [2.x.7039]  is the stored energy density function for magnetic field (as derived from first principles), scaled by the relative permeability constant. This definition collectively implies that the material is linearly magnetized, i.e., the magnetization vector and magnetic field vector are aligned. (This is certainly not obvious with the magnetic energy stated in its current form, but when the magnetic induction and magnetization are derived from  [2.x.7040]  and all magnetic fields are expressed in the  [2.x.7041] current configuration [2.x.7042]  then this correlation becomes clear.) As for the specifics of what the magnetic induction, stress tensor, and the various material tangents look like, we'll defer presenting these to the tutorial body where the full, unassisted implementation of the constitutive law is defined. 

[1.x.2712] 

The second material that we'll formulate is one for a magneto-viscoelastic material with a single dissipative mechanism `i`. The *free energy density function* that we'll be considering is defined as 

[1.x.2713] 

with 

[1.x.2714] 



[1.x.2715] 

and the evolution law 

[1.x.2716] 

for the internal viscous variable. We've chosen the magnetoelastic part of the energy  [2.x.7043]  to match that of the first material model that we explored, so this part needs no further explanation. As for the viscous part  [2.x.7044] , this component of the free energy (in conjunction with the evolution law for the viscous deformation tensor) is taken from  [2.x.7045]  (with the additional scaling by the viscous saturation function described in  [2.x.7046] ). It is derived in a thermodynamically consistent framework that, at its core, models the movement of polymer chains on a micro-scale level. 

To proceed beyond this point, we'll also need to consider the time discretization of the evolution law. Choosing the implicit first-order backwards difference scheme, then 

[1.x.2717] 

where the superscript  [2.x.7047]  denotes that the quantity is taken at the current timestep, and  [2.x.7048]  denotes quantities taken at the previous timestep (i.e., a history variable). The timestep size  [2.x.7049]  is the difference between the current time and that of the previous timestep. Rearranging the terms so that all internal variable quantities at the current time are on the left hand side of the equation, we get 

[1.x.2718] 

that matches  [2.x.7050]  equation 54. 

[1.x.2719] 

Now that we have shown all of these formulas for the thermodynamics and theory governing magneto-mechanics and constitutive models, let us outline what the program will do with all of this. We wish to do something *meaningful* with the materials laws that we've formulated, and so it makes sense to subject them to some mechanical and magnetic loading conditions that are, in some way, representative of some conditions that might be found either in an application or in a laboratory setting. One way to achieve that aim would be to embed these constitutive laws in a finite element model to simulate a device. In this instance, though, we'll keep things simple (we are focusing on the automatic and symbolic differentiation concepts, after all) and will find a concise way to faithfully replicate an industry-standard rheological experiment using an analytical expression for the loading conditions. 

The rheological experiment that we'll reproduce, which idealizes a laboratory experiment that was used to characterize magneto-active polymers, is detailed in  [2.x.7051]  (as well as  [2.x.7052] , in which it is documented along with the real-world experiments). The images below provide a visual description of the problem set up. 

 [2.x.7053]  

Under the assumptions that an incompressible medium is being tested, and that the deformation profile through the sample thickness is linear, then the displacement at some measurement point  [2.x.7054]  within the sample, expressed in radial coordinates, is 

[1.x.2720] 

where  [2.x.7055]  and  [2.x.7056]  are the radius at 

-- and angle of -- the sampling point,  [2.x.7057]  is the (constant) axial deformation,  [2.x.7058]  is the time-dependent torsion angle per unit length that will be prescribed using a sinusoidally repeating oscillation of fixed amplitude  [2.x.7059] . The magnetic field is aligned axially, i.e., in the  [2.x.7060]  direction. 

This summarizes everything that we need to fully characterize the idealized loading at any point within the rheological sample. We'll set up the problem in such a way that we "pick" a representative point with this sample, and subject it to a harmonic shear deformation at a constant axial deformation (by default, a compressive load) and a constant, axially applied magnetic field. We will record the stress and magnetic induction at this point, and will output that data to file for post-processing. Although its not necessary for this particular problem, we will also be computing the tangents as well. Even though they are not directly used in this particular piece of work, these second derivatives are needed to embed the constitutive law within a finite element model (one possible extension to this work). We'll therefore take the opportunity to check our hand calculations for correctness using the assisted differentiation frameworks. 

[1.x.2721] 

In addition to the already mentioned  [2.x.7061]  module, the following are a few references that discuss in more detail 

- magneto-mechanics, and some aspects of automated differentiation frameworks:  [2.x.7062] ,  [2.x.7063] , and 

- the automation of finite element frameworks using AD and/or SD:  [2.x.7064] ,  [2.x.7065] . 

 [2.x.7066]  


examples/step-71/doc/results.dox 



[1.x.2722] 

[1.x.2723] 

The first exploratory example produces the following output. It is verified that all three implementations produce identical results. 

[1.x.2724] 



[1.x.2725] 

To help summarize the results from the virtual experiment itself, below are some graphs showing the shear stress, plotted against the shear strain, at a select location within the material sample. The plots show the stress-strain curves under three different magnetic loads, and for the last cycle of the (mechanical) loading profile, when the rate-dependent material reaches a repeatable ("steady-state") response. These types of graphs are often referred to as [Lissajous plots](https://en.wikipedia.org/wiki/Lissajous_curve). The area of the ellipse that the curve takes for viscoelastic materials provides some measure of how much energy is dissipated by the material, and its ellipticity indicates the phase shift of the viscous response with respect to the elastic response. 

 [2.x.7067]  

It is not surprising to see that the magneto-elastic material response has an unloading curve that matches the loading curve -- the material is non-dissipative after all. But here it's clearly noticeable how the gradient of the curve increases as the applied magnetic field increases. The tangent at any point along this curve is related to the instantaneous shear modulus and, due to the way that the energy density function was defined, we expect that the shear modulus increases as the magnetic field strength increases. We observe much the same behavior for the magneto-viscoelastic material. The major axis of the ellipse traced by the loading-unloading curve has a slope that increases as a greater magnetic load is applied. At the same time, the more energy is dissipated by the material. 

As for the code output, this is what is printed to the console for the part pertaining to the rheological experiment conducted with the magnetoelastic material: 

[1.x.2726] 



And this portion of the output pertains to the experiment performed with the magneto-viscoelastic material: 

[1.x.2727] 



The timer output is also emitted to the console, so we can compare time taken to perform the hand- and assisted- calculations and get some idea of the overhead of using the AD and SD frameworks. Here are the timings taken from the magnetoelastic experiment using the AD framework, based on the Sacado component of the Trilinos library: 

[1.x.2728] 

With respect to the computations performed using automatic differentiation (as a reminder, this is with two levels of differentiation using the Sacado library in conjunction with dynamic forward auto-differentiable types), we observe that the assisted computations takes about  [2.x.7068]  longer to compute the desired quantities. This does seem like quite a lot of overhead but, as mentioned in the introduction, it's entirely subjective and circumstance-dependent as to whether or not this is acceptable or not: Do you value computer time more than human time for doing the necessary hand-computations of derivatives, verify their correctness, implement them, and verify the correctness of the implementation? If you develop a research code that will only be run for a relatively small number of experiments, you might value your own time more. If you develop a production code that will be run over and over on 10,000-core clusters for hours, your considerations might be different. In any case, the one nice feature of the AD approach is the "drop in" capability when functions and classes are templated on the scalar type. This means that minimal effort is required to start working with it. 

In contrast, the timings for magneto-viscoelastic material as implemented using just-in-time (JIT) compiled symbolic algebra indicate that, at some non-negligible cost during initialization, the calculations themselves are a lot more efficiently executed: 

[1.x.2729] 

Since the initialization phase need, most likely, only be executed once per thread, this initial expensive phase can be offset by the repeated use of a single  [2.x.7069]  instance. Even though the magneto-viscoelastic constitutive law has more terms to calculate when compared to its magnetoelastic counterpart, it still is a whole order of magnitude faster to execute the computations of the kinetic variables and tangents. And when compared to the hand computed variant that uses the caching scheme, the calculation time is nearly equal. So although using the symbolic framework requires a paradigm shift in terms of how one implements and manipulates the symbolic expressions, it can offer good performance and flexibility that the AD frameworks lack. 

On the point of data caching, the added cost of value caching for the magneto-viscoelastic material implementation is, in fact, about a  [2.x.7070]  increase in the time spent in `update_internal_data()` when compared to the implementation using intermediate values for the numerical experiments conducted with this material. Here's a sample output of the timing comparison extracted for the "hand calculated" variant when the caching data structure is removed: 

[1.x.2730] 



With some minor adjustment we can quite easily test the different optimization schemes for the batch optimizer. So let's compare the computational expense associated with the `LLVM` batch optimizer setting versus the alternatives. Below are the timings reported for the `lambda` optimization method (retaining the use of CSE): 

[1.x.2731] 

The primary observation here is that an order of magnitude greater time is spent in the "Assisted computation" section when compared to the `LLVM` approach. 

Last of all we'll test how `dictionary` substitution, in conjunction with CSE, performs. Dictionary substitution simply does all of the evaluation within the native CAS framework itself, with no transformation of the underlying data structures taking place. Only the use of CSE, which caches intermediate results, will provide any "acceleration" in this instance. With that in mind, here are the results from this selection: 

[1.x.2732] 

Needless to say, compared to the other two methods, these results took quite some time to produce... The `dictionary` substitution method is perhaps only really viable for simple expressions or when the number of calls is sufficiently small. 

[1.x.2733] 

Perhaps you've been convinced that these tools have some merit, and can be of immediate help or use to you. The obvious question now is which one to use. Focusing specifically at a continuum point level, where you would be using these frameworks to compute derivatives of a constitutive law in particular, we can say the following: 

- Automatic differentiation probably provides the simplest entry point into   the world of assisted differentiation. 

- Given a sufficiently generic implementation of a constitutive framework,   AD can often be used as a drop-in replacement for the intrinsic scalar types   and the helper classes can then be leveraged to compute first (and possibly   higher order) derivatives with minimal effort. 

- As a qualification to the above point, being a "drop-in replacement" does not   mean that you must not be contentious of what the algorithms that these numbers   are being passed through are doing. It is possible to inadvertently perform   an operation that would, upon differentiating, return an incorrect result.   So this is definitely something that one should be aware of.   A concrete example: When computing the eigenvalues of a tensor, if the tensor   is diagonal then a short-cut to the result is simply to return the diagonal   entries directly (as extracted from the input tensor). This is completely   correct in terms of computing the eigenvalues themselves, but not going   through the algorithm that would otherwise compute the eigenvalues for a   non-diagonal tensor has had an unintended side-effect, namely that the   eigenvalues appear (to the AD framework) to be completely decoupled from   one another and their cross-sensitivities are not encoded in the returned   result. Upon differentiating, many entries of the derivative tensor will   be missing. To fix this issue, one has to ensure that the standard eigenvalue   solving algorithm is used so that the sensitivities of the returned eigenvalues   with respect to one another are encoded in the result. 

- Computations involving AD number types may be expensive. The expense increases   (sometimes quite considerably) as the order of the differential operations   increases. This may be mitigated by computational complexity of surrounding   operations (such as a linear solve, for example), but is ultimately problem   specific. 

- AD is restricted to the case where only total derivatives are required. If a   differential operation requires a partial derivative with respect to an   independent variable then it is not appropriate to use it. 

- Each AD library has its own quirks (sad to say but, in the author's experience,   true), so it may take some trial and error to find the appropriate library and   choice of AD number to suit your purposes. The reason for these "quirks"   often boils down to the overall philosophy behind the library (data structures,   the use of template meta-programming, etc.) as well as the mathematical   implementation of the derivative computations (for example, manipulations of   results using logarithmic functions to change basis might restrict the domain   for the input values -- details all hidden from the user, of course).   Furthermore, one library might be able to compute the desired results quicker   than another, so some initial exploration might be beneficial in that regard. 

- Symbolic differentiation (well, the use of a CAS in general) provides the most   flexible framework with which to perform assisted computations. 

- The SD framework can do everything that the AD frameworks can, with the   additional benefit of having low-level control over when certain manipulations   and operations are performed. 

- Acceleration of expression evaluation is possible, potentially leading to   near-native performance of the SD framework compared to some hand implementations   (this comparison being dependent on the overall program design, of course) at   the expense of the initial optimization call. 

- Clever use of the  [2.x.7071]  could minimize the   expense of the costly call that optimizes the dependent expressions.   The possibility to serialize the  [2.x.7072]    that often (but not always) this expensive call can be done once and then   reused in a later simulation. 

- If two or more material laws differ by only their material parameters, for   instance, then a single batch optimizer can be shared between them as long   as those material parameters are considered to be symbolic. The implication   of this is that you can "differentiate once, evaluate in many contexts". 

- The SD framework may partially be used as a "drop-in replacement" for scalar   types, but one (at the very least) has to add some more framework around it   to perform the value substitution step, converting symbolic types to their   numerical counterparts. 

- It may not be possible to use SD numbers within some specialized algorithms.   For example, if an algorithm has an exit point or code branch based off of   some concrete, numerical value that the (symbolic) input argument should take,   then obviously this isn't going to work. One either has to reimplement the   algorithm specifically for SD number types (somewhat inconvenient, but   frequently possible as conditionals are supported by the    [2.x.7073]  class), or one must use a creative means   around this specific issue (e.g., introduce a symbolic expression that   represents the result returned by this algorithm, perhaps declaring it   to be a   [symbolic function](https://dealii.org/developer/doxygen/deal.II/namespaceDifferentiation_1_1SD.html#a876041f6048705c7a8ad0855cdb1bd7a)   if that makes sense within the context in which it is to be used. This can   later be substituted by its numerical values, and if declared a symbolic   function then its deferred derivatives may also be incorporated into the   calculations as substituted results.). 

- The biggest drawback to using SD is that using it requires a paradigm shift,   and that one has to frame most problems differently in order to take the   most advantage of it. (Careful consideration of how the data structures   are used and reused is also essential to get it to work effectively.) This may   mean that one needs to play around with it a bit and build up an understanding   of what the sequence of typical operations is and what specifically each step   does in terms of manipulating the underlying data. If one has the time and   inclination to do so, then the benefits of using this tool may be substantial. 

[1.x.2734] 

There are a few logical ways in which this program could be extended: 

- Perhaps the most obvious extension would be to implement and test other constitutive models.   This could still be within the realm of coupled magneto-mechanical problems, perhaps considering   alternatives to the "Neo-Hookean"-type elastic part of the energy functions, changing the   constitutive law for the dissipative energy (and its associated evolution law), or including   magnetic hysteretic effects or damage models for the composite polymer that these material   seek to model. 

- Of course, the implemented models could be modified or completely replaced with models that are   focused on other aspects of physics, such as electro-active polymers, biomechanical materials,   elastoplastic media, etc. 

- Implement a different time-discretization scheme for the viscoelastic evolution law. 

- Instead of deriving everything directly from an energy density function, use the    [2.x.7074]  to directly linearize the kinetic quantities.   This would mean that only a once-differentiable auto-differentiable number type   would be required, and would certainly improve the performance greatly.   Such an approach would also offer the opportunity for dissipative materials,   such as the magneto-viscoelastic one consider here, to be implemented in   conjunction with AD. This is because the linearization invokes the total   derivative of the dependent variables with respect to the field variables, which   is exactly what the AD frameworks can provide. 

- Investigate using other auto-differentiable number types and frameworks (such as   ADOL-C). Since each AD library has its own implementation, the choice of which   to use could result in performance increases and, in the most unfortunate cases,   more stable computations. It can at least be said that for the AD libraries that   deal.II supports, the accuracy of results should be largely unaffected by this decision. 

- Embed one of these constitutive laws within a finite element simulation. 

With less effort, one could think about re-writing nonlinear problem solvers such as the one implemented in step-15 using AD or SD approaches to compute the Newton matrix. Indeed, this is done in step-72. 


examples/step-72/doc/intro.dox 

 [2.x.7075]  

[1.x.2735] 




[1.x.2736] 

[1.x.2737] 

This program solves the same problem as step-15, that is, it solves for the [minimal surface equation](https://en.wikipedia.org/wiki/Minimal_surface)   [1.x.2738] 



Among the issues we had identified there (see the [1.x.2739] section) was that when wanting to use a Newton iteration, we needed to compute the derivative of the residual of the equation with regard to the solution  [2.x.7076]  (here, because the right hand side is zero, the residual is simply the left hand side). For the equation we have here, this is cumbersome but not impossible -- but one can easily imagine much more complicated equations where just implementing the residual itself correctly is a challenge, let alone doing so for the derivative necessary to compute the Jacobian matrix. We will address this issue in this program: Using the automatic differentiation techniques discussed in great detail in step-71, we will come up with a way how we only have to implement the residual and get the Jacobian for free. 

In fact, we can even go one step further. While in step-15 we have just taken the equation as a given, the minimal surface equation is actually the product of minimizing an energy. Specifically, the minimal surface equations are the Euler-Lagrange equations that correspond to minimizing the energy   [1.x.2740] 

where the *energy density* is given by   [1.x.2741] 

This is the same as saying that we seek to find the stationary point of the variation of the energy functional   [1.x.2742] 

as this is where the equilibrium solution to the boundary value problem lies. 

The key point then is that, maybe, we don't even need to implement the residual, but that implementing the simpler energy density  [2.x.7077]  might actually be enough. 

Our goal then is this: When using a Newton iteration, we need to repeatedly solve the linear partial differential equation   [1.x.2743] 

so that we can compute the update   [1.x.2744] 

with the solution  [2.x.7078]  of the Newton step. As discussed in step-15, we can compute the derivative  [2.x.7079]  by hand and obtain   [1.x.2745] 



So here then is what this program is about: It is about techniques that can help us with computing  [2.x.7080]  without having to implement it explicitly, either by providing an implementation of  [2.x.7081]  or an implementation of  [2.x.7082] . More precisely, we will implement three different approaches and compare them in terms of run-time but also -- maybe more importantly -- how much human effort it takes to implement them: 

- The method used in step-15 to form the Jacobian matrix. 

- Computing the Jacobian matrix from an implementation of the   residual  [2.x.7083] , using automatic differentiation. 

- Computing both the residual and Jacobian matrix from an   implementation of the energy functional  [2.x.7084] , also using automatic   differentiation. 

For the first of these methods, there are no conceptual changes compared to step-15. 




[1.x.2746] 

For the second method, let us outline how we will approach the issue using automatic differentiation to compute the linearization of the residual vector. To this end, let us change notation for a moment and denote by  [2.x.7085]  not the residual of the differential equation, but in fact the *residual vector* -- i.e., the *discrete residual*. We do so because that is what we *actually* do when we discretize the problem on a given mesh: We solve the problem  [2.x.7086]  where  [2.x.7087]  is the vector of unknowns. 

More precisely, the  [2.x.7088] th component of the residual is given by 

[1.x.2747] 

where  [2.x.7089] . Given this, the contribution for cell  [2.x.7090]  is 

[1.x.2748] 

Its first order Taylor expansion is given as 

[1.x.2749] 

and consequently we can compute the contribution of cell  [2.x.7091]  to the Jacobian matrix  [2.x.7092]  as  [2.x.7093] . The important point here is that on cell  [2.x.7094] , we can express 

[1.x.2750] 

For clarity, we have used  [2.x.7095]  and  [2.x.7096]  as counting indices to make clear that they are distinct from each other and from  [2.x.7097]  above. Because in this formula,  [2.x.7098]  only depends on the coefficients  [2.x.7099] , we can compute the derivative  [2.x.7100]  as a matrix via automatic differentiation of  [2.x.7101] . By the same argument as we always use, it is clear that  [2.x.7102]  does not actually depend on *all* unknowns  [2.x.7103] , but only on those unknowns for which  [2.x.7104]  is a shape function that lives on cell  [2.x.7105] , and so in practice, we restrict  [2.x.7106]  and  [2.x.7107]  to that part of the vector and matrix that corresponds to the *local* DoF indices, and then distribute from the local cell  [2.x.7108]  to the global objects. 

Using all of these realizations, the approach will then be to implement  [2.x.7109]  in the program and let the automatic differentiation machinery compute the derivatives  [2.x.7110]  from that. 




[1.x.2751] 

For the final implementation of the assembly process, we will move a level higher than the residual: our entire linear system will be determined directly from the energy functional that governs the physics of this boundary value problem. We can take advantage of the fact that we can calculate the total energy in the domain directly from the local contributions, i.e., 

[1.x.2752] 

In the discrete setting, this means that on each finite element we have 

[1.x.2753] 

If we implement the cell energy, which depends on the field solution, we can compute its first (discrete) variation 

[1.x.2754] 

and, thereafter, its second (discrete) variation 

[1.x.2755] 

So, from the cell contribution to the total energy function, we may expect to have the approximate residual and tangent contributions generated for us as long as we can provide an implementation of the local energy  [2.x.7111] . Again, due to the design of the automatic differentiation variables used in this tutorial, in practice these approximations for the contributions to the residual vector and tangent matrix are actually accurate to machine precision. 


examples/step-72/doc/results.dox 



[1.x.2756] 

Since there was no change to the physics of the problem that has first been analyzed in step-15, there is nothing to report about that. The only outwardly noticeable difference between them is that, by default, this program will only run 9 mesh refinement steps (as opposed to step-15, which executes 11 refinements). This will be observable in the simulation status that appears between the header text that prints which assembly method is being used, and the final timings. (All timings reported below were obtained in release mode.) 

[1.x.2757] 



So what is interesting for us to compare is how long the assembly process takes for the three different implementations, and to put that into some greater context. Below is the output for the hand linearization (as computed on a circa 2012 four core, eight thread laptop -- but we're only really interested in the relative time between the different implementations): 

[1.x.2758] 

And for the implementation that linearizes the residual in an automated manner using the Sacado dynamic forward AD number type: 

[1.x.2759] 

And, lastly, for the implementation that computes both the residual and its linearization directly from an energy functional (using nested Sacado dynamic forward AD numbers): 

[1.x.2760] 



It's evident that the more work that is passed off to the automatic differentiation framework to perform, the more time is spent during the assembly process. Accumulated over all refinement steps, using one level of automatic differentiation resulted in  [2.x.7112]  more computational time being spent in the assembly stage when compared to unassisted assembly, while assembling the discrete linear system took  [2.x.7113]  longer when deriving directly from the energy functional. Unsurprisingly, the overall time spent solving the linear system remained unchanged. This means that the proportion of time spent in the solve phase to the assembly phase shifted significantly as the number of times automated differentiation was performed at the finite element level. For many, this might mean that leveraging higher-order differentiation (at the finite element level) in production code leads to an unacceptable overhead, but it may still be useful during the prototyping phase. A good compromise between the two may, therefore, be the automated linearization of the finite element residual, which offers a lot of convenience at a measurable, but perhaps not unacceptable, cost. Alternatively, one could consider not re-building the Newton matrix in every step -- a topic that is explored in substantial depth in step-77. 

Of course, in practice the actual overhead is very much dependent on the problem being evaluated (e.g., how many components there are in the solution, what the nature of the function being differentiated is, etc.). So the exact results presented here should be interpreted within the context of this scalar problem alone, and when it comes to other problems, some preliminary investigation by the user is certainly warranted. 




[1.x.2761] 

Like step-71, there are a few items related to automatic differentiation that could be evaluated further: 

- The use of other AD frameworks should be investigated, with the outlook that   alternative implementations may provide performance benefits. 

- It is also worth evaluating AD number types other than those that have been   hard-coded into this tutorial. With regard to twice differentiable types   employed at the finite-element level, mixed differentiation modes ("RAD-FAD")   should in principle be more computationally efficient than the single   mode ("FAD-FAD") types employed here. The reason that the RAD-FAD type was not   selected by default is that, at the time of writing, there remain some   bugs in its implementation within the Sacado library that lead to memory leaks.   This is documented in the  [2.x.7114]  module. 

- It might be the case that using reduced precision types (i.e., `float`) as the   scalar types for the AD numbers could render a reduction in computational   expense during assembly. Using `float` as the data type for the   matrix and the residual is not unreasonable, given that the Newton   update is only meant to get us closer to the solution, but not   actually *to* the solution; as a consequence, it makes sense to   consider using reduced-precision data types for computing these   updates, and then accumulating these updates in a solution vector   that uses the full `double` precision accuracy. 

- One further method of possibly reducing resources during assembly is to frame   the AD implementations as a constitutive model. This would be similar to the   approach adopted in step-71, and pushes the starting point for the automatic   differentiation one level higher up the chain of computations. This, in turn,   means that less operations are tracked by the AD library, thereby reducing the   cost of differentiating (even though one would perform the differentiation at   each cell quadrature point). 

- step-77 is yet another variation of step-15 that addresses a very   different part of the problem: Line search and whether it is   necessary to re-build the Newton matrix in every nonlinear   iteration. Given that the results above show that using automatic   differentiation comes at a cost, the techniques in step-77 have the   potential to offset these costs to some degree. It would therefore   be quite interesting to combine the current program with the   techniques in step-77. For production codes, this would certainly be   the way to go. 


examples/step-74/doc/intro.dox 

 [2.x.7115]  

[1.x.2762] 


[1.x.2763] 

[1.x.2764] 

[1.x.2765] In this tutorial, we display the usage of the FEInterfaceValues class, which is designed for assembling face terms arising from discontinuous Galerkin (DG) methods. The FEInterfaceValues class provides an easy way to obtain the jump and the average of shape functions and of the solution across cell faces. This tutorial includes the following topics. <ol>    [2.x.7116]  The SIPG method for Poisson's equation, which has already been used in step-39 and step-59.    [2.x.7117]  Assembling of face terms using FEInterfaceValues and the system matrix using  [2.x.7118]  which is similar to step-12.    [2.x.7119]  Adaptive mesh refinement using an error estimator.    [2.x.7120]  Two test cases: convergence test for a smooth function and adaptive mesh refinement test for a singular solution.  [2.x.7121]  

[1.x.2766] In this example, we consider Poisson's equation 

[1.x.2767] 

subject to the boundary condition 

[1.x.2768] 

For simplicity, we assume that the diffusion coefficient  [2.x.7122]  is constant here. Note that if  [2.x.7123]  is discontinuous, we need to take this into account when computing jump terms on cell faces. 

We denote the mesh by  [2.x.7124] , and  [2.x.7125]  is a mesh cell. The sets of interior and boundary faces are denoted by  [2.x.7126]  and  [2.x.7127]  respectively. Let  [2.x.7128]  and  [2.x.7129]  be the two cells sharing a face  [2.x.7130] , and  [2.x.7131]  be the outer normal vector of  [2.x.7132] . Then the jump operator is given by the "here minus there" formula, 

[1.x.2769] 

and the averaging operator as 

[1.x.2770] 

respectively. Note that when  [2.x.7133] , we define  [2.x.7134]  and  [2.x.7135] . The discretization using the SIPG is given by the following weak formula (more details can be found in  [2.x.7136]  and the references therein) 

[1.x.2771] 






[1.x.2772] The penalty parameter is defined as  [2.x.7137] , where  [2.x.7138]  a local length scale associated with the cell face; here we choose an approximation of the length of the cell in the direction normal to the face:  [2.x.7139] , where  [2.x.7140]  are the two cells adjacent to the face  [2.x.7141]  and we we compute  [2.x.7142] . 

In the formula above,  [2.x.7143]  is the penalization constant. To ensure the discrete coercivity, the penalization constant has to be large enough  [2.x.7144] . People do not really have consensus on which of the formulas proposed in the literature should be used. (This is similar to the situation discussed in the "Results" section of step-47.) One can just pick a large constant, while other options could be the multiples of  [2.x.7145]  or  [2.x.7146] . In this code, we follow step-39 and use  [2.x.7147] . 




[1.x.2773] In this example, with a slight modification, we use the error estimator by Karakashian and Pascal  [2.x.7148]  

[1.x.2774] 

where 

[1.x.2775] 

Here we use  [2.x.7149]  instead of  [2.x.7150]  for the jump terms of  [2.x.7151]  (the first term in  [2.x.7152]  and  [2.x.7153] ). 

In order to compute this estimator, in each cell  [2.x.7154]  we compute 

[1.x.2776] 

Then the square of the error estimate per cell is 

[1.x.2777] 

The factor of  [2.x.7155]  results from the fact that the overall error estimator includes each interior face only once, and so the estimators per cell count it with a factor of one half for each of the two adjacent cells. Note that we compute  [2.x.7156]  instead of  [2.x.7157]  to simplify the implementation. The error estimate square per cell is then stored in a global vector, whose  [2.x.7158]  norm is equal to  [2.x.7159] . 

[1.x.2778] In the first test problem, we run a convergence test using a smooth manufactured solution with  [2.x.7160]  in 2D 

[1.x.2779] 

and  [2.x.7161] . We compute errors against the manufactured solution and evaluate the convergence rate. 

In the second test, we choose  [2.x.7162]  on a L-shaped domain  [2.x.7163]  in 2D. The solution is given in the polar coordinates by  [2.x.7164] , which has a singularity at the origin. An error estimator is constructed to detect the region with large errors, according to which the mesh is refined adaptively. 


examples/step-74/doc/results.dox 



[1.x.2780] 

The output of this program consist of the console output and solutions in vtu format. 

In the first test case, when you run the program, the screen output should look like the following: 

[1.x.2781] 



When using the smooth case with polynomial degree 3, the convergence table will look like this:  [2.x.7165]  

Theoretically, for polynomial degree  [2.x.7166] , the order of convergence in  [2.x.7167]  norm and  [2.x.7168]  seminorm should be  [2.x.7169]  and  [2.x.7170] , respectively. Our numerical results are in good agreement with theory. 

In the second test case, when you run the program, the screen output should look like the following: 

[1.x.2782] 



The following figure provides a log-log plot of the errors versus the number of degrees of freedom for this test case on the L-shaped domain. In order to interpret it, let  [2.x.7171]  be the number of degrees of freedom, then on uniformly refined meshes,  [2.x.7172]  is of order  [2.x.7173]  in 2D. Combining the theoretical results in the previous case, we see that if the solution is sufficiently smooth, we can expect the error in the  [2.x.7174]  norm to be of order  [2.x.7175]  and in  [2.x.7176]  seminorm to be  [2.x.7177] . It is not a priori clear that one would get the same kind of behavior as a function of  [2.x.7178]  on adaptively refined meshes like the ones we use for this second test case, but one can certainly hope. Indeed, from the figure, we see that the SIPG with adaptive mesh refinement produces asymptotically the kinds of hoped-for results: 

 [2.x.7179]  

In addition, we observe that the error estimator decreases at almost the same rate as the errors in the energy norm and  [2.x.7180]  seminorm, and one order lower than the  [2.x.7181]  error. This suggests its ability to predict regions with large errors. 

While this tutorial is focused on the implementation, the step-59 tutorial program achieves an efficient large-scale solver in terms of computing time with matrix-free solution techniques. Note that the step-59 tutorial does not work with meshes containing hanging nodes at this moment, because the multigrid interface matrices are not as easily determined, but that is merely the lack of some interfaces in deal.II, nothing fundamental. 


examples/step-75/doc/intro.dox 

 [2.x.7182]  

[1.x.2783] 




 [2.x.7183]  As a prerequisite of this program, you need to have the p4est library and the Trilinos library installed. The installation of deal.II together with these additional libraries is described in the [1.x.2784] file. 




[1.x.2785] 

[1.x.2786] 

In the finite element context, more degrees of freedom usually yield a more accurate solution but also require more computational effort. 

Throughout previous tutorials, we found ways to effectively distribute degrees of freedom by aligning the grid resolution locally with the complexity of the solution (adaptive mesh refinement, step-6). This approach is particularly effective if we do not only adapt the grid alone, but also locally adjust the polynomial degree of the associated finite element on each cell (hp-adaptation, step-27). 

In addition, assigning more processes to run your program simultaneously helps to tackle the computational workload in lesser time. Depending on the hardware architecture of your machine, your program must either be prepared for the case that all processes have access to the same memory (shared memory, step-18), or that processes are hosted on several independent nodes (distributed memory, step-40). 

In the high-performance computing segment, memory access turns out to be the current bottleneck on supercomputers. We can avoid storing matrices altogether by computing the effect of matrix-vector products on the fly with MatrixFree methods (step-37). They can be used for geometric multigrid methods (step-50) and also for polynomial multigrid methods to speed solving the system of equation tremendously. 

This tutorial combines all of these particularities and presents a state-of-the-art way how to solve a simple Laplace problem: utilizing both hp-adaptation and matrix-free hybrid multigrid methods on machines with distributed memory. 




[1.x.2787] 

For parallel applications in FEM, we partition the grid into subdomains (aka domain decomposition), which are assigned to processes. This partitioning happens on active cells in deal.II as demonstrated in step-40. There, each cell has the same finite element and the same number of degrees of freedom assigned, and approximately the same workload. To balance the workload among all processes, we have to balance the number of cells on all participating processes. 

With hp-adaptive methods, this is no longer the case: the finite element type may vary from cell to cell and consequently also the number of degrees of freedom. Matching the number of cells does not yield a balanced workload. In the matrix-free context, the workload can be assumed to be proportional the number of degrees of freedom of each process, since in the best case only the source and the destination vector have to be loaded. 

One could balance the workload by assigning weights to every cell which are proportional to the number of degrees of freedom, and balance the sum of all weights between all processes. Assigning individual weights to each cell can be realized with the class  [2.x.7184]  which we will use later. 




[1.x.2788] 

With hp-adaptive methods, we not only have to decide which cells we want to refine or coarsen, but we also have the choice how we want to do that: either by adjusting the grid resolution or the polynomial degree of the finite element. 

We will again base the decision on which cells to adapt on (a posteriori) computed error estimates of the current solution, e.g., using the KellyErrorEstimator. We will similarly decide how to adapt with (a posteriori) computed smoothness estimates: large polynomial degrees work best on smooth parts of the solution while fine grid resolutions are favorable on irregular parts. In step-27, we presented a way to calculate smoothness estimates based on the decay of Fourier coefficients. Let us take here the opportunity and present an alternative that follows the same idea, but with Legendre coefficients. 

We will briefly present the idea of this new technique, but limit its description to 1D for simplicity. Suppose  [2.x.7185]  is a finite element function defined on a cell  [2.x.7186]  as 

[1.x.2789] 

where each  [2.x.7187]  is a shape function. We can equivalently represent  [2.x.7188]  in the basis of Legendre polynomials  [2.x.7189]  as 

[1.x.2790] 

Our goal is to obtain a mapping between the finite element coefficients  [2.x.7190]  and the Legendre coefficients  [2.x.7191] . We will accomplish this by writing the problem as a  [2.x.7192] -projection of  [2.x.7193]  onto the Legendre basis. Each coefficient  [2.x.7194]  can be calculated via 

[1.x.2791] 

By construction, the Legendre polynomials are orthogonal under the  [2.x.7195] -inner product on  [2.x.7196] . Additionally, we assume that they have been normalized, so their inner products can be written as 

[1.x.2792] 

where  [2.x.7197]  is the Kronecker delta, and  [2.x.7198]  is the Jacobian of the mapping from  [2.x.7199]  to  [2.x.7200] , which (in this tutorial) is assumed to be constant (i.e., the mapping must be affine). 

Hence, combining all these assumptions, the projection matrix for expressing  [2.x.7201]  in the Legendre basis is just  [2.x.7202]  -- that is,  [2.x.7203]  times the identity matrix. Let  [2.x.7204]  be the Mapping from  [2.x.7205]  to its reference cell  [2.x.7206] . The entries in the right-hand side in the projection system are, therefore, 

[1.x.2793] 

Recalling the shape function representation of  [2.x.7207] , we can write this as  [2.x.7208] , where  [2.x.7209]  is the change-of-basis matrix with entries 

[1.x.2794] 

so the values of  [2.x.7210]  can be written  [2.x.7211] independently [2.x.7212]  of  [2.x.7213]  by factoring  [2.x.7214]  out front after transforming to reference coordinates. Hence, putting it all together, the projection problem can be written as 

[1.x.2795] 

which can be rewritten as simply 

[1.x.2796] 



At this point, we need to emphasize that most finite element applications use unstructured meshes for which mapping is almost always non-affine. Put another way: the assumption that  [2.x.7215]  is constant across the cell is not true for general meshes. Hence, a correct calculation of  [2.x.7216]  requires not only that we calculate the corresponding transformation matrix  [2.x.7217]  for every single cell, but that we also define a set of Legendre-like orthogonal functions on a cell  [2.x.7218]  which may have an arbitrary and very complex geometry. The second part, in particular, is very computationally expensive. The current implementation of the FESeries transformation classes relies on the simplification resulting from having a constant Jacobian to increase performance and thus only yields correct results for affine mappings. The transformation is only used for the purpose of smoothness estimation to decide on the type of adaptation, which is not a critical component of a finite element program. Apart from that, this circumstance does not pose a problem for this tutorial as we only use square-shaped cells. 

Eibner and Melenk  [2.x.7219]  argued that a function is analytic, i.e., representable by a power series, if and only if the absolute values of the Legendre coefficients decay exponentially with increasing index  [2.x.7220] : 

[1.x.2797] 

The rate of decay  [2.x.7221]  can be interpreted as a measure for the smoothness of that function. We can get it as the slope of a linear regression fit of the transformation coefficients: 

[1.x.2798] 



We will perform this fit on each cell  [2.x.7222]  to get a local estimate for the smoothness of the finite element approximation. The decay rate  [2.x.7223]  then acts as the decision indicator for hp-adaptation. For a finite element on a cell  [2.x.7224]  with a polynomial degree  [2.x.7225] , calculating the coefficients for  [2.x.7226]  proved to be a reasonable choice to estimate smoothness. You can find a more detailed and dimension independent description in  [2.x.7227] . 

All of the above is already implemented in the  [2.x.7228]  class and the  [2.x.7229]  namespace. With the error estimates and smoothness indicators, we are then left to flag the cells for actual refinement and coarsening. Some functions from the  [2.x.7230]  and  [2.x.7231]  namespaces will help us with that later. 




[1.x.2799] 

Finite element matrices are typically very sparse. Additionally, hp-adaptive methods correspond to matrices with highly variable numbers of nonzero entries per row. Some state-of-the-art preconditioners, like the algebraic multigrid (AMG) ones as used in step-40, behave poorly in these circumstances. 

We will thus rely on a matrix-free hybrid multigrid preconditioner. step-50 has already demonstrated the superiority of geometric multigrid methods method when combined with the MatrixFree framework. The application on hp-adaptive FEM requires some additional work though since the children of a cell might have different polynomial degrees. As a remedy, we perform a p-relaxation to linear elements first (similar to Mitchell  [2.x.7232] ) and then perform h-relaxation in the usual manner. On the coarsest level, we apply an algebraic multigrid solver. The combination of p-multigrid, h-multigrid, and AMG makes the solver to a hybrid multigrid solver. 

We will create a custom hybrid multigrid preconditioner with the special level requirements as described above with the help of the existing global-coarsening infrastructure via the use of MGTransferGlobalCoarsening. 




[1.x.2800] 

For elliptic equations, each reentrant corner typically invokes a singularity  [2.x.7233] . We can use this circumstance to put our hp-decision algorithms to a test: on all cells to be adapted, we would prefer a fine grid near the singularity, and a high polynomial degree otherwise. 

As the simplest elliptic problem to solve under these conditions, we chose the Laplace equation in a L-shaped domain with the reentrant corner in the origin of the coordinate system. 

To be able to determine the actual error, we manufacture a boundary value problem with a known solution. On the above mentioned domain, one solution to the Laplace equation is, in polar coordinates,  [2.x.7234] : 

[1.x.2801] 



See also  [2.x.7235]  or  [2.x.7236] . The solution looks as follows: 

<div style="text-align:center;">   <img src="https://www.dealii.org/images/steps/developer/step-75.solution.svg"        alt="Analytic solution."> </div> 

The singularity becomes obvious by investigating the solution's gradient in the vicinity of the reentrant corner, i.e., the origin 

[1.x.2802] 



As we know where the singularity will be located, we expect that our hp-decision algorithm decides for a fine grid resolution in this particular region, and high polynomial degree anywhere else. 

So let's see if that is actually the case, and how hp-adaptation performs compared to pure h-adaptation. But first let us have a detailed look at the actual code. 


examples/step-75/doc/results.dox 



[1.x.2803] 

When you run the program with the given parameters on four processes in release mode, your terminal output should look like this: 

[1.x.2804] 



When running the code with more processes, you will notice slight differences in the number of active cells and degrees of freedom. This is due to the fact that solver and preconditioner depend on the partitioning of the problem, which might yield to slight differences of the solution in the last digits and ultimately yields to different adaptation behavior. 

Furthermore, the number of iterations for the solver stays about the same in all cycles despite hp-adaptation, indicating the robustness of the proposed algorithms and promising good scalability for even larger problem sizes and on more processes. 

Let us have a look at the graphical output of the program. After all refinement cycles in the given parameter configuration, the actual discretized function space looks like the following with its partitioning on twelve processes on the left and the polynomial degrees of finite elements on the right. In the left picture, each color represents a unique subdomain. In the right picture, the lightest color corresponds to the polynomial degree two and the darkest one corresponds to degree six: 

<div class="twocolumn" style="width: 80%; text-align: center;">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-75.subdomains-07.svg"          alt="Partitioning after seven refinements.">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-75.fedegrees-07.svg"          alt="Local approximation degrees after seven refinements.">   </div> </div> 




[1.x.2805] 

[1.x.2806] 

[1.x.2807] 

The deal.II library offers multiple strategies to decide which type of adaptation to impose on cells: either adjust the grid resolution or change the polynomial degree. We only presented the [1.x.2808] strategy in this tutorial, while step-27 demonstrated the [1.x.2809] equivalent of the same idea. 

See the "possibilities for extensions" section of step-27 for an overview over these strategies, or the corresponding documentation for a detailed description. 

There, another strategy is mentioned that has not been shown in any tutorial so far: the strategy based on [1.x.2810]. The usage of this method for parallel distributed applications is more tricky than the others, so we will highlight the challenges that come along with it. We need information about the final state of refinement flags, and we need to transfer the solution across refined meshes. For the former, we need to attach the  [2.x.7237]  function to the  [2.x.7238]  signal in a way that it will be called [1.x.2811] the  [2.x.7239]  function. At this stage, all refinement flags and future FE indices are terminally set and a reliable prediction of the error is possible. The predicted error then needs to be transferred across refined meshes with the aid of  [2.x.7240]  

Try implementing one of these strategies into this tutorial and observe the subtle changes to the results. You will notice that all strategies are capable of identifying the singularities near the reentrant corners and will perform  [2.x.7241] -refinement in these regions, while preferring  [2.x.7242] -refinement in the bulk domain. A detailed comparison of these strategies is presented in  [2.x.7243]  . 




[1.x.2812] 

This tutorial focuses solely on matrix-free strategies. All hp-adaptive algorithms however also work with matrix-based approaches in the parallel distributed context. 

To create a system matrix, you can either use the  [2.x.7244]  function, or use an  [2.x.7245]  function similar to the one of step-27. You can then pass the system matrix to the solver as usual. 

You can time the results of both matrix-based and matrix-free implementations, quantify the speed-up, and convince yourself which variant is faster. 




[1.x.2813] 

For sake of simplicity, we have restricted ourselves to a single type of coarse-grid solver (CG with AMG), smoother (Chebyshev smoother with point Jacobi preconditioner), and geometric-coarsening scheme (global coarsening) within the multigrid algorithm. Feel free to try out alternatives and investigate their performance and robustness. 


examples/step-76/doc/intro.dox 



 [2.x.7246]  

[1.x.2814] 

[1.x.2815] 

[1.x.2816] 

This tutorial program solves the Euler equations of fluid dynamics, using an explicit time integrator with the matrix-free framework applied to a high-order discontinuous Galerkin discretization in space. The numerical approach used here is identical to that used in step-67, however, we utilize different advanced MatrixFree techniques to reach even a higher throughput. 

The two main features of this tutorial are: 

- the usage of shared-memory features from MPI-3.0 and 

- the usage of cell-centric loops, which allow to write to the global vector only   once and, therefore, are ideal for the usage of shared memory. 

Further topics we discuss in this tutorial are the usage and benefits of the template argument VectorizedArrayType (instead of simply using VectorizedArray<Number>) as well as the possibility to pass lambdas to MatrixFree loops. 

For details on the numerics, we refer to the documentation of step-67. We concentrate here only on the key differences. 

[1.x.2817] 

[1.x.2818] 

There exist many shared-memory libraries that are based on threads like TBB, OpenMP, or TaskFlow. Integrating such libraries into existing MPI programs allows one to use shared memory. However, these libraries come with an overhead for the programmer, since all parallelizable code sections have to be found and transformed according to the library used, including the difficulty when some third-party numerical library, like an iterative solver package, only relies on MPI. 

Considering a purely MPI-parallelized FEM application, one can identify that the major time and memory benefit of using shared memory would come from accessing the part of the solution vector owned by the processes on the same compute node without the need to make explicit copies and buffering them. Fur this propose, MPI-3.0 provides shared-memory features based on so-called windows, where processes can directly access the data of the neighbors on the same shared-memory domain. 

[1.x.2819] 

A few relevant MPI-3.0 commands are worth discussing in detail. A new MPI communicator  [2.x.7247] , which consists of processes from the communicator  [2.x.7248]  that have access to the same shared memory, can be created via: 

[1.x.2820] 



The following code snippet shows the simplified allocation routines of shared memory for the value type  [2.x.7249]  and the size  [2.x.7250] , as well as, how to query pointers to the data belonging to processes in the same shared-memory domain: 

[1.x.2821] 



Once the data is not needed anymore, the window has to be freed, which also frees the locally-owned data: 

[1.x.2822] 



[1.x.2823] 

The commands mentioned in the last section are integrated into  [2.x.7251]  and are used to allocate shared memory if an optional (second) communicator is provided to the reinit()-functions. 

For example, a vector can be set up with a partitioner (containing the global communicator) and a sub-communicator (containing the processes on the same compute node): 

[1.x.2824] 



Locally owned values and ghost values can be processed as usual. However, now users also have read access to the values of the shared-memory neighbors via the function: 

[1.x.2825] 



[1.x.2826] 

While  [2.x.7252]  provides the option to allocate shared memory and to access the values of shared memory of neighboring processes in a coordinated way, it does not actually exploit the benefits of the usage of shared memory itself. 

The MatrixFree infrastructure, however, does: 

- On the one hand, within the matrix-free loops  [2.x.7253]     [2.x.7254]  and  [2.x.7255]  only ghost   values that need to be updated  [2.x.7256] are [2.x.7257]  updated. Ghost values from   shared-memory neighbors can be accessed directly, making buffering, i.e.,   copying of the values into the ghost region of a vector possibly redundant.   To deal with possible race conditions, necessary synchronizations are   performed within MatrixFree. In the case that values have to be buffered,   values are copied directly from the neighboring shared-memory process,   bypassing more expensive MPI operations based on  [2.x.7258]  and    [2.x.7259] . 

- On the other hand, classes like FEEvaluation and FEFaceEvaluation can read   directly from the shared memory, so buffering the values is indeed   not necessary in certain cases. 

To be able to use the shared-memory capabilities of MatrixFree, MatrixFree has to be appropriately configured by providing the user-created sub-communicator: 

[1.x.2827] 






[1.x.2828] 

[1.x.2829] 

"Face-centric loops" (short FCL) visit cells and faces (inner and boundary ones) in separate loops. As a consequence, each entity is visited only once and fluxes between cells are evaluated only once. How to perform face-centric loops with the help of  [2.x.7260]  by providing three functions (one for the cell integrals, one for the inner, and one for the boundary faces) has been presented in step-59 and step-67. 

"Cell-centric loops" (short CCL or ECL (for element-centric loops) in the hyper.deal release paper), in contrast, process a cell and in direct succession process all its faces (i.e., visit all faces twice). Their benefit has become clear for modern CPU processor architecture in the literature  [2.x.7261] , although this kind of loop implies that fluxes have to be computed twice (for each side of an interior face). CCL has two primary advantages: 

- On the one hand, entries in the solution vector are written exactly once   back to main memory in the case of CCL, while in the case of FCL at least once   despite of cache-efficient scheduling of cell and face loops-due to cache   capacity misses. 

- On the other hand, since each entry of the solution vector is accessed exactly   once, no synchronization between threads is needed while accessing the solution   vector in the case of CCL. This absence of race conditions during writing into   the destination vector makes CCL particularly suitable for shared-memory   parallelization. 

One should also note that although fluxes are computed twice in the case of CCL, this does not automatically translate into doubling of the computation, since values already interpolated to the cell quadrature points can be interpolated to a face with a simple 1D interpolation. 

[1.x.2830] 

For cell-centric loop implementations, the function  [2.x.7262]  can be used, to which the user can pass a function that should be performed on each cell. 

To derive an appropriate function, which can be passed in  [2.x.7263]  one might, in principle, transform/merge the following three functions, which can be passed to a  [2.x.7264]  

[1.x.2831] 



in the following way: 

[1.x.2832] 



It should be noted that FEFaceEvaluation is initialized now with two numbers, the cell number and the local face number. The given example only highlights how to transform face-centric loops into cell-centric loops and is by no means efficient, since data is read and written multiple times from and to the global vector as well as computations are performed redundantly. Below, we will discuss advanced techniques that target these issues. 

To be able to use  [2.x.7265]  following flags of  [2.x.7266]  have to be enabled: 

[1.x.2833] 



In particular, these flags enable that the internal data structures are set up for all faces of the cells. 

Currently, cell-centric loops in deal.II only work for uniformly refined meshes and if no constraints are applied (which is the standard case DG is normally used). 




[1.x.2834] 

The examples given above have already used lambdas, which have been provided to matrix-free loops. The following short examples present how to transform functions between a version where a class and a pointer to one of its methods are used and a variant where lambdas are utilized. 

In the following code, a class and a pointer to one of its methods, which should be interpreted as cell integral, are passed to  [2.x.7267]  

[1.x.2835] 



[1.x.2836] 



However, it is also possible to pass an anonymous function via a lambda function with the same result: 

[1.x.2837] 



[1.x.2838] 

The class VectorizedArray<Number> is a key component to achieve the high node-level performance of the matrix-free algorithms in deal.II. It is a wrapper class around a short vector of  [2.x.7268]  entries of type Number and maps arithmetic operations to appropriate single-instruction/multiple-data (SIMD) concepts by intrinsic functions. The length of the vector can be queried by  [2.x.7269]  and its underlying number type by  [2.x.7270]  

In the default case ( [2.x.7271] ), the vector length is set at compile time of the library to match the highest value supported by the given processor architecture. However, also a second optional template argument can be specified as  [2.x.7272]  explicitly controls the  vector length within the capabilities of a particular instruction set. A full list of supported vector lengths is presented in the following table: 

 [2.x.7273]  

This allows users to select the vector length/ISA and, as a consequence, the number of cells to be processed at once in matrix-free operator evaluations, possibly reducing the pressure on the caches, an severe issue for very high degrees (and dimensions). 

A possible further reason to reduce the number of filled lanes is to simplify debugging: instead of having to look at, e.g., 8 cells, one can concentrate on a single cell. 

The interface of VectorizedArray also enables the replacement by any type with a matching interface. Specifically, this prepares deal.II for the  [2.x.7274]  class that is planned to become part of the C++23 standard. The following table compares the deal.II-specific SIMD classes and the equivalent C++23 classes: 


 [2.x.7275]  


examples/step-76/doc/results.dox 



[1.x.2839] 

Running the program with the default settings on a machine with 40 processes produces the following output: 

[1.x.2840] 



and the following visual output: 

 [2.x.7276]  

As a reference, the results of step-67 using FCL are: 

[1.x.2841] 



By the modifications shown in this tutorial, we were able to achieve a speedup of 27% for the Runge-Kutta stages. 

[1.x.2842] 

The algorithms are easily extendable to higher dimensions: a high-dimensional [1.x.2843] is part of the hyper.deal library. An extension of cell-centric loops to locally-refined meshes is more involved. 

[1.x.2844] 

The solver presented in this tutorial program can also be extended to the compressible Navier–Stokes equations by adding viscous terms, as also suggested in step-67. To keep as much of the performance obtained here despite the additional cost of elliptic terms, e.g. via an interior penalty method, that tutorial has proposed to switch the basis from FE_DGQ to FE_DGQHermite like in the step-59 tutorial program. The reasoning behind this switch is that in the case of FE_DGQ all values of neighboring cells (i.e.,  [2.x.7277]  layers) are needed, whilst in the case of FE_DGQHermite only 2 layers, making the latter significantly more suitable for higher degrees. The additional layers have to be, on the one hand, loaded from main memory during flux computation and, one the other hand, have to be communicated. Using the shared-memory capabilities introduced in this tutorial, the second point can be eliminated on a single compute node or its influence can be reduced in a hybrid context. 

[1.x.2845] 

Cell-centric loops could be used to create block Gauss-Seidel preconditioners that are multiplicative within one process and additive over processes. These type of preconditioners use during flux computation, in contrast to Jacobi-type preconditioners, already updated values from neighboring cells. The following pseudo-code visualizes how this could in principal be achieved: 

[1.x.2846] 



For this purpose, one can exploit the cell-data vector capabilities of MatrixFree and the range-based iteration capabilities of VectorizedArray. 

Please note that in the given example we process  [2.x.7278]  number of blocks, since each lane corresponds to one block. We consider blocks as updated if all blocks processed by a vector register have been updated. In the case of Cartesian meshes this is a reasonable approach, however, for general unstructured meshes this conservative approach might lead to a decrease in the efficiency of the preconditioner. A reduction of cells processed in parallel by explicitly reducing the number of lanes used by  [2.x.7279]  might increase the quality of the preconditioner, but with the cost that each iteration might be more expensive. This dilemma leads us to a further "possibility for extension": vectorization within an element. 


examples/step-77/doc/intro.dox 

 [2.x.7280]  

[1.x.2847]  [2.x.7281]  

[1.x.2848] 

[1.x.2849] 

The step-15 program solved the following, nonlinear equation describing the minimal surface problem: 

[1.x.2850] 

step-15 uses a Newton method, and Newton's method works by repeatedly solving a *linearized* problem for an update  [2.x.7282]  -- called the "search direction" --, computing a "step length"  [2.x.7283] , and then combining them to compute the new guess for the solution via 

[1.x.2851] 



In the course of the discussions in step-15, we found that it is awkward to compute the step length, and so just settled for simple choice: Always choose  [2.x.7284] . This is of course not efficient: We know that we can only realize Newton's quadratic convergence rate if we eventually are able to choose  [2.x.7285] , though we may have to choose it smaller for the first few iterations where we are still too far away to use this long a step length. 

Among the goals of this program is therefore to address this shortcoming. Since line search algorithms are not entirely trivial to implement, one does as one should do anyway: Import complicated functionality from an external library. To this end, we will make use of the interfaces deal.II has to one of the big nonlinear solver packages, namely the [KINSOL](https://computing.llnl.gov/projects/sundials/kinsol) sub-package of the [SUNDIALS](https://computing.llnl.gov/projects/sundials) suite. %SUNDIALS is, at its heart, a package meant to solve complex ordinary differential equations (ODEs) and differential-algebraic equations (DAEs), and the deal.II interfaces allow for this via the classes in the SUNDIALS namespace: Notably the  [2.x.7286]  and  [2.x.7287]  classes. But, because that is an important step in the solution of ODEs and DAEs with implicit methods, %SUNDIALS also has a solver for nonlinear problems called KINSOL, and deal.II has an interface to it in the form of the  [2.x.7288]  class. This is what we will use for the solution of our problem. 

But %SUNDIALS isn't just a convenient way for us to avoid writing a line search algorithm. In general, the solution of nonlinear problems is quite expensive, and one typically wants to save as much compute time as possible. One way one can achieve this is as follows: The algorithm in step-15 discretizes the problem and then in every iteration solves a linear system of the form 

[1.x.2852] 

where  [2.x.7289]  is the residual vector computed using the current vector of nodal values  [2.x.7290] ,  [2.x.7291]  is its derivative (called the "Jacobian"), and  [2.x.7292]  is the update vector that corresponds to the function  [2.x.7293]  mentioned above. The construction of  [2.x.7294]  has been thoroughly discussed in step-15, as has the way to solve the linear system in each Newton iteration. So let us focus on another aspect of the nonlinear solution procedure: Computing  [2.x.7295]  is expensive, and assembling the matrix  [2.x.7296]  even more so. Do we actually need to do that in every iteration? It turns out that in many applications, this is not actually necessary: These methods often converge even if we replace  [2.x.7297]  by an approximation  [2.x.7298]  and solve 

[1.x.2853] 

instead, then update 

[1.x.2854] 

This may require an iteration or two more because our update  [2.x.7299]  is not quite as good as  [2.x.7300] , but it may still be a win because we don't have to assemble  [2.x.7301]  quite as often. 

What kind of approximation  [2.x.7302]  would we like for  [2.x.7303] ? Theory says that as  [2.x.7304]  converges to the exact solution  [2.x.7305] , we need to ensure that  [2.x.7306]  needs to converge to  [2.x.7307] . In particular, since  [2.x.7308] , a valid choice is  [2.x.7309] . But so is choosing  [2.x.7310]  every, say, fifth iteration  [2.x.7311]  and for the other iterations, we choose  [2.x.7312]  equal to the last computed  [2.x.7313] . This is what we will do here: we will just re-use  [2.x.7314]  from the previous iteration, which may again be what we had used in the iteration before that,  [2.x.7315] . 

This scheme becomes even more interesting if, for the solution of the linear system with  [2.x.7316] , we don't just have to assemble a matrix, but also compute a good preconditioner. For example, if we were to use a sparse LU decomposition via the SparseDirectUMFPACK class, or used a geometric or algebraic multigrid. In those cases, we would also not have to update the preconditioner, whose computation may have taken about as long or longer than the assembly of the matrix in the first place. Indeed, with this mindset, we should probably think about using the *best* preconditioner we can think of, even though their construction is typically quite expensive: We will hope to amortize the cost of computing this preconditioner by applying it to more than one just one linear solve. 

The big question is, of course: By what criterion do we decide whether we can get away with the approximation  [2.x.7317]  based on a previously computed Jacobian matrix  [2.x.7318]  that goes back  [2.x.7319]  steps, or whether we need to -- at least in this iteration -- actually re-compute the Jacobian  [2.x.7320]  and the corresponding preconditioner? This is, like the issue with line search, one that requires a non-trivial amount of code that monitors the convergence of the overall algorithm. We *could* implement these sorts of things ourselves, but we probably *shouldn't*: KINSOL already does that for us. It will tell our code when to "update" the Jacobian matrix. 

One last consideration if we were to use an iterative solver instead of the sparse direct one mentioned above: Not only is it possible to get away with replacing  [2.x.7321]  by some approximation  [2.x.7322]  when solving for the update  [2.x.7323] , but one can also ask whether it is necessary to solve the linear system 

[1.x.2855] 

to high accuracy. The thinking goes like this: While our current solution  [2.x.7324]  is still far away from  [2.x.7325] , why would we solve this linear system particularly accurately? The update  [2.x.7326]  is likely still going to be far away from the exact solution, so why spend much time on solving the linear system to great accuracy? This is the kind of thinking that underlies algorithms such as the "Eisenstat-Walker trick"  [2.x.7327]  in which one is given a tolerance to which the linear system above in iteration  [2.x.7328]  has to be solved, with this tolerance dependent on the progress in the overall nonlinear solver. As before, one could try to implement this oneself, but KINSOL already provides this kind of information for us -- though we will not use it in this program since we use a direct solver that requires no solver tolerance and just solves the linear system exactly up to round-off. 

As a summary of all of these considerations, we could say the following: There is no need to reinvent the wheel. Just like deal.II provides a vast amount of finite-element functionality, %SUNDIALS' KINSOL package provides a vast amount of nonlinear solver functionality, and we better use it. 




[1.x.2856] 

KINSOL, like many similar packages, works in a pretty abstract way. At its core, it sees a nonlinear problem of the form 

[1.x.2857] 

and constructs a sequence of iterates  [2.x.7329]  which, in general, are vectors of the same length as the vector returned by the function  [2.x.7330] . To do this, there are a few things it needs from the user: 

- A way to resize a given vector to the correct size. 

- A way to evaluate, for a given vector  [2.x.7331] , the function  [2.x.7332] . This   function is generally called the "residual" operation because the   goal is of course to find a point  [2.x.7333]  for which  [2.x.7334] ;   if  [2.x.7335]  returns a nonzero vector, then this is the   [1.x.2858]   (i.e., the "rest", or whatever is "left over"). The function   that will do this is in essence the same as the computation of   the right hand side vector in step-15, but with an important difference:   There, the right hand side denoted the *negative* of the residual,   so we have to switch a sign. 

- A way to compute the matrix  [2.x.7336]  if that is necessary in the   current iteration, along with possibly a preconditioner or other   data structures (e.g., a sparse decomposition via   SparseDirectUMFPACK if that's what we choose to use to solve a   linear system). This operation will generally be called the   "setup" operation. 

- A way to solve a linear system  [2.x.7337]  with whatever   matrix  [2.x.7338]  was last computed. This operation will generally   be called the "solve" operation. 

All of these operations need to be provided to KINSOL by  [2.x.7339]  objects that take the appropriate set of arguments and that generally return an integer that indicates success (a zero return value) or failure (a nonzero return value). Specifically, the objects we will access are the  [2.x.7340]   [2.x.7341]   [2.x.7342]  and  [2.x.7343]  member variables. (See the documentation of these variables for their details.) In our implementation, we will use [lambda functions](https://en.cppreference.com/w/cpp/language/lambda) to implement these "callbacks" that in turn can call member functions; KINSOL will then call these callbacks whenever its internal algorithms think it is useful. 




[1.x.2859] 

The majority of the code of this tutorial program is as in step-15, and we will not comment on it in much detail. There is really just one aspect one has to pay some attention to, namely how to compute  [2.x.7344]  given a vector  [2.x.7345]  on the one hand, and  [2.x.7346]  given a vector  [2.x.7347]  separately. At first, this seems trivial: We just take the `assemble_system()` function and in the one case throw out all code that deals with the matrix and in the other case with the right hand side vector. There: Problem solved. 

But it isn't quite as simple. That's because the two are not independent if we have nonzero Dirichlet boundary values, as we do here. The linear system we want to solve contains both interior and boundary degrees of freedom, and when eliminating those degrees of freedom from those that are truly "free", using for example  [2.x.7348]  we need to know the matrix when assembling the right hand side vector. 

Of course, this completely contravenes the original intent: To *not* assemble the matrix if we can get away without it. We solve this problem as follows: 

- We set the starting guess for the solution vector,  [2.x.7349] , to one   where boundary degrees of freedom already have their correct values. 

- This implies that all updates can have zero updates for these   degrees of freedom, and we can build both residual vectors  [2.x.7350]    and Jacobian matrices  [2.x.7351]  that corresponds to linear systems whose   solutions are zero in these vector components. For this special   case, the assembly of matrix and right hand side vectors is   independent, and can be broken into separate functions. 

There is an assumption here that whenever KINSOL asks for a linear solver with the (approximation of the) Jacobian, that this will be for for an update  [2.x.7352]  (which has zero boundary values), a multiple of which will be added to the solution (which already has the right boundary values).  This may not be true and if so, we might have to rethink our approach. That said, it turns out that in practice this is exactly what KINSOL does when using a Newton method, and so our approach is successful. 


examples/step-77/doc/results.dox 



[1.x.2860] 

When running the program, you get output that looks like this: 

[1.x.2861] 



The way this should be interpreted is most easily explained by looking at the first few lines of the output on the first mesh: 

[1.x.2862] 

What is happening is this: 

- In the first residual computation, KINSOL computes the residual to see whether   the desired tolerance has been reached. The answer is no, so it requests the   user program to compute the Jacobian matrix (and the function then also   factorizes the matrix via SparseDirectUMFPACK). 

- KINSOL then instructs us to solve a linear system of the form    [2.x.7353]  with this matrix and the previously computed   residual vector. 

- It is then time to determine how far we want to go in this direction,   i.e., do line search. To this end, KINSOL requires us to compute the   residual vector  [2.x.7354]  for different step lengths    [2.x.7355] . For the first step above, it finds an acceptable  [2.x.7356]    after two tries, the second time around it takes three tries. 

- Having found a suitable updated solution  [2.x.7357] , the process is   repeated except now KINSOL is happy with the current Jacobian matrix   and does not instruct us to re-build the matrix and its factorization,   and instead asks us to solve a linear system with that same matrix. 

The program also writes the solution to a VTU file at the end of each mesh refinement cycle, and it looks as follows:  [2.x.7358]  


The key takeaway messages of this program are the following: 

- The solution is the same as the one we computed in step-15, i.e., the   interfaces to %SUNDIALS' KINSOL package really did what they were supposed   to do. This should not come as a surprise, but the important point is that   we don't have to spend the time implementing the complex algorithms that   underlie advanced nonlinear solvers ourselves. 

- KINSOL is able to avoid all sorts of operations such as rebuilding the   Jacobian matrix when that is not actually necessary. Comparing the   number of linear solves in the output above with the number of times   we rebuild the Jacobian and compute its factorization should make it   clear that this leads to very substantial savings in terms of compute   times, without us having to implement the intricacies of algorithms   that determine when we need to rebuild this information. 

[1.x.2863] 

[1.x.2864] 

For all but the small problems we consider here, a sparse direct solver requires too much time and memory -- we need an iterative solver like we use in many other programs. The trade-off between constructing an expensive preconditioner (say, a geometric or algebraic multigrid method) is different in the current case, however: Since we can re-use the same matrix for numerous linear solves, we can do the same for the preconditioner and putting more work into building a good preconditioner can more easily be justified than if we used it only for a single linear solve as one does for many other situations. 

But iterative solvers also afford other opportunities. For example (and as discussed briefly in the introduction), we may not need to solve to very high accuracy (small tolerances) in early nonlinear iterations as long as we are still far away from the actual solution. This was the basis of the Eisenstat-Walker trick mentioned there. 

KINSOL provides the function that does the linear solution with a target tolerance that needs to be reached. We ignore it in the program above because the direct solver we use does not need a tolerance and instead solves the linear system exactly (up to round-off, of course), but iterative solvers could make use of this kind of information -- and, in fact, should. 


examples/step-78/doc/intro.dox 

[1.x.2865] 

[1.x.2866] 

The Black-Scholes equation is a partial differential equation that falls a bit out of the ordinary scheme. It describes what the fair price of a "European call" stock option is. Without going into too much detail, a stock "option" is a contract one can buy from a bank that allows me, but not requires me, to buy a specific stock at a fixed price  [2.x.7359]  at a fixed future time  [2.x.7360]  in the future. The question one would then want to answer as a buyer of such an option is "How much do I think such a contract is worth?", or as the seller "How much do I need to charge for this contract?", both as a function of the time  [2.x.7361]  before the contract is up at time  [2.x.7362]  and as a function of the stock price  [2.x.7363] . Fischer Black and Myron Scholes derived a partial differential equation for the fair price  [2.x.7364]  for such options under the assumption that stock prices exhibit random price fluctuations with a given level of "volatility" plus a background exponential price increase (which one can think of as the inflation rate that simply devalues all money over time). For their work, Black and Scholes received the Nobel Prize in Economic Sciences in 1997, making this the first tutorial program dealing with a problem for which someone has gotten a Nobel Prize  [2.x.7365] . 

The equation reads as follows: 

[1.x.2867] 

where 

[1.x.2868] 



The way we should interpret this equation is that it is a time-dependent partial differential equation of one "space" variable  [2.x.7366]  as the price of the stock, and  [2.x.7367]  is the price of the option at time  [2.x.7368]  if the stock price at that time were  [2.x.7369] . 

[1.x.2869] 

There are a number of oddities in this equation that are worth discussing before moving on to its numerical solution. First, the "spatial" domain  [2.x.7370]  is unbounded, and thus  [2.x.7371]  can be unbounded in value. This is because there may be a practical upper bound for stock prices, but not a conceptual one. The boundary conditions  [2.x.7372]  as  [2.x.7373]  can then be interpreted as follows: What is the value of an option that allows me to buy a stock at price  [2.x.7374]  if the stock price (today or at time  [2.x.7375] ) is  [2.x.7376] ? One would expect that it is  [2.x.7377]  plus some adjustment for inflation, or, if we really truly consider huge values of  [2.x.7378] , we can neglect  [2.x.7379]  and arrive at the statement that the boundary values at the infinite boundary should be of the form  [2.x.7380]  as stated above. 

In practice, for us to use a finite element method to solve this, we are going to need to bound  [2.x.7381] . Since this equation describes prices, and it doesn't make sense to talk about prices being negative, we will set the lower bound of  [2.x.7382]  to be 0. Then, for an upper bound, we will choose a very large number, one that  [2.x.7383]  is not very likely to ever get to. We will call this  [2.x.7384] . So,  [2.x.7385] . 

Second, after truncating the domain, we need to ask what boundary values we should pose at this now finite boundary. To take care of this, we use "put-call" parity  [2.x.7386] . A "pull option" is one in which we are allowed, but not required, to *sell* a stock at price  [2.x.7387]  to someone at a future time  [2.x.7388] . This says 

[1.x.2870] 

where  [2.x.7389]  is the value of the call option, and  [2.x.7390]  is the value of the put option. Since we expect  [2.x.7391]  as  [2.x.7392] , this says 

[1.x.2871] 

and we can use this as a reasonable boundary condition at our finite point  [2.x.7393] . 

The second complication of the Block-Scholes equation is that we are given a final condition, and not an initial condition. This is because we know what the option is worth at time  [2.x.7394] : If the stock price at  [2.x.7395]  is  [2.x.7396] , then we have no incentive to use our option of buying a price  [2.x.7397]  because we can buy that stock for cheaper on the open market. So  [2.x.7398]  for  [2.x.7399] . On the other hand, if at time  [2.x.7400]  we have  [2.x.7401] , then we can buy the stock at price  [2.x.7402]  via the option and immediately sell it again on the market for price  [2.x.7403] , giving me a profit of  [2.x.7404] . In other words,  [2.x.7405]  for  [2.x.7406] . So, we only know values for  [2.x.7407]  at the *end time* but not the initial time -- in fact, finding out what a fair price at the current time (conventionally taken to be  [2.x.7408] ) is what solving these equations is all about. 

This means that this is not an equation that is posed going forward in time, but in fact going *backward* in time. Thus it makes sense to solve this problem in reverse by making the change of variables  [2.x.7409]  where now  [2.x.7410]  denotes the time before the strike time  [2.x.7411] . 

With all of this, we finally end up with the following problem: 

[1.x.2872] 



Conceptually, this is an advection-diffusion-reaction problem for the variable  [2.x.7412] : There is both a second-order derivative diffusion term, a first-order derivative advection term, and a zeroth-order reaction term. We can expect this problem to be a little bit forgiving in practice because for realistic values of the coefficients, it is diffusive dominated. But, because of the advective terms in the problem, we will have to be careful with mesh refinement and time step choice. There is also the issue that the diffusion term  is written in a non-conservative form and so integration by parts is not  immediately obvious. This will be discussed in the next section. 

[1.x.2873] 

We will solve this problem using an IMEX method. In particular, we first discretize in time with the theta method and will later pick different values of theta for the advective and diffusive terms. Let  [2.x.7413]  approximate  [2.x.7414] : 

[1.x.2874] 

Here,  [2.x.7415]  is the time step size. Given this time discretization, we can proceed to discretize space by multiplying with test functions and then integrating by parts. Because there are some interesting details in this due to the advective and non-advective terms in this equation, this process will be explained in detail. 

So, we begin by multiplying by test functions,  [2.x.7416] : 

[1.x.2875] 




As usual, (1) becomes  [2.x.7417]  and (4) becomes  [2.x.7418] , where  [2.x.7419] , and where we have taken the liberty of denoting by  [2.x.7420]  not only the function  [2.x.7421]  but also the vector of nodal values after discretization. 

The interesting parts come from (2) and (3). 


For (2), we have: 

[1.x.2876] 



There are two integrals here, that are more or less the same, with the differences being a slightly different coefficient in front of the integral, and a different time step for V. Therefore, we will outline this integral in the general case, and account for the differences at the end. So, consider the general integral, which we will solve using integration by parts: 

[1.x.2877] 



So, after adding in the constants and exchanging  [2.x.7422]  for  [2.x.7423]  where applicable, we arrive at the following for (2): 

[1.x.2878] 

But, because the matrix  [2.x.7424]  involves an advective term, we will choose  [2.x.7425]  there -- in other words, we use an explicit Euler method to treat advection. Conversely, since the matrix  [2.x.7426]  involves the diffusive term, we will choose  [2.x.7427]  there -- i.e., we treat diffusion using the second order Crank-Nicolson method. 

So, we arrive at the following: 

[1.x.2879] 



Now, to handle (3). For this, we will again proceed by considering the general case like above. 

[1.x.2880] 



So, again after adding in the constants and exchanging  [2.x.7428]  for  [2.x.7429]  where applicable, we arrive at the following for (3): 

[1.x.2881] 

Just as before, we will use  [2.x.7430]  for the matrix  [2.x.7431]  and  [2.x.7432]  for the matrix  [2.x.7433] . So, we arrive at the following for (3): 

[1.x.2882] 



Now, putting everything together, we obtain the following discrete form for the Black-Scholes Equation: 

[1.x.2883] 

So, altogether we have: 

[1.x.2884] 



As usual, we can write this with the unknown quantities on the left and the known ones on the right. This leads to the following linear system that would have to be solved in each time step: 

[1.x.2885] 









[1.x.2886] For this program, we will use the Method of Manufactured Solutions (MMS) to test  that it is working correctly. This means that we will choose our solution to be   a certain function similar to step-7. For our case, we will use: 

[1.x.2887] 

This means that, using our PDE, we arrive at the following problem: 

[1.x.2888] 

Where,  [2.x.7434] . This set-up now has right hand sides for the equation itself and for the boundary conditions at  [2.x.7435]  that we did not have before, along with "final" conditions (or, with  [2.x.7436] -time "initial conditions") that do not match the real situation. We will implement this in such a way in the code that it is easy to exchange -- the introduction of the changes above is just meant to enable the  use of a manufactured solution. 

If the program is working correctly, then it should produce (**) as the solution. This does mean that we need to modify our variational form somewhat to account for the non-zero right hand side. 

First, we define the following: 

[1.x.2889] 

So, we arrive at the new equation: 

[1.x.2890] 



We then solve this equation as outlined above. 


examples/step-78/doc/results.dox 



[1.x.2891] 


Below is the output of the program: 

[1.x.2892] 



What is more interesting is the output of the convergence tables. They are outputted into the console, as well into a LaTeX file. The convergence tables are shown above. Here, you can see that the the solution has a convergence rate of  [2.x.7437]  with respect to the  [2.x.7438] -norm, and the solution has a convergence rate of  [2.x.7439]  with respect to the  [2.x.7440] -norm. 


Below is the visualization of the solution. 

<div style="text-align:center;">   <img src="https://www.dealii.org/images/steps/developer/step-78.mms-solution.png"        alt="Solution of the MMS problem."> </div> 


examples/step-79/doc/intro.dox 

[1.x.2893] 

[1.x.2894] 

Topology Optimization of Elastic Media is a technique used to optimize a structure that is bearing some load. Ideally, we would like to minimize the maximum stress placed on a structure by selecting a region  [2.x.7441]  where material is placed. In other words, 

[1.x.2895] 



[1.x.2896] 



[1.x.2897] 



Here,  [2.x.7442]  is the stress within the body that is caused by the external forces  [2.x.7443] , where we have for simplicity assumed that the material is linear-elastic and so  [2.x.7444]  is the stress-strain tensor and  [2.x.7445]  is the small-deformation strain as a function of the displacement  [2.x.7446]  -- see step-8 and step-17 for more on linear elasticity. In the formulation above,  [2.x.7447]  is the maximal amount of material we are willing to provide to build the object. The last of the constraints is the partial differential equation that relates stress  [2.x.7448]  and forces  [2.x.7449]  and is simply the steady-state force balance. 

That said, the infinity norm above creates a problem: As a function of location of material, this objective function is necessarily not differentiable, making prospects of optimization rather bleak. So instead, a common approach in topology optimization is to find an approximate solution by optimizing a related problem: We would like to minimize the strain energy. This is a measure of the potential energy stored in an object due to its deformation, but also works as a measure of total deformation over the structure. 

[1.x.2898] 



[1.x.2899] 



[1.x.2900] 



The value of the objective function is calculated using a finite element method, where the solution is the displacements. This is placed inside of a nonlinear solver loop that solves for a vector denoting placement of material. 

[1.x.2901] 

In actual practice, we can only build objects in which the material is either present, or not present, at any given point -- i.e., we would have an indicator function  [2.x.7450]  that describes the material-filled region and that we want to find through the optimization problem. In this case, the optimization problem becomes combinatorial, and very expensive to solve. Instead, we use an approach called Solid Isotropic Material with Penalization, or SIMP.  [2.x.7451]  

The SIMP method is based on an idea of allowing the material to exist in a location with a density  [2.x.7452]  between 0 and 1. A density of 0 suggests the material is not there, and it is not a part of the structure, while a density of 1 suggests the material is present. Values between 0 and 1 do not reflect a design we can create in the real-world, but allow us to turn the combinatorial problem into a continuous one. One then looks at density values  [2.x.7453] , with the constraint that  [2.x.7454] . The minimum value  [2.x.7455] , typically chosen to be around  [2.x.7456] , avoids the possibility of having an infinite strain energy, but is small enough to provide accurate results. 

The straightforward application of the effect of this "density" on the elasticity of the media would be to simply multiply the stiffness tensor  [2.x.7457]  of the medium by the given density, that is,  [2.x.7458] . However, this approach often gives optimal solutions where density values are far from both 0 and 1. As one wants to find a real-world solution, meaning the material either is present or it is not, a penalty is applied to these in-between values. A simple and effective way to do this is to multiply the stiffness tensor by the density raised to some integer power penalty parameter  [2.x.7459] , so that  [2.x.7460] . This makes density values farther away from 0 or 1 less effective. It has been shown that using  [2.x.7461]  is sufficiently high to create 'black-and-white' solutions: that is, one gets optimal solutions in which material is either present or not present at all points. 

More material should always provide a structure with a lower strain energy, and so the inequality constraint can be viewed as an equality where the total volume used is the maximum volume. 

Using this density idea also allows us to reframe the volume constraint on the optimization problem. Use of SIMP then turns the optimization problem into the following: 

[1.x.2902] 



[1.x.2903] 



[1.x.2904] 



[1.x.2905] 

The final constraint, the balance of linear momentum (which we will refer to as the elasticity equation),  gives a method for finding  [2.x.7462]  and  [2.x.7463]  given the density  [2.x.7464] . 

[1.x.2906] The elasticity equation in the time independent limit reads 

[1.x.2907] 

In the situations we will care about, we will assume that the medium has a linear material response and in that case, we have that 

[1.x.2908] 

In everything we will do below, we will always consider the displacement field  [2.x.7465]  as the only solution variable, rather than considering  [2.x.7466]  and  [2.x.7467]  as solution variables (as is done in mixed formulations). 

Furthermore, we will make the assumption that the material is linear isotropic, in which case the stress-strain tensor can be expressed in terms of the Lam&eacute; parameters  [2.x.7468]  such that 

[1.x.2909] 

See step-8 for how this transformation works. 

Integrating the objective function by parts gives 

[1.x.2910] 

into which the linear elasticity equation can then be substituted, giving 

[1.x.2911] 

Because we are assuming no body forces, this simplifies further to 

[1.x.2912] 

which is the final form of the governing equation that we'll be considering from this point forward. 

[1.x.2913] 

Typically, the solutions to topology optimization problems are mesh-dependent, and as such the problem is ill-posed. This is because fractal structures are often formed as the mesh is refined further. As the mesh gains resolution, the optimal solution typically gains smaller and smaller structures. There are a few competing workarounds to this issue, but the most popular for first order optimization is the sensitivity filter, while second order optimization methods tend to prefer use of a density filter. 

As the filters affect the gradient and Hessian of the strain energy (i.e., the objective function), the choice of filter has an effect on the solution of the problem. The density filter as part of a second order method works by introducing an unfiltered density, which we refer to as  [2.x.7469] , and then requiring that the density be a convolution of the unfiltered density: 

[1.x.2914] 

Here,  [2.x.7470]  is an operator so that  [2.x.7471]  is some kind of average of the values of  [2.x.7472]  in the area around  [2.x.7473]  -- i.e., it is a smoothed version of  [2.x.7474] . 

This prevents checkerboarding; the radius of the filter allows the user to define an effective minimal beam width for the optimal structures we seek to find. 

<div style="text-align:center;">   <img src="https://www.dealii.org/images/steps/developer/step-79.checkerboard.png"        alt="Checkerboarding occurring in an MBB Beam"> </div> 

[1.x.2915] 

The minimization problem is now 

[1.x.2916] 



[1.x.2917] 



[1.x.2918] 



[1.x.2919] 



[1.x.2920] 



The inequality constraints are dealt with by first introducing slack variables, and second using log barriers to ensure that we obtain an interior-point method. The penalty parameter is going to be  [2.x.7475] , and the following slack variables are <ol>      [2.x.7476]   [2.x.7477]  - a slack variable corresponding to the lower bound [2.x.7478]       [2.x.7479]   [2.x.7480]  - a slack variable corresponding to the upper bound. [2.x.7481]   [2.x.7482]  This now gives the following problem: 

[1.x.2921] 



[1.x.2922] 



[1.x.2923] 



[1.x.2924] 



[1.x.2925] 



[1.x.2926] 



With these variables in place, we can then follow the usual approach to solving constrained optimization problems: We introduce a Lagrangian in which we combine the objective function and the constraints by multiplying the constraints by Lagrange multipliers. Specifically, we will use the following symbols for the Lagrange multipliers for the various constraints: <ol>      [2.x.7483]   [2.x.7484] : a Lagrange multiplier corresponding to the     elasticity constraint,  [2.x.7485]       [2.x.7486]   [2.x.7487] : a Lagrange multiplier corresponding to the convolution     filter constraint,  [2.x.7488]       [2.x.7489]   [2.x.7490] : a Lagrange multiplier corresponding to the lower slack variable, and  [2.x.7491]       [2.x.7492]   [2.x.7493] : a Lagrange multiplier corresponding to the upper slack variable.  [2.x.7494]   [2.x.7495]  With these variables, the Lagrangian function reads as follows: 

[1.x.2927] 



The solution of the optimization problem then needs to satisfy what are known as the [Karush-Kuhn-Tucker (KKT) conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions): The derivatives of the Lagrangian with respect to all of its arguments need to be equal to zero, and because we have inequality constraints, we also have "complementarity" conditions. Since we here have an infinite-dimensional problem, these conditions all involve directional derivatives of the Lagrangian with regard to certain test functions -- in other words, all of these conditions have to be stated in weak form as is typically the basis for finite element methods anyway. 

The barrier method allows us to initially weaken the "complementary slackness" as required by the typical KKT conditions. Typically, we would require that  [2.x.7496] , but the barrier formulations give KKT conditions where  [2.x.7497] , where  [2.x.7498]  is our barrier parameter. As part of the barrier method, this parameter must be driven close to 0 to give a good approximation of the original problem. 

In the following, let us state all of these conditions where  [2.x.7499]  is a test function that is naturally paired with variational derivatives of the Lagrangian with respect to the  [2.x.7500]  function. For simplicity, we introduce  [2.x.7501]  to indicate the portion of the boundary where forces are applied, and Neumann boundary conditions are used. 

<ol>  [2.x.7502]  Stationarity: 

[1.x.2928] 



[1.x.2929] 



[1.x.2930] 

 [2.x.7503]   [2.x.7504]  Primal Feasibility: 

[1.x.2931] 



[1.x.2932] 



[1.x.2933] 



[1.x.2934] 

 [2.x.7505]   [2.x.7506] Complementary Slackness: 

[1.x.2935] 



[1.x.2936] 

 [2.x.7507]   [2.x.7508]  Dual Feasibility: 

[1.x.2937] 

 [2.x.7509]   [2.x.7510]  

[1.x.2938] 

The optimality conditions above are, in addition to being convoluted, of a kind that is not easy to solve: They are generally nonlinear, and some of the relationships are also inequalities. We will address the nonlinearity using a Newton method to compute search directions, and come back to how to deal with the inequalities below when talking about step length procedures. 

Newton's method applied to the equations above results in the system of equations listed below. Therein, variational derivatives with respect to the  [2.x.7511]  variable are taken in the  [2.x.7512]  direction. 

<ol>  [2.x.7513]  Stationarity: These equations ensure we are at a critical point of the objective function when constrained. 

Equation 1 

[1.x.2939] 



Equation 2 

[1.x.2940] 



Equation 3 

[1.x.2941] 

 [2.x.7514]  

 [2.x.7515]  Primal Feasibility: These equations ensure the equality constraints are met. 

Equation 4 

[1.x.2942] 



Equation 5 

[1.x.2943] 



Equation 6 

[1.x.2944] 



Equation 7 

[1.x.2945] 

 [2.x.7516]  

 [2.x.7517] Complementary Slackness: These equations essentially ensure the barrier is met - in the final solution, we need  [2.x.7518] . 

Equation 8 

[1.x.2946] 



Equation 9 

[1.x.2947] 

 [2.x.7519]  

 [2.x.7520] Dual Feasibility: The Lagrange multiplier on slacks and slack variables must be kept greater than 0. (This is the only part not implemented in the  [2.x.7521]  function.) 

[1.x.2948] 

 [2.x.7522]   [2.x.7523]  




[1.x.2949] We use a quadrilateral mesh with  [2.x.7524]  elements to discretize the displacement and displacement Lagrange multiplier. Piecewise constant  [2.x.7525]  elements are used to discretize the density, unfiltered density, density slack variables, and multipliers for the slack variables and filter constraint. 

[1.x.2950] 

While most of the discussion above follows traditional and well-known approaches to solving nonlinear optimization problems, it turns out that the problem is actually quite difficult to solve in practice. In particular, it is quite nonlinear and an important question is not just to find search directions  [2.x.7526]  as discussed above based on a Newton method, but that one needs to spend quite a lot of attention to how far one wants to go in this direction. This is often called "line search" and comes down to the question of how to choose the step length  [2.x.7527]  so that we move from the current iterate  [2.x.7528]  to the next iterate  [2.x.7529]  in as efficient a way as possible. It is well understood that we need to eventually choose  [2.x.7530]  to realize the Newton's method's quadratic convergence; however, in the early iterations, taking such a long step might actually make things worse, either by leading to a point that has a worse objective function or at which the constraints are satisfied less well than they are at  [2.x.7531] . 

Very complex algorithms have been proposed to deal with this issue  [2.x.7532]   [2.x.7533] . Here, we implement a watchdog-search algorithm  [2.x.7534] . When discussing this algorithm, we will use the vector  [2.x.7535]  to represent all primal variables - the filtered and unfiltered densities, slack variables and displacement - and use the vector  [2.x.7536]  to represent all of the dual vectors. The (incremental) solution to the nonlinear system of equations stated above will now be referred to as  [2.x.7537]  and  [2.x.7538]  instead of  [2.x.7539] . A merit function (explained in more detail later) is referred to here as  [2.x.7540] . 

The watchdog algorithm applied to a subproblem with a given barrier parameter works in the following way: First, the current iteration is saved as a "watchdog" state, and the merit of the watchdog state is recorded. A maximal feasible Newton step is then taken. If the merit sufficiently decreased from the first step, this new step is accepted. If not, another maximal feasible Newton step is taken, and the merit is again compared to the watchdog merit. If after some number (typically between 5 and 8) of Newton steps, the merit did not adequately decrease, the algorithm takes a scaled Newton step from either the watchdog state or the last iteration that guarantees a sufficient decrease of the merit, and that step is accepted. Once a step is accepted, the norm of the KKT error is measured, and if it is sufficiently small, the barrier value is decreased. If it is not sufficiently small, the last accepted step is taken to be the new watchdog step, and the process is repeated. 


Above, the "maximal feasible step" is a scaling of the Newton step in both the primal and dual variables given by 

[1.x.2951] 



[1.x.2952] 



Above,  [2.x.7541]  is the "fraction to boundary" that is allowed on any step. Because the derivatives become ill-conditioned near the boundary, this technique stands in for a [trust region](https://en.wikipedia.org/wiki/Trust_region) and is necessary to ensure good approximations in the future.  [2.x.7542]  is taken to be  [2.x.7543] , which allows movement closer to the boundary as the barrier becomes smaller. In the future, when implementing the LOQO algorithm for barrier reduction, this must be kept to 0.8 as the barrier parameter can vary wildly. 

Separately, we need to deal with the log-barrier that we have used to enforce the positivity constraint on the slack variables  [2.x.7544] : In the statement of the final optimization problem we solve, we have added the term 

[1.x.2953] 

The question is how we should choose the penalty factor  [2.x.7545] . As with all penalty methods, we are in reality only interested in the limit as  [2.x.7546] , since this is then the problem we really wanted to solve, subject to the positivity constraints on the slack variables. On the other hand, we need to choose  [2.x.7547]  large enough to make the problem solvable in practice. Actual implementations therefore start with a larger value of  [2.x.7548]  and gradually decrease it as the outer iterations proceed. 

In the monotone method implemented here, the barrier parameter is updated whenever some level of convergence is achieved at the current barrier parameter. We use the  [2.x.7549]  norm of the KKT conditions to check for convergence at each barrier size. The requirement is that  [2.x.7550]  where  [2.x.7551]  is a constant over any barrier size and  [2.x.7552]  is the barrier parameter. This forces better convergence in later iterations, and is the same requirement as is used in [IPOPT](https://coin-or.github.io/Ipopt/) (an open source software package for large-scale nonlinear optimization). 

Here, the barrier is reduced linearly at larger values, and superlinearly at smaller values. At larger values, it is multiplied by a constant (around 0.6), and at lower values the barrier value is replaced by the barrier value raised to some exponent (around 1.2). This method has proven to be effective at keeping  the subproblem solvable at large barrier values, while still allowing  superlinear convergence at smaller barrier values. In practice, this looks like  the following: 

[1.x.2954] 



While taking large steps at reducing the barrier size when convergence is reached is widely used, more recent research has shown that it is typically faster to use algorithms that adaptively update barrier each iteration, i.e., methods in which we use concrete criteria at the end of each iteration to determine what the penalty parameter should be in the next iteration, rather than using reduction factors that are independent of the current solution. That said, such methods are also more complicated and we will not do this here. 

[1.x.2955] 

The algorithm outlined above makes use of a "merit function". Merit functions are used to determine whether a step from  [2.x.7553]  to a proposed point  [2.x.7554]  is beneficial. In unconstrained optimization problems, one can simply check this with the objective function we try to minimize, and typically uses conditions such as the [Wolfe and Goldstein conditions](https://en.wikipedia.org/wiki/Wolfe_conditions). 

In constrained optimization problems, the question is how to balance reduction in the objective function against a possible increase in the violation of constraints: A proposed step might make the objective function smaller but be further away from the set of points that satisfy the constraints -- or the other way around. This trade-off is typically resolved by using a merit function that combines the two criteria. 

Here, we use an exact  [2.x.7555]  merit function to test the steps: 

[1.x.2956] 



Here,  [2.x.7556]  is a penalty parameter. This merit function being exact means that there exists some  [2.x.7557]  so that for any  [2.x.7558] , the merit function has its minima at the same location as the original problem. This penalty parameter is updated (by recommendation of Nocedal and Wright  [2.x.7559] ) as follows: 

[1.x.2957] 

where  [2.x.7560]  is the Hessian of the objective function,  [2.x.7561]  is a vector of our decision (primal) variables,  [2.x.7562]  is the objective function, and  [2.x.7563]  is the error on a current equality constraint. 

Our use of this method is partially due to already having most of the necessary parts calculated in finding the right hand side, but also the use of an exact merit function ensures that it is minimized in the same location as the overall problem. Recent research has shown that one can replace merit functions by what are called "filter methods", and one should consider using these instead as they prove to be more efficient. 


examples/step-79/doc/results.dox 



[1.x.2958] 

[1.x.2959] The algorithms used above are tested against a traditional topology optimization  problem called the Messerschmitt-Bolkow-Blohm Beam (MBB Beam). 

This problem considers the optimal 2-d structure that can be built on a rectangle 6 units wide, and 1 unit tall. The bottom corners are fixed in place in the  [2.x.7564]  direction using a zero Dirichlet boundary condition, and a downward force is applied in the center of the top of the beam by enforcing a Neumann boundary condition. The rest of the boundary is allowed to move, and has no external force applied, which takes the form of a zero Neumann boundary condition. In essence, we are asking the following question: How should we design a bridge in a way so that if the bottom left and bottom right point of the bridge are on rollers that allow these points to move horizontally but not vertically, and so that the displacement in response to the vertical force in the center is minimal. 

While the total volume of the domain is 6 units, 3 units of material are allowed for the structure. Because of the symmetry of the problem, it could be posed on a rectangle of width 3 and height 1 by cutting the original domain in half, and using zero Dirichlet boundary conditions in the  [2.x.7565]  direction along the cut edge. That said, symmetry of the solution is a good indicator that the program is working as expected, so we solved the problem on the whole domain, as shown below.  [2.x.7566]  

<div style="text-align:center;">   <img src="https://www.dealii.org/images/steps/developer/step-79.mbbgeometry.png"        alt="The MBB problem domain and boundary conditions"> </div> 


Using the program discussed above, we find the minimum volume of the MBB Beam and the individual components of the solution look as follows: 

<div class="onecolumn" style="width: 80%; text-align: center;">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-79.filtereddensity.png"          alt="Filtered Density Solution">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-79.unfiltereddensity.png"          alt="Unfiltered Density Solution">   </div> </div> 


These pictures show that what we find here is in accordance with what one typically sees in other publications on the topic  [2.x.7567] . Maybe more interestingly, the result looks like a truss bridge (except that we apply the load at the top of the trusses, rather than the bottom as in real truss bridges, akin to a "deck truss" bridge), suggesting that the designs that have been used in bridge- building for centuries are indeed based on ideas we can now show to be optimal in some sense. 




[1.x.2960] 

The results shown above took around 75 iterations to find, which is quite concerning given the expense in solving the large linear systems in each iteration. Looking at the evolution, it does look as though the convergence has moments of happening quickly and moments of happening slowly. We believe this to be due to both a lack of precision on when and how to decrease the boundary values, as well as our choice of merit function being sub-optimal. In the future, a LOQO barrier update replacing the monotone reduction, as well as a Markov Filter in place of a merit function will decrease the number of necessary iterations significantly. 

The barrier decrease is most sensitive in the middle of the convergence, which is problematic, as it seems like we need it to decrease quickly, then slowly, then quickly again. 

Secondly, the linear solver used here is just the sparse direct solver based on the SparseDirectUMFPACK class. This works reasonably well on small problems, but the formulation of the optimization problem detailed above has quite a large number of variables and so the linear problem is not only large but also has a lot of nonzero entries in many rows, even on meshes that overall are still relatively coarse. As a consequence, the solver time dominates the computations, and more sophisticated approaches at solving the linear system are necessary. 


examples/step-8/doc/intro.dox 

[1.x.2961] 

[1.x.2962] 


In real life, most partial differential equations are really systems of equations. Accordingly, the solutions are usually vector-valued. The deal.II library supports such problems (see the extensive documentation in the  [2.x.7568]  module), and we will show that that is mostly rather simple. The only more complicated problems are in assembling matrix and right hand side, but these are easily understood as well. 

 [2.x.7569]  

In this tutorial program we will want to solve the [1.x.2963]. They are an extension to Laplace's equation with a vector-valued solution that describes the displacement in each space direction of a rigid body which is subject to a force. Of course, the force is also vector-valued, meaning that in each point it has a direction and an absolute value. 

One can write the elasticity equations in a number of ways. The one that shows the symmetry with the Laplace equation in the most obvious way is to write it as 

[1.x.2964] 

where  [2.x.7570]  is the vector-valued displacement at each point,  [2.x.7571]  the force, and  [2.x.7572]  is a rank-4 tensor (i.e., it has four indices) that encodes the stress-strain relationship -- in essence, it represents the [1.x.2965] in Hookes law that relates the displacement to the forces.  [2.x.7573]  will, in many cases, depend on  [2.x.7574]  if the body whose deformation we want to simulate is composed of different materials. 

While the form of the equations above is correct, it is not the way they are usually derived. In truth, the gradient of the displacement  [2.x.7575]  (a matrix) has no physical meaning whereas its symmetrized version, 

[1.x.2966] 

does and is typically called the "strain". (Here and in the following,  [2.x.7576] . We will also use the [1.x.2967] that whenever the same index appears twice in an equation, summation over this index is implied; we will, however, not distinguish between upper and lower indices.) With this definition of the strain, the elasticity equations then read as 

[1.x.2968] 

which you can think of as the more natural generalization of the Laplace equation to vector-valued problems. (The form shown first is equivalent to this form because the tensor  [2.x.7577]  has certain symmetries, namely that  [2.x.7578] , and consequently  [2.x.7579] .) 

One can of course alternatively write these equations in component form: 

[1.x.2969] 



In many cases, one knows that the material under consideration is isotropic, in which case by introduction of the two coefficients  [2.x.7580]  and  [2.x.7581]  the coefficient tensor reduces to 

[1.x.2970] 



The elastic equations can then be rewritten in much simpler a form: 

[1.x.2971] 

and the respective bilinear form is then 

[1.x.2972] 

or also writing the first term a sum over components: 

[1.x.2973] 



 [2.x.7582]  As written, the equations above are generally considered to be the right description for the displacement of three-dimensional objects if the displacement is small and we can assume that [1.x.2974] is valid. In that case, the indices  [2.x.7583]  above all run over the set  [2.x.7584]  (or, in the C++ source, over  [2.x.7585] ). However, as is, the program runs in 2d, and while the equations above also make mathematical sense in that case, they would only describe a truly two-dimensional solid. In particular, they are not the appropriate description of an  [2.x.7586]  cross-section of a body infinite in the  [2.x.7587]  direction; this is in contrast to many other two-dimensional equations that can be obtained by assuming that the body has infinite extent in  [2.x.7588] -direction and that the solution function does not depend on the  [2.x.7589]  coordinate. On the other hand, there are equations for two-dimensional models of elasticity; see for example the Wikipedia article on [1.x.2975], [1.x.2976] and [1.x.2977]. 

But let's get back to the original problem. How do we assemble the matrix for such an equation? A very long answer with a number of different alternatives is given in the documentation of the  [2.x.7590]  module. Historically, the solution shown below was the only one available in the early years of the library. It turns out to also be the fastest. On the other hand, if a few per cent of compute time do not matter, there are simpler and probably more intuitive ways to assemble the linear system than the one discussed below but that weren't available until several years after this tutorial program was first written; if you are interested in them, take a look at the  [2.x.7591]  module. 

Let us go back to the question of how to assemble the linear system. The first thing we need is some knowledge about how the shape functions work in the case of vector-valued finite elements. Basically, this comes down to the following: let  [2.x.7592]  be the number of shape functions for the scalar finite element of which we build the vector element (for example, we will use bilinear functions for each component of the vector-valued finite element, so the scalar finite element is the  [2.x.7593]  element which we have used in previous examples already, and  [2.x.7594]  in two space dimensions). Further, let  [2.x.7595]  be the number of shape functions for the vector element; in two space dimensions, we need  [2.x.7596]  shape functions for each component of the vector, so  [2.x.7597] . Then, the  [2.x.7598] th shape function of the vector element has the form 

[1.x.2978] 

where  [2.x.7599]  is the  [2.x.7600] th unit vector,  [2.x.7601]  is the function that tells us which component of  [2.x.7602]  is the one that is nonzero (for each vector shape function, only one component is nonzero, and all others are zero).  [2.x.7603]  describes the space dependence of the shape function, which is taken to be the  [2.x.7604] -th shape function of the scalar element. Of course, while  [2.x.7605]  is in the range  [2.x.7606] , the functions  [2.x.7607]  and  [2.x.7608]  have the ranges  [2.x.7609]  (in 2D) and  [2.x.7610] , respectively. 

For example (though this sequence of shape functions is not guaranteed, and you should not rely on it), the following layout could be used by the library: 

[1.x.2979] 

where here 

[1.x.2980] 



[1.x.2981] 



In all but very rare cases, you will not need to know which shape function  [2.x.7611]  of the scalar element belongs to a shape function  [2.x.7612]  of the vector element. Let us therefore define 

[1.x.2982] 

by which we can write the vector shape function as 

[1.x.2983] 

You can now safely forget about the function  [2.x.7613] , at least for the rest of this example program. 

Now using this vector shape functions, we can write the discrete finite element solution as 

[1.x.2984] 

with scalar coefficients  [2.x.7614] . If we define an analog function  [2.x.7615]  as test function, we can write the discrete problem as follows: Find coefficients  [2.x.7616]  such that 

[1.x.2985] 



If we insert the definition of the bilinear form and the representation of  [2.x.7617]  and  [2.x.7618]  into this formula: 

[1.x.2986] 

We note that here and in the following, the indices  [2.x.7619]  run over spatial directions, i.e.  [2.x.7620] , and that indices  [2.x.7621]  run over degrees of freedom. 

The local stiffness matrix on cell  [2.x.7622]  therefore has the following entries: 

[1.x.2987] 

where  [2.x.7623]  now are local degrees of freedom and therefore  [2.x.7624] . In these formulas, we always take some component of the vector shape functions  [2.x.7625] , which are of course given as follows (see their definition): 

[1.x.2988] 

with the Kronecker symbol  [2.x.7626] . Due to this, we can delete some of the sums over  [2.x.7627]  and  [2.x.7628] : 

[1.x.2989] 



Likewise, the contribution of cell  [2.x.7629]  to the right hand side vector is 

[1.x.2990] 



This is the form in which we will implement the local stiffness matrix and right hand side vectors. 

As a final note: in the step-17 example program, we will revisit the elastic problem laid out here, and will show how to solve it in %parallel on a cluster of computers. The resulting program will thus be able to solve this problem to significantly higher accuracy, and more efficiently if this is required. In addition, in step-20,  [2.x.7630]  "step-21", as well as a few other of the later tutorial programs, we will revisit some vector-valued problems and show a few techniques that may make it simpler to actually go through all the stuff shown above, with  [2.x.7631]  etc. 


examples/step-8/doc/results.dox 



[1.x.2991] 


There is not much to be said about the results of this program, other than that they look nice. All images were made using VisIt from the output files that the program wrote to disk. The first two pictures show the  [2.x.7632] - and  [2.x.7633] -displacements as scalar components: 

 [2.x.7634]  


You can clearly see the sources of  [2.x.7635] -displacement around  [2.x.7636]  and  [2.x.7637] , and of  [2.x.7638] -displacement at the origin. 

What one frequently would like to do is to show the displacement as a vector field, i.e., vectors that for each point illustrate the direction and magnitude of displacement. Unfortunately, that's a bit more involved. To understand why this is so, remember that we have just defined our finite element as a collection of two  components (in  [2.x.7639]  dimensions). Nowhere have we said that this is not just a pressure and a concentration (two scalar quantities) but that the two components actually are the parts of a vector-valued quantity, namely the displacement. Absent this knowledge, the DataOut class assumes that all individual variables we print are separate scalars, and VisIt and Paraview then faithfully assume that this is indeed what it is. In other words, once we have written the data as scalars, there is nothing in these programs that allows us to paste these two scalar fields back together as a vector field. Where we would have to attack this problem is at the root, namely in  [2.x.7640] . We won't do so here but instead refer the reader to the step-22 program where we show how to do this for a more general situation. That said, we couldn't help generating the data anyway that would show how this would look if implemented as discussed in step-22. The vector field then looks like this (VisIt and Paraview randomly select a few hundred vertices from which to draw the vectors; drawing them from each individual vertex would make the picture unreadable): 

 [2.x.7641]  


We note that one may have intuitively expected the solution to be symmetric about the  [2.x.7642] - and  [2.x.7643] -axes since the  [2.x.7644] - and  [2.x.7645] -forces are symmetric with respect to these axes. However, the force considered as a vector is not symmetric and consequently neither is the solution. 


examples/step-9/doc/intro.dox 

[1.x.2992] 

[1.x.2993] 


In this example, our aims are the following: <ol>    [2.x.7646] solve the advection equation  [2.x.7647] ;    [2.x.7648] show how we can use multiple threads to get results quicker if we have a     multi-processor machine;    [2.x.7649] develop a simple refinement criterion.  [2.x.7650]  While the second aim is difficult to describe in general terms without reference to the code, we will discuss the other two aims in the following. The use of multiple threads will then be detailed at the relevant places within the program. We will, however, follow the general discussion of the WorkStream approach detailed in the  [2.x.7651]  "Parallel computing with multiple processors accessing shared memory" documentation module. 




[1.x.2994] 

In the present example program, we want to numerically approximate the solution of the advection equation 

[1.x.2995] 

where  [2.x.7652]  is a vector field that describes the advection direction and speed (which may be dependent on the space variables if  [2.x.7653] ),  [2.x.7654]  is a source function, and  [2.x.7655]  is the solution. The physical process that this equation describes is that of a given flow field  [2.x.7656] , with which another substance is transported, the density or concentration of which is given by  [2.x.7657] . The equation does not contain diffusion of this second species within its carrier substance, but there are source terms. 

It is obvious that at the inflow, the above equation needs to be augmented by boundary conditions: 

[1.x.2996] 

where  [2.x.7658]  describes the inflow portion of the boundary and is formally defined by 

[1.x.2997] 

and  [2.x.7659]  being the outward normal to the domain at point  [2.x.7660] . This definition is quite intuitive, since as  [2.x.7661]  points outward, the scalar product with  [2.x.7662]  can only be negative if the transport direction  [2.x.7663]  points inward, i.e. at the inflow boundary. The mathematical theory states that we must not pose any boundary condition on the outflow part of the boundary. 

Unfortunately, the equation stated above cannot be solved in a stable way using the standard finite element method. The problem is that solutions to this equation possess insufficient regularity perpendicular to the transport direction: while they are smooth along the streamlines defined by the "wind field"  [2.x.7664] , they may be discontinuous perpendicular to this direction. This is easy to understand: what the equation  [2.x.7665]  means is in essence that the [1.x.2998]. But the equation has no implications for the derivatives in the perpendicular direction, and consequently if  [2.x.7666]  is discontinuous at a point on the inflow boundary, then this discontinuity will simply be transported along the streamline of the wind field that starts at this boundary point. These discontinuities lead to numerical instabilities that make a stable solution by a standard continuous finite element discretization impossible. 

A standard approach to address this difficulty is the  [2.x.7667] "streamline-upwind Petrov-Galerkin" [2.x.7668]  (SUPG) method, sometimes also called the streamline diffusion method. A good explanation of the method can be found in  [2.x.7669]  . Formally, this method replaces the step in which we derive the the weak form of the differential equation from the strong form: Instead of multiplying the equation by a test function  [2.x.7670]  and integrating over the domain, we instead multiply by  [2.x.7671] , where  [2.x.7672]  is a parameter that is chosen in the range of the (local) mesh width  [2.x.7673] ; good results are usually obtained by setting  [2.x.7674] . (Why this is called "streamline diffusion" will be explained below; for the moment, let us simply take for granted that this is how we derive a stable discrete formulation.) The value for  [2.x.7675]  here is small enough that we do not introduce excessive diffusion, but large enough that the resulting problem is well-posed. 

Using the test functions as defined above, an initial weak form of the problem would ask for finding a function  [2.x.7676]  so that for all test functions  [2.x.7677]  we have 

[1.x.2999] 

However, we would like to include inflow boundary conditions  [2.x.7678]  weakly into this problem, and this can be done by requiring that in addition to the equation above we also have 

[1.x.3000] 

for all test functions  [2.x.7679]  that live on the boundary and that are from a suitable test space. It turns out that a suitable space of test functions happens to be  [2.x.7680]  times the traces of the functions  [2.x.7681]  in the test space we already use for the differential equation in the domain. Thus, we require that for all test functions  [2.x.7682]  we have 

[1.x.3001] 

Without attempting a justification (see again the literature on the finite element method in general, and the streamline diffusion method in particular), we can combine the equations for the differential equation and the boundary values in the following weak formulation of our stabilized problem: find a discrete function  [2.x.7683]  such that for all discrete test functions  [2.x.7684]  there holds 

[1.x.3002] 




One would think that this leads to a system matrix to be inverted of the form 

[1.x.3003] 

with basis functions  [2.x.7685] .  However, this is a pitfall that happens to every numerical analyst at least once (including the author): we have here expanded the solution  [2.x.7686] , but if we do so, we will have to solve the problem 

[1.x.3004] 

where  [2.x.7687]  is the vector of expansion coefficients, i.e., we have to solve the transpose problem of what we might have expected naively. 

This is a point we made in the introduction of step-3. There, we argued that to avoid this very kind of problem, one should get in the habit of always multiplying with test functions [1.x.3005] instead of from the right to obtain the correct matrix right away. In order to obtain the form of the linear system that we need, it is therefore best to rewrite the weak formulation to 

[1.x.3006] 

and then to obtain 

[1.x.3007] 

as system matrix. We will assemble this matrix in the program. 




[1.x.3008] 

Looking at the bilinear form mentioned above, we see that the discrete solution has to satisfy an equation of which the left hand side in weak form has a domain term of the kind 

[1.x.3009] 

or if we split this up, the form 

[1.x.3010] 

If we wanted to see what strong form of the equation that would correspond to, we need to integrate the second term. This yields the following formulation, where for simplicity we'll ignore boundary terms for now: 

[1.x.3011] 

Let us assume for a moment that the wind field  [2.x.7688]  is divergence-free, i.e., that  [2.x.7689] . Then applying the product rule to the derivative of the term in square brackets on the right and using the divergence-freeness will give us the following: 

[1.x.3012] 

That means that the strong form of the equation would be of the sort 

[1.x.3013] 

What is important to recognize now is that  [2.x.7690]  is the  [2.x.7691] derivative in direction  [2.x.7692]  [2.x.7693] . So, if we denote this by  [2.x.7694]  (in the same way as we often write  [2.x.7695]  for the derivative in normal direction at the boundary), then the strong form of the equation is 

[1.x.3014] 

In other words, the unusual choice of test function is equivalent to the addition of term to the strong form that corresponds to a second order (i.e., diffusion) differential operator in the direction of the wind field  [2.x.7696] , i.e., in "streamline direction". A fuller account would also have to explore the effect of the test function on boundary values and why it is necessary to also use the same test function for the right hand side, but the discussion above might make clear where the name "streamline diffusion" for the method originates from. 




[1.x.3015] 

A "Galerkin method" is one where one obtains the weak formulation by multiplying the equation by a test function  [2.x.7697]  (and then integrating over  [2.x.7698] ) where the functions  [2.x.7699]  are from the same space as the solution  [2.x.7700]  (though possibly with different boundary values). But this is not strictly necessary: One could also imagine choosing the test functions from a different set of functions, as long as that different set has "as many dimensions" as the original set of functions so that we end up with as many independent equations as there are degrees of freedom (where all of this needs to be appropriately defined in the infinite-dimensional case). Methods that make use of this possibility (i.e., choose the set of test functions differently than the set of solutions) are called "Petrov-Galerkin" methods. In the current case, the test functions all have the form  [2.x.7701]  where  [2.x.7702]  is from the set of solutions. 




[1.x.3016] 

[Upwind methods](https://en.wikipedia.org/wiki/Upwind_scheme) have a long history in the derivation of stabilized schemes for advection equations. Generally, the idea is that instead of looking at a function "here", we look at it a small distance further "upstream" or "upwind", i.e., where the information "here" originally came from. This might suggest not considering  [2.x.7703] , but something like  [2.x.7704] . Or, equivalently upon integration, we could evaluate  [2.x.7705]  and instead consider  [2.x.7706]  a bit downstream:  [2.x.7707] . This would be cumbersome for a variety of reasons: First, we would have to define what  [2.x.7708]  should be if  [2.x.7709]  happens to be outside  [2.x.7710] ; second, computing integrals numerically would be much more awkward since we no longer evaluate  [2.x.7711]  and  [2.x.7712]  at the same quadrature points. But since we assume that  [2.x.7713]  is small, we can do a Taylor expansion: 

[1.x.3017] 

This form for the test function should by now look familiar. 




[1.x.3018] 

As the resulting matrix is no longer symmetric positive definite, we cannot use the usual Conjugate Gradient method (implemented in the SolverCG class) to solve the system. Instead, we use the GMRES (Generalized Minimum RESidual) method (implemented in SolverGMRES) that is suitable for problems of the kind we have here. 




[1.x.3019] 

For the problem which we will solve in this tutorial program, we use the following domain and functions (in  [2.x.7714]  space dimensions): 

[1.x.3020] 

For  [2.x.7715] , we extend  [2.x.7716]  and  [2.x.7717]  by simply duplicating the last of the components shown above one more time. 

With all of this, the following comments are in order: <ol>  [2.x.7718]  The advection field  [2.x.7719]  transports the solution roughly in diagonal direction from lower left to upper right, but with a wiggle structure superimposed.  [2.x.7720]  The right hand side adds to the field generated by the inflow boundary conditions a blob in the lower left corner, which is then transported along.  [2.x.7721]  The inflow boundary conditions impose a weighted sinusoidal structure that is transported along with the flow field. Since  [2.x.7722]  on the boundary, the weighting term never gets very large.  [2.x.7723]  




[1.x.3021] 

In all previous examples with adaptive refinement, we have used an error estimator first developed by Kelly et al., which assigns to each cell  [2.x.7724]  the following indicator: 

[1.x.3022] 

where  [2.x.7725]  denotes the jump of the normal derivatives across a face  [2.x.7726]  of the cell  [2.x.7727] . It can be shown that this error indicator uses a discrete analogue of the second derivatives, weighted by a power of the cell size that is adjusted to the linear elements assumed to be in use here: 

[1.x.3023] 

which itself is related to the error size in the energy norm. 

The problem with this error indicator in the present case is that it assumes that the exact solution possesses second derivatives. This is already questionable for solutions to Laplace's problem in some cases, although there most problems allow solutions in  [2.x.7728] . If solutions are only in  [2.x.7729] , then the second derivatives would be singular in some parts (of lower dimension) of the domain and the error indicators would not reduce there under mesh refinement. Thus, the algorithm would continuously refine the cells around these parts, i.e. would refine into points or lines (in 2d). 

However, for the present case, solutions are usually not even in  [2.x.7730]  (and this missing regularity is not the exceptional case as for Laplace's equation), so the error indicator described above is not really applicable. We will thus develop an indicator that is based on a discrete approximation of the gradient. Although the gradient often does not exist, this is the only criterion available to us, at least as long as we use continuous elements as in the present example. To start with, we note that given two cells  [2.x.7731] ,  [2.x.7732]  of which the centers are connected by the vector  [2.x.7733] , we can approximate the directional derivative of a function  [2.x.7734]  as follows: 

[1.x.3024] 

where  [2.x.7735]  and  [2.x.7736]  denote  [2.x.7737]  evaluated at the centers of the respective cells. We now multiply the above approximation by  [2.x.7738]  and sum over all neighbors  [2.x.7739]  of  [2.x.7740] : 

[1.x.3025] 

If the vectors  [2.x.7741]  connecting  [2.x.7742]  with its neighbors span the whole space (i.e. roughly:  [2.x.7743]  has neighbors in all directions), then the term in parentheses in the left hand side expression forms a regular matrix, which we can invert to obtain an approximation of the gradient of  [2.x.7744]  on  [2.x.7745] : 

[1.x.3026] 

We will denote the approximation on the right hand side by  [2.x.7746] , and we will use the following quantity as refinement criterion: 

[1.x.3027] 

which is inspired by the following (not rigorous) argument: 

[1.x.3028] 




examples/step-9/doc/results.dox 



[1.x.3029] 


The results of this program are not particularly spectacular. They consist of the console output, some grid files, and the solution on each of these grids. First for the console output: 

[1.x.3030] 



Quite a number of cells are used on the finest level to resolve the features of the solution. Here are the fourth and tenth grids: <div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-9-grid-3.png"          alt="Fourth grid in the refinement cycle, showing some adaptivity to features."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-9-grid-9.png"          alt="Tenth grid in the refinement cycle, showing that the waves are fully captured."          width="400" height="400">   </div> </div> and the fourth and tenth solutions: <div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-9-solution-3.png"          alt="Fourth solution, showing that we resolve most features but some          are sill unresolved and appear blury."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-9-solution-9.png"          alt="Tenth solution, showing a fully resolved flow."          width="400" height="400">   </div> </div> and both the grid and solution zoomed in: <div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-9-solution-3-zoom.png"          alt="Detail of the fourth solution, showing that we resolve most          features but some are sill unresolved and appear blury. In particular,          the larger cells need to be refined."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-9-solution-9-zoom.png"          alt="Detail of the tenth solution, showing that we needed a lot more          cells than were present in the fourth solution."          width="400" height="400">   </div> </div> 

The solution is created by that part that is transported along the wiggly advection field from the left and lower boundaries to the top right, and the part that is created by the source in the lower left corner, and the results of which are also transported along. The grid shown above is well-adapted to resolve these features. The comparison between plots shows that, even though we are using a high-order approximation, we still need adaptive mesh refinement to fully resolve the wiggles. 


