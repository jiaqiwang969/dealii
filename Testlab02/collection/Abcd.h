examples/step-1/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

<h3> About the tutorial </h3>

Since this is the first tutorial program, let us comment first on how
this tutorial and the rest of the deal.II documentation is supposed to
work. The documentation for deal.II comes essentially at three
different levels:
- The tutorial: This is a collection of programs that shows how
  deal.II is used in practice. It doesn't typically discuss individual
  functions at the level of individual arguments, but rather wants to
  give the big picture of how things work together. In other words, it
  discusses "concepts": what are the building blocks of deal.II and
  how are they used together in finite element programs.
- The manual: This is the documentation of every single class and
  every single (member) function in deal.II. You get there if, for
  example, you click on the "Main page" or "Classes" tab at the top of
  this page. This is the place where you would look up what the second
  argument of Triangulation::create_triangulation_compatibility means,
  to give just one slightly obscure example. You need this level of
  documentation for when you know what you want to do, but forgot how
  exactly the function was named, what its arguments are, or what it
  returns. Note that you also get into the manual whenever you read
  through the tutorial and click on any of the class or function
  names, i.e. the tutorial contains a great many links into the manual
  for whenever you need a more detailed description of a function or
  class. On the other hand, the manual is not a good place to learn
  deal.II since it gives you a microscopic view of things without
  telling you how a function might fit into the bigger picture.
- Modules: These are groups of classes and functions that work
  together or have related functionality. If you click on the
  "Modules" tab at the top of this page, you end up on a page that
  lists a number of such groups. Each module discusses the underlying
  principles of these classes; for example, the @ref Sparsity module
  talks about all sorts of different issues related to storing
  sparsity patterns of matrices. This is documentation at an
  intermediate level: they give you an overview of what's there in a
  particular area. For example when you wonder what finite element
  classes exist, you would take a look at the @ref fe module. The
  modules are, of course, also cross-linked to the manual (and, at
  times, to the tutorial); if you click on a class name, say on
  Triangulation, would will also at the very top right under the class
  name get a link to the modules this class is a member of if you want
  to learn more about its context.

Let's come back to the tutorial, since you are looking at the first program
(or "step") of it. Each tutorial program is subdivided into the following
sections:
<ol>
  <li> <b>Introduction:</b> This is a discussion of what the program
       does, including the mathematical model, and
       what programming techniques are new compared to previous
       tutorial programs.
  <li> <b>The commented program:</b> An extensively documented listing of the
       source code. Here, we often document individual lines, or
       blocks of code, and discuss what they do, how they do it, and
       why. The comments frequently reference the introduction,
       i.e. you have to understand <i>what</i> the program wants to achieve
       (a goal discussed in the introduction) before you can
       understand <i>how</i> it intends to get there.
  <li> <b>Results:</b> The output of the program, with comments and
       interpretation. This section also frequently has a subsection
       that gives suggestions on how to extend the program in various
       direction; in the earlier programs, this is intended to give
       you directions for little experiments designed to make your
       familiar with deal.II, while in later programs it is more about
       how to use more advanced numerical techniques.
  <li> <b>The plain program:</b> The source code stripped of
       all comments. This is useful if you want to see the "big
       picture" of the code, since the commented version of the
       program has so much text in between that it is often difficult
       to see the entire code of a single function on the screen at
       once.
</ol>

The tutorials are not only meant to be static documentation, but you
should play with them. To this end, go to the
<code>examples/step-1</code> directory (or whatever the number of the
tutorial is that you're interested in) and type
@code
  cmake .
  make
  make run
@endcode
The first command sets up the files that describe which include files this
tutorial program depends on, how to compile it and how to run it. This command
should find the installed deal.II libraries as well that were generated when
you compiled and installed everything as described in the
<a href="../../readme.html" target="body">README</a> file.
If this command should fail to find the deal.II library, then you need to
provide the path to the installation using the command
@code
  cmake -DDEAL_II_DIR=/path/to/installed/deal.II .
@endcode
instead.

The second of the commands above compiles the sources into an executable, while the
last one executes it (strictly speaking, <code>make run</code> will also
compile the code if the executable doesn't exist yet, so you could
have skipped the second command if you wanted). This is all that's
needed to run the code and produce the output that is discussed in the
"Results" section of the tutorial programs. This sequence needs to be repeated
in all of the tutorial directories you want to play with.

When learning the library, you need to play with it and see what
happens. To this end, open the <code>examples/step-1/step-1.cc</code>
source file with your favorite editor and modify it in some way, save it and
run it as above. A few suggestions for possibly modifications are given at the
end of the results section of this program, where we also provide a few links
to other useful pieces of information.


<h3> Video lectures on tutorial programs </h3>

This and several of the other tutorial programs are also discussed and
demonstrated in <a
href="http://www.math.colostate.edu/~bangerth/videos.html">Wolfgang
Bangerth's video lectures</a> on deal.II and computational science. In
particular, you can see the steps he executes to run this and other
programs, and you will get a much better idea of the tools that can be
used to work with deal.II. In particular, lectures 2 and 4 give an overview of
deal.II and of the building blocks of any finite element code.
(@dealiiVideoLectureSeeAlso{2,4})

If you are not yet familiar with using Linux and running things on the
command line, you may be interested in watching lectures 2.9 and 2.91.
(@dealiiVideoLectureSeeAlso{2.9,2.91}) These give overviews over the command
line and on what happens when compiling programs, respectively.

Note that deal.II is actively developed, and in the course of this
development we occasionally rename or deprecate functions or classes
that are still referenced in these video lectures.  For
example, the step-1 code shown in video lecture 5 uses a class
HyperShellBoundary which was replaced with SphericalManifold class
later on. Additionally, as of deal.II version 9.0, GridGenerator::hyper_shell()
now automatically attaches a SphericalManifold to the Triangulation. Otherwise
the rest of the lecture material is relevant.

<h3> What this program does </h3>

Let's come back to step-1, the current program.
In this first example, we don't actually do very much, but show two
techniques: what is the syntax to generate triangulation objects, and
some elements of simple loops over all cells. We create two grids, one
which is a regularly refined square (not very exciting, but a common
starting grid for some problems), and one more geometric attempt: a
ring-shaped domain, which is refined towards the inner edge. Through
this, you will get to know three things every finite element program
will have to have somewhere: An object of type Triangulation for the
mesh; a call to the GridGenerator functions to generate a mesh; and
loops over all cells that involve iterators (iterators are a
generalization of pointers and are frequently used in the C++ standard
library; in the context of deal.II, the @ref Iterators module talks
about them).

The program is otherwise small enough that it doesn't need a whole lot
of introduction.

@dealiiVideoLecture{5,6}


<h3> About scientific computing in general </h3>

If you are reading through this tutorial program, chances are that you are
interested in continuing to use deal.II for your own projects. Thus, you are
about to embark on an exercise in programming using a large-scale scientific
computing library. Unless you are already an experienced user of large-scale
programming methods, this may be new territory for you &mdash; with all the
new rules that go along with it such as the fact that you will have to deal
with code written by others, that you may have to think about documenting your
own code because you may not remember what exactly it is doing a year down the
road (or because others will be using it as well), or coming up with ways to
test that your program is doing the right thing. None of this is something
that we typically train mathematicians, engineers, or scientists in but that
is important when you start writing software of more than a few hundred
lines. Remember: Producing software is not the same as just writing code.

To make your life easier on this journey let us point to some resources that
are worthwhile browsing through before you start any large-scale programming:

- The <a href="https://github.com/dealii/dealii/wiki/Frequently-Asked-Questions">
  deal.II FAQ</a> has a good number of answers to questions about
  particular aspects of deal.II, but also to more general questions such as "How
  do I debug scientific computing codes?" or "Can I train myself to write code
  that has fewer bugs?".

- You will benefit from becoming a better programmer. An excellent
  resource to this end is the book
  [Code Complete](https://en.wikipedia.org/wiki/Code_Complete)
  by Steve McConnell @cite CodeComplete . It's already
  a few years old, with the last edition published in 2004, but it has
  lost none of its appeal as a guide to good programming practices,
  and some of the principal developers use it as a group reading
  project with every generation of their research group members.

- The <a href="http://software-carpentry.org/">Software Carpentry project</a>
  that provides introductions to many topics that are important to dealing
  with software, such as version control, make files, testing, etc. It is
  specifically written for scientists and engineers, not for computer
  scientists, and has a focus on short, practical lessons.

- The <a href="https://bssw.io/">Better Scientific Software
  project</a> has a lot of resources (and interesting blog posts) that
  cover many aspects of writing scientific software.

- The <a href="https://ideas-productivity.org/">IDEAS
  project</a> also has resources on software development, in
  particular for parallel computing. In the "Events" section on
  that site are recorded tutorials and webinars that cover many
  interesting topics.

- An article on <a href="http://arxiv.org/abs/1210.0530">Best
  Practices for Scientific Computing</a> that gives an introduction to
  many of the ways by which you can make sure you are an efficient
  programmer writing programs that work.

As a general recommendation: If you expect to spend more than a few days
writing software in the future, do yourself the favor of learning tools that
can make your life more productive, in particular debuggers and integrated
development environments. (@dealiiVideoLectureSeeAlso{7,8,8.01,25})
You will find that you will get the time spent
learning these tools back severalfold soon by being more productive!
Several of the video lectures referenced above show how to use tools
such as integrated development environments or debuggers.


examples/step-1/doc/results.dox
<h1>Results</h1>

Running the program produces graphics of two grids (grid-1.svg and grid-2.svg).
You can open these with most every web browser -- in the simplest case,
just open the current directory in your file system explorer and click
on the file. If you like working on the command line, you call your
web browser with the file: `firefox grid-1.svg`, `google-chrome grid-1.svg`,
or whatever the name of your browser is. If you do this, the two meshes
should look like this:

<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-1.grid-1-r9.2.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-1.grid-2-r9.2.png" alt="">
    </td>
  </tr>
</table>

The left one, well, is not very exciting. The right one is &mdash; at least
&mdash; unconventional. The pictures color-code the "refinement level" of each
cell: How many times did a coarse mesh cell have to be subdivided to obtain
the given cell. In the left image, this is boring since the mesh was
refined globally a number of times, i.e., <i>every</i> cell was
refined the same number of times.

(While the second mesh is entirely artificial and made-up, and
certainly not very practical in applications, to everyone's surprise it
has found its way into the literature: see @cite Mu05. Apparently it is
good for some things at least.)


<h3> Possibilities for extensions </h3>

<h4> Different adaptive refinement strategies </h4>

This program obviously does not have a whole lot of functionality, but
in particular the <code>second_grid</code> function has a bunch of
places where you can play with it. For example, you could modify the
criterion by which we decide which cells to refine. An example would
be to change the condition to this:
@code
      for (auto &cell: triangulation.active_cell_iterators())
        if (cell->center()[1] > 0)
          cell->set_refine_flag ();
@endcode
This would refine all cells for which the $y$-coordinate of the cell's
center is greater than zero (the <code>TriaAccessor::center</code>
function that we call by dereferencing the <code>cell</code> iterator
returns a Point<2> object; subscripting <code>[0]</code> would give
the $x$-coordinate, subscripting <code>[1]</code> the
$y$-coordinate). By looking at the functions that TriaAccessor
provides, you can also use more complicated criteria for refinement.

In general, what you can do with operations of the form
`cell->something()` is a bit difficult to find in the documentation
because `cell` is not a pointer but an iterator. The functions you can
call on a cell can be found in the documentation of the classes
`TriaAccessor` (which has functions that can also be called on faces
of cells or, more generally, all sorts of geometric objects that
appear in a triangulation), and `CellAccessor` (which adds a few
functions that are specific to *cells*).

A more thorough description of the whole iterator concept can be found
in the @ref Iterators documentation module.


<h4> Different geometries </h4>

Another possibility would be to generate meshes of entirely different
geometries altogether. While for complex geometries there is no way around
using meshes obtained from mesh generators, there is a good number of
geometries for which deal.II can create meshes using the functions in the
GridGenerator namespace. Many of these geometries (such as the one used in this
example program) contain cells with curved faces: put another way, we expect the
new vertices placed on the boundary to lie along a circle. deal.II handles complex
geometries with the Manifold class (and classes inheriting from it); in particular,
the functions in GridGenerator corresponding to non-Cartesian grids (such as
GridGenerator::hyper_shell or GridGenerator::truncated_cone) attach a Manifold
object to the part of the triangulation that should be curved (SphericalManifold
and CylindricalManifold, respectively) and use another manifold on the parts that
should be flat (FlatManifold). See the documentation
of Manifold or the @ref manifold "manifold module" for descriptions of the design
philosophy and interfaces of these classes. Take a look at what they provide and
see how they could be used in a program like this.

We also discuss a variety of other ways to create and manipulate meshes (and
describe the process of attaching Manifolds) in step-49.


<h4> Comments about programming and debugging </h4>

We close with a comment about modifying or writing programs with deal.II in
general. When you start working with tutorial programs or your own
applications, you will find that mistakes happen: your program will contain
code that either aborts the program right away or bugs that simply lead to
wrong results. In either case, you will find it extremely helpful to know how
to work with a debugger: you may get by for a while by just putting debug
output into your program, compiling it, and running it, but ultimately finding
bugs with a debugger is much faster, much more convenient, and more reliable
because you don't have to recompile the program all the time and because you
can inspect the values of variables and how they change.

Rather than postponing learning how to use a debugger till you really can't
see any other way to find a bug, here's the one piece of
advice we will provide in this program: learn how to use a debugger as soon as
possible. It will be time well invested.
(@dealiiVideoLectureSeeAlso{25}) The deal.II Frequently Asked
Questions (FAQ) page linked to from the top-level <a
href="http://www.dealii.org/">deal.II webpage</a> also provides a good number
of hints on debugging deal.II programs.


<h4> More about graphical output </h4>

It is often useful to include meshes into your theses or publications.
For this, it may not be very useful to color-code the cells by
refinement level, and to print the cell number onto each cell. But
it doesn't have to be that way -- the GridOut class allows setting flags
for each possible output format (see the classes in the GridOutFlags
namespace) that control how exactly a mesh is plotted. You can of
course also choose other output file formats such as VTK or VTU; this
is particularly useful for 3d meshes where a 2d format such as SVG
is not particular useful because it fixes a particular viewpoint onto
the 3d object. As a consequence, you might want to explore other
options in the GridOut class.


examples/step-10/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>


This is a rather short example which only shows some aspects of using
higher order mappings. By <em>mapping</em> we mean the transformation
between the unit cell (i.e. the unit line, square, or cube) to the
cells in real space. In all the previous examples, we have implicitly
used linear or d-linear mappings; you will not have noticed this at
all, since this is what happens if you do not do anything
special. However, if your domain has curved boundaries, there are
cases where the piecewise linear approximation of the boundary
(i.e. by straight line segments) is not sufficient, and you want that
your computational domain is an approximation to the real domain using
curved boundaries as well. If the boundary approximation uses
piecewise quadratic parabolas to approximate the true boundary, then
we say that this is a quadratic or $Q_2$ approximation. If we
use piecewise graphs of cubic polynomials, then this is a $Q_3$
approximation, and so on.



For some differential equations, it is known that piecewise linear
approximations of the boundary, i.e. $Q_1$ mappings, are not
sufficient if the boundary of the exact domain is curved. Examples are the
biharmonic equation using $C^1$ elements, or the Euler
equations of gas dynamics on domains with curved reflective boundaries. In these cases,
it is necessary to compute the integrals using a higher order
mapping. If we do not use such a higher
order mapping, the order of approximation of the boundary dominates
the order of convergence of the entire numerical scheme, irrespective
of the order of convergence of the discretization in the interior of
the domain.



Rather than demonstrating the use of higher order mappings with one of
these more complicated examples, we do only a brief computation:
calculating the value of $\pi=3.141592653589793238462643\ldots$ by two
different methods.



The first method uses a triangulated approximation of the circle with unit
radius and integrates a unit magnitude constant function ($f = 1$) over it. Of
course, if the domain were the exact unit circle, then the area would be $\pi$,
but since we only use an approximation by piecewise polynomial segments, the
value of the area we integrate over is not exactly $\pi$. However, it is known
that as we refine the triangulation, a $Q_p$ mapping approximates the boundary
with an order $h^{p+1}$, where $h$ is the mesh size. We will check the values
of the computed area of the circle and their convergence towards $\pi$ under
mesh refinement for different mappings. We will also find a convergence
behavior that is surprising at first, but has a good explanation.



The second method works similarly, but this time does not use the area
of the triangulated unit circle, but rather its perimeter. $\pi$ is then
approximated by half of the perimeter, as we choose the radius equal to one.


@note This tutorial shows in essence how to choose a particular
mapping for integrals, by attaching a particular geometry to the
triangulation (as had already been done in step-1, for example) and
then passing a mapping argument to the FEValues class that is used for
all integrals in deal.II. The geometry we choose is a circle, for
which deal.II already has a class (SphericalManifold) that can be
used. If you want to define your own geometry, for example because it
is complicated and cannot be described by the classes already
available in deal.II, you will want to read through step-53.


examples/step-10/doc/results.dox
<h1>Results</h1>


The program performs two tasks, the first being to generate a
visualization of the mapped domain, the second to compute pi by the
two methods described. Let us first take a look at the generated
graphics. They are generated in Gnuplot format, and can be viewed with
the commands
@code
set style data lines
set size ratio -1
unset key
unset xtics
unset ytics
plot [-1:1][-1:1] "ball_0_mapping_q_1.dat" lw 4 lt rgb "black"
@endcode
or using one of the other filenames. The second line makes sure that
the aspect ratio of the generated output is actually 1:1, i.e. a
circle is drawn as a circle on your screen, rather than as an
ellipse. The third line switches off the key in the graphic, as that
will only print information (the filename) which is not that important
right now. Similarly, the fourth and fifth disable tick marks. The plot
is then generated with a specific line width ("lw", here set to 4)
and line type ("lt", here chosen by saying that the line should be
drawn using the RGB color "black").

The following table shows the triangulated computational domain for $Q_1$,
$Q_2$, and $Q_3$ mappings, for the original coarse grid (left), and a once
uniformly refined grid (right).

<div class="twocolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_10_ball_0_q1.svg"
         alt="Five-cell discretization of the disk."
         width="400" height="400">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_10_ball_1_q1.svg"
         alt="20-cell discretization of the disk (i.e., five cells
              refined once)."
         width="400" height="400">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_10_ball_0_q2.svg"
         alt="Five-cell discretization of the disk with quadratic edges. The
              boundary is nearly indistinguishable from the actual circle."
         width="400" height="400"
         >
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_10_ball_1_q2.svg"
         alt="20-cell discretization with quadratic edges."
         width="400" height="400">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_10_ball_0_q3.svg"
         alt="Five-cell discretization of the disk with cubic edges. The
              boundary is nearly indistinguishable from the actual circle."
         width="400" height="400">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_10_ball_1_q3.svg"
         alt="20-cell discretization with cubic edges."
         width="400" height="400">
  </div>
</div>

These pictures show the obvious advantage of higher order mappings: they
approximate the true boundary quite well also on rather coarse meshes. To
demonstrate this a little further, here is part of the upper right quarter
circle of the coarse meshes with $Q_2$ and $Q_3$ mappings, where the dashed
red line marks the actual circle:

<div class="twocolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_10_exact_vs_interpolate_q2.svg"
         alt="Close-up of quadratic discretization. The distance between the
         quadratic interpolant and the actual circle is small."
         width="400" height="400">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_10_exact_vs_interpolate_q3.svg"
         alt="Close-up of cubic discretization. The distance between the
         cubic interpolant and the actual circle is very small."
         width="400" height="400">
  </div>
</div>

Obviously the quadratic mapping approximates the boundary quite well,
while for the cubic mapping the difference between approximated domain
and true one is hardly visible already for the coarse grid. You can
also see that the mapping only changes something at the outer
boundaries of the triangulation. In the interior, all lines are still
represented by linear functions, resulting in additional computations
only on cells at the boundary. Higher order mappings are therefore
usually not noticeably slower than lower order ones, because the
additional computations are only performed on a small subset of all
cells.



The second purpose of the program was to compute the value of pi to
good accuracy. This is the output of this part of the program:
@code
Output of grids into gnuplot files:
===================================
Refinement level: 0
Degree = 1
Degree = 2
Degree = 3

Refinement level: 1
Degree = 1
Degree = 2
Degree = 3

Computation of Pi by the area:
==============================
Degree = 1
cells      eval.pi            error
    5 1.9999999999999993 1.1416e+00    -
   20 2.8284271247461890 3.1317e-01 1.87
   80 3.0614674589207174 8.0125e-02 1.97
  320 3.1214451522580511 2.0148e-02 1.99
 1280 3.1365484905459380 5.0442e-03 2.00
 5120 3.1403311569547516 1.2615e-03 2.00

Degree = 2
cells      eval.pi            error
    5 3.1045694996615860 3.7023e-02    -
   20 3.1391475703122267 2.4451e-03 3.92
   80 3.1414377167038290 1.5494e-04 3.98
  320 3.1415829366419006 9.7169e-06 4.00
 1280 3.1415920457576898 6.0783e-07 4.00
 5120 3.1415926155921117 3.7998e-08 4.00

Degree = 3
cells      eval.pi            error
    5 3.1410033851499288 5.8927e-04    -
   20 3.1415830393583839 9.6142e-06 5.94
   80 3.1415925017363797 1.5185e-07 5.98
  320 3.1415926512106696 2.3791e-09 6.00
 1280 3.1415926535525927 3.7200e-11 6.00
 5120 3.1415926535892100 5.8302e-13 6.00

Degree = 4
cells      eval.pi            error
    5 3.1415871927401131 5.4608e-06    -
   20 3.1415926314742428 2.2116e-08 7.95
   80 3.1415926535026202 8.7173e-11 7.99
  320 3.1415926535894498 3.4350e-13 7.99
 1280 3.1415926535897896 3.4671e-15 6.63
 5120 3.1415926535897909 2.4009e-15 0.53

Computation of Pi by the perimeter:
===================================
Degree = 1
cells      eval.pi            error
    5 2.8284271247461898 3.1317e-01    -
   20 3.0614674589207178 8.0125e-02 1.97
   80 3.1214451522580520 2.0148e-02 1.99
  320 3.1365484905459389 5.0442e-03 2.00
 1280 3.1403311569547525 1.2615e-03 2.00
 5120 3.1412772509327724 3.1540e-04 2.00

Degree = 2
cells      eval.pi            error
    5 3.1248930668550594 1.6700e-02    -
   20 3.1404050605605449 1.1876e-03 3.81
   80 3.1415157631807009 7.6890e-05 3.95
  320 3.1415878042798613 4.8493e-06 3.99
 1280 3.1415923498174534 3.0377e-07 4.00
 5120 3.1415926345931995 1.8997e-08 4.00

Degree = 3
cells      eval.pi            error
    5 3.1414940401456048 9.8613e-05    -
   20 3.1415913432549156 1.3103e-06 6.23
   80 3.1415926341726910 1.9417e-08 6.08
  320 3.1415926532906897 2.9910e-10 6.02
 1280 3.1415926535851355 4.6578e-12 6.00
 5120 3.1415926535897190 7.4216e-14 5.97

Degree = 4
cells      eval.pi            error
    5 3.1415921029432572 5.5065e-07     -
   20 3.1415926513737595 2.2160e-09  7.96
   80 3.1415926535810712 8.7222e-12  7.99
  320 3.1415926535897576 3.5525e-14  7.94
 1280 3.1415926535897936 4.6729e-16  6.25
 5120 3.1415926535897918 1.4929e-15 -1.68
@endcode



One of the immediate observations from the output is that in all cases the
values converge quickly to the true value of
$\pi=3.141592653589793238462643$. Note that for the $Q_4$ mapping, we are
already in the regime of roundoff errors and the convergence rate levels off,
which is already quite a lot. However, also note that for the $Q_1$ mapping,
even on the finest grid the accuracy is significantly worse than on the coarse
grid for a $Q_3$ mapping!



The last column of the output shows the convergence order, in powers of the
mesh width $h$. In the introduction, we had stated that the convergence order
for a $Q_p$ mapping should be $h^{p+1}$. However, in the example shown, the
order is rather $h^{2p}$! This at first surprising fact is explained by the
properties of the $Q_p$ mapping. At order <i>p</i>, it uses support points
that are based on the <i>p</i>+1 point Gauss-Lobatto quadrature rule that
selects the support points in such a way that the quadrature rule converges at
order 2<i>p</i>. Even though these points are here only used for interpolation
of a <i>p</i>th order polynomial, we get a superconvergence effect when
numerically evaluating the integral, resulting in the observed high order of
convergence. (This effect is also discussed in detail in the following
publication: A. Bonito, A. Demlow, and J. Owen: "A priori error
estimates for finite element approximations to eigenvalues and
eigenfunctions of the Laplace-Beltrami operator", submitted, 2018.)


examples/step-11/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

The problem we will be considering is the solution of Laplace's problem with
Neumann boundary conditions only:
@f{eqnarray*}
  -\Delta u &=& f \qquad \mathrm{in}\ \Omega,
  \\
  \partial_n u &=& g \qquad \mathrm{on}\ \partial\Omega.
@f}
It is well known that if this problem is to have a solution, then the forces
need to satisfy the compatibility condition
@f[
  \int_\Omega f\; dx + \int_{\partial\Omega} g\; ds = 0.
@f]
We will consider the special case that $\Omega$ is the circle of radius 1
around the origin, and $f=-2$, $g=1$. This choice satisfies the compatibility
condition.

The compatibility condition allows a solution of the above equation, but it
nevertheless retains an ambiguity: since only derivatives of the solution
appear in the equations, the solution is only determined up to a constant. For
this reason, we have to pose another condition for the numerical solution,
which fixes this constant.

For this, there are various possibilities:
<ol>
<li> Fix one node of the discretization to zero or any other fixed value.
  This amounts to an additional condition $u_h(x_0)=0$. Although this is
  common practice, it is not necessarily a good idea, since we know that the
  solutions of Laplace's equation are only in $H^1$, which does not allow for
  the definition of point values because it is not a subset of the continuous
  functions. Therefore, even though fixing one node is allowed for
  discretized functions, it is not for continuous functions, and one can
  often see this in a resulting error spike at this point in the numerical
  solution.

<li> Fixing the mean value over the domain to zero or any other value. This
  is allowed on the continuous level, since $H^1(\Omega)\subset L^1(\Omega)$
  by Sobolev's inequality, and thus also on the discrete level since we
  there only consider subsets of $H^1$.

<li> Fixing the mean value over the boundary of the domain to zero or any
  other value. This is also allowed on the continuous level, since
  $H^{1/2}(\partial\Omega)\subset L^1(\partial\Omega)$, again by Sobolev's
  inequality.
</ol>
We will choose the last possibility, since we want to demonstrate another
technique with it.

While this describes the problem to be solved, we still have to figure out how
to implement it. Basically, except for the additional mean value constraint,
we have solved this problem several times, using Dirichlet boundary values,
and we only need to drop the treatment of Dirichlet boundary nodes. The use of
higher order mappings is also rather trivial and will be explained at the
various places where we use it; in almost all conceivable cases, you will only
consider the objects describing mappings as a black box which you need not
worry about, because their only uses seem to be to be passed to places deep
inside the library where functions know how to handle them (i.e. in the
<code>FEValues</code> classes and their descendants).

The tricky point in this program is the use of the mean value
constraint. Fortunately, there is a class in the library which knows how to
handle such constraints, and we have used it quite often already, without
mentioning its generality. Note that if we assume that the boundary nodes are
spaced equally along the boundary, then the mean value constraint
@f[
  \int_{\partial \Omega} u(x) \; ds = 0
@f]
can be written as
@f[
  \sum_{i\in\partial\Omega_h} u_i = 0,
@f]
where the sum shall run over all degree of freedom indices which are located
on the boundary of the computational domain. Let us denote by $i_0$ that index
on the boundary with the lowest number (or any other conveniently chosen
index), then the constraint can also be represented by
@f[
  u_{i_0} = \sum_{i\in\partial\Omega_h\backslash i_0} -u_i.
@f]
This, luckily, is exactly the form of constraints for which the
AffineConstraints class was designed. Note that we have used this
class in several previous examples for the representation of hanging nodes
constraints, which also have this form: there, the middle vertex shall have
the mean of the values of the adjacent vertices. In general, the
AffineConstraints class is designed to handle affine constraints
of the form
@f[
  CU = b
@f]
where $C$ denotes a matrix, $b$ denotes a vector, and $U$ the vector of nodal
values. In this case, since $C$ represents one homogeneous constraint, $b$ is
the zero vector.

In this example, the mean value along the boundary allows just such a
representation, with $C$ being a matrix with just one row (i.e. there is only
one constraint). In the implementation, we will create an AffineConstraints
object, add one constraint (i.e. add another row to the matrix) referring to the
first boundary node $i_0$, and insert the weights with which all the other nodes
contribute, which in this example happens to be just $-1$.

Later, we will use this object to eliminate the first boundary node from the
linear system of equations, reducing it to one which has a solution without
the ambiguity of the constant shift value. One of the problems of the
implementation will be that the explicit elimination of this node results in a
number of additional elements in the matrix, of which we do not know in
advance where they are located and how many additional entries will be in each
of the rows of the matrix. We will show how we can use an intermediate object
to work around this problem.

But now on to the implementation of the program solving this problem...


examples/step-11/doc/results.dox
<h1>Results</h1>

This is what the program outputs:
@code
Using mapping with degree 1:
============================
cells  |u|_1    error
    5 0.680402 0.572912
   20 1.088141 0.165173
   80 1.210142 0.043172
  320 1.242375 0.010939
 1280 1.250569 0.002745
 5120 1.252627 0.000687

Using mapping with degree 2:
============================
cells  |u|_1    error
    5 1.177062 0.076252
   20 1.228978 0.024336
   80 1.245175 0.008139
  320 1.250881 0.002433
 1280 1.252646 0.000668
 5120 1.253139 0.000175

Using mapping with degree 3:
============================
cells  |u|_1    error
    5 1.193493 0.059821
   20 1.229825 0.023489
   80 1.245221 0.008094
  320 1.250884 0.002430
 1280 1.252646 0.000668
 5120 1.253139 0.000175
@endcode
As we expected, the convergence order for each of the different
mappings is clearly quadratic in the mesh size. What <em>is</em>
interesting, though, is that the error for a bilinear mapping
(i.e. degree 1) is more than three times larger than that for the
higher order mappings; it is therefore clearly advantageous in this
case to use a higher order mapping, not because it improves the order
of convergence but just to reduce the constant before the convergence
order. On the other hand, using a cubic mapping only improves the
result further by insignificant amounts, except on very coarse
grids.

We can also visualize the underlying meshes by using, for instance,
ParaView. The image below shows initial meshes for different mapping
degrees:

<img src="https://www.dealii.org/images/steps/developer/step-11.cycle_0.png" alt="">

Clearly, the effect is most pronounced when we go from the linear to
quadratic mapping. This is also reflected in the error values given
in the table above. The effect of going from quadratic to cubic degree
is less dramatic, but still tangible owing to a more accurate
description of the circular boundary.

Next, let's look at the meshes after three global refinements

<img src="https://www.dealii.org/images/steps/developer/step-11.cycle_3.png" alt="">

Here, the differences are much less visible, especially for higher order
mappings. Indeed, at this refinement level the error values reported
in the table are essentially identical between mappings of degrees two
and three.


examples/step-12/doc/intro.dox
<br>

<i> Note: A variant called step-12b of this tutorial exists, using
MeshWorker and LocalIntegrators instead of assembling matrices using
FEInterfaceValues as is done in this tutorial.
</i>

<a name="Intro"></a>
<h1>An example of an advection problem using the Discountinuous Galerkin method</h1>

<h3>Overview</h3>

This example is devoted to the <em>discontinuous
Galerkin method</em>, or in short, the DG method. It includes the following topics.
<ol>
  <li> Discretization of the linear advection equation with the DG method.
  <li> Assembling of jump terms and other expressions on the interface between cells using FEInterfaceValues.
  <li> Assembling of the system matrix using the MeshWorker::mesh_loop().
</ol>

The particular concern of this program are the loops of DG methods. These turn
out to be especially complex, primarily because for the face terms, we have to
distinguish the cases of boundary, regular interior faces and interior faces
with hanging nodes, respectively. The MeshWorker::mesh_loop() handles the
complexity on iterating over cells and faces and allows specifying "workers"
for the different cell and face terms. The integration of face terms itself,
including on adaptively refined faces, is done using the FEInterfaceValues
class.

<h3>The equation</h3>

The model problem solved in this example is the linear advection equation
@f[
  \nabla\cdot \left({\mathbf \beta} u\right)=0 \qquad\mbox{in }\Omega,
@f]
subject to the boundary conditions
@f[
u=g\quad\mbox{on }\Gamma_-,
@f]
on the inflow part $\Gamma_-$ of the boundary $\Gamma=\partial\Omega$
of the domain.  Here, ${\mathbf \beta}={\mathbf \beta}({\bf x})$ denotes a
vector field, $u$ the (scalar) solution
function, $g$ a boundary value function,
@f[
\Gamma_- \dealcoloneq \{{\bf x}\in\Gamma, {\mathbf \beta}({\bf x})\cdot{\bf n}({\bf x})<0\}
@f]
the inflow part of the boundary of the domain and ${\bf n}$ denotes
the unit outward normal to the boundary $\Gamma$. This equation is the
conservative version of the advection equation already considered in
step-9 of this tutorial.


On each cell $T$, we multiply by a test function $v_h$ from the left and integrate by parts
to get:
@f[
  \left( v_h, \nabla \cdot (\beta u_h) \right)_T
= -(\nabla v_h, \beta u_h) + \int_\Gamma v_h u_h \beta \cdot n
@f]
When summing this expression over all cells $T$, the boundary integral is done over
all internal and external faces and as such there are three cases:
<ol>
<li> outer boundary on the inflow (we replace $u_h$ by given $g$):
  $\int_{\Gamma_-} v_h g \beta \cdot n$
<li> outer boundary on the outflow:
  $\int_{\Gamma_+} v_h u_h \beta \cdot n$
<li> inner faces (integral from two sides turns into jump, we use the upwind velocity):
  $\int_F [v_h] u_h^{\text{upwind}} \beta \cdot n$
</ol>

Here, the jump is defined as $[v] = v^+ - v^-$, where the superscripts refer
to the left ('+') and right ('-') values at the face. The upwind value
$u^{\text{upwind}}$ is defined to be $u^+$ if $\beta \cdot n>0$ and $u^-$ otherwise.

As a result, the mesh-dependent weak form reads:
@f[
\sum_{T\in \mathbb T_h} -\bigl(\nabla \phi_i,{\mathbf \beta}\cdot \phi_j \bigr)_T +
\sum_{F\in\mathbb F_h^i} \bigl< [\phi_i], \phi_j^{upwind} \beta\cdot \mathbf n\bigr>_{F} +
\bigl<\phi_i, \phi_j \beta\cdot \mathbf n\bigr>_{\Gamma_+}
= -\bigl<\phi_i, g \beta\cdot\mathbf n\bigr>_{\Gamma_-}.
@f]
Here, $\mathbb T_h$ is the set of all active cells of the triangulation
and $\mathbb F_h^i$ is the set of all active interior faces. This formulation
is known as the upwind discontinuous Galerkin method.

In order to implement this bilinear form, we need to compute the cell terms
(first sum) using the usual way to achieve integration on a cell, the interface terms (second sum) using
FEInterfaceValues, and the boundary terms (the other two terms).
The summation of all those is done by MeshWorker::mesh_loop().



<h3>The test problem</h3>

We solve the advection equation on
$\Omega=[0,1]^2$ with ${\mathbf \beta}=\frac{1}{|x|}(-x_2, x_1)$
representing a circular counterclockwise flow field, and $g=1$
on ${\bf x}\in\Gamma_-^1 := [0,0.5]\times\{0\}$ and $g=0$ on ${\bf x}\in
\Gamma_-\setminus \Gamma_-^1$.

We solve on a sequence of meshes by refining the mesh adaptively by estimating
the norm of the gradient on each cell. After solving on each mesh, we output
the solution in vtk format and compute the $L^\infty$ norm of the solution. As
the exact solution is either 0 or 1, we can measure the magnitude of the
overshoot of the numerical solution with this.


examples/step-12/doc/results.dox
<h1>Results</h1>


The output of this program consist of the console output and
solutions in vtk format:
@code
Cycle 0
  Number of active cells:       64
  Number of degrees of freedom: 256
  Solver converged in 4 iterations.
  Writing solution to <solution-0.vtk>
  L-infinity norm: 1.09057
Cycle 1
  Number of active cells:       112
  Number of degrees of freedom: 448
  Solver converged in 9 iterations.
  Writing solution to <solution-1.vtk>
  L-infinity norm: 1.10402
Cycle 2
  Number of active cells:       214
  Number of degrees of freedom: 856
  Solver converged in 16 iterations.
  Writing solution to <solution-2.vtk>
  L-infinity norm: 1.09813
Cycle 3
  Number of active cells:       415
  Number of degrees of freedom: 1660
  Solver converged in 26 iterations.
  Writing solution to <solution-3.vtk>
  L-infinity norm: 1.09579
Cycle 4
  Number of active cells:       796
  Number of degrees of freedom: 3184
  Solver converged in 44 iterations.
  Writing solution to <solution-4.vtk>
  L-infinity norm: 1.09612
Cycle 5
  Number of active cells:       1561
  Number of degrees of freedom: 6244
  Solver converged in 81 iterations.
  Writing solution to <solution-5.vtk>
@endcode

We show the solutions on the initial mesh, the mesh after two
and after five adaptive refinement steps.

<img src="https://www.dealii.org/images/steps/developer/step-12.sol-0.png" alt="">
<img src="https://www.dealii.org/images/steps/developer/step-12.sol-2.png" alt="">
<img src="https://www.dealii.org/images/steps/developer/step-12.sol-5.png" alt="">

And finally we show a plot of a 3d computation.

<img src="https://www.dealii.org/images/steps/developer/step-12.sol-5-3d.png" alt="">


<a name="dg-vs-cg"></a>
<h3>Why use discontinuous elements</h3>

In this program we have used discontinuous elements. It is a legitimate
question to ask why not simply use the normal, continuous ones. Of course, to
everyone with a background in numerical methods, the answer is obvious: the
continuous Galerkin (cG) method is not stable for the transport equation,
unless one specifically adds stabilization terms. The DG method, however,
<i>is</i> stable. Illustrating this with the current program is not very
difficult; in fact, only the following minor modifications are necessary:
- Change the element to FE_Q instead of FE_DGQ.
- Add handling of hanging node constraints in exactly the same way as step-6.
- We need a different solver; the direct solver in step-29 is a convenient
  choice.
An experienced deal.II user will be able to do this in less than 10 minutes.

While the 2d solution has been shown above, containing a number of small
spikes at the interface that are, however, stable in height under mesh
refinement, results look much different when using a continuous element:

<table align="center">
  <tr>
    <td valign="top">
      0 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-12.cg.sol-0.png" alt="">
    </td>
    <td valign="top">
      1 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-12.cg.sol-1.png" alt="">
    </td>
  </tr>
  <tr>
    <td valign="top">
      2 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-12.cg.sol-2.png" alt="">
    </td>
    <td valign="top">
      3 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-12.cg.sol-3.png" alt="">
    </td>
  </tr>
  <tr>
    <td valign="top">
      4 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-12.cg.sol-4.png" alt="">
    </td>
    <td valign="top">
      5 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-12.cg.sol-5.png" alt="">
    </td>
  </tr>
</table>

In refinement iteration 5, the image can't be plotted in a reasonable way any
more as a 3d plot. We thus show a color plot with a range of $[-1,2]$ (the
solution values of the exact solution lie in $[0,1]$, of course). In any case,
it is clear that the continuous Galerkin solution exhibits oscillatory
behavior that gets worse and worse as the mesh is refined more and more.

There are a number of strategies to stabilize the cG method, if one wants to
use continuous elements for some reason. Discussing these methods is beyond
the scope of this tutorial program; an interested reader could, for example,
take a look at step-31.



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

Given that the exact solution is known in this case, one interesting
avenue for further extensions would be to confirm the order of
convergence for this program. In the current case, the solution is
non-smooth, and so we can not expect to get a particularly high order
of convergence, even if we used higher order elements. But even if the
solution <i>is</i> smooth, the equation is not elliptic and so it is not
immediately clear that we should obtain a convergence order that
equals that of the optimal interpolation estimates (i.e. for example
that we would get $h^3$ convergence in the $L^2$ norm by using
quadratic elements).

In fact, for hyperbolic equations, theoretical predictions often
indicate that the best one can hope for is an order one half below the
interpolation estimate. For example, for the streamline diffusion
method (an alternative method to the DG method used here to stabilize
the solution of the transport equation), one can prove that for
elements of degree $p$, the order of convergence is $p+\frac 12$ on
arbitrary meshes. While the observed order is frequently $p+1$ on
uniformly refined meshes, one can construct so-called Peterson meshes
on which the worse theoretical bound is actually attained. This should
be relatively simple to verify, for example using the
VectorTools::integrate_difference function.

A different direction is to observe that the solution of transport problems
often has discontinuities and that therefore a mesh in which we <i>bisect</i>
every cell in every coordinate direction may not be optimal. Rather, a better
strategy would be to only cut cells in the direction parallel to the
discontinuity. This is called <i>anisotropic mesh refinement</i> and is the
subject of step-30.


examples/step-12b/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

This is a variant of step-16 with the only change that we are using the
MeshWorker framework with the pre-made LocalIntegrator helper classes instead
of assembling the face terms using FEInterfaceValues.

The details of this framework on how it is used in practice will be explained
as part of this tutorial program.

<h3>The testcase</h3>

The problem we solve here is the same as the one in step-12.


examples/step-12b/doc/results.dox
<h1>Results</h1>


The output of this program is very similar to step-16 and we are not repeating the output here.

We show the solutions on the initial mesh, the mesh after two
and after five adaptive refinement steps.

<img src="https://www.dealii.org/images/steps/developer/step-12.sol-0.png" alt="">
<img src="https://www.dealii.org/images/steps/developer/step-12.sol-2.png" alt="">
<img src="https://www.dealii.org/images/steps/developer/step-12.sol-5.png" alt="">


Then we show the final grid (after 5 refinement steps) and the solution again,
this time with a nicer 3d rendering (obtained using the DataOutBase::write_vtk
function and the VTK-based VisIt visualization program) that better shows the
sharpness of the jump on the refined mesh and the over- and undershoots of the
solution along the interface:

<img src="https://www.dealii.org/images/steps/developer/step-12.grid-5.png" alt="">
<img src="https://www.dealii.org/images/steps/developer/step-12.3d-solution.png" alt="">


And finally we show a plot of a 3d computation.

<img src="https://www.dealii.org/images/steps/developer/step-12.sol-5-3d.png" alt="">


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

For ideas for further extensions, please see see step-12.


examples/step-13/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

<h3>Background and purpose</h3>


In this example program, we will not so much be concerned with
describing new ways how to use deal.II and its facilities, but rather
with presenting methods of writing modular and extensible finite
element programs. The main reason for this is the size and complexity
of modern research software: applications implementing modern error
estimation concepts and adaptive solution methods tend to become
rather large. For example, when this program was written in 2002, the
three largest applications by the main
authors of deal.II, are at the time of writing of this example
program:
<ol>
<li> a program for solving conservation hyperbolic equations by the
     Discontinuous Galerkin Finite Element method: 33,775 lines of
     code;
<li> a parameter estimation program: 28,980 lines of code;
<li> a wave equation solver: 21,020 lines of code.
</ol>

(The library proper - without example programs and test suite - has slightly
more than 150,000 lines of code as of spring 2002. It is of course several
times larger now.) The sizes of these applications are at the edge of what
one person, even an experienced programmer, can manage.



The numbers above make one thing rather clear: monolithic programs that
are not broken up into smaller, mostly independent pieces have no way
of surviving, since even the author will quickly lose the overview of
the various dependencies between different parts of a program. Only
data encapsulation, for example using object oriented programming
methods, and modularization by defining small but fixed interfaces can
help structure data flow and mutual interdependencies. It is also an
absolute prerequisite if more than one person is developing a program,
since otherwise confusion will quickly prevail as one developer
would need to know if another changed something about the internals of
a different module if they were not cleanly separated.



In previous examples, you have seen how the library itself is broken
up into several complexes each building atop the underlying ones, but
relatively independent of the other ones:
<ol>
<li>the triangulation class complex, with associated iterator classes;
<li>the finite element classes;
<li>the DoFHandler class complex, with associated iterators, built on
    the triangulation and finite element classes;
<li>the classes implementing mappings between unit and real cells;
<li>the FEValues class complex, built atop the finite elements and
    mappings.
</ol>
Besides these, and a large number of smaller classes, there are of
course the following "tool" modules:
<ol>
<li>output in various graphical formats;
<li>linear algebra classes.
</ol>
These complexes can also be found as a flow chart on the front page of
the deal.II manual website.



The goal of this program is now to give an example of how a relatively
simple finite element program could be structured such that we end up
with a set of modules that are as independent of each other as
possible. This allows to change the program at one end, without having to
worry that it might break at the other, as long as we do not touch the
interface through which the two ends communicate. The interface in
C++, of course, is the declaration of abstract base classes.



Here, we will implement (again) a Laplace solver, although with a
number of differences compared to previous example programs:
<ol>
<li>The classes that implement the process of numerically solving the
    equation are no more responsible for driving the process of
    "solving-estimating error-refining-solving again", but we delegate
    this to external functions. This allows first to use it as a
    building block in a larger context, where the solution of a
    Laplace equation might only be one part (for example, in a
    nonlinear problem, where Laplace equations might have to be solved
    in each nonlinear step). It would also allow to build a framework
    around this class that would allow using solvers for other
    equations (but with the same external interface) instead, in case
    some techniques shall be evaluated for different types of partial
    differential equations.
<li>It splits the process of evaluating the computed solution to a
    separate set of classes. The reason is that one is usually not
    interested in the solution of a PDE per se, but rather in certain
    aspects of it. For example, one might wish to compute the traction
    at a certain boundary in elastic computations, or in the signal of
    a seismic wave at a receiver position at a given
    location. Sometimes, one might have an interest in several of
    these aspects. Since the evaluation of a solution is something
    that does not usually affect the process of solution, we split it
    off into a separate module, to allow for the development of such
    evaluation filters independently of the development of the solver
    classes.
<li>Separate the classes that implement mesh refinement from the
    classes that compute the solution.
<li>Separate the description of the test case with which we will
    present the program, from the rest of the program.
<li>Parallelize the assembly of linear systems using the WorkStream
    facilities. This follows the extensive description that can be
    found in the @ref threads "Parallel computing with multiple processors accessing shared memory"
    documentation module. The implementation essentially follows what
    has already been described in step-9.
</ol>



The things the program does are not new. In fact, this is more like a
melange of previous programs, cannibalizing various parts and
functions from earlier examples. It is the way they are arranged in
this program that should be the focus of the reader, i.e. the software
design techniques used in the program to achieve the goal of
implementing the desired mathematical method. However, we must
stress that software design is in part also a subjective matter:
different persons have different programming backgrounds and have
different opinions about the "right" style of programming; this
program therefore expresses only what the author considers useful
practice, and is not necessarily a style that you have to adopt in
order to write successful numerical software if you feel uncomfortable
with the chosen ways. It should serve as a case study, however,
inspiring the reader with ideas to the desired end.



Once you have worked through the program, you will remark that it is
already somewhat complex in its structure. Nevertheless, it
only has about 850 lines of code, without comments. In real
applications, there would of course be comments and class
documentation, which would bring that to maybe 1200 lines. Yet, compared to
the applications listed above, this is still small, as they are 20 to
25 times as large. For programs as large, a proper design right from
the start is thus indispensable. Otherwise, it will have to be
redesigned at one point in its life, once it becomes too large to be
manageable.



Despite of this, all three programs listed above have undergone major
revisions, or even rewrites. The wave program, for example, was once
entirely teared to parts when it was still significantly smaller, just
to assemble it again in a more modular form. By that time, it had
become impossible to add functionality without affecting older parts
of the code (the main problem with the code was the data flow: in time
dependent application, the major concern is when to store data to disk
and when to reload it again; if this is not done in an organized
fashion, then you end up with data released too early, loaded too
late, or not released at all). Although the present example program
thus draws from several years of experience, it is certainly not
without flaws in its design, and in particular might not be suited for
an application where the objective is different. It should serve as an
inspiration for writing your own application in a modular way, to
avoid the pitfalls of too closely coupled codes.



<h3>What the program does</h3>


What the program actually does is not even the main point of this
program, the structure of the program is more important. However, in a
few words, a description would be: solve the Laplace equation for a
given right hand side such that the solution is the function
$u(x,t)=\exp(x+\sin(10y+5x^2))$. The goal of the
computation is to get the value of the solution at the point
$x_0=(0.5,0.5)$, and to compare the accuracy with
which we resolve this value for two refinement criteria, namely global
refinement and refinement by the error indicator by Kelly et al. which
we have already used in previous examples.



The results will, as usual, be discussed in the respective section of
this document. In doing so, we will find a slightly irritating
observation about the relative performance of the two refinement
criteria. In a later example program, building atop this one, we will
devise a different method that should hopefully perform better than
the techniques discussed here.



So much now for all the theoretical and anecdotal background. The best
way of learning about a program is to look at it, so here it is:


examples/step-13/doc/results.dox
<h1>Results</h1>



The results of this program are not that interesting - after all
its purpose was not to demonstrate some new mathematical idea, and
also not how to program with deal.II, but rather to use the material
which we have developed in the previous examples to form something
which demonstrates a way to build modern finite element software in a
modular and extensible way.



Nevertheless, we of course show the results of the program. Of
foremost interest is the point value computation, for which we had
implemented the corresponding evaluation class. The results (i.e. the
output) of the program looks as follows:
@code
Running tests with "global" refinement criterion:
-------------------------------------------------
Refinement cycle: 0 1 2 3 4 5 6
DoFs  u(x_0)
   25 1.2868
   81 1.6945
  289 1.4658
 1089 1.5679
 4225 1.5882
16641 1.5932
66049 1.5945

Running tests with "kelly" refinement criterion:
------------------------------------------------
Refinement cycle: 0 1 2 3 4 5 6 7 8 9 10 11
DoFs  u(x_0)
   25 1.2868
   47 0.8775
   89 1.5365
  165 1.2974
  316 1.6442
  589 1.5221
 1093 1.5724
 2042 1.5627
 3766 1.5916
 7124 1.5876
13111 1.5942
24838 1.5932
@endcode


What surprises here is that the exact value is 1.59491554..., and that
it is apparently surprisingly complicated to compute the solution even to
only one per cent accuracy, although the solution is smooth (in fact
infinitely often differentiable). This smoothness is shown in the
graphical output generated by the program, here coarse grid and the
first 9 refinement steps of the Kelly refinement indicator:


<table width="80%" align="center">
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-13.solution-kelly-0.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-13.solution-kelly-1.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-13.solution-kelly-2.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-13.solution-kelly-3.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-13.solution-kelly-4.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-13.solution-kelly-5.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-13.solution-kelly-6.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-13.solution-kelly-7.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-13.solution-kelly-8.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-13.solution-kelly-9.png" alt="">
    </td>
  </tr>
</table>


While we're already at watching pictures, this is the eighth grid, as
viewed from top:


<img src="https://www.dealii.org/images/steps/developer/step-13.grid-kelly-8.png" alt="">


However, we are not yet finished with evaluation the point value
computation. In fact, plotting the error
$e=|u(x_0)-u_h(x_0)|$ for the two
refinement criteria yields the following picture:


<img src="https://www.dealii.org/images/steps/developer/step-13.error.png" alt="">




What <em>is</em> disturbing about this picture is that not only is the
adaptive mesh refinement not better than global refinement as one
would usually expect, it is even significantly worse since its
convergence is irregular, preventing all extrapolation techniques when
using the values of subsequent meshes! On the other hand, global
refinement provides a perfect $1/N$ or $h^{-2}$
convergence history and provides every opportunity to even improve on
the point values by extrapolation. Global mesh refinement must
therefore be considered superior in this example! This is even more
surprising as the evaluation point is not somewhere in the left part
where the mesh is coarse, but rather to the right and the adaptive
refinement should refine the mesh around the evaluation point as well.



We thus close the discussion of this example program with a question:

<p align="center">
  <strong><em>What is wrong with adaptivity if it is not better than
  global refinement?</em></strong>




<em>Exercise at the end of this example:</em> There is a simple reason
for the bad and irregular behavior of the adapted mesh solutions. It
is simple to find out by looking at the mesh around the evaluation
point in each of the steps - the data for this is in the output files
of the program. An exercise would therefore be to modify the mesh
refinement routine such that the problem (once you remark it) is
avoided. The second exercise is to check whether the results are then
better than global refinement, and if so if even a better order of
convergence (in terms of the number of degrees of freedom) is
achieved, or only by a better constant.



(<em>Very brief answers for the impatient:</em> at steps with larger
errors, the mesh is not regular at the point of evaluation, i.e. some
of the adjacent cells have hanging nodes; this destroys some
superapproximation effects of which the globally refined mesh can
profit. Answer 2: this quick hack
@code
  bool refinement_indicated = false;
  for (const auto &cell : triangulation.active_cell_iterators())
    for (const auto v : cell->vertex_indices())
	  if (cell->vertex(v) == Point<dim>(.5,.5))
	    {
	      cell->clear_coarsen_flag();
	      refinement_indicated |= cell->refine_flag_set();
	    }
  if (refinement_indicated)
    for (const auto &cell : triangulation.active_cell_iterators())
      for (const auto v : cell->vertex_indices())
	    if (cell->vertex(v) == Point<dim>(.5,.5))
	      cell->set_refine_flag ();
@endcode
in the refinement function of the Kelly refinement class right before
executing refinement would improve the results (exercise: what does
the code do?), making them consistently better than global
refinement. Behavior is still irregular, though, so no results about
an order of convergence are possible.)


examples/step-14/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

<h3>The maths</h3>

The Heidelberg group of Professor Rolf Rannacher, to which the three initial
authors of the deal.II library belonged during their PhD time and partly also
afterwards, has been involved with adaptivity and error estimation for finite
element discretizations since the mid-1990ies. The main achievement is the
development of error estimates for arbitrary functionals of the solution, and
of optimal mesh refinement for its computation.

We will not discuss the derivation of these concepts in too great detail, but
will implement the main ideas in the present example program. For a thorough
introduction into the general idea, we refer to the seminal work of Becker and
Rannacher @cite BR95, @cite BR96r, and the overview article of the same authors in
Acta Numerica @cite BR01; the first introduces the concept of error
estimation and adaptivity for general functional output for the Laplace
equation, while the second gives many examples of applications of these
concepts to a large number of other, more complicated equations. For
applications to individual types of equations, see also the publications by
Becker @cite Bec95, @cite Bec98, Kanschat @cite Kan96, @cite FK97, Suttmeier
@cite Sut96, @cite RS97, @cite RS98c, @cite RS99, Bangerth @cite BR99b,
@cite Ban00w, @cite BR01a, @cite Ban02, and Hartmann @cite Har02, @cite HH01,
@cite HH01b. All of these works, from the original introduction by Becker and
Rannacher to individual contributions to particular equations, have later been
summarized in a book by Bangerth and Rannacher that covers all of these topics,
see @cite BR03.


The basic idea is the following: in applications, one is not usually
interested in the solution per se, but rather in certain aspects of it. For
example, in simulations of flow problems, one may want to know the lift or
drag of a body immersed in the fluid; it is this quantity that we want to know
to best accuracy, and whether the rest of the solution of the describing
equations is well resolved is not of primary interest. Likewise, in elasticity
one might want to know about values of the stress at certain points to guess
whether maximal load values of joints are safe, for example. Or, in radiative
transfer problems, mean flux intensities are of interest.

In all the cases just listed, it is the evaluation of a functional $J(u)$ of
the solution which we are interested in, rather than the values of $u$
everywhere. Since the exact solution $u$ is not available, but only its
numerical approximation $u_h$, it is sensible to ask whether the computed
value $J(u_h)$ is within certain limits of the exact value $J(u)$, i.e. we
want to bound the error with respect to this functional, $J(u)-J(u_h)$.

For simplicity of exposition, we henceforth assume that both the quantity of
interest $J$, as well as the equation are linear, and we will in particular
show the derivation for the Laplace equation with homogeneous Dirichlet
boundary conditions, although the concept is much more general. For this
general case, we refer to the references listed above.  The goal is to obtain
bounds on the error, $J(e)=J(u)-J(u_h)$. For this, let us denote by $z$ the
solution of a dual problem, defined as follows:
@f[
  a(\varphi,z) = J(\varphi) \qquad \forall \varphi,
@f]
where $a(\cdot,\cdot)$ is the bilinear form associated with the differential
equation, and the test functions are chosen from the corresponding solution
space. Then, taking as special test function $\varphi=e$ the error, we have
that
@f[
  J(e) = a(e,z)
@f]
and we can, by Galerkin orthogonality, rewrite this as
@f[
  J(e) = a(e,z-\varphi_h)
@f]
where $\varphi_h$ can be chosen from the discrete test space in
whatever way we find convenient.

Concretely, for Laplace's equation, the error identity reads
@f[
  J(e) = (\nabla e, \nabla(z-\varphi_h)).
@f]
Because we want to use this formula not only to compute error, but
also to refine the mesh, we need to rewrite the expression above as a
sum over cells where each cell's contribution can then be used as an
error indicator for this cell.
Thus, we split the scalar products into terms for each cell, and
integrate by parts on each of them:
@f{eqnarray*}
  J(e)
  &=&
  \sum_K (\nabla (u-u_h), \nabla (z-\varphi_h))_K
  \\
  &=&
  \sum_K (-\Delta (u-u_h), z-\varphi_h)_K
  + (\partial_n (u-u_h), z-z_h)_{\partial K}.
@f}
Next we use that $-\Delta u=f$, and that for solutions of the Laplace
equation, the solution is smooth enough that $\partial_n u$ is
continuous almost everywhere -- so the terms involving $\partial_n u$ on one
cell cancels with that on its neighbor, where the normal vector has the
opposite sign. (The same is not true for $\partial_n u_h$, though.)
At the boundary of the domain, where there is no neighbor cell
with which this term could cancel, the weight $z-\varphi_h$ can be chosen as
zero, and the whole term disappears.

Thus, we have
@f{eqnarray*}
  J(e)
  &=&
  \sum_K (f+\Delta u_h, z-\varphi_h)_K
  - (\partial_n u_h, z-\varphi_h)_{\partial K\backslash \partial\Omega}.
@f}
In a final step, note that when taking the normal derivative of $u_h$, we mean
the value of this quantity as taken from this side of the cell (for the usual
Lagrange elements, derivatives are not continuous across edges). We then
rewrite the above formula by exchanging half of the edge integral of cell $K$
with the neighbor cell $K'$, to obtain
@f{eqnarray*}
  J(e)
  &=&
  \sum_K (f+\Delta u_h, z-\varphi_h)_K
  - \frac 12 (\partial_n u_h|_K + \partial_{n'} u_h|_{K'},
              z-\varphi_h)_{\partial K\backslash \partial\Omega}.
@f}
Using that for the normal vectors on adjacent cells we have $n'=-n$, we define the jump of the
normal derivative by
@f[
  [\partial_n u_h] \dealcoloneq \partial_n u_h|_K + \partial_{n'} u_h|_{K'}
  =
  \partial_n u_h|_K - \partial_n u_h|_{K'},
@f]
and get the final form after setting the discrete function $\varphi_h$, which
is by now still arbitrary, to the point interpolation of the dual solution,
$\varphi_h=I_h z$:
@f{eqnarray*}
  J(e)
  &=&
  \sum_K (f+\Delta u_h, z-I_h z)_K
  - \frac 12 ([\partial_n u_h],
              z-I_h z)_{\partial K\backslash \partial\Omega}.
@f}

With this, we have obtained an exact representation of the error of the finite
element discretization with respect to arbitrary (linear) functionals
$J(\cdot)$. Its structure is a weighted form of a residual estimator, as both
$f+\Delta u_h$ and $[\partial_n u_h]$ are cell and edge residuals that vanish
on the exact solution, and $z-I_h z$ are weights indicating how important the
residuals on a certain cell is for the evaluation of the given functional.
Furthermore, it is a cell-wise quantity, so we can use it as a mesh refinement
criterion. The question, is: how to evaluate it? After all, the evaluation
requires knowledge of the dual solution $z$, which carries the information
about the quantity we want to know to best accuracy.

In some, very special cases, this dual solution is known. For example, if the
functional $J(\cdot)$ is the point evaluation, $J(\varphi)=\varphi(x_0)$, then
the dual solution has to satisfy
@f[
  -\Delta z = \delta(x-x_0),
@f]
with the Dirac delta function on the right hand side, and the dual solution is
the Green's function with respect to the point $x_0$. For simple geometries,
this function is analytically known, and we could insert it into the error
representation formula.

However, we do not want to restrict ourselves to such special cases. Rather,
we will compute the dual solution numerically, and approximate $z$ by some
numerically obtained $\tilde z$. We note that it is not sufficient to compute
this approximation $\tilde z$ using the same method as used for the primal
solution $u_h$, since then $\tilde z-I_h \tilde z=0$, and the overall error
estimate would be zero. Rather, the approximation $\tilde z$ has to be from a
larger space than the primal finite element space. There are various ways to
obtain such an approximation (see the cited literature), and we will choose to
compute it with a higher order finite element space. While this is certainly
not the most efficient way, it is simple since we already have all we need to
do that in place, and it also allows for simple experimenting. For more
efficient methods, again refer to the given literature, in particular
@cite BR95, @cite BR03.

With this, we end the discussion of the mathematical side of this program and
turn to the actual implementation.


@note There are two steps above that do not seem necessary if all you
care about is computing the error: namely, (i) the subtraction of
$\phi_h$ from $z$, and (ii) splitting the integral into a sum of cells
and integrating by parts on each. Indeed, neither of these two steps
change $J(e)$ at all, as we only ever consider identities above until
the substitution of $z$ by $\tilde z$. In other words, if you care
only about <i>estimating the global error</i> $J(e)$, then these steps
are not necessary. On the other hand, if you want to use the error
estimate also as a refinement criterion for each cell of the mesh,
then it is necessary to (i) break the estimate into a sum of cells,
and (ii) massage the formulas in such a way that each cell's
contributions have something to do with the local error. (While the
contortions above do not change the value of the <i>sum</i> $J(e)$,
they change the values we compute for each cell $K$.) To this end, we
want to write everything in the form "residual times dual weight"
where a "residual" is something that goes to zero as the approximation
becomes $u_h$ better and better. For example, the quantity $\partial_n
u_h$ is not a residual, since it simply converges to the (normal
component of) the gradient of the exact solution. On the other hand,
$[\partial_n u_h]$ is a residual because it converges to $[\partial_n
u]=0$. All of the steps we have taken above in developing the final
form of $J(e)$ have indeed had the goal of bringing the final formula
into a form where each term converges to zero as the discrete solution
$u_h$ converges to $u$. This then allows considering each cell's
contribution as an "error indicator" that also converges to zero -- as
it should as the mesh is refined.



<h3>The software</h3>

The step-14 example program builds heavily on the techniques already used in
the step-13 program. Its implementation of the dual weighted residual error
estimator explained above is done by deriving a second class, properly called
<code>DualSolver</code>, from the <code>Solver</code> base class, and having a class
(<code>WeightedResidual</code>) that joins the two again and controls the solution
of the primal and dual problem, and then uses both to compute the error
indicator for mesh refinement.

The program continues the modular concept of the previous example, by
implementing the dual functional, describing quantity of interest, by an
abstract base class, and providing two different functionals which implement
this interface. Adding a different quantity of interest is thus simple.

One of the more fundamental differences is the handling of data. A common case
is that you develop a program that solves a certain equation, and test it with
different right hand sides, different domains, different coefficients and
boundary values, etc. Usually, these have to match, so that exact solutions
are known, or that their combination makes sense at all.

We demonstrate a way how this can be achieved in a simple, yet very flexible
way. We will put everything that belongs to a certain setup into one class,
and provide a little C++ mortar around it, so that entire setups (domains,
coefficients, right hand sides, etc.) can be exchanged by only changing
something in <em>one</em> place.

Going this way a little further, we have also centralized all the other
parameters that describe how the program is to work in one place, such as the
order of the finite element, the maximal number of degrees of freedom, the
evaluation objects that shall be executed on the computed solutions, and so
on. This allows for simpler configuration of the program, and we will show in
a later program how to use a library class that can handle setting these
parameters by reading an input file. The general aim is to reduce the places
within a program where one may have to look when wanting to change some
parameter, as it has turned out in practice that one forgets where they are as
programs grow. Furthermore, putting all options describing what the program
does in a certain run into a file (that can be stored with the results) helps
repeatability of results more than if the various flags were set somewhere in
the program, where their exact values are forgotten after the next change to
this place.

Unfortunately, the program has become rather long. While this admittedly
reduces its usefulness as an example program, we think that it is a very good
starting point for development of a program for other kinds of problems,
involving different equations than the Laplace equation treated here.
Furthermore, it shows everything that we can show you about our way of a
posteriori error estimation, and its structure should make it simple for you
to adjust this method to other problems, other functionals, other geometries,
coefficients, etc.

The author believes that the present program is his masterpiece among the
example programs, regarding the mathematical complexity, as well as the
simplicity to add extensions. If you use this program as a basis for your own
programs, we would kindly like to ask you to state this fact and the name of
the author of the example program, Wolfgang Bangerth, in publications that
arise from that, of your program consists in a considerable part of the
example program.


examples/step-14/doc/results.dox
<h1>Results</h1>

<h3>Point values</h3>


This program offers a lot of possibilities to play around. We can thus
only show a small part of all possible results that can be obtained
with the help of this program. However, you are encouraged to just try
it out, by changing the settings in the main program. Here, we start
by simply letting it run, unmodified:
@code
Refinement cycle: 0
   Number of degrees of freedom=72
   Point value=0.03243
   Estimated error=0.000702385
Refinement cycle: 1
   Number of degrees of freedom=67
   Point value=0.0324827
   Estimated error=0.000888953
Refinement cycle: 2
   Number of degrees of freedom=130
   Point value=0.0329619
   Estimated error=0.000454606
Refinement cycle: 3
   Number of degrees of freedom=307
   Point value=0.0331934
   Estimated error=0.000241254
Refinement cycle: 4
   Number of degrees of freedom=718
   Point value=0.0333675
   Estimated error=7.4912e-05
Refinement cycle: 5
   Number of degrees of freedom=1665
   Point value=0.0334083
   Estimated error=3.69111e-05
Refinement cycle: 6
   Number of degrees of freedom=3975
   Point value=0.033431
   Estimated error=1.54218e-05
Refinement cycle: 7
   Number of degrees of freedom=8934
   Point value=0.0334406
   Estimated error=6.28359e-06
Refinement cycle: 8
   Number of degrees of freedom=21799
   Point value=0.0334444
@endcode


First let's look what the program actually computed. On the seventh
grid, primal and dual numerical solutions look like this (using a
color scheme intended to evoke the snow-capped mountains of
Colorado that the original author of this program now calls
home):
<table align="center">
  <tr>
    <td width="50%">
      <img src="https://www.dealii.org/images/steps/developer/step-14.point-value.solution-7.9.2.png" alt="">
    </td>
    <td width="50%">
      <img src="https://www.dealii.org/images/steps/developer/step-14.point-value.solution-7-dual.9.2.png" alt="">
    </td>
  </tr>
</table>
Apparently, the region at the bottom left is so unimportant for the
point value evaluation at the top right that the grid is left entirely
unrefined there, even though the solution has singularities at the inner
corner of that cell! Due
to the symmetry in right hand side and domain, the solution should
actually look like at the top right in all four corners, but the mesh
refinement criterion involving the dual solution chose to refine them
differently -- because we said that we really only care about a single
function value somewhere at the top right.



Here are some of the meshes that are produced in refinement cycles 0,
2, 4 (top row), and 5, 7, and 8 (bottom row):

<table width="80%" align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-value.grid-0.9.2.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-value.grid-2.9.2.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-value.grid-4.9.2.png" alt="" width="100%"></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-value.grid-5.9.2.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-value.grid-7.9.2.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-value.grid-8.9.2.png" alt="" width="100%"></td>
  </tr>
</table>

Note the subtle interplay between resolving the corner singularities,
and resolving around the point of evaluation. It will be rather
difficult to generate such a mesh by hand, as this would involve to
judge quantitatively how much which of the four corner singularities
should be resolved, and to set the weight compared to the vicinity of
the evaluation point.



The program prints the point value and the estimated error in this
quantity. From extrapolating it, we can guess that the exact value is
somewhere close to 0.0334473, plus or minus 0.0000001 (note that we get
almost 6 valid digits from only 22,000 (primal) degrees of
freedom. This number cannot be obtained from the value of the
functional alone, but I have used the assumption that the error
estimator is mostly exact, and extrapolated the computed value plus
the estimated error, to get an approximation of the true
value. Computing with more degrees of freedom shows that this
assumption is indeed valid.



From the computed results, we can generate two graphs: one that shows
the convergence of the error $J(u)-J(u_h)$ (taking the
extrapolated value as correct) in the point value, and the value that
we get by adding up computed value $J(u_h)$ and estimated
error eta (if the error estimator $eta$ were exact, then the value
$J(u_h)+\eta$ would equal the exact point value, and the error
in this quantity would always be zero; however, since the error
estimator is only a - good - approximation to the true error, we can
by this only reduce the size of the error). In this graph, we also
indicate the complexity ${\cal O}(1/N)$ to show that mesh refinement
acts optimal in this case. The second chart compares
true and estimated error, and shows that the two are actually very
close to each other, even for such a complicated quantity as the point
value:


<table width="80%" align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-value.error.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-value.error-estimation.png" alt="" width="100%"></td>
  </tr>
</table>


<h3>Comparing refinement criteria</h3>


Since we have accepted quite some effort when using the mesh
refinement driven by the dual weighted error estimator (for solving
the dual problem, and for evaluating the error representation), it is
worth while asking whether that effort was successful. To this end, we
first compare the achieved error levels for different mesh refinement
criteria. To generate this data, simply change the value of the mesh
refinement criterion variable in the main program. The results are
thus (for the weight in the Kelly indicator, we have chosen the
function $1/(r^2+0.1^2)$, where $r$
is the distance to the evaluation point; it can be shown that this is
the optimal weight if we neglect the effects of boundaries):

<img src="https://www.dealii.org/images/steps/developer/step-14.point-value.error-comparison.png" alt="">



Checking these numbers, we see that for global refinement, the error
is proportional to $O(1/(sqrt(N) log(N)))$, and for the dual
estimator $O(1/N)$. Generally speaking, we see that the dual
weighted error estimator is better than the other refinement
indicators, at least when compared with those that have a similarly
regular behavior. The Kelly indicator produces smaller errors, but
jumps about the picture rather irregularly, with the error also
changing signs sometimes. Therefore, its behavior does not allow to
extrapolate the results to larger values of N. Furthermore, if we
trust the error estimates of the dual weighted error estimator, the
results can be improved by adding the estimated error to the computed
values. In terms of reliability, the weighted estimator is thus better
than the Kelly indicator, although the latter sometimes produces
smaller errors.



<h3>Evaluation of point stresses</h3>


Besides evaluating the values of the solution at a certain point, the
program also offers the possibility to evaluate the x-derivatives at a
certain point, and also to tailor mesh refinement for this. To let the
program compute these quantities, simply replace the two occurrences of
<code>PointValueEvaluation</code> in the main function by
<code>PointXDerivativeEvaluation</code>, and let the program run:
@code
Refinement cycle: 0
   Number of degrees of freedom=72
   Point x-derivative=-0.0719397
   Estimated error=-0.0126173
Refinement cycle: 1
   Number of degrees of freedom=61
   Point x-derivative=-0.0707956
   Estimated error=-0.00774316
Refinement cycle: 2
   Number of degrees of freedom=131
   Point x-derivative=-0.0568671
   Estimated error=-0.00313426
Refinement cycle: 3
   Number of degrees of freedom=247
   Point x-derivative=-0.053033
   Estimated error=-0.00136114
Refinement cycle: 4
   Number of degrees of freedom=532
   Point x-derivative=-0.0526429
   Estimated error=-0.000558868
Refinement cycle: 5
   Number of degrees of freedom=1267
   Point x-derivative=-0.0526955
   Estimated error=-0.000220116
Refinement cycle: 6
   Number of degrees of freedom=2864
   Point x-derivative=-0.0527495
   Estimated error=-9.46731e-05
Refinement cycle: 7
   Number of degrees of freedom=6409
   Point x-derivative=-0.052785
   Estimated error=-4.21543e-05
Refinement cycle: 8
   Number of degrees of freedom=14183
   Point x-derivative=-0.0528028
   Estimated error=-2.04241e-05
Refinement cycle: 9
   Number of degrees of freedom=29902
   Point x-derivative=-0.052814
@endcode



The solution looks roughly the same as before (the exact solution of
course <em>is</em> the same, only the grid changed a little), but the
dual solution is now different. A close-up around the point of
evaluation shows this:
<table align="center">
  <tr>
    <td width="50%">
      <img src="https://www.dealii.org/images/steps/developer/step-14.point-derivative.solution-7-dual.png" alt="">
    </td>
    <td width="50%">
      <img src="https://www.dealii.org/images/steps/developer/step-14.point-derivative.solution-7-dual-close-up.png" alt="">
    </td>
</table>
This time, the grids in refinement cycles 0, 5, 6, 7, 8, and 9 look
like this:

<table align="center" width="80%">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-derivative.grid-0.9.2.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-derivative.grid-5.9.2.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-derivative.grid-6.9.2.png" alt="" width="100%"></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-derivative.grid-7.9.2.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-derivative.grid-8.9.2.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-derivative.grid-9.9.2.png" alt="" width="100%"></td>
  </tr>
</table>

Note the asymmetry of the grids compared with those we obtained for
the point evaluation. This is due to the fact that the domain and the primal
solution may be symmetric about the diagonal, but the $x$-derivative is
not, and the latter enters the refinement criterion.



Then, it is interesting to compare actually computed values of the
quantity of interest (i.e. the x-derivative of the solution at one
point) with a reference value of -0.0528223... plus or minus
0.0000005. We get this reference value by computing on finer grid after
some more mesh refinements, with approximately 130,000 cells.
Recall that if the error is $O(1/N)$ in the optimal case, then
taking a mesh with ten times more cells gives us one additional digit
in the result.



In the left part of the following chart, you again see the convergence
of the error towards this extrapolated value, while on the right you
see a comparison of true and estimated error:

<table width="80%" align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-derivative.error.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.point-derivative.error-estimation.png" alt="" width="100%"></td>
  </tr>
</table>

After an initial phase where the true error changes its sign, the
estimated error matches it quite well, again. Also note the dramatic
improvement in the error when using the estimated error to correct the
computed value of $J(u_h)$.



<h3>step-13 revisited</h3>


If instead of the <code>Exercise_2_3</code> data set, we choose
<code>CurvedRidges</code> in the main function, and choose $(0.5,0.5)$
as the evaluation point, then we can redo the
computations of the previous example program, to compare whether the
results obtained with the help of the dual weighted error estimator
are better than those we had previously.



First, the meshes after 9 adaptive refinement cycles obtained with
the point evaluation and derivative evaluation refinement
criteria, respectively, look like this:

<table width="80%" align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.step-13.point-value.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.step-13.point-derivative.png" alt="" width="100%"></td>
  </tr>
</table>

The features of the solution can still be seen in the mesh, but since the
solution is smooth, the singularities of the dual solution entirely
dominate the mesh refinement criterion, and lead to strongly
concentrated meshes. The solution after the seventh refinement step looks
like the following:

<table width="40%" align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-14.step-13.solution-7.9.2.png" alt="" width="100%"></td>
  </tr>
</table>

Obviously, the solution is worse at some places, but the mesh
refinement process should have taken care that these places are not
important for computing the point value.




The next point is to compare the new (duality based) mesh refinement
criterion with the old ones. These are the results:

<img src="https://www.dealii.org/images/steps/developer/step-14.step-13.error-comparison.png" alt="">



The results are, well, somewhat mixed. First, the Kelly indicator
disqualifies itself by its unsteady behavior, changing the sign of the
error several times, and with increasing errors under mesh
refinement. The dual weighted error estimator has a monotone decrease
in the error, and is better than the weighted Kelly and global
refinement, but the margin is not as large as expected. This is, here,
due to the fact the global refinement can exploit the regular
structure of the meshes around the point of evaluation, which leads to
a better order of convergence for the point error. However, if we had
a mesh that is not locally rectangular, for example because we had to
approximate curved boundaries, or if the coefficients were not
constant, then this advantage of globally refinement meshes would
vanish, while the good performance of the duality based estimator
would remain.




<h3>Conclusions and outlook</h3>


The results here are not too clearly indicating the superiority of the
dual weighted error estimation approach for mesh refinement over other
mesh refinement criteria, such as the Kelly indicator. This is due to
the relative simplicity of the shown applications. If you are not
convinced yet that this approach is indeed superior, you are invited
to browse through the literature indicated in the introduction, where
plenty of examples are provided where the dual weighted approach can
reduce the necessary numerical work by orders of magnitude, making
this the only way to compute certain quantities to reasonable
accuracies at all.



Besides the objections you may raise against its use as a mesh
refinement criterion, consider that accurate knowledge of the error in
the quantity one might want to compute is of great use, since we can
stop computations when we are satisfied with the accuracy. Using more
traditional approaches, it is very difficult to get accurate estimates
for arbitrary quantities, except for, maybe, the error in the energy
norm, and we will then have no guarantee that the result we computed
satisfies any requirements on its accuracy. Also, as was shown for the
evaluation of point values and derivatives, the error estimate can be
used to extrapolate the results, yielding much higher accuracy in the
quantity we want to know.



Leaving these mathematical considerations, we tried to write the
program in a modular way, such that implementing another test case, or
another evaluation and dual functional is simple. You are encouraged
to take the program as a basis for your own experiments, and to play a
little.


examples/step-15/doc/intro.dox
<br>

<i>
This program grew out of a student project by Sven Wetterauer at the
University of Heidelberg, Germany. Most of the work for this program
is by him.
</i>
<br>


<a name="Intro"></a>
<h1>Introduction</h1>

<h3>Foreword</h3>

This program deals with an example of a non-linear elliptic partial
differential equation, the
[minimal surface equation](https://en.wikipedia.org/wiki/Minimal_surface).
You can imagine the solution of this equation to describe
the surface spanned by a soap film that is enclosed by a
closed wire loop. We imagine the wire to not just be a planar loop, but in
fact curved. The surface tension of the soap film will then reduce the surface
to have minimal surface. The solution of the minimal surface equation
describes this shape with the wire's vertical displacement as a boundary
condition. For simplicity, we will here assume that the surface can be written
as a graph $u=u(x,y)$ although it is clear that it is not very hard to
construct cases where the wire is bent in such a way that the surface can only
locally be constructed as a graph but not globally.

Because the equation is non-linear, we can't solve it directly. Rather, we
have to use Newton's method to compute the solution iteratively.

@dealiiVideoLecture{31.5,31.55,31.6}
(@dealiiVideoLectureSeeAlso{31.65,31.7})



<h3>Classical formulation</h3>

In a classical sense, the problem is given in the following form:


  @f{align*}
    -\nabla \cdot \left( \frac{1}{\sqrt{1+|\nabla u|^{2}}}\nabla u \right) &= 0 \qquad
    \qquad &&\textrm{in} ~ \Omega
    \\
    u&=g \qquad\qquad &&\textrm{on} ~ \partial \Omega.
  @f}

$\Omega$ is the domain we get by projecting the wire's positions into $x-y$
space. In this example, we choose $\Omega$ as the unit disk.

As described above, we solve this equation using Newton's method in which we
compute the $n$th approximate solution from the $(n-1)$th one, and use
a damping parameter $\alpha^n$ to get better global convergence behavior:
  @f{align*}
    F'(u^{n},\delta u^{n})&=- F(u^{n})
    \\
    u^{n+1}&=u^{n}+\alpha^n \delta u^{n}
  @f}
with
  @f[
    F(u) \dealcoloneq -\nabla \cdot \left( \frac{1}{\sqrt{1+|\nabla u|^{2}}}\nabla u \right)
  @f]
and $F'(u,\delta u)$ the derivative of F in direction of $\delta u$:
@f[
  F'(u,\delta u)=\lim \limits_{\epsilon \rightarrow 0}{\frac{F(u+\epsilon \delta u)-
  F(u)}{\epsilon}}.
@f]

Going through the motions to find out what $F'(u,\delta u)$ is, we find that
we have to solve a linear elliptic PDE in every Newton step, with $\delta u^n$
as the solution of:

  @f[
  - \nabla \cdot \left( \frac{1}{\left(1+|\nabla u^{n}|^{2}\right)^{\frac{1}{2}}}\nabla
  \delta u^{n} \right) +
  \nabla \cdot \left( \frac{\nabla u^{n} \cdot
  \nabla \delta u^{n}}{\left(1+|\nabla u^{n}|^{2}\right)^{\frac{3}{2}}} \nabla u^{n}
  \right)  =
  -\left( - \nabla \cdot \left( \frac{1}{\left(1+|\nabla u^{n}|^{2}\right)^{\frac{1}{2}}}
  \nabla u^{n} \right) \right)
  @f]

In order to solve the minimal surface equation, we have to solve this equation
repeatedly, once per Newton step. To solve this, we have to take a look at the
boundary condition of this problem. Assuming that $u^{n}$ already has the
right boundary values, the Newton update $\delta u^{n}$ should have zero
boundary conditions, in order to have the right boundary condition after
adding both.  In the first Newton step, we are starting with the solution
$u^{0}\equiv 0$, the Newton update still has to deliver the right boundary
condition to the solution $u^{1}$.


Summing up, we have to solve the PDE above with the boundary condition $\delta
u^{0}=g$ in the first step and with $\delta u^{n}=0$ in all the following steps.

@note In some sense, one may argue that if the program already
  implements $F(u)$, it is duplicative to also have to implement
  $F'(u,\delta)$. As always, duplication tempts bugs and we would like
  to avoid it. While we do not explore this issue in this program, we
  will come back to it at the end of the <a
  href="#extensions">Possibilities for extensions</a> section below,
  and specifically in step-72.


<h3>Weak formulation of the problem</h3>

Starting with the strong formulation above, we get the weak formulation by multiplying
both sides of the PDE with a test function $\varphi$ and integrating by parts on both sides:
  @f[
  \left( \nabla \varphi , \frac{1}{\left(1+|\nabla u^{n}|^{2}\right)^{\frac{1}{2}}}\nabla
  \delta u^{n} \right)-\left(\nabla \varphi ,\frac{\nabla u^{n} \cdot \nabla
  \delta u^{n}}{\left(1+|\nabla u^{n}|^{2}\right)^{\frac{3}{2}}}\nabla u^{n}  \right)
  = -\left(\nabla \varphi , \frac{1}{\left(1+|\nabla u^{n}|^{2}\right)^{\frac{1}{2}}} \nabla u^{n}
   \right).
  @f]
Here the solution $\delta u^{n}$ is a function in $H^{1}(\Omega)$, subject to
the boundary conditions discussed above.
Reducing this space to a finite dimensional space with basis $\left\{
\varphi_{0},\dots , \varphi_{N-1}\right\}$, we can write the solution:

@f[
  \delta u^{n}=\sum_{j=0}^{N-1} \delta U_{j} \varphi_{j}.
@f]

Using the basis functions as test functions and defining $a_{n} \dealcoloneq \frac{1}
{\sqrt{1+|\nabla u^{n}|^{2}}}$, we can rewrite the weak formulation:

@f[
  \sum_{j=0}^{N-1}\left[ \left( \nabla \varphi_{i} , a_{n} \nabla \varphi_{j} \right) -
  \left(\nabla u^{n}\cdot \nabla \varphi_{i} , a_{n}^{3} \nabla u^{n} \cdot \nabla
  \varphi_{j} \right) \right] \cdot \delta U_{j}=-\left( \nabla \varphi_{i} , a_{n}
  \nabla u^{n}\right) \qquad \forall i=0,\dots ,N-1,
@f]

where the solution $\delta u^{n}$ is given by the coefficients $\delta U^{n}_{j}$.
This linear system of equations can be rewritten as:

@f[
  A^{n}\; \delta U^{n}=b^{n},
@f]

where the entries of the matrix $A^{n}$ are given by:

@f[
  A^{n}_{ij} \dealcoloneq \left( \nabla \varphi_{i} , a_{n} \nabla \varphi_{j} \right) -
  \left(\nabla u^{n}\cdot \nabla \varphi_{i} , a_{n}^{3} \nabla u^{n} \cdot \nabla
  \varphi_{j} \right),
@f]

and the right hand side $b^{n}$ is given by:

@f[
  b^{n}_{i} \dealcoloneq -\left( \nabla \varphi_{i} , a_{n} \nabla u^{n}\right).
@f]


<h3> Questions about the appropriate solver </h3>

The matrix that corresponds to the Newton step above can be reformulated to
show its structure a bit better. Rewriting it slightly, we get that it has the
form
@f[
  A_{ij}
  =
  \left(
    \nabla \varphi_i,
    B
    \nabla \varphi_j
  \right),
@f]
where the matrix $B$ (of size $d \times d$ in $d$ space dimensions) is given
by the following expression:
@f[
  B
  =
  a_n \left\{
   \mathbf I
   -
   a_n^2 [\nabla u_n] \otimes [\nabla u_n]
  \right\}
  =
  a_n \left\{
   \mathbf I
   -
  \frac{\nabla u_n}{\sqrt{1+|\nabla u^{n}|^{2}}} \otimes
  \frac{\nabla u_n}{\sqrt{1+|\nabla u^{n}|^{2}}}
  \right\}.
@f]
From this expression, it is obvious that
$B$ is symmetric, and so $A$ is symmetric as well.
On the other hand, $B$ is also positive definite, which confers the same
property onto $A$. This can be seen by noting that the vector $v_1 =
\frac{\nabla u^n}{|\nabla u^n|}$ is an eigenvector of $B$ with eigenvalue
$\lambda_1=a_n \left(1-\frac{|\nabla u^n|^2}{1+|\nabla u^n|^2}\right) > 0$ while all vectors $v_2\ldots v_d$
that are perpendicular to $v_1$ and each other are eigenvectors with
eigenvalue $a_n$. Since all eigenvalues are positive, $B$ is positive definite
and so is $A$. We can thus use the CG method for solving the Newton steps.
(The fact that the matrix $A$ is symmetric and positive definite should not come
as a surprise. It results from taking the derivative of an operator that
results from taking the derivative of an energy functional: the minimal
surface equation simply minimizes some non-quadratic energy. Consequently,
the Newton matrix, as the matrix of second derivatives of a scalar energy,
must be symmetric since the derivative with regard to the $i$th and $j$th
degree of freedom should clearly commute. Likewise, if the energy functional
is convex, then the matrix of second derivatives must be positive definite,
and the direct calculation above simply reaffirms this.)

It is worth noting, however, that the positive definiteness degenerates for
problems where $\nabla u$ becomes large. In other words, if we simply multiply
all boundary values by 2, then to first order $u$ and $\nabla u$ will also be
multiplied by two, but as a consequence the smallest eigenvalue of $B$ will
become smaller and the matrix will become more ill-conditioned. (More
specifically, for $|\nabla u^n|\rightarrow\infty$ we have that
$\lambda_1 \propto a_n \frac{1}{|\nabla u^n|^2}$ whereas
$\lambda_2\ldots \lambda_d=a_n$; thus, the condition number of $B$,
which is a multiplicative factor in the condition number of $A$ grows
like ${\cal O}(|\nabla u^n|^2)$.) It is simple
to verify with the current program that indeed multiplying the boundary values
used in the current program by larger and larger values results in a problem
that will ultimately no longer be solvable using the simple preconditioned CG
method we use here.


<h3> Choice of step length and globalization </h3>

As stated above, Newton's method works by computing a direction
$\delta u^n$ and then performing the update $u^{n+1} = u^{n}+\alpha^n
\delta u^{n}$ with a step length $0 < \alpha^n \le 1$. It is a common
observation that for strongly nonlinear models, Newton's method does
not converge if we always choose $\alpha^n=1$ unless one starts with
an initial guess $u^0$ that is sufficiently close to the solution $u$
of the nonlinear problem. In practice, we don't always have such an
initial guess, and consequently taking full Newton steps (i.e., using
$\alpha=1$) does frequently not work.

A common strategy therefore is to use a smaller step length for the
first few steps while the iterate $u^n$ is still far away from the
solution $u$ and as we get closer use larger values for $\alpha^n$
until we can finally start to use full steps $\alpha^n=1$ as we are
close enough to the solution. The question is of course how to choose
$\alpha^n$. There are basically two widely used approaches: line
search and trust region methods.

In this program, we simply always choose the step length equal to
0.1. This makes sure that for the testcase at hand we do get
convergence although it is clear that by not eventually reverting to
full step lengths we forego the rapid, quadratic convergence that
makes Newton's method so appealing. Obviously, this is a point one
eventually has to address if the program was made into one that is
meant to solve more realistic problems. We will comment on this issue
some more in the <a href="#Results">results section</a>, and use an
even better approach in step-77.


<h3> Summary of the algorithm and testcase </h3>

Overall, the program we have here is not unlike step-6 in many regards. The
layout of the main class is essentially the same. On the other hand, the
driving algorithm in the <code>run()</code> function is different and works as
follows:
<ol>
<li>
  Start with the function $u^{0}\equiv 0$ and modify it in such a way
  that the values of $u^0$ along the boundary equal the correct
  boundary values $g$ (this happens in
  <code>MinimalSurfaceProblem::set_boundary_values</code>). Set
  $n=0$.
</li>

<li>
  Compute the Newton update by solving the system $A^{n}\;\delta
  U^{n}=b^{n}$
  with boundary condition $\delta u^{n}=0$ on $\partial \Omega$.
</li>

<li>
  Compute a step length $\alpha^n$. In this program, we always set
  $\alpha^n=0.1$. To make things easier to extend later on, this
  happens in a function of its own, namely in
  <code>MinimalSurfaceProblem::determine_step_length</code>.
  (The strategy of always choosing $\alpha^n=0.1$ is of course not
  optimal -- we should choose a step length that works for a given
  search direction -- but it requires a bit of work to do that. In the
  end, we leave these sorts of things to external packages: step-77
  does that.)
</li>

<li>
  The new approximation of the solution is given by
  $u^{n+1}=u^{n}+\alpha^n \delta u^{n}$.
</li>

<li>
  If $n$ is a multiple of 5 then refine the mesh, transfer the
  solution $u^{n+1}$ to the new mesh and set the values of $u^{n+1}$
  in such a way that along the boundary we have
  $u^{n+1}|_{\partial\Gamma}=g$ (again in
  <code>MinimalSurfaceProblem::set_boundary_values</code>). Note that
  this isn't automatically
  guaranteed even though by construction we had that before mesh
  refinement $u^{n+1}|_{\partial\Gamma}=g$ because mesh refinement
  adds new nodes to the mesh where we have to interpolate the old
  solution to the new nodes upon bringing the solution from the old to
  the new mesh. The values we choose by interpolation may be close to
  the exact boundary conditions but are, in general, nonetheless not
  the correct values.
</li>

<li>
  Set $n\leftarrow n+1$ and go to step 2.
</li>
</ol>

The testcase we solve is chosen as follows: We seek to find the solution of
minimal surface over the unit disk $\Omega=\{\mathbf x: \|\mathbf
x\|<1\}\subset {\mathbb R}^2$ where the surface attains the values
$u(x,y)|{\partial\Omega} = g(x,y) \dealcoloneq \sin(2 \pi (x+y))$ along the
boundary.


examples/step-15/doc/results.dox
<h1>Results</h1>


The output of the program looks as follows:
@code
Mesh refinement step 0
  Initial residual: 1.53143
  Residual: 1.08746
  Residual: 0.966748
  Residual: 0.859602
  Residual: 0.766462
  Residual: 0.685475

Mesh refinement step 1
  Initial residual: 0.868959
  Residual: 0.762125
  Residual: 0.677792
  Residual: 0.605762
  Residual: 0.542748
  Residual: 0.48704

Mesh refinement step 2
  Initial residual: 0.426445
  Residual: 0.382731
  Residual: 0.343865
  Residual: 0.30918
  Residual: 0.278147
  Residual: 0.250327

Mesh refinement step 3
  Initial residual: 0.282026
  Residual: 0.253146
  Residual: 0.227414
  Residual: 0.20441
  Residual: 0.183803
  Residual: 0.165319

Mesh refinement step 4
  Initial residual: 0.154404
  Residual: 0.138723
  Residual: 0.124694
  Residual: 0.112124
  Residual: 0.100847
  Residual: 0.0907222

....
@endcode

Obviously, the scheme converges, if not very fast. We will come back to
strategies for accelerating the method below.

One can visualize the solution after each set of five Newton
iterations, i.e., on each of the meshes on which we approximate the
solution. This yields the following set of images:

<div class="twocolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_15_solution_1.png"
         alt="Solution after zero cycles with contour lines." width="230" height="273">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_15_solution_2.png"
         alt="Solution after one cycle with contour lines." width="230" height="273">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_15_solution_3.png"
         alt="Solution after two cycles with contour lines." width="230" height="273">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_15_solution_4.png"
         alt="Solution after three cycles with contour lines." width="230" height="273">
  </div>
</div>

It is clearly visible, that the solution minimizes the surface
after each refinement. The solution converges to a picture one
would imagine a soap bubble to be that is located inside a wire loop
that is bent like
the boundary. Also it is visible, how the boundary
is smoothed out after each refinement. On the coarse mesh,
the boundary doesn't look like a sine, whereas it does the
finer the mesh gets.

The mesh is mostly refined near the boundary, where the solution
increases or decreases strongly, whereas it is coarsened on
the inside of the domain, where nothing interesting happens,
because there isn't much change in the solution. The ninth
solution and mesh are shown here:

<div class="onecolumn" style="width: 60%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_15_solution_9.png"
         alt="Grid and solution of the ninth cycle with contour lines." width="507" height="507">
  </div>
</div>



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

The program shows the basic structure of a solver for a nonlinear, stationary
problem. However, it does not converge particularly fast, for good reasons:

- The program always takes a step size of 0.1. This precludes the rapid,
  quadratic convergence for which Newton's method is typically chosen.
- It does not connect the nonlinear iteration with the mesh refinement
  iteration.

Obviously, a better program would have to address these two points.
We will discuss them in the following.


<h4> Step length control </h4>

Newton's method has two well known properties:
- It may not converge from arbitrarily chosen starting points. Rather, a
  starting point has to be close enough to the solution to guarantee
  convergence. However, we can enlarge the area from which Newton's method
  converges by damping the iteration using a <i>step length</i> 0<$\alpha^n\le
  1$.
- It exhibits rapid convergence of quadratic order if (i) the step length is
  chosen as $\alpha^n=1$, and (ii) it does in fact converge with this choice
  of step length.

A consequence of these two observations is that a successful strategy is to
choose $\alpha^n<1$ for the initial iterations until the iterate has come
close enough to allow for convergence with full step length, at which point we
want to switch to $\alpha^n=1$. The question is how to choose $\alpha^n$ in an
automatic fashion that satisfies these criteria.

We do not want to review the literature on this topic here, but only briefly
mention that there are two fundamental approaches to the problem: backtracking
line search and trust region methods. The former is more widely used for
partial differential equations and essentially does the following:
- Compute a search direction
- See if the resulting residual of $u^n + \alpha^n\;\delta u^n$ with
  $\alpha^n=1$ is "substantially smaller" than that of $u^n$ alone.
- If so, then take $\alpha^n=1$.
- If not, try whether the residual is "substantially smaller" with
  $\alpha^n=2/3$.
- If so, then take $\alpha^n=2/3$.
- If not, try whether the residual is "substantially smaller" with
  $\alpha^n=(2/3)^2$.
- Etc.
One can of course choose other factors $r, r^2, \ldots$ than the $2/3,
(2/3)^2, \ldots$ chosen above, for $0<r<1$. It is obvious where the term
"backtracking" comes from: we try a long step, but if that doesn't work we try
a shorter step, and ever shorter step, etc. The function
<code>determine_step_length()</code> is written the way it is to support
exactly this kind of use case.

Whether we accept a particular step length $\alpha^n$ depends on how we define
"substantially smaller". There are a number of ways to do so, but without
going into detail let us just mention that the most common ones are to use the
Wolfe and Armijo-Goldstein conditions. For these, one can show the following:
- There is always a step length $\alpha^n$ for which the conditions are
  satisfied, i.e., the iteration never gets stuck as long as the problem is
  convex.
- If we are close enough to the solution, then the conditions allow for
  $\alpha^n=1$, thereby enabling quadratic convergence.

We will not dwell on this here any further but leave the implementation of
such algorithms as an exercise. We note, however, that when implemented
correctly then it is a common observation that most reasonably nonlinear
problems can be solved in anywhere between 5 and 15 Newton iterations to
engineering accuracy &mdash; substantially fewer than we need with the current
version of the program.

More details on globalization methods including backtracking can be found,
for example, in @cite GNS08 and @cite NW99.

A separate point, very much worthwhile making, however, is that in practice
the implementation of efficient nonlinear solvers is about as complicated as
the implementation of efficient finite element methods. One should not
attempt to reinvent the wheel by implementing all of the necessary steps
oneself. Substantial pieces of the puzzle are already available in
the LineMinimization::line_search() function and could be used to this end.
But, instead, just like building finite element solvers on libraries
such as deal.II, one should be building nonlinear solvers on libraries such
as [SUNDIALS](https://computing.llnl.gov/projects/sundials). In fact,
deal.II has interfaces to SUNDIALS and in particular to its nonlinear solver
sub-package KINSOL through the SUNDIALS::KINSOL class. It would not be
very difficult to base the current problem on that interface --
indeed, that is what step-77 does.



<h4> Integrating mesh refinement and nonlinear and linear solvers </h4>

We currently do exactly 5 iterations on each mesh. But is this optimal? One
could ask the following questions:
- Maybe it is worthwhile doing more iterations on the initial meshes since
  there, computations are cheap.
- On the other hand, we do not want to do too many iterations on every mesh:
  yes, we could drive the residual to zero on every mesh, but that would only
  mean that the nonlinear iteration error is far smaller than the
  discretization error.
- Should we use solve the linear systems in each Newton step with higher or
  lower accuracy?

Ultimately, what this boils down to is that we somehow need to couple the
discretization error on the current mesh with the nonlinear residual we want
to achieve with the Newton iterations on a given mesh, and to the linear
iteration we want to achieve with the CG method within each Newton
iterations.

How to do this is, again, not entirely trivial, and we again leave it as a
future exercise.



<h4> Using automatic differentiation to compute the Jacobian matrix </h4>

As outlined in the introduction, when solving a nonlinear problem of
the form
  @f[
    F(u) \dealcoloneq
    -\nabla \cdot \left( \frac{1}{\sqrt{1+|\nabla u|^{2}}}\nabla u \right)
    = 0
  @f]
we use a Newton iteration that requires us to repeatedly solve the
linear partial differential equation
  @f{align*}
    F'(u^{n},\delta u^{n}) &=- F(u^{n})
  @f}
so that we can compute the update
  @f{align*}
    u^{n+1}&=u^{n}+\alpha^n \delta u^{n}
  @f}
with the solution $\delta u^{n}$ of the Newton step. For the problem
here, we could compute the derivative $F'(u,\delta u)$ by hand and
obtained
  @f[
  F'(u,\delta u)
  =
  - \nabla \cdot \left( \frac{1}{\left(1+|\nabla u|^{2}\right)^{\frac{1}{2}}}\nabla
  \delta u \right) +
  \nabla \cdot \left( \frac{\nabla u \cdot
  \nabla \delta u}{\left(1+|\nabla u|^{2}\right)^{\frac{3}{2}}} \nabla u
  \right).
  @f]
But this is already a sizable expression that is cumbersome both to
derive and to implement. It is also, in some sense, duplicative: If we
implement what $F(u)$ is somewhere in the code, then $F'(u,\delta u)$
is not an independent piece of information but is something that, at
least in principle, a computer should be able to infer itself.
Wouldn't it be nice if that could actually happen? That is, if we
really only had to implement $F(u)$, and $F'(u,\delta u)$ was then somehow
done implicitly? That is in fact possible, and runs under the name
"automatic differentiation". step-71 discusses this very
concept in general terms, and step-72 illustrates how this can be
applied in practice for the very problem we are considering here.


examples/step-16/doc/intro.dox
<br>

<i> Note: A variant called step-16b of this tutorial exists, that uses
MeshWorker and LocalIntegrators instead of assembling matrices manually as it
is done in this tutorial.
</i>

<a name="Intro"></a>
<h1>Introduction</h1>


This example shows the basic usage of the multilevel functions in deal.II. It
solves almost the same problem as used in step-6, but demonstrating the things
one has to provide when using multigrid as a preconditioner. In particular, this
requires that we define a hierarchy of levels, provide transfer operators from
one level to the next and back, and provide representations of the Laplace
operator on each level.

In order to allow sufficient flexibility in conjunction with systems of
differential equations and block preconditioners, quite a few different objects
have to be created before starting the multilevel method, although
most of what needs to be done is provided by deal.II itself. These are
  - the object handling transfer between grids; we use the MGTransferPrebuilt
    class for this that does almost all of the work inside the library,
  - the solver on the coarsest level; here, we use MGCoarseGridHouseholder,
  - the smoother on all other levels, which in our case will be the
    mg::SmootherRelaxation class using SOR as the underlying method,
  - and mg::Matrix, a class having a special level multiplication, i.e. we
    basically store one matrix per grid level and allow multiplication with it.

Most of these objects will only be needed inside the function that
actually solves the linear system. There, these objects are combined
in an object of type Multigrid, containing the implementation of the
V-cycle, which is in turn used by the preconditioner PreconditionMG,
ready for plug-in into a linear solver of the LAC library.

The multigrid method implemented here for adaptively refined meshes follows the
outline in the @ref mg_paper "Multigrid paper", which describes the underlying
implementation in deal.II and also introduces a lot of the nomenclature. First,
we have to distinguish between level meshes, namely cells that have the same
refinement distance from the coarse mesh, and the leaf mesh consisting of active
cells of the hierarchy (in older work we refer to this as the global mesh, but
this term is overused). Most importantly, the leaf mesh is not identical with
the level mesh on the finest level. The following image shows what we consider
to be a "level mesh":

<p align="center">
  @image html "multigrid.png" ""
</p>

The fine level in this mesh consists only of the degrees of freedom that are
defined on the refined cells, but does not extend to that part of the domain
that is not refined. While this guarantees that the overall effort grows as
${\cal O}(N)$ as necessary for optimal multigrid complexity, it leads to
problems when defining where to smooth and what boundary conditions to pose for
the operators defined on individual levels if the level boundary is not an
external boundary. These questions are discussed in detail in the article cited
above.

<h3>The testcase</h3>

The problem we solve here is similar to step-6, with two main
differences: first, the multigrid preconditioner, obviously. We also
change the discontinuity of the coefficients such that the local
assembler does not look more complicated than necessary.


examples/step-16/doc/results.dox
<h1>Results</h1>

On the finest mesh, the solution looks like this:

<p align="center">
  <img src="https://www.dealii.org/images/steps/developer/step-16.solution.png" alt="">
</p>

More importantly, we would like to see if the multigrid method really improved
the solver performance. Therefore, here is the textual output:

<pre>
Cycle 0
   Number of active cells:       80
   Number of degrees of freedom: 89 (by level: 8, 25, 89)
   Number of CG iterations: 8

Cycle 1
   Number of active cells:       158
   Number of degrees of freedom: 183 (by level: 8, 25, 89, 138)
   Number of CG iterations: 9

Cycle 2
   Number of active cells:       302
   Number of degrees of freedom: 352 (by level: 8, 25, 89, 223, 160)
   Number of CG iterations: 10

Cycle 3
   Number of active cells:       578
   Number of degrees of freedom: 649 (by level: 8, 25, 89, 231, 494, 66)
   Number of CG iterations: 10

Cycle 4
   Number of active cells:       1100
   Number of degrees of freedom: 1218 (by level: 8, 25, 89, 274, 764, 417, 126)
   Number of CG iterations: 10

Cycle 5
   Number of active cells:       2096
   Number of degrees of freedom: 2317 (by level: 8, 25, 89, 304, 779, 1214, 817)
   Number of CG iterations: 11

Cycle 6
   Number of active cells:       3986
   Number of degrees of freedom: 4366 (by level: 8, 25, 89, 337, 836, 2270, 897, 1617)
   Number of CG iterations: 10

Cycle 7
   Number of active cells:       7574
   Number of degrees of freedom: 8350 (by level: 8, 25, 89, 337, 1086, 2835, 2268, 1789, 3217)
   Number of CG iterations: 11
</pre>

That's almost perfect multigrid performance: the linear residual gets reduced by 12 orders of
magnitude in 10 iteration steps, and the results are almost independent of the mesh size. That's
obviously in part due to the simple nature of the problem solved, but
it shows the power of multigrid methods.


<h3> Possibilities for extensions </h3>


We encourage you to generate timings for the solve() call and compare to
step-6. You will see that the multigrid method has quite an overhead
on coarse meshes, but that it always beats other methods on fine
meshes because of its optimal complexity.

A close inspection of this program's performance shows that it is mostly
dominated by matrix-vector operations. step-37 shows one way
how this can be avoided by working with matrix-free methods.

Another avenue would be to use algebraic multigrid methods. The geometric
multigrid method used here can at times be a bit awkward to implement because it
needs all those additional data structures, and it becomes even more difficult
if the program is to run in %parallel on machines coupled through MPI, for
example. In that case, it would be simpler if one could use a black-box
preconditioner that uses some sort of multigrid hierarchy for good performance
but can figure out level matrices and similar things by itself. Algebraic
multigrid methods do exactly this, and we will use them in step-31 for the
solution of a Stokes problem and in step-32 and step-40 for a parallel
variation. That said, a parallel version of this example program with MPI can be
found in step-50.

Finally, one may want to think how to use geometric multigrid for other kinds of
problems, specifically @ref vector_valued "vector valued problems". This is the
topic of step-56 where we use the techniques shown here for the Stokes equation.


examples/step-16b/doc/intro.dox
<br>

<a name="Intro"></a>
<h1>Introduction</h1>

This is a variant of step-16 with the only change that we are using the
MeshWorker framework with the pre-made LocalIntegrator helper classes instead
of manually assembling the matrices.

The details of this framework on how it is used in practice will be explained
as part of this tutorial program.

<h3>The testcase</h3>

The problem we solve here is the same as the one in step-16.


examples/step-16b/doc/results.dox
<h1>Results</h1>

As in step-16, the solution looks like this on the finest mesh:

<p align="center">
  <img src="https://www.dealii.org/images/steps/developer/step-16.solution.png" alt="">
</p>

The output is formatted in a slightly different way compared to step-16 but is
functionally the same and shows the same convergence properties:
<pre>
DEAL::Cycle 0
DEAL::   Number of active cells:       20
DEAL::   Number of degrees of freedom: 25 (by level: 8, 25)
DEAL:cg::Starting value 0.510691
DEAL:cg::Convergence step 6 value 4.59193e-14
DEAL::Cycle 1
DEAL::   Number of active cells:       44
DEAL::   Number of degrees of freedom: 55 (by level: 8, 25, 45)
DEAL:cg::Starting value 0.440678
DEAL:cg::Convergence step 8 value 1.99419e-13
DEAL::Cycle 2
DEAL::   Number of active cells:       86
DEAL::   Number of degrees of freedom: 105 (by level: 8, 25, 69, 49)
DEAL:cg::Starting value 0.371855
DEAL:cg::Convergence step 9 value 1.13984e-13
DEAL::Cycle 3
DEAL::   Number of active cells:       170
DEAL::   Number of degrees of freedom: 200 (by level: 8, 25, 77, 174)
DEAL:cg::Starting value 0.318967
DEAL:cg::Convergence step 9 value 2.62112e-13
DEAL::Cycle 4
DEAL::   Number of active cells:       332
DEAL::   Number of degrees of freedom: 388 (by level: 8, 25, 86, 231, 204)
DEAL:cg::Starting value 0.276534
DEAL:cg::Convergence step 10 value 1.69562e-13
DEAL::Cycle 5
DEAL::   Number of active cells:       632
DEAL::   Number of degrees of freedom: 714 (by level: 8, 25, 89, 231, 514, 141)
DEAL:cg::Starting value 0.215300
DEAL:cg::Convergence step 10 value 6.47463e-13
DEAL::Cycle 6
DEAL::   Number of active cells:       1202
DEAL::   Number of degrees of freedom: 1332 (by level: 8, 25, 89, 282, 771, 435, 257)
DEAL:cg::Starting value 0.175848
DEAL:cg::Convergence step 10 value 1.80664e-13
DEAL::Cycle 7
DEAL::   Number of active cells:       2288
DEAL::   Number of degrees of freedom: 2511 (by level: 8, 25, 89, 318, 779, 1420, 829, 30)
DEAL:cg::Starting value 0.136724
DEAL:cg::Convergence step 11 value 9.73331e-14
</pre>


examples/step-17/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

<h3>Overview</h3>

This program does not introduce any new mathematical ideas; in fact, all it
does is to do the exact same computations that step-8
already does, but it does so in a different manner: instead of using deal.II's
own linear algebra classes, we build everything on top of classes deal.II
provides that wrap around the linear algebra implementation of the <a
href="http://www.mcs.anl.gov/petsc/" target="_top">PETSc</a> library. And
since PETSc allows to distribute matrices and vectors across several computers
within an MPI network, the resulting code will even be able to solve the
problem in %parallel. If you don't know what PETSc is, then this would be a
good time to take a quick glimpse at their homepage.

As a prerequisite of this program, you need to have PETSc installed, and if
you want to run in %parallel on a cluster, you also need <a
href="http://www-users.cs.umn.edu/~karypis/metis/index.html"
target="_top">METIS</a> to partition meshes. The installation of deal.II
together with these two additional libraries is described in the <a
href="../../readme.html" target="body">README</a> file.

Now, for the details: as mentioned, the program does not compute anything new,
so the use of finite element classes, etc., is exactly the same as before. The
difference to previous programs is that we have replaced almost all uses of
classes <code>Vector</code> and <code>SparseMatrix</code> by their
near-equivalents <code>PETScWrappers::MPI::Vector</code> and
<code>PETScWrappers::MPI::SparseMatrix</code> that store data in a way so that
every processor in the MPI network only stores
a part of the matrix or vector. More specifically, each processor will
only store those rows of the matrix that correspond to a degree of
freedom it "owns". For vectors, they either store only elements that
correspond to degrees of freedom the processor owns (this is what is
necessary for the right hand side), or also some additional elements
that make sure that every processor has access the solution components
that live on the cells the processor owns (so-called
@ref GlossLocallyActiveDof "locally active DoFs") or also on neighboring cells
(so-called @ref GlossLocallyRelevantDof "locally relevant DoFs").

The interface the classes from the PETScWrapper namespace provide is very similar to that
of the deal.II linear algebra classes, but instead of implementing this
functionality themselves, they simply pass on to their corresponding PETSc
functions. The wrappers are therefore only used to give PETSc a more modern,
object oriented interface, and to make the use of PETSc and deal.II objects as
interchangeable as possible. The main point of using PETSc is that it can run
in %parallel. We will make use of this by partitioning the domain into as many
blocks ("subdomains") as there are processes in the MPI network. At the same
time, PETSc also provides dummy MPI stubs, so you can run this program on a
single machine if PETSc was configured without MPI.


<h3>Parallelizing software with MPI</h3>

Developing software to run in %parallel via MPI requires a bit of a change in
mindset because one typically has to split up all data structures so that
every processor only stores a piece of the entire problem. As a consequence,
you can't typically access all components of a solution vector on each
processor -- each processor may simply not have enough memory to hold the
entire solution vector. Because data is split up or "distributed" across
processors, we call the programming model used by MPI "distributed memory
computing" (as opposed to "shared memory computing", which would mean
that multiple processors can all access all data within one memory
space, for example whenever multiple cores in a single machine work
on a common task). Some of the fundamentals of distributed memory
computing are discussed in the
@ref distributed "Parallel computing with multiple processors using distributed memory"
documentation module, which is itself a sub-module of the
@ref Parallel "Parallel computing" module.

In general, to be truly able to scale to large numbers of processors, one
needs to split between the available processors <i>every</i> data structure
whose size scales with the size of the overall problem. (For a definition
of what it means for a program to "scale", see
@ref GlossParallelScaling "this glossary entry".) This includes, for
example, the triangulation, the matrix, and all global vectors (solution, right
hand side). If one doesn't split all of these objects, one of those will be
replicated on all processors and will eventually simply become too large
if the problem size (and the number of available processors) becomes large.
(On the other hand, it is completely fine to keep objects with a size that
is independent of the overall problem size on every processor. For example,
each copy of the executable will create its own finite element object, or the
local matrix we use in the assembly.)

In the current program (as well as in the related step-18), we will not go
quite this far but present a gentler introduction to using MPI. More
specifically, the only data structures we will parallelize are matrices and
vectors. We do, however, not split up the Triangulation and
DoFHandler classes: each process still has a complete copy of
these objects, and all processes have exact copies of what the other processes
have. We will then simply have to mark, in each copy of the triangulation
on each of the processors, which processor owns which cells. This
process is called "partitioning" a mesh into @ref GlossSubdomainId "subdomains".

For larger problems, having to store the <i>entire</i> mesh on every processor
will clearly yield a bottleneck. Splitting up the mesh is slightly, though not
much more complicated (from a user perspective, though it is <i>much</i> more
complicated under the hood) to achieve and
we will show how to do this in step-40 and some other programs. There are
numerous occasions where, in the course of discussing how a function of this
program works, we will comment on the fact that it will not scale to large
problems and why not. All of these issues will be addressed in step-18 and
in particular step-40, which scales to very large numbers of processes.

Philosophically, the way MPI operates is as follows. You typically run a
program via
@code
  mpirun -np 32 ./step-17
@endcode
which means to run it on (say) 32 processors. (If you are on a cluster system,
you typically need to <i>schedule</i> the program to run whenever 32 processors
become available; this will be described in the documentation of your
cluster. But under the hood, whenever those processors become available,
the same call as above will generally be executed.) What this does is that
the MPI system will start 32 <i>copies</i> of the <code>step-17</code>
executable. (The MPI term for each of these running executables is that you
have 32 @ref GlossMPIProcess "MPI processes".)
This may happen on different machines that can't even read
from each others' memory spaces, or it may happen on the same machine, but
the end result is the same: each of these 32 copies will run with some
memory allocated to it by the operating system, and it will not directly
be able to read the memory of the other 31 copies. In order to collaborate
in a common task, these 32 copies then have to <i>communicate</i> with
each other. MPI, short for <i>Message Passing Interface</i>, makes this
possible by allowing programs to <i>send messages</i>. You can think
of this as the mail service: you can put a letter to a specific address
into the mail and it will be delivered. But that's the extent to which
you can control things. If you want the receiver to do something
with the content of the letter, for example return to you data you want
from over there, then two things need to happen: (i) the receiver needs
to actually go check whether there is anything in their mailbox, and (ii) if
there is, react appropriately, for example by sending data back. If you
wait for this return message but the original receiver was distracted
and not paying attention, then you're out of luck: you'll simply have to
wait until your requested over there will be worked on. In some cases,
bugs will lead the original receiver to never check your mail, and in that
case you will wait forever -- this is called a <i>deadlock</i>.
(@dealiiVideoLectureSeeAlso{39,41,41.25,41.5})

In practice, one does not usually program at the level of sending and
receiving individual messages, but uses higher level operations. For
example, in the program we will use function calls that take a number
from each processor, add them all up, and return the sum to all
processors. Internally, this is implemented using individual messages,
but to the user this is transparent. We call such operations <i>collectives</i>
because <i>all</i> processors participate in them. Collectives allow us
to write programs where not every copy of the executable is doing something
completely different (this would be incredibly difficult to program) but
where in essence all copies are doing the same thing (though on different
data) for themselves, running through the same blocks of code; then they
communicate data through collectives; and then go back to doing something
for themselves again running through the same blocks of data. This is the
key piece to being able to write programs, and it is the key component
to making sure that programs can run on any number of processors,
since we do not have to write different code for each of the participating
processors.

(This is not to say that programs are never written in ways where
different processors run through different blocks of code in their
copy of the executable. Programs internally also often communicate
in other ways than through collectives. But in practice, %parallel finite
element codes almost always follow the scheme where every copy
of the program runs through the same blocks of code at the same time,
interspersed by phases where all processors communicate with each other.)

In reality, even the level of calling MPI collective functions is too
low. Rather, the program below will not contain any direct
calls to MPI at all, but only deal.II functions that hide this
communication from users of the deal.II. This has the advantage that
you don't have to learn the details of MPI and its rather intricate
function calls. That said, you do have to understand the general
philosophy behind MPI as outlined above.


<h3>What this program does</h3>

The techniques this program then demonstrates are:
- How to use the PETSc wrapper classes; this will already be visible in the
  declaration of the principal class of this program, <code>ElasticProblem</code>.
- How to partition the mesh into subdomains; this happens in the
  <code>ElasticProblem::setup_system()</code> function.
- How to parallelize operations for jobs running on an MPI network; here, this
  is something one has to pay attention to in a number of places, most
  notably in the  <code>ElasticProblem::assemble_system()</code> function.
- How to deal with vectors that store only a subset of vector entries
  and for which we have to ensure that they store what we need on the
  current processors. See for example the
  <code>ElasticProblem::solve()</code> and <code>ElasticProblem::refine_grid()</code>
  functions.
- How to deal with status output from programs that run on multiple
  processors at the same time. This is done via the <code>pcout</code>
  variable in the program, initialized in the constructor.

Since all this can only be demonstrated using actual code, let us go straight to the
code without much further ado.


examples/step-17/doc/results.dox
<h1>Results</h1>


If the program above is compiled and run on a single processor
machine, it should generate results that are very similar to those
that we already got with step-8. However, it becomes more interesting
if we run it on a multicore machine or a cluster of computers. The
most basic way to run MPI programs is using a command line like
@code
  mpirun -np 32 ./step-17
@endcode
to run the step-17 executable with 32 processors.

(If you work on a cluster, then there is typically a step in between where you
need to set up a job script and submit the script to a scheduler. The scheduler
will execute the script whenever it can allocate 32 unused processors for your
job. How to write such job
scripts differs from cluster to cluster, and you should find the documentation
of your cluster to see how to do this. On my system, I have to use the command
<code>qsub</code> with a whole host of options to run a job in parallel.)

Whether directly or through a scheduler, if you run this program on 8
processors, you should get output like the following:
@code
Cycle 0:
   Number of active cells:       64
   Number of degrees of freedom: 162 (by partition: 22+22+20+20+18+16+20+24)
   Solver converged in 23 iterations.
Cycle 1:
   Number of active cells:       124
   Number of degrees of freedom: 302 (by partition: 38+42+36+34+44+44+36+28)
   Solver converged in 35 iterations.
Cycle 2:
   Number of active cells:       238
   Number of degrees of freedom: 570 (by partition: 68+80+66+74+58+68+78+78)
   Solver converged in 46 iterations.
Cycle 3:
   Number of active cells:       454
   Number of degrees of freedom: 1046 (by partition: 120+134+124+130+154+138+122+124)
   Solver converged in 55 iterations.
Cycle 4:
   Number of active cells:       868
   Number of degrees of freedom: 1926 (by partition: 232+276+214+248+230+224+234+268)
   Solver converged in 77 iterations.
Cycle 5:
   Number of active cells:       1654
   Number of degrees of freedom: 3550 (by partition: 418+466+432+470+442+474+424+424)
   Solver converged in 93 iterations.
Cycle 6:
   Number of active cells:       3136
   Number of degrees of freedom: 6702 (by partition: 838+796+828+892+866+798+878+806)
   Solver converged in 127 iterations.
Cycle 7:
   Number of active cells:       5962
   Number of degrees of freedom: 12446 (by partition: 1586+1484+1652+1552+1556+1576+1560+1480)
   Solver converged in 158 iterations.
Cycle 8:
   Number of active cells:       11320
   Number of degrees of freedom: 23586 (by partition: 2988+2924+2890+2868+2864+3042+2932+3078)
   Solver converged in 225 iterations.
Cycle 9:
   Number of active cells:       21424
   Number of degrees of freedom: 43986 (by partition: 5470+5376+5642+5450+5630+5470+5416+5532)
   Solver converged in 282 iterations.
Cycle 10:
   Number of active cells:       40696
   Number of degrees of freedom: 83754 (by partition: 10660+10606+10364+10258+10354+10322+10586+10604)
   Solver converged in 392 iterations.
Cycle 11:
   Number of active cells:       76978
   Number of degrees of freedom: 156490 (by partition: 19516+20148+19390+19390+19336+19450+19730+19530)
   Solver converged in 509 iterations.
Cycle 12:
   Number of active cells:       146206
   Number of degrees of freedom: 297994 (by partition: 37462+37780+37000+37060+37232+37328+36860+37272)
   Solver converged in 705 iterations.
Cycle 13:
   Number of active cells:       276184
   Number of degrees of freedom: 558766 (by partition: 69206+69404+69882+71266+70348+69616+69796+69248)
   Solver converged in 945 iterations.
Cycle 14:
   Number of active cells:       523000
   Number of degrees of freedom: 1060258 (by partition: 132928+132296+131626+132172+132170+133588+132252+133226)
   Solver converged in 1282 iterations.
Cycle 15:
   Number of active cells:       987394
   Number of degrees of freedom: 1994226 (by partition: 253276+249068+247430+248402+248496+251380+248272+247902)
   Solver converged in 1760 iterations.
Cycle 16:
   Number of active cells:       1867477
   Number of degrees of freedom: 3771884 (by partition: 468452+474204+470818+470884+469960+
471186+470686+475694)
   Solver converged in 2251 iterations.
@endcode
(This run uses a few more refinement cycles than the code available in
the examples/ directory. The run also used a version of METIS from
2004 that generated different partitionings; consequently,
the numbers you get today are slightly different.)

As can be seen, we can easily get to almost four million unknowns. In fact, the
code's runtime with 8 processes was less than 7 minutes up to (and including)
cycle 14, and 14 minutes including the second to last step. (These are numbers
relevant to when the code was initially written, in 2004.) I lost the timing
information for the last step, though, but you get the idea. All this is after
release mode has been enabled by running <code>make release</code>, and
with the generation of graphical output switched off for the reasons stated in
the program comments above.
(@dealiiVideoLectureSeeAlso{18})
The biggest 2d computations I did had roughly 7.1
million unknowns, and were done on 32 processes. It took about 40 minutes.
Not surprisingly, the limiting factor for how far one can go is how much memory
one has, since every process has to hold the entire mesh and DoFHandler objects,
although matrices and vectors are split up. For the 7.1M computation, the memory
consumption was about 600 bytes per unknown, which is not bad, but one has to
consider that this is for every unknown, whether we store the matrix and vector
entries locally or not.



Here is some output generated in the 12th cycle of the program, i.e. with roughly
300,000 unknowns:

<table align="center" style="width:80%">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-17.12-ux.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-17.12-uy.png" alt="" width="100%"></td>
  </tr>
</table>

As one would hope for, the x- (left) and y-displacements (right) shown here
closely match what we already saw in step-8. As shown
there and in step-22, we could as well have produced a
vector plot of the displacement field, rather than plotting it as two
separate scalar fields. What may be more interesting,
though, is to look at the mesh and partition at this step:

<table align="center" width="80%">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-17.12-grid.png" alt="" width="100%"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-17.12-partition.png" alt="" width="100%"></td>
  </tr>
</table>

Again, the mesh (left) shows the same refinement pattern as seen
previously. The right panel shows the partitioning of the domain across the 8
processes, each indicated by a different color. The picture shows that the
subdomains are smaller where mesh cells are small, a fact that needs to be
expected given that the partitioning algorithm tries to equilibrate the number
of cells in each subdomain; this equilibration is also easily identified in
the output shown above, where the number of degrees per subdomain is roughly
the same.



It is worth noting that if we ran the same program with a different number of
processes, that we would likely get slightly different output: a different
mesh, different number of unknowns and iterations to convergence. The reason
for this is that while the matrix and right hand side are the same independent
of the number of processes used, the preconditioner is not: it performs an
ILU(0) on the chunk of the matrix of <em>each processor separately</em>. Thus,
it's effectiveness as a preconditioner diminishes as the number of processes
increases, which makes the number of iterations increase. Since a different
preconditioner leads to slight changes in the computed solution, this will
then lead to slightly different mesh cells tagged for refinement, and larger
differences in subsequent steps. The solution will always look very similar,
though.



Finally, here are some results for a 3d simulation. You can repeat these by
changing
@code
        ElasticProblem<2> elastic_problem;
@endcode
to
@code
        ElasticProblem<3> elastic_problem;
@endcode
in the main function. If you then run the program in parallel,
you get something similar to this (this is for a job with 16 processes):
@code
Cycle 0:
   Number of active cells:       512
   Number of degrees of freedom: 2187 (by partition: 114+156+150+114+114+210+105+102+120+120+96+123+141+183+156+183)
   Solver converged in 27 iterations.
Cycle 1:
   Number of active cells:       1604
   Number of degrees of freedom: 6549 (by partition: 393+291+342+354+414+417+570+366+444+288+543+525+345+387+489+381)
   Solver converged in 42 iterations.
Cycle 2:
   Number of active cells:       4992
   Number of degrees of freedom: 19167 (by partition: 1428+1266+1095+1005+1455+1257+1410+1041+1320+1380+1080+1050+963+1005+1188+1224)
   Solver converged in 65 iterations.
Cycle 3:
   Number of active cells:       15485
   Number of degrees of freedom: 56760 (by partition: 3099+3714+3384+3147+4332+3858+3615+3117+3027+3888+3942+3276+4149+3519+3030+3663)
   Solver converged in 96 iterations.
Cycle 4:
   Number of active cells:       48014
   Number of degrees of freedom: 168762 (by partition: 11043+10752+9846+10752+9918+10584+10545+11433+12393+11289+10488+9885+10056+9771+11031+8976)
   Solver converged in 132 iterations.
Cycle 5:
   Number of active cells:       148828
   Number of degrees of freedom: 492303 (by partition: 31359+30588+34638+32244+30984+28902+33297+31569+29778+29694+28482+28032+32283+30702+31491+28260)
   Solver converged in 179 iterations.
Cycle 6:
   Number of active cells:       461392
   Number of degrees of freedom: 1497951 (by partition: 103587+100827+97611+93726+93429+88074+95892+88296+96882+93000+87864+90915+92232+86931+98091+90594)
   Solver converged in 261 iterations.
@endcode



The last step, going up to 1.5 million unknowns, takes about 55 minutes with
16 processes on 8 dual-processor machines (of the kind available in 2003). The
graphical output generated by
this job is rather large (cycle 5 already prints around 82 MB of data), so
we contend ourselves with showing output from cycle 4:

<table width="80%" align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-17.4-3d-partition.png" width="100%" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-17.4-3d-ux.png" alt="" width="100%"></td>
  </tr>
</table>



The left picture shows the partitioning of the cube into 16 processes, whereas
the right one shows the x-displacement along two cutplanes through the cube.



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

The program keeps a complete copy of the Triangulation and DoFHandler objects
on every processor. It also creates complete copies of the solution vector,
and it creates output on only one processor. All of this is obviously
the bottleneck as far as parallelization is concerned.

Internally, within deal.II, parallelizing the data
structures used in hierarchic and unstructured triangulations is a hard
problem, and it took us a few more years to make this happen. The step-40
tutorial program and the @ref distributed documentation module talk about how
to do these steps and what it takes from an application perspective. An
obvious extension of the current program would be to use this functionality to
completely distribute computations to many more processors than used here.


examples/step-18/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>


This tutorial program is another one in the series on the elasticity problem
that we have already started with step-8 and step-17. It extends it into two
different directions: first, it solves the quasistatic but time dependent
elasticity problem for large deformations with a Lagrangian mesh movement
approach. Secondly, it shows some more techniques for solving such problems
using %parallel processing with PETSc's linear algebra. In addition to this,
we show how to work around one of the two major bottlenecks of step-17, namely
that we generated graphical output from only one process, and that this scaled
very badly with larger numbers of processes and on large problems. (The other
bottleneck, namely that every processor has to hold the entire mesh and
DoFHandler, is addressed in step-40.) Finally, a
good number of assorted improvements and techniques are demonstrated that have
not been shown yet in previous programs.

As before in step-17, the program runs just as fine on a single sequential
machine as long as you have PETSc installed. Information on how to tell
deal.II about a PETSc installation on your system can be found in the deal.II
README file, which is linked to from the <a href="../../index.html">main
documentation page</a>
in your installation of deal.II, or on <a href="http://www.dealii.org/">the
deal.II webpage</a>.


<h3>Quasistatic elastic deformation</h3>

<h4>Motivation of the model</h4>

In general, time-dependent small elastic deformations are described by the
elastic wave equation
@f[
  \rho \frac{\partial^2 \mathbf{u}}{\partial t^2}
  + c \frac{\partial \mathbf{u}}{\partial t}
  - \textrm{div}\  ( C \varepsilon(\mathbf{u})) = \mathbf{f}
  \qquad
  \textrm{in}\ \Omega,
@f]
where $\mathbf{u}=\mathbf{u} (\mathbf{x},t)$ is the deformation of the body, $\rho$
and $c$ the density and attenuation coefficient, and $\mathbf{f}$ external forces.
In addition, initial conditions
@f[
  \mathbf{u}(\cdot, 0) = \mathbf{u}_0(\cdot)
  \qquad
  \textrm{on}\ \Omega,
@f]
and Dirichlet (displacement) or Neumann (traction) boundary conditions need
to be specified for a unique solution:
@f{eqnarray*}
  \mathbf{u}(\mathbf{x},t) &=& \mathbf{d}(\mathbf{x},t)
  \qquad
  \textrm{on}\ \Gamma_D\subset\partial\Omega,
  \\
  \mathbf{n} \ C \varepsilon(\mathbf{u}(\mathbf{x},t)) &=& \mathbf{b}(\mathbf{x},t)
  \qquad
  \textrm{on}\ \Gamma_N=\partial\Omega\backslash\Gamma_D.
@f}
In above formulation, $\varepsilon(\mathbf{u})= \frac 12 (\nabla \mathbf{u} + \nabla
\mathbf{u}^T)$ is the symmetric gradient of the displacement, also called the
<em>strain</em>. $C$ is a tensor of rank 4, called the <em>stress-strain
  tensor</em> (the inverse of the <a
  href="https://en.wikipedia.org/wiki/Hooke%27s_law#Hooke's_law_for_continuous_media"><em>compliance
  tensor</em></a>)
that contains knowledge of the elastic strength of the material; its
symmetry properties make sure that it maps symmetric tensors of rank 2
(&ldquo;matrices&rdquo; of dimension $d$, where $d$ is the spatial dimensionality) onto
symmetric tensors of the same rank. We will comment on the roles of the strain
and stress tensors more below. For the moment it suffices to say that we
interpret the term $\textrm{div}\  ( C \varepsilon(\mathbf{u}))$ as the vector with
components $\frac \partial{\partial x_j} C_{ijkl} \varepsilon(\mathbf{u})_{kl}$,
where summation over indices $j,k,l$ is implied.

The quasistatic limit of this equation is motivated as follows: each small
perturbation of the body, for example by changes in boundary condition or the
forcing function, will result in a corresponding change in the configuration
of the body. In general, this will be in the form of waves radiating away from
the location of the disturbance. Due to the presence of the damping term,
these waves will be attenuated on a time scale of, say, $\tau$. Now, assume
that all changes in external forcing happen on times scales that are
much larger than $\tau$. In that case, the dynamic nature of the change is
unimportant: we can consider the body to always be in static equilibrium,
i.e. we can assume that at all times the body satisfies
@f{eqnarray*}
  - \textrm{div}\  ( C \varepsilon(\mathbf{u})) &=& \mathbf{f}(\mathbf{x},t)
  \qquad
  \textrm{in}\ \Omega,
  \\
  \mathbf{u}(\mathbf{x},t) &=& \mathbf{d}(\mathbf{x},t)
  \qquad
  \textrm{on}\ \Gamma_D,
  \\
  \mathbf{n} \ C \varepsilon(\mathbf{u}(\mathbf{x},t)) &=& \mathbf{b}(\mathbf{x},t)
  \qquad
  \textrm{on}\ \Gamma_N.
@f}
Note that the differential equation does not contain any time derivatives any
more -- all time dependence is introduced through boundary conditions and a
possibly time-varying force function $\mathbf{f}(\mathbf{x},t)$. The changes in
configuration can therefore be considered as being stationary
instantaneously. An alternative view of this is that $t$ is not really a time
variable, but only a time-like parameter that governs the evolution of the
problem.

While these equations are sufficient to describe small deformations, computing
large deformations is a little more complicated and, in general, leads
to nonlinear equations such as those treated in step-44. In the
following, let us consider some of the tools one would employ when
simulating problems in which the deformation becomes <i>large</i>.

@note The model we will consider below is not founded on anything that
would be mathematically sound: we will consider a model in which we
produce a small deformation, deform the physical coordinates of the
body by this deformation, and then consider the next loading step
again as a linear problem. This isn't consistent, since the assumption
of linearity implies that deformations are infinitesimal and so moving
around the vertices of our mesh by a finite amount before solving the
next linear problem is an inconsistent approach. We should therefore
note that it is not surprising that the equations discussed below
can't be found in the literature: <b>The model considered here has
little to do with reality!</b> On the other hand, the implementation
techniques we consider are very much what one would need to use when
implementing a <i>real</i> model, as we will see in step-44.


To come back to defining our "artificial" model, let us first
introduce a tensorial stress variable $\sigma$, and write the differential
equations in terms of the stress:
@f{eqnarray*}
  - \textrm{div}\  \sigma &=& \mathbf{f}(\mathbf{x},t)
  \qquad
  \textrm{in}\ \Omega(t),
  \\
  \mathbf{u}(\mathbf{x},t) &=& \mathbf{d}(\mathbf{x},t)
  \qquad
  \textrm{on}\ \Gamma_D\subset\partial\Omega(t),
  \\
  \mathbf{n} \ C \varepsilon(\mathbf{u}(\mathbf{x},t)) &=& \mathbf{b}(\mathbf{x},t)
  \qquad
  \textrm{on}\ \Gamma_N=\partial\Omega(t)\backslash\Gamma_D.
@f}
Note that these equations are posed on a domain $\Omega(t)$ that
changes with time, with the boundary moving according to the
displacements $\mathbf{u}(\mathbf{x},t)$ of the points on the boundary. To
complete this system, we have to specify the incremental relationship between
the stress and the strain, as follows:
<a name="step_18.stress-strain"></a>
@f[
  \dot\sigma = C \varepsilon (\dot{\mathbf{u}}),
  \qquad
  \qquad
  \textrm{[stress-strain]}
@f]
where a dot indicates a time derivative. Both the stress $\sigma$ and the
strain $\varepsilon(\mathbf{u})$ are symmetric tensors of rank 2.


<h4>Time discretization</h4>

Numerically, this system is solved as follows: first, we discretize
the time component using a backward Euler scheme. This leads to a
discrete equilibrium of force at time step $n$:
@f[
  -\textrm{div}\  \sigma^n = f^n,
@f]
where
@f[
  \sigma^n = \sigma^{n-1} + C \varepsilon (\Delta \mathbf{u}^n),
@f]
and $\Delta \mathbf{u}^n$ the incremental displacement for time step
$n$. In addition, we have to specify initial data $\mathbf{u}(\cdot,0)=\mathbf{u}_0$.
This way, if we want to solve for the displacement increment, we
have to solve the following system:
@f{align*}
  - \textrm{div}\   C \varepsilon(\Delta\mathbf{u}^n) &= \mathbf{f} + \textrm{div}\  \sigma^{n-1}
  \qquad
  &&\textrm{in}\ \Omega(t_{n-1}),
  \\
  \Delta \mathbf{u}^n(\mathbf{x},t) &= \mathbf{d}(\mathbf{x},t_n) - \mathbf{d}(\mathbf{x},t_{n-1})
  \qquad
  &&\textrm{on}\ \Gamma_D\subset\partial\Omega(t_{n-1}),
  \\
  \mathbf{n} \ C \varepsilon(\Delta \mathbf{u}^n(\mathbf{x},t)) &= \mathbf{b}(\mathbf{x},t_n)-\mathbf{b}(\mathbf{x},t_{n-1})
  \qquad
  &&\textrm{on}\ \Gamma_N=\partial\Omega(t_{n-1})\backslash\Gamma_D.
@f}
The weak form of this set of equations, which as usual is the basis for the
finite element formulation, reads as follows: find $\Delta \mathbf{u}^n \in
\{v\in H^1(\Omega(t_{n-1}))^d: v|_{\Gamma_D}=\mathbf{d}(\cdot,t_n) - \mathbf{d}(\cdot,t_{n-1})\}$
such that
<a name="step_18.linear-system"></a>
@f{align*}
  (C \varepsilon(\Delta\mathbf{u}^n), \varepsilon(\varphi) )_{\Omega(t_{n-1})}
  &=
  (\mathbf{f}, \varphi)_{\Omega(t_{n-1})}
  -(\sigma^{n-1},\varepsilon(\varphi))_{\Omega(t_{n-1})}
  \\
  &\qquad
  +(\mathbf{b}(\mathbf{x},t_n)-\mathbf{b}(\mathbf{x},t_{n-1}), \varphi)_{\Gamma_N}
  +(\sigma^{n-1} \mathbf{n}, \varphi)_{\Gamma_N}
  \\
  &\qquad\qquad
  \forall \varphi \in \{\mathbf{v}\in H^1(\Omega(t_{n-1}))^d: \mathbf{v}|_{\Gamma_D}=0\}.
@f}
Using that $\sigma^{n-1} \mathbf{n}
            = [C \varepsilon(\mathbf{u}^{n-1})] \mathbf{n}
            = \mathbf{b}(\mathbf x, t_{n-1})$,
these equations can be simplified to
@f{align*}
  (C \varepsilon(\Delta\mathbf{u}^n), \varepsilon(\varphi) )_{\Omega(t_{n-1})}
  &=
  (\mathbf{f}, \varphi)_{\Omega(t_{n-1})}
  -(\sigma^{n-1},\varepsilon(\varphi))_{\Omega(t_{n-1})}
  +(\mathbf{b}(\mathbf{x},t_n),t_{n-1}), \varphi)_{\Gamma_N}
  \\
  &\qquad\qquad
  \forall \varphi \in \{\mathbf{v}\in H^1(\Omega(t_{n-1}))^d: \mathbf{v}|_{\Gamma_D}=0\}.
  \qquad
  \qquad
  \textrm{[linear-system]}
@f}

We note that, for simplicity, in the program we will always assume that there
are no boundary forces, i.e. $\mathbf{b} = 0$, and that the deformation of the
body is driven by body forces $\mathbf{f}$ and prescribed boundary displacements
$\mathbf{d}$ alone. It is also worth noting that when integrating by parts, we
would get terms of the form $(C \varepsilon(\Delta\mathbf{u}^n), \nabla \varphi
)_{\Omega(t_{n-1})}$, but that we replace them with the term involving the
symmetric gradient $\varepsilon(\varphi)$ instead of $\nabla\varphi$. Due to
the symmetry of $C$, the two terms are mathematically equivalent, but
the symmetric version avoids the potential for round-off errors making
the resulting matrix slightly non-symmetric.

The system at time step $n$, to be solved on the old domain
$\Omega(t_{n-1})$, has exactly the form of a stationary elastic
problem, and is therefore similar to what we have already implemented
in previous example programs. We will therefore not comment on the
space discretization beyond saying that we again use lowest order
continuous finite elements.

There are differences, however:
<ol>
  <li> We have to move (update) the mesh after each time step, in order to be
  able to solve the next time step on a new domain;

  <li> We need to know $\sigma^{n-1}$ to compute the next incremental
  displacement, i.e. we need to compute it at the end of the time step
  to make sure it is available for the next time step. Essentially,
  the stress variable is our window to the history of deformation of
  the body.
</ol>
These two operations are done in the functions <code>move_mesh</code> and
<code>update_quadrature_point_history</code> in the program. While moving
the mesh is only a technicality, updating the stress is a little more
complicated and will be discussed in the next section.


<h4>Updating the stress variable</h4>

As indicated above, we need to have the stress variable $\sigma^n$ available
when computing time step $n+1$, and we can compute it using
<a name="step_18.stress-update"></a>
@f[
  \sigma^n = \sigma^{n-1} + C \varepsilon (\Delta \mathbf{u}^n).
  \qquad
  \qquad
  \textrm{[stress-update]}
@f]
There are, despite the apparent simplicity of this equation, two questions
that we need to discuss. The first concerns the way we store $\sigma^n$: even
if we compute the incremental updates $\Delta\mathbf{u}^n$ using lowest-order
finite elements, then its symmetric gradient $\varepsilon(\Delta\mathbf{u}^n)$ is
in general still a function that is not easy to describe. In particular, it is
not a piecewise constant function, and on general meshes (with cells that are
not rectangles %parallel to the coordinate axes) or with non-constant
stress-strain tensors $C$ it is not even a bi- or trilinear function. Thus, it
is a priori not clear how to store $\sigma^n$ in a computer program.

To decide this, we have to see where it is used. The only place where we
require the stress is in the term
$(\sigma^{n-1},\varepsilon(\varphi))_{\Omega(t_{n-1})}$. In practice, we of
course replace this term by numerical quadrature:
@f[
  (\sigma^{n-1},\varepsilon(\varphi))_{\Omega(t_{n-1})}
  =
  \sum_{K\subset {T}}
  (\sigma^{n-1},\varepsilon(\varphi))_K
  \approx
  \sum_{K\subset {T}}
  \sum_q
  w_q \ \sigma^{n-1}(\mathbf{x}_q) : \varepsilon(\varphi(\mathbf{x}_q),
@f]
where $w_q$ are the quadrature weights and $\mathbf{x}_q$ the quadrature points on
cell $K$. This should make clear that what we really need is not the stress
$\sigma^{n-1}$ in itself, but only the values of the stress in the quadrature
points on all cells. This, however, is a simpler task: we only have to provide
a data structure that is able to hold one symmetric tensor of rank 2 for each
quadrature point on all cells (or, since we compute in parallel, all
quadrature points of all cells that the present MPI process &ldquo;owns&rdquo;). At the
end of each time step we then only have to evaluate $\varepsilon(\Delta \mathbf{u}^n(\mathbf{x}_q))$, multiply it by the stress-strain tensor $C$, and use the
result to update the stress $\sigma^n(\mathbf{x}_q)$ at quadrature point $q$.

The second complication is not visible in our notation as chosen above. It is
due to the fact that we compute $\Delta u^n$ on the domain $\Omega(t_{n-1})$,
and then use this displacement increment to both update the stress as well as
move the mesh nodes around to get to $\Omega(t_n)$ on which the next increment
is computed. What we have to make sure, in this context, is that moving the
mesh does not only involve moving around the nodes, but also making
corresponding changes to the stress variable: the updated stress is a variable
that is defined with respect to the coordinate system of the material in the
old domain, and has to be transferred to the new domain. The reason for this
can be understood as follows: locally, the incremental deformation $\Delta\mathbf{u}$ can be decomposed into three parts, a linear translation (the constant part
of the displacement increment field in the neighborhood of a point), a
dilational
component (that part of the gradient of the displacement field that has a
nonzero divergence), and a rotation. A linear translation of the material does
not affect the stresses that are frozen into it -- the stress values are
simply translated along. The dilational or compressional change produces a
corresponding stress update. However, the rotational component does not
necessarily induce a nonzero stress update (think, in 2d, for example of the
situation where $\Delta\mathbf{u}=(y, -x)^T$, with which $\varepsilon(\Delta
\mathbf{u})=0$). Nevertheless, if the material was prestressed in a certain
direction, then this direction will be rotated along with the material.  To
this end, we have to define a rotation matrix $R(\Delta \mathbf{u}^n)$ that
describes, in each point the rotation due to the displacement increments. It
is not hard to see that the actual dependence of $R$ on $\Delta \mathbf{u}^n$ can
only be through the curl of the displacement, rather than the displacement
itself or its full gradient (as mentioned above, the constant components of
the increment describe translations, its divergence the dilational modes, and
the curl the rotational modes). Since the exact form of $R$ is cumbersome, we
only state it in the program code, and note that the correct updating formula
for the stress variable is then
<a name="step_18.stress-update+rot"></a>
@f[
  \sigma^n
  =
  R(\Delta \mathbf{u}^n)^T
  [\sigma^{n-1} + C \varepsilon (\Delta \mathbf{u}^n)]
  R(\Delta \mathbf{u}^n).
  \qquad
  \qquad
  \textrm{[stress-update+rot]}
@f]

Both stress update and rotation are implemented in the function
<code>update_quadrature_point_history</code> of the example program.


<h3>Parallel graphical output</h3>

In step-17, the main bottleneck for %parallel computations as far as run time
is concerned
was that only the first processor generated output for the entire domain.
Since generating graphical output is expensive, this did not scale well when
larger numbers of processors were involved. We will address this here. (For a
definition of what it means for a program to "scale", see
@ref GlossParallelScaling "this glossary entry".)

Basically, what we need to do is let every process
generate graphical output for that subset of cells that it owns, write them
into separate files and have a way to display all files for a certain timestep
at the same time. This way the code produces one <code>.vtu</code> file per process per
time step. The two common VTK file viewers ParaView and VisIt both support
opening more than one <code>.vtu</code> file at once. To simplify the process of picking
the correct files and allow moving around in time, both support record files
that reference all files for a given timestep. Sadly, the record files have a
different format between VisIt and Paraview, so we write out both formats.

The code will generate the files <code>solution-TTTT.NNN.vtu</code>,
where <code>TTTT</code> is the timestep number (starting from 1) and
<code>NNN</code> is the process rank (starting from
0). These files contain the locally owned cells for the timestep and
processor. The files <code>solution-TTTT.visit</code> is the visit record
for timestep <code>TTTT</code>, while <code>solution-TTTT.pvtu</code> is
the same for ParaView. (More recent versions of VisIt can actually read
<code>.pvtu</code> files as well, but it doesn't hurt to output both
kinds of record files.) Finally, the file
<code>solution.pvd</code> is a special record only supported by ParaView that references
all time steps. So in ParaView, only solution.pvd needs to be opened, while
one needs to select the group of all .visit files in VisIt for the same
effect.


<h3>A triangulation with automatic partitioning</h3>

In step-17, we used a regular triangulation that was simply replicated on
every processor, and a corresponding DoFHandler. Both had no idea that they
were used in a %parallel context -- they just existed in their entirety
on every processor, and we argued that this was eventually going to be a
major memory bottleneck.

We do not address this issue here (we will do so in step-40) but make
the situation slightly more automated. In step-17, we created the triangulation
and then manually "partitioned" it, i.e., we assigned
@ref GlossSubdomainId "subdomain ids" to every cell that indicated which
@ref GlossMPIProcess "MPI process" "owned" the cell. Here, we use a class
parallel::shared::Triangulation that at least does this part automatically:
whenever you create or refine such a triangulation, it automatically
partitions itself among all involved processes (which it knows about because
you have to tell it about the @ref GlossMPICommunicator "MPI communicator"
that connects these processes upon construction of the triangulation).
Otherwise, the parallel::shared::Triangulation looks, for all practical
purposes, like a regular Triangulation object.

The convenience of using this class does not only result from being able
to avoid the manual call to GridTools::partition(). Rather, the DoFHandler
class now also knows that you want to use it in a parallel context, and
by default automatically enumerates degrees of freedom in such a way
that all DoFs owned by process zero come before all DoFs owned by process 1,
etc. In other words, you can also avoid the call to
DoFRenumbering::subdomain_wise().

There are other benefits. For example, because the triangulation knows that
it lives in a %parallel universe, it also knows that it "owns" certain
cells (namely, those whose subdomain id equals its MPI rank; previously,
the triangulation only stored these subdomain ids, but had no way to
make sense of them). Consequently, in the assembly function, you can
test whether a cell is "locally owned" (i.e., owned by the current
process, see @ref GlossLocallyOwnedCell) when you loop over all cells
using the syntax
@code
  if (cell->is_locally_owned())
@endcode
This knowledge extends to the DoFHandler object built on such triangulations,
which can then identify which degrees of freedom are locally owned
(see @ref GlossLocallyOwnedDof) via calls such as
DoFHandler::compute_n_locally_owned_dofs_per_processor() and
DoFTools::extract_locally_relevant_dofs(). Finally, the DataOut class
also knows how to deal with such triangulations and will simply skip
generating graphical output on cells not locally owned.

Of course, as has been noted numerous times in the discussion in step-17,
keeping the entire triangulation on every process will not scale: large
problems may simply not fit into each process's memory any more, even if
we have sufficiently many processes around to solve them in a reasonable
time. In such cases, the parallel::shared::Triangulation is no longer
a reasonable basis for computations and we will show in step-40 how the
parallel::distributed::Triangulation class can be used to work around
this, namely by letting each process store only a <i>part</i> of the
triangulation.


<h3>Overall structure of the program</h3>

The overall structure of the program can be inferred from the <code>run()</code>
function that first calls <code>do_initial_timestep()</code> for the first time
step, and then <code>do_timestep()</code> on all subsequent time steps. The
difference between these functions is only that in the first time step we
start on a coarse mesh, solve on it, refine the mesh adaptively, and then
start again with a clean state on that new mesh. This procedure gives us a
better starting mesh, although we should of course keep adapting the mesh as
iterations proceed -- this isn't done in this program, but commented on below.

The common part of the two functions treating time steps is the following
sequence of operations on the present mesh:
<ul>
<li> <code>assemble_system ()</code> [via <code>solve_timestep ()</code>]:
  This first function is also the most interesting one. It assembles the
  linear system corresponding to the discretized version of equation
  <a href="#step_18.linear-system">[linear-system]</a>. This leads to a system matrix $A_{ij} = \sum_K
  A^K_{ij}$ built up of local contributions on each cell $K$ with entries
  @f[
    A^K_{ij} = (C \varepsilon(\varphi_j), \varepsilon(\varphi_i))_K;
  @f]
  In practice, $A^K$ is computed using numerical quadrature according to the
  formula
  @f[
    A^K_{ij} = \sum_q w_q [\varepsilon(\varphi_i(\mathbf{x}_q)) : C :
                           \varepsilon(\varphi_j(\mathbf{x}_q))],
  @f]
  with quadrature points $\mathbf{x}_q$ and weights $w_q$. We have built these
  contributions before, in step-8 and step-17, but in both of these cases we
  have done so rather clumsily by using knowledge of how the rank-4 tensor $C$
  is composed, and considering individual elements of the strain tensors
  $\varepsilon(\varphi_i),\varepsilon(\varphi_j)$. This is not really
  convenient, in particular if we want to consider more complicated elasticity
  models than the isotropic case for which $C$ had the convenient form
  $C_{ijkl}  = \lambda \delta_{ij} \delta_{kl} + \mu (\delta_{ik} \delta_{jl}
  + \delta_{il} \delta_{jk})$. While we in fact do not use a more complicated
  form than this in the present program, we nevertheless want to write it in a
  way that would easily allow for this. It is then natural to introduce
  classes that represent symmetric tensors of rank 2 (for the strains and
  stresses) and 4 (for the stress-strain tensor $C$). Fortunately, deal.II
  provides these: the <code>SymmetricTensor<rank,dim></code> class template
  provides a full-fledged implementation of such tensors of rank <code>rank</code>
  (which needs to be an even number) and dimension <code>dim</code>.

  What we then need is two things: a way to create the stress-strain rank-4
  tensor $C$ as well as to create a symmetric tensor of rank 2 (the strain
  tensor) from the gradients of a shape function $\varphi_i$ at a quadrature
  point $\mathbf{x}_q$ on a given cell. At the top of the implementation of this
  example program, you will find such functions. The first one,
  <code>get_stress_strain_tensor</code>, takes two arguments corresponding to
  the Lam&eacute; constants $\lambda$ and $\mu$ and returns the stress-strain tensor
  for the isotropic case corresponding to these constants (in the program, we
  will choose constants corresponding to steel); it would be simple to replace
  this function by one that computes this tensor for the anisotropic case, or
  taking into account crystal symmetries, for example. The second one,
  <code>get_strain</code> takes an object of type <code>FEValues</code> and indices
  $i$ and $q$ and returns the symmetric gradient, i.e. the strain,
  corresponding to shape function $\varphi_i(\mathbf{x}_q)$, evaluated on the cell
  on which the <code>FEValues</code> object was last reinitialized.

  Given this, the innermost loop of <code>assemble_system</code> computes the
  local contributions to the matrix in the following elegant way (the variable
  <code>stress_strain_tensor</code>, corresponding to the tensor $C$, has
  previously been initialized with the result of the first function above):
  @code
for (unsigned int i=0; i<dofs_per_cell; ++i)
  for (unsigned int j=0; j<dofs_per_cell; ++j)
    for (unsigned int q_point=0; q_point<n_q_points;
         ++q_point)
      {
        const SymmetricTensor<2,dim>
          eps_phi_i = get_strain (fe_values, i, q_point),
          eps_phi_j = get_strain (fe_values, j, q_point);

        cell_matrix(i,j)
          += (eps_phi_i * stress_strain_tensor * eps_phi_j *
              fe_values.JxW (q_point));
      }
  @endcode
  It is worth noting the expressive power of this piece of code, and to
  compare it with the complications we had to go through in previous examples
  for the elasticity problem. (To be fair, the SymmetricTensor class
  template did not exist when these previous examples were written.) For
  simplicity, <code>operator*</code> provides for the (double summation) product
  between symmetric tensors of even rank here.

  Assembling the local contributions
  @f{eqnarray*}
      f^K_i &=&
      (\mathbf{f}, \varphi_i)_K -(\sigma^{n-1},\varepsilon(\varphi_i))_K
      \\
      &\approx&
      \sum_q
      w_q \left\{
        \mathbf{f}(\mathbf{x}_q) \cdot \varphi_i(\mathbf{x}_q) -
        \sigma^{n-1}_q : \varepsilon(\varphi_i(\mathbf{x}_q))
      \right\}
  @f}
  to the right hand side of <a href="#step_18.linear-system">[linear-system]</a> is equally
  straightforward (note that we do not consider any boundary tractions $\mathbf{b}$ here). Remember that we only had to store the old stress in the
  quadrature points of cells. In the program, we will provide a variable
  <code>local_quadrature_points_data</code> that allows to access the stress
  $\sigma^{n-1}_q$ in each quadrature point. With this the code for the right
  hand side looks as this, again rather elegant:
  @code
for (unsigned int i=0; i<dofs_per_cell; ++i)
  {
    const unsigned int
      component_i = fe.system_to_component_index(i).first;

    for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
      {
        const SymmetricTensor<2,dim> &old_stress
          = local_quadrature_points_data[q_point].old_stress;

        cell_rhs(i) += (body_force_values[q_point](component_i) *
                        fe_values.shape_value (i,q_point)
                        -
                        old_stress *
                        get_strain (fe_values,i,q_point)) *
                       fe_values.JxW (q_point);
      }
  }
  @endcode
  Note that in the multiplication $\mathbf{f}(\mathbf{x}_q) \cdot \varphi_i(\mathbf{x}_q)$, we have made use of the fact that for the chosen finite element, only
  one vector component (namely <code>component_i</code>) of $\varphi_i$ is
  nonzero, and that we therefore also have to consider only one component of
  $\mathbf{f}(\mathbf{x}_q)$.

  This essentially concludes the new material we present in this function. It
  later has to deal with boundary conditions as well as hanging node
  constraints, but this parallels what we had to do previously in other
  programs already.

<li> <code>solve_linear_problem ()</code> [via <code>solve_timestep ()</code>]:
  Unlike the previous one, this function is not really interesting, since it
  does what similar functions have done in all previous tutorial programs --
  solving the linear system using the CG method, using an incomplete LU
  decomposition as a preconditioner (in the %parallel case, it uses an ILU of
  each processor's block separately). It is virtually unchanged
  from step-17.

<li> <code>update_quadrature_point_history ()</code> [via
  <code>solve_timestep ()</code>]: Based on the displacement field $\Delta \mathbf{u}^n$ computed before, we update the stress values in all quadrature points
  according to <a href="#step_18.stress-update">[stress-update]</a> and <a href="#step_18.stress-update+rot">[stress-update+rot]</a>,
  including the rotation of the coordinate system.

<li> <code>move_mesh ()</code>: Given the solution computed before, in this
  function we deform the mesh by moving each vertex by the displacement vector
  field evaluated at this particular vertex.

<li> <code>output_results ()</code>: This function simply outputs the solution
  based on what we have said above, i.e. every processor computes output only
  for its own portion of the domain. In addition to the solution, we also compute the norm of
  the stress averaged over all the quadrature points on each cell.
</ul>

With this general structure of the code, we only have to define what case we
want to solve. For the present program, we have chosen to simulate the
quasistatic deformation of a vertical cylinder for which the bottom boundary
is fixed and the top boundary is pushed down at a prescribed vertical
velocity. However, the horizontal velocity of the top boundary is left
unspecified -- one can imagine this situation as a well-greased plate pushing
from the top onto the cylinder, the points on the top boundary of the cylinder
being allowed to slide horizontally along the surface of the plate, but forced
to move downward by the plate. The inner and outer boundaries of the cylinder
are free and not subject to any prescribed deflection or traction. In
addition, gravity acts on the body.

The program text will reveal more about how to implement this situation, and
the results section will show what displacement pattern comes out of this
simulation.


examples/step-18/doc/results.dox
<h1>Results</h1>


Running the program takes a good while if one uses debug mode; it takes about
eleven minutes on my i7 desktop. Fortunately, the version compiled with
optimizations is much faster; the program only takes about a minute and a half
after recompiling with the command <tt>make release</tt> on the same machine, a
much more reasonable time.


If run, the program prints the following output, explaining what it is
doing during all that time:
@verbatim
\$ time make run
[ 66%] Built target \step-18
[100%] Run \step-18 with Release configuration
Timestep 1 at time 1
  Cycle 0:
    Number of active cells:       3712 (by partition: 3712)
    Number of degrees of freedom: 17226 (by partition: 17226)
    Assembling system... norm of rhs is 1.88062e+10
    Solver converged in 103 iterations.
    Updating quadrature point data...
  Cycle 1:
    Number of active cells:       12812 (by partition: 12812)
    Number of degrees of freedom: 51738 (by partition: 51738)
    Assembling system... norm of rhs is 1.86145e+10
    Solver converged in 121 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 2 at time 2
    Assembling system... norm of rhs is 1.84169e+10
    Solver converged in 122 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 3 at time 3
    Assembling system... norm of rhs is 1.82355e+10
    Solver converged in 122 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 4 at time 4
    Assembling system... norm of rhs is 1.80728e+10
    Solver converged in 117 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 5 at time 5
    Assembling system... norm of rhs is 1.79318e+10
    Solver converged in 116 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 6 at time 6
    Assembling system... norm of rhs is 1.78171e+10
    Solver converged in 115 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 7 at time 7
    Assembling system... norm of rhs is 1.7737e+10
    Solver converged in 112 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 8 at time 8
    Assembling system... norm of rhs is 1.77127e+10
    Solver converged in 111 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 9 at time 9
    Assembling system... norm of rhs is 1.78207e+10
    Solver converged in 113 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 10 at time 10
    Assembling system... norm of rhs is 1.83544e+10
    Solver converged in 115 iterations.
    Updating quadrature point data...
    Moving mesh...

[100%] Built target run
make run  176.82s user 0.15s system 198% cpu 1:28.94 total
@endverbatim
In other words, it is computing on 12,000 cells and with some 52,000
unknowns. Not a whole lot, but enough for a coupled three-dimensional
problem to keep a computer busy for a while. At the end of the day,
this is what we have for output:
@verbatim
\$ ls -l *vtu *visit
-rw-r--r-- 1 drwells users 1706059 Feb 13 19:36 solution-0010.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:36 solution-0010.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:36 solution-0010.visit
-rw-r--r-- 1 drwells users 1707907 Feb 13 19:36 solution-0009.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:36 solution-0009.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:36 solution-0009.visit
-rw-r--r-- 1 drwells users 1703771 Feb 13 19:35 solution-0008.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0008.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0008.visit
-rw-r--r-- 1 drwells users 1693671 Feb 13 19:35 solution-0007.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0007.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0007.visit
-rw-r--r-- 1 drwells users 1681847 Feb 13 19:35 solution-0006.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0006.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0006.visit
-rw-r--r-- 1 drwells users 1670115 Feb 13 19:35 solution-0005.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0005.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0005.visit
-rw-r--r-- 1 drwells users 1658559 Feb 13 19:35 solution-0004.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0004.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0004.visit
-rw-r--r-- 1 drwells users 1639983 Feb 13 19:35 solution-0003.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0003.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0003.visit
-rw-r--r-- 1 drwells users 1625851 Feb 13 19:35 solution-0002.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0002.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0002.visit
-rw-r--r-- 1 drwells users 1616035 Feb 13 19:34 solution-0001.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:34 solution-0001.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:34 solution-0001.visit
@endverbatim


If we visualize these files with VisIt or Paraview, we get to see the full picture
of the disaster our forced compression wreaks on the cylinder (colors in the
images encode the norm of the stress in the material):


<div class="threecolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0002.0000.png"
           alt="Time = 2"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 2
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0005.0000.png"
           alt="Time = 5"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 5
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0007.0000.png"
           alt="Time = 7"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 7
    </div>
  </div>
</div>


<div class="threecolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0008.0000.png"
           alt="Time = 8"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 8
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0009.0000.png"
           alt="Time = 9"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 9
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0010.0000.png"
           alt="Time = 10"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 10
    </div>
  </div>
</div>


As is clearly visible, as we keep compressing the cylinder, it starts
to bow out near the fully constrained bottom surface and, after about eight
time units, buckle in an azimuthally symmetric manner.


Although the result appears plausible for the symmetric geometry and loading,
it is yet to be established whether or not the computation is fully converged.
In order to see whether it is, we ran the program again with one more global
refinement at the beginning and with the time step halved. This would have
taken a very long time on a single machine, so we used a proper workstation and
ran it on 16 processors in parallel. The beginning of the output now looks like
this:
@verbatim
Timestep 1 at time 0.5
  Cycle 0:
    Number of active cells:       29696 (by partition: 1808+1802+1894+1881+1870+1840+1884+1810+1876+1818+1870+1884+1854+1903+1816+1886)
    Number of degrees of freedom: 113100 (by partition: 6936+6930+7305+7116+7326+6869+7331+6786+7193+6829+7093+7162+6920+7280+6843+7181)
    Assembling system... norm of rhs is 1.10765e+10
    Solver converged in 209 iterations.
    Updating quadrature point data...
  Cycle 1:
    Number of active cells:       102034 (by partition: 6387+6202+6421+6341+6408+6201+6428+6428+6385+6294+6506+6244+6417+6527+6299+6546)
    Number of degrees of freedom: 359337 (by partition: 23255+21308+24774+24019+22304+21415+22430+22184+22298+21796+22396+21592+22325+22553+21977+22711)
    Assembling system... norm of rhs is 1.35759e+10
    Solver converged in 268 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 2 at time 1
    Assembling system... norm of rhs is 1.34674e+10
    Solver converged in 267 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 3 at time 1.5
    Assembling system... norm of rhs is 1.33607e+10
    Solver converged in 265 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 4 at time 2
    Assembling system... norm of rhs is 1.32558e+10
    Solver converged in 263 iterations.
    Updating quadrature point data...
    Moving mesh...

[...]

Timestep 20 at time 10
    Assembling system... norm of rhs is 1.47755e+10
    Solver converged in 425 iterations.
    Updating quadrature point data...
    Moving mesh...
@endverbatim
That's quite a good number of unknowns, given that we are in 3d. The output of
this program are 16 files for each time step:
@verbatim
\$ ls -l solution-0001*
-rw-r--r-- 1 wellsd2 user 761065 Feb 13 21:09 solution-0001.000.vtu
-rw-r--r-- 1 wellsd2 user 759277 Feb 13 21:09 solution-0001.001.vtu
-rw-r--r-- 1 wellsd2 user 761217 Feb 13 21:09 solution-0001.002.vtu
-rw-r--r-- 1 wellsd2 user 761605 Feb 13 21:09 solution-0001.003.vtu
-rw-r--r-- 1 wellsd2 user 756917 Feb 13 21:09 solution-0001.004.vtu
-rw-r--r-- 1 wellsd2 user 752669 Feb 13 21:09 solution-0001.005.vtu
-rw-r--r-- 1 wellsd2 user 735217 Feb 13 21:09 solution-0001.006.vtu
-rw-r--r-- 1 wellsd2 user 750065 Feb 13 21:09 solution-0001.007.vtu
-rw-r--r-- 1 wellsd2 user 760273 Feb 13 21:09 solution-0001.008.vtu
-rw-r--r-- 1 wellsd2 user 777265 Feb 13 21:09 solution-0001.009.vtu
-rw-r--r-- 1 wellsd2 user 772469 Feb 13 21:09 solution-0001.010.vtu
-rw-r--r-- 1 wellsd2 user 760833 Feb 13 21:09 solution-0001.011.vtu
-rw-r--r-- 1 wellsd2 user 782241 Feb 13 21:09 solution-0001.012.vtu
-rw-r--r-- 1 wellsd2 user 748905 Feb 13 21:09 solution-0001.013.vtu
-rw-r--r-- 1 wellsd2 user 738413 Feb 13 21:09 solution-0001.014.vtu
-rw-r--r-- 1 wellsd2 user 762133 Feb 13 21:09 solution-0001.015.vtu
-rw-r--r-- 1 wellsd2 user   1421 Feb 13 21:09 solution-0001.pvtu
-rw-r--r-- 1 wellsd2 user    364 Feb 13 21:09 solution-0001.visit
@endverbatim


Here are first the mesh on which we compute as well as the partitioning
for the 16 processors:


<div class="twocolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-000mesh.png"
           alt="Discretization"
           width="400">
    </div>
    <div class="text" align="center">
      Discretization
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0002.p.png"
           alt="Parallel partitioning"
           width="400">
    </div>
    <div class="text" align="center">
      Parallel partitioning
    </div>
  </div>
</div>


Finally, here is the same output as we have shown before for the much smaller
sequential case:

<div class="threecolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0002.s.png"
           alt="Time = 2"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 2
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0005.s.png"
           alt="Time = 5"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 5
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0007.s.png"
           alt="Time = 7"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 7
    </div>
  </div>
</div>


<div class="threecolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0008.s.png"
           alt="Time = 8"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 8
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0009.s.png"
           alt="Time = 9"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 9
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0010.s.png"
           alt="Time = 10"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 10
    </div>
  </div>
</div>


As before, we observe that at high axial compression the cylinder begins
to buckle, but this time ultimately collapses on itself. In contrast to our
first run, towards the end of the simulation the deflection pattern becomes
nonsymmetric (the central bulge deflects laterally). The model clearly does not
provide for this (all our forces and boundary deflections are symmetric) but the
effect is probably physically correct anyway: in reality, small inhomogeneities
in the body's material properties would lead it to buckle to one side
to evade the forcing; in numerical simulations, small perturbations
such as numerical round-off or an inexact solution of a linear system
by an iterative solver could have the same effect. Another typical source for
asymmetries in adaptive computations is that only a certain fraction of cells
is refined in each step, which may lead to asymmetric meshes even if the
original coarse mesh was symmetric.


If one compares this with the previous run, the results both qualitatively
and quantitatively different. The previous computation was
therefore certainly not converged, though we can't say for sure anything about
the present one. One would need an even finer computation to find out. However,
the point may be moot: looking at the last picture in detail, it is pretty
obvious that not only is the linear small deformation model we chose completely
inadequate, but for a realistic simulation we would also need to make sure that
the body does not intersect itself during deformation (if we continued
compressing the cylinder we would observe some self-intersection).
Without such a formulation we cannot expect anything to make physical sense,
even if it produces nice pictures!


<h3>Possibilities for extensions</h3>

The program as is does not really solve an equation that has many applications
in practice: quasi-static material deformation based on a purely elastic law
is almost boring. However, the program may serve as the starting point for
more interesting experiments, and that indeed was the initial motivation for
writing it. Here are some suggestions of what the program is missing and in
what direction it may be extended:

<h5>Plasticity models</h5>

 The most obvious extension is to use a more
realistic material model for large-scale quasistatic deformation. The natural
choice for this would be plasticity, in which a nonlinear relationship between
stress and strain replaces equation <a href="#step_18.stress-strain">[stress-strain]</a>. Plasticity
models are usually rather complicated to program since the stress-strain
dependence is generally non-smooth. The material can be thought of being able
to withstand only a maximal stress (the yield stress) after which it starts to
&ldquo;flow&rdquo;. A mathematical description to this can be given in the form of a
variational inequality, which alternatively can be treated as minimizing the
elastic energy
@f[
  E(\mathbf{u}) =
  (\varepsilon(\mathbf{u}), C\varepsilon(\mathbf{u}))_{\Omega}
  - (\mathbf{f}, \mathbf{u})_{\Omega} - (\mathbf{b}, \mathbf{u})_{\Gamma_N},
@f]
subject to the constraint
@f[
  f(\sigma(\mathbf{u})) \le 0
@f]
on the stress. This extension makes the problem to be solved in each time step
nonlinear, so we need another loop within each time step.

Without going into further details of this model, we refer to the excellent
book by Simo and Hughes on &ldquo;Computational Inelasticity&rdquo; for a
comprehensive overview of computational strategies for solving plastic
models. Alternatively, a brief but concise description of an algorithm for
plasticity is given in an article by S. Commend, A. Truty, and Th. Zimmermann;
@cite CTZ04.


<h5>Stabilization issues</h5>

The formulation we have chosen, i.e. using
piecewise (bi-, tri-)linear elements for all components of the displacement
vector, and treating the stress as a variable dependent on the displacement is
appropriate for most materials. However, this so-called displacement-based
formulation becomes unstable and exhibits spurious modes for incompressible or
nearly-incompressible materials. While fluids are usually not elastic (in most
cases, the stress depends on velocity gradients, not displacement gradients,
although there are exceptions such as electro-rheologic fluids), there are a
few solids that are nearly incompressible, for example rubber. Another case is
that many plasticity models ultimately let the material become incompressible,
although this is outside the scope of the present program.

Incompressibility is characterized by Poisson's ratio
@f[
  \nu = \frac{\lambda}{2(\lambda+\mu)},
@f]
where $\lambda,\mu$ are the Lam&eacute; constants of the material.
Physical constraints indicate that $-1\le \nu\le \frac 12$ (the condition
also follows from mathematical stability considerations). If $\nu$
approaches $\frac 12$, then the material becomes incompressible. In that
case, pure displacement-based formulations are no longer appropriate for the
solution of such problems, and stabilization techniques have to be employed
for a stable and accurate solution. The book and paper cited above give
indications as to how to do this, but there is also a large volume of
literature on this subject; a good start to get an overview of the topic can
be found in the references of the paper by H.-Y. Duan and Q. Lin; @cite DL05.


<h5>Refinement during timesteps</h5>

In the present form, the program
only refines the initial mesh a number of times, but then never again. For any
kind of realistic simulation, one would want to extend this so that the mesh
is refined and coarsened every few time steps instead. This is not hard to do,
in fact, but has been left for future tutorial programs or as an exercise, if
you wish.

The main complication one has to overcome is that one has to
transfer the data that is stored in the quadrature points of the cells of the
old mesh to the new mesh, preferably by some sort of projection scheme. The
general approach to this would go like this:

- At the beginning, the data is only available in the quadrature points of
  individual cells, not as a finite element field that is defined everywhere.

- So let us find a finite element field that <i>is</i> defined everywhere so
  that we can later interpolate it to the quadrature points of the new
  mesh. In general, it will be difficult to find a continuous finite element
  field that matches the values in the quadrature points exactly because the
  number of degrees of freedom of these fields does not match the number of
  quadrature points there are, and the nodal values of this global field will
  either be over- or underdetermined. But it is usually not very difficult to
  find a discontinuous field that matches the values in the quadrature points;
  for example, if you have a QGauss(2) quadrature formula (i.e. 4 points per
  cell in 2d, 8 points in 3d), then one would use a finite element of kind
  FE_DGQ(1), i.e. bi-/tri-linear functions as these have 4 degrees of freedom
  per cell in 2d and 8 in 3d.

- There are functions that can make this conversion from individual points to
  a global field simpler. The following piece of pseudo-code should help if
  you use a QGauss(2) quadrature formula. Note that the multiplication by the
  projection matrix below takes a vector of scalar components, i.e., we can only
  convert one set of scalars at a time from the quadrature points to the degrees
  of freedom and vice versa. So we need to store each component of stress separately,
  which requires <code>dim*dim</code> vectors. We'll store this set of vectors in a 2D array to
  make it easier to read off components in the same way you would the stress tensor.
  Thus, we'll loop over the components of stress on each cell and store
  these values in the global history field. (The prefix <code>history_</code>
  indicates that we work with quantities related to the history variables defined
  in the quadrature points.)
  @code
    FE_DGQ<dim>     history_fe (1);
    DoFHandler<dim> history_dof_handler (triangulation);
    history_dof_handler.distribute_dofs (history_fe);

    std::vector< std::vector< Vector<double> > >
                 history_field (dim, std::vector< Vector<double> >(dim)),
                 local_history_values_at_qpoints (dim, std::vector< Vector<double> >(dim)),
                 local_history_fe_values (dim, std::vector< Vector<double> >(dim));

    for (unsigned int i=0; i<dim; i++)
      for (unsigned int j=0; j<dim; j++)
      {
        history_field[i][j].reinit(history_dof_handler.n_dofs());
        local_history_values_at_qpoints[i][j].reinit(quadrature.size());
        local_history_fe_values[i][j].reinit(history_fe.n_dofs_per_cell());
      }

    FullMatrix<double> qpoint_to_dof_matrix (history_fe.dofs_per_cell,
                                             quadrature.size());
    FETools::compute_projection_from_quadrature_points_matrix
              (history_fe,
               quadrature, quadrature,
               qpoint_to_dof_matrix);

    typename DoFHandler<dim>::active_cell_iterator cell = dof_handler.begin_active(),
                                                   endc = dof_handler.end(),
                                                   dg_cell = history_dof_handler.begin_active();

    for (; cell!=endc; ++cell, ++dg_cell)
      {

        PointHistory<dim> *local_quadrature_points_history
          = reinterpret_cast<PointHistory<dim> *>(cell->user_pointer());

        Assert (local_quadrature_points_history >= &quadrature_point_history.front(),
                ExcInternalError());
        Assert (local_quadrature_points_history < &quadrature_point_history.back(),
                ExcInternalError());

        for (unsigned int i=0; i<dim; i++)
          for (unsigned int j=0; j<dim; j++)
          {
            for (unsigned int q=0; q<quadrature.size(); ++q)
              local_history_values_at_qpoints[i][j](q)
                = local_quadrature_points_history[q].old_stress[i][j];

            qpoint_to_dof_matrix.vmult (local_history_fe_values[i][j],
                                        local_history_values_at_qpoints[i][j]);

            dg_cell->set_dof_values (local_history_fe_values[i][j],
                                     history_field[i][j]);
          }
      }
  @endcode

- Now that we have a global field, we can refine the mesh and transfer the
  history_field vector as usual using the SolutionTransfer class. This will
  interpolate everything from the old to the new mesh.

- In a final step, we have to get the data back from the now interpolated
  global field to the quadrature points on the new mesh. The following code
  will do that:
  @code
    FullMatrix<double> dof_to_qpoint_matrix (quadrature.size(),
                                             history_fe.n_dofs_per_cell());
    FETools::compute_interpolation_to_quadrature_points_matrix
              (history_fe,
               quadrature,
               dof_to_qpoint_matrix);

    typename DoFHandler<dim>::active_cell_iterator cell = dof_handler.begin_active(),
                                                   endc = dof_handler.end(),
                                                   dg_cell = history_dof_handler.begin_active();

    for (; cell != endc; ++cell, ++dg_cell)
    {
      PointHistory<dim> *local_quadrature_points_history
       = reinterpret_cast<PointHistory<dim> *>(cell->user_pointer());

      Assert (local_quadrature_points_history >= &quadrature_point_history.front(),
              ExcInternalError());
      Assert (local_quadrature_points_history < &quadrature_point_history.back(),
              ExcInternalError());

      for (unsigned int i=0; i<dim; i++)
        for (unsigned int j=0; j<dim; j++)
        {
          dg_cell->get_dof_values (history_field[i][j],
                                   local_history_fe_values[i][j]);

          dof_to_qpoint_matrix.vmult (local_history_values_at_qpoints[i][j],
                                      local_history_fe_values[i][j]);

          for (unsigned int q=0; q<quadrature.size(); ++q)
            local_quadrature_points_history[q].old_stress[i][j]
              = local_history_values_at_qpoints[i][j](q);
      }
  @endcode

It becomes a bit more complicated once we run the program in parallel, since
then each process only stores this data for the cells it owned on the old
mesh. That said, using a parallel vector for <code>history_field</code> will
do the trick if you put a call to <code>compress</code> after the transfer
from quadrature points into the global vector.


<h5>Ensuring mesh regularity</h5>

At present, the program makes no attempt
to make sure that a cell, after moving its vertices at the end of the time
step, still has a valid geometry (i.e. that its Jacobian determinant is
positive and bounded away from zero everywhere). It is, in fact, not very hard
to set boundary values and forcing terms in such a way that one gets distorted
and inverted cells rather quickly. Certainly, in some cases of large
deformation, this is unavoidable with a mesh of finite mesh size, but in some
other cases this should be preventable by appropriate mesh refinement and/or a
reduction of the time step size. The program does not do that, but a more
sophisticated version definitely should employ some sort of heuristic defining
what amount of deformation of cells is acceptable, and what isn't.


examples/step-19/doc/intro.dox

<br>

<i>
This program was contributed by Wolfgang Bangerth, Rene Gassmoeller, and Peter Munch.

Wolfgang Bangerth acknowledges support through NSF
awards DMS-1821210, EAR-1550901, and OAC-1835673.
</i>

@note Support for particles exists in deal.II primarily due to the initial
  efforts of Rene Gassmoeller. Please acknowledge this work by citing
  the publication @cite GLHPW2018 if you use particle functionality in your
  own work.

<a name="Intro"></a>
<h1>Introduction</h1>

The finite element method in general, and deal.II in particular, were invented
to solve partial differential equations -- in other words, to solve
[continuum mechanics](https://en.wikipedia.org/wiki/Continuum_mechanics) problems.
On the other hand, sometimes one wants to solve problems in which it is useful
to track individual objects ("particles") and how their positions evolve. If
this simply leads to a set of ordinary differential equations, for example
if you want to track the positions of the planets in the solar system over
time, then deal.II is clearly not your right tool. On the other hand, if
this evolution is due to the interaction with the solution of partial differential
equation, or if having a mesh to determine which particles interact
with others (such as in the
[smoothed particle hydrodynamics (SPH)](https://en.wikipedia.org/wiki/Smoothed-particle_hydrodynamics)
method), then deal.II has support for you.

The case we will consider here is how electrically charged particles move through
an electric field. As motivation, we will consider
[cathode rays](https://en.wikipedia.org/wiki/Cathode_ray): Electrons emitted by a
heated piece of metal that is negatively charged (the "cathode"), and that are
then accelerated by an electric field towards the positively charged electrode
(the "anode"). The anode is typically ring-shaped so that the majority of
electrons can fly through the hole in the form of an electron beam. In the olden
times, they might then have illuminated the screen of a TV built from a
[cathode ray tube](https://en.wikipedia.org/wiki/Cathode-ray_tube).
Today, instead, electron beams are useful in
[X-ray machines](https://en.wikipedia.org/wiki/X-ray_tube),
[electron beam lithography](https://en.wikipedia.org/wiki/Electron-beam_lithography),
[electron beam welding](https://en.wikipedia.org/wiki/Electron-beam_welding), and
a number of other areas.

The equations we will then consider are as follows: First, we need to describe
the electric field. This is most easily accomplished by noting that the electric
potential $V$ satisfied the equation
@f[
  -\epsilon_0 \Delta V = \rho
@f]
where $\epsilon_0$ is the dielectric constant of vacuum, and $\rho$ is the charge
density. This is augmented by boundary conditions that we will choose as follows:
@f{align*}{
  V &= -V_0 && \text{on}\; \Gamma_\text{cathode}\subset\partial\Omega \\
  V &= +V_0 && \text{on}\; \Gamma_\text{anode}\subset\partial\Omega \\
  \epsilon\frac{\partial V}{\partial n} &= 0
   && \text{on}\; \partial\Omega\setminus\Gamma_\text{cathode}\setminus\Gamma_\text{anode}.
@f}
In other words, we prescribe voltages $+V_0$ and $-V_0$ at the two electrodes
and insulating (Neumann) boundary conditions elsewhere. Since the dynamics of the
particles are purely due to the electric field $\mathbf E=\nabla V$, we could
as well have prescribed $2V_0$ and $0$ at the two electrodes -- all that matters
is the voltage difference at the two electrodes.

Given this electric potential $V$ and the electric field $\mathbf E=\nabla V$,
we can describe the trajectory of the $i$th particle using the differential
equation
@f[
  m {\ddot {\mathbf x}}_i = e\mathbf E,
@f]
where $m,e$ are the mass and electric charge of each particle. In practice, it
is convenient to write this as a system of first-order differential equations
in the position $\mathbf x$ and velocity $\mathbf v$:
@f{align*}{
  {\dot {\mathbf v}}_i &= \frac{e\mathbf E}{m}, \\
  {\dot {\mathbf x}}_i &= {\mathbf v}_i.
@f}
The deal.II class we will use to deal with particles, Particles::ParticleHandler,
stores particles in a way so that the position $\mathbf x_i$ is part of the
Particles::ParticleHandler data structures. (It stores particles sorted
by cell they are in, and consequently needs to know where each particle is.)
The velocity $\mathbf v_i$, on the other hand, is of no concern to
Particles::ParticleHandler and consequently we will store it as a
"property" of each particle that we will update in each time step. Properties
can also be used to store any other quantity we might care about each particle:
its charge, or if they were larger than just an electron, its color, mass,
attitude in space, chemical composition, etc.

There remain two things to discuss to complete the model:
Where particles start and what the charge density $\rho$ is.

First, historically, cathode rays used very large electric fields to pull
electrons out of the metal. This produces only a relatively small current. One
can do better by heating the cathode: a statistical fraction of electrons in that
case have enough thermal energy to leave the metal; the electric field then just
has to be strong enough to pull them away from the attraction of their host
body. We will model this in the following way: We will create a new particle if
(i) the electric field points away from the electrode, i.e., if
$\mathbf E \cdot \mathbf n < 0$ where $\mathbf n$ is the normal vector at a
face pointing out of the domain (into the electrode), and (ii) the electric
field exceeds a threshold value $|\mathbf E|\ge E_\text{threshold}$. This is
surely not a sufficiently accurate model for what really happens, but is good
enough for our current tutorial program.

Second, in principle we would have to model the charge density via
@f[
  \rho(\mathbf x) = \sum_i e\delta(\mathbf x-\mathbf x_i).
@f]

@note
The issue now is that in reality, a cathode ray tube in an old television
yields a current of somewhere around a few milli-Amperes. In the much higher
energy beams of particle accelerators, the current may only be a few
nano-Ampere. But an Ampere is $6\times 10^{18}$ electrons flowing per
second. Now, as you will see in the results section, we really only simulate
a few microseconds ($10^{-5}$ seconds), but that still results in very very
large numbers of electrons -- far more than we can hope to simulate
with a program as small as the current one. As a consequence, let us
presume that each particle represents $N$ electrons. Then the particle
mass and charge are also $Nm$ and $Ne$ and the equations we have to
solve are
@f[
  (Nm) {\ddot {\mathbf x}}_i = (Ne)\mathbf E,
@f]
which is of course exactly the same as above. On the other hand, the charge
density for these "clumps" of electrons is given by
@f[
  \rho(\mathbf x) = \sum_i (Ne)\delta(\mathbf x-\mathbf x_i).
@f]
It is this form that we will implement in the program, where $N$ is chosen
rather large in the program to ensure that the particles actually affect
the electric field. (This may not be realistic in practice: In most cases,
there are just not enough electrons to actually affect the overall
electric field. But realism is not our goal here.)


@note One may wonder why the equation for the electric field (or, rather,
the electric potential) has no time derivative whereas the equations for
the electron positions do. In essence, this is a modeling assumption: We
assume that the particles move so slowly that at any given time the
electric field is in equilibrium. This is saying, in other words, that
the velocity of the electrons is much less than the speed of light. In
yet other words, we can rephrase this in terms of the electrode voltage
$V_0$: Since every volt of electric potential accelerates electrons by
approximately 600 km/s (neglecting relativistic effects), requiring
$|\mathbf v_i\|\ll c$ is equivalent to saying that $2V_0 \ll 500 \text{V}$.
Under this assumption (and the assumption that the total number
of electrons is small), one can also neglect the creation of
magnetic fields by the moving charges, which would otherwise also affect
the movement of the electrons.


<h3>Time discretization</h3>

The equations outlined above form a set of coupled differential equations.
Let us bring them all together in one place again to make that clear:
@f{align*}{
  -\epsilon_0 \Delta V &= \sum_i e\delta(\mathbf x-\mathbf x_i)
  \\
  {\dot {\mathbf x}}_i &= {\mathbf v}_i,
  \\
  {\dot {\mathbf v}}_i &= \frac{e\mathbf E}{m} = \frac{e\mathbf \nabla V}{m}.
@f}
Because of the awkward dependence of the electric potential on the
particle locations, we don't want to solve this as a coupled system
but instead use a decoupled approach where we first solve for the
potential in each time step and then the particle locations. (One
could also do it the other way around, of course.) This is very
much in the same spirit as we do in step-21, step-31, and step-32,
to name just a few, and can all be understood in the context of
the operator splitting methods discussed in step-58.

So, if we denote by an upper index $n$ the time step, and if we
use a simple time discretization for the ODE, then this means
that we have to solve the following set of equations in each time
step:
@f{align*}{
  -\epsilon_0 \Delta V^{(n)} &= \sum_i e\delta(\mathbf x-\mathbf x_i^{(n-1)})
  \\
  \frac{{\mathbf v}_i^{(n)}-{\mathbf v}_i^{(n-1)}}{\Delta t} &= \frac{e\nabla V^{(n)}}{m}
  \\
  \frac{{\mathbf x}_i^{(n)}-{\mathbf x}_i^{(n-1)}}{\Delta t} &= {\mathbf v}_i^{(n)}.
@f}
There are of course many better ways to do a time discretization (for
example the simple [leapfrog scheme](https://en.wikipedia.org/wiki/Leapfrog_integration))
but this isn't the point of the tutorial program, and so we will be content
with what we have here. (We will comment on a piece of this puzzle in the
<a href="#extensions">possibilities for extensions</a> section of this program,
however.)

There remains the question of how we should choose the time step size $\Delta t$.
The limitation here is that the Particles::ParticleHandler class needs to
keep track of which cell each particle is in. This is particularly an issue if
we are running computations in parallel (say, in step-70) because in that case
every process only stores those cells it owns plus one layer of "ghost cells".
That's not relevant here, but in general we should make sure that over the
course of each time step, a particle moves only from one cell to any
of its immediate neighbors (face, edge, or vertex neighbors). If we can ensure
that, then Particles::ParticleHandler is guaranteed to be able to figure out
which cell a particle ends up in. To do this, a useful rule of thumb
is that we should choose the time step so that for all particles the expected
distance the particle moves by is less than one cell diameter:
@f[
  \Delta t \le \frac{h_i}{\|\mathbf v_i\|} \qquad\qquad \forall i,
@f]
or equivalently
@f[
  \Delta t \le \min_i \frac{h_i}{\|\mathbf v_i\|}.
@f]
Here, $h_i$ is the length of the shortest edge of the cell on which particle
$i$ is located -- in essence, a measure of the size of a cell.

On the other hand, a particle might already be at the boundary of one cell
and the neighboring cell might be once further refined. So then the time to
cross that *neighboring* cell would actually be half the amount above,
suggesting
@f[
  \Delta t \le \min_i \frac{\tfrac 12 h_i}{\|\mathbf v_i\|}.
@f]

But even that is not good enough: The formula above updates the particle
positions in each time using the formula
@f[
\frac{{\mathbf x}_i^{(n)}-{\mathbf x}_i^{(n-1)}}{\Delta t} = {\mathbf v}_i^{(n)},
@f]
that is, using the *current* velocity ${\mathbf v}_i^{n}$. But we don't have
the current velocity yet at the time when we need to choose $\Delta t$ -- which
is after we have updated the potential $V^{(n)}$ but before we update the
velocity from ${\mathbf v}_i^{(n-1)}$ to ${\mathbf v}_i^{(n)}$. All we have is
${\mathbf v}_i^{(n-1)}$. So we need an additional safety factor for our final
choice:
@f[
  \Delta t^{(n)} =
  c_\text{safety} \min_i \frac{\tfrac 12 h_i}{\|\mathbf v_i^{(n-1)}\|}.
@f]
How large should $c_\text{safety}$ be? That depends on how much of underestimate
$\|\mathbf v_i^{(n-1)}\|$ might be compared to $\|\mathbf v_i^{(n)}\|$, and that
is actually quite easy to assess: A particle created in one time step with
zero velocity will roughly pick up equal velocity increments in each successive
time step if the electric field it encounters along the way were roughly
constant. So the maximal difference between $\|\mathbf v_i^{(n-1)}\|$ and
$\|\mathbf v_i^{(n)}\|$ would be a factor of two. As a consequence,
we will choose $c_\text{safety}=0.5$.

There is only one other case we ought to consider: What happens in
the very first time step? There, any particles to be moved along have just
been created, but they have a zero velocity. So we don't know what
velocity we should choose for them. Of course, in all other time steps
there are also particles that have just been created, but in general,
the particles with the highest velocity limit the time step size and so the
newly created particles with their zero velocity don't matter. But if we *only*
have such particles?

In that case, we can use the following approximation: If a particle
starts at $\mathbf v^{(0)}=0$, then the update formula tells us that
@f[
  {\mathbf v}_i^{(1)} = \frac{e\nabla V^{(1)}}{m} \Delta t,
@f]
and consequently
@f[
    \frac{{\mathbf x}_i^{(1)}-{\mathbf x}_i^{(0)}}{\Delta t} = {\mathbf v}_i^{(1)},
@f]
which we can write as
@f[
    {\mathbf x}_i^{(1)} - {\mathbf x}_i^{(0)} = \frac{e\nabla V^{(1)}}{m} \Delta t^2.
@f]
Not wanting to move a particle by more than $\frac 12 h_i$ then implies that we should
choose the time step as
@f[
  \Delta t
  \le
  \min_i
  \sqrt{ \frac{h_i m}{e \|\nabla V^{(1)}\| }}.
@f]
Using the same argument about neighboring cells possibly being smaller by
a factor of two then leads to the final formula for time step zero:
@f[
  \Delta t
  =
  \min_i
  \sqrt{ \frac{\frac 12 h_i m}{e \|\nabla V^{(1)}\| } }.
@f]

Strictly speaking, we would have to evaluate the electric potential $V^{(1)}$ at
the location of each particle, but a good enough approximation is to use the
maximum of the values at the vertices of the respective cell. (Why the vertices
and not the midpoint? Because the gradient of the solution of the Laplace equation,
i.e., the electric field, is largest in corner singularities which are located
at the vertices of cells.) This has the advantage that we can make good use of the
FEValues functionality which can recycle pre-computed material as long as the
quadrature points are the same from one cell to the next.

We could always run this kind of scheme to estimate the difference between
$\mathbf v_i^{(n-1)}$ and $\mathbf v_i^{(n)}$, but it relies on evaluating the
electric field $\mathbf E$ on each cell, and that is expensive. As a
consequence, we will limit this approach to the very first time step.


<h3>Spatial discretization</h3>

Having discussed the time discretization, the discussion of the spatial
discretization is going to be short: We use quadratic finite elements,
i.e., the space $Q_2$, to approximate the electric potential $V$. The
mesh is adapted a couple of times during the initial time step. All
of this is entirely standard if you have read step-6, and the implementation
does not provide for any kind of surprise.



<h3>Dealing with particles programmatically</h3>

Adding and moving particles is, in practice, not very difficult in deal.II.
To add one, the `create_particles()` function of this program simply
uses a code snippet of the following form:
@code
  Particles::Particle<dim> new_particle;
  new_particle.set_location(location);
  new_particle.set_reference_location
      (mapping.transform_real_to_unit_cell(cell, location));
  new_particle.set_id(n_current_particles);

  particle_handler.insert_particle(new_particle, cell);
@endcode
In other words, it is not all that different from inserting an object
into a `std::set` or `std::map`: Create the object, set its properties
(here, the current location, its reference cell location, and its id)
and call `insert_particle`. The only thing that may be surprising is
the reference location: In order to evaluate things such as
$\nabla V(\mathbf x_i)$, it is necessary to evaluate finite element
fields at locations $\mathbf x_i$. But this requires evaluating the
finite element shape functions at points on the reference cell
$\hat{\mathbf x}_i$. To make this efficient, every particle doesn't
just store its location and the cell it is on, but also what location
that point corresponds to in the cell's reference coordinate system.

Updating a particle's position is then no more difficult: One just has
to call
@code
  particle->set_location(new_location);
@endcode
We do this in the `move_particles()` function. The only difference
is that we then have to tell the Particles::ParticleHandler class
to also find what cell that position corresponds to (and, when computing
in parallel, which process owns this cell). For efficiency reason,
this is most easily done after updating all particles' locations,
and is achieved via the
Particles::ParticleHandler::sort_particles_into_subdomains_and_cells()
function.

There are, of course, times where a particle may leave the domain in
question. In that case,
Particles::ParticleHandler::sort_particles_into_subdomains_and_cells()
can not find a surrounding cell and simply deletes the particle. But, it
is often useful to track the number of particles that have been lost
this way, and for this the Particles::ParticleHandler class offers a
"signal" that one can attach to. We show how to do this in the
constructor of the main class to count how many particles were lost
in each time step. Specifically, the way this works is that
the Particles::ParticleHandler class has a "signal" to which one
can attach a function that will be executed whenever the signal
is triggered. Here, this looks as follows:
@code
    particle_handler.signals.particle_lost.connect(
      [this](const typename Particles::ParticleIterator<dim> &        particle,
             const typename Triangulation<dim>::active_cell_iterator &cell)
      {
        this->track_lost_particle(particle, cell);
      });
@endcode
That's a bit of a mouthful, but what's happening is this: We declare
a lambda function that "captures" the `this` pointer (so that we can access
member functions of the surrounding object inside the lambda function), and
that takes two arguments:
- A reference to the particle that has been "lost".
- A reference to the cell it was on last.
The lambda function then simply calls the `CathodeRaySimulator::track_lost_particle`
function with these arguments. When we attach this lambda function to the
signal, the Particles::ParticleHandler::sort_particles_into_subdomains_and_cells()
function will trigger the signal for every particle for which it can't
find a new home. This gives us the chance to record where the particle
is, and to record statistics on it.


@note In this tutorial program, we insert particles by hand and at
  locations we specifically choose based on conditions that include
  the solution of the electrostatic problem. But there are other cases
  where one primarily wants to use particles as passive objects, for
  example to trace and visualize the flow field of a fluid flow
  problem. In those cases, there are numerous functions in the
  Particles::Generators namespace that can generate particles
  automatically. One of the functions of this namespace is also used
  in the step-70 tutorial program, for example.


<h3>The test case</h3>

The test case here is not meant to be a realistic depiction of a cathode
ray tube, but it has the right general characteristics and the point is,
in any case, only to demonstrate how one would implement deal.II codes
that use particles.

The following picture shows the geometry that we're going to use:

<p align="center">
  <img src="https://www.dealii.org/images/steps/developer/step-19.geometry.png"
       alt="The geometry used in this program"
       width="600">
</p>

In this picture, the parts of the boundary marked in red and blue are the
cathode, held at an electric potential $V=-V_0$. The part of the cathode shown
in red is the part that is heated, leading to electrons leaving the metal
and then being accelerated by the electric field (a few electric
field lines are also shown). The green part of the boundary is the anode,
held at $V=+V_0$. The rest of the boundary satisfies a Neumann boundary
condition.

This setup mimics real devices. The re-entrant corner results in an
electric potential $V$ whose derivative (the electric field $\mathbf E$)
has a singularity -- in other words, it becomes very large in the vicinity
of the corner, allowing it to rip electrons away from the metal. These
electrons are then accelerated towards the (green) anode which has a
hole in the middle through which the electrons can escape the device and
fly on to hit the screen, where they excite the "phosphor" to then emit
the light that we see from these old-style TV screens. The non-heated
part of the cathode is not subject
to the emission of electrons -- in the code, we will mark this as the
"focussing element" of the tube, because its negative electric voltage
repels the electrons and makes sure that they do not just fly
away from the heated part of the cathode perpendicular to the boundary,
but in fact bend their paths towards the anode on the right.

The electric field lines also shown in the picture illustrate
that the electric field connects the negative and positive
electrodes, respectively. The accelerating force the electrons
experience is along these field lines. Finally, the picture shows the
mesh used in the computation, illustrating that there are
singularities at the tip of the re-rentrant corner as well
as at all places where the boundary conditions change; these
singularities are visible because the mesh is refined in these
locations.

Of practical interest is to figure out which fraction of the
electrons emitted from the cathode actually make it through the
hole in the anode -- electrons that just bounce into the anode
itself are not actually doing anything useful other than converting
electricity into heat. As a consequence, in the `track_lost_particle()`
function (which is called for each particle that leaves the domain,
see above), we will estimate where it might have left the domain
and report this in the output.


@note It is worth repeating that neither the geometry used here,
nor in fact any other aspect of this program is intended to represent
anything even half-way realistic. Tutorial programs are our tools to
teach how deal.II works, and we often use situations for which we
have some kind of intuition since this helps us interpret the output
of a program, but that's about the extent to which we intend the
program to do anything of use besides being a teaching tool.


examples/step-19/doc/results.dox
<h1>Results</h1>

When this program is run, it produces output that looks as follows:
```
Timestep 1
  Field degrees of freedom:                                 4989
  Total number of particles in simulation:  20
  Number of particles lost this time step:  0

  Now at t=2.12647e-07, dt=2.12647e-07.

Timestep 2
  Field degrees of freedom:                 4989
  Total number of particles in simulation:  24
  Number of particles lost this time step:  0

  Now at t=4.14362e-07, dt=2.01715e-07.

Timestep 3
  Field degrees of freedom:                 4989
  Total number of particles in simulation:  28
  Number of particles lost this time step:  0

  Now at t=5.96019e-07, dt=1.81657e-07.

Timestep 4
  Field degrees of freedom:                 4989
  Total number of particles in simulation:  32
  Number of particles lost this time step:  0

  Now at t=7.42634e-07, dt=1.46614e-07.


...


  Timestep 1000
  Field degrees of freedom:                 4989
  Total number of particles in simulation:  44
  Number of particles lost this time step:  6
  Fraction of particles lost through anode: 0.0601266

  Now at t=4.93276e-05, dt=4.87463e-08.

Timestep 1001
  Field degrees of freedom:                 4989
  Total number of particles in simulation:  44
  Number of particles lost this time step:  0
  Fraction of particles lost through anode: 0.0601266

  Now at t=4.93759e-05, dt=4.82873e-08.


...


Timestep 2091
  Field degrees of freedom:                 4989
  Total number of particles in simulation:  44
  Number of particles lost this time step:  0
  Fraction of particles lost through anode: 0.0503338

  Now at t=9.99237e-05, dt=4.26254e-08.

Timestep 2092
  Field degrees of freedom:                 4989
  Total number of particles in simulation:  44
  Number of particles lost this time step:  0
  Fraction of particles lost through anode: 0.0503338

  Now at t=9.99661e-05, dt=4.24442e-08.

Timestep 2093
  Field degrees of freedom:                 4989
  Total number of particles in simulation:  44
  Number of particles lost this time step:  2
  Fraction of particles lost through anode: 0.050308

  Now at t=0.0001, dt=3.38577e-08.
```

Picking a random few time steps, we can visualize the solution in the
form of streamlines for the electric field and dots for the electrons:
<div class="twocolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-19.solution.0000.png"
         alt="The solution at time step 0 (t=0 seconds)."
         width="500">
    <br>
    Solution at time step 0 (t=0 seconds).
    <br>
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-19.solution.1400.png"
         alt="The solution at time step 1400 (t=0.000068 seconds)."
         width="500">
    <br>
    Solution at time step 1400 (t=0.000068 seconds).
    <br>
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-19.solution.0700.png"
         alt="The solution at time step 700 (t=0.000035 seconds)."
         width="500">
    <br>
    Solution at time step 700 (t=0.000035 seconds).
    <br>
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-19.solution.2092.png"
         alt="The solution at time step 2092 (t=0.0001 seconds)."
         width="500">
    <br>
    Solution at time step 2092 (t=0.0001 seconds).
    <br>
  </div>
</div>

That said, a more appropriate way to visualize the results of this
program are by creating a video that shows how these electrons move, and how
the electric field changes in response to their motion:

@htmlonly
<p align="center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/HwUtE7xuteE"
   frameborder="0"
   allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
   allowfullscreen></iframe>
 </p>
@endhtmlonly

What you can see here is how the "focus element" of the boundary with its negative
voltage repels the electrons and makes sure that they do not just fly away
perpendicular from the cathode (as they do in the initial part of their
trajectories). It also shows how the electric field lines
move around over time, in response to the charges flying by -- in other words,
the feedback the particles have on the electric field that itself drives the
motion of the electrons.

The movie suggests that electrons move in "bunches" or "bursts". One element of
this appearance is an artifact of how the movie was created: Every frame of the
movie corresponds to one time step, but the time step length varies. More specifically,
the fastest particle moving through the smallest cell determines the length of the
time step (see the discussion in the introduction), and consequently time steps
are small whenever a (fast) particle moves through the small cells at the right
edge of the domain; time steps are longer again once the particle has left
the domain. This slowing-accelerating effect can easily be visualized by plotting
the time step length shown in the screen output.

The second part of this is real, however: The simulation creates a large group
of particles in the beginning, and fewer after about the 300th time step. This
is probably because of the negative charge of the particles in the simulation:
They reduce the magnitude of the electric field at the (also negatively charged
electrode) and consequently reduce the number of points on the cathode at which
the magnitude exceeds the threshold necessary to draw an electron out of the
electrode.


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

<h4> Avoiding a performance bottleneck with particles </h4>

The `assemble_system()`, `move_particles()`, and `update_timestep_size()`
functions all call Particles::ParticleHandler::particles_in_cell() and
Particles::ParticleHandler::n_particles_in_cell() that query information
about the particles located on the current cell. While this is convenient,
it's also inefficient. To understand why this is so, one needs to know
how particles are stored in Particles::ParticleHandler: namely, in a
data structure in which particles are ordered in some kind of linear
fashion sorted by the cell they are on. Consequently, in order to find
the particles associated with a given cell, these functions need to
search for the first (and possibly last) particle on a given cell --
an effort that costs ${\cal O}(\log N)$ operations where $N$ is the
number of particles. But this is repeated on every cell; assuming that
for large computations, the number of cells and particles are roughly
proportional, the accumulated cost of these function calls is then
${\cal O}(N \log N)$ and consequently larger than the ${\cal O}(N)$
cost that we should shoot for with all parts of a program.

We can make this cheaper, though. First, instead of calling
Particles::ParticleHandler::n_particles_in_cell(), we might first call
Particles::ParticleHandler::particles_in_cell() and then compute the
number of particles on a cell by just computing the distance of the last
to the first particle on the current cell:
@code
  const typename Particles::ParticleHandler<dim, spacedim>::particle_iterator_range
    particles_in_cell = particle_handler.particles_in_cell(cell);
  const unsigned int
    n_particles_in_cell = std::distance (particles_in_cell.begin(),
                                         particles_in_cell.end());
@endcode
The first of these calls is of course still ${\cal O}(\log N)$,
but at least the second call only takes a compute time proportional to
the number of particles on the current cell and so, when accumulated
over all cells, has a cost of ${\cal O}(N)$.

But we can even get rid of the first of these calls with some proper algorithm
design. That's because particles are ordered in the same way as cells, and so
we can just walk them as we move along on the cells. The following outline
of an algorithm does this:
@code
  auto begin_particle_on_cell = particle_handler.begin();
  for (const auto &cell : dof_handler.active_cell_iterators())
    {
      unsigned int n_particles_on_cell = 0;
      auto end_particle_on_cell = begin_particle_on_cell;
      while (end_particle_on_cell->get_surrounding_cell(triangulation)
             == cell)
        {
          ++n_particles_on_cell;
          ++end_particle_on_cell;
        }

      ...now operate on the range of particles from begin_particle_on_cell
         to end_particle_on_cell, all of which are known to be on the current
         cell...;

      // Move the begin iterator forward so that it points to the first
      // particle on the next cell
      begin_particle_on_cell = end_particle_on_cell;
    }
@endcode

In this code, we touch every cell exactly once and we never have to search
the big data structure for the first or last particle on each cell. As a
consequence, the algorithm costs a total of ${\cal O}(N)$ for a complete
sweep of all particles and all cells.

It would not be very difficult to implement this scheme for all three of the
functions in this program that have this issue.


<h4> More statistics about electrons </h4>

The program already computes the fraction of the electrons that leave the
domain through the hole in the anode. But there are other quantities one might be
interested in. For example, the average velocity of these particles. It would
not be very difficult to obtain each particle's velocity from its properties,
in the same way as we do in the `move_particles()` function, and compute
statistics from it.


<h4> A better-synchronized visualization </h4>

As discussed above, there is a varying time difference between different frames
of the video because we create output for every time step. A better way to
create movies would be to generate a new output file in fixed time intervals,
regardless of how many time steps lie between each such point.


<h4> A better time integrator </h4>

The problem we are considering in this program is a coupled, multiphysics
problem. But the way we solve it is by first computing the (electric) potential
field and then update the particle locations. This is what is called an
"operator-splitting method", a concept we will investigate in more detail
in step-58.

While it is awkward to think of a way to solve this problem that does not involve
splitting the problem into a PDE piece and a particles piece, one
*can* (and probably should!) think of a better way to update the particle
locations. Specifically, the equations we use to update the particle location
are
@f{align*}{
  \frac{{\mathbf v}_i^{(n)}-{\mathbf v}_i^{(n-1)}}{\Delta t} &= \frac{e\nabla V^{(n)}}{m}
  \\
  \frac{{\mathbf x}_i^{(n)}-{\mathbf x}_i^{(n-1)}}{\Delta t} &= {\mathbf v}_i^{(n)}.
@f}
This corresponds to a simple forward-Euler time discretization -- a method of
first order accuracy in the time step size $\Delta t$ that we know we should
avoid because we can do better. Rather, one might want to consider a scheme such
as the
[leapfrog scheme](https://en.wikipedia.org/wiki/Leapfrog_integration)
or more generally
[symplectic integrators](https://en.wikipedia.org/wiki/Symplectic_integrator)
such as the
[Verlet scheme](https://en.wikipedia.org/wiki/Verlet_integration).


<h4> Parallelization </h4>

In release mode, the program runs in about 3.5 minutes on one of the author's
laptops at the time of writing this. That's acceptable. But what if we wanted
to make the simulation three-dimensional? If we wanted to not use a maximum
of around 100 particles at any given time (as happens with the parameters
used here) but 100,000? If we needed a substantially finer mesh?

In those cases, one would want to run the program not just on a single processor,
but in fact on as many as one has available. This requires parallelization
both the PDE solution as well as over particles. In practice, while there
are substantial challenges to making this efficient and scale well, these
challenges are all addressed in deal.II itself. For example, step-40 shows
how to parallelize the finite element part, and step-70 shows how one can
then also parallelize the particles part.


examples/step-2/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

@dealiiVideoLecture{9}

After we have created a grid in the previous example, we now show how
to define degrees of freedom on this mesh. For this example, we
will use the lowest order ($Q_1$) finite elements, for which the degrees
of freedom are associated with the vertices of the mesh. Later
examples will demonstrate higher order elements where degrees of freedom are
not necessarily associated with vertices any more, but can be associated
with edges, faces, or cells.

The term "degree of freedom" is commonly used in the finite element community
to indicate two slightly different, but related things. The first is that we'd
like to represent the finite element solution as a linear combination of shape
functions, in the form $u_h(\mathbf x) = \sum_{j=0}^{N-1} U_j \varphi_j(\mathbf
x)$. Here, $U_j$ is a vector of expansion coefficients. Because we don't know
their values yet (we will compute them as the solution of a linear or
nonlinear system), they are called "unknowns" or "degrees of freedom". The
second meaning of the term can be explained as follows: A mathematical
description of finite element problems is often to say that we are looking for
a finite dimensional function $u_h \in V_h$ that satisfies some set of equations
(e.g. $a(u_h,\varphi_h)=(f,\varphi_h)$ for all test functions $\varphi_h\in
V_h$). In other words, all we say here that the solution needs to lie in some
space $V_h$. However, to actually solve this problem on a computer we need to
choose a basis of this space; this is the set of shape functions
$\varphi_j(\mathbf x)$ we have used above in the expansion of $u_h(\mathbf x)$
with coefficients $U_j$. There are of course many bases of the space $V_h$,
but we will specifically choose the one that is described by the finite
element functions that are traditionally defined locally on the cells of the
mesh. Describing "degrees of freedom" in this context requires us to simply
<i>enumerate</i> the basis functions of the space $V_h$. For $Q_1$ elements
this means simply enumerating the vertices of the mesh in some way, but for
higher order elements, one also has to enumerate the shape functions that are
associated with edges, faces, or cell interiors of the mesh. In other words,
the enumeration of degrees of freedom is an entirely separate thing from the
indices we use for vertices. The class that
provides this enumeration of the basis functions of $V_h$ is called DoFHandler.

Defining degrees of freedom ("DoF"s in short) on a mesh is a rather
simple task, since the library does all the work for you. Essentially,
all you have to do is create a finite element object (from one of the
many finite element classes deal.II already has, see for example the
@ref fe documentation) and give it to a DoFHandler object through the
DoFHandler::distribute_dofs function ("distributing DoFs" is the term we use
to describe the process of <i>enumerating</i> the basis functions as discussed
above). The DoFHandler is a class that
knows which degrees of freedom live where, i.e., it can answer
questions like "how many degrees of freedom are there globally" and
"on this cell, give me the global indices of the shape functions that
live here". This is the sort of information you need when determining
how big your system matrix should be, and when copying the
contributions of a single cell into the global matrix.

<h3> Sparsity </h3>

The next step would then be to compute a matrix and right hand side
corresponding to a particular differential equation using this finite element
and mesh. We will keep this step for the step-3 program and rather talk about
one practical aspect of a finite element program, namely that finite element
matrices are always very sparse: almost all entries in these
matrices are zero.

To be more precise, we say that a matrix is sparse
if the number of nonzero entries <i>per row</i> in the matrix is
bounded by a number that is independent of the overall number of degrees of
freedom. For example, the simple 5-point stencil of a finite difference
approximation of the Laplace equation leads to a sparse matrix since the
number of nonzero entries per row is five, and therefore independent of the
total size of the matrix. For more complicated problems -- say, the Stokes
problem of step-22 -- and in particular in 3d, the number of entries per row
may be several hundred. But the important point is that this number is
independent of the overall size of the problem: If you refine the mesh, the
maximal number of unknowns per row remains the same.

Sparsity is one of the distinguishing feature of
the finite element method compared to, say, approximating the solution of a
partial differential equation using a Taylor expansion and matching
coefficients, or using a Fourier basis.

In practical terms, it is the sparsity of matrices that enables us to solve
problems with millions or billions of unknowns. To understand this, note that
a matrix with $N$ rows, each with a fixed upper bound for the number of
nonzero entries, requires ${\cal O}(N)$ memory locations for storage, and a
matrix-vector multiplication also requires only ${\cal O}(N)$
operations. Consequently, if we had a linear solver that requires only a fixed
number of matrix-vector multiplications to come up with the solution of a
linear system with this matrix, then we would have a solver that can find the
values of all $N$ unknowns with optimal complexity, i.e., with a total of
${\cal O}(N)$ operations. It is clear that this wouldn't be possible if the
matrix were not sparse (because then the number of entries in the matrix would
have to be ${\cal O}(N^s)$ with some $s>1$, and doing a fixed number of
matrix-vector products would take ${\cal O}(N^s)$ operations),
but it also requires very specialized solvers such as
multigrid methods to satisfy the requirement that the solution requires only a
fixed number of matrix-vector multiplications. We will frequently look at the
question of what solver to use in the remaining programs of this tutorial.

The sparsity is generated by the fact that finite element shape
functions are defined <i>locally</i> on individual cells, rather than
globally, and that the local differential operators in the bilinear
form only couple shape functions whose support overlaps. (The "support" of
a function is the area where it is nonzero. For the finite element method,
the support of a shape function is generally the cells adjacent to the vertex,
edge, or face it is defined on.) In other words, degrees of freedom $i$ and $j$
that are not defined on the same cell do not overlap, and consequently
the matrix entry $A_{ij}$ will be zero.  (In some cases such
as the Discontinuous Galerkin method, shape functions may also connect
to neighboring cells through face integrals. But finite element
methods do not generally couple shape functions beyond the immediate
neighbors of a cell on which the function is defined.)


<h3> How degrees of freedom are enumerated </h3>

By default, the DoFHandler class enumerates degrees of freedom on a mesh in a
rather random way; consequently, the sparsity pattern is also not
optimized for any particular purpose. To show this, the code below will
demonstrate a simple way to output the "sparsity pattern" that corresponds to
a DoFHandler, i.e., an object that represents all of the potentially nonzero
elements of a matrix one may build when discretizing a partial differential
equation on a mesh and its DoFHandler. This lack of structure in the sparsity
pattern will be apparent from the pictures we show below.

For most applications and algorithms, the exact way in which degrees of freedom
are numbered does not matter. For example, the Conjugate Gradient method we
use to solve linear systems does not care. On the other hand,
some algorithms do care: in particular, some preconditioners such as SSOR
will work better if they can walk through degrees of freedom in a particular
order, and it would be nice if we could just sort them in such a way that
SSOR can iterate through them from zero to $N$ in this order. Other examples
include computing incomplete LU or Cholesky factorizations, or if we care
about the block structure of matrices (see step-20 for an example).
deal.II therefore has algorithms that can re-enumerate degrees of freedom
in particular ways in namespace DoFRenumbering. Renumbering can be thought
of as choosing a different, permuted basis of the finite element space. The
sparsity pattern and matrices that result from this renumbering are therefore
also simply a permutation of rows and columns compared to the ones we would
get without explicit renumbering.

In the program below, we will use the algorithm of Cuthill and McKee to do
so. We will show the sparsity pattern for both the original enumeration of
degrees of freedom and of the renumbered version below,
in the <a href="#Results">results section</a>.


examples/step-2/doc/results.dox
<h1>Results</h1>

The program has, after having been run, produced two sparsity
patterns. We can visualize them by opening the <code>.svg</code> files in a web browser.

The results then look like this (every point denotes an entry which
might be nonzero; of course the fact whether the entry actually is
zero or not depends on the equation under consideration, but the
indicated positions in the matrix tell us which shape functions can
and which can't couple when discretizing a local, i.e. differential,
equation):
<table style="width:60%" align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-2.sparsity-1.svg" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-2.sparsity-2.svg" alt=""></td>
  </tr>
</table>

The different regions in the left picture, indicated by kinks in the lines and
single dots on the left and top, represent the degrees of
freedom on the different refinement levels of the triangulation.  As
can be seen in the right picture, the sparsity pattern is much better
clustered around the main diagonal of the matrix after
renumbering. Although this might not be apparent, the number of
nonzero entries is the same in both pictures, of course.



<h3> Possibilities for extensions </h3>

Just as with step-1, you may want to play with the program a bit to
familiarize yourself with deal.II. For example, in the
<code>distribute_dofs</code> function, we use linear finite elements
(that's what the argument "1" to the FE_Q object is). Explore how the
sparsity pattern changes if you use higher order elements, for example
cubic or quintic ones (by using 3 and 5 as the respective arguments).

You could also explore how the sparsity pattern changes by refining
the mesh. You will see that not only the size of the matrix
changes, but also its bandwidth (the distance from the diagonal of
those nonzero elements of the matrix that are farthest away from the
diagonal), though the ratio of bandwidth to size typically shrinks,
i.e. the matrix clusters more around the diagonal.

Another idea of experiments would be to try other renumbering
strategies than Cuthill-McKee from the DoFRenumbering namespace and see how
they affect the sparsity pattern.

You can also visualize the output using <a
href="http://www.gnuplot.info/">GNUPLOT</a> (one of the simpler visualization
programs; maybe not the easiest to use since it is command line driven, but
also universally available on all Linux and other Unix-like systems) by changing from <code>print_svg()</code> to <code>print_gnuplot()</code> in <code>distribute_dofs()</code> and <code>renumber_dofs()</code>:
@code
examples/\step-2> gnuplot

        G N U P L O T
        Version 3.7 patchlevel 3
        last modified Thu Dec 12 13:00:00 GMT 2002
        System: Linux 2.6.11.4-21.10-default

        Copyright(C) 1986 - 1993, 1998 - 2002
        Thomas Williams, Colin Kelley and many others

        Type `help` to access the on-line reference manual
        The gnuplot FAQ is available from
        http://www.gnuplot.info/gnuplot-faq.html

        Send comments and requests for help to <info-gnuplot@dartmouth.edu>
        Send bugs, suggestions and mods to <bug-gnuplot@dartmouth.edu>


Terminal type set to 'x11'
gnuplot> set style data points
gnuplot> plot "sparsity_pattern.1"
@endcode

Another practice based on
<a href="http://www.gnuplot.info/">GNUPLOT</a> is trying to
print out the mesh with locations and numbering of the support
points. For that, you need to include header files for GridOut and MappingQ1.
The code for this is:
@code
  std::ofstream out("gnuplot.gpl");
  out << "plot '-' using 1:2 with lines, "
      << "'-' with labels point pt 2 offset 1,1"
      << std::endl;
  GridOut().write_gnuplot (triangulation, out);
  out << "e" << std::endl;
  const int dim = 2;
  std::map<types::global_dof_index, Point<dim> > support_points;
  DoFTools::map_dofs_to_support_points (MappingQ1<dim>(),
                                        dof_handler,
                                        support_points);
  DoFTools::write_gnuplot_dof_support_point_info(out,
                                                 support_points);
  out << "e" << std::endl;
@endcode
After we run the code, we get a file called gnuplot.gpl. To view this
file, we can run the following code in the command line:
@code
gnuplot -p gnuplot.gpl
@endcode.
With that, you will get a picture similar to
@image html support_point_dofs1.png
depending on the mesh you are looking at. For more information, see DoFTools::write_gnuplot_dof_support_point_info.


examples/step-20/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

@dealiiVideoLecture{19,20,21}

This program is devoted to two aspects: the use of mixed finite elements -- in
particular Raviart-Thomas elements -- and using block matrices to define
solvers, preconditioners, and nested versions of those that use the
substructure of the system matrix. The equation we are going to solve is again
the Poisson equation, though with a matrix-valued coefficient:
@f{eqnarray*}
  -\nabla \cdot K({\mathbf x}) \nabla p &=& f \qquad {\textrm{in}\ } \Omega, \\
  p &=& g \qquad {\textrm{on}\ }\partial\Omega.
@f}
$K({\mathbf x})$ is assumed to be uniformly positive definite, i.e., there is
$\alpha>0$ such that the eigenvalues $\lambda_i({\mathbf x})$ of $K(x)$ satisfy
$\lambda_i({\mathbf x})\ge \alpha$. The use of the symbol $p$ instead of the usual
$u$ for the solution variable will become clear in the next section.

After discussing the equation and the formulation we are going to use to solve
it, this introduction will cover the use of block matrices and vectors, the
definition of solvers and preconditioners, and finally the actual test case we
are going to solve.

We are going to extend this tutorial program in step-21 to
solve not only the mixed Laplace equation, but add another equation that
describes the transport of a mixture of two fluids.

The equations covered here fall into the class of vector-valued problems. A
toplevel overview of this topic can be found in the @ref vector_valued module.


<h3>The equations</h3>

In the form above, the Poisson equation (i.e., the Laplace equation with a nonzero
right hand side) is generally considered a good model equation
for fluid flow in porous media. Of course, one typically models fluid flow through
the <a href="https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations">Navier-Stokes
equations</a> or, if fluid velocities are slow or the viscosity is large, the
<a href="https://en.wikipedia.org/wiki/Stokes_flow">Stokes
equations</a>
(which we cover in step-22).
In the first of these two models, the forces that act are inertia and
viscous friction, whereas in the second it is only viscous friction -- i.e.,
forces that one fluid particle exerts on a nearby one. This is appropriate
if you have free flow in a large domain, say a pipe, a river, or in the air.
On the other hand, if the fluid is confined in pores, then friction forces
exerted by the pore walls on the fluid become more and more important and
internal viscous friction becomes less and less important. Modeling this
then first leads to the
<a href="https://en.wikipedia.org/wiki/Darcy%27s_law#Brinkman_form_of_Darcy's_law">Brinkman
model</a> if both effects are important, and in the limit of very small pores
to the <a href="https://en.wikipedia.org/wiki/Darcy%27s_law">Darcy equations</a>.
The latter is just a different name for the Poisson or Laplace equation,
connotating it with the area to which one wants to apply it: slow flow
in a porous medium. In essence it says that the velocity is proportional
to the negative pressure gradient that drives the fluid through the
porous medium.

The Darcy equation models this pressure that drives the flow. (Because the
solution variable is a pressure, we here use the name $p$ instead of the
name $u$ more commonly used for the solution of partial differential equations.)
Typical applications of this view of the Laplace equation are then modeling
groundwater flow, or the flow of hydrocarbons in oil reservoirs. In these
applications, $K$ is the permeability tensor, i.e., a measure for how much
resistance the soil or rock matrix asserts on the fluid flow.

In the applications named above, a desirable feature for a numerical
scheme is that it should be locally conservative, i.e., that whatever
flows into a cell also flows out of it (or the difference is equal to
the integral over the source terms over each cell, if the sources are
nonzero). However, as it turns out, the usual discretizations of the
Laplace equation (such as those used in step-3, step-4, or step-6) do
not satisfy this property. But, one can achieve this by choosing a
different formulation of the problem and a particular combination of
finite element spaces.


<h3>Formulation, weak form, and discrete problem</h3>

To this end, one first introduces a second variable, called the velocity,
${\mathbf u}=-K\nabla p$. By its definition, the velocity is a vector in the
negative
direction of the pressure gradient, multiplied by the permeability tensor. If
the permeability tensor is proportional to the unit matrix, this equation is
easy to understand and intuitive: the higher the permeability, the higher the
velocity; and the velocity is proportional to the gradient of the pressure, going from
areas of high pressure to areas of low pressure (thus the negative sign).

With this second variable, one then finds an alternative version of the
Laplace equation, called the <i>mixed formulation</i>:
@f{eqnarray*}
  K^{-1} {\mathbf u} + \nabla p &=& 0 \qquad {\textrm{in}\ } \Omega, \\
  -{\textrm{div}}\ {\mathbf u} &=& -f \qquad {\textrm{in}\ }\Omega, \\
  p &=& g \qquad {\textrm{on}\ } \partial\Omega.
@f}
Here, we have multiplied the equation defining the velocity ${\mathbf
u}$ by $K^{-1}$ because this makes the set of equations symmetric: one
of the equations has the gradient, the second the negative divergence,
and these two are of course adjoints of each other, resulting in a
symmetric bilinear form and a consequently symmetric system matrix
under the common assumption that $K$ is a symmetric tensor.

The weak formulation of this problem is found by multiplying the two
equations with test functions and integrating some terms by parts:
@f{eqnarray*}
  A(\{{\mathbf u},p\},\{{\mathbf v},q\}) = F(\{{\mathbf v},q\}),
@f}
where
@f{eqnarray*}
  A(\{{\mathbf u},p\},\{{\mathbf v},q\})
  &=&
  ({\mathbf v}, K^{-1}{\mathbf u})_\Omega - ({\textrm{div}}\ {\mathbf v}, p)_\Omega
  - (q,{\textrm{div}}\ {\mathbf u})_\Omega
  \\
  F(\{{\mathbf v},q\}) &=& -(g,{\mathbf v}\cdot {\mathbf n})_{\partial\Omega} - (f,q)_\Omega.
@f}
Here, ${\mathbf n}$ is the outward normal vector at the boundary. Note how in this
formulation, Dirichlet boundary values of the original problem are
incorporated in the weak form.

To be well-posed, we have to look for solutions and test functions in the
space $H({\textrm{div}})=\{{\mathbf w}\in L^2(\Omega)^d:\ {\textrm{div}}\ {\mathbf w}\in L^2\}$
for $\mathbf u$,$\mathbf v$, and $L^2$ for $p,q$. It is a well-known fact stated in
almost every book on finite element theory that if one chooses discrete finite
element spaces for the approximation of ${\mathbf u},p$ inappropriately, then the
resulting discrete problem is instable and the discrete solution
will not converge to the exact solution. (Some details on the problem
considered here -- which falls in the class of "saddle-point problems"
-- can be found on the Wikipedia page on the <a
href="https://en.wikipedia.org/wiki/Ladyzhenskaya%E2%80%93Babu%C5%A1ka%E2%80%93Brezzi_condition">Ladyzhenskaya-Babuska-Brezzi
(LBB) condition</a>.)

To overcome this, a number of different finite element pairs for ${\mathbf u},p$
have been developed that lead to a stable discrete problem. One such pair is
to use the Raviart-Thomas spaces $RT(k)$ for the velocity ${\mathbf u}$ and
discontinuous elements of class $DQ(k)$ for the pressure $p$. For details
about these spaces, we refer in particular to the book on mixed finite element
methods by Brezzi and Fortin, but many other books on the theory of finite
elements, for example the classic book by Brenner and Scott, also state the
relevant results. In any case, with appropriate choices of function
spaces, the discrete formulation reads as follows: Find ${\mathbf
u}_h,p_h$ so that
@f{eqnarray*}
  A(\{{\mathbf u}_h,p_h\},\{{\mathbf v}_h,q_h\}) = F(\{{\mathbf v}_h,q_h\})
  \qquad\qquad \forall {\mathbf v}_h,q_h.
@f}


Before continuing, let us briefly pause and show that the choice of
function spaces above provides us with the desired local conservation
property. In particular, because the pressure space consists of
discontinuous piecewise polynomials, we can choose the test function
$q$ as the function that is equal to one on any given cell $K$ and
zero everywhere else. If we also choose ${\mathbf v}=0$ everywhere
(remember that the weak form above has to hold for <i>all</i> discrete
test functions $q,v$), then putting these choices of test functions
into the weak formulation above implies in particular that
@f{eqnarray*}
  - (1,{\textrm{div}}\ {\mathbf u}_h)_K
  =
  -(1,f)_K,
@f}
which we can of course write in more explicit form as
@f{eqnarray*}
  \int_K {\textrm{div}}\ {\mathbf u}_h
 =
  \int_K f.
@f}
Applying the divergence theorem results in the fact that ${\mathbf
u}_h$ has to satisfy, for every choice of cell $K$, the relationship
@f{eqnarray*}
  \int_{\partial K} {\mathbf u}_h\cdot{\mathbf n}
  =
  \int_K f.
@f}
If you now recall that ${\mathbf u}$ was the velocity, then the
integral on the left is exactly the (discrete) flux across the
boundary of the cell $K$. The statement is then that the flux must be equal
to the integral over the sources within $K$. In particular, if there
are no sources (i.e., $f=0$ in $K$), then the statement is that
<i>total</i> flux is zero, i.e., whatever flows into a cell must flow out
of it through some other part of the cell boundary. This is what we call
<i>local conservation</i> because it holds for every cell.

On the other hand, the usual continuous $Q_k$ elements would not result in
this kind of property when used for the pressure (as, for example, we
do in step-43) because one can not choose a discrete test function
$q_h$ that is one on a cell $K$ and zero everywhere else: It would be
discontinuous and consequently not in the finite element
space. (Strictly speaking, all we can say is that the proof above
would not work for continuous elements. Whether these elements might
still result in local conservation is a different question as one
could think that a different kind of proof might still work; in
reality, however, the property really does not hold.)



<h3>Assembling the linear system</h3>

The deal.II library (of course) implements Raviart-Thomas elements $RT(k)$ of
arbitrary order $k$, as well as discontinuous elements $DG(k)$. If we forget
about their particular properties for a second, we then have to solve a
discrete problem
@f{eqnarray*}
  A(x_h,w_h) = F(w_h),
@f}
with the bilinear form and right hand side as stated above, and $x_h=\{{\mathbf u}_h,p_h\}$, $w_h=\{{\mathbf v}_h,q_h\}$. Both $x_h$ and $w_h$ are from the space
$X_h=RT(k)\times DQ(k)$, where $RT(k)$ is itself a space of $dim$-dimensional
functions to accommodate for the fact that the flow velocity is vector-valued.
The necessary question then is: how do we do this in a program?

Vector-valued elements have already been discussed in previous tutorial
programs, the first time and in detail in step-8. The main difference there
was that the vector-valued space $V_h$ is uniform in all its components: the
$dim$ components of the displacement vector are all equal and from the same
function space. What we could therefore do was to build $V_h$ as the outer
product of the $dim$ times the usual $Q(1)$ finite element space, and by this
make sure that all our shape functions have only a single non-zero vector
component. Instead of dealing with vector-valued shape functions, all we did
in step-8 was therefore to look at the (scalar) only non-zero component and
use the <code>fe.system_to_component_index(i).first</code> call to figure out
which component this actually is.

This doesn't work with Raviart-Thomas elements: following from their
construction to satisfy certain regularity properties of the space
$H({\textrm{div}})$, the shape functions of $RT(k)$ are usually nonzero in all
their vector components at once. For this reason, were
<code>fe.system_to_component_index(i).first</code> applied to determine the only
nonzero component of shape function $i$, an exception would be generated. What
we really need to do is to get at <em>all</em> vector components of a shape
function. In deal.II diction, we call such finite elements
<em>non-primitive</em>, whereas finite elements that are either scalar or for
which every vector-valued shape function is nonzero only in a single vector
component are called <em>primitive</em>.

So what do we have to do for non-primitive elements? To figure this out, let
us go back in the tutorial programs, almost to the very beginnings. There, we
learned that we use the <code>FEValues</code> class to determine the values and
gradients of shape functions at quadrature points. For example, we would call
<code>fe_values.shape_value(i,q_point)</code> to obtain the value of the
<code>i</code>th shape function on the quadrature point with number
<code>q_point</code>. Later, in step-8 and other tutorial programs, we learned
that this function call also works for vector-valued shape functions (of
primitive finite elements), and that it returned the value of the only
non-zero component of shape function <code>i</code> at quadrature point
<code>q_point</code>.

For non-primitive shape functions, this is clearly not going to work: there is
no single non-zero vector component of shape function <code>i</code>, and the call
to <code>fe_values.shape_value(i,q_point)</code> would consequently not make
much sense. However, deal.II offers a second function call,
<code>fe_values.shape_value_component(i,q_point,comp)</code> that returns the
value of the <code>comp</code>th vector component of shape function  <code>i</code> at
quadrature point <code>q_point</code>, where <code>comp</code> is an index between
zero and the number of vector components of the present finite element; for
example, the element we will use to describe velocities and pressures is going
to have $dim+1$ components. It is worth noting that this function call can
also be used for primitive shape functions: it will simply return zero for all
components except one; for non-primitive shape functions, it will in general
return a non-zero value for more than just one component.

We could now attempt to rewrite the bilinear form above in terms of vector
components. For example, in 2d, the first term could be rewritten like this
(note that $u_0=x_0, u_1=x_1, p=x_2$):
@f{eqnarray*}
  ({\mathbf u}_h^i, K^{-1}{\mathbf u}_h^j)
  =
  &\left((x_h^i)_0, K^{-1}_{00} (x_h^j)_0\right) +
   \left((x_h^i)_0, K^{-1}_{01} (x_h^j)_1\right) + \\
  &\left((x_h^i)_1, K^{-1}_{10} (x_h^j)_0\right) +
   \left((x_h^i)_1, K^{-1}_{11} (x_h^j)_1\right).
@f}
If we implemented this, we would get code like this:

@code
  for (unsigned int q=0; q<n_q_points; ++q)
    for (unsigned int i=0; i<dofs_per_cell; ++i)
      for (unsigned int j=0; j<dofs_per_cell; ++j)
        local_matrix(i,j) += (k_inverse_values[q][0][0] *
                              fe_values.shape_value_component(i,q,0) *
                              fe_values.shape_value_component(j,q,0)
                              +
                              k_inverse_values[q][0][1] *
                              fe_values.shape_value_component(i,q,0) *
                              fe_values.shape_value_component(j,q,1)
                              +
                              k_inverse_values[q][1][0] *
                              fe_values.shape_value_component(i,q,1) *
                              fe_values.shape_value_component(j,q,0)
                              +
                              k_inverse_values[q][1][1] *
                              fe_values.shape_value_component(i,q,1) *
                              fe_values.shape_value_component(j,q,1)
                             ) *
                             fe_values.JxW(q);
@endcode

This is, at best, tedious, error prone, and not dimension independent. There
are obvious ways to make things dimension independent, but in the end, the
code is simply not pretty. What would be much nicer is if we could simply
extract the ${\mathbf u}$ and $p$ components of a shape function $x_h^i$. In the
program we do that in the following way:

@code
  const FEValuesExtractors::Vector velocities (0);
  const FEValuesExtractors::Scalar pressure (dim);

  ...

  for (unsigned int q=0; q<n_q_points; ++q)
    for (unsigned int i=0; i<dofs_per_cell; ++i)
      for (unsigned int j=0; j<dofs_per_cell; ++j)
        local_matrix(i,j) += (fe_values[velocities].value (i, q) *
                              k_inverse_values[q] *
                              fe_values[velocities].value (j, q)
                              -
                              fe_values[velocities].divergence (i, q) *
                              fe_values[pressure].value (j, q)
                              -
                              fe_values[pressure].value (i, q) *
                              fe_values[velocities].divergence (j, q)) *
                              fe_values.JxW(q);
@endcode

This is, in fact, not only the first term of the bilinear form, but the
whole thing (sans boundary contributions).

What this piece of code does is, given an <code>fe_values</code> object, to extract
the values of the first $dim$ components of shape function <code>i</code> at
quadrature points <code>q</code>, that is the velocity components of that shape
function. Put differently, if we write shape functions $x_h^i$ as the tuple
$\{{\mathbf u}_h^i,p_h^i\}$, then the function returns the velocity part of this
tuple. Note that the velocity is of course a <code>dim</code>-dimensional tensor, and
that the function returns a corresponding object. Similarly, where we
subscript with the pressure extractor, we extract the scalar pressure
component. The whole mechanism is described in more detail in the
@ref vector_valued module.

In practice, it turns out that we can do a bit better if we evaluate the shape
functions, their gradients and divergences only once per outermost loop, and
store the result, as this saves us a few otherwise repeated computations (it is
possible to save even more repeated operations by calculating all relevant
quantities in advance and then only inserting the results in the actual loop,
see step-22 for a realization of that approach).
The final result then looks like this, working in every space dimension:

@code
  for (const auto &cell : dof_handler.active_cell_iterators())
    {
      fe_values.reinit (cell);
      local_matrix = 0;
      local_rhs = 0;

      right_hand_side.value_list (fe_values.get_quadrature_points(),
                                  rhs_values);
      k_inverse.value_list (fe_values.get_quadrature_points(),
                            k_inverse_values);

      for (unsigned int q=0; q<n_q_points; ++q)
        for (unsigned int i=0; i<dofs_per_cell; ++i)
          {
            const Tensor<1,dim> phi_i_u     = fe_values[velocities].value (i, q);
            const double        div_phi_i_u = fe_values[velocities].divergence (i, q);
            const double        phi_i_p     = fe_values[pressure].value (i, q);

            for (unsigned int j=0; j<dofs_per_cell; ++j)
              {
                const Tensor<1,dim> phi_j_u     = fe_values[velocities].value (j, q);
                const double        div_phi_j_u = fe_values[velocities].divergence (j, q);
                const double        phi_j_p     = fe_values[pressure].value (j, q);

                local_matrix(i,j) += (phi_i_u * k_inverse_values[q] * phi_j_u
                                      - div_phi_i_u * phi_j_p
                                      - phi_i_p * div_phi_j_u) *
                                     fe_values.JxW(q);
              }

            local_rhs(i) += -phi_i_p *
                            rhs_values[q] *
                            fe_values.JxW(q);
          }
@endcode

This very closely resembles the form in which we have originally written down
the bilinear form and right hand side.

There is one final term that we have to take care of: the right hand side
contained the term $(g,{\mathbf v}\cdot {\mathbf n})_{\partial\Omega}$, constituting the
weak enforcement of pressure boundary conditions. We have already seen in
step-7 how to deal with face integrals: essentially exactly the same as with
domain integrals, except that we have to use the FEFaceValues class
instead of <code>FEValues</code>. To compute the boundary term we then simply have
to loop over all boundary faces and integrate there. The mechanism works in
the same way as above, i.e. the extractor classes also work on FEFaceValues objects:

@code
        for (const auto &face : cell->face_iterators())
          if (face->at_boundary())
            {
              fe_face_values.reinit(cell, face);

              pressure_boundary_values.value_list(
                fe_face_values.get_quadrature_points(), boundary_values);

              for (unsigned int q = 0; q < n_face_q_points; ++q)
                for (unsigned int i = 0; i < dofs_per_cell; ++i)
                  local_rhs(i) += -(fe_face_values[velocities].value(i, q) *
                                    fe_face_values.normal_vector(q) *
                                    boundary_values[q] *
                                    fe_face_values.JxW(q));
@endcode

You will find the exact same code as above in the sources for the present
program. We will therefore not comment much on it below.


<h3>Linear solvers and preconditioners</h3>

After assembling the linear system we are faced with the task of solving
it. The problem here is that the matrix possesses two undesirable properties:
- It is <a href="https://en.wikipedia.org/wiki/Definiteness_of_a_matrix">indefinite</a>,
  i.e., it has both positive and negative eigenvalues.
  We don't want to prove this property here, but note that this is true
  for all matrices of the form
  $\left(\begin{array}{cc} M & B \\ B^T & 0 \end{array}\right)$
  such as the one here where $M$ is positive definite.
- The matrix has a zero block at the bottom right (there is no term in
  the bilinear form that couples the pressure $p$ with the
  pressure test function $q$).

At least it is symmetric, but the first issue above still means that
the Conjugate Gradient method is not going to work since it is only
applicable to problems in which the matrix is symmetric and positive definite.
We would have to resort to other iterative solvers instead, such as
MinRes, SymmLQ, or GMRES, that can deal with indefinite systems. However, then
the next problem immediately surfaces: Due to the zero block, there are zeros
on the diagonal and none of the usual, "simple" preconditioners (Jacobi, SSOR)
will work as they require division by diagonal elements.

For the matrix sizes we expect to run with this program, the by far simplest
approach would be to just use a direct solver (in particular, the
SparseDirectUMFPACK class that is bundled with deal.II). step-29 goes this
route and shows that solving <i>any</i> linear system can be done in just
3 or 4 lines of code.

But then, this is a tutorial: We teach how to do things. Consequently,
in the following, we will introduce some techniques that can be used in cases
like these. Namely, we will consider the linear system as not consisting of one
large matrix and vectors, but we will want to decompose matrices
into <i>blocks</i> that correspond to the individual operators that appear in
the system. We note that the resulting solver is not optimal -- there are
much better ways to efficiently compute the system, for example those
explained in the results section of step-22 or the one we use in step-43
for a problem similar to the current one. Here, our goal is simply to
introduce new solution techniques and how they can be implemented in
deal.II.


<h4>Solving using the Schur complement</h4>

In view of the difficulties using standard solvers and preconditioners
mentioned above, let us take another look at the matrix. If we sort our
degrees of freedom so that all velocity come before all pressure variables,
then we can subdivide the linear system $Ax=b$ into the following blocks:
@f{eqnarray*}
  \left(\begin{array}{cc}
    M & B \\ B^T & 0
  \end{array}\right)
  \left(\begin{array}{cc}
    U \\ P
  \end{array}\right)
  =
  \left(\begin{array}{cc}
    F \\ G
  \end{array}\right),
@f}
where $U,P$ are the values of velocity and pressure degrees of freedom,
respectively, $M$ is the mass matrix on the velocity space, $B^T$ corresponds to
the negative divergence operator, and $B$ is its transpose and corresponds
to the gradient.

By block elimination, we can then re-order this system in the following way
(multiply the first row of the system by $B^TM^{-1}$ and then subtract the
second row from it):
@f{eqnarray*}
  B^TM^{-1}B P &=& B^TM^{-1} F - G, \\
  MU &=& F - BP.
@f}
Here, the matrix $S=B^TM^{-1}B$ (called the
<a href="https://en.wikipedia.org/wiki/Schur_complement">Schur complement</a>
of $A$)
is obviously symmetric and, owing to the positive definiteness of $M$ and the
fact that $B$ has full column rank, $S$ is also positive
definite.

Consequently, if we could compute $S$, we could apply the Conjugate Gradient
method to it. However, computing $S$ is expensive because it requires us
to compute the inverse of the (possibly large) matrix $M$; and $S$ is in fact
also a full matrix because even though $M$ is sparse, its inverse $M^{-1}$
will generally be a dense matrix.
On the other hand, the CG algorithm doesn't require
us to actually have a representation of $S$: It is sufficient to form
matrix-vector products with it. We can do so in steps, using the fact that
matrix products are associative (i.e., we can set parentheses in such a
way that the product is more convenient to compute):
To compute $Sv=(B^TM^{-1}B)v=B^T(M^{-1}(Bv))$, we
<ol>
 <li> compute $w = B v$;
 <li> solve $My = w$ for $y=M^{-1}w$, using the CG method applied to the
  positive definite and symmetric mass matrix $M$;
 <li> compute $z=B^Ty$ to obtain $z=Sv$.
</ol>
Note how we evaluate the expression $B^TM^{-1}Bv$ right to left to
avoid matrix-matrix products; this way, all we have to do is evaluate
matrix-vector products.

In the following, we will then have to come up with ways to represent the
matrix $S$ so that it can be used in a Conjugate Gradient solver,
as well as to define ways in which we can precondition the solution
of the linear system involving $S$, and deal with solving linear systems
with the matrix $M$ (the second step above).

@note The key point in this consideration is to recognize that to implement
an iterative solver such as CG or GMRES, we never actually need the actual
<i>elements</i> of a matrix! All that is required is that we can form
matrix-vector products. The same is true for preconditioners. In deal.II we
encode this requirement by only requiring that matrices and preconditioners
given to solver classes have a <code>vmult()</code> member function that
does the matrix-vector product. How a class chooses to implement this
function is not important to the solver. Consequently, classes can
implement it by, for example, doing a sequence of products and linear
solves as discussed above.


<h4>The LinearOperator framework in deal.II</h4>

deal.II includes support for describing such linear operations in a very
general way. This is done with the LinearOperator class that, like
@ref ConceptMatrixType "the MatrixType concept",
defines a minimal interface for <i>applying</i> a linear operation to a
vector:
@code
    std::function<void(Range &, const Domain &)> vmult;
    std::function<void(Range &, const Domain &)> vmult_add;
    std::function<void(Domain &, const Range &)> Tvmult;
    std::function<void(Domain &, const Range &)> Tvmult_add;
@endcode
The key difference between a LinearOperator and an ordinary matrix is
however that a LinearOperator does not allow any further access to the
underlying object. All you can do with a LinearOperator is to apply its
"action" to a vector! We take the opportunity to introduce the
LinearOperator concept at this point because it is a very useful tool that
allows you to construct complex solvers and preconditioners in a very
intuitive manner.

As a first example let us construct a LinearOperator object that represents
$M^{-1}$. This means that whenever the <code>vmult()</code> function of
this operator is called it has to solve a linear system. This requires us
to specify a solver (and corresponding) preconditioner. Assuming that
<code>M</code> is a reference to the upper left block of the system matrix
we can write:
@code
    const auto op_M = linear_operator(M);

    PreconditionJacobi<SparseMatrix<double>> preconditioner_M;
    preconditioner_M.initialize(M);

    ReductionControl reduction_control_M(2000, 1.0e-18, 1.0e-10);
    SolverCG<Vector<double>>    solver_M(reduction_control_M);

    const auto op_M_inv = inverse_operator(op_M, solver_M, preconditioner_M);
@endcode
Rather than using a SolverControl we use the ReductionControl class here
that stops iterations when either an absolute tolerance is reached (for
which we choose $10^{-18}$) or when the residual is reduced by a certain
factor (here, $10^{-10}$). In contrast the SolverControl class only checks
for absolute tolerances. We have to use ReductionControl in our case to
work around a minor issue: The right hand sides that we  will feed to
<code>op_M_inv</code> are essentially formed by residuals that naturally
decrease vastly in norm as the outer iterations progress. This makes
control by an absolute tolerance very error prone.

We now have a LinearOperator <code>op_M_inv</code> that we can use to
construct more complicated operators such as the Schur complement $S$.
Assuming that <code>B</code> is a reference to the upper right block
constructing a LinearOperator <code>op_S</code> is a matter of two lines:
@code
    const auto op_B = linear_operator(B);
    const auto op_S = transpose_operator(op_B) * op_M_inv * op_B;
@endcode
Here, the multiplication of three LinearOperator objects yields a composite
object <code>op_S</code> whose <code>vmult()</code> function first applies
$B$, then $M^{-1}$ (i.e. solving an equation with $M$), and finally $B^T$
to any given input vector. In that sense <code>op_S.vmult()</code> is
similar to the following code:
@code
    B.vmult (tmp1, src); // multiply with the top right block: B
    solver_M(M, tmp2, tmp1, preconditioner_M); // multiply with M^-1
    B.Tvmult (dst, tmp2); // multiply with the bottom left block: B^T
@endcode
(<code>tmp1</code> and <code>tmp2</code> are two temporary vectors). The
key point behind this approach is the fact that we never actually create an
inner product of matrices. Instead, whenever we have to perform a matrix
vector multiplication with <code>op_S</code> we simply run all individual
<code>vmult</code> operations in above sequence.

@note We could have achieved the same goal of creating a "matrix like"
object by implementing a specialized class <code>SchurComplement</code>
that provides a suitable <code>vmult()</code> function. Skipping over some
details this might have looked like the following:
@code
class SchurComplement
{
  public:

  // ...

  void SchurComplement::vmult (Vector<double>       &dst,
                               const Vector<double> &src) const
  {
    B.vmult (tmp1, src);
    solver_M(M, tmp2, tmp1, preconditioner_M);
    B.Tvmult (dst, tmp2);
  }
};
@endcode
Even though both approaches are exactly equivalent, the LinearOperator
class has a big advantage over this manual approach.
It provides so-called
<i><a href="https://en.wikipedia.org/wiki/Syntactic_sugar">syntactic sugar</a></i>:
Mathematically, we think about $S$ as being the composite matrix
$S=B^TM^{-1}B$ and the LinearOperator class allows you to write this out
more or less verbatim,
@code
const auto op_M_inv = inverse_operator(op_M, solver_M, preconditioner_M);
const auto op_S = transpose_operator(op_B) * op_M_inv * op_B;
@endcode
The manual approach on the other hand obscures this fact.

All that is left for us to do now is to form the right hand sides of the
two equations defining $P$ and $U$, and then solve them with the Schur
complement matrix and the mass matrix, respectively. For example the right
hand side of the first equation reads $B^TM^{-1}F-G$. This could be
implemented as follows:
@code
    Vector<double> schur_rhs (P.size());
    Vector<double> tmp (U.size());
    op_M_inv.vmult (tmp, F);
    transpose_operator(op_B).vmult (schur_rhs, tmp);
    schur_rhs -= G;
@endcode
Again, this is a perfectly valid approach, but the fact that deal.II
requires us to manually resize the final and temporary vector, and that
every operation takes up a new line makes this hard to read. This is the
point where a second class in the linear operator framework can will help
us. Similarly in spirit to LinearOperator, a PackagedOperation stores a
"computation":
@code
    std::function<void(Range &)> apply;
    std::function<void(Range &)> apply_add;
@endcode
The class allows
<a href="https://en.wikipedia.org/wiki/Lazy_evaluation">lazy evaluation</a>
of expressions involving vectors and linear operators. This is done by
storing the computational expression and only performing the computation
when either the object is converted to a vector object, or
PackagedOperation::apply() (or PackagedOperation::apply_add()) is invoked
by hand. Assuming that <code>F</code> and <code>G</code> are the two
vectors of the right hand side we can simply write:
@code
    const auto schur_rhs = transpose_operator(op_B) * op_M_inv * F - G;
@endcode
Here, <code>schur_rhs</code> is a PackagedOperation that <i>records</i> the
computation we specified. It does not create a vector with the actual
result immediately.

With these prerequisites at hand, solving for $P$ and $U$ is a matter of
creating another solver and inverse:
@code
    SolverControl solver_control_S(2000, 1.e-12);
    SolverCG<Vector<double>>    solver_S(solver_control_S);
    PreconditionIdentity preconditioner_S;

    const auto op_S_inv = inverse_operator(op_S, solver_S, preconditioner_S);

    P = op_S_inv * schur_rhs;
    U = op_M_inv * (F - op_B * P);
@endcode

@note The functionality that we developed in this example step by hand is
already readily available in the library. Have a look at
schur_complement(), condense_schur_rhs(), and postprocess_schur_solution().


<h4>A preconditioner for the Schur complement</h4>

One may ask whether it would help if we had a preconditioner for the Schur
complement $S=B^TM^{-1}B$. The general answer, as usual, is: of course. The
problem is only, we don't know anything about this Schur complement matrix. We
do not know its entries, all we know is its action. On the other hand, we have
to realize that our solver is expensive since in each iteration we have to do
one matrix-vector product with the Schur complement, which means that we have
to do invert the mass matrix once in each iteration.

There are different approaches to preconditioning such a matrix. On the one
extreme is to use something that is cheap to apply and therefore has no real
impact on the work done in each iteration. The other extreme is a
preconditioner that is itself very expensive, but in return really brings down
the number of iterations required to solve with $S$.

We will try something along the second approach, as much to improve the
performance of the program as to demonstrate some techniques. To this end, let
us recall that the ideal preconditioner is, of course, $S^{-1}$, but that is
unattainable. However, how about
@f{eqnarray*}
  \tilde S^{-1} = [B^T ({\textrm{diag}\ }M)^{-1}B]^{-1}
@f}
as a preconditioner? That would mean that every time we have to do one
preconditioning step, we actually have to solve with $\tilde S$. At first,
this looks almost as expensive as solving with $S$ right away. However, note
that in the inner iteration, we do not have to calculate $M^{-1}$, but only
the inverse of its diagonal, which is cheap.

Thankfully, the LinearOperator framework makes this very easy to write out.
We already used a Jacobi preconditioner (<code>preconditioner_M</code>) for
the $M$ matrix earlier. So all that is left to do is to write out how the
approximate Schur complement should look like:
@code
    const auto op_aS =
      transpose_operator(op_B) * linear_operator(preconditioner_M) * op_B;
@endcode
Note how this operator differs in simply doing one Jacobi sweep
(i.e. multiplying with the inverses of the diagonal) instead of multiplying
with the full $M^{-1}$. (This is how a single Jacobi preconditioner step
with $M$ is defined: it is the multiplication with the inverse of the
diagonal of $M$; in other words, the operation $({\textrm{diag}\ }M)^{-1}x$
on a vector $x$ is exactly what PreconditionJacobi does.)

With all this we almost have the preconditioner completed: it should be the
inverse of the approximate Schur complement. We implement this again by
creating a linear operator with inverse_operator() function. This time
however we would like to choose a relatively modest tolerance for the CG
solver (that inverts <code>op_aS</code>). The reasoning is that
<code>op_aS</code> is only coarse approximation to <code>op_S</code>, so we
actually do not need to invert it exactly. This, however creates a subtle
problem: <code>preconditioner_S</code> will be used in the final outer CG
iteration to create an orthogonal basis. But for this to work, it must be
precisely the same linear operation for every invocation. We ensure this by
using an IterationNumberControl that allows us to fix the number of CG
iterations that are performed to a fixed small number (in our case 30):
@code
    IterationNumberControl iteration_number_control_aS(30, 1.e-18);
    SolverCG<Vector<double>>           solver_aS(iteration_number_control_aS);
    PreconditionIdentity preconditioner_aS;
    const auto preconditioner_S =
      inverse_operator(op_aS, solver_aS, preconditioner_aS);
@endcode

That's all!

Obviously, applying this inverse of the approximate Schur complement is a very
expensive preconditioner, almost as expensive as inverting the Schur
complement itself. We can expect it to significantly reduce the number of
outer iterations required for the Schur complement. In fact it does: in a
typical run on 7 times refined meshes using elements of order 0, the number of
outer iterations drops from 592 to 39. On the other hand, we now have to apply
a very expensive preconditioner 25 times. A better measure is therefore simply
the run-time of the program: on a current laptop (as of January 2019), it
drops from 3.57 to 2.05 seconds for this test case. That doesn't seem too
impressive, but the savings become more pronounced on finer meshes and with
elements of higher order. For example, an seven times refined mesh and
using elements of order 2 (which amounts to about 0.4 million degrees of
freedom) yields an improvement of 1134 to 83 outer iterations, at a runtime
of 168 seconds to 40 seconds. Not earth shattering, but significant.


<h3>Definition of the test case</h3>

In this tutorial program, we will solve the Laplace equation in mixed
formulation as stated above. Since we want to monitor convergence of the
solution inside the program, we choose right hand side, boundary conditions,
and the coefficient so that we recover a solution function known to us. In
particular, we choose the pressure solution
@f{eqnarray*}
  p = -\left(\frac \alpha 2 xy^2 + \beta x - \frac \alpha 6 x^3\right),
@f}
and for the coefficient we choose the unit matrix $K_{ij}=\delta_{ij}$ for
simplicity. Consequently, the exact velocity satisfies
@f{eqnarray*}
  {\mathbf u} =
  \left(\begin{array}{cc}
    \frac \alpha 2 y^2 + \beta - \frac \alpha 2 x^2 \\
    \alpha xy
  \end{array}\right).
@f}
This solution was chosen since it is exactly divergence free, making it a
realistic test case for incompressible fluid flow. By consequence, the right
hand side equals $f=0$, and as boundary values we have to choose
$g=p|_{\partial\Omega}$.

For the computations in this program, we choose $\alpha=0.3,\beta=1$. You can
find the resulting solution in the <a name="#Results">results section
below</a>, after the commented program.


examples/step-20/doc/results.dox
<h1>Results</h1>

<h3>Output of the program and graphical visualization</h3>


If we run the program as is, we get this output for the $32\times 32$
mesh we use (for a total of 1024 cells with 1024 pressure degrees of
freedom since we use piecewise constants, and 2112 velocities because
the Raviart-Thomas element defines one degree per freedom per face and
there are $1024 + 32 = 1056$ faces parallel to the $x$-axis and the same
number parallel to the $y$-axis):
@verbatim
\$ make run
[ 66%] Built target \step-20
Scanning dependencies of target run
[100%] Run \step-20 with Release configuration
Number of active cells: 1024
Total number of cells: 1365
Number of degrees of freedom: 3136 (2112+1024)
24 CG Schur complement iterations to obtain convergence.
Errors: ||e_p||_L2 = 0.0445032,   ||e_u||_L2 = 0.010826
[100%] Built target run
@endverbatim

The fact that the number of iterations is so small, of course, is due to
the good (but expensive!) preconditioner we have developed. To get
confidence in the solution, let us take a look at it. The following three
images show (from left to right) the x-velocity, the y-velocity, and the
pressure:

<table style="width:60%" align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-20.u_new.jpg" width="400" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-20.v_new.jpg" width="400" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-20.p_new.jpg" width="400" alt=""></td>
  </tr>
</table>



Let us start with the pressure: it is highest at the left and lowest at the
right, so flow will be from left to right. In addition, though hardly visible
in the graph, we have chosen the pressure field such that the flow left-right
flow first channels towards the center and then outward again. Consequently,
the x-velocity has to increase to get the flow through the narrow part,
something that can easily be seen in the left image. The middle image
represents inward flow in y-direction at the left end of the domain, and
outward flow in y-direction at the right end of the domain.



As an additional remark, note how the x-velocity in the left image is only
continuous in x-direction, whereas the y-velocity is continuous in
y-direction. The flow fields are discontinuous in the other directions. This
very obviously reflects the continuity properties of the Raviart-Thomas
elements, which are, in fact, only in the space H(div) and not in the space
$H^1$. Finally, the pressure field is completely discontinuous, but
that should not surprise given that we have chosen <code>FE_DGQ(0)</code> as
the finite element for that solution component.



<h3>Convergence</h3>


The program offers two obvious places where playing and observing convergence
is in order: the degree of the finite elements used (passed to the constructor
of the <code>MixedLaplaceProblem</code> class from <code>main()</code>), and
the refinement level (determined in
<code>MixedLaplaceProblem::make_grid_and_dofs</code>). What one can do is to
change these values and observe the errors computed later on in the course of
the program run.



If one does this, one finds the following pattern for the $L_2$ error
in the pressure variable:
<table align="center" class="doxtable">
  <tr>
    <th></th>
    <th colspan="3" align="center">Finite element order</th>
  </tr>
  <tr>
    <th>Refinement level</th>
    <th>0</th>
    <th>1</th>
    <th>2</th>
  </tr>
  <tr>
    <th>0</th>  <td>1.45344</td>  <td>0.0831743</td>  <td>0.0235186</td>
  </tr>
  <tr>
    <th>1</th>  <td>0.715099</td>  <td>0.0245341</td>  <td>0.00293983</td>
  </tr>
  <tr>
    <th>2</th>  <td>0.356383</td>  <td>0.0063458</td>  <td>0.000367478</td>
  </tr>
  <tr>
    <th>3</th>  <td>0.178055</td>  <td>0.00159944</td>  <td>4.59349e-05</td>
  </tr>
  <tr>
    <th>4</th>  <td>0.0890105</td>  <td>0.000400669</td>  <td>5.74184e-06</td>
  </tr>
  <tr>
    <th>5</th>  <td>0.0445032</td>  <td>0.000100218</td>  <td>7.17799e-07</td>
  </tr>
  <tr>
    <th>6</th>  <td>0.0222513</td>  <td>2.50576e-05</td>  <td>9.0164e-08</td>
  </tr>
  <tr>
    <th></th>  <th>$O(h)$</th>  <th>$O(h^2)$</th>  <th>$O(h^3)$</th>
  </tr>
</table>

The theoretically expected convergence orders are very nicely reflected by the
experimentally observed ones indicated in the last row of the table.



One can make the same experiment with the $L_2$ error
in the velocity variables:
<table align="center" class="doxtable">
  <tr>
    <th></th>
    <th colspan="3" align="center">Finite element order</th>
  </tr>
  <tr>
    <th>Refinement level</th>
    <th>0</th>
    <th>1</th>
    <th>2</th>
  </tr>
  <tr>
    <th>0</th> <td>0.367423</td> <td>0.127657</td> <td>5.10388e-14</td>
  </tr>
  <tr>
    <th>1</th> <td>0.175891</td> <td>0.0319142</td> <td>9.04414e-15</td>
  </tr>
  <tr>
    <th>2</th> <td>0.0869402</td> <td>0.00797856</td> <td>1.23723e-14</td>
  </tr>
  <tr>
    <th>3</th> <td>0.0433435</td> <td>0.00199464</td> <td>1.86345e-07</td>
  </tr>
  <tr>
    <th>4</th> <td>0.0216559</td> <td>0.00049866</td> <td>2.72566e-07</td>
  </tr>
  <tr>
    <th>5</th> <td>0.010826</td> <td>0.000124664</td> <td>3.57141e-07</td>
  </tr>
  <tr>
    <th>6</th> <td>0.00541274</td> <td>3.1166e-05</td> <td>4.46124e-07</td>
  </tr>
  <tr>
    <th></th>  <td>$O(h)$</td>  <td>$O(h^2)$</td>  <td>$O(h^3)$</td>
  </tr>
</table>
The result concerning the convergence order is the same here.



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

<h4>More realistic permeability fields</h4>

Realistic flow computations for ground water or oil reservoir simulations will
not use a constant permeability. Here's a first, rather simple way to change
this situation: we use a permeability that decays very rapidly away from a
central flowline until it hits a background value of 0.001. This is to mimic
the behavior of fluids in sandstone: in most of the domain, the sandstone is
homogeneous and, while permeable to fluids, not overly so; on the other stone,
the stone has cracked, or faulted, along one line, and the fluids flow much
easier along this large crack. Here is how we could implement something like
this:
@code
template <int dim>
void
KInverse<dim>::value_list (const std::vector<Point<dim> > &points,
                           std::vector<Tensor<2,dim> >    &values) const
{
  Assert (points.size() == values.size(),
	  ExcDimensionMismatch (points.size(), values.size()));

  for (unsigned int p=0; p<points.size(); ++p)
    {
      values[p].clear ();

      const double distance_to_flowline
        = std::fabs(points[p][1]-0.2*std::sin(10*points[p][0]));

      const double permeability = std::max(std::exp(-(distance_to_flowline*
                                                      distance_to_flowline)
                                                    / (0.1 * 0.1)),
                                           0.001);

      for (unsigned int d=0; d<dim; ++d)
	values[p][d][d] = 1./permeability;
    }
}
@endcode
Remember that the function returns the inverse of the permeability tensor.



With a significantly higher mesh resolution, we can visualize this, here with
x- and y-velocity:

<table style="width:60%" align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-20.u-wiggle.png" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-20.v-wiggle.png" alt=""></td>
  </tr>
</table>

It is obvious how fluids flow essentially only along the middle line, and not
anywhere else.



Another possibility would be to use a random permeability field. A simple way
to achieve this would be to scatter a number of centers around the domain and
then use a permeability field that is the sum of (negative) exponentials for
each of these centers. Flow would then try to hop from one center of high
permeability to the next one. This is an entirely unscientific attempt at
describing a random medium, but one possibility to implement this behavior
would look like this:
@code
template <int dim>
class KInverse : public TensorFunction<2,dim>
{
  public:
    KInverse ();

    virtual void value_list (const std::vector<Point<dim> > &points,
			     std::vector<Tensor<2,dim> >    &values) const;

  private:
    std::vector<Point<dim> > centers;
};


template <int dim>
KInverse<dim>::KInverse ()
{
  const unsigned int N = 40;
  centers.resize (N);
  for (unsigned int i=0; i<N; ++i)
    for (unsigned int d=0; d<dim; ++d)
      centers[i][d] = 2.*rand()/RAND_MAX-1;
}


template <int dim>
void
KInverse<dim>::value_list (const std::vector<Point<dim> > &points,
                           std::vector<Tensor<2,dim> >    &values) const
{
  Assert (points.size() == values.size(),
	  ExcDimensionMismatch (points.size(), values.size()));

  for (unsigned int p=0; p<points.size(); ++p)
    {
      values[p].clear ();

      double permeability = 0;
      for (unsigned int i=0; i<centers.size(); ++i)
        permeability += std::exp(-(points[p] - centers[i]).norm_square() / (0.1 * 0.1));

      const double normalized_permeability
        = std::max(permeability, 0.005);

      for (unsigned int d=0; d<dim; ++d)
	values[p][d][d] = 1./normalized_permeability;
    }
}
@endcode

A piecewise constant interpolation of the diagonal elements of the
inverse of this tensor (i.e., of <code>normalized_permeability</code>)
looks as follows:

<img src="https://www.dealii.org/images/steps/developer/step-20.k-random.png" alt="">


With a permeability field like this, we would get x-velocities and pressures as
follows:

<table style="width:60%" align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-20.u-random.png" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-20.p-random.png" alt=""></td>
  </tr>
</table>

We will use these permeability fields again in step-21 and step-43.


<h4>Better linear solvers</h4>

As mentioned in the introduction, the Schur complement solver used here is not
the best one conceivable (nor is it intended to be a particularly good
one). Better ones can be found in the literature and can be built using the
same block matrix techniques that were introduced here. We pick up on this
theme again in step-22, where we first build a Schur complement solver for the
Stokes equation as we did here, and then in the <a
href="step_22.html#improved-solver">Improved Solvers</a> section discuss better
ways based on solving the system as a whole but preconditioning based on
individual blocks. We will also come back to this in step-43.


examples/step-21/doc/intro.dox
<a name="Intro"></a> <h1>Introduction</h1>

This program grew out of a student project by Yan Li at Texas A&amp;M
University. Most of the work for this program is by her.

In this project, we propose a numerical simulation for two phase
flow problems in porous media. This problem includes one
elliptic equation and one nonlinear, time dependent transport
equation. This is therefore also the first time-dependent tutorial
program (besides the somewhat strange time-dependence of @ref step_18
"step-18").

The equations covered here are an extension of the material already covered in
step-20. In particular, they fall into the class of
vector-valued problems. A toplevel overview of this topic can be found in the
@ref vector_valued module.


<h3>The two phase flow problem</h3>

Modeling of two phase flow in porous media is important for both
environmental remediation and the management of petroleum and groundwater
reservoirs. Practical situations involving two phase flow include the
dispersal of a nonaqueous phase liquid in an aquifer, or the joint
movement of a mixture of fluids such as oil and water in a
reservoir. Simulation models, if they are to provide realistic
predictions, must accurately account for these effects.

To derive the governing equations, consider two phase flow in a
reservoir $\Omega$ under the assumption that the movement of fluids is
dominated by viscous effects; i.e. we neglect the effects of gravity,
compressibility, and capillary pressure. Porosity will be considered
to be constant. We will denote variables referring to either of the two
phases using subscripts $w$ and $o$, short for water and oil. The
derivation of the equations holds for other pairs of fluids as well,
however.

The velocity with which molecules of each of the two phases move is
determined by Darcy's law that states that the velocity is
proportional to the pressure gradient:
@f{eqnarray*}
  \mathbf{u}_{j}
  =
  -\frac{k_{rj}(S)}{\mu_{j}} \mathbf{K} \cdot \nabla p
@f}
where $\mathbf{u}_{j}$ is the velocity of phase $j=o,w$, $K$ is the
permeability tensor, $k_{rj}$ is the relative permeability of phase
$j$, $p$ is the
pressure and $\mu_{j}$ is the viscosity of phase $j$. Finally, $S$ is
the saturation (volume fraction), i.e. a function with values between
0 and 1 indicating the composition of the mixture of fluids. In
general, the coefficients $K, k_{rj}, \mu$ may be spatially dependent
variables, and we will always treat them as non-constant functions in
the following.

We combine Darcy's law with the statement of conservation of mass for
each phase,
@f[
  \textrm{div}\ \mathbf{u}_{j} = q_j,
@f]
with a source term for each phase. By summing over the two phases,
we can express the governing equations in terms of the
so-called pressure equation:
@f{eqnarray*}
- \nabla \cdot (\mathbf{K}\lambda(S) \nabla p)= q.
@f}
Here, $q$ is the sum source term, and
@f[
  \lambda(S) = \frac{k_{rw}(S)}{\mu_{w}}+\frac{k_{ro}(S)}{\mu_{o}}
@f]
is the total mobility.

So far, this looks like an ordinary stationary, Poisson-like equation that we
can solve right away with the techniques of the first few tutorial programs
(take a look at step-6, for example, for something very
similar). However, we have not said anything yet about the saturation, which
of course is going to change as the fluids move around.

The second part of the equations is the description of the
dynamics of the saturation, i.e., how the relative concentration of the
two fluids changes with time. The saturation equation for the displacing
fluid (water) is given by the following conservation law:
@f{eqnarray*}
  S_{t} + \nabla \cdot (F(S) \mathbf{u}) = q_{w},
@f}
which can be rewritten by using the product rule of the divergence operator
in the previous equation:
@f{eqnarray*}
  S_{t} + F(S) \left[\nabla \cdot \mathbf{u}\right]
        + \mathbf{u} \cdot \left[ \nabla F(S)\right]
  = S_{t} + F(S) q + \mathbf{u} \cdot \nabla F(S) = q_{w}.
@f}
Here, $q=\nabla\cdot \mathbf{u}$ is the total influx introduced
above, and $q_{w}$ is the flow rate of the displacing fluid (water).
These two are related to the fractional flow $F(S)$ in the following way:
@f[
  q_{w} = F(S) q,
@f]
where the fractional flow is often parameterized via the (heuristic) expression
@f[
  F(S)
  =
  \frac{k_{rw}(S)/\mu_{w}}{k_{rw}(S)/\mu_{w} + k_{ro}(S)/\mu_{o}}.
@f]
Putting it all together yields the saturation equation in the following,
advected form:
@f{eqnarray*}
  S_{t} + \mathbf{u} \cdot \nabla F(S) = 0,
@f}
where $\mathbf u$ is the total velocity
@f[
  \mathbf{u} =
  \mathbf{u}_{o} + \mathbf{u}_{w} = -\lambda(S) \mathbf{K}\cdot\nabla p.
@f]
Note that the advection equation contains the term $\mathbf{u} \cdot \nabla
F(S)$ rather than $\mathbf{u} \cdot \nabla S$ to indicate that the saturation
is not simply transported along; rather, since the two phases move with
different velocities, the saturation can actually change even in the advected
coordinate system. To see this, rewrite $\mathbf{u} \cdot \nabla F(S)
= \mathbf{u} F'(S) \cdot \nabla S$ to observe that the <i>actual</i>
velocity with which the phase with saturation $S$ is transported is
$\mathbf u F'(S)$ whereas the other phase is transported at velocity
$\mathbf u (1-F'(S))$. $F(S)$ is consequently often referred to as the
<i>fractional flow</i>.

In summary, what we get are the following two equations:
@f{eqnarray*}
  - \nabla \cdot (\mathbf{K}\lambda(S) \nabla p) &=& q
  \qquad \textrm{in}\ \Omega\times[0,T],
  \\
  S_{t} + \mathbf{u} \cdot \nabla F(S) &=& 0
  \qquad \textrm{in}\ \Omega\times[0,T].
@f}
Here, $p=p(\mathbf x, t), S=S(\mathbf x, t)$ are now time dependent
functions: while at every time instant the flow field is in
equilibrium with the pressure (i.e. we neglect dynamic
accelerations), the saturation is transported along with the flow and
therefore changes over time, in turn affected the flow field again
through the dependence of the first equation on $S$.

This set of equations has a peculiar character: one of the two
equations has a time derivative, the other one doesn't. This
corresponds to the character that the pressure and velocities are
coupled through an instantaneous constraint, whereas the saturation
evolves over finite time scales.

Such systems of equations are called Differential Algebraic Equations
(DAEs), since one of the equations is a differential equation, the
other is not (at least not with respect to the time variable) and is
therefore an "algebraic" equation. (The notation comes from the field
of ordinary differential equations, where everything that does not
have derivatives with respect to the time variable is necessarily an
algebraic equation.) This class of equations contains pretty
well-known cases: for example, the time dependent Stokes and
Navier-Stokes equations (where the algebraic constraint is that the
divergence of the flow field, $\textrm{div}\ \mathbf u$, must be zero)
as well as the time dependent Maxwell equations (here, the algebraic
constraint is that the divergence of the electric displacement field
equals the charge density, $\textrm{div}\ \mathbf D = \rho$ and that the
divergence of the magnetic flux density is zero: $\textrm{div}\ \mathbf
B = 0$); even the quasistatic model of step-18 falls into this
category. We will see that the different character of the two equations
will inform our discretization strategy for the two equations.


<h3>Time discretization</h3>

In the reservoir simulation community, it is common to solve the equations
derived above by going back to the first order, mixed formulation. To this
end, we re-introduce the total velocity $\mathbf u$ and write the equations in
the following form:
@f{eqnarray*}
  \mathbf{u}+\mathbf{K}\lambda(S) \nabla p&=&0 \\
  \nabla \cdot\mathbf{u} &=& q \\
  S_{t} + \mathbf{u} \cdot \nabla F(S) &=& 0.
@f}
This formulation has the additional benefit that we do not have to express the
total velocity $\mathbf u$ appearing in the transport equation as a function
of the pressure, but can rather take the primary variable for it. Given the
saddle point structure of the first two equations and their similarity to the
mixed Laplace formulation we have introduced in step-20, it
will come as no surprise that we will use a mixed discretization again.

But let's postpone this for a moment. The first business we have with these
equations is to think about the time discretization. In reservoir simulation,
there is a rather standard algorithm that we will use here. It first solves
the pressure using an implicit equation, then the saturation using an explicit
time stepping scheme. The algorithm is called IMPES for IMplicit Pressure
Explicit Saturation and was first proposed a long time ago: by Sheldon et
al. in 1959 and Stone and Gardner in 1961 (J. W. Sheldon, B. Zondek and
W. T. Cardwell: <i>One-dimensional, incompressible, non-capillary, two-phase
fluid flow in a porous medium</i>, Trans. SPE AIME, 216 (1959), pp. 290-296; H.
L. Stone and A. O. Gardner Jr: <i>Analysis of gas-cap or dissolved-gas
reservoirs</i>, Trans. SPE AIME, 222 (1961), pp. 92-104).
In a slightly modified form, this algorithm can be
written as follows: for each time step, solve
@f{eqnarray*}
  \mathbf{u}^{n+1}+\mathbf{K}\lambda(S^n) \nabla p^{n+1}&=&0 \\
  \nabla \cdot\mathbf{u}^{n+1} &=& q^{n+1} \\
  \frac {S^{n+1}-S^n}{\triangle t} + \mathbf{u}^{n+1} \cdot \nabla F(S^n) &=& 0,
@f}
where $\triangle t$ is the length of a time step. Note how we solve the
implicit pressure-velocity system that only depends on the previously computed
saturation $S^n$, and then do an explicit time step for $S^{n+1}$ that only
depends on the previously known $S^n$ and the just computed
$\mathbf{u}^{n+1}$. This way, we never have to iterate for the nonlinearities
of the system as we would have if we used a fully implicit method. (In
a more modern perspective, this should be seen as an "operator
splitting" method. step-58 has a long description of the idea behind this.)

We can then state the problem in weak form as follows, by multiplying each
equation with test functions $\mathbf v$, $\phi$, and $\sigma$ and integrating
terms by parts:
@f{eqnarray*}
  \left((\mathbf{K}\lambda(S^n))^{-1} \mathbf{u}^{n+1},\mathbf v\right)_\Omega -
  (p^{n+1}, \nabla\cdot\mathbf v)_\Omega &=&
  - (p^{n+1}, \mathbf v)_{\partial\Omega}
  \\
  (\nabla \cdot\mathbf{u}^{n+1}, \phi)_\Omega &=& (q^{n+1},\phi)_\Omega
@f}
Note that in the first term, we have to prescribe the pressure $p^{n+1}$ on
the boundary $\partial\Omega$ as boundary values for our problem. $\mathbf n$
denotes the unit outward normal vector to $\partial K$, as usual.

For the saturation equation, we obtain after integrating by parts
@f{eqnarray*}
  (S^{n+1}, \sigma)_\Omega
  -
  \triangle t
  \sum_K
  \left\{
  \left(F(S^n), \nabla \cdot (\mathbf{u}^{n+1} \sigma)\right)_K
  -
  \left(F(S^n) (\mathbf n \cdot \mathbf{u}^{n+1}, \sigma\right)_{\partial K}
  \right\}
  &=&
  (S^n,\sigma)_\Omega.
@f}
Using the fact that $\nabla \cdot \mathbf{u}^{n+1}=q^{n+1}$, we can rewrite the
cell term to get an equation as follows:
@f{eqnarray*}
  (S^{n+1}, \sigma)_\Omega
  -
  \triangle t
  \sum_K
  \left\{
  \left(F(S^n) \mathbf{u}^{n+1}, \nabla \sigma\right)_K
  -
  \left(F(S^n) (\mathbf n \cdot \mathbf{u}^{n+1}), \sigma\right)_{\partial K}
  \right\}
  &=&
  (S^n,\sigma)_\Omega +
  \triangle t \sum_K  \left(F(S^n) q^{n+1}, \sigma\right)_K.
@f}
We introduce an object of type DiscreteTime in order to keep track of the
current value of time and time step in the code. This class encapsulates many
complexities regarding adjusting time step size and stopping at a specified
final time.



<h3>Space discretization</h3>

In each time step, we then apply the mixed finite method of @ref step_20
"step-20" to the velocity and pressure. To be well-posed, we choose
Raviart-Thomas spaces $RT_{k}$ for $\mathbf{u}$ and discontinuous elements of
class $DGQ_{k}$ for $p$. For the saturation, we will also choose $DGQ_{k}$
spaces.

Since we have discontinuous spaces, we have to think about how to evaluate
terms on the interfaces between cells, since discontinuous functions are not
really defined there. In particular, we have to give a meaning to the last
term on the left hand side of the saturation equation. To this end, let us
define that we want to evaluate it in the following sense:
@f{eqnarray*}
  &&\left(F(S^n) (\mathbf n \cdot \mathbf{u}^{n+1}), \sigma\right)_{\partial K}
  \\
  &&\qquad =
  \left(F(S^n_+) (\mathbf n \cdot \mathbf{u}^{n+1}_+), \sigma\right)_{\partial K_+}
  +
  \left(F(S^n_-) (\mathbf n \cdot \mathbf{u}^{n+1}_-), \sigma\right)_{\partial K_-},
@f}
where $\partial K_{-} \dealcoloneq \{x\in \partial K, \mathbf{u}(x) \cdot \mathbf{n}<0\}$
denotes the inflow boundary and $\partial K_{+} \dealcoloneq \{\partial K \setminus
\partial K_{-}\}$ is the outflow part of the boundary.
The quantities $S_+,\mathbf{u}_+$ then correspond to the values of these
variables on the present cell, whereas $S_-,\mathbf{u}_-$ (needed on the
inflow part of the boundary of $K$) are quantities taken from the neighboring
cell. Some more context on discontinuous element techniques and evaluation of
fluxes can also be found in step-12 and step-12b.


<h3>Linear solvers</h3>

The linear solvers used in this program are a straightforward extension of the
ones used in step-20 (but without LinearOperator). Essentially, we simply have
to extend everything from
two to three solution components. If we use the discrete spaces
mentioned above and put shape functions into the bilinear forms, we
arrive at the following linear system to be solved for time step $n+1$:
@f[
\left(
\begin{array}{ccc}
M^u(S^{n}) & B^{T}& 0\\
B &    0 & 0\\
\triangle t\; H &    0& M^S
\end{array}
\right)
\left(
\begin{array}{c}
\mathbf{U}^{n+1} \\ P^{n+1} \\ S^{n+1}
\end{array}
\right)
=
\left(
\begin{array}{c}
0 \\ F_2 \\ F_3
\end{array}
\right)
@f]
where the individual matrices and vectors are defined as follows using
shape functions $\mathbf v_i$ (of type Raviart Thomas $RT_k$) for
velocities and $\phi_i$ (of type $DGQ_k$) for both pressures and saturations:
@f{eqnarray*}
M^u(S^n)_{ij} &=&
\left((\mathbf{K}\lambda(S^n))^{-1} \mathbf{v}_i,\mathbf
v_j\right)_\Omega,
\\
B_{ij} &=&
-(\nabla \cdot \mathbf v_j, \phi_i)_\Omega,
\\
H_{ij} &=&
  -
  \sum_K
  \left\{
  \left(F(S^n) \mathbf v_i, \nabla \phi_j)\right)_K
  -
  \left(F(S^n_+) (\mathbf n \cdot (\mathbf v_i)_+), \phi_j\right)_{\partial K_+}
  -
  \left(F(S^n_-) (\mathbf n \cdot (\mathbf v_i)_-), \phi_j\right)_{\partial K_-},
  \right\}
\\
M^S_{ij} &=&
(\phi_i, \phi_j)_\Omega,
\\
(F_2)_i &=&
-(q^{n+1},\phi_i)_\Omega,
\\
(F_3)_i &=&
(S^n,\phi_i)_\Omega +\triangle t \sum_K  \left(F(S^n) q^{n+1}, \phi_i\right)_K.
@f}

@note Due to historical accidents, the role of matrices $B$ and $B^T$
has been reverted in this program compared to step-20. In other words,
here $B$ refers to the divergence and $B^T$ to the gradient operators
when it was the other way around in step-20.

The system above presents a complication: Since the matrix $H_{ij}$
depends on $\mathbf u^{n+1}$ implicitly (the velocities are needed to
determine which parts of the boundaries $\partial K$ of cells are
influx or outflux parts), we can only assemble this matrix after we
have solved for the velocities.

The solution scheme then involves the following steps:
<ol>
  <li>Solve for the pressure $p^{n+1}$ using the Schur complement
  technique introduced in step-20.

  <li>Solve for the velocity $\mathbf u^{n+1}$ as also discussed in
  step-20.

  <li>Compute the term $F_3-\triangle t\; H \mathbf u^{n+1}$, using
  the just computed velocities.

  <li>Solve for the saturation $S^{n+1}$.
</ol>

In this scheme, we never actually build the matrix $H$, but rather
generate the right hand side of the third equation once we are ready
to do so.

In the program, we use a variable <code>solution</code> to store the
solution of the present time step. At the end of each step, we copy
its content, i.e. all three of its block components, into the variable
<code>old_solution</code> for use in the next time step.


<h3>Choosing a time step</h3>

A general rule of thumb in hyperbolic transport equations like the equation we
have to solve for the saturation equation is that if we use an explicit time
stepping scheme, then we should use a time step such that the distance that a
particle can travel within one time step is no larger than the diameter of a
single cell. In other words, here, we should choose
@f[
  \triangle t_{n+1} \le \frac h{|\mathbf{u}^{n+1}(\mathbf{x})|}.
@f]
Fortunately, we are in a position where we can do that: we only need the
time step when we want to assemble the right hand side of the saturation
equation, which is after we have already solved for $\mathbf{u}^{n+1}$. All we
therefore have to do after solving for the velocity is to loop over all
quadrature points in the domain and determine the maximal magnitude of the
velocity. We can then set the time step for the saturation equation to
@f[
  \triangle t_{n+1} = \frac {\min_K h_K}{\max_{\mathbf{x}}|\mathbf{u}^{n+1}(\mathbf{x})|}.
@f]

Why is it important to do this? If we don't, then we will end up with lots of
places where our saturation is larger than one or less than zero, as can
easily be verified. (Remember that the saturation corresponds to something
like the water fraction in the fluid mixture, and therefore must physically be
between 0 and 1.) On the other hand, if we choose our time step according to
the criterion listed above, this only happens very very infrequently &mdash;
in fact only once for the entire run of the program. However, to be on the
safe side, however, we run a function <code>project_back_saturation</code> at
the end of each time step, that simply projects the saturation back onto the
interval $[0,1]$, should it have gotten out of the physical range. This is
useful since the functions $\lambda(S)$ and $F(S)$ do not represent anything
physical outside this range, and we should not expect the program to do
anything useful once we have negative saturations or ones larger than one.

Note that we will have similar restrictions on the time step also in
step-23 and step-24 where we solve the time dependent
wave equation, another hyperbolic problem. We will also come back to the issue
of time step choice below in the section on <a href="#extensions">possible
extensions to this program</a>.


<h3>The test case</h3>

For simplicity, this program assumes that there is no source, $q=0$, and that
the heterogeneous porous medium is isotropic $\mathbf{K}(\mathbf{x}) =
k(\mathbf{x}) \mathbf{I}$. The first one of these is a realistic assumption in
oil reservoirs: apart from injection and production wells, there are usually
no mechanisms for fluids to appear or disappear out of the blue. The second
one is harder to justify: on a microscopic level, most rocks are isotropic,
because they consist of a network of interconnected pores. However, this
microscopic scale is out of the range of today's computer simulations, and we
have to be content with simulating things on the scale of meters. On that
scale, however, fluid transport typically happens through a network of cracks
in the rock, rather than through pores. However, cracks often result from
external stress fields in the rock layer (for example from tectonic faulting)
and the cracks are therefore roughly aligned. This leads to a situation where
the permeability is often orders of magnitude larger in the direction parallel
to the cracks than perpendicular to the cracks. A problem typically faces in
reservoir simulation, however, is that the modeler doesn't know the direction
of cracks because oil reservoirs are not accessible to easy inspection. The
only solution in that case is to assume an effective, isotropic permeability.

Whatever the matter, both of these restrictions, no sources and isotropy,
would be easy to lift with a few lines of code in the program.

Next, for simplicity, our numerical simulation will be done on the
unit cell $\Omega = [0,1]\times [0,1]$ for $t\in [0,T]$. Our initial
conditions are $S(\mathbf{x},0)=0$; in the oil reservoir picture, where $S$
would indicate the water saturation, this means that the reservoir contains
pure oil at the beginning. Note that we do not need any initial
conditions for pressure or velocity, since the equations do not contain time
derivatives of these variables. Finally, we impose the following pressure
boundary conditions:
@f[
  p(\mathbf{x},t)=1-x_1 \qquad \textrm{on}\ \partial\Omega.
@f]
Since the pressure and velocity solve a mixed form Poisson equation, the
imposed pressure leads to a resulting flow field for the velocity. On the
other hand, this flow field determines whether a piece of the boundary is of
inflow or outflow type, which is of relevance because we have to impose
boundary conditions for the saturation on the inflow part of the boundary,
@f[
  \Gamma_{in}(t) = \{\mathbf{x}\in\partial\Omega:
                     \mathbf{n} \cdot \mathbf{u}(\mathbf{x},t) < 0\}.
@f]
On this inflow boundary, we impose the following saturation values:
@f{eqnarray}
  S(\mathbf{x},t) = 1 & \textrm{on}\ \Gamma_{in}\cap\{x_1=0\},
  \\
  S(\mathbf{x},t) = 0 & \textrm{on}\ \Gamma_{in}\backslash \{x_1=0\}.
@f}
In other words, we have pure water entering the reservoir at the left, whereas
the other parts of the boundary are in contact with undisturbed parts of the
reservoir and whenever influx occurs on these boundaries, pure oil will enter.

In our simulations, we choose the total mobility as
@f[
  \lambda (S) = \frac{1.0}{\mu} S^2 +(1-S)^2
@f]
where we use $\mu=0.2$ for the viscosity. In addition, the fractional flow of
water is given by
@f[
  F(S)=\frac{S^2}{S^2+\mu (1-S)^2}
@f]

@note Coming back to this testcase in step-43 several years later revealed an
oddity in the setup of this testcase. To this end, consider that we can
rewrite the advection equation for the saturation as $S_{t} + (\mathbf{u}
F'(S)) \cdot \nabla S = 0$. Now, at the initial time, we have $S=0$, and with
the given choice of function $F(S)$, we happen to have $F'(0)=0$. In other
words, at $t=0$, the equation reduces to $S_t=0$ for all $\mathbf x$, so the
saturation is zero everywhere and it is going to stay zero everywhere! This is
despite the fact that $\mathbf u$ is not necessarily zero: the combined fluid
is moving, but we've chosen our partial flux $F(S)$ in such a way that
infinitesimal amounts of wetting fluid also only move at infinitesimal speeds
(i.e., they stick to the medium more than the non-wetting phase in which they
are embedded). That said, how can we square this with the knowledge that
wetting fluid is invading from the left, leading to the flow patterns seen in
the <a href="#Results">results section</a>? That's where we get into
mathematics: Equations like the transport equation we are considering here
have infinitely many solutions, but only one of them is physical: the one that
results from the so-called viscosity limit, called the <a
href="http://en.wikipedia.org/wiki/Viscosity_solution">viscosity
solution</a>. The thing is that with discontinuous elements we arrive at this
viscosity limit because using a numerical flux introduces a finite amount of
artificial viscosity into the numerical scheme. On the other hand, in step-43,
we use an artificial viscosity that is proportional to $\|\mathbf u F'(S)\|$
on every cell, which at the initial time is zero. Thus, the saturation there is
zero and remains zero; the solution we then get is <i>one</i> solution of the
advection equation, but the method does not converge to the viscosity solution
without further changes. We will therefore use a different initial condition in
that program.


Finally, to come back to the description of the testcase, we will show results
for computations with the two permeability
functions introduced at the end of the results section of @ref step_20
"step-20":
<ul>
  <li>A function that models a single, winding crack that snakes through the
  domain. In analogy to step-20, but taking care of the slightly
  different geometry we have here, we describe this by the following function:
  @f[
    k(\mathbf x)
    =
    \max \left\{ e^{-\left(\frac{x_2-\frac 12 - 0.1\sin(10x_1)}{0.1}\right)^2}, 0.01 \right\}.
  @f]
  Taking the maximum is necessary to ensure that the ratio between maximal and
  minimal permeability remains bounded. If we don't do that, permeabilities
  will span many orders of magnitude. On the other hand, the ratio between
  maximal and minimal permeability is a factor in the condition number of the
  Schur complement matrix, and if too large leads to problems for which our
  linear solvers will no longer converge properly.

  <li>A function that models a somewhat random medium. Here, we choose
  @f{eqnarray*}
    k(\mathbf x)
    &=&
    \min \left\{ \max \left\{ \sum_{i=1}^N \sigma_i(\mathbf{x}), 0.01 \right\}, 4\right\},
    \\
    \sigma_i(\mathbf x)
    &=&
    e^{-\left(\frac{|\mathbf{x}-\mathbf{x}_i|}{0.05}\right)^2},
  @f}
  where the centers $\mathbf{x}_i$ are $N$ randomly chosen locations inside
  the domain. This function models a domain in which there are $N$ centers of
  higher permeability (for example where rock has cracked) embedded in a
  matrix of more pristine, unperturbed background rock. Note that here we have
  cut off the permeability function both above and below to ensure a bounded
  condition number.
</ul>


examples/step-21/doc/results.dox
<h1>Results</h1>

The code as presented here does not actually compute the results
found on the web page. The reason is, that even on a decent
computer it runs more than a day. If you want to reproduce these
results, modify the end time of the DiscreteTime object to `250` within the
constructor of TwoPhaseFlowProblem.

If we run the program, we get the following kind of output:
@code
Number of active cells: 1024
Number of degrees of freedom: 4160 (2112+1024+1024)

Timestep 1
   22 CG Schur complement iterations for pressure.
   1 CG iterations for saturation.
   Now at t=0.0326742, dt=0.0326742.

Timestep 2
   17 CG Schur complement iterations for pressure.
   1 CG iterations for saturation.
   Now at t=0.0653816, dt=0.0327074.

Timestep 3
   17 CG Schur complement iterations for pressure.
   1 CG iterations for saturation.
   Now at t=0.0980651, dt=0.0326836.

...
@endcode
As we can see, the time step is pretty much constant right from the start,
which indicates that the velocities in the domain are not strongly dependent
on changes in saturation, although they certainly are through the factor
$\lambda(S)$ in the pressure equation.

Our second observation is that the number of CG iterations needed to solve the
pressure Schur complement equation drops from 22 to 17 between the first and
the second time step (in fact, it remains around 17 for the rest of the
computations). The reason is actually simple: Before we solve for the pressure
during a time step, we don't reset the <code>solution</code> variable to
zero. The pressure (and the other variables) therefore have the previous time
step's values at the time we get into the CG solver. Since the velocities and
pressures don't change very much as computations progress, the previous time
step's pressure is actually a good initial guess for this time step's
pressure. Consequently, the number of iterations we need once we have computed
the pressure once is significantly reduced.

The final observation concerns the number of iterations needed to solve for
the saturation, i.e. one. This shouldn't surprise us too much: the matrix we
have to solve with is the mass matrix. However, this is the mass matrix for
the $DGQ_0$ element of piecewise constants where no element couples with the
degrees of freedom on neighboring cells. The matrix is therefore a diagonal
one, and it is clear that we should be able to invert this matrix in a single
CG iteration.


With all this, here are a few movies that show how the saturation progresses
over time. First, this is for the single crack model, as implemented in the
<code>SingleCurvingCrack::KInverse</code> class:

<img src="https://www.dealii.org/images/steps/developer/step-21.centerline.gif" alt="">

As can be seen, the water rich fluid snakes its way mostly along the
high-permeability zone in the middle of the domain, whereas the rest of the
domain is mostly impermeable. This and the next movie are generated using
<code>n_refinement_steps=7</code>, leading to a $128\times 128$ mesh with some
16,000 cells and about 66,000 unknowns in total.


The second movie shows the saturation for the random medium model of class
<code>RandomMedium::KInverse</code>, where we have randomly distributed
centers of high permeability and fluid hops from one of these zones to
the next:

<img src="https://www.dealii.org/images/steps/developer/step-21.random2d.gif" alt="">


Finally, here is the same situation in three space dimensions, on a mesh with
<code>n_refinement_steps=5</code>, which produces a mesh of some 32,000 cells
and 167,000 degrees of freedom:

<img src="https://www.dealii.org/images/steps/developer/step-21.random3d.gif" alt="">

To repeat these computations, all you have to do is to change the line
@code
      TwoPhaseFlowProblem<2> two_phase_flow_problem(0);
@endcode
in the main function to
@code
      TwoPhaseFlowProblem<3> two_phase_flow_problem(0);
@endcode
The visualization uses a cloud technique, where the saturation is indicated by
colored but transparent clouds for each cell. This way, one can also see
somewhat what happens deep inside the domain. A different way of visualizing
would have been to show isosurfaces of the saturation evolving over
time. There are techniques to plot isosurfaces transparently, so that one can
see several of them at the same time like the layers of an onion.

So why don't we show such isosurfaces? The problem lies in the way isosurfaces
are computed: they require that the field to be visualized is continuous, so
that the isosurfaces can be generated by following contours at least across a
single cell. However, our saturation field is piecewise constant and
discontinuous. If we wanted to plot an isosurface for a saturation $S=0.5$,
chances would be that there is no single point in the domain where that
saturation is actually attained. If we had to define isosurfaces in that
context at all, we would have to take the interfaces between cells, where one
of the two adjacent cells has a saturation greater than and the other cell a
saturation less than 0.5. However, it appears that most visualization programs
are not equipped to do this kind of transformation.


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

There are a number of areas where this program can be improved. Three of them
are listed below. All of them are, in fact, addressed in a tutorial program
that forms the continuation of the current one: step-43.


<h4>Solvers</h4>

At present, the program is not particularly fast: the 2d random medium
computation took about a day for the 1,000 or so time steps. The corresponding
3d computation took almost two days for 800 time steps. The reason why it
isn't faster than this is twofold. First, we rebuild the entire matrix in
every time step, although some parts such as the $B$, $B^T$, and $M^S$ blocks
never change.

Second, we could do a lot better with the solver and
preconditioners. Presently, we solve the Schur complement $B^TM^u(S)^{-1}B$
with a CG method, using $[B^T (\textrm{diag}(M^u(S)))^{-1} B]^{-1}$ as a
preconditioner. Applying this preconditioner is expensive, since it involves
solving a linear system each time. This may have been appropriate for @ref
step_20 "step-20", where we have to solve the entire problem only
once. However, here we have to solve it hundreds of times, and in such cases
it is worth considering a preconditioner that is more expensive to set up the
first time, but cheaper to apply later on.

One possibility would be to realize that the matrix we use as preconditioner,
$B^T (\textrm{diag}(M^u(S)))^{-1} B$ is still sparse, and symmetric on top of
that. If one looks at the flow field evolve over time, we also see that while
$S$ changes significantly over time, the pressure hardly does and consequently
$B^T (\textrm{diag}(M^u(S)))^{-1} B \approx B^T (\textrm{diag}(M^u(S^0)))^{-1}
B$. In other words, the matrix for the first time step should be a good
preconditioner also for all later time steps.  With a bit of
back-and-forthing, it isn't hard to actually get a representation of it as a
SparseMatrix object. We could then hand it off to the SparseMIC class to form
a sparse incomplete Cholesky decomposition. To form this decomposition is
expensive, but we have to do it only once in the first time step, and can then
use it as a cheap preconditioner in the future. We could do better even by
using the SparseDirectUMFPACK class that produces not only an incomplete, but
a complete decomposition of the matrix, which should yield an even better
preconditioner.

Finally, why use the approximation $B^T (\textrm{diag}(M^u(S)))^{-1} B$ to
precondition $B^T M^u(S)^{-1} B$? The latter matrix, after all, is the mixed
form of the Laplace operator on the pressure space, for which we use linear
elements. We could therefore build a separate matrix $A^p$ on the side that
directly corresponds to the non-mixed formulation of the Laplacian, for
example using the bilinear form $(\mathbf{K}\lambda(S^n) \nabla
\varphi_i,\nabla\varphi_j)$. We could then form an incomplete or complete
decomposition of this non-mixed matrix and use it as a preconditioner of the
mixed form.

Using such techniques, it can reasonably be expected that the solution process
will be faster by at least an order of magnitude.


<h4>Time stepping</h4>

In the introduction we have identified the time step restriction
@f[
  \triangle t_{n+1} \le \frac h{|\mathbf{u}^{n+1}(\mathbf{x})|}
@f]
that has to hold globally, i.e. for all $\mathbf x$. After discretization, we
satisfy it by choosing
@f[
  \triangle t_{n+1} = \frac {\min_K h_K}{\max_{\mathbf{x}}|\mathbf{u}^{n+1}(\mathbf{x})|}.
@f]

This restriction on the time step is somewhat annoying: the finer we make the
mesh the smaller the time step; in other words, we get punished twice: each
time step is more expensive to solve and we have to do more time steps.

This is particularly annoying since the majority of the additional work is
spent solving the implicit part of the equations, i.e. the pressure-velocity
system, whereas it is the hyperbolic transport equation for the saturation
that imposes the time step restriction.

To avoid this bottleneck, people have invented a number of approaches. For
example, they may only re-compute the pressure-velocity field every few time
steps (or, if you want, use different time step sizes for the
pressure/velocity and saturation equations). This keeps the time step
restriction on the cheap explicit part while it makes the solution of the
implicit part less frequent. Experiments in this direction are
certainly worthwhile; one starting point for such an approach is the paper by
Zhangxin Chen, Guanren Huan and Baoyan Li: <i>An improved IMPES method for
two-phase flow in porous media</i>, Transport in Porous Media, 54 (2004),
pp. 361&mdash;376. There are certainly many other papers on this topic as well, but
this one happened to land on our desk a while back.



<h4>Adaptivity</h4>

Adaptivity would also clearly help. Looking at the movies, one clearly sees
that most of the action is confined to a relatively small part of the domain
(this particularly obvious for the saturation, but also holds for the
velocities and pressures). Adaptivity can therefore be expected to keep the
necessary number of degrees of freedom low, or alternatively increase the
accuracy.

On the other hand, adaptivity for time dependent problems is not a trivial
thing: we would have to change the mesh every few time steps, and we would
have to transport our present solution to the next mesh every time we change
it (something that the SolutionTransfer class can help with). These are not
insurmountable obstacles, but they do require some additional coding and more
than we felt comfortable was worth packing into this tutorial program.


examples/step-22/doc/intro.dox
<br>

<i>This program was contributed by Martin Kronbichler and Wolfgang
Bangerth.
<br>
This material is based upon work partly supported by the National
Science Foundation under Award No. EAR-0426271 and The California Institute of
Technology. Any opinions, findings, and conclusions or recommendations
expressed in this publication are those of the author and do not
necessarily reflect the views of the National Science Foundation or of The
California Institute of Technology.
</i>



<a name="Intro"></a>
<h1>Introduction</h1>

This program deals with the Stokes system of equations which reads as
follows in non-dimensionalized form:
@f{eqnarray*}
  -2\; \textrm{div}\; \varepsilon(\textbf{u}) + \nabla p &=& \textbf{f},
  \\
  -\textrm{div}\; \textbf{u} &=& 0,
@f}
where $\textbf u$ denotes the velocity of a fluid, $p$ is its
pressure, $\textbf f$ are external forces, and
$\varepsilon(\textbf{u})= \nabla^s{\textbf{u}}= \frac 12 \left[
(\nabla \textbf{u}) + (\nabla \textbf{u})^T\right]$  is the
rank-2 tensor of symmetrized gradients; a component-wise definition
of it is $\varepsilon(\textbf{u})_{ij}=\frac
12\left(\frac{\partial u_i}{\partial x_j} + \frac{\partial u_j}{\partial x_i}\right)$.

The Stokes equations describe the steady-state motion of a
slow-moving, viscous fluid such as honey, rocks in the earth mantle,
or other cases where inertia does not play a significant role. If a
fluid is moving fast enough that inertia forces are significant
compared to viscous friction, the Stokes equations are no longer
valid; taking into account inertia effects then leads to the
nonlinear Navier-Stokes equations. However, in this tutorial program,
we will focus on the simpler Stokes system.

Note that when deriving the more general compressible Navier-Stokes equations,
the diffusion is modeled as the divergence of the stress tensor
@f{eqnarray*}
  \tau = - \mu (2\varepsilon(\textbf{u}) - \frac{2}{3}\nabla \cdot \textbf{u} I),
@f}
where $\mu$ is the viscosity of the fluid. With the assumption of $\mu=1$
(assume constant viscosity and non-dimensionalize the equation by dividing out
$\mu$) and assuming incompressibility ($\textrm{div}\; \textbf{u}=0$), we
arrive at the formulation from above:
@f{eqnarray*}
  \textrm{div}\; \tau = -2\textrm{div}\;\varepsilon(\textbf{u}).
@f}
A different formulation uses the Laplace operator ($-\triangle \textbf{u}$)
instead of the symmetrized gradient. A big difference here is that the
different components of the velocity do not couple. If you assume additional
regularity of the solution $\textbf{u}$ (second partial derivatives exist and
are continuous), the formulations are equivalent:
@f{eqnarray*}
  \textrm{div}\; \tau
  = -2\textrm{div}\;\varepsilon(\textbf{u})
  = -\triangle \textbf{u} + \nabla \cdot (\nabla\textbf{u})^T
  = -\triangle \textbf{u}.
@f}
This is because the $i$th entry of  $\nabla \cdot (\nabla\textbf{u})^T$ is given by:
@f{eqnarray*}
[\nabla \cdot (\nabla\textbf{u})^T]_i
= \sum_j \frac{\partial}{\partial x_j} [(\nabla\textbf{u})^T]_{i,j}
= \sum_j \frac{\partial}{\partial x_j} [(\nabla\textbf{u})]_{j,i}
= \sum_j \frac{\partial}{\partial x_j} \frac{\partial}{\partial x_i} \textbf{u}_j
= \sum_j \frac{\partial}{\partial x_i} \frac{\partial}{\partial x_j} \textbf{u}_j
= \frac{\partial}{\partial x_i} \textrm{div}\; \textbf{u}
= 0.
@f}
If you can not assume the above mentioned regularity, or if your viscosity is
not a constant, the equivalence no longer holds. Therefore, we decided to
stick with the more physically accurate symmetric tensor formulation in this
tutorial.


To be well-posed, we will have to add boundary conditions to the
equations. What boundary conditions are readily possible here will
become clear once we discuss the weak form of the equations.

The equations covered here fall into the class of vector-valued problems. A
toplevel overview of this topic can be found in the @ref vector_valued module.


<h3>Weak form</h3>

The weak form of the equations is obtained by writing it in vector
form as
@f{eqnarray*}
  \begin{pmatrix}
    {-2\; \textrm{div}\; \varepsilon(\textbf{u}) + \nabla p}
    \\
    {-\textrm{div}\; \textbf{u}}
  \end{pmatrix}
  =
  \begin{pmatrix}
  {\textbf{f}}
  \\
  0
  \end{pmatrix},
@f}
forming the dot product from the left with a vector-valued test
function $\phi = \begin{pmatrix}\textbf{v} \\ q\end{pmatrix}$ and integrating
over the domain $\Omega$, yielding the following set of equations:
@f{eqnarray*}
  (\mathrm v,
   -2\; \textrm{div}\; \varepsilon(\textbf{u}) + \nabla p)_{\Omega}
  -
  (q,\textrm{div}\; \textbf{u})_{\Omega}
  =
  (\textbf{v}, \textbf{f})_\Omega,
@f}
which has to hold for all test functions $\phi = \begin{pmatrix}\textbf{v}
\\ q\end{pmatrix}$.

A generally good rule of thumb is that if one <i>can</i> reduce how
many derivatives are taken on any variable in the formulation, then
one <i>should</i> in fact do that using integration by parts. (This is
motivated by the theory of <a
href="https://en.wikipedia.org/wiki/Partial_differential_equation">partial
differential equations</a>, and in particular the difference between
strong and <a href="https://en.wikipedia.org/wiki/Weak_solution">weak
solutions</a>.) We have already done that for the Laplace equation,
where we have integrated the second derivative by parts to obtain the
weak formulation that has only one derivative on both test and trial
function.

In the current context, we integrate by parts the second term:
@f{eqnarray*}
  (\textbf{v}, -2\; \textrm{div}\; \varepsilon(\textbf{u}))_{\Omega}
  - (\textrm{div}\; \textbf{v}, p)_{\Omega}
  + (\textbf{n}\cdot\textbf{v}, p)_{\partial\Omega}
  -
  (q,\textrm{div}\; \textbf{u})_{\Omega}
  =
  (\textbf{v}, \textbf{f})_\Omega.
@f}
Likewise, we integrate by parts the first term to obtain
@f{eqnarray*}
  (\nabla \textbf{v}, 2\; \varepsilon(\textbf{u}))_{\Omega}
  -
  (\textbf{n} \otimes \textbf{v}, 2\; \varepsilon(\textbf{u}))_{\partial\Omega}
  - (\textrm{div}\; \textbf{v}, p)_{\Omega}
  + (\textbf{n}\cdot\textbf{v}, p)_{\partial\Omega}
  -
  (q,\textrm{div}\; \textbf{u})_{\Omega}
  =
  (\textbf{v}, \textbf{f})_\Omega,
@f}
where the scalar product between two tensor-valued quantities is here
defined as
@f{eqnarray*}
  (\nabla \textbf{v}, 2\; \varepsilon(\textbf{u}))_{\Omega}
  =
  2 \int_\Omega \sum_{i,j=1}^d \frac{\partial v_j}{\partial x_i}
  \varepsilon(\textbf{u})_{ij} \ dx.
@f}
Using this, we have now reduced the requirements on our variables to
first derivatives for $\mathbf u,\mathbf v$ and no derivatives at all
for $p,q$.

Because the scalar product between a general tensor like
$\nabla\textbf{v}$ and a symmetric tensor like
$\varepsilon(\textbf{u})$ equals the scalar product between the
symmetrized forms of the two, we can also write the bilinear form
above as follows:
@f{eqnarray*}
  (\varepsilon(\textbf{v}), 2\; \varepsilon(\textbf{u}))_{\Omega}
  -
  (\textbf{n} \otimes \textbf{v}, 2\; \varepsilon(\textbf{u}))_{\partial\Omega}
  - (\textrm{div}\; \textbf{v}, p)_{\Omega}
  + (\textbf{n}\cdot\textbf{v}, p)_{\partial\Omega}
  -
  (q,\textrm{div}\; \textbf{u})_{\Omega}
  =
  (\textbf{v}, \textbf{f})_\Omega,
@f}
We will deal with the boundary terms in the next section, but it is already
clear from the domain terms
@f{eqnarray*}
  (\varepsilon(\textbf{v}), 2\; \varepsilon(\textbf{u}))_{\Omega}
  - (\textrm{div}\; \textbf{v}, p)_{\Omega}
  -
  (q,\textrm{div}\; \textbf{u})_{\Omega}
@f}
of the bilinear form that the Stokes equations yield a symmetric bilinear
form, and consequently a symmetric (if indefinite) system matrix.


<h3>Boundary conditions</h3>

@dealiiVideoLecture{21.5}
(@dealiiVideoLectureSeeAlso{21.55,21.6,21.65})

The weak form just derived immediately presents us with different
possibilities for imposing boundary conditions:
<ol>
<li>Dirichlet velocity boundary conditions: On a part
    $\Gamma_D\subset\partial\Omega$ we may impose Dirichlet conditions
    on the velocity $\textbf u$:

    @f{eqnarray*}
        \textbf u = \textbf g_D \qquad\qquad \textrm{on}\ \Gamma_D.
    @f}
    Because test functions $\textbf{v}$ come from the tangent space of
    the solution variable, we have that $\textbf{v}=0$ on $\Gamma_D$
    and consequently that
    @f{eqnarray*}
      -(\textbf{n} \otimes \mathrm
        v, 2\; \varepsilon(\textbf{u}))_{\Gamma_D}
      +
      (\textbf{n}\cdot\textbf{v}, p)_{\Gamma_D}
      = 0.
    @f}
    In other words, as usual, strongly imposed boundary values do not
    appear in the weak form.

    It is noteworthy that if we impose Dirichlet boundary values on the entire
    boundary, then the pressure is only determined up to a constant. An
    algorithmic realization of that would use similar tools as have been seen in
    step-11.

<li>Neumann-type or natural boundary conditions: On the rest of the boundary
    $\Gamma_N=\partial\Omega\backslash\Gamma_D$, let us re-write the
    boundary terms as follows:
    @f{eqnarray*}
      -(\textbf{n} \otimes \mathrm
        v, 2\; \varepsilon(\textbf{u}))_{\Gamma_N}
      +
      (\textbf{n}\cdot\textbf{v}, p)_{\Gamma_N}
      &=&
      \sum_{i,j=1}^d
      -(n_i v_j, 2\; \varepsilon(\textbf{u})_{ij})_{\Gamma_N}
      +
      \sum_{i=1}^d
      (n_i v_i, p)_{\Gamma_N}
      \\
      &=&
      \sum_{i,j=1}^d
      -(n_i v_j, 2\; \varepsilon(\textbf{u})_{ij})_{\Gamma_N}
      +
      \sum_{i,j=1}^d
      (n_i v_j, p \delta_{ij})_{\Gamma_N}
      \\
      &=&
      \sum_{i,j=1}^d
      (n_i v_j,p \delta_{ij} - 2\; \varepsilon(\textbf{u})_{ij})_{\Gamma_N}
      \\
      &=&
      (\textbf{n} \otimes \textbf{v},
      p \textbf{I} - 2\; \varepsilon(\textbf{u}))_{\Gamma_N}.
      \\
      &=&
      (\textbf{v},
       \textbf{n}\cdot [p \textbf{I} - 2\; \varepsilon(\textbf{u})])_{\Gamma_N}.
    @f}
    In other words, on the Neumann part of the boundary we can
    prescribe values for the total stress:
    @f{eqnarray*}
      \textbf{n}\cdot [p \textbf{I} - 2\; \varepsilon(\textbf{u})]
      =
      \textbf g_N \qquad\qquad \textrm{on}\ \Gamma_N.
    @f}
    If the boundary is subdivided into Dirichlet and Neumann parts
    $\Gamma_D,\Gamma_N$, this then leads to the following weak form:
    @f{eqnarray*}
      (\varepsilon(\textbf{v}), 2\; \varepsilon(\textbf{u}))_{\Omega}
      - (\textrm{div}\; \textbf{v}, p)_{\Omega}
      -
      (q,\textrm{div}\; \textbf{u})_{\Omega}
      =
      (\textbf{v}, \textbf{f})_\Omega
      -
      (\textbf{v}, \textbf g_N)_{\Gamma_N}.
    @f}


<li>Robin-type boundary conditions: Robin boundary conditions are a mixture of
    Dirichlet and Neumann boundary conditions. They would read
    @f{eqnarray*}
      \textbf{n}\cdot [p \textbf{I} - 2\; \varepsilon(\textbf{u})]
      =
      \textbf S \textbf u \qquad\qquad \textrm{on}\ \Gamma_R,
    @f}
    with a rank-2 tensor (matrix) $\textbf S$. The associated weak form is
    @f{eqnarray*}
      (\varepsilon(\textbf{v}), 2\; \varepsilon(\textbf{u}))_{\Omega}
      - (\textrm{div}\; \textbf{v}, p)_{\Omega}
      -
      (q,\textrm{div}\; \textbf{u})_{\Omega}
      +
      (\textbf S \textbf u, \textbf{v})_{\Gamma_R}
      =
      (\textbf{v}, \textbf{f})_\Omega.
    @f}

<li>Partial boundary conditions: It is possible to combine Dirichlet and
    Neumann boundary conditions by only enforcing each of them for certain
    components of the velocity. For example, one way to impose artificial
    boundary conditions is to require that the flow is perpendicular to the
    boundary, i.e. the tangential component $\textbf u_{\textbf t}=(\textbf
    1-\textbf n\otimes\textbf n)\textbf u$ be zero, thereby constraining
    <code>dim</code>-1 components of the velocity. The remaining component can
    be constrained by requiring that the normal component of the normal
    stress be zero, yielding the following set of boundary conditions:
    @f{eqnarray*}
      \textbf u_{\textbf t} &=& 0,
      \\
      \textbf n \cdot \left(\textbf{n}\cdot [p \textbf{I} - 2\;
      \varepsilon(\textbf{u})] \right)
      &=&
      0.
    @f}

    An alternative to this is when one wants the flow to be <i>parallel</i>
    rather than perpendicular to the boundary (in deal.II, the
    VectorTools::compute_no_normal_flux_constraints function can do this for
    you). This is frequently the case for problems with a free boundary
    (e.g. at the surface of a river or lake if vertical forces of the flow are
    not large enough to actually deform the surface), or if no significant
    friction is exerted by the boundary on the fluid (e.g. at the interface
    between earth mantle and earth core where two fluids meet that are
    stratified by different densities but that both have small enough
    viscosities to not introduce much tangential stress on each other).
    In formulas, this means that
    @f{eqnarray*}
      \textbf{n}\cdot\textbf u &=& 0,
      \\
      (\textbf 1-\textbf n\otimes\textbf n)
      \left(\textbf{n}\cdot [p \textbf{I} - 2\;
      \varepsilon(\textbf{u})] \right)
      &=&
      0,
    @f}
    the first condition (which needs to be imposed strongly) fixing a single
    component of the velocity, with the second (which would be enforced in the
    weak form) fixing the remaining two components.
</ol>

Despite this wealth of possibilities, we will only use Dirichlet and
(homogeneous) Neumann boundary conditions in this tutorial program.


<h3>Discretization</h3>

As developed above, the weak form of the equations with Dirichlet and Neumann
boundary conditions on $\Gamma_D$ and $\Gamma_N$ reads like this: find
$\textbf u\in \textbf V_g = \{\varphi \in H^1(\Omega)^d: \varphi_{\Gamma_D}=\textbf
g_D\}, p\in Q=L^2(\Omega)$ so that
@f{eqnarray*}
  (\varepsilon(\textbf{v}), 2\; \varepsilon(\textbf{u}))_{\Omega}
  - (\textrm{div}\; \textbf{v}, p)_{\Omega}
  -
  (q,\textrm{div}\; \textbf{u})_{\Omega}
  =
  (\textbf{v}, \textbf{f})_\Omega
  -
  (\textbf{v}, \textbf g_N)_{\Gamma_N}
@f}
for all test functions
$\textbf{v}\in \textbf V_0 = \{\varphi \in H^1(\Omega)^d: \varphi_{\Gamma_D}=0\},q\in
Q$.

These equations represent a symmetric <a
href="https://en.wikipedia.org/wiki/Ladyzhenskaya%E2%80%93Babu%C5%A1ka%E2%80%93Brezzi_condition">saddle
point problem</a>. It is well known
that then a solution only exists if the function spaces in which we search for
a solution have to satisfy certain conditions, typically referred to as the
Babuska-Brezzi or Ladyzhenskaya-Babuska-Brezzi (LBB) conditions. The continuous
function spaces above satisfy these. However, when we discretize the equations by
replacing the continuous variables and test functions by finite element
functions in finite dimensional spaces $\textbf V_{g,h}\subset \textbf V_g,
Q_h\subset Q$, we have to make sure that $\textbf V_h,Q_h$ also satisfy the LBB
conditions. This is similar to what we had to do in step-20.

For the Stokes equations, there are a number of possible choices to ensure
that the finite element spaces are compatible with the LBB condition. A simple
and accurate choice that we will use here is $\textbf u_h\in Q_{p+1}^d,
p_h\in Q_p$, i.e. use elements one order higher for the velocities than for the
pressures.

This then leads to the following discrete problem: find $\textbf u_h,p_h$ so
that
@f{eqnarray*}
  (\varepsilon(\textbf{v}_h), 2\; \varepsilon(\textbf u_h))_{\Omega}
  - (\textrm{div}\; \textbf{v}_h, p_h)_{\Omega}
  -
  (q_h,\textrm{div}\; \textbf{u}_h)_{\Omega}
  =
  (\textbf{v}_h, \textbf{f})_\Omega
  -
  (\textbf{v}_h, \textbf g_N)_{\Gamma_N}
@f}
for all test functions $\textbf{v}_h, q_h$. Assembling the linear system
associated with this problem follows the same lines used in @ref step_20
"step-20", step-21, and explained in detail in the @ref
vector_valued module.



<h3>Linear solver and preconditioning issues</h3>

The weak form of the discrete equations naturally leads to the following
linear system for the nodal values of the velocity and pressure fields:
@f{eqnarray*}
  \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
  \left(\begin{array}{c}
    U \\ P
  \end{array}\right)
  =
  \left(\begin{array}{c}
    F \\ G
  \end{array}\right),
@f}
Like in step-20 and step-21, we will solve this
system of equations by forming the Schur complement, i.e. we will first find
the solution $P$ of
@f{eqnarray*}
  BA^{-1}B^T P &=& BA^{-1} F - G, \\
@f}
and then
@f{eqnarray*}
  AU &=& F - B^TP.
@f}
The way we do this is pretty much exactly like we did in these previous
tutorial programs, i.e. we use the same classes <code>SchurComplement</code>
and <code>InverseMatrix</code> again. There are two significant differences,
however:

<ol>
<li>
First, in the mixed Laplace equation we had to deal with the question of how
to precondition the Schur complement $B^TM^{-1}B$, which was spectrally
equivalent to the Laplace operator on the pressure space (because $B$
represents the gradient operator, $B^T$ its adjoint $-\textrm{div}$, and $M$
the identity (up to the material parameter $K^{-1}$), so $B^TM^{-1}B$ is
something like $-\textrm{div} \mathbf 1 \nabla = -\Delta$). Consequently, the
matrix is badly conditioned for small mesh sizes and we had to come up with an
elaborate preconditioning scheme for the Schur complement.

<li>
Second, every time we multiplied with $B^TM^{-1}B$ we had to solve with the
mass matrix $M$. This wasn't particularly difficult, however, since the mass
matrix is always well conditioned and so simple to invert using CG and a
little bit of preconditioning.
</ol>
In other words, preconditioning the inner solver for $M$ was simple whereas
preconditioning the outer solver for $B^TM^{-1}B$ was complicated.

Here, the situation is pretty much exactly the opposite. The difference stems
from the fact that the matrix at the heart of the Schur complement does not
stem from the identity operator but from a variant of the Laplace operator,
$-\textrm{div} \nabla^s$ (where $\nabla^s$ is the symmetric gradient)
acting on a vector field. In the investigation of this issue
we largely follow the paper D. Silvester and A. Wathen:
"Fast iterative solution of stabilised Stokes systems part II. Using
general block preconditioners." (SIAM J. Numer. Anal., 31 (1994),
pp. 1352-1367), which is available online <a
href="http://siamdl.aip.org/getabs/servlet/GetabsServlet?prog=normal&id=SJNAAM000031000005001352000001&idtype=cvips&gifs=Yes" target="_top">here</a>.
Principally, the difference in the matrix at the heart of the Schur
complement has two consequences:

<ol>
<li>
First, it makes the outer preconditioner simple: the Schur complement
corresponds to the operator $-\textrm{div} (-\textrm{div} \nabla^s)^{-1}
\nabla$ on the pressure space; forgetting about the fact that we deal with
symmetric gradients instead of the regular one, the Schur complement is
something like $-\textrm{div} (-\textrm{div} \nabla)^{-1} \nabla =
-\textrm{div} (-\Delta)^{-1} \nabla$, which, even if not mathematically
entirely concise, is spectrally equivalent to the identity operator (a
heuristic argument would be to commute the operators into
$-\textrm{div}(-\Delta)^{-1} \nabla = -\textrm{div}\nabla(-\Delta)^{-1} =
-\Delta(-\Delta)^{-1} = \mathbf 1$). It turns out that it isn't easy to solve
this Schur complement in a straightforward way with the CG method:
using no preconditioner, the condition number of the Schur complement matrix
depends on the size ratios of the largest to the smallest cells, and one still
needs on the order of 50-100 CG iterations. However, there is a simple cure:
precondition with the mass matrix on the pressure space and we get down to a
number between 5-15 CG iterations, pretty much independently of the structure
of the mesh (take a look at the <a href="#Results">results section</a> of this
program to see that indeed the number of CG iterations does not change as we
refine the mesh).

So all we need in addition to what we already have is the mass matrix on the
pressure variables and we will store it in a separate object.



<li>
While the outer preconditioner has become simpler compared to the
mixed Laplace case discussed in step-20, the issue of
the inner solver has become more complicated. In the mixed Laplace
discretization, the Schur complement has the form $B^TM^{-1}B$. Thus,
every time we multiplied with the Schur complement, we had to solve a
linear system $M_uz=y$; this isn't too complicated there, however,
since the mass matrix $M_u$ on the pressure space is well-conditioned.


On the other hand, for the Stokes equation we consider here, the Schur
complement is $BA^{-1}B^T$ where the matrix $A$ is related to the
Laplace operator (it is, in fact, the matrix corresponding to the
bilinear form $(\nabla^s \varphi_i, \nabla^s\varphi_j)$). Thus,
solving with $A$ is a lot more complicated: the matrix is badly
conditioned and we know that we need many iterations unless we have a
very good preconditioner. What is worse, we have to solve with $A$
every time we multiply with the Schur complement, which is 5-15 times
using the preconditioner described above.

Because we have to solve with $A$ several times, it pays off to spend
a bit more time once to create a good preconditioner for this
matrix. So here's what we're going to do: if in 2d, we use the
ultimate preconditioner, namely a direct sparse LU decomposition of
the matrix. This is implemented using the SparseDirectUMFPACK class
that uses the UMFPACK direct solver to compute the decomposition. To
use it, you will have to build deal.II with UMFPACK support (which is the
default); see the <a href="../../readme.html#optional-software">ReadMe file</a>
for instructions. With this, the inner solver converges in one iteration.

In 2d, we can do this sort of thing because even reasonably large problems
rarely have more than a few 100,000 unknowns with relatively few nonzero
entries per row. Furthermore, the bandwidth of matrices in 2d is ${\cal
O}(\sqrt{N})$ and therefore moderate. For such matrices, sparse factors can be
computed in a matter of a few seconds. (As a point of reference, computing the
sparse factors of a matrix of size $N$ and bandwidth $B$ takes ${\cal
O}(NB^2)$ operations. In 2d, this is ${\cal O}(N^2)$; though this is a higher
complexity than, for example, assembling the linear system which takes ${\cal
O}(N)$, the constant for computing the decomposition is so small that it
doesn't become the dominating factor in the entire program until we get to
very large %numbers of unknowns in the high 100,000s or more.)

The situation changes in 3d, because there we quickly have many more
unknowns and the bandwidth of matrices (which determines the number of
nonzero entries in sparse LU factors) is ${\cal O}(N^{2/3})$, and there
are many more entries per row as well. This makes using a sparse
direct solver such as UMFPACK inefficient: only for problem sizes of a
few 10,000 to maybe 100,000 unknowns can a sparse decomposition be
computed using reasonable time and memory resources.

What we do in that case is to use an incomplete LU decomposition (ILU) as a
preconditioner, rather than actually computing complete LU factors. As it so
happens, deal.II has a class that does this: SparseILU. Computing the ILU
takes a time that only depends on the number of nonzero entries in the sparse
matrix (or that we are willing to fill in the LU factors, if these should be
more than the ones in the matrix), but is independent of the bandwidth of the
matrix. It is therefore an operation that can efficiently also be computed in
3d. On the other hand, an incomplete LU decomposition, by definition, does not
represent an exact inverse of the matrix $A$. Consequently, preconditioning
with the ILU will still require more than one iteration, unlike
preconditioning with the sparse direct solver. The inner solver will therefore
take more time when multiplying with the Schur complement: an unavoidable
trade-off.
</ol>

In the program below, we will make use of the fact that the SparseILU and
SparseDirectUMFPACK classes have a very similar interface and can be used
interchangeably. All that we need is a switch class that, depending on the
dimension, provides a type that is either of the two classes mentioned
above. This is how we do that:
@code
template <int dim>
struct InnerPreconditioner;

template <>
struct InnerPreconditioner<2>
{
  using type = SparseDirectUMFPACK;
};

template <>
struct InnerPreconditioner<3>
{
  using type = SparseILU<double>;
};
@endcode

From here on, we can refer to the type <code>typename
InnerPreconditioner@<dim@>::%type</code> and automatically get the correct
preconditioner class. Because of the similarity of the interfaces of the two
classes, we will be able to use them interchangeably using the same syntax in
all places.


<h4> Is this how one should solve the Stokes equations? </h4>

The discussions above showed *one* way in which the linear system that
results from the Stokes equations can be solved, and because the
tutorial programs are teaching tools that makes sense. But is this the
way this system of equations *should* be solved?

The answer to this is no. The primary bottleneck with the approach,
already identified above, is that we have to repeatedly solve linear
systems with $A$ inside the Schur complement, and because we don't
have a good preconditioner for the Schur complement, these solves just
have to happen too often. A better approach is to use a block
decomposition, which is based on an observation of Silvester and
Wathen @cite SW94 and explained in much greater detail in
@cite elman2005 . An implementation of this alternative approach is
discussed below, in the section on a <a href="#block-schur">block Schur
complementation preconditioner</a> in the results section of this program.


<h4> A note on the structure of the linear system </h4>

Above, we have claimed that the linear system has the form
@f{eqnarray*}
  \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
  \left(\begin{array}{cc}
    U \\ P
  \end{array}\right)
  =
  \left(\begin{array}{cc}
    F \\ G
  \end{array}\right),
@f}
i.e., in particular that there is a zero block at the bottom right of the
matrix. This then allowed us to write the Schur complement as
$S=B A^{-1} B^T$. But this is not quite correct.

Think of what would happen if there are constraints on some
pressure variables (see the
@ref constraints "Constraints on degrees of freedom" documentation
module), for example because we use adaptively
refined meshes and continuous pressure finite elements so that there
are hanging nodes. Another cause for such constraints are Dirichlet
boundary conditions on the pressure. Then the AffineConstraints
class, upon copying the local contributions to the matrix into the
global linear system will zero out rows and columns corresponding
to constrained degrees of freedom and put a positive entry on
the diagonal. (You can think of this entry as being one for
simplicity, though in reality it is a value of the same order
of magnitude as the other matrix entries.) In other words,
the bottom right block is really not empty at all: It has
a few entries on the diagonal, one for each constrained
pressure degree of freedom, and a correct description
of the linear system we have to solve is that it has the
form
@f{eqnarray*}
  \left(\begin{array}{cc}
    A & B^T \\ B & D_c
  \end{array}\right)
  \left(\begin{array}{cc}
    U \\ P
  \end{array}\right)
  =
  \left(\begin{array}{cc}
    F \\ G
  \end{array}\right),
@f}
where $D_c$ is the zero matrix with the exception of the
positive diagonal entries for the constrained degrees of
freedom. The correct Schur complement would then in fact
be the matrix $S = B A^{-1} B^T - D_c $ instead of the one
stated above.

Thinking about this makes us, first, realize that the
resulting Schur complement is now indefinite because
$B A^{-1} B^T$ is symmetric and positive definite whereas
$D_c$ is a positive semidefinite, and subtracting the latter
from the former may no longer be positive definite. This
is annoying because we could no longer employ the Conjugate
Gradient method on this true Schur complement. That said, we could
fix the issue in AffineConstraints::distribute_local_to_global() by
simply putting *negative* values onto the diagonal for the constrained
pressure variables -- because we really only put something nonzero
to ensure that the resulting matrix is not singular; we really didn't
care whether that entry is positive or negative. So if the entries
on the diagonal of $D_c$ were negative, then $S$ would again be a
symmetric and positive definite matrix.

But, secondly, the code below doesn't actually do any of that: It
happily solves the linear system with the wrong Schur complement
$S = B A^{-1} B^T$ that just ignores the issue altogether. Why
does this even work? To understand why this is so, recall that
when writing local contributions into the global matrix,
AffineConstraints::distribute_local_to_global() zeros out the
rows and columns that correspond to constrained degrees of freedom.
This means that $B$ has some zero rows, and $B^T$ zero columns.
As a consequence, if one were to multiply out what the entries
of $S$ are, one would realize that it has zero rows and columns
for all constrained pressure degrees of freedom, including a
zero on the diagonal. The nonzero entries of $D_c$ would fit
into exactly those zero diagonal locations, and ensure that $S$
is invertible. Not doing so, strictly speaking, means that $S$
remains singular: It is symmetric and positive definite on the
subset of non-constrained pressure degrees of freedom, and
simply the zero matrix on the constrained pressures. Why
does the Conjugate Gradient method work for this matrix?
Because AffineConstraints::distribute_local_to_global()
also makes sure that the right hand side entries that
correspond to these zero rows of the matrix are *also*
zero, i.e., the right hand side is compatible.

What this means is that whatever the values of the solution
vector for these constrained pressure degrees of freedom,
these rows will always have a zero residual and, if one
were to consider what the CG algorithm does internally, just
never produce any updates to the solution vector. In other
words, the CG algorithm just *ignores* these rows, despite the
fact that the matrix is singular. This only works because these
degrees of freedom are entirely decoupled from the rest of the
linear system (because the entire row and corresponding column
are zero). At the end of the solution process, the constrained
pressure values in the solution vector therefore remain exactly
as they were when we started the call to the solver; they are
finally overwritten with their correct values when we call
AffineConstraints::distribute() after the CG solver is done.

The upshot of this discussion is that the assumption that the
bottom right block of the big matrix is zero is a bit
simplified, but that just going with it does not actually lead
to any practical problems worth addressing.


<h3>The testcase</h3>

The domain, right hand side and boundary conditions we implement below relate
to a problem in geophysics: there, one wants to compute the flow field of
magma in the earth's interior under a mid-ocean rift. Rifts are places where
two continental plates are very slowly drifting apart (a few centimeters per
year at most), leaving a crack in the earth crust that is filled with magma
from below. Without trying to be entirely realistic, we model this situation
by solving the following set of equations and boundary conditions on the
domain $\Omega=[-2,2]\times[0,1]\times[-1,0]$:
@f{eqnarray*}
  -2\; \textrm{div}\; \varepsilon(\textbf{u}) + \nabla p &=& 0,
  \\
  -\textrm{div}\; \textbf{u} &=& 0,
  \\
  \mathbf u &=&   \left(\begin{array}{c}
    -1 \\ 0 \\0
  \end{array}\right)
  \qquad\qquad \textrm{at}\ z=0, x<0,
  \\
  \mathbf u &=&   \left(\begin{array}{c}
    +1 \\ 0 \\0
  \end{array}\right)
  \qquad\qquad \textrm{at}\ z=0, x>0,
  \\
  \mathbf u &=&   \left(\begin{array}{c}
    0 \\ 0 \\0
  \end{array}\right)
  \qquad\qquad \textrm{at}\ z=0, x=0,
@f}
and using natural boundary conditions $\textbf{n}\cdot [p \textbf{I} - 2
\varepsilon(\textbf{u})] = 0$ everywhere else. In other words, at the
left part of the top surface we prescribe that the fluid moves with the
continental plate to the left at speed $-1$, that it moves to the right on the
right part of the top surface, and impose natural flow conditions everywhere
else. If we are in 2d, the description is essentially the same, with the
exception that we omit the second component of all vectors stated above.

As will become apparent in the <a href="#Results">results section</a>, the
flow field will pull material from below and move it to the left and right
ends of the domain, as expected. The discontinuity of velocity boundary
conditions will produce a singularity in the pressure at the center of the top
surface that sucks material all the way to the top surface to fill the gap
left by the outward motion of material at this location.


<h3>Implementation</h3>

<h4>Using imhomogeneous constraints for implementing Dirichlet boundary conditions</h4>

In all the previous tutorial programs, we used the AffineConstraints object merely
for handling hanging node constraints (with exception of step-11). However,
the class can also be used to implement Dirichlet boundary conditions, as we
will show in this program, by fixing some node values $x_i = b_i$. Note that
these are inhomogeneous constraints, and we have to pay some special
attention to that. The way we are going to implement this is to first read
in the boundary values into the AffineConstraints object by using the call

@code
  VectorTools::interpolate_boundary_values (dof_handler,
                                            1,
                                            BoundaryValues<dim>(),
                                            constraints);
@endcode

very similar to how we were making the list of boundary nodes
before (note that we set Dirichlet conditions only on boundaries with
boundary flag 1). The actual application of the boundary values is then
handled by the AffineConstraints object directly, without any additional
interference.

We could then proceed as before, namely by filling the matrix, and then
calling a condense function on the constraints object of the form
@code
  constraints.condense (system_matrix, system_rhs);
@endcode

Note that we call this on the system matrix and system right hand side
simultaneously, since resolving inhomogeneous constraints requires knowledge
about both the matrix entries and the right hand side. For efficiency
reasons, though, we choose another strategy: all the constraints collected
in the AffineConstraints object can be resolved on the fly while writing local data
into the global matrix, by using the call
@code
  constraints.distribute_local_to_global (local_matrix, local_rhs,
                                          local_dof_indices,
                                          system_matrix, system_rhs);
@endcode

This technique is further discussed in the step-27 tutorial
program. All we need to know here is that this functions does three things
at once: it writes the local data into the global matrix and right hand
side, it distributes the hanging node constraints and additionally
implements (inhomogeneous) Dirichlet boundary conditions. That's nice, isn't
it?

We can conclude that the AffineConstraints class provides an alternative to using
MatrixTools::apply_boundary_values for implementing Dirichlet boundary
conditions.


<a name="constraint-matrix">
<h4>Using AffineConstraints for increasing performance</h4>
</a>

Frequently, a sparse matrix contains a substantial amount of elements that
actually are zero when we are about to start a linear solve. Such elements are
introduced when we eliminate constraints or implement Dirichlet conditions,
where we usually delete all entries in constrained rows and columns, i.e., we
set them to zero. The fraction of elements that are present in the sparsity
pattern, but do not really contain any information, can be up to one fourth
of the total number of elements in the matrix for the 3D application
considered in this tutorial program. Remember that matrix-vector products or
preconditioners operate on all the elements of a sparse matrix (even those
that are zero), which is an inefficiency we will avoid here.

An advantage of directly resolving constrained degrees of freedom is that we
can avoid having most of the entries that are going to be zero in our sparse
matrix &mdash; we do not need constrained entries during matrix construction
(as opposed to the traditional algorithms, which first fill the matrix, and
only resolve constraints afterwards). This will save both memory and time
when forming matrix-vector products. The way we are going to do that is to
pass the information about constraints to the function that generates the
sparsity pattern, and then set a <tt>false</tt> argument specifying that we
do not intend to use constrained entries:
@code
  DoFTools::make_sparsity_pattern (dof_handler, sparsity_pattern,
                                   constraints, false);
@endcode
This functions obviates, by the way, also the call to the
<tt>condense()</tt> function on the sparsity pattern.


<h4>Performance optimizations</h4>

The program developed below has seen a lot of TLC. We have run it over and
over under profiling tools (mainly <a
href="http://www.valgrind.org/">valgrind</a>'s cachegrind and callgrind
tools, as well as the KDE <a
href="http://kcachegrind.sourceforge.net/">KCachegrind</a> program for
visualization) to see where the bottlenecks are. This has paid off: through
this effort, the program has become about four times as fast when
considering the runtime of the refinement cycles zero through three,
reducing the overall number of CPU instructions executed from
869,574,060,348 to 199,853,005,625. For higher refinement levels, the gain
is probably even larger since some algorithms that are not ${\cal O}(N)$
have been eliminated.

Essentially, there are currently two algorithms in the program that do not
scale linearly with the number of degrees of freedom: renumbering of degrees
of freedom (which is ${\cal O}(N \log N)$, and the linear solver (which is
${\cal O}(N^{4/3})$). As for the first, while reordering degrees of freedom
may not scale linearly, it is an indispensable part of the overall algorithm
as it greatly improves the quality of the sparse ILU, easily making up for
the time spent on computing the renumbering; graphs and timings to
demonstrate this are shown in the documentation of the DoFRenumbering
namespace, also underlining the choice of the Cuthill-McKee reordering
algorithm chosen below.

As for the linear solver: as mentioned above, our implementation here uses a
Schur complement formulation. This is not necessarily the very best choice
but demonstrates various important techniques available in deal.II. The
question of which solver is best is again discussed in the <a
href="#improved-solver">section on improved solvers in the results part</a>
of this program, along with code showing alternative solvers and a
comparison of their results.

Apart from this, many other algorithms have been tested and improved during
the creation of this program. For example, in building the sparsity pattern,
we originally used a (now no longer existing) BlockCompressedSparsityPattern
object that added one element at a time; however, its data structures were poorly
adapted for the large numbers of nonzero entries per row created by our
discretization in 3d, leading to a quadratic behavior. Replacing the internal
algorithms in deal.II to set many elements at a time, and using a
BlockCompressedSimpleSparsityPattern (which has, as of early 2015, been in turn
replaced by BlockDynamicSparsityPattern) as a better adapted data structure,
removed this bottleneck at the price of a slightly higher memory
consumption. Likewise, the implementation of the decomposition step in the
SparseILU class was very inefficient and has been replaced by one that is
about 10 times faster. Even the vmult function of the SparseILU has been
improved to save about twenty percent of time. Small improvements were
applied here and there. Moreover, the AffineConstraints object has been used
to eliminate a lot of entries in the sparse matrix that are eventually going
to be zero, see <a href="#constraint-matrix">the section on using advanced
features of the AffineConstraints class</a>.

A profile of how many CPU instructions are spent at the various
different places in the program during refinement cycles
zero through three in 3d is shown here:

<img src="https://www.dealii.org/images/steps/developer/step-22.profile-3.png" alt="">

As can be seen, at this refinement level approximately three quarters of the
instruction count is spent on the actual solver (the SparseILU::vmult calls
on the left, the SparseMatrix::vmult call in the middle for the Schur
complement solve, and another box representing the multiplications with
SparseILU and SparseMatrix in the solve for <i>U</i>). About one fifth of
the instruction count is spent on matrix assembly and sparse ILU computation
(box in the lower right corner) and the rest on other things. Since floating
point operations such as in the SparseILU::vmult calls typically take much
longer than many of the logical operations and table lookups in matrix
assembly, the fraction of the run time taken up by matrix assembly is
actually significantly less than the fraction of instructions, as will
become apparent in the comparison we make in the results section.

For higher refinement levels, the boxes representing the solver as well as
the blue box at the top right stemming from reordering algorithm are going
to grow at the expense of the other parts of the program, since they don't
scale linearly. The fact that at this moderate refinement level (3168 cells
and 93176 degrees of freedom) the linear solver already makes up about three
quarters of the instructions is a good sign that most of the algorithms used
in this program are well-tuned and that major improvements in speeding up
the program are most likely not to come from hand-optimizing individual
aspects but by changing solver algorithms. We will address this point in the
discussion of results below as well.

As a final point, and as a point of reference, the following picture also
shows how the profile looked at an early stage of optimizing this program:

<img src="https://www.dealii.org/images/steps/developer/step-22.profile-3.original.png" alt="">

As mentioned above, the runtime of this version was about four times as long as
for the first profile, with the SparseILU decomposition taking up about 30% of
the instruction count, and operations an early, inefficient version of
DynamicSparsityPattern about 10%. Both these bottlenecks have since been
completely removed.


examples/step-22/doc/results.dox
<a name="Results"></a>
<h1>Results</h1>

<h3>Output of the program and graphical visualization</h3>

<h4>2D calculations</h4>

Running the program with the space dimension set to 2 in the <code>main</code>
function yields the following output (in "release mode",
@dealiiVideoLectureSeeAlso{18}):
@code
examples/\step-22> make run
Refinement cycle 0
   Number of active cells: 64
   Number of degrees of freedom: 679 (594+85)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 1
   Number of active cells: 160
   Number of degrees of freedom: 1683 (1482+201)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 2
   Number of active cells: 376
   Number of degrees of freedom: 3813 (3370+443)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 3
   Number of active cells: 880
   Number of degrees of freedom: 8723 (7722+1001)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 4
   Number of active cells: 2008
   Number of degrees of freedom: 19383 (17186+2197)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 5
   Number of active cells: 4288
   Number of degrees of freedom: 40855 (36250+4605)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure
@endcode

The entire computation above takes about 2 seconds on a reasonably
quick (for 2015 standards) machine.

What we see immediately from this is that the number of (outer)
iterations does not increase as we refine the mesh. This confirms the
statement in the introduction that preconditioning the Schur
complement with the mass matrix indeed yields a matrix spectrally
equivalent to the identity matrix (i.e. with eigenvalues bounded above
and below independently of the mesh size or the relative sizes of
cells). In other words, the mass matrix and the Schur complement are
spectrally equivalent.

In the images below, we show the grids for the first six refinement
steps in the program.  Observe how the grid is refined in regions
where the solution rapidly changes: On the upper boundary, we have
Dirichlet boundary conditions that are -1 in the left half of the line
and 1 in the right one, so there is an abrupt change at $x=0$. Likewise,
there are changes from Dirichlet to Neumann data in the two upper
corners, so there is need for refinement there as well:

<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.2d.mesh-0.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.2d.mesh-1.png" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.2d.mesh-2.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.2d.mesh-3.png" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.2d.mesh-4.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.2d.mesh-5.png" alt="">
    </td>
  </tr>
</table>

Finally, following is a plot of the flow field. It shows fluid
transported along with the moving upper boundary and being replaced by
material coming from below:

<img src="https://www.dealii.org/images/steps/developer/step-22.2d.solution.png" alt="">

This plot uses the capability of VTK-based visualization programs (in
this case of VisIt) to show vector data; this is the result of us
declaring the velocity components of the finite element in use to be a
set of vector components, rather than independent scalar components in
the <code>StokesProblem@<dim@>::%output_results</code> function of this
tutorial program.



<h4>3D calculations</h4>

In 3d, the screen output of the program looks like this:

@code
Refinement cycle 0
   Number of active cells: 32
   Number of degrees of freedom: 1356 (1275+81)
   Assembling...
   Computing preconditioner...
   Solving...  13 outer CG Schur complement iterations for pressure.

Refinement cycle 1
   Number of active cells: 144
   Number of degrees of freedom: 5088 (4827+261)
   Assembling...
   Computing preconditioner...
   Solving...  14 outer CG Schur complement iterations for pressure.

Refinement cycle 2
   Number of active cells: 704
   Number of degrees of freedom: 22406 (21351+1055)
   Assembling...
   Computing preconditioner...
   Solving...  14 outer CG Schur complement iterations for pressure.

Refinement cycle 3
   Number of active cells: 3168
   Number of degrees of freedom: 93176 (89043+4133)
   Assembling...
   Computing preconditioner...
   Solving...  15 outer CG Schur complement iterations for pressure.

Refinement cycle 4
   Number of active cells: 11456
   Number of degrees of freedom: 327808 (313659+14149)
   Assembling...
   Computing preconditioner...
   Solving...  15 outer CG Schur complement iterations for pressure.

Refinement cycle 5
   Number of active cells: 45056
   Number of degrees of freedom: 1254464 (1201371+53093)
   Assembling...
   Computing preconditioner...
   Solving...  14 outer CG Schur complement iterations for pressure.
@endcode

Again, we see that the number of outer iterations does not increase as
we refine the mesh. Nevertheless, the compute time increases
significantly: for each of the iterations above separately, it takes about
0.14 seconds, 0.63 seconds, 4.8 seconds, 35 seconds, 2 minutes and 33 seconds,
and 13 minutes and 12 seconds. This overall superlinear (in the number of
unknowns) increase in runtime is due to the fact that our inner solver is not
${\cal O}(N)$: a simple experiment shows that as we keep refining the mesh, the
average number of ILU-preconditioned CG iterations to invert the
velocity-velocity block $A$ increases.

We will address the question of how possibly to improve our solver <a
href="#improved-solver">below</a>.

As for the graphical output, the grids generated during the solution
look as follow:

<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.3d.mesh-0.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.3d.mesh-1.png" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.3d.mesh-2.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.3d.mesh-3.png" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.3d.mesh-4.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.3d.mesh-5.png" alt="">
    </td>
  </tr>
</table>

Again, they show essentially the location of singularities introduced
by boundary conditions. The vector field computed makes for an
interesting graph:

<img src="https://www.dealii.org/images/steps/developer/step-22.3d.solution.png" alt="">

The isocontours shown here as well are those of the pressure
variable, showing the singularity at the point of discontinuous
velocity boundary conditions.



<h3>Sparsity pattern</h3>

As explained during the generation of the sparsity pattern, it is
important to have the numbering of degrees of freedom in mind when
using preconditioners like incomplete LU decompositions. This is most
conveniently visualized using the distribution of nonzero elements in
the stiffness matrix.

If we don't do anything special to renumber degrees of freedom (i.e.,
without using DoFRenumbering::Cuthill_McKee, but with using
DoFRenumbering::component_wise to ensure that degrees of freedom are
appropriately sorted into their corresponding blocks of the matrix and
vector), then we get the following image after the first adaptive
refinement in two dimensions:

<img src="https://www.dealii.org/images/steps/developer/step-22.2d.sparsity-nor.png" alt="">

In order to generate such a graph, you have to insert a piece of
code like the following to the end of the setup step.
@code
  {
    std::ofstream out ("sparsity_pattern.gpl");
    sparsity_pattern.print_gnuplot(out);
  }
@endcode

It is clearly visible that the nonzero entries are spread over almost the
whole matrix.  This makes preconditioning by ILU inefficient: ILU generates a
Gaussian elimination (LU decomposition) without fill-in elements, which means
that more tentative fill-ins left out will result in a worse approximation of
the complete decomposition.

In this program, we have thus chosen a more advanced renumbering of
components.  The renumbering with DoFRenumbering::Cuthill_McKee and grouping
the components into velocity and pressure yields the following output:

<img src="https://www.dealii.org/images/steps/developer/step-22.2d.sparsity-ren.png" alt="">

It is apparent that the situation has improved a lot. Most of the elements are
now concentrated around the diagonal in the (0,0) block in the matrix. Similar
effects are also visible for the other blocks. In this case, the ILU
decomposition will be much closer to the full LU decomposition, which improves
the quality of the preconditioner. (It may be interesting to note that the
sparse direct solver UMFPACK does some %internal renumbering of the equations
before actually generating a sparse LU decomposition; that procedure leads to
a very similar pattern to the one we got from the Cuthill-McKee algorithm.)

Finally, we want to have a closer
look at a sparsity pattern in 3D. We show only the (0,0) block of the
matrix, again after one adaptive refinement. Apart from the fact that the matrix
size has increased, it is also visible that there are many more entries
in the matrix. Moreover, even for the optimized renumbering, there will be a
considerable amount of tentative fill-in elements. This illustrates why UMFPACK
is not a good choice in 3D - a full decomposition needs many new entries that
 eventually won't fit into the physical memory (RAM):

<img src="https://www.dealii.org/images/steps/developer/step-22.3d.sparsity_uu-ren.png" alt="">



<h3>Possibilities for extensions</h3>

<a name="improved-solver">
<h4>Improved linear solver in 3D</h4>
</a>

We have seen in the section of computational results that the number of outer
iterations does not depend on the mesh size, which is optimal in a sense of
scalability. This does, however, not apply to the solver as a whole, as
mentioned above:
We did not look at the number of inner iterations when generating the inverse of
the matrix $A$ and the mass matrix $M_p$. Of course, this is unproblematic in
the 2D case where we precondition $A$ with a direct solver and the
<code>vmult</code> operation of the inverse matrix structure will converge in
one single CG step, but this changes in 3D where we only use an ILU
preconditioner.  There, the number of required preconditioned CG steps to
invert $A$ increases as the mesh is refined, and each <code>vmult</code>
operation involves on average approximately 14, 23, 36, 59, 75 and 101 inner
CG iterations in the refinement steps shown above. (On the other hand,
the number of iterations for applying the inverse pressure mass matrix is
always around five, both in two and three dimensions.)  To summarize, most work
is spent on solving linear systems with the same matrix $A$ over and over again.
What makes this look even worse is the fact that we
actually invert a matrix that is about 95 percent the size of the total system
matrix and stands for 85 percent of the non-zero entries in the sparsity
pattern. Hence, the natural question is whether it is reasonable to solve a
linear system with matrix $A$ for about 15 times when calculating the solution
to the block system.

The answer is, of course, that we can do that in a few other (most of the time
better) ways.
Nevertheless, it has to be remarked that an indefinite system as the one
at hand puts indeed much higher
demands on the linear algebra than standard elliptic problems as we have seen
in the early tutorial programs. The improvements are still rather
unsatisfactory, if one compares with an elliptic problem of similar
size. Either way, we will introduce below a number of improvements to the
linear solver, a discussion that we will re-consider again with additional
options in the step-31 program.

<a name="improved-ilu">
<h5>Better ILU decomposition by smart reordering</h5>
</a>
A first attempt to improve the speed of the linear solution process is to choose
a dof reordering that makes the ILU being closer to a full LU decomposition, as
already mentioned in the in-code comments. The DoFRenumbering namespace compares
several choices for the renumbering of dofs for the Stokes equations. The best
result regarding the computing time was found for the King ordering, which is
accessed through the call DoFRenumbering::boost::king_ordering. With that
program, the inner solver needs considerably less operations, e.g. about 62
inner CG iterations for the inversion of $A$ at cycle 4 compared to about 75
iterations with the standard Cuthill-McKee-algorithm. Also, the computing time
at cycle 4 decreased from about 17 to 11 minutes for the <code>solve()</code>
call. However, the King ordering (and the orderings provided by the
DoFRenumbering::boost namespace in general) has a serious drawback - it uses
much more memory than the in-build deal versions, since it acts on abstract
graphs rather than the geometry provided by the triangulation. In the present
case, the renumbering takes about 5 times as much memory, which yields an
infeasible algorithm for the last cycle in 3D with 1.2 million
unknowns.

<h5>Better preconditioner for the inner CG solver</h5>
Another idea to improve the situation even more would be to choose a
preconditioner that makes CG for the (0,0) matrix $A$ converge in a
mesh-independent number of iterations, say 10 to 30. We have seen such a
candidate in step-16: multigrid.

<h5>Block Schur complement preconditioner</h5>
<a name="block-schur"></a>
Even with a good preconditioner for $A$, we still
need to solve of the same linear system repeatedly (with different
right hand sides, though) in order to make the Schur complement solve
converge. The approach we are going to discuss here is how inner iteration
and outer iteration can be combined. If we persist in calculating the Schur
complement, there is no other possibility.

The alternative is to attack the block system at once and use an approximate
Schur complement as efficient preconditioner. The idea is as
follows: If we find a block preconditioner $P$ such that the matrix
@f{eqnarray*}
  P^{-1}\left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
@f}
is simple, then an iterative solver with that preconditioner will converge in a
few iterations. Using the Schur complement $S = B A^{-1} B^T$, one finds that
@f{eqnarray*}
  P^{-1}
  =
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ S^{-1} B A^{-1} & -S^{-1}
  \end{array}\right)
@f}
would appear to be a good choice since
@f{eqnarray*}
  P^{-1}\left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
  =
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ S^{-1} B A^{-1} & -S^{-1}
  \end{array}\right)\cdot \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
  =
  \left(\begin{array}{cc}
    I & A^{-1} B^T \\ 0 & I
  \end{array}\right).
@f}
This is the approach taken by the paper by Silvester and Wathen referenced
to in the introduction (with the exception that Silvester and Wathen use
right preconditioning). In this case, a Krylov-based iterative method would
converge in one step only if exact inverses of $A$ and $S$ were applied,
since all the eigenvalues are one (and the number of iterations in such a
method is bounded by the number of distinct eigenvalues). Below, we will
discuss the choice of an adequate solver for this problem. First, we are
going to have a closer look at the implementation of the preconditioner.

Since $P$ is aimed to be a preconditioner only, we shall use approximations to
the inverse of the Schur complement $S$ and the matrix $A$. Hence, the Schur
complement will be approximated by the pressure mass matrix $M_p$, and we use
a preconditioner to $A$ (without an InverseMatrix class around it) for
approximating $A^{-1}$.

Here comes the class that implements the block Schur
complement preconditioner. The <code>vmult</code> operation for block vectors
according to the derivation above can be specified by three successive
operations:
@code
template <class PreconditionerA, class PreconditionerMp>
class BlockSchurPreconditioner : public Subscriptor
{
  public:
    BlockSchurPreconditioner (const BlockSparseMatrix<double>         &S,
          const InverseMatrix<SparseMatrix<double>,PreconditionerMp>  &Mpinv,
          const PreconditionerA &Apreconditioner);

  void vmult (BlockVector<double>       &dst,
              const BlockVector<double> &src) const;

  private:
    const SmartPointer<const BlockSparseMatrix<double> > system_matrix;
    const SmartPointer<const InverseMatrix<SparseMatrix<double>,
                       PreconditionerMp > > m_inverse;
    const PreconditionerA &a_preconditioner;

    mutable Vector<double> tmp;

};

template <class PreconditionerA, class PreconditionerMp>
BlockSchurPreconditioner<PreconditionerA, PreconditionerMp>::BlockSchurPreconditioner(
          const BlockSparseMatrix<double>                            &S,
          const InverseMatrix<SparseMatrix<double>,PreconditionerMp> &Mpinv,
          const PreconditionerA &Apreconditioner
          )
                :
                system_matrix           (&S),
                m_inverse               (&Mpinv),
                a_preconditioner        (Apreconditioner),
                tmp                     (S.block(1,1).m())
{}

        // Now the interesting function, the multiplication of
        // the preconditioner with a BlockVector.
template <class PreconditionerA, class PreconditionerMp>
void BlockSchurPreconditioner<PreconditionerA, PreconditionerMp>::vmult (
                                     BlockVector<double>       &dst,
                                     const BlockVector<double> &src) const
{
        // Form u_new = A^{-1} u
  a_preconditioner.vmult (dst.block(0), src.block(0));
        // Form tmp = - B u_new + p
        // (<code>SparseMatrix::residual</code>
        // does precisely this)
  system_matrix->block(1,0).residual(tmp, dst.block(0), src.block(1));
        // Change sign in tmp
  tmp *= -1;
        // Multiply by approximate Schur complement
        // (i.e. a pressure mass matrix)
  m_inverse->vmult (dst.block(1), tmp);
}
@endcode

Since we act on the whole block system now, we have to live with one
disadvantage: we need to perform the solver iterations on
the full block system instead of the smaller pressure space.

Now we turn to the question which solver we should use for the block
system. The first observation is that the resulting preconditioned matrix cannot
be solved with CG since it is neither positive definite nor symmetric.

The deal.II libraries implement several solvers that are appropriate for the
problem at hand. One choice is the solver @ref SolverBicgstab "BiCGStab", which
was used for the solution of the unsymmetric advection problem in step-9. The
second option, the one we are going to choose, is @ref SolverGMRES "GMRES"
(generalized minimum residual). Both methods have their pros and cons - there
are problems where one of the two candidates clearly outperforms the other, and
vice versa.
<a href="http://en.wikipedia.org/wiki/GMRES#Comparison_with_other_solvers">Wikipedia</a>'s
article on the GMRES method gives a comparative presentation.
A more comprehensive and well-founded comparison can be read e.g. in the book by
J.W. Demmel (Applied Numerical Linear Algebra, SIAM, 1997, section 6.6.6).

For our specific problem with the ILU preconditioner for $A$, we certainly need
to perform hundreds of iterations on the block system for large problem sizes
(we won't beat CG!). Actually, this disfavors GMRES: During the GMRES
iterations, a basis of Krylov vectors is successively built up and some
operations are performed on these vectors. The more vectors are in this basis,
the more operations and memory will be needed. The number of operations scales
as ${\cal O}(n + k^2)$ and memory as ${\cal O}(kn)$, where $k$ is the number of
vectors in the Krylov basis and $n$ the size of the (block) matrix.
To not let these demands grow excessively, deal.II limits the size $k$ of the
basis to 30 vectors by default.
Then, the basis is rebuilt. This implementation of the GMRES method is called
GMRES(k), with default $k=30$. What we have gained by this restriction,
namely a bound on operations and memory requirements, will be compensated by
the fact that we use an incomplete basis - this will increase the number of
required iterations.

BiCGStab, on the other hand, won't get slower when many iterations are needed
(one iteration uses only results from one preceding step and
not all the steps as GMRES). Besides the fact the BiCGStab is more expensive per
step since two matrix-vector products are needed (compared to one for
CG or GMRES), there is one main reason which makes BiCGStab not appropriate for
this problem: The preconditioner applies the inverse of the pressure
mass matrix by using the InverseMatrix class. Since the application of the
inverse matrix to a vector is done only in approximative way (an exact inverse
is too expensive), this will also affect the solver. In the case of BiCGStab,
the Krylov vectors will not be orthogonal due to that perturbation. While
this is uncritical for a small number of steps (up to about 50), it ruins the
performance of the solver when these perturbations have grown to a significant
magnitude in the coarse of iterations.

We did some experiments with BiCGStab and found it to
be faster than GMRES up to refinement cycle 3 (in 3D), but it became very slow
for cycles 4 and 5 (even slower than the original Schur complement), so the
solver is useless in this situation. Choosing a sharper tolerance for the
inverse matrix class (<code>1e-10*src.l2_norm()</code> instead of
<code>1e-6*src.l2_norm()</code>) made BiCGStab perform well also for cycle 4,
but did not change the failure on the very large problems.

GMRES is of course also effected by the approximate inverses, but it is not as
sensitive to orthogonality and retains a relatively good performance also for
large sizes, see the results below.

With this said, we turn to the realization of the solver call with GMRES with
$k=100$ temporary vectors:

@code
      const SparseMatrix<double> &pressure_mass_matrix
        = preconditioner_matrix.block(1,1);
      SparseILU<double> pmass_preconditioner;
      pmass_preconditioner.initialize (pressure_mass_matrix,
        SparseILU<double>::AdditionalData());

      InverseMatrix<SparseMatrix<double>,SparseILU<double> >
        m_inverse (pressure_mass_matrix, pmass_preconditioner);

      BlockSchurPreconditioner<typename InnerPreconditioner<dim>::type,
                               SparseILU<double> >
        preconditioner (system_matrix, m_inverse, *A_preconditioner);

      SolverControl solver_control (system_matrix.m(),
                                    1e-6*system_rhs.l2_norm());
      GrowingVectorMemory<BlockVector<double> > vector_memory;
      SolverGMRES<BlockVector<double> >::AdditionalData gmres_data;
      gmres_data.max_n_tmp_vectors = 100;

      SolverGMRES<BlockVector<double> > gmres(solver_control, vector_memory,
                                              gmres_data);

      gmres.solve(system_matrix, solution, system_rhs,
                  preconditioner);

      constraints.distribute (solution);

      std::cout << " "
                << solver_control.last_step()
                << " block GMRES iterations";
@endcode

Obviously, one needs to add the include file @ref SolverGMRES
"<lac/solver_gmres.h>" in order to make this run.
We call the solver with a BlockVector template in order to enable
GMRES to operate on block vectors and matrices.
Note also that we need to set the (1,1) block in the system
matrix to zero (we saved the pressure mass matrix there which is not part of the
problem) after we copied the information to another matrix.

Using the Timer class, we collect some statistics that compare the runtime
of the block solver with the one from the problem implementation above.
Besides the solution with the two options we also check if the solutions
of the two variants are close to each other (i.e. this solver gives indeed the
same solution as we had before) and calculate the infinity
norm of the vector difference.

Let's first see the results in 2D:
@code
Refinement cycle 0
   Number of active cells: 64
   Number of degrees of freedom: 679 (594+85) [0.00162792 s]
   Assembling...  [0.00108981 s]
   Computing preconditioner... [0.0025959 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [0.00479603s ]
      Block Schur preconditioner: 12 GMRES iterations [0.00441718 s]
   l_infinity difference between solution vectors: 5.38258e-07

Refinement cycle 1
   Number of active cells: 160
   Number of degrees of freedom: 1683 (1482+201) [0.00345707 s]
   Assembling...  [0.00237417 s]
   Computing preconditioner... [0.00605702 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [0.0123992s ]
      Block Schur preconditioner: 12 GMRES iterations [0.011909 s]
   l_infinity difference between solution vectors: 1.74658e-05

Refinement cycle 2
   Number of active cells: 376
   Number of degrees of freedom: 3813 (3370+443) [0.00729299 s]
   Assembling...  [0.00529909 s]
   Computing preconditioner... [0.0167508 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [0.031672s ]
      Block Schur preconditioner: 12 GMRES iterations [0.029232 s]
   l_infinity difference between solution vectors: 7.81569e-06

Refinement cycle 3
   Number of active cells: 880
   Number of degrees of freedom: 8723 (7722+1001) [0.017709 s]
   Assembling...  [0.0126002 s]
   Computing preconditioner... [0.0435679 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [0.0971651s ]
      Block Schur preconditioner: 12 GMRES iterations [0.0992041 s]
   l_infinity difference between solution vectors: 1.87249e-05

Refinement cycle 4
   Number of active cells: 2008
   Number of degrees of freedom: 19383 (17186+2197) [0.039988 s]
   Assembling...  [0.028281 s]
   Computing preconditioner... [0.118314 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [0.252133s ]
      Block Schur preconditioner: 13 GMRES iterations [0.269125 s]
   l_infinity difference between solution vectors: 6.38657e-05

Refinement cycle 5
   Number of active cells: 4288
   Number of degrees of freedom: 40855 (36250+4605) [0.0880702 s]
   Assembling...  [0.0603511 s]
   Computing preconditioner... [0.278339 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [0.53846s ]
      Block Schur preconditioner: 13 GMRES iterations [0.578667 s]
   l_infinity difference between solution vectors: 0.000173363
@endcode

We see that there is no huge difference in the solution time between the
block Schur complement preconditioner solver and the Schur complement
itself. The reason is simple: we used a direct solve as preconditioner for
$A$ - so we cannot expect any gain by avoiding the inner iterations. We see
that the number of iterations has slightly increased for GMRES, but all in
all the two choices are fairly similar.

The picture of course changes in 3D:

@code
Refinement cycle 0
   Number of active cells: 32
   Number of degrees of freedom: 1356 (1275+81) [0.00845218 s]
   Assembling...  [0.019372 s]
   Computing preconditioner... [0.00712395 s]
   Solving...
      Schur complement: 13 outer CG iterations for p  [0.0320101s ]
      Block Schur preconditioner: 22 GMRES iterations [0.0048759 s]
   l_infinity difference between solution vectors: 2.15942e-05

Refinement cycle 1
   Number of active cells: 144
   Number of degrees of freedom: 5088 (4827+261) [0.0346942 s]
   Assembling...  [0.0857739 s]
   Computing preconditioner... [0.0465031 s]
   Solving...
      Schur complement: 14 outer CG iterations for p  [0.349258s ]
      Block Schur preconditioner: 35 GMRES iterations [0.048759 s]
   l_infinity difference between solution vectors: 1.77657e-05

Refinement cycle 2
   Number of active cells: 704
   Number of degrees of freedom: 22406 (21351+1055) [0.175669 s]
   Assembling...  [0.437447 s]
   Computing preconditioner... [0.286435 s]
   Solving...
      Schur complement: 14 outer CG iterations for p  [3.65519s ]
      Block Schur preconditioner: 63 GMRES iterations [0.497787 s]
   l_infinity difference between solution vectors: 5.08078e-05

Refinement cycle 3
   Number of active cells: 3168
   Number of degrees of freedom: 93176 (89043+4133) [0.790985 s]
   Assembling...  [1.97598 s]
   Computing preconditioner... [1.4325 s]
   Solving...
      Schur complement: 15 outer CG iterations for p  [29.9666s ]
      Block Schur preconditioner: 128 GMRES iterations [5.02645 s]
   l_infinity difference between solution vectors: 0.000119671

Refinement cycle 4
   Number of active cells: 11456
   Number of degrees of freedom: 327808 (313659+14149) [3.44995 s]
   Assembling...  [7.54772 s]
   Computing preconditioner... [5.46306 s]
   Solving...
      Schur complement: 15 outer CG iterations for p  [139.987s ]
      Block Schur preconditioner: 255 GMRES iterations [38.0946 s]
   l_infinity difference between solution vectors: 0.00020793

Refinement cycle 5
   Number of active cells: 45056
   Number of degrees of freedom: 1254464 (1201371+53093) [19.6795 s]
   Assembling...  [28.6586 s]
   Computing preconditioner... [22.401 s]
   Solving...
      Schur complement: 14 outer CG iterations for p  [796.767s ]
      Block Schur preconditioner: 524 GMRES iterations [355.597 s]
   l_infinity difference between solution vectors: 0.000501219
@endcode

Here, the block preconditioned solver is clearly superior to the Schur
complement, but the advantage gets less for more mesh points. This is
because GMRES(k) scales worse with the problem size than CG, as we discussed
above.  Nonetheless, the improvement by a factor of 3-6 for moderate problem
sizes is quite impressive.


<h5>Combining the block preconditioner and multigrid</h5>
An ultimate linear solver for this problem could be imagined as a
combination of an optimal
preconditioner for $A$ (e.g. multigrid) and the block preconditioner
described above, which is the approach taken in the step-31
and step-32 tutorial programs (where we use an algebraic multigrid
method) and step-56 (where we use a geometric multigrid method).


<h5>No block matrices and vectors</h5>
Another possibility that can be taken into account is to not set up a block
system, but rather solve the system of velocity and pressure all at once. The
options are direct solve with UMFPACK (2D) or GMRES with ILU
preconditioning (3D). It should be straightforward to try that.



<h4>More interesting testcases</h4>

The program can of course also serve as a basis to compute the flow in more
interesting cases. The original motivation to write this program was for it to
be a starting point for some geophysical flow problems, such as the
movement of magma under places where continental plates drift apart (for
example mid-ocean ridges). Of course, in such places, the geometry is more
complicated than the examples shown above, but it is not hard to accommodate
for that.

For example, by using the following modification of the boundary values
function
@code
template <int dim>
double
BoundaryValues<dim>::value (const Point<dim>  &p,
                            const unsigned int component) const
{
  Assert (component < this->n_components,
          ExcIndexRange (component, 0, this->n_components));

  const double x_offset = std::atan(p[1]*4)/3;

  if (component == 0)
    return (p[0] < x_offset ? -1 : (p[0] > x_offset ? 1 : 0));
  return 0;
}
@endcode
and the following way to generate the mesh as the domain
$[-2,2]\times[-2,2]\times[-1,0]$
@code
    std::vector<unsigned int> subdivisions (dim, 1);
    subdivisions[0] = 4;
    if (dim>2)
      subdivisions[1] = 4;

    const Point<dim> bottom_left = (dim == 2 ?
                                    Point<dim>(-2,-1) :
                                    Point<dim>(-2,-2,-1));
    const Point<dim> top_right   = (dim == 2 ?
                                    Point<dim>(2,0) :
                                    Point<dim>(2,2,0));

    GridGenerator::subdivided_hyper_rectangle (triangulation,
                                               subdivisions,
                                               bottom_left,
                                               top_right);
@endcode
then we get images where the fault line is curved:
<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.3d-extension.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-22.3d-grid-extension.png" alt="">
    </td>
  </tr>
</table>


examples/step-23/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

@dealiiVideoLecture{28}

This is the first of a number of tutorial programs that will finally
cover "real" time-dependent problems, not the slightly odd form of time
dependence found in step-18 or the DAE model of step-21. In particular, this program introduces
the wave equation in a bounded domain. Later, step-24
will consider an example of absorbing boundary conditions, and @ref
step_25 "step-25" a kind of nonlinear wave equation producing
solutions called solitons.

The wave equation in its prototypical form reads as follows: find
$u(x,t), x\in\Omega, t\in[0,T]$ that satisfies
@f{eqnarray*}
	\frac{\partial^2 u}{\partial t^2}
	-
	\Delta u &=& f
	\qquad
	\textrm{in}\ \Omega\times [0,T],
\\
	u(x,t) &=& g
	\qquad
	\textrm{on}\ \partial\Omega\times [0,T],
\\
	u(x,0) &=& u_0(x)
	\qquad
	\textrm{in}\ \Omega,
\\
	\frac{\partial u(x,0)}{\partial t} &=& u_1(x)
	\qquad
	\textrm{in}\ \Omega.
@f}
Note that since this is an equation with second-order time
derivatives, we need to pose two initial conditions, one for the value
and one for the time derivative of the solution.

Physically, the equation describes the motion of an elastic medium. In
2-d, one can think of how a membrane moves if subjected to a
force. The Dirichlet boundary conditions above indicate that the
membrane is clamped at the boundary at a height $g(x,t)$ (this height
might be moving as well &mdash; think of people holding a blanket and
shaking it up and down). The first initial condition equals the
initial deflection of the membrane, whereas the second one gives its
velocity. For example, one could think of pushing the membrane down
with a finger and then letting it go at $t=0$ (nonzero deflection but
zero initial velocity), or hitting it with a hammer at $t=0$ (zero
deflection but nonzero velocity). Both cases would induce motion in
the membrane.


<h3>Time discretization</h3>

<h4>Method of lines or Rothe's method?</h4>
There is a long-standing debate in the numerical analysis community
over whether a discretization of time dependent equations should
involve first discretizing the time variable leading to a stationary
PDE at each time step that is then solved using standard finite
element techniques (this is called the Rothe method), or whether
one should first discretize the spatial variables, leading to a large
system of ordinary differential equations that can then be handled by
one of the usual ODE solvers (this is called the method of lines).

Both of these methods have advantages and disadvantages.
Traditionally, people have preferred the method of lines, since it
allows to use the very well developed machinery of high-order ODE
solvers available for the rather stiff ODEs resulting from this
approach, including step length control and estimation of the temporal
error.

On the other hand, Rothe's method becomes awkward when using
higher-order time stepping method, since one then has to write down a
PDE that couples the solution of the present time step not only with
that at the previous time step, but possibly also even earlier
solutions, leading to a significant number of terms.

For these reasons, the method of lines was the method of choice for a
long time. However, it has one big drawback: if we discretize the
spatial variable first, leading to a large ODE system, we have to
choose a mesh once and for all. If we are willing to do this, then
this is a legitimate and probably superior approach.

If, on the other hand, we are looking at the wave equation and many
other time dependent problems, we find that the character of a
solution changes as time progresses. For example, for the wave
equation, we may have a single wave travelling through the domain,
where the solution is smooth or even constant in front of and behind
the wave &mdash; adaptivity would be really useful for such cases, but the
key is that the area where we need to refine the mesh changes from
time step to time step!

If we intend to go that way, i.e. choose a different mesh for each
time step (or set of time steps), then the method of lines is not
appropriate any more: instead of getting one ODE system with a number
of variables equal to the number of unknowns in the finite element
mesh, our number of unknowns now changes all the time, a fact that
standard ODE solvers are certainly not prepared to deal with at
all. On the other hand, for the Rothe method, we just get a PDE for
each time step that we may choose to discretize independently of the
mesh used for the previous time step; this approach is not without
perils and difficulties, but at least is a sensible and well-defined
procedure.

For all these reasons, for the present program, we choose to use the
Rothe method for discretization, i.e. we first discretize in time and
then in space. We will not actually use adaptive meshes at all, since
this involves a large amount of additional code, but we will comment
on this some more in the <a href="#Results">results section below</a>.


<h4>Rothe's method!</h4>

Given these considerations, here is how we will proceed: let us first
define a simple time stepping method for this second order problem,
and then in a second step do the spatial discretization, i.e. we will
follow Rothe's approach.

For the first step, let us take a little detour first: in order to
discretize a second time derivative, we can either discretize it
directly, or we can introduce an additional variable and transform the
system into a first order system. In many cases, this turns out to be
equivalent, but dealing with first order systems is often simpler. To
this end, let us introduce
@f[
	v = \frac{\partial u}{\partial t},
@f]
and call this variable the <i>velocity</i> for obvious reasons. We can
then reformulate the original wave equation as follows:
@f{eqnarray*}
	\frac{\partial u}{\partial t}
	-
	v
	&=& 0
	\qquad
	\textrm{in}\ \Omega\times [0,T],
\\
	\frac{\partial v}{\partial t}
	-
	\Delta u &=& f
	\qquad
	\textrm{in}\ \Omega\times [0,T],
\\
	u(x,t) &=& g
	\qquad
	\textrm{on}\ \partial\Omega\times [0,T],
\\
	u(x,0) &=& u_0(x)
	\qquad
	\textrm{in}\ \Omega,
\\
	v(x,0) &=& u_1(x)
	\qquad
	\textrm{in}\ \Omega.
@f}
The advantage of this formulation is that it now only contains first
time derivatives for both variables, for which it is simple to write
down time stepping schemes. Note that we do not have boundary
conditions for $v$ at first. However, we could enforce $v=\frac{\partial
g}{\partial t}$ on the boundary. It turns out in numerical examples that this
is actually necessary: without doing so the solution doesn't look particularly
wrong, but the Crank-Nicolson scheme does not conserve energy if one doesn't
enforce these boundary conditions.

With this formulation, let us introduce the following time
discretization where a superscript $n$ indicates the number of a time
step and $k=t_n-t_{n-1}$ is the length of the present time step:
\f{eqnarray*}
  \frac{u^n - u^{n-1}}{k}
  - \left[\theta v^n + (1-\theta) v^{n-1}\right] &=& 0,
  \\
  \frac{v^n - v^{n-1}}{k}
  - \Delta\left[\theta u^n + (1-\theta) u^{n-1}\right]
  &=& \theta f^n + (1-\theta) f^{n-1}.
\f}
Note how we introduced a parameter $\theta$ here. If we chose
$\theta=0$, for example, the first equation would reduce to
$\frac{u^n - u^{n-1}}{k}  - v^{n-1} = 0$, which is well-known as the
forward or explicit Euler method. On the other hand, if we set
$\theta=1$, then we would get
$\frac{u^n - u^{n-1}}{k}  - v^n = 0$, which corresponds to the
backward or implicit Euler method. Both these methods are first order
accurate methods. They are simple to implement, but they are not
really very accurate.

The third case would be to choose $\theta=\frac 12$. The first of the
equations above would then read $\frac{u^n - u^{n-1}}{k}
- \frac 12 \left[v^n + v^{n-1}\right] = 0$. This method is known as
the Crank-Nicolson method and has the advantage that it is second
order accurate. In addition, it has the nice property that it
preserves the energy in the solution (physically, the energy is the
sum of the kinetic energy of the particles in the membrane plus the
potential energy present due to the fact that it is locally stretched;
this quantity is a conserved one in the continuous equation, but most
time stepping schemes do not conserve it after time
discretization). Since $v^n$ also appears in the equation for $u^n$,
the Crank-Nicolson scheme is also implicit.

In the program, we will leave $\theta$ as a parameter, so that it will
be easy to play with it. The results section will show some numerical
evidence comparing the different schemes.

The equations above (called the <i>semidiscretized</i> equations
because we have only discretized the time, but not space), can be
simplified a bit by eliminating $v^n$ from the first equation and
rearranging terms. We then get
\f{eqnarray*}
  \left[ 1-k^2\theta^2\Delta \right] u^n &=&
  	 \left[ 1+k^2\theta(1-\theta)\Delta\right] u^{n-1} + k v^{n-1}
   	 + k^2\theta\left[\theta f^n + (1-\theta) f^{n-1}\right],\\
   v^n &=& v^{n-1} + k\Delta\left[ \theta u^n + (1-\theta) u^{n-1}\right]
   + k\left[\theta f^n + (1-\theta) f^{n-1}\right].
\f}
In this form, we see that if we are given the solution
$u^{n-1},v^{n-1}$ of the previous timestep, that we can then solve for
the variables $u^n,v^n$ separately, i.e. one at a time. This is
convenient. In addition, we recognize that the operator in the first
equation is positive definite, and the second equation looks
particularly simple.


<h3>Space discretization</h3>

We have now derived equations that relate the approximate
(semi-discrete) solution $u^n(x)$ and its time derivative $v^n(x)$ at
time $t_n$ with the solutions $u^{n-1}(x),v^{n-1}(x)$ of the previous
time step at $t_{n-1}$. The next step is to also discretize the
spatial variable using the usual finite element methodology. To this
end, we multiply each equation with a test function, integrate over
the entire domain, and integrate by parts where necessary. This leads
to
\f{eqnarray*}
  (u^n,\varphi) + k^2\theta^2(\nabla u^n,\nabla \varphi) &=&
  (u^{n-1},\varphi) - k^2\theta(1-\theta)(\nabla u^{n-1},\nabla \varphi)
  +
  k(v^{n-1},\varphi)
  + k^2\theta
  \left[
  \theta (f^n,\varphi) + (1-\theta) (f^{n-1},\varphi)
  \right],
  \\
  (v^n,\varphi)
   &=&
   (v^{n-1},\varphi)
    -
    k\left[ \theta (\nabla u^n,\nabla\varphi) +
    (1-\theta) (\nabla u^{n-1},\nabla \varphi)\right]
  + k
  \left[
  \theta (f^n,\varphi) + (1-\theta) (f^{n-1},\varphi)
  \right].
\f}

It is then customary to approximate $u^n(x) \approx u^n_h(x) = \sum_i
U_i^n\phi_i^n(x)$, where $\phi_i^n(x)$ are the shape functions used
for the discretization of the $n$-th time step and $U_i^n$ are the
unknown nodal values of the solution. Similarly, $v^n(x) \approx
v^n_h(x) = \sum_i V_i^n\phi_i^n(x)$. Finally, we have the solutions of
the previous time step, $u^{n-1}(x) \approx u^{n-1}_h(x) = \sum_i
U_i^{n-1}\phi_i^{n-1}(x)$ and $v^{n-1}(x) \approx v^{n-1}_h(x) = \sum_i
V_i^{n-1}\phi_i^{n-1}(x)$. Note that since the solution of the previous
time step has already been computed by the time we get to time step
$n$, $U^{n-1},V^{n-1}$ are known. Furthermore, note that the solutions
of the previous step may have been computed on a different mesh, so
we have to use shape functions $\phi^{n-1}_i(x)$.

If we plug these expansions into above equations and test with the
test functions from the present mesh, we get the following linear
system:
\f{eqnarray*}
  (M^n + k^2\theta^2 A^n)U^n &=&
  M^{n,n-1}U^{n-1} - k^2\theta(1-\theta) A^{n,n-1}U^{n-1}
  +
  kM^{n,n-1}V^{n-1}
  + k^2\theta
  \left[
  \theta F^n + (1-\theta) F^{n-1}
  \right],
  \\
  M^nV^n
   &=&
   M^{n,n-1}V^{n-1}
    -
    k\left[ \theta A^n U^n +
    (1-\theta) A^{n,n-1} U^{n-1}\right]
   + k
  \left[
  \theta F^n + (1-\theta) F^{n-1}
  \right],
\f}
where
@f{eqnarray*}
	M^n_{ij} &=& (\phi_i^n, \phi_j^n),
	\\
	A^n_{ij} &=& (\nabla\phi_i^n, \nabla\phi_j^n),
	\\
	M^{n,n-1}_{ij} &=& (\phi_i^n, \phi_j^{n-1}),
	\\
	A^{n,n-1}_{ij} &=& (\nabla\phi_i^n, \nabla\phi_j^{n-1}),
	\\
	F^n_{i} &=& (f^n,\phi_i^n),
	\\
	F^{n-1}_{i} &=& (f^{n-1},\phi_i^n).
@f}

If we solve these two equations, we can move the solution one step
forward and go on to the next time step.

It is worth noting that if we choose the same mesh on each time step
(as we will in fact do in the program below), then we have the same
shape functions on time step $n$ and $n-1$,
i.e. $\phi^n_i=\phi_i^{n-1}=\phi_i$. Consequently, we get
$M^n=M^{n,n-1}=M$ and $A^n=A^{n,n-1}=A$. On the other hand, if we had
used different shape functions, then we would have to compute
integrals that contain shape functions defined on two meshes. This is a
somewhat messy process that we omit here, but that is treated in some
detail in step-28.

Under these conditions (i.e. a mesh that doesn't change), one can optimize the
solution procedure a bit by basically eliminating the solution of the second
linear system. We will discuss this in the introduction of the @ref step_25
"step-25" program.

<h3>Energy conservation</h3>

One way to compare the quality of a time stepping scheme is to see whether the
numerical approximation preserves conservation properties of the continuous
equation. For the wave equation, the natural quantity to look at is the
energy. By multiplying the wave equation by $u_t$, integrating over $\Omega$,
and integrating by parts where necessary, we find that
@f[
	\frac{d}{d t}
	\left[\frac 12 \int_\Omega \left(\frac{\partial u}{\partial
	t}\right)^2 + (\nabla u)^2 \; dx\right]
	=
	\int_\Omega f \frac{\partial u}{\partial t} \; dx
	+
	\int_{\partial\Omega} n\cdot\nabla u
	\frac{\partial g}{\partial t} \; dx.
@f]
By consequence, in absence of body forces and constant boundary values, we get
that
@f[
	E(t) = \frac 12 \int_\Omega \left(\frac{\partial u}{\partial
	t}\right)^2 + (\nabla u)^2 \; dx
@f]
is a conserved quantity, i.e. one that doesn't change with time. We
will compute this quantity after each time
step. It is straightforward to see that if we replace $u$ by its finite
element approximation, and $\frac{\partial u}{\partial t}$ by the finite
element approximation of the velocity $v$, then
@f[
	E(t_n) = \frac 12 \left<V^n, M^n V^n\right>
	+
	\frac 12 \left<U^n, A^n U^n\right>.
@f]
As we will see in the results section, the Crank-Nicolson scheme does indeed
conserve the energy, whereas neither the forward nor the backward Euler scheme
do.


<h3>Who are Courant, Friedrichs, and Lewy?</h3>

One of the reasons why the wave equation is nasty to solve numerically is that
explicit time discretizations are only stable if the time step is small
enough. In particular, it is coupled to the spatial mesh width $h$. For the
lowest order discretization we use here, the relationship reads
@f[
	k\le \frac hc
@f]
where $c$ is the wave speed, which in our formulation of the wave equation has
been normalized to one. Consequently, unless we use the implicit schemes with
$\theta>0$, our solutions will not be numerically stable if we violate this
restriction. Implicit schemes do not have this restriction for stability, but
they become inaccurate if the time step is too large.

This condition was first recognized by Courant, Friedrichs, and Lewy &mdash;
in 1928, long before computers became available for numerical
computations! (This result appeared in the German language article
R. Courant, K. Friedrichs and H. Lewy: <i>&Uuml;ber die partiellen
Differenzengleichungen der mathematischen Physik</i>, Mathematische
Annalen, vol. 100, no. 1, pages 32-74, 1928.)
This condition on the time step is most frequently just referred
to as the <i>CFL</i> condition. Intuitively, the CFL condition says
that the time step must not be larger than the time it takes a wave to
cross a single cell.

In the program, we will refine the square
$[-1,1]^2$ seven times uniformly, giving a mesh size of $h=\frac 1{64}$, which
is what we set the time step to. The fact that we set the time step and mesh
size individually in two different places is error prone: it is too easy to
refine the mesh once more but forget to also adjust the time step. @ref
step_24 "step-24" shows a better way how to keep these things in sync.


<h3>The test case</h3>

Although the program has all the hooks to deal with nonzero initial and
boundary conditions and body forces, we take a simple case where the domain is
a square $[-1,1]^2$ and
@f{eqnarray*}
	f &=& 0,
	\\
	u_0 &=& 0,
	\\
	u_1 &=& 0,
	\\
	g &=& \left\{\begin{matrix}\sin (4\pi t)
	&\qquad& \text{for }\ t\le \frac 12, x=-1, -\frac 13<y<\frac 13
	\\
	 0
	&&\text{otherwise}
	\end{matrix}
	\right.
@f}
This corresponds to a membrane initially at rest and clamped all around, where
someone is waving a part of the clamped boundary once up and down, thereby
shooting a wave into the domain.


examples/step-23/doc/results.dox
<h1>Results</h1>

When the program is run, it produces the following output:
@code
Number of active cells: 16384
Number of degrees of freedom: 16641

Time step 1 at t=0.015625
   u-equation: 8 CG iterations.
   v-equation: 22 CG iterations.
   Total energy: 1.17887
Time step 2 at t=0.03125
   u-equation: 8 CG iterations.
   v-equation: 20 CG iterations.
   Total energy: 2.9655
Time step 3 at t=0.046875
   u-equation: 8 CG iterations.
   v-equation: 21 CG iterations.
   Total energy: 4.33761
Time step 4 at t=0.0625
   u-equation: 7 CG iterations.
   v-equation: 21 CG iterations.
   Total energy: 5.35499
Time step 5 at t=0.078125
   u-equation: 7 CG iterations.
   v-equation: 21 CG iterations.
   Total energy: 6.18652
Time step 6 at t=0.09375
   u-equation: 7 CG iterations.
   v-equation: 20 CG iterations.
   Total energy: 6.6799

...

Time step 31 at t=0.484375
   u-equation: 7 CG iterations.
   v-equation: 20 CG iterations.
   Total energy: 21.9068
Time step 32 at t=0.5
   u-equation: 7 CG iterations.
   v-equation: 20 CG iterations.
   Total energy: 23.3394
Time step 33 at t=0.515625
   u-equation: 7 CG iterations.
   v-equation: 20 CG iterations.
   Total energy: 23.1019

...

Time step 319 at t=4.98438
   u-equation: 7 CG iterations.
   v-equation: 20 CG iterations.
   Total energy: 23.1019
Time step 320 at t=5
   u-equation: 7 CG iterations.
   v-equation: 20 CG iterations.
   Total energy: 23.1019
@endcode

What we see immediately is that the energy is a constant at least after
$t=\frac 12$ (until which the boundary source term $g$ is nonzero, injecting
energy into the system).

In addition to the screen output, the program writes the solution of each time
step to an output file. If we process them adequately and paste them into a
movie, we get the following:

<img src="https://www.dealii.org/images/steps/developer/step-23.movie.gif" alt="Animation of the solution of step 23.">

The movie shows the generated wave nice traveling through the domain and back,
being reflected at the clamped boundary. Some numerical noise is trailing the
wave, an artifact of a too-large mesh size that can be reduced by reducing the
mesh width and the time step.


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

If you want to explore a bit, try out some of the following things:
<ul>
  <li>Varying $\theta$. This gives different time stepping schemes, some of
  which are stable while others are not. Take a look at how the energy
  evolves.

  <li>Different initial and boundary conditions, right hand sides.

  <li>More complicated domains or more refined meshes. Remember that the time
  step needs to be bounded by the mesh width, so changing the mesh should
  always involve also changing the time step. We will come back to this issue
  in step-24.

  <li>Variable coefficients: In real media, the wave speed is often
  variable. In particular, the "real" wave equation in realistic media would
  read
  @f[
     \rho(x) \frac{\partial^2 u}{\partial t^2}
     -
     \nabla \cdot
     a(x) \nabla u = f,
  @f]
  where $\rho(x)$ is the density of the material, and $a(x)$ is related to the
  stiffness coefficient. The wave speed is then $c=\sqrt{a/\rho}$.

  To make such a change, we would have to compute the mass and Laplace
  matrices with a variable coefficient. Fortunately, this isn't too hard: the
  functions MatrixCreator::create_laplace_matrix and
  MatrixCreator::create_mass_matrix have additional default parameters that can
  be used to pass non-constant coefficient functions to them. The required
  changes are therefore relatively small. On the other hand, care must be
  taken again to make sure the time step is within the allowed range.

  <li>In the in-code comments, we discussed the fact that the matrices for
  solving for $U^n$ and $V^n$ need to be reset in every time because of
  boundary conditions, even though the actual content does not change. It is
  possible to avoid copying by not eliminating columns in the linear systems,
  which is implemented by appending a @p false argument to the call:
  @code
    MatrixTools::apply_boundary_values(boundary_values,
                                       matrix_u,
                                       solution_u,
                                       system_rhs,
                                       false);
  @endcode

  <li>deal.II being a library that supports adaptive meshes it would of course be
  nice if this program supported change the mesh every few time steps. Given the
  structure of the solution &mdash; a wave that travels through the domain &mdash;
  it would seem appropriate if we only refined the mesh where the wave currently is,
  and not simply everywhere. It is intuitively clear that we should be able to
  save a significant amount of cells this way. (Though upon further thought one
  realizes that this is really only the case in the initial stages of the simulation.
  After some time, for wave phenomena, the domain is filled with reflections of
  the initial wave going in every direction and filling every corner of the domain.
  At this point, there is in general little one can gain using local mesh
  refinement.)

  To make adaptively changing meshes possible, there are basically two routes.
  The "correct" way would be to go back to the weak form we get using Rothe's
  method. For example, the first of the two equations to be solved in each time
  step looked like this:
  \f{eqnarray*}
  (u^n,\varphi) + k^2\theta^2(\nabla u^n,\nabla \varphi) &=&
  (u^{n-1},\varphi) - k^2\theta(1-\theta)(\nabla u^{n-1},\nabla \varphi)
  +
  k(v^{n-1},\varphi)
  + k^2\theta
  \left[
  \theta (f^n,\varphi) + (1-\theta) (f^{n-1},\varphi)
  \right].
  \f}
  Now, note that we solve for $u^n$ on mesh ${\mathbb T}^n$, and
  consequently the test functions $\varphi$ have to be from the space
  $V_h^n$ as well. As discussed in the introduction, terms like
  $(u^{n-1},\varphi)$ then require us to integrate the solution of the
  previous step (which may have been computed on a different mesh
  ${\mathbb T}^{n-1}$) against the test functions of the current mesh,
  leading to a matrix $M^{n,n-1}$. This process of integrating shape
  functions from different meshes is, at best, awkward. It can be done
  but because it is difficult to ensure that ${\mathbb T}^{n-1}$ and
  ${\mathbb T}^{n}$ differ by at most one level of refinement, one
  has to recursively match cells from both meshes. It is feasible to
  do this, but it leads to lengthy and not entirely obvious code.

  The second approach is the following: whenever we change the mesh,
  we simply interpolate the solution from the last time step on the old
  mesh to the new mesh, using the SolutionTransfer class. In other words,
  instead of the equation above, we would solve
  \f{eqnarray*}
  (u^n,\varphi) + k^2\theta^2(\nabla u^n,\nabla \varphi) &=&
  (I^n u^{n-1},\varphi) - k^2\theta(1-\theta)(\nabla I^n u^{n-1},\nabla \varphi)
  +
  k(I^n v^{n-1},\varphi)
  + k^2\theta
  \left[
  \theta (f^n,\varphi) + (1-\theta) (f^{n-1},\varphi)
  \right],
  \f}
  where $I^n$ interpolates a given function onto mesh ${\mathbb T}^n$.
  This is a much simpler approach because, in each time step, we no
  longer have to worry whether $u^{n-1},v^{n-1}$ were computed on the
  same mesh as we are using now or on a different mesh. Consequently,
  the only changes to the code necessary are the addition of a function
  that computes the error, marks cells for refinement, sets up a
  SolutionTransfer object, transfers the solution to the new mesh, and
  rebuilds matrices and right hand side vectors on the new mesh. Neither
  the functions building the matrices and right hand sides, nor the
  solvers need to be changed.

  While this second approach is, strictly speaking,
  not quite correct in the Rothe framework (it introduces an addition source
  of error, namely the interpolation), it is nevertheless what
  almost everyone solving time dependent equations does. We will use this
  method in step-31, for example.
</ul>


examples/step-24/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

This program grew out of a student project by Xing Jin at Texas A&amp;M
University. Most of the work for this program is by her. Some of the work on
this tutorial program has been funded by NSF under grant DMS-0604778.

The program is part of a project that aims to simulate thermoacoustic
tomography imaging. In thermoacoustic tomography, pulsed electromagnetic
energy is delivered into biological issues. Tissues absorb some of this energy
and those parts of the tissue that absorb the most energy generate
thermoacoustic waves through thermoelastic expansion. For imaging, one uses
that different kinds of tissue, most importantly healthy and diseased tissue,
absorb different amounts of energy and therefore expand at different
rates. The experimental setup is to measure the amplitude of the pressure
waves generated by these sources on the surface of the tissue and try to
reconstruct the source distributions, which is indicative for the distribution
of absorbers and therefore of different kinds of tissue. Part of this project
is to compare simulated data with actual measurements, so one has to solve the
"forward problem", i.e. the wave equation that describes the propagation of
pressure waves in tissue. This program is therefore a continuation of @ref
step_23 "step-23", where the wave equation was first introduced.


<h3>The problem</h3>

The temperature at a given location, neglecting thermal diffusion, can be
stated as

@f[
\rho C_p \frac{\partial}{\partial t}T(t,\mathbf r) = H(t,\mathbf r)
@f]

Here $\rho (\mathbf r) $ is the density; $C_p (\mathbf r) $ is the specific
heat; $\frac{\partial T}{\partial t}(t,\mathbf r)$ is the temperature rise due
to the delivered microwave energy; and $H(t,\mathbf r)$ is the heating
function defined as the thermal energy per time and volume transformed from
deposited microwave energy.

Let us assume that tissues have heterogeneous dielectric properties but
homogeneous acoustic properties. The basic acoustic generation equation in an
acoustically homogeneous medium can be described as follows: if $u$ is the
vector-valued displacement, then tissue certainly reacts to changes in
pressure by acceleration:
@f[
\rho \frac{\partial^2}{\partial t^2}u(t,\mathbf r) =
-\nabla p(t,\mathbf r).
@f]
Furthermore, it contracts due to excess pressure and expands based on changes in temperature:
@f[
\nabla \cdot u(t,\mathbf r) = -\frac{p(t,\mathbf r)}{\rho c_0^2}+\beta T(t,\mathbf r) .
@f]
Here, $\beta$ is a thermoexpansion coefficient.

Let us now make the assumption that heating only happens on a time
scale much shorter than wave propagation through tissue (i.e. the temporal
length of the microwave pulse that heats the tissue is much shorter than the
time it takes a wave to cross the domain). In that case, the heating
rate $H(t,\mathbf r)$ can be written as $H(t,\mathbf r) = a(\mathbf
r)\delta(t)$ (where $a(\mathbf r)$ is a map of absorption strengths for
microwave energy and $\delta(t)$ is the Dirac delta function), which together
with the first equation above will yield
an instantaneous jump in the temperature $T(\mathbf r)$ at time $t=0$.
Using this assumption, and taking all equations together, we can
rewrite and combine the above as follows:
@f[
\Delta p-\frac{1}{c_0^2} \frac{\partial^2 p}{\partial t^2} = \lambda
a(\mathbf r)\frac{d\delta(t)}{dt}
@f]
where $\lambda = - \frac{\beta}{C_p}$.

This somewhat strange equation with the derivative of a Dirac delta function
on the right hand side can be rewritten as an initial value problem as follows:
@f{eqnarray*}
\Delta \bar{p}- \frac{1}{c_0^2} \frac{\partial^2 \bar{p}}{\partial t^2} & = &
0 \\
\bar{p}(0,\mathbf r) &=& c_0^2 \lambda a(\mathbf r) = b(\mathbf r)  \\
\frac{\partial\bar{p}(0,\mathbf r)}{\partial t} &=& 0.
@f}
(A derivation of this transformation into an initial value problem is given at
the end of this introduction as an appendix.)

In the inverse problem, it is the initial condition $b(\mathbf r) = c_0^2 \lambda a(\mathbf r)$ that
one would like to recover, since it is a map of absorption strengths for
microwave energy, and therefore presumably an indicator to discern healthy
from diseased tissue.

In real application, the thermoacoustic source is very small as compared to
the medium.  The propagation path of the thermoacoustic waves can then be
approximated as from the source to the infinity. Furthermore, detectors are
only a limited distance from the source. One only needs to evaluate the values
when the thermoacoustic waves pass through the detectors, although they do
continue beyond. This is therefore a problem where we are only interested in a
small part of an infinite medium, and we do not want waves generated somewhere
to be reflected at the boundary of the domain which we consider
interesting. Rather, we would like to simulate only that part of the wave
field that is contained inside the domain of interest, and waves that hit the
boundary of that domain to simply pass undisturbed through the boundary. In
other words, we would like the boundary to absorb any waves that hit it.

In general, this is a hard problem: Good absorbing boundary conditions are
nonlinear and/or numerically very expensive. We therefore opt for a simple
first order approximation to absorbing boundary conditions that reads
@f[
\frac{\partial\bar{p}}{\partial\mathbf n} =
-\frac{1}{c_0} \frac{\partial\bar{p}}{\partial t}
@f]
Here, $\frac{\partial\bar{p}}{\partial\mathbf n}$ is the normal derivative at
the boundary. It should be noted that this is not a particularly good boundary
condition, but it is one of the very few that are reasonably simple to implement.


<h3>Weak form and discretization</h3>

As in step-23, one first introduces a second variable, which is
defined as the derivative of the pressure potential:
@f[
v = \frac{\partial\bar{p}}{\partial t}
@f]

With the second variable, one then transforms the forward problem into
two separate equations:
@f{eqnarray*}
\bar{p}_{t} - v & = & 0 \\
\Delta\bar{p} - \frac{1}{c_0^2}\,v_{t} & = & f
@f}
with initial conditions:
@f{eqnarray*}
\bar{p}(0,\mathbf r) & = & b(r) \\
v(0,\mathbf r)=\bar{p}_t(0,\mathbf r) & = & 0.
@f}
Note that we have introduced a right hand side $f(t,\mathbf r)$ here to show
how to derive these formulas in the general case, although in the application
to the thermoacoustic problem $f=0$.

The semi-discretized, weak version of this model, using the general $\theta$ scheme
introduced in step-23 is then:
@f{eqnarray*}
\left(\frac{\bar{p}^n-\bar{p}^{n-1}}{k},\phi\right)_\Omega-
\left(\theta v^{n}+(1-\theta)v^{n-1},\phi\right)_\Omega & = & 0   \\
-\left(\nabla((\theta\bar{p}^n+(1-\theta)\bar{p}^{n-1})),\nabla\phi\right)_\Omega-
\frac{1}{c_0}\left(\frac{\bar{p}^n-\bar{p}^{n-1}}{k},\phi\right)_{\partial\Omega} -
\frac{1}{c_0^2}\left(\frac{v^n-v^{n-1}}{k},\phi\right)_\Omega & =
& \left(\theta f^{n}+(1-\theta)f^{n-1}, \phi\right)_\Omega,
@f}
where $\phi$ is an arbitrary test function, and where we have used the
absorbing boundary condition to integrate by parts:
absorbing boundary conditions are incorporated into the weak form by using
@f[
\int_\Omega\varphi \, \Delta p\; dx =
-\int_\Omega\nabla \varphi \cdot \nabla p dx +
\int_{\partial\Omega}\varphi \frac{\partial p}{\partial {\mathbf n}}ds.
@f]

From this we obtain the discrete model by introducing a finite number of shape
functions, and get
@f{eqnarray*}
M\bar{p}^{n}-k \theta M v^n & = & M\bar{p}^{n-1}+k (1-\theta)Mv^{n-1},\\

(-c_0^2k \theta A-c_0 B)\bar{p}^n-Mv^{n} & = &
(c_0^2k(1-\theta)A-c_0B)\bar{p}^{n-1}-Mv^{n-1}+c_0^2k(\theta F^{n}+(1-\theta)F^{n-1}).
@f}
The matrices $M$ and $A$ are here as in step-23, and the
boundary mass matrix
@f[
	B_{ij} = \left(\varphi_i,\varphi_j\right)_{\partial\Omega}
@f]
results from the use of absorbing boundary conditions.

Above two equations can be rewritten in a matrix form with the pressure and its derivative as
an unknown vector:
@f[
\left(\begin{array}{cc}
 M         &       -k\theta M \\
c_0^2\,k\,\theta\,A+c_0\,B  &  M   \\
               \end{array} \right)\\
\left(\begin{array}{c}
 \bar{p}^{n}    \\
 \bar{v}^{n}
              \end{array}\right)=\\
\left(\begin{array}{l}
 G_1  \\
 G_2 -(\theta F^{n}+(1-\theta)F ^{n-1})c_{0}^{2}k \\
                \end{array}\right)
@f]

where
@f[
\left(\begin{array}{c}
G_1 \\
G_2 \\
   \end{array} \right)=\\
\left(\begin{array}{l}
 M\bar{p}^{n-1}+k(1-\theta)Mv^{n-1}\\
 (-c_{0}^{2}k (1-\theta)A+c_0 B)\bar{p}^{n-1} +Mv^{n-1}
                \end{array}\right)
@f]

By simple transformations, one then obtains two equations for
the pressure potential and its derivative, just as in the previous tutorial program:
@f{eqnarray*}
(M+(k\,\theta\,c_{0})^{2}A+c_0k\theta B)\bar{p}^{n} & = &
G_{1}+(k\, \theta)G_{2}-(c_0k)^2\theta (\theta F^{n}+(1-\theta)F^{n-1}) \\
Mv^n & = & -(c_0^2\,k\, \theta\, A+c_0B)\bar{p}^{n}+ G_2 -
c_0^2k(\theta F^{n}+(1-\theta)F^{n-1})
@f}


<h3>What the program does</h3>

Compared to step-23, this programs adds the treatment of a
simple absorbing boundary conditions. In addition, it deals with data obtained
from actual experimental measurements. To this end, we need to evaluate the
solution at points at which the experiment also evaluates a real pressure
field. We will see how to do that using the VectorTools::point_value function
further down below.



<h3>Appendix: PDEs with Dirac delta functions as right hand side and their transformation to an initial value problem</h3>

In the derivation of the initial value problem for the wave equation, we
initially found that the equation had the derivative of a Dirac delta function
as a right hand side:
@f[
\Delta p-\frac{1}{c_0^2} \frac{\partial^2 p}{\partial t^2} = \lambda
a(\mathbf r)\frac{d\delta(t)}{dt}.
@f]
In order to see how to transform this single equation into the usual statement
of a PDE with initial conditions, let us make the assumption that the
physically quite reasonable medium is at rest initially, i.e. $p(t,\mathbf
r)=\frac{\partial p(t,\mathbf r)}{\partial t}=0$ for $t<0$. Next, let us form
the indefinite integral with respect to time of both sides:
@f[
\int^t \Delta p\; dt -\int^t \frac{1}{c_0^2} \frac{\partial^2 p}{\partial t^2}
\; dt
=
\int^t \lambda a(\mathbf r)\frac{d\delta(t)}{dt} \;dt.
@f]
This immediately leads to the statement
@f[
P(t,\mathbf r) - \frac{1}{c_0^2} \frac{\partial p}{\partial t}
=
\lambda a(\mathbf r) \delta(t),
@f]
where $P(t,\mathbf r)$ is such that $\frac{dP(t,\mathbf r)}{dt}=\Delta
p$. Next, we form the (definite) integral over time from $t=-\epsilon$ to
$t=+\epsilon$ to find
@f[
\int_{-\epsilon}^{\epsilon} P(t,\mathbf r)\; dt
- \frac{1}{c_0^2} \left[ p(\epsilon,\mathbf r) - p(-\epsilon,\mathbf r) \right]
=
\int_{-\epsilon}^{\epsilon} \lambda a(\mathbf r) \delta(t) \; dt.
@f]
If we use the property of the delta function that $\int_{-\epsilon}^{\epsilon}
\delta(t)\; dt = 1$, and assume that $P$ is a continuous function in time, we find
as we let $\epsilon$ go to zero that
@f[
- \lim_{\epsilon\rightarrow 0}\frac{1}{c_0^2} \left[ p(\epsilon,\mathbf r) - p(-\epsilon,\mathbf r) \right]
=
\lambda a(\mathbf r).
@f]
In other words, using that $p(-\epsilon,\mathbf r)=0$, we retrieve the initial
condition
@f[
  \frac{1}{c_0^2} p(0,\mathbf r)
  =
  \lambda a(\mathbf r).
@f]
At the same time, we know that for every $t>0$ the delta function is zero, so
for $0<t<T$ we get the equation
@f[
\Delta p-\frac{1}{c_0^2} \frac{\partial^2 p}{\partial t^2} = 0.
@f]
Consequently, we have obtained a representation of the wave equation and one
initial condition from the original somewhat strange equation.

Finally, because we here have an equation with two time derivatives, we still
need a second initial condition. To this end, let us go back to the equation
@f[
\Delta p-\frac{1}{c_0^2} \frac{\partial^2 p}{\partial t^2} = \lambda
a(\mathbf r)\frac{d\delta(t)}{dt}.
@f]
and integrate it in time from $t=-\epsilon$ to $t=+\epsilon$. This leads to
@f[
P(\epsilon)-P(-\epsilon)
-\frac{1}{c_0^2} \left[\frac{\partial p(\epsilon)}{\partial t} -
                       \frac{\partial p(-\epsilon)}{\partial t}\right]
 = \lambda a(\mathbf r) \int_{-\epsilon}^{\epsilon}\frac{d\delta(t)}{dt} \; dt.
@f]
Using integration by parts of the form
@f[
  \int_{-\epsilon}^{\epsilon}\varphi(t)\frac{d\delta(t)}{dt} \; dt
  =
  -\int_{-\epsilon}^{\epsilon}\frac{d\varphi(t)}{dt} \delta(t)\; dt
@f]
where we use that $\delta(\pm \epsilon)=0$ and inserting $\varphi(t)=1$, we
see that in fact
@f[
  \int_{-\epsilon}^{\epsilon}\frac{d\delta(t)}{dt} \; dt
  =
  0.
@f]

Now, let $\epsilon\rightarrow 0$. Assuming that $P$ is a continuous function in
time, we see that
@f[
  P(\epsilon)-P(-\epsilon) \rightarrow 0,
@f]
and consequently
@f[
  \frac{\partial p(\epsilon)}{\partial t} -
                       \frac{\partial p(-\epsilon)}{\partial t}
		       \rightarrow 0.
@f]
However, we have assumed that $\frac{\partial p(-\epsilon)}{\partial t}=0$.
Consequently, we obtain as the second initial condition that
@f[
  \frac{\partial p(0)}{\partial t} = 0,
@f]
completing the system of equations.


examples/step-24/doc/results.dox
<h1>Results</h1>

The program writes both graphical data for each time step as well as the
values evaluated at each detector location to disk. We then
draw them in plots. Experimental data were also collected for comparison.
Currently our experiments have only been done in two dimensions by
circularly scanning a single detector. The tissue sample here is a thin slice
in the $X-Y$ plane ($Z=0$), and we assume that signals from other $Z$
directions won't contribute to the data. Consequently, we only have to compare
our experimental data with two dimensional simulated data.

<h3> One absorber </h3>

This movie shows the thermoacoustic waves generated by a single small absorber
propagating in the medium (in our simulation, we assume the medium is mineral
oil, which has a acoustic speed of 1.437 $\frac{mm}{\mu s}$):

<img src="https://www.dealii.org/images/steps/developer/step-24.one_movie.gif" alt="">

For a single absorber, we of course have to change the
<code>InitialValuesP</code> class accordingly.

Next, let us compare experimental and computational results. The visualization
uses a technique long used in seismology, where the data of each detector is
plotted all in one graph. The way this is done is by offsetting each
detector's signal a bit compared to the previous one. For example, here is a
plot of the first four detectors (from bottom to top, with time in
microseconds running from left to right) using the source setup used in the
program, to make things a bit more interesting compared to the present case of
only a single source:

<img src="https://www.dealii.org/images/steps/developer/step-24.traces.png" alt="">

One thing that can be seen, for example, is that the arrival of the second and
fourth signals shifts to earlier times for greater detector numbers (i.e. the
topmost ones), but not the first and the third; this can be interpreted to
mean that the origin of these signals must be closer to the latter detectors
than to the former ones.

If we stack not only 4, but all 160 detectors in one graph, the individual
lines blur, but where they run together they create a pattern of darker or
lighter grayscales.  The following two figures show the results obtained at
the detector locations stacked in that way. The left figure is obtained from
experiments, and the right is the simulated data.
In the experiment, a single small strong absorber was embedded in
weaker absorbing tissue:

<table width="100%">
<tr>
<td>
<img src="https://www.dealii.org/images/steps/developer/step-24.one.png" alt="">
</td>
<td>
<img src="https://www.dealii.org/images/steps/developer/step-24.one_s.png" alt="">
</td>
</tr>
</table>

It is obvious that the source location is closer to the detectors at angle
$180^\circ$. All the other signals that can be seen in the experimental data
result from the fact that there are weak absorbers also in the rest of the
tissue, which surrounds the signals generated by the small strong absorber in
the center. On the other hand, in the simulated data, we only simulate the
small strong absorber.

In reality, detectors have limited bandwidth. The thermoacoustic waves passing
through the detector will therefore be filtered. By using a high-pass filter
(implemented in MATLAB and run against the data file produced by this program),
the simulated results can be made to look closer to the experimental
data:

<img src="https://www.dealii.org/images/steps/developer/step-24.one_sf.png" alt="">

In our simulations, we see spurious signals behind the main wave that
result from numerical artifacts. This problem can be alleviated by using finer
mesh, resulting in the following plot:

<img src="https://www.dealii.org/images/steps/developer/step-24.one_s2.png" alt="">



<h3>Multiple absorbers</h3>

To further verify the program, we will also show simulation results for
multiple absorbers. This corresponds to the case that is actually implemented
in the program. The following movie shows the propagation of the generated
thermoacoustic waves in the medium by multiple absorbers:

<img src="https://www.dealii.org/images/steps/developer/step-24.multi_movie.gif" alt="">

Experimental data and our simulated data are compared in the following two
figures:
<table width="100%">
<tr>
<td>
<img src="https://www.dealii.org/images/steps/developer/step-24.multi.png" alt="">
</td>
<td>
<img src="https://www.dealii.org/images/steps/developer/step-24.multi_s.png" alt="">
</td>
</tr>
</table>

Note that in the experimental data, the first signal (i.e. the left-most dark
line) results from absorption at the tissue boundary, and therefore reaches
the detectors first and before any of the signals from the interior. This
signal is also faintly visible at the end of the traces, around 30 $\mu s$,
which indicates that the signal traveled through the entire tissue to reach
detectors at the other side, after all the signals originating from the
interior have reached them.

As before, the numerical result better matches experimental ones by applying a
bandwidth filter that matches the actual behavior of detectors (left) and by
choosing a finer mesh (right):

<table width="100%">
<tr>
<td>
<img src="https://www.dealii.org/images/steps/developer/step-24.multi_sf.png" alt="">
</td>
<td>
<img src="https://www.dealii.org/images/steps/developer/step-24.multi_s2.png" alt="">
</td>
</tr>
</table>

One of the important differences between the left and the right figure is that
the curves look much less "angular" at the right. The angularity comes from
the fact that while waves in the continuous equation travel equally fast in
all directions, this isn't the case after discretization: there, waves that
travel diagonal to cells move at slightly different speeds to those that move
parallel to mesh lines. This anisotropy leads to wave fronts that aren't
perfectly circular (and would produce sinusoidal signals in the stacked
plots), but are bulged out in certain directions. To make things worse, the
circular mesh we use (see for example step-6 for a view of the
coarse mesh) is not isotropic either. The net result is that the signal fronts
are not sinusoidal unless the mesh is sufficiently fine. The right image is a
lot better in this respect, though artifacts in the form of trailing spurious
waves can still be seen.


examples/step-25/doc/intro.dox
<a name="Intro"></a> <h1>Introduction</h1>

This program grew out of a student project by Ivan Christov at Texas A&amp;M
University. Most of the work for this program is by him.

The goal of this program is to solve the sine-Gordon soliton equation
in 1, 2 or 3 spatial dimensions. The motivation for solving this
equation is that very little is known about the nature of the
solutions in 2D and 3D, even though the 1D case has been studied
extensively.

Rather facetiously, the sine-Gordon equation's moniker is a pun on the
so-called Klein-Gordon equation, which is a relativistic version of
the Schrödinger equation for particles with non-zero mass. The resemblance is not just
superficial, the sine-Gordon equation has been shown to model some
unified-field phenomena such as interaction of subatomic particles
(see, e.g., Perring &amp; Skyrme in Nuclear %Physics <b>31</b>) and the
Josephson (quantum) effect in superconductor junctions (see, e.g., <a
href="http://en.wikipedia.org/wiki/Long_Josephson_junction">http://en.wikipedia.org/wiki/Long_Josephson_junction</a>).
Furthermore, from the mathematical standpoint, since the sine-Gordon
equation is "completely integrable," it is a candidate for study using
the usual methods such as the inverse scattering
transform. Consequently, over the years, many interesting
solitary-wave, and even stationary, solutions to the sine-Gordon
equation have been found. In these solutions, particles correspond to
localized features. For more on the sine-Gordon equation, the
inverse scattering transform and other methods for finding analytical
soliton equations, the reader should consult the following "classical"
references on the subject: G. L. Lamb's <i>Elements of Soliton
Theory</i> (Chapter 5, Section 2) and G. B. Whitham's <i>Linear and
Nonlinear Waves</i> (Chapter 17, Sections 10-13).

@note We will cover a separate nonlinear equation from quantum
  mechanics, the Nonlinear Schr&ouml;dinger Equation, in step-58.

<h3>Statement of the problem</h3>
The sine-Gordon initial-boundary-value problem (IBVP) we wish to solve
consists of the following equations:
\f{eqnarray*}
  u_{tt}-\Delta u &=& -\sin(u) \quad\mbox{for}\quad (x,t) \in \Omega \times (t_0,t_f],\\
  {\mathbf n} \cdot \nabla u &=& 0 \quad\mbox{for}\quad (x,t) \in \partial\Omega
           \times (t_0,t_f],\\
  u(x,t_0) &=& u_0(x).
\f}
It is a nonlinear equation similar to the wave equation we
discussed in step-23 and step-24.
We have chosen to enforce zero Neumann boundary conditions in order for waves
to reflect off the boundaries of our domain. It should be noted, however, that
Dirichlet boundary conditions are not appropriate for this problem. Even
though the solutions to the sine-Gordon equation are localized, it only makes
sense to specify (Dirichlet) boundary conditions at $x=\pm\infty$, otherwise
either a solution does not exist or only the trivial solution $u=0$ exists.

However, the form of the equation above is not ideal for numerical
discretization. If we were to discretize the second-order time
derivative directly and accurately, then  we would need a large
stencil (i.e., several time steps would need to be kept in the
memory), which could become expensive. Therefore, in complete analogy
to what we did in step-23 and step-24,
we split the
second-order (in time) sine-Gordon equation into a system of two
first-order (in time) equations, which we call the split, or velocity,
formulation. To this end, by setting $v = u_t$, it is easy to see that the sine-Gordon equation is equivalent to
\f{eqnarray*}
  u_t - v &=& 0,\\
  v_t - \Delta u &=& -\sin(u).
\f}

<h3>Discretization of the equations in time</h3>
Now, we can discretize the split formulation in time using the
$\theta$-method, which has a stencil of only two time steps. By
choosing a $\theta\in [0,1]$, the latter discretization allows us to
choose from a continuum of schemes. In particular, if we pick
$\theta=0$ or $\theta=1$, we obtain the first-order accurate explicit
or implicit Euler method, respectively. Another important choice is
$\theta=\frac{1}{2}$, which gives the second-order accurate
Crank-Nicolson scheme. Henceforth, a superscript $n$ denotes the
values of the variables at the $n^{\mathrm{th}}$ time step, i.e. at
$t=t_n \dealcoloneq n k$, where $k$ is the (fixed) time step size. Thus,
the split formulation of the time-discretized sine-Gordon equation becomes
\f{eqnarray*}
  \frac{u^n - u^{n-1}}{k} - \left[\theta v^n + (1-\theta) v^{n-1}\right] &=& 0,\\
  \frac{v^n - v^{n-1}}{k} - \Delta\left[\theta u^n + (1-\theta) u^{n-1}\right]
  &=& -\sin\left[\theta u^n + (1-\theta) u^{n-1}\right].
\f}

We can simplify the latter via a bit of algebra. Eliminating $v^n$ from the first equation and rearranging, we obtain
\f{eqnarray*}
  \left[ 1-k^2\theta^2\Delta \right] u^n &=&
         \left[ 1+k^2\theta(1-\theta)\Delta\right] u^{n-1} + k v^{n-1}
         - k^2\theta\sin\left[\theta u^n + (1-\theta) u^{n-1}\right],\\
   v^n &=& v^{n-1} + k\Delta\left[ \theta u^n + (1-\theta) u^{n-1}\right]
         - k\sin\left[ \theta u^n + (1-\theta) u^{n-1} \right].
\f}

It may seem as though we can just proceed to discretize the equations
in space at this point. While this is true for the second equation
(which is linear in $v^n$), this would not work for all $\theta$ since the
first equation above is nonlinear. Therefore, a nonlinear solver must be
implemented, then the equations can be discretized in space and solved.

To this end, we can use Newton's method. Given the nonlinear equation $F(u^n) = 0$, we produce successive approximations to $u^n$ as follows:
\f{eqnarray*}
  \mbox{ Find } \delta u^n_l \mbox{ s.t. } F'(u^n_l)\delta u^n_l = -F(u^n_l)
  \mbox{, set }  u^n_{l+1} = u^n_l + \delta u^n_l.
\f}
The iteration can be initialized with the old time step, i.e. $u^n_0 = u^{n-1}$,
and eventually it will produce a solution to the first equation of
the split formulation (see above). For the time discretization of the
sine-Gordon equation under consideration here, we have that
\f{eqnarray*}
  F(u^n_l) &=&  \left[ 1-k^2\theta^2\Delta \right] u^n_l -
                 \left[ 1+k^2\theta(1-\theta)\Delta\right] u^{n-1} - k v^{n-1}
                 + k^2\theta\sin\left[\theta u^n_l + (1-\theta) u^{n-1}\right],\\
  F'(u^n_l) &=& 1-k^2\theta^2\Delta + k^2\theta^2\cos\left[\theta u^n_l
                        + (1-\theta) u^{n-1}\right].
\f}
Notice that while $F(u^n_l)$ is a function, $F'(u^n_l)$ is an operator.

<h3>Weak formulation of the time-discretized equations</h3>
With hindsight, we choose both the solution and the test space to be $H^1(\Omega)$. Hence, multiplying by a test function $\varphi$ and integrating, we obtain the following variational (or weak) formulation of the split formulation (including the nonlinear solver for the first equation) at each time step:
\f{eqnarray*}
  &\mbox{ Find}& \delta u^n_l \in H^1(\Omega) \mbox{ s.t. }
  \left( F'(u^n_l)\delta u^n_l, \varphi \right)_{\Omega}
  = -\left(F(u^n_l), \varphi \right)_{\Omega} \;\forall\varphi\in H^1(\Omega),
  \mbox{ set } u^n_{l+1} = u^n_l + \delta u^n_l,\; u^n_0 = u^{n-1}.\\
  &\mbox{ Find}& v^n \in H^1(\Omega) \mbox{ s.t. }
  \left( v^n, \varphi \right)_{\Omega} = \left( v^{n-1}, \varphi \right)_{\Omega}
         - k\theta\left( \nabla u^n, \nabla\varphi \right)_{\Omega}
         - k (1-\theta)\left( \nabla u^{n-1}, \nabla\varphi \right)_{\Omega}
         - k\left(\sin\left[ \theta u^n + (1-\theta) u^{n-1} \right],
         \varphi \right)_{\Omega} \;\forall\varphi\in H^1(\Omega).
\f}
Note that the we have used integration by parts and the zero Neumann
boundary conditions on all terms involving the Laplacian
operator. Moreover, $F(\cdot)$ and $F'(\cdot)$ are as defined above,
and $(\cdot,\cdot)_{\Omega}$ denotes the usual $L^2$ inner product
over the domain $\Omega$, i.e. $(f,g)_{\Omega} = \int_\Omega fg
\,\mathrm{d}x$. Finally, notice that the first equation is, in fact,
the definition of an iterative procedure, so it is solved multiple
times during each time step until a stopping criterion is met.

<h3>Discretization of the weak formulation in space</h3>
Using the Finite Element Method, we discretize the variational
formulation in space. To this end, let $V_h$ be a finite-dimensional
$H^1(\Omega)$-conforming finite element space ($\mathrm{dim}\, V_h = N
< \infty$) with nodal basis $\{\varphi_1,\ldots,\varphi_N\}$. Now,
we can expand all functions in the weak formulation (see above) in
terms of the nodal basis. Henceforth, we shall denote by a capital
letter the vector of coefficients (in the nodal basis) of a function
denoted by the same letter in lower case; e.g., $u^n = \sum_{i=1}^N
U^n_i \varphi_i$ where $U^n \in {R}^N$ and $u^n \in
H^1(\Omega)$. Thus, the finite-dimensional version of the variational formulation requires that we solve the following matrix equations at each time step:
@f{eqnarray*}
  F_h'(U^{n,l})\delta U^{n,l} &=& -F_h(U^{n,l}), \qquad
        U^{n,l+1} = U^{n,l} + \delta U^{n,l}, \qquad U^{n,0} = U^{n-1}; \\
  MV^n &=& MV^{n-1} - k \theta AU^n -k (1-\theta) AU^{n-1} - k S(u^n,u^{n-1}).
@f}
Above, the matrix $F_h'(\cdot)$ and the vector $F_h(\cdot)$ denote the discrete versions of the gadgets discussed above, i.e.,
\f{eqnarray*}
  F_h(U^{n,l}) &=&  \left[ M+k^2\theta^2A \right] U^{n,l} -
                \left[ M-k^2\theta(1-\theta)A \right] U^{n-1} - k MV^{n-1}
                + k^2\theta S(u^n_l, u^{n-1}),\\
  F_h'(U^{n,l}) &=& M+k^2\theta^2A
                                + k^2\theta^2N(u^n_l,u^{n-1})
\f}
Again, note that the first matrix equation above is, in fact, the
definition of an iterative procedure, so it is solved multiple times
until a stopping criterion is met. Moreover, $M$ is the mass matrix,
i.e. $M_{ij} = \left( \varphi_i,\varphi_j \right)_{\Omega}$, $A$ is
the Laplace matrix, i.e. $A_{ij} = \left( \nabla \varphi_i, \nabla
\varphi_j \right)_{\Omega}$, $S$ is the nonlinear term in the
equation that defines our auxiliary velocity variable, i.e. $S_j(f,g) = \left(
  \sin\left[ \theta f + (1-\theta) g\right], \varphi_j \right)_{\Omega}$, and
$N$ is the nonlinear term in the Jacobian matrix of $F(\cdot)$,
i.e. $N_{ij}(f,g) = \left( \cos\left[ \theta f + (1-\theta) g\right]\varphi_i,
  \varphi_j \right)_{\Omega}$.

What solvers can we use for the first equation? Let's look at the matrix we
have to invert:
@f[
  (M+k^2\theta^2(A + N))_{ij} =
  \int_\Omega (1+k^2\theta^2 \cos \alpha)
  \varphi_i\varphi_j \; dx
  + k^2 \theta^2 \int_\Omega \nabla\varphi_i\nabla\varphi_j \; dx,
@f]
for some $\alpha$ that depends on the present and previous solution. First,
note that the matrix is symmetric. In addition, if the time step $k$ is small
enough, i.e. if $k\theta<1$, then the matrix is also going to be positive
definite. In the program below, this will always be the case, so we will use
the Conjugate Gradient method together with the SSOR method as
preconditioner. We should keep in mind, however, that this will fail
if we happen to use a bigger time step. Fortunately, in that case
the solver will just throw an exception indicating a failure to converge,
rather than silently producing a wrong result. If that happens, then we can
simply replace the CG method by something that can handle indefinite symmetric
systems. The GMRES solver is typically the standard method for all "bad"
linear systems, but it is also a slow one. Possibly better would be a solver
that utilizes the symmetry, such as, for example, SymmLQ, which is also
implemented in deal.II.

This program uses a clever optimization over step-23 and @ref
step_24 "step-24": If you read the above formulas closely, it becomes clear
that the velocity $V$ only ever appears in products with the mass matrix. In
step-23 and step-24, we were, therefore, a bit
wasteful: in each time step, we would solve a linear system with the mass
matrix, only to multiply the solution of that system by $M$ again in the next
time step. This can, of course, be avoided, and we do so in this program.


<h3>The test case</h3>

There are a few analytical solutions for the sine-Gordon equation, both in 1D
and 2D. In particular, the program as is computes the solution to a problem
with a single kink-like solitary wave initial condition.  This solution is
given by Leibbrandt in \e Phys. \e Rev. \e Lett. \b 41(7), and is implemented
in the <code>ExactSolution</code> class.

It should be noted that this closed-form solution, strictly speaking, only holds
for the infinite-space initial-value problem (not the Neumann
initial-boundary-value problem under consideration here). However, given that
we impose \e zero Neumann boundary conditions, we expect that the solution to
our initial-boundary-value problem would be close to the solution of the
infinite-space initial-value problem, if reflections of waves off the
boundaries of our domain do \e not occur. In practice, this is of course not
the case, but we can at least assume that this were so.

The constants $\vartheta$ and $\lambda$ in the 2D solution and $\vartheta$,
$\phi$ and $\tau$ in the 3D solution are called the B&auml;cklund
transformation parameters. They control such things as the orientation and
steepness of the kink. For the purposes of testing the code against the exact
solution, one should choose the parameters so that the kink is aligned with
the grid.

The solutions that we implement in the <code>ExactSolution</code> class are
these:
<ul>
  <li>In 1D:
  @f[
  u(x,t) =
  -4 \arctan\left[
     \frac{m}{\sqrt{1-m^2}}
     \frac{\sin\left(\sqrt{1-m^2}t+c_2\right)}
     {\cosh\left(mx+c_1\right)}
     \right],
  @f]
  where we choose $m=\frac 12, c_1=c_2=0$.

  In 1D, more interesting analytical solutions are known. Many of them are
  listed on http://mathworld.wolfram.com/Sine-GordonEquation.html .

  <li>In 2D:
  @f[
    u(x,y,t) = 4 \arctan \left[a_0 e^{s\xi}\right],
  @f]
  where $\xi$ is defined as
  @f[
    \xi = x \cos\vartheta + \sin(\vartheta) (y\cosh\lambda + t\sinh \lambda),
  @f]
  and where we choose $\vartheta=\frac \pi 4, \lambda=a_0=s=1$.

  <li>In 3D:
  @f[
    u(x,y,z,t) = 4 \arctan \left[c_0 e^{s\xi}\right],
  @f]
  where $\xi$ is defined as
  @f[
    \xi = x \cos\vartheta + y \sin \vartheta \cos\phi +
          \sin \vartheta \sin\phi (z\cosh\tau + t\sinh \tau),
  @f]
  and where we choose $\vartheta=\phi=\frac{\pi}{4}, \tau=c_1=s=1$.
</ul>


Since it makes it easier to play around, the <code>InitialValues</code> class
that is used to set &mdash; surprise! &mdash; the initial values of our
simulation simply queries the class that describes the exact solution for the
value at the initial time, rather than duplicating the effort to implement a
solution function.


examples/step-25/doc/results.dox
<h1>Results</h1>
The explicit Euler time stepping scheme  ($\theta=0$) performs adequately for the problems we wish to solve. Unfortunately, a rather small time step has to be chosen due to stability issues --- $k\sim h/10$ appears to work for most the simulations we performed. On the other hand, the Crank-Nicolson scheme ($\theta=\frac{1}{2}$) is unconditionally stable, and (at least for the case of the 1D breather) we can pick the time step to be as large as $25h$ without any ill effects on the solution. The implicit Euler scheme ($\theta=1$) is "exponentially damped," so it is not a good choice for solving the sine-Gordon equation, which is conservative. However, some of the damped schemes in the continuum that is offered by the $\theta$-method were useful for eliminating spurious oscillations due to boundary effects.

In the simulations below, we solve the sine-Gordon equation on the interval $\Omega =
[-10,10]$ in 1D and on the square $\Omega = [-10,10]\times [-10,10]$ in 2D. In
each case, the respective grid is refined uniformly 6 times, i.e. $h\sim
2^{-6}$.

<h3>An (1+1)-d Solution</h3>
The first example we discuss is the so-called 1D (stationary) breather
solution of the sine-Gordon equation. The breather has the following
closed-form expression, as mentioned in the Introduction:
\f[
u_{\mathrm{breather}}(x,t) = -4\arctan \left(\frac{m}{\sqrt{1-m^2}} \frac{\sin\left(\sqrt{1-m^2}t +c_2\right)}{\cosh(mx+c_1)} \right),
\f]
where $c_1$, $c_2$ and $m<1$ are constants. In the simulation below, we have chosen $c_1=0$, $c_2=0$, $m=0.5$. Moreover, it is know that the period of oscillation of the breather is $2\pi\sqrt{1-m^2}$, hence we have chosen $t_0=-5.4414$ and $t_f=2.7207$ so that we can observe three oscillations of the solution. Then, taking $u_0(x) = u_{\mathrm{breather}}(x,t_0)$, $\theta=0$ and $k=h/10$, the program computed the following solution.

<img src="https://www.dealii.org/images/steps/developer/step-25.1d-breather.gif" alt="Animation of the 1D stationary breather.">

Though not shown how to do this in the program, another way to visualize the
(1+1)-d solution is to use output generated by the DataOutStack class; it
allows to "stack" the solutions of individual time steps, so that we get
2D space-time graphs from 1D time-dependent
solutions. This produces the space-time plot below instead of the animation
above.

<img src="https://www.dealii.org/images/steps/developer/step-25.1d-breather_stp.png" alt="A space-time plot of the 1D stationary breather.">

Furthermore, since the breather is an analytical solution of the sine-Gordon
equation, we can use it to validate our code, although we have to assume that
the error introduced by our choice of Neumann boundary conditions is small
compared to the numerical error. Under this assumption, one could use the
VectorTools::integrate_difference function to compute the difference between
the numerical solution and the function described by the
<code>ExactSolution</code> class of this program. For the
simulation shown in the two images above, the $L^2$ norm of the error in the
finite element solution at each time step remained on the order of
$10^{-2}$. Hence, we can conclude that the numerical method has been
implemented correctly in the program.


<h3>A few (2+1)D Solutions</h3>

The only analytical solution to the sine-Gordon equation in (2+1)D that can be found in the literature is the so-called kink solitary wave. It has the following closed-form expression:
  @f[
    u(x,y,t) = 4 \arctan \left[a_0 e^{s\xi}\right]
  @f]
with
  @f[
    \xi = x \cos\vartheta + \sin(\vartheta) (y\cosh\lambda + t\sinh \lambda)
  @f]
where $a_0$, $\vartheta$ and $\lambda$ are constants. In the simulation below
we have chosen $a_0=\lambda=1$. Notice that if $\vartheta=\pi$ the kink is
stationary, hence it would make a good solution against which we can
validate the program in 2D because no reflections off the boundary of the
domain occur.

The simulation shown below was performed with $u_0(x) = u_{\mathrm{kink}}(x,t_0)$, $\theta=\frac{1}{2}$, $k=20h$, $t_0=1$ and $t_f=500$. The $L^2$ norm of the error of the finite element solution at each time step remained on the order of $10^{-2}$, showing that the program is working correctly in 2D, as well as 1D. Unfortunately, the solution is not very interesting, nonetheless we have included a snapshot of it below for completeness.

<img src="https://www.dealii.org/images/steps/developer/step-25.2d-kink.png" alt="Stationary 2D kink.">

Now that we have validated the code in 1D and 2D, we move to a problem where the analytical solution is unknown.

To this end, we rotate the kink solution discussed above about the $z$
axis: we let  $\vartheta=\frac{\pi}{4}$. The latter results in a
solitary wave that is not aligned with the grid, so reflections occur
at the boundaries of the domain immediately. For the simulation shown
below, we have taken $u_0(x)=u_{\mathrm{kink}}(x,t_0)$,
$\theta=\frac{2}{3}$, $k=20h$, $t_0=0$ and $t_f=20$. Moreover, we had
to pick $\theta=\frac{2}{3}$ because for any $\theta\le\frac{1}{2}$
oscillations arose at the boundary, which are likely due to the scheme
and not the equation, thus picking a value of $\theta$ a good bit into
the "exponentially damped" spectrum of the time stepping schemes
assures these oscillations are not created.

<img src="https://www.dealii.org/images/steps/developer/step-25.2d-angled_kink.gif" alt="Animation of a moving 2D kink, at 45 degrees to the axes of the grid, showing boundary effects.">

Another interesting solution to the sine-Gordon equation (which cannot be
obtained analytically) can be produced by using two 1D breathers to construct
the following separable 2D initial condition:
\f[
  u_0(x) =
  u_{\mathrm{pseudobreather}}(x,t_0) =
  16\arctan \left(
    \frac{m}{\sqrt{1-m^2}}
    \frac{\sin\left(\sqrt{1-m^2}t_0\right)}{\cosh(mx_1)} \right)
  \arctan \left(
    \frac{m}{\sqrt{1-m^2}}
    \frac{\sin\left(\sqrt{1-m^2}t_0\right)}{\cosh(mx_2)} \right),
\f]
where $x=(x_1,x_2)\in{R}^2$, $m=0.5<1$ as in the 1D case we discussed
above. For the simulation shown below, we have chosen $\theta=\frac{1}{2}$,
$k=10h$, $t_0=-5.4414$ and $t_f=2.7207$. The solution is pretty interesting
--- it acts like a breather (as far as the pictures are concerned); however,
it appears to break up and reassemble, rather than just oscillate.

<img src="https://www.dealii.org/images/steps/developer/step-25.2d-pseudobreather.gif" alt="Animation of a 2D pseudobreather.">


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

It is instructive to change the initial conditions. Most choices will not lead
to solutions that stay localized (in the soliton community, such
solutions are called "stationary", though the solution does change
with time), but lead to solutions where the wave-like
character of the equation dominates and a wave travels away from the location
of a localized initial condition. For example, it is worth playing around with
the <code>InitialValues</code> class, by replacing the call to the
<code>ExactSolution</code> class by something like this function:
@f[
  u_0(x,y) = \cos\left(\frac x2\right)\cos\left(\frac y2\right)
@f]
if $|x|,|y|\le \frac\pi 2$, and $u_0(x,y)=0$ outside this region.

A second area would be to investigate whether the scheme is
energy-preserving. For the pure wave equation, discussed in @ref
step_23 "step-23", this is the case if we choose the time stepping
parameter such that we get the Crank-Nicolson scheme. One could do a
similar thing here, noting that the energy in the sine-Gordon solution
is defined as
@f[
  E(t) = \frac 12 \int_\Omega \left(\frac{\partial u}{\partial
  t}\right)^2
  + \left(\nabla u\right)^2 + 2 (1-\cos u) \; dx.
@f]
(We use $1-\cos u$ instead of $-\cos u$ in the formula to ensure that all
contributions to the energy are positive, and so that decaying solutions have
finite energy on unbounded domains.)

Beyond this, there are two obvious areas:

- Clearly, adaptivity (i.e. time-adaptive grids) would be of interest
  to problems like these. Their complexity leads us to leave this out
  of this program again, though the general comments in the
  introduction of @ref step_23 "step-23" remain true.

- Faster schemes to solve this problem. While computers today are
  plenty fast enough to solve 2d and, frequently, even 3d stationary
  problems within not too much time, time dependent problems present
  an entirely different class of problems. We address this topic in
  step-48 where we show how to solve this problem in parallel and
  without assembling or inverting any matrix at all.


examples/step-26/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

@dealiiVideoLecture{29,30}
(@dealiiVideoLectureSeeAlso{31.7})


This program implements the heat equation
@f{align*}
  \frac{\partial u(\mathbf x, t)}{\partial t}
  -
  \Delta u(\mathbf x, t)
  &=
  f(\mathbf x, t),
  \qquad\qquad &&
  \forall \mathbf x \in \Omega, t\in (0,T),
  \\
  u(\mathbf x, 0) &= u_0(\mathbf x) &&
  \forall \mathbf x \in \Omega, \\
  \\
  u(\mathbf x, t) &= g(\mathbf x,t) &&
  \forall \mathbf x \in \partial\Omega, t \in (0,T).
@f}
In some sense, this equation is simpler than the ones we have discussed in the
preceding programs step-23, step-24, step-25, namely the wave equation. This
is due to the fact that the heat equation smoothes out the solution over time,
and is consequently more forgiving in many regards. For example, when using
implicit time stepping methods, we can actually take large time steps, we have
less trouble with the small disturbances we introduce through adapting the
mesh every few time steps, etc.

Our goal here will be to solve the equations above using the theta-scheme that
discretizes the equation in time using the following approach, where we would
like $u^n(\mathbf x)$ to approximate $u(\mathbf x, t_n)$ at some time $t_n$:
@f{align*}
  \frac{u^n(\mathbf x)-u^{n-1}(\mathbf x)}{k_n}
  -
  \left[
  (1-\theta)\Delta u^{n-1}(\mathbf x)
  +
  \theta\Delta u^n(\mathbf x)
  \right]
  &=
  \left[
  (1-\theta)f(\mathbf x, t_{n-1})
  +
  \theta f(\mathbf x, t_n)
  \right].
@f}
Here, $k_n=t_n-t_{n-1}$ is the time step size. The theta-scheme generalizes
the explicit Euler ($\theta=0$), implicit Euler ($\theta=1$) and
Crank-Nicolson ($\theta=\frac 12$) time discretizations. Since the latter has
the highest convergence order, we will choose $\theta=\frac 12$ in the program
below, but make it so that playing with this parameter remains simple. (If you
are interested in playing with higher order methods, take a look at step-52.)

Given this time discretization, space discretization happens as it always
does, by multiplying with test functions, integrating by parts, and then
restricting everything to a finite dimensional subspace. This yields the
following set of fully discrete equations after multiplying through with
$k_n$:
@f{align*}
  M U^n-MU^{n-1}
  +
  k_n \left[
  (1-\theta)A U^{n-1}
  +
  \theta A U^n
  \right]
  &=
  k_n
  \left[
  (1-\theta)F^{n-1}
  +
  \theta F^n
  \right],
@f}
where $M$ is the mass matrix and $A$ is the stiffness matrix that results from
discretizing the Laplacian. Bringing all known quantities to the right hand
side yields the linear system we have to solve in every step:
@f{align*}
  (M
  +
  k_n \theta A) U^n
  &=
  MU^{n-1}
  -
  k_n
  (1-\theta)A U^{n-1}
  +
  k_n
  \left[
  (1-\theta)F^{n-1}
  +
  \theta F^n
  \right].
@f}
The linear system on the left hand side is symmetric and positive definite, so
we should have no trouble solving it with the Conjugate Gradient method.

We can start the iteration above if we have the set of nodal coefficients
$U^0$ at the initial time. Here, we take the ones we get by interpolating the
initial values $u_0(\mathbf x)$ onto the mesh used for the first time step. We
will also need to choose a time step; we will here just choose it as fixed,
but clearly advanced simulators will want to choose it adaptively. We will
briefly come back to this in the <a href="#Results">results section
below</a>.


<h3> Adapting meshes for time dependent problems </h3>

When solving the wave equation and its variants in the previous few programs,
we kept the mesh fixed. Just as for stationary equations, one can make a good
case that this is not the smartest approach and that significant savings can
be had by adapting the mesh. There are, however, significant difficulties
compared to the stationary case. Let us go through them in turn:

<ul>
  <li><i>Time step size and minimal mesh size</i>: For stationary problems, the
  general approach is "make the mesh as fine as it is necessary". For problems
  with singularities, this often leads to situations where we get many levels
  of refinement into corners or along interfaces. The very first tutorial to
  use adaptive meshes, step-6, is a point in case already.

  However, for time dependent problems, we typically need to choose the time
  step related to the mesh size. For explicit time discretizations, this is
  obvious, since we need to respect a CFL condition that ties the time step
  size to the smallest mesh size. For implicit time discretizations, no such
  hard restriction exists, but in practice we still want to make the time step
  smaller if we make the mesh size smaller since we typically have error
  estimates of the form $\|e\| \le {\cal O}(k^p + h^q)$ where $p,q$ are the
  convergence orders of the time and space discretization, respectively. We
  can only make the error small if we decrease both terms. Ideally, an
  estimate like this would suggest to choose $k \propto h^{q/p}$. Because, at
  least for problems with non-smooth solutions, the error is typically
  localized in the cells with the smallest mesh size, we have to indeed choose
  $k \propto h_{\text{min}}^{q/p}$, using the <i>smallest</i> mesh size.

  The consequence is that refining the mesh further in one place implies not
  only the moderate additional effort of increasing the number of degrees of
  freedom slightly, but also the much larger effort of having the solve the
  <i>global</i> linear system more often because of the smaller time step.

  In practice, one typically deals with this by acknowledging that we can not
  make the time step arbitrarily small, and consequently can not make the
  local mesh size arbitrarily small. Rather, we set a maximal level of
  refinement and when we flag cells for refinement, we simply do not refine
  those cells whose children would exceed this maximal level of refinement.

  There is a similar problem in that we will choose a right hand side that
  will switch on in different parts of the domain at different times. To avoid
  being caught flat footed with too coarse a mesh in areas where we suddenly
  need a finer mesh, we will also enforce in our program a <i>minimal</i> mesh
  refinement level.

  <li><i>Test functions from different meshes</i>: Let us consider again the
  semi-discrete equations we have written down above:
  @f{align*}
    \frac{u^n(\mathbf x)-u^{n-1}(\mathbf x)}{k_n}
    -
    \left[
    (1-\theta)\Delta u^{n-1}(\mathbf x)
    +
    \theta\Delta u^n(\mathbf x)
    \right]
    &=
    \left[
    (1-\theta)f(\mathbf x, t_{n-1})
    +
    \theta f(\mathbf x, t_n)
    \right].
  @f}
  We can here consider $u^{n-1}$ as data since it has presumably been computed
  before. Now, let us replace
  @f{align*}
    u^n(\mathbf x)\approx u_h^n(\mathbf x)
    =
    \sum_j U^n \varphi_j(\mathbf x),
  @f}
  multiply with test functions $\varphi_i(\mathbf x)$ and integrate by parts
  where necessary. In a process as outlined above, this would yield
  @f{align*}
    \sum_j
    (M
    +
    k_n \theta A)_{ij} U^n_j
    &=
    (\varphi_i, u_h^{n-1})
    -
    k_n
    (1-\theta)(\nabla \varphi_i, \nabla u_h^{n-1})
    +
    k_n
    \left[
    (1-\theta)F^{n-1}
    +
    \theta F^n
    \right].
  @f}
  Now imagine that we have changed the mesh between time steps $n-1$ and
  $n$. Then the problem is that the basis functions we use for $u_h^n$ and
  $u^{n-1}$ are different! This pertains to the terms on the right hand side,
  the first of which we could more clearly write as (the second follows the
  same pattern)
  @f{align*}
    (\varphi_i, u_h^{n-1})
    =
    (\varphi_i^n, u_h^{n-1})
    =
    \sum_{j=1}^{N_{n-1}}
    (\varphi_i^n, \varphi_j^{n-1}) U^{n-1}_j,
    \qquad\qquad
    i=1\ldots N_n.
  @f}
  If the meshes used in these two time steps are the same, then
  $(\varphi_i^n, \varphi_j^{n-1})$ forms a square mass matrix
  $M_{ij}$. However, if the meshes are not the same, then in general the matrix
  is rectangular. Worse, it is difficult to even compute these integrals
  because if we loop over the cells of the mesh at time step $n$, then we need
  to evaluate $\varphi_j^{n-1}$ at the quadrature points of these cells, but
  they do not necessarily correspond to the cells of the mesh at time step
  $n-1$ and $\varphi_j^{n-1}$ is not defined via these cells; the same of
  course applies if we wanted to compute the integrals via integration on the
  cells of mesh $n-1$.

  In any case, what we have to face is a situation where we need to integrate
  shape functions defined on two different meshes. This can be done, and is in
  fact demonstrated in step-28, but the process is at best described by the
  word "awkward".

  In practice, one does not typically want to do this. Rather, we avoid the
  whole situation by interpolating the solution from the old to the new mesh
  every time we adapt the mesh. In other words, rather than solving the
  equations above, we instead solve the problem
  @f{align*}
    \sum_j
    (M
    +
    k_n \theta A)_{ij} U^n_j
    &=
    (\varphi_i, I_h^n u_h^{n-1})
    -
    k_n
    (1-\theta)(\nabla \varphi_i, \nabla I_h^n u_h^{n-1})
    +
    k_n
    \left[
    (1-\theta)F^{n-1}
    +
    \theta F^n
    \right],
  @f}
  where $I_h^n$ is the interpolation operator onto the finite element space
  used in time step $n$. This is not the optimal approach since it introduces
  an additional error besides time and space discretization, but it is a
  pragmatic one that makes it feasible to do time adapting meshes.
</ul>



<h3> What could possibly go wrong? Verifying whether the code is correct </h3>

There are a number of things one can typically get wrong when implementing a
finite element code. In particular, for time dependent problems, the following
are common sources of bugs:
- The time integration, for example by getting the coefficients in front of
  the terms involving the current and previous time steps wrong (e.g., mixing
  up a factor $\theta$ for $1-\theta$).
- Handling the right hand side, for example forgetting a factor of $k_n$ or
  $\theta$.
- Mishandling the boundary values, again for example forgetting a factor of
  $k_n$ or $\theta$, or forgetting to apply nonzero boundary values not only
  to the right hand side but also to the system matrix.

A less common problem is getting the initial conditions wrong because one can
typically see that it is wrong by just outputting the first time step. In any
case, in order to verify the correctness of the code, it is helpful to have a
testing protocol that allows us to verify each of these components
separately. This means:
- Testing the code with nonzero initial conditions but zero right hand side
  and boundary values and verifying that the time evolution is correct.
- Then testing with zero initial conditions and boundary values but nonzero
  right hand side and again ensuring correctness.
- Finally, testing with zero initial conditions and right hand side but
  nonzero boundary values.

This sounds complicated, but fortunately, for linear partial differential
equations without coefficients (or constant coefficients) like the one here,
there is a fairly standard protocol that rests on the following observation:
if you choose as your domain a square $[0,1]^2$ (or, with slight
modifications, a rectangle), then the exact solution can be written as
@f{align*}
  u(x,y,t) = a(t) \sin(n_x \pi x) \sin(n_y \pi y)
@f}
(with integer constants $n_x,n_y$)
if only the initial condition, right hand side and boundary values are all
of the form $\sin(n_x \pi x) \sin(n_y \pi y)$ as well. This is due to the fact
that the function $\sin(n_x \pi x) \sin(n_y \pi y)$ is an eigenfunction of the
Laplace operator and allows us to compute things like the time factor $a(t)$
analytically and, consequently, compare with what we get numerically.

As an example, let us consider the situation where we have
$u_0(x,y)=\sin(n_x \pi x) \sin(n_x \pi y)$ and
$f(x,y,t)=0$. With the claim (ansatz) of the form for
$u(x,y,t)$ above, we get that
@f{align*}
  \left(\frac{\partial}{\partial t} -\Delta\right)
  u(x,y,t)
  &=
  \left(\frac{\partial}{\partial t} -\Delta\right)
  a(t) \sin(n_x \pi x) \sin(n_y \pi y)
  \\
  &=
  \left(a'(t) + (n_x^2+n_y^2)\pi^2 a(t) \right) \sin(n_x \pi x) \sin(n_y \pi y).
@f}
For this to be equal to $f(x,y,t)=0$, we need that
@f{align*}
  a'(t) + (n_x^2+n_y^2)\pi^2 a(t) = 0
@f}
and due to the initial conditions, $a(0)=1$. This differential equation can be
integrated to yield
@f{align*}
  a(t) = - e^{-(n_x^2+n_y^2)\pi^2 t}.
@f}
In other words, if the initial condition is a product of sines, then the
solution has exactly the same shape of a product of sines that decays to zero
with a known time dependence. This is something that is easy to test if you
have a sufficiently fine mesh and sufficiently small time step.

What is typically going to happen if you get the time integration scheme wrong
(e.g., by having the wrong factors of $\theta$ or $k$ in front of the various
terms) is that you don't get the right temporal behavior of the
solution. Double check the various factors until you get the right
behavior. You may also want to verify that the temporal decay rate (as
determined, for example, by plotting the value of the solution at a fixed
point) does not double or halve each time you double or halve the time step or
mesh size. You know that it's not the handling of the
boundary conditions or right hand side because these were both zero.

If you have so verified that the time integrator is correct, take the
situation where the right hand side is nonzero but the initial conditions are
zero: $u_0(x,y)=0$ and
$f(x,y,t)=\sin(n_x \pi x) \sin(n_x \pi y)$. Again,
@f{align*}
  \left(\frac{\partial}{\partial t} -\Delta\right)
  u(x,y,t)
  &=
  \left(\frac{\partial}{\partial t} -\Delta\right)
  a(t) \sin(n_x \pi x) \sin(n_y \pi y)
  \\
  &=
  \left(a'(t) + (n_x^2+n_y^2)\pi^2 a(t) \right) \sin(n_x \pi x) \sin(n_y \pi y),
@f}
and for this to be equal to $f(x,y,t)$, we need that
@f{align*}
  a'(t) + (n_x^2+n_y^2)\pi^2 a(t) = 1
@f}
and due to the initial conditions, $a(0)=0$. Integrating this equation in time
yields
@f{align*}
  a(t) = \frac{1}{(n_x^2+n_y^2)\pi^2} \left[ 1 - e^{-(n_x^2+n_y^2)\pi^2 t} \right].
@f}

Again, if you have the wrong factors of $\theta$ or $k$ in front of the right
hand side terms you will either not get the right temporal behavior of the
solution, or it will converge to a maximum value other than
$\frac{1}{(n_x^2+n_y^2)\pi^2}$.

Once we have verified that the time integration and right hand side handling
are correct using this scheme, we can go on to verifying that we have the
boundary values correct, using a very similar approach.



<h3> The testcase </h3>

Solving the heat equation on a simple domain with a simple right hand side
almost always leads to solutions that are exceedingly boring, since they
become very smooth very quickly and then do not move very much any
more. Rather, we here solve the equation on the L-shaped domain with zero
Dirichlet boundary values and zero initial conditions, but as right hand side
we choose
@f{align*}
  f(\mathbf x, t)
  =
  \left\{
  \begin{array}{ll}
    \chi_1(\mathbf x)
    & \text{if \(0\le t \le 0.2\tau\) or \(\tau\le t \le 1.2\tau\) or \(2\tau\le t
    \le 2.2\tau\), etc}
    \\
    \chi_2(\mathbf x)
    & \text{if \(0.5\le t \le 0.7\tau\) or \(1.5\tau\le t \le 1.7\tau\) or \(2.5\tau\le t
    \le 2.7\tau\), etc}
    \\
    0
    & \text{otherwise}
  \end{array}
  \right.
@f}
Here,
@f{align*}
  \chi_1(\mathbf x) &=
  \left\{
  \begin{array}{ll}
    1
    & \text{if \(x>0.5\) and \(y>-0.5\)}
    \\
    0
    & \text{otherwise}
  \end{array}
  \right.
  \\
  \chi_2(\mathbf x) &=
  \left\{
  \begin{array}{ll}
    1
    & \text{if \(x>-0.5\) and \(y>0.5\)}
    \\
    0
    & \text{otherwise}
  \end{array}
  \right.
@f}
In other words, in every period of length $\tau$, the right hand side first
flashes on in domain 1, then off completely, then on in domain 2, then off
completely again. This pattern is probably best observed via the little
animation of the solution shown in the <a href="#Results">results
section</a>.

If you interpret the heat equation as finding the spatially and temporally
variable temperature distribution of a conducting solid, then the test case
above corresponds to an L-shaped body where we keep the boundary at zero
temperature, and heat alternatingly in two parts of the domain. While heating
is in effect, the temperature rises in these places, after which it diffuses
and diminishes again. The point of these initial conditions is that they
provide us with a solution that has singularities both in time (when sources
switch on and off) as well as time (at the reentrant corner as well as at the
edges and corners of the regions where the source acts).


examples/step-26/doc/results.dox
<h1>Results</h1>

As in many of the tutorials, the actual output of the program matters less
than how we arrived there. Nonetheless, here it is:
@code
===========================================
Number of active cells: 48
Number of degrees of freedom: 65

Time step 1 at t=0.002
     7 CG iterations.

===========================================
Number of active cells: 60
Number of degrees of freedom: 81


Time step 1 at t=0.002
     7 CG iterations.

===========================================
Number of active cells: 105
Number of degrees of freedom: 136


Time step 1 at t=0.002
     7 CG iterations.

[...]

Time step 249 at t=0.498
     13 CG iterations.
Time step 250 at t=0.5
     14 CG iterations.

===========================================
Number of active cells: 1803
Number of degrees of freedom: 2109
@endcode

Maybe of more interest is a visualization of the solution and the mesh on which
it was computed:

<img src="https://www.dealii.org/images/steps/developer/step-26.movie.gif" alt="Animation of the solution of step 26.">

The movie shows how the two sources switch on and off and how the mesh reacts
to this. It is quite obvious that the mesh as is is probably not the best we
could come up with. We'll get back to this in the next section.


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

There are at least two areas where one can improve this program significantly:
adaptive time stepping and a better choice of the mesh.

<h4>Adaptive time stepping</h4>

Having chosen an implicit time stepping scheme, we are not bound by any
CFL-like condition on the time step. Furthermore, because the time scales on
which change happens on a given cell in the heat equation are not bound to the
cells diameter (unlike the case with the wave equation, where we had a fixed
speed of information transport that couples the temporal and spatial scales),
we can choose the time step as we please. Or, better, choose it as we deem
necessary for accuracy.

Looking at the solution, it is clear that the action does not happen uniformly
over time: a lot is changing around the time we switch on a source, things
become less dramatic once a source is on for a little while, and we enter a
long phase of decline when both sources are off. During these times, we could
surely get away with a larger time step than before without sacrificing too
much accuracy.

The literature has many suggestions on how to choose the time step size
adaptively. Much can be learned, for example, from the way ODE solvers choose
their time steps. One can also be inspired by a posteriori error estimators
that can, ideally, be written in a way that the consist of a temporal and a
spatial contribution to the overall error. If the temporal one is too large,
we should choose a smaller time step. Ideas in this direction can be found,
for example, in the PhD thesis of a former principal developer of deal.II,
Ralf Hartmann, published by the University of Heidelberg, Germany, in 2002.


<h4>Better time stepping methods</h4>

We here use one of the simpler time stepping methods, namely the second order
in time Crank-Nicolson method. However, more accurate methods such as
Runge-Kutta methods are available and should be used as they do not represent
much additional effort. It is not difficult to implement this for the current
program, but a more systematic treatment is also given in step-52.


<h4>Better refinement criteria</h4>

If you look at the meshes in the movie above, it is clear that they are not
particularly well suited to the task at hand. In fact, they look rather
random.

There are two factors at play. First, there are some islands where cells
have been refined but that are surrounded by non-refined cells (and there
are probably also a few occasional coarsened islands). These are not terrible,
as they most of the time do not affect the approximation quality of the mesh,
but they also don't help because so many of their additional degrees of
freedom are in fact constrained by hanging node constraints. That said,
this is easy to fix: the Triangulation class takes an argument to its
constructor indicating a level of "mesh smoothing". Passing one of many
possible flags, this instructs the triangulation to refine some additional
cells, or not to refine some cells, so that the resulting mesh does not have
these artifacts.

The second problem is more severe: the mesh appears to lag the solution.
The underlying reason is that we only adapt the mesh once every fifth
time step, and only allow for a single refinement in these cases. Whenever a
source switches on, the solution had been very smooth in this area before and
the mesh was consequently rather coarse. This implies that the next time step
when we refine the mesh, we will get one refinement level more in this area,
and five time steps later another level, etc. But this is not enough: first,
we should refine immediately when a source switches on (after all, in the
current context we at least know what the right hand side is), and we should
allow for more than one refinement level. Of course, all of this can be done
using deal.II, it just requires a bit of algorithmic thinking in how to make
this work!


<h4>Positivity preservation</h4>

To increase the accuracy and resolution of your simulation in time, one
typically decreases the time step size $k_n$. If you start playing around
with the time step in this particular example, you will notice that the
solution becomes partly negative, if $k_n$ is below a certain threshold.
This is not what we would expect to happen (in nature).

To get an idea of this behavior mathematically, let us consider a general,
fully discrete problem:
@f{align*}
  A u^{n} = B u^{n-1}.
@f}
The general form of the $i$th equation then reads:
@f{align*}
  a_{ii} u^{n}_i &= b_{ii} u^{n-1}_i +
  \sum\limits_{j \in S_i} \left( b_{ij} u^{n-1}_j - a_{ij} u^{n}_j \right),
@f}
where $S_i$ is the set of degrees of freedom that DoF $i$ couples with (i.e.,
for which either the matrix $A$ or matrix $B$ has a nonzero entry at position
$(i,j)$). If all coefficients
fulfill the following conditions:
@f{align*}
  a_{ii} &> 0, & b_{ii} &\geq 0, & a_{ij} &\leq 0, & b_{ij} &\geq 0,
  &
  \forall j &\in S_i,
@f}
all solutions $u^{n}$ keep their sign from the previous ones $u^{n-1}$, and
consequently from the initial values $u^0$. See e.g.
<a href="http://bookstore.siam.org/cs14/">Kuzmin, H&auml;m&auml;l&auml;inen</a>
for more information on positivity preservation.

Depending on the PDE to solve and the time integration scheme used, one is
able to deduce conditions for the time step $k_n$. For the heat equation with
the Crank-Nicolson scheme,
<a href="https://doi.org/10.2478/cmam-2010-0025">Schatz et. al.</a> have
translated it to the following ones:
@f{align*}
  (1 - \theta) k a_{ii} &\leq m_{ii},\qquad \forall i,
  &
  \theta k \left| a_{ij} \right| &\geq m_{ij},\qquad j \neq i,
@f}
where $M = m_{ij}$ denotes the mass matrix and $A = a_{ij}$ the stiffness
matrix with $a_{ij} \leq 0$ for $j \neq i$, respectively. With
$a_{ij} \leq 0$, we can formulate bounds for the global time step $k$ as
follows:
@f{align*}
  k_{\text{max}} &= \frac{ 1 }{ 1 - \theta }
  \min\left( \frac{ m_{ii} }{ a_{ii} } \right),~ \forall i,
  &
  k_{\text{min}} &= \frac{ 1 }{ \theta  }
  \max\left( \frac{ m_{ij} }{ \left|a_{ij}\right| } \right),~ j \neq i.
@f}
In other words, the time step is constrained by <i>both a lower
and upper bound</i> in case of a Crank-Nicolson scheme. These bounds should be
considered along with the CFL condition to ensure significance of the performed
simulations.

Being unable to make the time step as small as we want to get more
accuracy without losing the positivity property is annoying. It raises
the question of whether we can at least <i>compute</i> the minimal time step
we can choose  to ensure positivity preservation in this particular tutorial.
Indeed, we can use
the SparseMatrix objects for both mass and stiffness that are created via
the MatrixCreator functions. Iterating through each entry via SparseMatrixIterators
lets us check for diagonal and off-diagonal entries to set a proper time step
dynamically. For quadratic matrices, the diagonal element is stored as the
first member of a row (see SparseMatrix documentation). An exemplary code
snippet on how to grab the entries of interest from the <code>mass_matrix</code>
is shown below.

@code
Assert (mass_matrix.m() == mass_matrix.n(), ExcNotQuadratic());
const unsigned int num_rows = mass_matrix.m();
double mass_matrix_min_diag    = std::numeric_limits<double>::max(),
       mass_matrix_max_offdiag = 0.;

SparseMatrixIterators::Iterator<double,true> row_it (&mass_matrix, 0);

for(unsigned int m = 0; m<num_rows; ++m)
{
  // check the diagonal element
  row_it = mass_matrix.begin(m);
  mass_matrix_min_diag = std::min(row_it->value(), mass_matrix_min_diag);
  ++row_it;

  // check the off-diagonal elements
  for(; row_it != mass_matrix.end(m); ++row_it)
    mass_matrix_max_offdiag = std::max(row_it->value(), mass_matrix_max_offdiag);
}
@endcode

Using the information so computed, we can bound the time step via the formulas
above.


examples/step-27/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

This tutorial program attempts to show how to use $hp$-finite element methods
with deal.II. It solves the Laplace equation and so builds only on the first
few tutorial programs, in particular on step-4 for dimension
independent programming and step-6 for adaptive mesh refinement.

The $hp$-finite element method was proposed in the early 1980s by
Babu&scaron;ka and Guo as an alternative to either
(i) mesh refinement (i.e., decreasing the mesh parameter $h$ in a finite
element computation) or (ii) increasing the polynomial degree $p$ used for
shape functions. It is based on the observation that increasing the polynomial
degree of the shape functions reduces the approximation error if the solution
is sufficiently smooth. On the other hand, it is well known
that even for the generally well-behaved class of elliptic problems, higher
degrees of regularity can not be guaranteed in the vicinity of boundaries,
corners, or where coefficients are discontinuous; consequently, the
approximation can not be improved in these areas by increasing the polynomial
degree $p$ but only by refining the mesh, i.e., by reducing the mesh size
$h$. These differing means to reduce the
error have led to the notion of $hp$-finite elements, where the approximating
finite element spaces are adapted to have a high polynomial degree $p$
wherever the solution is sufficiently smooth, while the mesh width $h$ is
reduced at places wherever the solution lacks regularity. It was
already realized in the first papers on this method that $hp$-finite elements
can be a powerful tool that can guarantee that the error is reduced not only
with some negative power of the number of degrees of freedom, but in fact
exponentially.

In order to implement this method, we need several things above and beyond
what a usual finite element program needs, and in particular above what we
have introduced in the tutorial programs leading up to step-6. In particular,
we will have to discuss the following aspects:
<ul>
  <li>Instead of using the same finite element on all cells, we now will want
  a collection of finite element objects, and associate each cell with one
  of these objects in this collection.</li>

  <li>Degrees of freedom will then have to be allocated on each cell depending
  on what finite element is associated with this particular cell. Constraints
  will have to be generated in the same way as for hanging nodes, but we now
  also have to deal with the case where two neighboring cells have different
  finite elements assigned.</li>

  <li>We will need to be able to assemble cell and face contributions
  to global matrices and right hand side vectors.</li>

  <li>After solving the resulting linear system, we will want to
  analyze the solution. In particular, we will want to compute error
  indicators that tell us whether a given cell should be refined
  and/or whether the polynomial degree of the shape functions used on
  it should be increased.</li>
</ul>

We will discuss all these aspects in the following subsections of this
introduction. It will not come as a big surprise that most of these
tasks are already well supported by functionality provided by the
deal.II, and that we will only have to provide the logic of what the
program should do, not exactly how all this is going to happen.

In deal.II, the $hp$-functionality is largely packaged into
the hp-namespace. This namespace provides classes that handle
$hp$-discretizations, assembling matrices and vectors, and other
tasks. We will get to know many of them further down below. In
addition, most of the functions in the DoFTools, and VectorTools
namespaces accept $hp$-objects in addition to the non-$hp$-ones. Much of
the $hp$-implementation is also discussed in the @ref hp documentation
module and the links found there.

It may be worth giving a slightly larger perspective at the end of
this first part of the introduction. $hp$-functionality has been
implemented in a number of different finite element packages (see, for
example, the list of references cited in the @ref hp_paper "hp-paper").
However, by and large, most of these packages have implemented it only
for the (i) the 2d case, and/or (ii) the discontinuous Galerkin
method. The latter is a significant simplification because
discontinuous finite elements by definition do not require continuity
across faces between cells and therefore do not require the special
treatment otherwise necessary whenever finite elements of different
polynomial degree meet at a common face. In contrast, deal.II
implements the most general case, i.e., it allows for continuous and
discontinuous elements in 1d, 2d, and 3d, and automatically handles
the resulting complexity. In particular, it handles computing the
constraints (similar to hanging node constraints) of elements of
different degree meeting at a face or edge. The many algorithmic and
data structure techniques necessary for this are described in the
@ref hp_paper "hp-paper" for those interested in such detail.

We hope that providing such a general implementation will help explore
the potential of $hp$-methods further.



<h3>Finite element collections</h3>

Now on again to the details of how to use the $hp$-functionality in
deal.II. The first aspect we have to deal with is that now we do not
have only a single finite element any more that is used on all cells,
but a number of different elements that cells can choose to use. For
this, deal.II introduces the concept of a <i>finite element
collection</i>, implemented in the class hp::FECollection. In essence,
such a collection acts like an object of type
<code>std::vector@<FiniteElement@></code>, but with a few more bells
and whistles and a memory management better suited to the task at
hand. As we will later see, we will also use similar quadrature
collections, and &mdash; although we don't use them here &mdash; there
is also the concept of mapping collections. All of these classes are
described in the @ref hpcollection overview.

In this tutorial program, we will use continuous Lagrange elements of
orders 2 through 7 (in 2d) or 2 through 5 (in 3d). The collection of
used elements can then be created as follows:
@code
  hp::FECollection<dim> fe_collection;
  for (unsigned int degree = 2; degree <= max_degree; ++degree)
    fe_collection.push_back(FE_Q<dim>(degree));
@endcode



<h3>The DoFHandler class in <i>hp</i>-mode, associating cells with finite elements, and constraints</h3>

The next task we have to consider is what to do with the list of
finite element objects we want to use. In previous tutorial programs,
starting with step-2, we have seen that the DoFHandler
class is responsible for making the connection between a mesh
(described by a Triangulation object) and a finite element, by
allocating the correct number of degrees of freedom for each vertex,
face, edge, and cell of the mesh.

The situation here is a bit more complicated since we do not just have
a single finite element object, but rather may want to use different
elements on different cells. We therefore need two things: (i) a
version of the DoFHandler class that can deal with this situation, and
(ii) a way to tell the DoFHandler which element to use on which cell.

The first of these two things is implemented in the <i>hp</i>-mode of
the DoFHandler class: rather than associating it with a triangulation
and a single finite element object, it is associated with a triangulation
and a finite element collection. The second part is achieved by a loop
over all cells of this DoFHandler and for each cell setting the index
of the finite element within the collection that shall be used on this
cell. We call the index of the finite element object within the
collection that shall be used on a cell the cell's <i>active FE
index</i> to indicate that this is the finite element that is active
on this cell, whereas all the other elements of the collection are
inactive on it. The general outline of this reads like this:

@code
  DoFHandler<dim> dof_handler(triangulation);
  for (auto &cell: dof_handler.active_cell_iterators())
    cell->set_active_fe_index(...);
  dof_handler.distribute_dofs(fe_collection);
@endcode

Dots in the call to <code>set_active_fe_index()</code> indicate that
we will have to have some sort of strategy later on to decide which
element to use on which cell; we will come back to this later. The
main point here is that the first and last line of this code snippet
is pretty much exactly the same as for the non-$hp$-case.

Another complication arises from the fact that this time we do not
simply have hanging nodes from local mesh refinement, but we also have
to deal with the case that if there are two cells with different
active finite element indices meeting at a face (for example a Q2 and
a Q3 element) then we have to compute additional constraints on the
finite element field to ensure that it is continuous. This is
conceptually very similar to how we compute hanging node constraints,
and in fact the code looks exactly the same:
@code
  AffineConstraints<double> constraints;
  DoFTools::make_hanging_node_constraints(dof_handler, constraints);
@endcode
In other words, the DoFTools::make_hanging_node_constraints deals not
only with hanging node constraints, but also with $hp$-constraints at
the same time.



<h3>Assembling matrices and vectors with hp-objects</h3>

Following this, we have to set up matrices and vectors for the linear system
of the correct size and assemble them. Setting them up works in exactly the
same way as for the non-$hp$-case. Assembling requires a bit more thought.

The main idea is of course unchanged: we have to loop over all cells, assemble
local contributions, and then copy them into the global objects. As discussed
in some detail first in step-3, deal.II has the FEValues class that pulls
the finite element description, mapping, and quadrature formula
together and aids in evaluating values and gradients of shape functions as
well as other information on each of the quadrature points mapped to the real
location of a cell. Every time we move on to a new cell we re-initialize this
FEValues object, thereby asking it to re-compute that part of the information
that changes from cell to cell. It can then be used to sum up local
contributions to bilinear form and right hand side.

In the context of $hp$-finite element methods, we have to deal with the fact
that we do not use the same finite element object on each cell. In fact, we
should not even use the same quadrature object for all cells, but rather
higher order quadrature formulas for cells where we use higher order finite
elements. Similarly, we may want to use higher order mappings on such cells as
well.

To facilitate these considerations, deal.II has a class hp::FEValues that does
what we need in the current context. The difference is that instead of a
single finite element, quadrature formula, and mapping, it takes collections
of these objects. It's use is very much like the regular FEValues class,
i.e., the interesting part of the loop over all cells would look like this:

@code
  hp::FEValues<dim> hp_fe_values(mapping_collection,
                                 fe_collection,
                                 quadrature_collection,
                                 update_values | update_gradients |
                                 update_quadrature_points | update_JxW_values);

  for (const auto &cell : dof_handler.active_cell_iterators())
    {
      hp_fe_values.reinit(cell);

      const FEValues<dim> &fe_values = hp_fe_values.get_present_fe_values();

      ...  // assemble local contributions and copy them into global object
    }
@endcode

In this tutorial program, we will always use a Q1 mapping, so the mapping
collection argument to the hp::FEValues construction will be omitted. Inside
the loop, we first initialize the hp::FEValues object for the current
cell. The second, third and fourth arguments denote the index within their
respective collections of the quadrature, mapping, and finite element objects
we wish to use on this cell. These arguments can be omitted (and are in the
program below), in which case <code>cell-@>active_fe_index()</code> is used
for this index. The order of these arguments is chosen in this way because one
may sometimes want to pick a different quadrature or mapping object from their
respective collections, but hardly ever a different finite element than the
one in use on this cell, i.e., one with an index different from
<code>cell-@>active_fe_index()</code>. The finite element collection index is
therefore the last default argument so that it can be conveniently omitted.

What this <code>reinit</code> call does is the following: the
hp::FEValues class checks whether it has previously already allocated a
non-$hp$-FEValues object for this combination of finite element, quadrature,
and mapping objects. If not, it allocates one. It then re-initializes this
object for the current cell, after which there is now a FEValues object for
the selected finite element, quadrature and mapping usable on the current
cell. A reference to this object is then obtained using the call
<code>hp_fe_values.get_present_fe_values()</code>, and will be used in the
usual fashion to assemble local contributions.



<h3>A simple indicator for hp-refinement and estimating smoothness</h3>

One of the central pieces of the adaptive finite element method is that we
inspect the computed solution (a posteriori) with an indicator that tells us
which are the cells where the error is largest, and then refine them. In many
of the other tutorial programs, we use the KellyErrorEstimator class to get an
indication of the size of the error on a cell, although we also discuss more
complicated strategies in some programs, most importantly in step-14.

In any case, as long as the decision is only "refine this cell" or "do not
refine this cell", the actual refinement step is not particularly
challenging. However, here we have a code that is capable of hp-refinement,
i.e., we suddenly have two choices whenever we detect that the error on a
certain cell is too large for our liking: we can refine the cell by splitting
it into several smaller ones, or we can increase the polynomial degree of the
shape functions used on it. How do we know which is the more promising
strategy? Answering this question is the central problem in $hp$-finite
element research at the time of this writing.

In short, the question does not appear to be settled in the literature at this
time. There are a number of more or less complicated schemes that address it,
but there is nothing like the KellyErrorEstimator that is universally accepted
as a good, even if not optimal, indicator of the error. Most proposals use the
fact that it is beneficial to increase the polynomial degree whenever the
solution is locally smooth whereas it is better to refine the mesh wherever it
is rough. However, the questions of how to determine the local smoothness of
the solution as well as the decision when a solution is smooth enough to allow
for an increase in $p$ are certainly big and important ones.

In the following, we propose a simple estimator of the local smoothness of a
solution. As we will see in the results section, this estimator has flaws, in
particular as far as cells with local hanging nodes are concerned. We
therefore do not intend to present the following ideas as a complete solution
to the problem. Rather, it is intended as an idea to approach it that merits
further research and investigation. In other words, we do not intend to enter
a sophisticated proposal into the fray about answers to the general
question. However, to demonstrate our approach to $hp$-finite elements, we
need a simple indicator that does generate some useful information that is
able to drive the simple calculations this tutorial program will perform.


<h4>The idea</h4>

Our approach here is simple: for a function $u({\bf x})$ to be in the
Sobolev space $H^s(K)$ on a cell $K$, it has to satisfy the condition
@f[
   \int_K |\nabla^s u({\bf x})|^2 \; d{\bf x} < \infty.
@f]
Assuming that the cell $K$ is not degenerate, i.e., that the mapping from the
unit cell to cell $K$ is sufficiently regular, above condition is of course
equivalent to
@f[
   \int_{\hat K} |\nabla^s \hat u(\hat{\bf x})|^2 \; d\hat{\bf x} < \infty\,,
@f]
where $\hat u(\hat{\bf x})$ is the function $u({\bf x})$ mapped back onto the unit cell
$\hat K$. From here, we can do the following: first, let us define the
Fourier series of $\hat u$ as
@f[
   \hat u(\hat{\bf x})
   = \sum_{\bf k} \hat U_{\bf k}\,e^{-i {\bf k}\cdot \hat{\bf x}},
@f]
with Fourier vectors ${\bf k}=(k_x,k_y)$ in 2d, ${\bf k}=(k_x,k_y,k_z)$
in 3d, etc, and $k_x,k_y,k_z=0,2\pi,4\pi,\ldots$. The coefficients of expansion
$\hat U_{\bf k}$ can be obtained using $L^2$-orthogonality of the exponential basis
@f[
\int_{\hat K} e^{-i {\bf m}\cdot \hat{\bf x}} e^{i {\bf n}\cdot \hat{\bf x}} d\hat{\bf x} = \delta_{\bf m \bf n},
@f]
that leads to the following expression
@f[
   \hat U_{\bf k}
   = \int_{\hat K} e^{i {\bf k}\cdot \hat{\bf x}} \hat u(\hat{\bf x}) d\hat{\bf x} \,.
@f]
It becomes clear that we can then write the $H^s$ norm of $\hat u$ as
@f[
  \int_{\hat K} |\nabla^s \hat u(\hat{\bf x})|^2 \; d\hat{\bf x}
  =
  \int_{\hat K}
  \left|
    \sum_{\bf k} |{\bf k}|^s e^{-i{\bf k}\cdot \hat{\bf x}} \hat U_{\bf k}
  \right|^2 \; d\hat{\bf x}
  =
  \sum_{\bf k}
    |{\bf k}|^{2s}
    |\hat U_{\bf k}|^2.
@f]
In other words, if this norm is to be finite (i.e., for $\hat u(\hat{\bf x})$ to be in $H^s(\hat K)$), we need that
@f[
   |\hat U_{\bf k}| = {\cal O}\left(|{\bf k}|^{-\left(s+1/2+\frac{d-1}{2}+\epsilon\right)}\right).
@f]
Put differently: the higher regularity $s$ we want, the faster the
Fourier coefficients have to go to zero. If you wonder where the
additional exponent $\frac{d-1}2$ comes from: we would like to make
use of the fact that $\sum_l a_l < \infty$ if the sequence $a_l =
{\cal O}(l^{-1-\epsilon})$ for any $\epsilon>0$. The problem is that we
here have a summation not only over a single variable, but over all
the integer multiples of $2\pi$ that are located inside the
$d$-dimensional sphere, because we have vector components $k_x, k_y,
\ldots$. In the same way as we prove that the sequence $a_l$ above
converges by replacing the sum by an integral over the entire line, we
can replace our $d$-dimensional sum by an integral over
$d$-dimensional space. Now we have to note that between distance $|{\bf k}|$
and $|{\bf k}|+d|{\bf k}|$, there are, up to a constant, $|{\bf k}|^{d-1}$ modes, in
much the same way as we can transform the volume element $dx\;dy$ into
$2\pi r\; dr$. Consequently, it is no longer $|{\bf k}|^{2s}|\hat
U_{\bf k}|^2$ that has to decay as ${\cal O}(|{\bf k}|^{-1-\epsilon})$, but
it is in fact $|{\bf k}|^{2s}|\hat U_{\bf k}|^2 |{\bf k}|^{d-1}$. A
comparison of exponents yields the result.

We can turn this around: Assume we are given a function $\hat u$ of unknown
smoothness. Let us compute its Fourier coefficients $\hat U_{\bf k}$
and see how fast they decay. If they decay as
@f[
   |\hat U_{\bf k}| = {\cal O}(|{\bf k}|^{-\mu-\epsilon}),
@f]
then consequently the function we had here was in $H^{\mu-d/2}$.


<h4>What we have to do</h4>

So what do we have to do to estimate the local smoothness of $u({\bf x})$ on
a cell $K$? Clearly, the first step is to compute the Fourier coefficients
of our solution. Fourier series being infinite series, we simplify our
task by only computing the first few terms of the series, such that
$|{\bf k}|\le 2\pi N$ with a cut-off $N$. Let us parenthetically remark
that we want to choose $N$ large enough so that we capture at least
the variation of those shape functions that vary the most. On the
other hand, we should not choose $N$ too large: clearly, a finite
element function, being a polynomial, is in $C^\infty$ on any given
cell, so the coefficients will have to decay exponentially at one
point; since we want to estimate the smoothness of the function this
polynomial approximates, not of the polynomial itself, we need to
choose a reasonable cutoff for $N$. Either way, computing this series
is not particularly hard: from the definition
@f[
   \hat U_{\bf k}
   = \int_{\hat K} e^{i {\bf k}\cdot \hat{\bf x}} \hat u(\hat{\bf x}) d\hat{\bf x}
@f]
we see that we can compute the coefficient $\hat U_{\bf k}$ as
@f[
   \hat U_{\bf k}
   =
   \sum_{i=0}^{\textrm{dofs per cell}}
   \left[\int_{\hat K} e^{i {\bf k}\cdot \hat{\bf x}} \hat \varphi_i(\hat{\bf x})
   d\hat{\bf x} \right] u_i,
@f]
where $u_i$ is the value of the $i$th degree of freedom on this
cell. In other words, we can write it as a matrix-vector product
@f[
   \hat U_{\bf k}
   = {\cal F}_{{\bf k},j} u_j,
@f]
with the matrix
@f[
   {\cal F}_{{\bf k},j}
   =
   \int_{\hat K} e^{i {\bf k}\cdot \hat{\bf x}} \hat \varphi_j(\hat{\bf x}) d\hat{\bf x}.
@f]
This matrix is easily computed for a given number of shape functions
$\varphi_j$ and Fourier modes $N$. Consequently, finding the
coefficients $\hat U_{\bf k}$ is a rather trivial job.
To simplify our life even further, we will use FESeries::Fourier class which
does exactly this.

The next task is that we have to estimate how fast these coefficients
decay with $|{\bf k}|$. The problem is that, of course, we have only
finitely many of these coefficients in the first place. In other
words, the best we can do is to fit a function $\alpha |{\bf k}|^{-\mu}$
to our data points $\hat U_{\bf k}$, for example by
determining $\alpha,\mu$ via a least-squares procedure:
@f[
   \min_{\alpha,\mu}
   \frac 12 \sum_{{\bf k}, |{\bf k}|\le N}
   \left( |\hat U_{\bf k}| - \alpha |{\bf k}|^{-\mu}\right)^2
@f]
However, the problem with this is that it leads to a nonlinear
problem, a fact that we would like to avoid. On the other hand, we can
transform the problem into a simpler one if we try to fit the
logarithm of our coefficients to the logarithm of $\alpha |{\bf k}|^{-\mu}$,
like this:
@f[
   \min_{\alpha,\mu}
   Q(\alpha,\mu) =
   \frac 12 \sum_{{\bf k}, |{\bf k}|\le N}
   \left( \ln |\hat U_{\bf k}| - \ln (\alpha |{\bf k}|^{-\mu})\right)^2.
@f]
Using the usual facts about logarithms, we see that this yields the
problem
@f[
   \min_{\beta,\mu}
   Q(\beta,\mu) =
   \frac 12 \sum_{{\bf k}, |{\bf k}|\le N}
   \left( \ln |\hat U_{\bf k}| - \beta + \mu \ln |{\bf k}|\right)^2,
@f]
where $\beta=\ln \alpha$. This is now a problem for which the
optimality conditions $\frac{\partial Q}{\partial\beta}=0,
\frac{\partial Q}{\partial\mu}=0$, are linear in $\beta,\mu$. We can
write these conditions as follows:
@f[
   \left(\begin{array}{cc}
   \sum_{{\bf k}, |{\bf k}|\le N} 1 &
   \sum_{{\bf k}, |{\bf k}|\le N} \ln |{\bf k}|
   \\
   \sum_{{\bf k}, |{\bf k}|\le N} \ln |{\bf k}| &
   \sum_{{\bf k}, |{\bf k}|\le N} (\ln |{\bf k}|)^2
   \end{array}\right)
   \left(\begin{array}{c}
   \beta \\ -\mu
   \end{array}\right)
   =
   \left(\begin{array}{c}
   \sum_{{\bf k}, |{\bf k}|\le N} \ln |\hat U_{{\bf k}}|
   \\
   \sum_{{\bf k}, |{\bf k}|\le N} \ln |\hat U_{{\bf k}}| \ln |{\bf k}|
   \end{array}\right)
@f]
This linear system is readily inverted to yield
@f[
   \beta =
   \frac
   {
   \left(\sum_{{\bf k}, |{\bf k}|\le N} (\ln |{\bf k}|)^2\right)
   \left(\sum_{{\bf k}, |{\bf k}|\le N} \ln |\hat U_{{\bf k}}|\right)
   -
   \left(\sum_{{\bf k}, |{\bf k}|\le N} \ln |{\bf k}|\right)
   \left(\sum_{{\bf k}, |{\bf k}|\le N} \ln |\hat U_{{\bf k}}| \ln |{\bf k}| \right)
   }
   {
   \left(\sum_{{\bf k}, |{\bf k}|\le N} 1\right)
   \left(\sum_{{\bf k}, |{\bf k}|\le N} (\ln |{\bf k}|)^2\right)
   -
   \left(\sum_{{\bf k}, |{\bf k}|\le N} \ln |{\bf k}|\right)^2
   }
@f]
and
@f[
   \mu =
   \frac
   {
   \left(\sum_{{\bf k}, |{\bf k}|\le N} \ln |{\bf k}|\right)
   \left(\sum_{{\bf k}, |{\bf k}|\le N} \ln |\hat U_{{\bf k}}|\right)
   -
   \left(\sum_{{\bf k}, |{\bf k}|\le N} 1\right)
   \left(\sum_{{\bf k}, |{\bf k}|\le N} \ln |\hat U_{{\bf k}}| \ln |{\bf k}| \right)
   }
   {
   \left(\sum_{{\bf k}, |{\bf k}|\le N} 1\right)
   \left(\sum_{{\bf k}, |{\bf k}|\le N} (\ln |{\bf k}|)^2\right)
   -
   \left(\sum_{{\bf k}, |{\bf k}|\le N} \ln |{\bf k}|\right)^2
   }.
@f]

This is nothing else but linear regression fit and to do that we will use
FESeries::linear_regression().
While we are not particularly interested in the actual value of
$\beta$, the formula above gives us a mean to calculate the value of
the exponent $\mu$ that we can then use to determine that
$\hat u(\hat{\bf x})$ is in $H^s(\hat K)$ with $s=\mu-\frac d2$.

These steps outlined above are applicable to many different scenarios, which
motivated the introduction of a generic function
SmoothnessEstimator::Fourier::coefficient_decay() in deal.II, that combines all
the tasks described in this section in one simple function call. We will use it
in the implementation of this program.


<h4>Compensating for anisotropy</h4>

In the formulas above, we have derived the Fourier coefficients $\hat U_{\bf
k}$. Because ${\bf k}$ is a vector, we will get a number of Fourier
coefficients $\hat U_{{\bf k}}$ for the same absolute value $|{\bf k}|$,
corresponding to the Fourier transform in different directions. If we now
consider a function like $|x|y^2$ then we will find lots of large Fourier
coefficients in $x$-direction because the function is non-smooth in this
direction, but fast-decaying Fourier coefficients in $y$-direction because the
function is smooth there. The question that arises is this: if we simply fit
our polynomial decay $\alpha |{\bf k}|^\mu$ to <i>all</i> Fourier coefficients,
we will fit it to a smoothness <i>averaged in all spatial directions</i>. Is
this what we want? Or would it be better to only consider the largest
coefficient $\hat U_{{\bf k}}$ for all ${\bf k}$ with the same magnitude,
essentially trying to determine the smoothness of the solution in that spatial
direction in which the solution appears to be roughest?

One can probably argue for either case. The issue would be of more interest if
deal.II had the ability to use anisotropic finite elements, i.e., ones that use
different polynomial degrees in different spatial directions, as they would be
able to exploit the directionally variable smoothness much better. Alas, this
capability does not exist at the time of writing this tutorial program.

Either way, because we only have isotopic finite element classes, we adopt the
viewpoint that we should tailor the polynomial degree to the lowest amount of
regularity, in order to keep numerical efforts low. Consequently, instead of
using the formula
@f[
   \mu =
   \frac
   {
   \left(\sum_{{\bf k}, |{\bf k}|\le N} \ln |{\bf k}|\right)
   \left(\sum_{{\bf k}, |{\bf k}|\le N} \ln |\hat U_{{\bf k}}|\right)
   -
   \left(\sum_{{\bf k}, |{\bf k}|\le N} 1\right)
   \left(\sum_{{\bf k}, |{\bf k}|\le N} \ln |\hat U_{{\bf k}}| \ln |{\bf k}| \right)
   }
   {
   \left(\sum_{{\bf k}, |{\bf k}|\le N} 1\right)
   \left(\sum_{{\bf k}, |{\bf k}|\le N} (\ln |{\bf k}|)^2\right)
   -
   \left(\sum_{{\bf k}, |{\bf k}|\le N} \ln |{\bf k}|\right)^2
   }.
@f]
To calculate $\mu$ as shown above, we have to slightly modify all sums:
instead of summing over all Fourier modes, we only sum over those for which
the Fourier coefficient is the largest one among all $\hat U_{{\bf k}}$ with
the same magnitude $|{\bf k}|$, i.e., all sums above have to replaced by the
following sums:
@f[
  \sum_{{\bf k}, |{\bf k}|\le N}
  \longrightarrow
  \sum_{\begin{matrix}{{\bf k}, |{\bf k}|\le N} \\ {|\hat U_{{\bf k}}| \ge |\hat U_{{\bf k}'}|
  \ \textrm{for all}\ {\bf k}'\ \textrm{with}\ |{\bf k}'|=|{\bf k}|}\end{matrix}}.
@f]
This is the form we will implement in the program.


<h4>Questions about cell sizes</h4>

One may ask whether it is a problem that we only compute the Fourier transform
on the <i>reference cell</i> (rather than the real cell) of the
solution. After all, we stretch the solution by a factor $\frac 1h$ during the
transformation, thereby shifting the Fourier frequencies by a factor of
$h$. This is of particular concern since we may have neighboring cells with
mesh sizes $h$ that differ by a factor of 2 if one of them is more refined
than the other. The concern is also motivated by the fact that, as we will see
in the results section below, the estimated smoothness of the solution should
be a more or less continuous function, but exhibits jumps at locations where
the mesh size jumps. It therefore seems natural to ask whether we have to
compensate for the transformation.

The short answer is "no". In the process outlined above, we attempt to find
coefficients $\beta,\mu$ that minimize the sum of squares of the terms
@f[
   \ln |\hat U_{{\bf k}}| - \beta + \mu \ln |{\bf k}|.
@f]
To compensate for the transformation means not attempting to fit a decay
$|{\bf k}|^\mu$ with respect to the Fourier frequencies ${\bf k}$ <i>on the unit
cell</i>, but to fit the coefficients $\hat U_{{\bf k}}$ computed on the
reference cell <i>to the Fourier frequencies on the real cell $|\bf
k|h$</i>, where $h$ is the norm of the transformation operator (i.e., something
like the diameter of the cell). In other words, we would have to minimize the
sum of squares of the terms
@f[
   \ln |\hat U_{{\bf k}}| - \beta + \mu \ln (|{\bf k}|h).
@f]
instead. However, using fundamental properties of the logarithm, this is
simply equivalent to minimizing
@f[
   \ln |\hat U_{{\bf k}}| - (\beta - \mu \ln h) + \mu \ln (|{\bf k}|).
@f]
In other words, this and the original least squares problem will produce the
same best-fit exponent $\mu$, though the offset will in one case be $\beta$
and in the other $\beta-\mu \ln h$. However, since we are not interested in
the offset at all but only in the exponent, it doesn't matter whether we scale
Fourier frequencies in order to account for mesh size effects or not, the
estimated smoothness exponent will be the same in either case.



<h3>Complications with linear systems for hp-discretizations</h3>

<h4>Creating the sparsity pattern</h4>

One of the problems with $hp$-methods is that the high polynomial degree of
shape functions together with the large number of constrained degrees of
freedom leads to matrices with large numbers of nonzero entries in some
rows. At the same time, because there are areas where we use low polynomial
degree and consequently matrix rows with relatively few nonzero
entries. Consequently, allocating the sparsity pattern for these matrices is a
challenge: we cannot simply assemble a SparsityPattern by starting with an
estimate of the bandwidth without using a lot of extra memory.

The way in which we create a SparsityPattern for the underlying linear system is
tightly coupled to the strategy we use to enforce constraints. deal.II supports
handling constraints in linear systems in two ways:
<ol>
  <li>Assembling the matrix without regard to the constraints and applying them
  afterwards with AffineConstraints::condense, or</li>
  <li>Applying constraints as we assemble the system with
  AffineConstraints::distribute_local_to_global.</li>
</ol>
Most programs built on deal.II use the DoFTools::make_sparsity_pattern function
to allocate a DynamicSparsityPattern that takes constraints into account. The
system matrix then uses a SparsityPattern copied over from the
DynamicSparsityPattern. This method is explained in step-2 and used in most
tutorial programs.

The early tutorial programs use first or second degree finite elements, so
removing entries in the sparsity pattern corresponding to constrained degrees of
freedom does not have a large impact on the overall number of zeros explicitly
stored by the matrix. However, since as many as a third of the degrees of
freedom may be constrained in an hp-discretization (and, with higher degree
elements, these constraints can couple one DoF to as many as ten or twenty other
DoFs), it is worthwhile to take these constraints into consideration since the
resulting matrix will be much sparser (and, therefore, matrix-vector products or
factorizations will be substantially faster too).


<h4>Eliminating constrained degrees of freedom</h4>

A second problem particular to $hp$-methods arises because we have so
many constrained degrees of freedom: typically up to about one third
of all degrees of freedom (in 3d) are constrained because they either
belong to cells with hanging nodes or because they are on cells
adjacent to cells with a higher or lower polynomial degree. This is,
in fact, not much more than the fraction of constrained degrees of
freedom in non-$hp$-mode, but the difference is that each constrained
hanging node is constrained not only against the two adjacent degrees
of freedom, but is constrained against many more degrees of freedom.

It turns out that the strategy presented first in step-6 to eliminate the
constraints while computing the element matrices and vectors with
AffineConstraints::distribute_local_to_global is the most efficient approach
also for this case. The alternative strategy to first build the matrix without
constraints and then "condensing" away constrained degrees of freedom is
considerably more expensive. It turns out that building the sparsity pattern
by this inefficient algorithm requires at least ${\cal O}(N \log N)$ in the
number of unknowns, whereas an ideal finite element program would of course
only have algorithms that are linear in the number of unknowns. Timing the
sparsity pattern creation as well as the matrix assembly shows that the
algorithm presented in step-6 (and used in the code below) is indeed faster.

In our program, we will also treat the boundary conditions as (possibly
inhomogeneous) constraints and eliminate the matrix rows and columns to
those as well. All we have to do for this is to call the function that
interpolates the Dirichlet boundary conditions already in the setup phase in
order to tell the AffineConstraints object about them, and then do the
transfer from local to global data on matrix and vector simultaneously. This
is exactly what we've shown in step-6.



<h3>The test case</h3>

The test case we will solve with this program is a re-take of the one we
already look at in step-14: we solve the Laplace equation
@f[
   -\Delta u = f
@f]
in 2d, with $f=(x+1)(y+1)$, and with zero Dirichlet boundary values for
$u$. We do so on the domain $[-1,1]^2\backslash[-\frac 12,\frac 12]^2$,
i.e., a square with a square hole in the middle.

The difference to step-14 is of course that we use $hp$-finite
elements for the solution. The test case is of interest because it has
re-entrant corners in the corners of the hole, at which the solution has
singularities. We therefore expect that the solution will be smooth in the
interior of the domain, and rough in the vicinity of the singularities. The
hope is that our refinement and smoothness indicators will be able to see this
behavior and refine the mesh close to the singularities, while the polynomial
degree is increased away from it. As we will see in the results section, this
is indeed the case.


examples/step-27/doc/results.dox
<h1>Results</h1>

In this section, we discuss a few results produced from running the
current tutorial program. More results, in particular the extension to
3d calculations and determining how much compute time the individual
components of the program take, are given in the @ref hp_paper "hp-paper".

When run, this is what the program produces:

@code
> make run
[ 66%] Built target step-27
[100%] Run step-27 with Release configuration
Cycle 0:
   Number of active cells      : 768
   Number of degrees of freedom: 3264
   Number of constraints       : 384
Cycle 1:
   Number of active cells      : 807
   Number of degrees of freedom: 4764
   Number of constraints       : 756
Cycle 2:
   Number of active cells      : 927
   Number of degrees of freedom: 8226
   Number of constraints       : 1856
Cycle 3:
   Number of active cells      : 978
   Number of degrees of freedom: 12146
   Number of constraints       : 2944
Cycle 4:
   Number of active cells      : 1104
   Number of degrees of freedom: 16892
   Number of constraints       : 3998
Cycle 5:
   Number of active cells      : 1149
   Number of degrees of freedom: 22078
   Number of constraints       : 5230
@endcode

The first thing we learn from this is that the number of constrained degrees
of freedom is on the order of 20-25% of the total number of degrees of
freedom, at least on the later grids when we have elements of relatively
high order (in 3d, the fraction of constrained degrees of freedom can be up
to 30%). This is, in fact, on the same order of magnitude as for
non-$hp$-discretizations. For example, in the last step of the step-6
program, we have 18353 degrees of freedom, 4432 of which are
constrained. The difference is that in the latter program, each constrained
hanging node is constrained against only the two adjacent degrees of
freedom, whereas in the $hp$-case, constrained nodes are constrained against
many more degrees of freedom. Note also that the current program also
includes nodes subject to Dirichlet boundary conditions in the list of
constraints. In cycle 0, all the constraints are actually because of
boundary conditions.

Of maybe more interest is to look at the graphical output. First, here is the
solution of the problem:

<img src="https://www.dealii.org/images/steps/developer/step-27-solution.png"
     alt="Elevation plot of the solution, showing the lack of regularity near
          the interior (reentrant) corners."
     width="200" height="200">

Secondly, let us look at the sequence of meshes generated:

<div class="threecolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-00.svg"
         alt="Triangulation containing reentrant corners without adaptive refinement."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-01.svg"
         alt="Triangulation containing reentrant corners with one level of
         refinement. New cells are placed near the corners."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-02.svg"
         alt="Triangulation containing reentrant corners with two levels of
         refinement. New cells are placed near the corners."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-03.svg"
         alt="Triangulation containing reentrant corners with three levels of
         refinement. New cells are placed near the corners."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-04.svg"
         alt="Triangulation containing reentrant corners with four levels of
         refinement. New cells are placed near the corners."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-05.svg"
         alt="Triangulation containing reentrant corners with five levels of
         refinement. New cells are placed near the corners."
         width="200" height="200">
  </div>
</div>

It is clearly visible how the mesh is refined near the corner singularities,
as one would expect it. More interestingly, we should be curious to see the
distribution of finite element polynomial degrees to these mesh cells, where
the lightest color corresponds to degree two and the darkest one corresponds
to degree seven:

<div class="threecolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-00.svg"
         alt="Initial grid where all cells contain just biquadratic functions."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-01.svg"
         alt="Depiction of local approximation degrees after one refinement."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-02.svg"
         alt="Depiction of local approximation degrees after two refinements."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-03.svg"
         alt="Depiction of local approximation degrees after three refinements."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-04.svg"
         alt="Depiction of local approximation degrees after four refinements."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-05.svg"
         alt="Depiction of local approximation degrees after five refinements."
         width="200" height="200">
  </div>
</div>

While this is certainly not a perfect arrangement, it does make some sense: we
use low order elements close to boundaries and corners where regularity is
low. On the other hand, higher order elements are used where (i) the error was
at one point fairly large, i.e., mainly in the general area around the corner
singularities and in the top right corner where the solution is large, and
(ii) where the solution is smooth, i.e., far away from the boundary.

This arrangement of polynomial degrees of course follows from our smoothness
estimator. Here is the estimated smoothness of the solution, with darker colors
indicating least smoothness and lighter indicating the smoothest areas:

<div class="threecolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-00.svg"
         alt="Estimated regularity per cell on the initial grid."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-01.svg"
         alt="Depiction of the estimated regularity per cell after one refinement."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-02.svg"
         alt="Depiction of the estimated regularity per cell after two refinements."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-03.svg"
         alt="Depiction of the estimated regularity per cell after three refinements."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-04.svg"
         alt="Depiction of the estimated regularity per cell after four refinements."
         width="200" height="200">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-05.svg"
         alt="Depiction of the estimated regularity per cell after five refinements."
         width="200" height="200">
  </div>
</div>

The primary conclusion one can draw from this is that the loss of regularity at
the internal corners is a highly localized phenomenon; it only seems to impact
the cells adjacent to the corner itself, so when we refine the mesh the black
coloring is no longer visible. Besides the corners, this sequence of plots
implies that the smoothness estimates are somewhat independent of the mesh
refinement, particularly when we are far away from boundaries.
It is also obvious that the smoothness estimates are independent of the actual
size of the solution (see the picture of the solution above), as it should be.
A point of larger concern, however, is that one realizes on closer inspection
that the estimator we have overestimates the smoothness of the solution on
cells with hanging nodes. This in turn leads to higher polynomial degrees in
these areas, skewing the allocation of finite elements onto cells.

We have no good explanation for this effect at the moment. One theory is that
the numerical solution on cells with hanging nodes is, of course, constrained
and therefore not entirely free to explore the function space to get close to
the exact solution. This lack of degrees of freedom may manifest itself by
yielding numerical solutions on these cells with suppressed oscillation,
meaning a higher degree of smoothness. The estimator picks this signal up and
the estimated smoothness overestimates the actual value. However, a definite
answer to what is going on currently eludes the authors of this program.

The bigger question is, of course, how to avoid this problem. Possibilities
include estimating the smoothness not on single cells, but cell assemblies or
patches surrounding each cell. It may also be possible to find simple
correction factors for each cell depending on the number of constrained
degrees of freedom it has. In either case, there are ample opportunities for
further research on finding good $hp$-refinement criteria. On the other hand,
the main point of the current program was to demonstrate using the
$hp$-technology in deal.II, which is unaffected by our use of a possible
sub-optimal refinement criterion.



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

<h4>Different hp-decision strategies</h4>

This tutorial demonstrates only one particular strategy to decide between $h$- and
$p$-adaptation. In fact, there are many more ways to automatically decide on the
adaptation type, of which a few are already implemented in deal.II:
<ul>
  <li><i>Fourier coefficient decay:</i> This is the strategy currently
  implemented in this tutorial. For more information on this strategy, see
  the general documentation of the SmoothnessEstimator::Fourier namespace.</li>

  <li><i>Legendre coefficient decay:</i> This strategy is quite similar
  to the current one, but uses Legendre series expansion rather than the
  Fourier one: instead of sinusoids as basis functions, this strategy uses
  Legendre polynomials. Of course, since we approximate the solution using a
  finite-dimensional polynomial on each cell, the expansion of the solution in
  Legendre polynomials is also finite and, consequently, when we talk about the
  "decay" of this expansion, we can only consider the finitely many nonzero
  coefficients of this expansion, rather than think about it in asymptotic terms.
  But, if we have enough of these coefficients, we can certainly think of the
  decay of these coefficients as characteristic of the decay of the coefficients
  of the exact solution (which is, in general, not polynomial and so will have an
  infinite Legendre expansion), and considering the coefficients we have should
  reveal something about the properties of the exact solution.

  The transition from the Fourier strategy to the Legendre one is quite simple:
  You just need to change the series expansion class and the corresponding
  smoothness estimation function to be part of the proper namespaces
  FESeries::Legendre and SmoothnessEstimator::Legendre. This strategy is used
  in step-75. For the theoretical background of this strategy, consult the
  general documentation of the SmoothnessEstimator::Legendre namespace, as well
  as @cite mavriplis1994hp , @cite eibner2007hp and @cite davydov2017hp .</li>

  <li><i>Refinement history:</i> The last strategy is quite different
  from the other two. In theory, we know how the error will converge
  after changing the discretization of the function space. With
  $h$-refinement the solution converges algebraically as already pointed
  out in step-7. If the solution is sufficiently smooth, though, we
  expect that the solution will converge exponentially with increasing
  polynomial degree of the finite element. We can compare a proper
  prediction of the error with the actual error in the following step to
  see if our choice of adaptation type was justified.

  The transition to this strategy is a bit more complicated. For this, we need
  an initialization step with pure $h$- or $p$-refinement and we need to
  transfer the predicted errors over adapted meshes. The extensive
  documentation of the hp::Refinement::predict_error() function describes not
  only the theoretical details of this approach, but also presents a blueprint
  on how to implement this strategy in your code. For more information, see
  @cite melenk2001hp .

  Note that with this particular function you cannot predict the error for
  the next time step in time-dependent problems. Therefore, this strategy
  cannot be applied to this type of problem without further ado. Alternatively,
  the following approach could be used, which works for all the other
  strategies as well: start each time step with a coarse mesh, keep refining
  until happy with the result, and only then move on to the next time step.</li>
</ul>

Try implementing one of these strategies into this tutorial and observe the
subtle changes to the results. You will notice that all strategies are
capable of identifying the singularities near the reentrant corners and
will perform $h$-refinement in these regions, while preferring $p$-refinement
in the bulk domain. A detailed comparison of these strategies is presented
in @cite fehling2020 .


<h4>Parallel hp-adaptive finite elements</h4>

All functionality presented in this tutorial already works for both
sequential and parallel applications. It is possible without too much
effort to change to either the parallel::shared::Triangulation or the
parallel::distributed::Triangulation classes. If you feel eager to try
it, we recommend reading step-18 for the former and step-40 for the
latter case first for further background information on the topic, and
then come back to this tutorial to try out your newly acquired skills.

We go one step further in step-75: Here, we combine hp-adapative and
MatrixFree methods in combination with
parallel::distributed::Triangulation objects.


examples/step-28/doc/intro.dox
<br>

<i>This program was contributed by Yaqi Wang and Wolfgang
Bangerth. Results from this program are used and discussed in the publication
"Three-dimensional h-adaptivity for the multigroup neutron diffusion
equations" by Yaqi Wang, Wolfgang Bangerth and Jean Ragusa. The paper's full
bibliographic details are as follows:
@code
@Article{WBR09,
  author  = {Yaqi Wang and Wolfgang Bangerth and Jean Ragusa},
  title   = {Three-dimensional h-adaptivity for the multigroup
             neutron diffusion equations},
  journal = {Progr. Nucl. Energy},
  year    = 2009,
  volume  = 51,
  pages   = {543--555}
}
@endcode
The paper is available <a target="_top"
href="https://www.semanticscholar.org/paper/Three-dimensional-h-adaptivity-for-the-multigroup-Wang-Bangerth/900592e8e891d9b888d59a69ec58bf2bbda56b4b">here</a>.
</i>

<br>


<a name="Intro"></a> <h1>Introduction</h1>
In this example, we intend to solve the multigroup diffusion approximation of
the neutron transport equation. Essentially, the way to view this is as follows: In a
nuclear reactor, neutrons are speeding around at different energies, get
absorbed or scattered, or start a new fission
event. If viewed at long enough length scales, the movement of neutrons can be
considered a diffusion process.

A mathematical description of this would group neutrons into energy bins, and
consider the balance equations for the neutron fluxes in each of these
bins, or energy groups. The scattering, absorption, and fission events would
then be operators within the diffusion equation describing the neutron
fluxes. Assume we have energy groups $g=1,\ldots,G$, where by convention we
assume that the neutrons with the highest energy are in group 1 and those with
the lowest energy in group $G$. Then the neutron flux of each group satisfies the
following equations:
@f{eqnarray*}
\frac 1{v_g}\frac{\partial \phi_g(x,t)}{\partial t}
&=&
\nabla \cdot(D_g(x) \nabla \phi_g(x,t))
-
\Sigma_{r,g}(x)\phi_g(x,t)
\\
&& \qquad
+
\chi_g\sum_{g'=1}^G\nu\Sigma_{f,g'}(x)\phi_{g'}(x,t)
+
\sum_{g'\ne g}\Sigma_{s,g'\to g}(x)\phi_{g'}(x,t)
+
s_{\mathrm{ext},g}(x,t)
@f}
augmented by appropriate boundary conditions. Here, $v_g$ is the velocity of
neutrons within group $g$. In other words, the change in
time in flux of neutrons in group $g$ is governed by the following
processes:
<ul>
<li> Diffusion $\nabla \cdot(D_g(x) \nabla \phi_g(x,t))$. Here, $D_g$ is the
  (spatially variable) diffusion coefficient.
<li> Absorption $\Sigma_{r,g}(x)\phi_g(x,t)$ (note the
  negative sign). The coefficient $\Sigma_{r,g}$ is called the <i>removal
  cross section</i>.
<li> Nuclear fission $\chi_g\sum_{g'=1}^G\nu\Sigma_{f,g'}(x)\phi_{g'}(x,t)$.
  The production of neutrons of energy $g$ is
  proportional to the flux of neutrons of energy $g'$ times the
  probability $\Sigma_{f,g'}$ that neutrons of energy $g'$ cause a fission
  event times the number $\nu$ of neutrons produced in each fission event
  times the probability that a neutron produced in this event has energy
  $g$. $\nu\Sigma_{f,g'}$ is called the <i>fission cross section</i> and
  $\chi_g$ the <i>fission spectrum</i>. We will denote the term
  $\chi_g\nu\Sigma_{f,g'}$ as the <i>fission distribution cross
    section</i> in the program.
<li> Scattering $\sum_{g'\ne g}\Sigma_{s,g'\to g}(x)\phi_{g'}(x,t)$
  of neutrons of energy $g'$ producing neutrons
  of energy $g$. $\Sigma_{s,g'\to g}$ is called the <i>scattering cross
    section</i>. The case of elastic, in-group scattering $g'=g$ exists, too, but
  we subsume this into the removal cross section. The case $g'<g$ is called
  down-scattering, since a neutron loses energy in such an event. On the
  other hand, $g'>g$ corresponds to up-scattering: a neutron gains energy in
  a scattering event from the thermal motion of the atoms surrounding it;
  up-scattering is therefore only an important process for neutrons with
  kinetic energies that are already on the same order as the thermal kinetic
  energy (i.e. in the sub $eV$ range).
<li> An extraneous source $s_{\mathrm{ext},g}$.
</ul>

For realistic simulations in reactor analysis, one may want to split the
continuous spectrum of neutron energies into many energy groups, often up to 100.
However, if neutron energy spectra are known well enough for some type of
reactor (for example Pressurized Water Reactors, PWR), it is possible to obtain
satisfactory results with only 2 energy groups.

In the program shown in this tutorial program, we provide the structure to
compute with as many energy groups as desired. However, to keep computing
times moderate and in order to avoid tabulating hundreds of coefficients, we
only provide the coefficients for above equations for a two-group simulation,
i.e. $g=1,2$. We do, however, consider a realistic situation by assuming that
the coefficients are not constant, but rather depend on the materials that are
assembled into reactor fuel assemblies in rather complicated ways (see
below).


<h3>The eigenvalue problem</h3>

If we consider all energy groups at once, we may write above equations in the
following operator form:
@f{eqnarray*}
\frac 1v \frac{\partial \phi}{\partial t}
=
-L\phi
+
F\phi
+
X\phi
+
s_{\mathrm{ext}},
@f}
where $L,F,X$ are sinking, fission, and scattering operators,
respectively. $L$ here includes both the diffusion and removal terms. Note
that $L$ is symmetric, whereas $F$ and $X$ are not.

It is well known that this equation admits a stable solution if all
eigenvalues of the operator $-L+F+X$ are negative. This can be readily seen by
multiplying the equation by $\phi$ and integrating over the domain, leading to
@f{eqnarray*}
  \frac 1{2v} \frac{\partial}{\partial t}  \|\phi\|^2 = ((-L+F+X)\phi,\phi).
@f}
Stability means that the solution does not grow, i.e. we want the left hand
side to be less than zero, which is the case if the eigenvalues of the
operator on the right are all negative. For obvious reasons, it is
not very desirable if a nuclear reactor produces neutron fluxes that grow
exponentially, so eigenvalue analyses are the bread-and-butter of nuclear
engineers. The main point of the program is therefore to consider the
eigenvalue problem
@f{eqnarray*}
  (L-F-X) \phi = \lambda \phi,
@f}
where we want to make sure that all eigenvalues are positive. Note that $L$,
being the diffusion operator plus the absorption (removal), is positive
definite; the condition that all eigenvalues are positive therefore means that
we want to make sure that fission and inter-group scattering are weak enough
to not shift the spectrum into the negative.

In nuclear engineering, one typically looks at a slightly different
formulation of the eigenvalue problem. To this end, we do not just multiply
with $\phi$ and integrate, but rather multiply with $\phi(L-X)^{-1}$. We then
get the following evolution equation:
@f{eqnarray*}
  \frac 1{2v} \frac{\partial}{\partial t}  \|\phi\|^2_{(L-X)^{-1}} = ((L-X)^{-1}(-L+F+X)\phi,\phi).
@f}
Stability is then guaranteed if the eigenvalues of the following problem are
all negative:
@f{eqnarray*}
  (L-X)^{-1}(-L+F+X)\phi = \lambda_F \phi,
@f}
which is equivalent to the eigenvalue problem
@f{eqnarray*}
  (L-X)\phi = \frac 1{\lambda_F+1} F \phi.
@f}
The typical formulation in nuclear engineering is to write this as
@f{eqnarray*}
  (L-X) \phi = \frac 1{k_{\mathrm{eff}}} F \phi,
@f}
where $k_{\mathrm{eff}}=\frac 1{\lambda^F+1}$.
Intuitively, $k_{\mathrm{eff}}$ is something like the multiplication
factor for neutrons per typical time scale and should be less than or equal to
one for stable operation of a reactor: if it is less than one, the chain reaction will
die down, whereas nuclear bombs for example have a $k$-eigenvalue larger than
one. A stable reactor should have $k_{\mathrm{eff}}=1$.

For those who wonder how this can be achieved in practice without
inadvertently getting slightly larger than one and triggering a nuclear bomb:
first, fission processes happen on different time scales. While most neutrons
are released very quickly after a fission event, a small number of neutrons
are only released by daughter nuclei after several further decays, up to 10-60
seconds after the fission was initiated. If one is therefore slightly beyond
$k_{\mathrm{eff}}=1$, one therefore has many seconds to react until all the
neutrons created in fission re-enter the fission cycle. Nevertheless, control
rods in nuclear reactors absorbing neutrons -- and therefore reducing
$k_{\mathrm{eff}}$ -- are designed in such a way that they are all the way in
the reactor in at most 2 seconds.

One therefore has on the order of 10-60 seconds to regulate the nuclear reaction
if $k_{\mathrm{eff}}$ should be larger than one for some time, as indicated by
a growing neutron flux. Regulation can be achieved by continuously monitoring
the neutron flux, and if necessary increase or reduce neutron flux by moving
neutron-absorbing control rods a few millimeters into or out of the
reactor. On a longer scale, the water cooling the reactor contains boron, a
good neutron absorber. Every few hours, boron concentrations are adjusted by
adding boron or diluting the coolant.

Finally, some of the absorption and scattering reactions have some
stability built in; for example, higher neutron fluxes result in locally
higher temperatures, which lowers the density of water and therefore reduces
the number of scatterers that are necessary to moderate neutrons from high to
low energies before they can start fission events themselves.

In this tutorial program, we solve above $k$-eigenvalue problem for two energy
groups, and we are looking for the largest multiplication factor
$k_{\mathrm{eff}}$, which is proportional to the inverse of the minimum
eigenvalue plus one. To solve the eigenvalue problem, we generally
use a modified version of the <i>inverse power method</i>. The algorithm looks
like this:

<ol>
<li> Initialize $\phi_g$ and $k_{\mathrm{eff}}$ with $\phi_g^{(0)}$
  and $k_{\mathrm{eff}}^{(0)}$ and let $n=1$.

<li> Define the so-called <i>fission source</i> by
  @f{eqnarray*}
    s_f^{(n-1)}(x)
    =
    \frac{1}{k_{\mathrm{eff}}^{(n-1)}}
    \sum_{g'=1}^G\nu\Sigma_{f,g'}(x)\phi_{g'}^{(n-1)}(x).
  @f}

<li> Solve for all group fluxes $\phi_g,g=1,\ldots,G$ using
  @f{eqnarray*}
    -\nabla \cdot D_g\nabla \phi_g^{(n)}
    +
    \Sigma_{r,g}\phi_g^{(n)}
    =
    \chi_g s_f^{(n-1)}
    +
    \sum_{g'< g} \Sigma_{s,g'\to g} \phi_{g'}^{(n)}
    +
    \sum_{g'> g}\Sigma_{s,g'\to g}\phi_{g'}^{(n-1)}.
  @f}

<li> Update
  @f{eqnarray*}
    k_{\mathrm{eff}}^{(n)}
    =
    \sum_{g'=1}^G
    \int_{\Omega}\nu\Sigma_{f,g'}(x)
    \phi_{g'}^{(n)}(x)dx.
  @f}

<li> Compare $k_{\mathrm{eff}}^{(n)}$ with $k_{\mathrm{eff}}^{(n-1)}$.
  If the change greater than a prescribed tolerance then set $n=n+1$ repeat
  the iteration starting at step 2, otherwise end the iteration.
</ol>

Note that in this scheme, we do not solve group fluxes exactly in each power
iteration, but rather consider previously compute $\phi_{g'}^{(n)}$ only for
down-scattering events $g'<g$. Up-scattering is only treated by using old
iterators $\phi_{g'}^{(n-1)}$, in essence assuming that the scattering
operator is triangular. This is physically motivated since up-scattering does
not play a too important role in neutron scattering. In addition, practices
shows that the inverse power iteration is stable even using this
simplification.

Note also that one can use lots of extrapolation techniques to accelerate the
power iteration laid out above. However, none of these are implemented in this
example.


<h3>Meshes and mesh refinement</h3>

One may wonder whether it is appropriate to solve for the solutions of the
individual energy group equations on the same meshes. The question boils down
to this: will $\phi_g$ and $\phi_{g'}$ have similar smoothness properties? If
this is the case, then it is appropriate to use the same mesh for the two; a
typical application could be chemical combustion, where typically the
concentrations of all or most chemical species change rapidly within the flame
front. As it turns out, and as will be apparent by looking at the
graphs shown in the results section of this tutorial program, this isn't the
case here, however: since the diffusion coefficient is different for different
energy groups, fast neutrons (in bins with a small group number $g$) have a very
smooth flux function, whereas slow neutrons (in bins with a large group
number) are much more affected by the local material properties and have a
correspondingly rough solution if the coefficient are rough as in the case we
compute here. Consequently, we will want to use different meshes to compute
each energy group.

This has two implications that we will have to consider: First, we need to
find a way to refine the meshes individually. Second, assembling the source
terms for the inverse power iteration, where we have to integrate solution
$\phi_{g'}^{(n)}$ defined on mesh $g'$ against the shape functions defined on
mesh $g$, becomes a much more complicated task.


<h4>Mesh refinement</h4>

We use the usual paradigm: solve on a given mesh, then evaluate an error
indicator for each cell of each mesh we have. Because it is so convenient, we
again use the a posteriori error estimator by Kelly, Gago, Zienkiewicz
and Babuska which approximates the error per cell by integrating the jump of
the gradient of the solution along the faces of each cell. Using this, we
obtain indicators
@f{eqnarray*}
\eta_{g,K}, \qquad g=1,2,\ldots,G,\qquad K\in{\cal T}_g,
@f}
where ${\cal T}_g$ is the triangulation used in the solution of
$\phi_g$. The question is what to do with this. For one, it is clear that
refining only those cells with the highest error indicators might lead to bad
results. To understand this, it is important to realize that $\eta_{g,K}$
scales with the second derivative of $\phi_g$. In other words, if we have two
energy groups $g=1,2$ whose solutions are equally smooth but where one is
larger by a factor of 10,000, for example, then only the cells of that mesh
will be refined, whereas the mesh for the solution of small magnitude will
remain coarse. This is probably not what one wants, since we can consider both
components of the solution equally important.

In essence, we would therefore have to scale $\eta_{g,K}$ by an importance
factor $z_g$ that says how important it is to resolve $\phi_g$ to any given
accuracy. Such important factors can be computed using duality techniques
(see, for example, the step-14 tutorial program, and the
reference to the book by Bangerth and Rannacher cited there). We won't go
there, however, and simply assume that all energy groups are equally
important, and will therefore normalize the error indicators $\eta_{g,K}$ for
group $g$ by the maximum of the solution $\phi_g$. We then refine the cells
whose errors satisfy
@f{eqnarray*}
  \frac{\eta_{g,K}}{\|\phi_g\|_\infty}
  >
  \alpha_1
  \displaystyle{\max_{\begin{matrix}1\le g\le G \\ K\in {\cal T}_g\end{matrix}}
    \frac{\eta_{g,K}}{\|\phi_g\|_\infty}}
@f}
and coarsen the cells where
@f{eqnarray*}
  \frac{\eta_{g,K}}{\|\phi_g\|_\infty}
  <
  \alpha_2
  \displaystyle{\max_{\begin{matrix}1\le g\le G \\ K\in {\cal T}_g\end{matrix}}
    \frac{\eta_{g,K}}{\|\phi_g\|_\infty}}.
@f}
We chose $\alpha_1=0.3$ and $\alpha_2=0.01$ in the code. Note that this will,
of course, lead to different meshes for the different energy groups.

The strategy above essentially means the following: If for energy group $g$
there are many cells $K\in {\cal T}_g$ on which the error is large, for
example because the solution is globally very rough, then many cells will be
above the threshold. On the other hand, if there are a few cells with large
and many with small errors, for example because the solution is overall rather
smooth except at a few places, then only the few cells with large errors will
be refined. Consequently, the strategy allows for meshes that track the global
smoothness properties of the corresponding solutions rather well.


<h4>Assembling terms on different meshes</h4>

As pointed out above, the multigroup refinement strategy results in
different meshes for the different solutions $\phi_g$. So what's the problem?
In essence it goes like this: in step 3 of the eigenvalue iteration, we have
form the weak form for the equation to compute $\phi_g^{(n)}$ as usual by
multiplication with test functions $\varphi_g^i$ defined on the mesh for
energy group $g$; in the process, we have to
compute the right hand side vector that contains terms of the following form:
@f{eqnarray*}
  F_i = \int_\Omega f(x) \varphi_g^i(x) \phi_{g'}(x) \ dx,
@f}
where $f(x)$ is one of the coefficient functions $\Sigma_{s,g'\to g}$ or
$\nu\chi_g\Sigma_{f,g'}$ used in the right hand side
of eigenvalue equation. The difficulty now is that $\phi_{g'}$ is defined on
the mesh for energy group $g'$, i.e. it can be expanded as
$\phi_{g'}(x)=\sum_j\phi_{g'}^j \varphi_{g'}^j(x)$, with basis functions
$\varphi_{g'}^j(x)$ defined on mesh $g'$. The contribution to the right hand
side can therefore be written as
@f{eqnarray*}
  F_i = \sum_j \left\{\int_\Omega f(x) \varphi_g^i(x) \varphi_{g'}^j(x)
  \ dx \right\} \phi_{g'}^j ,
@f}
On the other hand, the test functions $\varphi_g^i(x)$ are defined on mesh
$g$. This means that we can't just split the integral $\Omega$ into integrals
over the cells of either mesh $g$ or $g'$, since the respectively other basis
functions may not be defined on these cells.

The solution to this problem lies in the fact that both the meshes for $g$ and
$g'$ are derived by adaptive refinement from a common coarse mesh. We can
therefore always find a set of cells, which we denote by ${\cal T}_g \cap
{\cal T}_{g'}$, that satisfy the following conditions:
<ul>
<li> the union of the cells covers the entire domain, and
<li> a cell $K \in {\cal T}_g \cap {\cal T}_{g'}$ is active on at least
  one of the two meshes.
</ul>
A way to construct this set is to take each cell of coarse mesh and do the
following steps: (i) if the cell is active on either ${\cal T}_g$ or
${\cal T}_{g'}$, then add this cell to the set; (ii) otherwise, i.e. if
this cell has children on both meshes, then do step (i) for each of the
children of this cell. In fact, deal.II has a function
GridTools::get_finest_common_cells that computes exactly this set
of cells that are active on at least one of two meshes.

With this, we can write above integral as follows:
@f{eqnarray*}
  F_i
  =
  \sum_{K \in {\cal T}_g \cap {\cal T}_{g'}}
  \sum_j \left\{\int_K f(x) \varphi_g^i(x) \varphi_{g'}^j(x)
  \ dx \right\} \phi_{g'}^j.
@f}
 In the code, we
compute the right hand side in the function
<code>NeutronDiffusionProblem::assemble_rhs</code>, where (among other things) we
loop over the set of common most refined cells, calling the function
<code>NeutronDiffusionProblem::assemble_common_cell</code> on each pair of
these cells.

By construction, there are now three cases to be considered:
<ol>
<li> The cell $K$ is active on both meshes, i.e. both the basis
  functions $\varphi_g^i$ as well as $\varphi_{g'}^j$ are defined on $K$.
<li> The cell $K$ is active on mesh $g$, but not $g'$, i.e. the
  $\varphi_g^i$  are defined on $K$, whereas the $\varphi_{g'}^j$ are defined
  on children of $K$.
<li> The cell $K$ is active on mesh $g'$, but not $g$, with opposite
  conclusions than in (ii).
</ol>

To compute the right hand side above, we then need to have different code for
these three cases, as follows:
<ol>
<li> If the cell $K$ is active on both meshes, then we can directly
  evaluate the integral. In fact, we don't even have to bother with the basis
  functions $\varphi_{g'}$, since all we need is the values of $\phi_{g'}$ at
  the quadrature points. We can do this using the
  FEValues::get_function_values function. This is done directly in
  the <code>NeutronDiffusionProblem::assemble_common_cell</code> function.

<li> If the cell $K$ is active on mesh $g$, but not $g'$, then the
  basis functions $\varphi_{g'}^j$ are only defined either on the children
  $K_c,0\le c<2^{\texttt{dim}}$, or on children of these children if cell $K$
  is refined more than once on mesh $g'$.

  Let us assume for a second that $K$ is only once more refined on mesh $g'$
  than on mesh $g$. Using the fact that we use embedded finite element spaces
  where each basis function on one mesh can be written as a linear combination
  of basis functions on the next refined mesh, we can expand the restriction
  of $\phi_g^i$ to child cell $K_c$ into the basis functions defined on that
  child cell (i.e. on cells on which the basis functions $\varphi_{g'}^l$ are
  defined):
  @f{eqnarray*}
    \phi_g^i|_{K_c} = B_c^{il} \varphi_{g'}^l|_{K_c}.
  @f}
  Here, and in the following, summation over indices appearing twice is
  implied. The matrix $B_c$ is the matrix that interpolated data from a cell
  to its $c$-th child.

  Then we can write the contribution of cell $K$ to the right hand side
  component $F_i$ as
  @f{eqnarray*}
    F_i|_K
    &=&
    \left\{ \int_K f(x) \varphi_g^i(x) \varphi_{g'}^j(x)
    \ dx \right\} \phi_{g'}^j
    \\
    &=&
    \left\{
    \sum_{0\le c<2^{\texttt{dim}}}
    B_c^{il} \int_{K_c} f(x) \varphi_{g'}^l(x) \varphi_{g'}^j(x)
    \ dx \right\} \phi_{g'}^j.
  @f}
  In matrix notation, this can be written as
  @f{eqnarray*}
    F_i|_K
    =
    \sum_{0\le c<2^{\texttt{dim}}}
    F_i|_{K_c},
    \qquad
    \qquad
    F_i|_{K_c} = B_c^{il} M_{K_c}^{lj}  \phi_{g'}^j
    = (B_c M_{K_c})^{ij} \phi_{g'}^j,
  @f}
  where $M_{K_c}^{lj}=\int_{K_c} f(x) \varphi_{g'}^l(x) \varphi_{g'}^j(x)$ is
  the weighted mass matrix on child $c$ of cell $K$.

  The next question is what happens if a child $K_c$ of $K$ is not
  active. Then, we have to apply the process recursively, i.e. we have to
  interpolate the basis functions $\varphi_g^i$ onto child $K_c$ of $K$, then
  onto child $K_{cc'}$ of that cell, onto child $K_{cc'c''}$ of that one, etc,
  until we find an active cell. We then have to sum up all the contributions
  from all the children, grandchildren, etc, of cell $K$, with contributions
  of the form
  @f{eqnarray*}
    F_i|_{K_{cc'}} = (B_cB_{c'} M_{K_{cc'}})^{ij}  \phi_{g'}^j,
  @f}
  or
  @f{eqnarray*}
    F_i|_{K_{cc'c''}} = (B_c B_{c'} B_{c''}M_{K_{cc'c''}})^{ij}
    \phi_{g'}^j,
  @f}
  etc. We do this process recursively, i.e. if we sit on cell $K$ and see that
  it has children on grid $g'$, then we call a function
  <code>assemble_case_2</code> with an identity matrix; the function will
  multiply it's argument from the left with the prolongation matrix; if the
  cell has further children, it will call itself with this new matrix,
  otherwise it will perform the integration.

<li> The last case is where $K$ is active on mesh $g'$ but not mesh
  $g$. In that case, we have to express basis function $\varphi_{g'}^j$ in
  terms of the basis functions defined on the children of cell $K$, rather
  than $\varphi_g^i$ as before. This of course works in exactly the same
  way. If the children of $K$ are active on mesh $g$, then
  leading to the expression
  @f{eqnarray*}
    F_i|_K
    &=&
    \left\{ \int_K f(x) \varphi_g^i(x) \varphi_{g'}^j(x)
    \ dx \right\} \phi_{g'}^j
    \\
    &=&
    \left\{
    \sum_{0\le c<2^{\texttt{dim}}}
    \int_{K_c} f(x) \varphi_g^i(x) B_c^{jl} \varphi_{g}^l(x)
    \ dx \right\} \phi_{g'}^j.
  @f}
  In matrix notation, this expression now reads as
  @f{eqnarray*}
    F_i|_K
    =
    \sum_{0\le c<2^{\texttt{dim}}}
    F_i|_{K_c},
    \qquad
    \qquad
    F_i|_{K_c} = M_{K_c}^{il} B_c^{jl}  \phi_{g'}^j
    =
    (M_{K_c} B_c^T)^{ij} \phi_{g'}^j,
  @f}
  and correspondingly for cases where cell $K$ is refined more than once on
  mesh $g$:
  @f{eqnarray*}
    F_i|_{K_{cc'}} = (M_{K_{cc'}} B_{c'}^T B_c^T)^{ij}  \phi_{g'}^j,
  @f}
  or
  @f{eqnarray*}
    F_i|_{K_{cc'c''}} = (M_{K_{cc'c''}} B_{c''}^T B_{c'}^T B_c^T)^{ij}
    \phi_{g'}^j,
  @f}
  etc. In other words, the process works in exactly the same way as before,
  except that we have to take the transpose of the prolongation matrices and
  need to multiply it to the mass matrix from the other side.
</ol>


The expressions for cases (ii) and (iii) can be understood as repeatedly
interpolating either the left or right basis functions in the scalar product
$(f \varphi_g^i, \varphi_{g'}^j)_K$ onto child cells, and then finally
forming the inner product (the mass matrix) on the final cell. To make the
symmetry in these cases more obvious, we can write them like this: for case
(ii), we have
@f{eqnarray*}
  F_i|_{K_{cc'\cdots c^{(k)}}}
  = [B_c B_{c'} \cdots B_{c^{(k)}} M_{K_{cc'\cdots c^{(k)}}}]^{ij}
    \phi_{g'}^j,
@f}
whereas for case (iii) we get
@f{eqnarray*}
  F_i|_{K_{cc'\cdots c^{(k)}}}
  = [(B_c B_{c'} \cdots B_{c^{(k)}} M_{K_{cc'\cdots c^{(k)}}})^T]^{ij}
    \phi_{g'}^j,
@f}



<h3>Description of the test case</h3>

A nuclear reactor core is composed of different types of assemblies. An
assembly is essentially the smallest unit that can be moved in and out of a
reactor, and is usually rectangular or square. However, assemblies are not
fixed units, as they are assembled from a complex lattice of different fuel
rods, control rods, and instrumentation elements that are held in place
relative to each other by spacers that are permanently attached to the rods.
To make things more complicated, there are different kinds of assemblies that
are used at the same time in a reactor, where assemblies differ in the type
and arrangement of rods they are made up of.

Obviously, the arrangement of assemblies as well as the arrangement of rods
inside them affect the distribution of neutron fluxes in the reactor (a fact
that will be obvious by looking at the solution shown below in the results
sections of this program). Fuel rods, for example, differ from each other in
the enrichment of U-235 or Pu-239. Control rods, on the other hand, have zero
fission, but nonzero scattering and absorption cross sections.

This whole arrangement would make the description or spatially dependent
material parameters very complicated. It will not become much simpler, but we
will make one approximation: we merge the volume inhabited by each cylindrical
rod and the surrounding water into volumes of quadratic cross section into
so-called `pin cells' for which homogenized material data are obtained with
nuclear database and knowledge of neutron spectrum. The homogenization makes
all material data piecewise constant on the solution domain for a reactor with
fresh fuel. Spatially dependent material parameters are then looked up for the
quadratic assembly in which a point is located, and then for the quadratic pin
cell within this assembly.

In this tutorial program, we simulate a quarter of a reactor consisting of $4
\times 4$ assemblies. We use symmetry (Neumann) boundary conditions to reduce
the problem to one quarter of the domain, and consequently only simulate a
$2\times 2$ set of assemblies. Two of them will be UO${}_2$ fuel, the other
two of them MOX fuel. Each of these assemblies consists of $17\times 17$ rods
of different compositions. In total, we therefore create a $34\times 34$
lattice of rods. To make things simpler later on, we reflect this fact by
creating a coarse mesh of $34\times 34$ cells (even though the domain is a
square, for which we would usually use a single cell). In deal.II, each cell
has a <code>material_id</code> which one may use to associated each cell with a
particular number identifying the material from which this cell's volume is
made of; we will use this material ID to identify which of the 8 different
kinds of rods that are used in this testcase make up a particular cell. Note
that upon mesh refinement, the children of a cell inherit the material ID,
making it simple to track the material even after mesh refinement.

The arrangement of the rods will be clearly visible in the images shown in
the results section. The cross sections for materials and for both energy
groups are taken from a OECD/NEA benchmark problem. The detailed configuration
and material data is given in the code.


<h3>What the program does (and how it does that)</h3>

As a coarse overview of what exactly the program does, here is the basic
layout: starting on a coarse mesh that is the same for each energy group, we
compute inverse eigenvalue iterations to compute the $k$-eigenvalue on a given
set of meshes. We stop these iterations when the change in the eigenvalue
drops below a certain tolerance, and then write out the meshes and solutions
for each energy group for inspection by a graphics program. Because the meshes
for the solutions are different, we have to generate a separate output file
for each energy group, rather than being able to add all energy group
solutions into the same file.

After this, we evaluate the error indicators as explained in one of the sections
above for each of the meshes, and refine and coarsen the cells of each mesh
independently. Since the eigenvalue iterations are fairly expensive, we don't
want to start all over on the new mesh; rather, we use the SolutionTransfer
class to interpolate the solution on the previous mesh to the next one upon
mesh refinement. A simple experiment will convince you that this is a lot
cheaper than if we omitted this step. After doing so, we resume our eigenvalue
iterations on the next set of meshes.

The program is controlled by a parameter file, using the ParameterHandler
class. We will show a
parameter file in the results section of this tutorial. For the moment suffice
it to say that it controls the polynomial degree of the finite elements used,
the number of energy groups (even though all that is presently implemented are
the coefficients for a 2-group problem), the tolerance where to stop the
inverse eigenvalue iteration, and the number of refinement cycles we will do.


examples/step-28/doc/results.dox
<h1>Results</h1>

We can run the program with the following input file:
@code
# Listing of Parameters
# ---------------------
# Polynomial degree of the finite element to be used
set Finite element degree     = 2

# The number of energy different groups considered
set Number of energy groups   = 2

# Inner power iterations are stopped when the change in k_eff falls below this
# tolerance
set Power iteration tolerance = 1e-12

# Number of refinement cycles to be performed
set Refinement cycles         = 12
@endcode
The output of this program then consists of the console output, a file
named `convergence_table' to record main results of mesh iteration,
and the graphical output in vtu format.

The console output looks like this:
@code
Cycle 0:
   Numbers of active cells:       1156 1156
   Numbers of degrees of freedom: 4761 4761

Iter number: 1 k_eff=319.375676634310 flux_ratio=6.836246075630 max_thermal=1.433899030144
Iter number: 2 k_eff=0.834072546055 flux_ratio=5.204601882144 max_thermal=0.004630925876
Iter number: 3 k_eff=0.862826188043 flux_ratio=4.645051765984 max_thermal=0.005380396338
...
Iter number:69 k_eff=0.906841960370 flux_ratio=4.384056022578 max_thermal=0.008466414246
Iter number:70 k_eff=0.906841960371 flux_ratio=4.384056022583 max_thermal=0.008466414246

   Cycle=0, n_dofs=9522,  k_eff=0.906841960371, time=7.623425000000


Cycle 1:
   Numbers of active cells:       1156 2380
   Numbers of degrees of freedom: 4761 10667

Iter number: 1 k_eff=0.906838267472 flux_ratio=4.385474405125 max_thermal=0.008463675976
...

Cycle 11:
   Numbers of active cells:       11749 47074
   Numbers of degrees of freedom: 50261 204523

Iter number: 1 k_eff=0.906798057750 flux_ratio=4.384878772166 max_thermal=0.008464822382
Iter number: 2 k_eff=0.906833008185 flux_ratio=4.384868138638 max_thermal=0.008465057191
...
Iter number:32 k_eff=0.906834736550 flux_ratio=4.384846081793 max_thermal=0.008465019607
Iter number:33 k_eff=0.906834736551 flux_ratio=4.384846081798 max_thermal=0.008465019607

   Cycle=11, n_dofs=254784,  k_eff=0.906834736551, time=238.593762000000
@endcode

We see that power iteration does converge faster after cycle 0 due to the initialization
with solution from last mesh iteration.
The contents of `convergence_table' are,
@code
0 4761 4761 0.906841960371 4.38405602258
1 4761 10667 0.906837901031 4.38548908776
2 4761 18805 0.906836075928 4.3854666475
3 6629 27301 0.90683550011 4.38540458087
4 12263 48095 0.906835001796 4.38538179873
5 17501 69297 0.906834858174 4.38485382341
6 19933 78605 0.90683482406 4.38485065879
7 23979 93275 0.906834787555 4.38484837926
8 30285 117017 0.906834761604 4.38484654495
9 40087 154355 0.906834746215 4.38484608319
10 45467 179469 0.906834740155 4.38484600505
11 50261 204523 0.906834736551 4.3848460818
@endcode
The meanings of columns are: number of mesh iteration, numbers of degrees of
 freedom of fast energy group, numbers of DoFs of thermal group, converged
k-effective and the ratio between maximum of fast flux and maximum of thermal one.

The grids of fast and thermal energy groups at mesh iteration #9 look
as follows:

<img width="400" src="https://www.dealii.org/images/steps/developer/step-28.grid-0.9.order2.png" alt="">
&nbsp;
<img width="400" src="https://www.dealii.org/images/steps/developer/step-28.grid-1.9.order2.png" alt="">

We see that the grid of thermal group is much finer than the one of fast group.
The solutions on these grids are, (Note: flux are normalized with total fission
source equal to 1)

<img width="400" src="https://www.dealii.org/images/steps/developer/step-28.solution-0.9.order2.png" alt="">
&nbsp;
<img width="400" src="https://www.dealii.org/images/steps/developer/step-28.solution-1.9.order2.png" alt="">

Then we plot the convergence data with polynomial order being equal to 1,2 and 3.

<img src="https://www.dealii.org/images/steps/developer/step-28.convergence.png" alt="">

The estimated `exact' k-effective = 0.906834721253 which is simply from last
mesh iteration of polynomial order 3 minus 2e-10. We see that h-adaptive calculations
deliver an algebraic convergence. And the higher polynomial order is, the faster mesh
iteration converges. In our problem, we need smaller number of DoFs to achieve same
accuracy with higher polynomial order.


examples/step-29/doc/intro.dox
<br>

<i>
This program was contributed by Moritz Allmaras at Texas A&amp;M
University. Some of the work on this tutorial program has been funded
by NSF under grant DMS-0604778.
</i>

<b>Note:</b> In order to run this program, deal.II must be configured to use
the UMFPACK sparse direct solver. Refer to the <a
href="../../readme.html#umfpack">ReadMe</a> for instructions how to do this.


<a name="Intro"></a>
<h1>Introduction</h1>


A question that comes up frequently is how to solve problems involving complex
valued functions with deal.II. For many problems, instead of working with
complex valued finite elements directly, it is often more convenient to split complex valued
functions into their real and imaginary parts and use separate scalar finite
element fields for discretizing each one of them. Basically this amounts to
viewing a single complex valued equation as a system of two real valued
equations. This short example demonstrates how this can be implemented in
deal.II by using an <code>FE_system</code> object to stack two finite element
fields representing real and imaginary parts. (The opposite approach,
keeping everything complex-valued, is demonstrated in a different
tutorial program: see step-58 for this.)
When split into real and imaginary parts, the equations covered here
fall into the class of vector-valued problems. A toplevel overview of
this topic can be found in the @ref vector_valued module.

In addition to this discussion, we also discuss the ParameterHandler
class, which provides a convenient way for reading parameters from a
configuration file at runtime without the need to recompile the
program code.


<h3>Problem setting</h3>

The original purpose of this program is to simulate the focusing properties
of an ultrasound wave generated by a transducer lens with variable
geometry. Recent applications in medical imaging use ultrasound waves not only
for imaging purposes, but also to excite certain local effects in a
material, like changes in optical properties, that can then be measured by
other imaging techniques. A vital ingredient for these methods is the ability
to focus the intensity of the ultrasound wave in a particular part of the
material, ideally in a point, to be able to examine the properties of the
material at that particular location.

To derive a model for this problem, we think of ultrasound as a pressure wave
governed by the wave equation:
@f[
	\frac{\partial^2 U}{\partial t^2}	-	c^2 \Delta U = 0
@f]
where $c$ is the wave speed (that for simplicity we assume to be constant), $U
= U(x,t),\;x \in \Omega,\;t\in\mathrm{R}$. The boundary
$\Gamma=\partial\Omega$ is divided into two parts $\Gamma_1$ and
$\Gamma_2=\Gamma\setminus\Gamma_1$, with $\Gamma_1$ representing the
transducer lens and $\Gamma_2$ an absorbing boundary (that is, we want to
choose boundary conditions on $\Gamma_2$ in such a way that they imitate a
larger domain). On $\Gamma_1$, the transducer generates a wave of constant
frequency ${\omega}>0$ and constant amplitude (that we chose to be 1 here):
@f[
U(x,t) = \cos{\omega t}, \qquad x\in \Gamma_1
@f]

If there are no other (interior or boundary) sources, and since the only
source has frequency $\omega$, then the solution admits a separation of
variables of the form $U(x,t) = \textrm{Re}\left(u(x)\,e^{i\omega
t})\right)$. The complex-valued function $u(x)$ describes the spatial
dependency of amplitude and phase (relative to the source) of the waves of
frequency ${\omega}$, with the amplitude being the quantity that we are
interested in. By plugging this form of the solution into the wave equation,
we see that for $u$ we have
@f{eqnarray*}
-\omega^2 u(x) - c^2\Delta u(x) &=& 0, \qquad x\in\Omega,\\
u(x) &=& 1,  \qquad x\in\Gamma_1.
@f}

For finding suitable conditions on $\Gamma_2$ that model an absorbing
boundary, consider a wave of the form $V(x,t)=e^{i(k\cdot x -\omega t)}$ with
frequency ${\omega}$ traveling in direction $k\in {\mathrm{R}^2}$. In order
for $V$ to solve the wave equation, $|k|={\frac{\omega}{c}}$ must
hold. Suppose that this wave hits the boundary in $x_0\in\Gamma_2$ at a right
angle, i.e. $n=\frac{k}{|k|}$ with $n$ denoting the outer unit normal of
$\Omega$ in $x_0$. Then at $x_0$, this wave satisfies the equation
@f[
c (n\cdot\nabla V) + \frac{\partial V}{\partial t} = (i\, c\, |k| - i\, \omega) V = 0.
@f]
Hence, by enforcing the boundary condition
@f[
c (n\cdot\nabla U) + \frac{\partial U}{\partial t} = 0, \qquad x\in\Gamma_2,
@f]
waves that hit the boundary $\Gamma_2$ at a right angle will be perfectly
absorbed. On the other hand, those parts of the wave field that do not hit a
boundary at a right angle do not satisfy this condition and enforcing it as a
boundary condition will yield partial reflections, i.e. only parts of the wave
will pass through the boundary as if it wasn't here whereas the remaining
fraction of the wave will be reflected back into the domain.

If we are willing to accept this as a sufficient approximation to an absorbing
boundary we finally arrive at the following problem for $u$:
@f{eqnarray*}
-\omega^2 u - c^2\Delta u &=& 0, \qquad x\in\Omega,\\
c (n\cdot\nabla u) + i\,\omega\,u &=&0, \qquad x\in\Gamma_2,\\
u &=& 1,  \qquad x\in\Gamma_1.
@f}
This is a Helmholtz equation (similar to the one in step-7, but this time with
''the bad sign'') with Dirichlet data on $\Gamma_1$ and mixed boundary
conditions on $\Gamma_2$. Because of the condition on $\Gamma_2$, we cannot just
treat the equations for real and imaginary parts of $u$ separately. What we can
do however is to view the PDE for $u$ as a system of two PDEs for the real and
imaginary parts of $u$, with the boundary condition on $\Gamma_2$ representing
the coupling terms between the two components of the system. This works along
the following lines: Let $v=\textrm{Re}\;u,\; w=\textrm{Im}\;u$, then in terms
of $v$ and $w$ we have the following system:
@f{eqnarray*}
  \left.\begin{array}{ccc}
    -\omega^2 v - c^2\Delta v &=& 0 \quad\\
    -\omega^2 w - c^2\Delta w &=& 0 \quad
  \end{array}\right\} &\;& x\in\Omega,
	\\
  \left.\begin{array}{ccc}
    c (n\cdot\nabla v) - \omega\,w &=& 0 \quad\\
    c (n\cdot\nabla w) + \omega\,v &=& 0 \quad
  \end{array}\right\} &\;& x\in\Gamma_2,
	\\
	\left.\begin{array}{ccc}
    v &=& 1 \quad\\
    w &=& 0 \quad
  \end{array}\right\} &\;& x\in\Gamma_1.
@f}

For test functions $\phi,\psi$ with $\phi|_{\Gamma_1}=\psi|_{\Gamma_1}=0$, after
the usual multiplication, integration over $\Omega$ and applying integration by
parts, we get the weak formulation
@f{eqnarray*}
-\omega^2 \langle \phi, v \rangle_{\mathrm{L}^2(\Omega)}
+ c^2 \langle \nabla \phi, \nabla v \rangle_{\mathrm{L}^2(\Omega)}
- c \omega \langle \phi, w \rangle_{\mathrm{L}^2(\Gamma_2)} &=& 0, \\
-\omega^2 \langle \psi, w \rangle_{\mathrm{L}^2(\Omega)}
+ c^2 \langle \nabla \psi, \nabla w \rangle_{\mathrm{L}^2(\Omega)}
+ c \omega \langle \psi, v \rangle_{\mathrm{L}^2(\Gamma_2)} &=& 0.
@f}

We choose finite element spaces $V_h$ and $W_h$ with bases $\{\phi_j\}_{j=1}^n,
\{\psi_j\}_{j=1}^n$ and look for approximate solutions
@f[
v_h = \sum_{j=1}^n \alpha_j \phi_j, \;\; w_h = \sum_{j=1}^n \beta_j \psi_j.
@f]
Plugging into the variational form yields the equation system
@f[
\renewcommand{\arraystretch}{2.0}
\left.\begin{array}{ccc}
\sum_{j=1}^n
\left(
-\omega^2 \langle \phi_i, \phi_j \rangle_{\mathrm{L}^2(\Omega)}
+ c^2 \langle \nabla \phi_i, \nabla \phi_j \rangle_{\mathrm{L}^2(\Omega)}
\right)
\alpha_j
- \left(
c\omega \langle \phi_i,\psi_j\rangle_{\mathrm{L}^2(\Gamma_2)}\right)\beta_j
&=& 0 \\
\sum_{j=1}^n
\left(
-\omega^2 \langle \psi_i, \psi_j \rangle_{\mathrm{L}^2(\Omega)}
+ c^2 \langle \nabla \psi_i, \nabla \psi_j \rangle_{\mathrm{L}^2(\Omega)}
\right)\beta_j
+ \left(
c\omega \langle
\psi_i,\phi_j\rangle_{\mathrm{L}^2(\Gamma_2)}
\right)\alpha_j
&=& 0
\end{array}\right\}\;\;\forall\; i =1,\ldots,n.
@f]
In matrix notation:
@f[
\renewcommand{\arraystretch}{2.0}
\left(
\begin{array}{cc}
-\omega^2 \langle \phi_i, \phi_j \rangle_{\mathrm{L}^2(\Omega)}
+ c^2 \langle \nabla \phi_i, \nabla \phi_j \rangle_{\mathrm{L}^2(\Omega)}
& -c\omega \langle \phi_i,\psi_j\rangle_{\mathrm{L}^2(\Gamma_2)} \\
c\omega \langle \psi_i,\phi_j\rangle_{\mathrm{L}^2(\Gamma_2)}
& -\omega^2 \langle \psi_{i}, \psi_j \rangle_{\mathrm{L}^2(\Omega)}
+ c^2 \langle \nabla \psi_{i}, \nabla \psi_j  \rangle_{\mathrm{L}^2(\Omega)}
\end{array}
\right)
\left(
\begin{array}{c}
\alpha \\ \beta
\end{array}
\right)
=
\left(
\begin{array}{c}
0 \\ 0
\end{array}
\right)
@f]
(One should not be fooled by the right hand side being zero here, that is
because we haven't included the Dirichlet boundary data yet.)
Because of the alternating sign in the off-diagonal blocks, we can already
see that this system is non-symmetric, in fact it is even indefinite.
Of course, there is no necessity to choose the spaces $V_h$ and $W_h$ to be
the same. However, we expect real and imaginary part of the solution to
have similar properties and will therefore indeed take $V_h=W_h$ in the
implementation, and also use the same basis functions $\phi_i = \psi_i$ for
both spaces. The reason for the notation using different symbols is just that
it allows us to distinguish between shape functions for $v$ and $w$, as this
distinction plays an important role in the implementation.


<h3>The test case</h3>

For the computations, we will consider wave propagation in the unit square,
with ultrasound generated by a transducer lens that is shaped like a segment
of the circle with center at $(0.5, d)$ and a
radius slightly greater than $d$; this shape should lead to a focusing of the sound
wave at the center of the circle. Varying $d$ changes the "focus" of the lens
and affects the spatial distribution of the intensity of $u$, where our main
concern is how well $|u|=\sqrt{v^2+w^2}$ is focused.

In the program below, we will implement the complex-valued Helmholtz equations
using the formulation with split real and imaginary parts. We will also
discuss how to generate a domain that looks like a square with a slight bulge
simulating the transducer (in the
<code>UltrasoundProblem<dim>::make_grid()</code> function), and how to
generate graphical output that not only contains the solution components $v$ and
$w$, but also the magnitude $\sqrt{v^2+w^2}$ directly in the output file (in
<code>UltrasoundProblem<dim>::output_results()</code>). Finally, we use the
ParameterHandler class to easily read parameters like the focal distance $d$,
wave speed $c$, frequency $\omega$, and a number of other parameters from an
input file at run-time, rather than fixing those parameters in the source code
where we would have to re-compile every time we want to change parameters.


examples/step-29/doc/results.dox
<a name="Results"></a>
<h1>Results</h1>

The current program reads its run-time parameters from an input file
called <code>\step-29.prm</code> that looks like this:
@code
subsection Mesh & geometry parameters
  # Distance of the focal point of the lens to the x-axis
  set Focal distance        = 0.3

  # Number of global mesh refinement steps applied to initial coarse grid
  set Number of refinements = 5
end


subsection Physical constants
  # Wave speed
  set c     = 1.5e5

  # Frequency
  set omega = 3.0e7
end


subsection Output parameters
  # Name of the output file (without extension)
  set Output file   = solution

  # A name for the output format to be used
  set Output format = vtu
end
@endcode

As can be seen, we set
$d=0.3$, which amounts to a focus of the transducer lens
at $x=0.5$, $y=0.3$. The coarse mesh is refined 5 times,
resulting in 160x160 cells, and the output is written in vtu
format. The parameter reader understands many more parameters
pertaining in particular to the generation of output, but we
need none of these parameters here and therefore stick with
their default values.

Here's the console output of the program in debug mode:

@code
> make run
[ 66%] Built target step-29
[100%] Run step-29 with Debug configuration
Generating grid... done (0.820449s)
  Number of active cells:  25600
Setting up system... done (1.18392s)
  Number of degrees of freedom: 51842
Assembling system matrix... done (2.33291s)
Solving linear system... done (1.34837s)
Generating output... done (2.05782s)
[100%] Built target run
@endcode

(Of course, execution times will differ if you run the program
locally.) The fact that most of the time is spent on assembling
the system matrix and generating output is due to the many assertions
that need to be checked in debug mode. In release mode these parts
of the program run much faster whereas solving the linear system is
hardly sped up at all:

@code
> make run
[ 66%] Built target step-29
Scanning dependencies of target run
[100%] Run step-29 with Release configuration
DEAL::Generating grid... done (0.0144960s)
DEAL::  Number of active cells:  25600
DEAL::Setting up system... done (0.0356880s)
DEAL::  Number of degrees of freedom: 51842
DEAL::Assembling system matrix... done (0.0436570s)
DEAL::Solving linear system... done (1.54733s)
DEAL::Generating output... done (0.720528s)
[100%] Built target run
@endcode

The graphical output of the program looks as follows:


<table align="center" class="doxtable">
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-29.v.png" alt="v = Re(u)">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-29.w.png" alt="w = Im(u)">
    </td>
  </tr>
  <tr>
    <td colspan="2">
      <img src="https://www.dealii.org/images/steps/developer/step-29.intensity.png" alt="|u|">
    </td>
  </tr>
</table>

The first two pictures show the real and imaginary parts of
$u$, whereas the last shows the intensity $|u|$. One can clearly
see that the intensity is focused around the focal point of the
lens (0.5, 0.3), and that the focus
is rather sharp in $x$-direction but more blurred in $y$-direction, which is a
consequence of the geometry of the focusing lens, its finite aperture,
and the wave nature of the problem.

Because colorful graphics are always fun, and to stress the focusing
effects some more, here is another set of images highlighting how well
the intensity is actually focused in $x$-direction:

<table align="center" class="doxtable">
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-29.surface.png" alt="|u|">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-29.contours.png" alt="|u|">
    </td>
  </tr>
</table>


As a final note, the structure of the program makes it easy to
determine which parts of the program scale nicely as the mesh is
refined and which parts don't. Here are the run times for 5, 6, and 7
global refinements:

@code
> make run
[ 66%] Built target step-29
[100%] Run step-29 with Release configuration
DEAL::Generating grid... done (0.0135260s)
DEAL::  Number of active cells:  25600
DEAL::Setting up system... done (0.0213910s)
DEAL::  Number of degrees of freedom: 51842
DEAL::Assembling system matrix... done (0.0414300s)
DEAL::Solving linear system... done (1.56621s)
DEAL::Generating output... done (0.729605s)
[100%] Built target run

> make run
[ 66%] Built target step-29
[100%] Run step-29 with Release configuration
DEAL::Generating grid... done (0.0668490s)
DEAL::  Number of active cells:  102400
DEAL::Setting up system... done (0.109694s)
DEAL::  Number of degrees of freedom: 206082
DEAL::Assembling system matrix... done (0.160784s)
DEAL::Solving linear system... done (7.86577s)
DEAL::Generating output... done (2.89320s)
[100%] Built target run

> make run
[ 66%] Built target step-29
[100%] Run step-29 with Release configuration
DEAL::Generating grid... done (0.293154s)
DEAL::  Number of active cells:  409600
DEAL::Setting up system... done (0.491301s)
DEAL::  Number of degrees of freedom: 821762
DEAL::Assembling system matrix... done (0.605386s)
DEAL::Solving linear system... done (45.1989s)
DEAL::Generating output... done (11.2292s)
[100%] Built target run
@endcode

Each time we refine the mesh once, so the number of cells and degrees
of freedom roughly quadruples from each step to the next. As can be seen,
generating the grid, setting up degrees of freedom, assembling the
linear system, and generating output scale pretty closely to linear,
whereas solving the linear system is an operation that requires 8
times more time each time the number of degrees of freedom is
increased by a factor of 4, i.e. it is ${\cal O}(N^{3/2})$. This can
be explained by the fact that (using optimal ordering) the
bandwidth of a finite element matrix is $B={\cal O}(N^{(dim-1)/dim})$,
and the effort to solve a banded linear system using LU decomposition
is ${\cal O}(BN)$. This also explains why the program does run in 3d
as well (after changing the dimension on the
<code>UltrasoundProblem</code> object), but scales very badly and
takes extraordinary patience before it finishes solving the linear
system on a mesh with appreciable resolution, even though all the
other parts of the program scale very nicely.



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

An obvious possible extension for this program is to run it in 3d
&mdash; after all, the world around us is three-dimensional, and
ultrasound beams propagate in three-dimensional media. You can try
this by simply changing the template parameter of the principal class
in <code>main()</code> and running it. This won't get you very far,
though: certainly not if you do 5 global refinement steps as set in
the parameter file. You'll simply run out of memory as both the mesh
(with its $(2^5)^3 \cdot 5^3=2^{15}\cdot 125 \approx 4\cdot 10^6$ cells)
and in particular the sparse direct solver take too much memory. You
can solve with 3 global refinement steps, however, if you have a bit
of time: in early 2011, the direct solve takes about half an
hour. What you'll notice, however, is that the solution is completely
wrong: the mesh size is simply not small enough to resolve the
solution's waves accurately, and you can see this in plots of the
solution. Consequently, this is one of the cases where adaptivity is
indispensable if you don't just want to throw a bigger (presumably
%parallel) machine at the problem.


examples/step-3/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

@dealiiVideoLecture{10}

<h3>The basic set up of finite element methods</h3>

This is the first example where we actually use finite elements to compute
something. We
will solve a simple version of Poisson's equation with zero boundary
values, but a nonzero right hand side:
@f{align*}
  -\Delta u &= f \qquad\qquad & \text{in}\ \Omega,
  \\
  u &= 0 \qquad\qquad & \text{on}\ \partial\Omega.
@f}
We will solve this equation on the square, $\Omega=[-1,1]^2$, for which
you've already learned how to generate a mesh in step-1 and step-2. In
this program, we will also only consider the particular case
$f(\mathbf x)=1$ and come back to how to implement the more general
case in the next tutorial program, step-4.

If you've learned about the basics of the finite element method, you will
remember the steps we need to take to approximate the solution $u$ by a finite
dimensional approximation. Specifically, we first need to derive the weak form
of the equation above, which we obtain by multiplying the equation by a test
function $\varphi$ <i>from the left</i> (we will come back to the reason for
multiplying from the left and not from the right below) and integrating over
the domain $\Omega$:
@f{align*}
  -\int_\Omega \varphi \Delta u = \int_\Omega \varphi f.
@f}
This can be integrated by parts:
@f{align*}
  \int_\Omega \nabla\varphi \cdot \nabla u
  -
  \int_{\partial\Omega} \varphi \mathbf{n}\cdot \nabla u
   = \int_\Omega \varphi f.
@f}
The test function $\varphi$ has to satisfy the same kind of boundary
conditions (in mathematical terms: it needs to come from the tangent space of
the set in which we seek the solution), so on the boundary $\varphi=0$ and
consequently the weak form we are looking for reads
@f{align*}
  (\nabla\varphi, \nabla u)
   = (\varphi, f),
@f}
where we have used the common notation $(a,b)=\int_\Omega a\; b$. The problem
then asks for a function $u$ for which this statement is true for all test
functions $\varphi$ from the appropriate space (which here is the space
$H^1$).

Of course we can't find such a function on a computer in the general case, and
instead we seek an approximation $u_h(\mathbf x)=\sum_j U_j \varphi_j(\mathbf
x)$, where the $U_j$ are unknown expansion coefficients we need to determine
(the "degrees of freedom" of this problem), and $\varphi_i(\mathbf x)$ are the
finite element shape functions we will use. To define these shape functions,
we need the following:

- A mesh on which to define shape functions. You have already seen how to
  generate and manipulate the objects that describe meshes in step-1 and
  step-2.
- A finite element that describes the shape functions we want to use on the
  reference cell (which in deal.II is always the unit interval $[0,1]$, the
  unit square $[0,1]^2$ or the unit cube $[0,1]^3$, depending on which space
  dimension you work in). In step-2, we had already used an object of type
  FE_Q<2>, which denotes the usual Lagrange elements that define shape
  functions by interpolation on support points. The simplest one is
  FE_Q<2>(1), which uses polynomial degree 1. In 2d, these are often referred
  to as <i>bilinear</i>, since they are linear in each of the two coordinates
  of the reference cell. (In 1d, they would be <i>linear</i> and in 3d
  <i>tri-linear</i>; however, in the deal.II documentation, we will frequently
  not make this distinction and simply always call these functions "linear".)
- A DoFHandler object that enumerates all the degrees of freedom on the mesh,
  taking the reference cell description the finite element object provides as
  the basis. You've also already seen how to do this in step-2.
- A mapping that tells how the shape functions on the real cell are obtained
  from the shape functions defined by the finite element class on the
  reference cell. By default, unless you explicitly say otherwise, deal.II
  will use a (bi-, tri-)linear mapping for this, so in most cases you don't
  have to worry about this step.

Through these steps, we now have a set of functions $\varphi_i$, and we can
define the weak form of the discrete problem: Find a function $u_h$, i.e., find
the expansion coefficients $U_j$ mentioned above, so that
@f{align*}
  (\nabla\varphi_i, \nabla u_h)
   = (\varphi_i, f),
   \qquad\qquad
   i=0\ldots N-1.
@f}
Note that we here follow the convention that everything is counted starting at
zero, as common in C and C++. This equation can be rewritten as a linear
system if you insert the representation $u_h(\mathbf x)=\sum_j U_j
\varphi_j(\mathbf x)$ and then observe that
@f{align*}{
  (\nabla\varphi_i, \nabla u_h)
  &= \left(\nabla\varphi_i, \nabla \Bigl[\sum_j U_j \varphi_j\Bigr]\right)
\\
  &= \sum_j \left(\nabla\varphi_i, \nabla \left[U_j \varphi_j\right]\right)
\\
  &= \sum_j \left(\nabla\varphi_i, \nabla \varphi_j \right) U_j.
@f}
With this, the problem reads: Find a vector $U$ so that
@f{align*}{
  A U = F,
@f}
where the matrix $A$ and the right hand side $F$ are defined as
@f{align*}
  A_{ij} &= (\nabla\varphi_i, \nabla \varphi_j),
  \\
  F_i &= (\varphi_i, f).
@f}


<h3> Should we multiply by a test function from the left or from the right? </h3>

Before we move on with describing how these quantities can be computed, note
that if we had multiplied the original equation from the <i>right</i> by a
test function rather than from the left, then we would have obtained a linear
system of the form
@f{align*}
  U^T A = F^T
@f}
with a row vector $F^T$. By transposing this system, this is of course
equivalent to solving
@f{align*}
  A^T U = F
@f}
which here is the same as above since $A=A^T$. But in general is not,
and in order to avoid
any sort of confusion, experience has shown that simply getting into the habit
of multiplying the equation from the left rather than from the right (as is
often done in the mathematical literature) avoids a common class of errors as
the matrix is automatically correct and does not need to be transposed when
comparing theory and implementation. See step-9 for the first example in this
tutorial where we have a non-symmetric bilinear form for which it makes a
difference whether we multiply from the right or from the left.


<h3> Computing the matrix and right hand side vector </h3>

Now we know what we need (namely: objects that hold the matrix and
vectors, as well as ways to compute $A_{ij},F_i$), and we can look at what it
takes to make that happen:

- The object for $A$ is of type SparseMatrix while those for $U$ and $F$ are of
  type Vector. We will see in the program below what classes are used to solve
  linear systems.
- We need a way to form the integrals. In the finite element method, this is
  most commonly done using quadrature, i.e. the integrals are replaced by a
  weighted sum over a set of points on each cell. That is, we first split the
  integral over $\Omega$ into integrals over all cells,
  @f{align*}
    A_{ij} &= (\nabla\varphi_i, \nabla \varphi_j)
    = \sum_{K \in {\mathbb T}} \int_K \nabla\varphi_i \cdot \nabla \varphi_j,
    \\
    F_i &= (\varphi_i, f)
    = \sum_{K \in {\mathbb T}} \int_K \varphi_i f,
  @f}
  and then approximate each cell's contribution by quadrature:
  @f{align*}
    A^K_{ij} &=
    \int_K \nabla\varphi_i \cdot \nabla \varphi_j
    \approx
    \sum_q \nabla\varphi_i(\mathbf x^K_q) \cdot \nabla
    \varphi_j(\mathbf x^K_q) w_q^K,
    \\
    F^K_i &=
    \int_K \varphi_i f
    \approx
    \sum_q \varphi_i(\mathbf x^K_q) f(\mathbf x^K_q) w^K_q,
  @f}
  where $\mathbf x^K_q$ is the $q$th quadrature point on cell $K$, and $w^K_q$
  the $q$th quadrature weight. There are different parts to what is needed in
  doing this, and we will discuss them in turn next.
- First, we need a way to describe the location $\mathbf x_q^K$ of quadrature
  points and their weights $w^K_q$. They are usually mapped from the reference
  cell in the same way as shape functions, i.e., implicitly using the
  MappingQ1 class or, if you explicitly say so, through one of the other
  classes derived from Mapping. The locations and weights on the reference
  cell are described by objects derived from the Quadrature base
  class. Typically, one chooses a quadrature formula (i.e. a set of points and
  weights) so that the quadrature exactly equals the integral in the matrix;
  this can be achieved because all factors in the integral are polynomial, and
  is done by Gaussian quadrature formulas, implemented in the QGauss class.
- We then need something that can help us evaluate $\varphi_i(\mathbf x^K_q)$
  on cell $K$. This is what the FEValues class does: it takes a finite element
  objects to describe $\varphi$ on the reference cell, a quadrature object to
  describe the quadrature points and weights, and a mapping object (or
  implicitly takes the MappingQ1 class) and provides values and derivatives of
  the shape functions on the real cell $K$ as well as all sorts of other
  information needed for integration, at the quadrature points located on $K$.

FEValues really is the central class in the assembly process. One way you can
view it is as follows: The FiniteElement and derived classes describe shape
<i>functions</i>, i.e., infinite dimensional objects: functions have values at
every point. We need this for theoretical reasons because we want to perform
our analysis with integrals over functions. However, for a computer, this is a
very difficult concept, since they can in general only deal with a finite
amount of information, and so we replace integrals by sums over quadrature
points that we obtain by mapping (the Mapping object) using  points defined on
a reference cell (the Quadrature object) onto points on the real cell. In
essence, we reduce the problem to one where we only need a finite amount of
information, namely shape function values and derivatives, quadrature weights,
normal vectors, etc, exclusively at a finite set of points. The FEValues class
is the one that brings the three components together and provides this finite
set of information on a particular cell $K$. You will see it in action when we
assemble the linear system below.

It is noteworthy that all of this could also be achieved if you simply created
these three objects yourself in an application program, and juggled the
information yourself. However, this would neither be simpler (the FEValues
class provides exactly the kind of information you actually need) nor faster:
the FEValues class is highly optimized to only compute on each cell the
particular information you need; if anything can be re-used from the previous
cell, then it will do so, and there is a lot of code in that class to make
sure things are cached wherever this is advantageous.

The final piece of this introduction is to mention that after a linear
system is obtained, it is solved using an iterative solver and then
postprocessed: we create an output file using the DataOut class that can then
be visualized using one of the common visualization programs.

@note The preceding overview of all the important steps of any finite element
implementation has its counterpart in deal.II: The library can naturally be
grouped into a number of "modules" that cover the basic concepts just
outlined. You can access these modules through the tab at the top of this
page. An overview of the most fundamental groups of concepts is also available
on the <a href="index.html">front page of the deal.II manual</a>.


<h3>About the implementation</h3>

Although this is the simplest possible equation you can solve using the finite
element method, this program shows the basic structure of most finite
element programs and also serves as the template that almost all of the
following programs will essentially follow. Specifically, the main class of
this program looks like this:
@code
class Step3
{
  public:
    Step3 ();
    void run ();

  private:
    void make_grid ();
    void setup_system ();
    void assemble_system ();
    void solve ();
    void output_results () const;

    Triangulation<2>     triangulation;
    FE_Q<2>              fe;
    DoFHandler<2>        dof_handler;

    SparsityPattern      sparsity_pattern;
    SparseMatrix<double> system_matrix;
    Vector<double>       solution;
    Vector<double>       system_rhs;
};
@endcode

This follows the object oriented programming mantra of <a
href="http://en.wikipedia.org/wiki/Encapsulation_(object-oriented_programming)">data
encapsulation</a>, i.e. we do our best to hide almost all internal details of
this class in private members that are not accessible to the outside.

Let's start with the member variables: These follow the building blocks we
have outlined above in the bullet points, namely we need a Triangulation and a
DoFHandler object, and a finite element object that describes the kinds of
shape functions we want to use. The second group of objects relate to the
linear algebra: the system matrix and right hand side as well as the solution
vector, and an object that describes the sparsity pattern of the matrix. This
is all this class needs (and the essentials that any solver for a stationary
PDE requires) and that needs to survive throughout the entire program. In
contrast to this, the FEValues object we need for assembly is only required
throughout assembly, and so we create it as a local object in the function
that does that and destroy it again at its end.

Secondly, let's look at the member functions. These, as well, already form the
common structure that almost all following tutorial programs will use:
<ul>
  <li> <code>make_grid()</code>: This is what one could call a
       <i>preprocessing function</i>. As its name suggests, it sets up the
       object that stores the triangulation. In later examples, it could also
       deal with boundary conditions, geometries, etc.
  <li> <code>setup_system()</code>: This then is the function in which all the
       other data structures are set up that are needed to solve the
       problem. In particular, it will initialize the DoFHandler object and
       correctly size the various objects that have to do with the linear
       algebra. This function is often separated from the preprocessing
       function above because, in a time dependent program, it may be called
       at least every few time steps whenever the mesh
       is adaptively refined (something we will see how to do in step-6). On
       the other hand, setting up the mesh itself in the preprocessing
       function above is done only once at the beginning of the program and
       is, therefore, separated into its own function.
  <li> <code>assemble_system()</code>: This, then is where the contents of the
       matrix and right hand side are computed, as discussed at length in the
       introduction above. Since doing something with this linear system is
       conceptually very different from computing its entries, we separate it
       from the following function.
  <li> <code>solve()</code>: This then is the function in which we compute the
       solution $U$ of the linear system $AU=F$. In the current program, this
       is a simple task since the matrix is so simple, but it will become a
       significant part of a program's size whenever the problem is not so
       trivial any more (see, for example, step-20, step-22, or step-31 once
       you've learned a bit more about the library).
  <li> <code>output_results()</code>: Finally, when you have computed a
       solution, you probably want to do something with it. For example, you
       may want to output it in a format that can be visualized, or you may
       want to compute quantities you are interested in: say, heat fluxes in a
       heat exchanger, air friction coefficients of a wing, maximum bridge
       loads, or simply the value of the numerical solution at a point. This
       function is therefore the place for postprocessing your solution.
</ul>
All of this is held together by the single public function (other than the
constructor), namely the <code>run()</code> function. It is the one that is
called from the place where an object of this type is created, and it is the
one that calls all the other functions in their proper order. Encapsulating
this operation into the <code>run()</code> function, rather than calling all
the other functions from <code>main()</code> makes sure that you
can change how the separation of concerns within this class is
implemented. For example, if one of the functions becomes too big, you can
split it up into two, and the only places you have to be concerned about
changing as a consequence are within this very same class, and not anywhere
else.

As mentioned above, you will see this general structure &mdash; sometimes with
variants in spelling of the functions' names, but in essentially this order of
separation of functionality &mdash; again in many of the
following tutorial programs.


<h3> A note on types </h3>

deal.II defines a number of integral %types via alias in namespace dealii::types.
(In the previous sentence, the word "integral" is used as the <i>adjective</i>
that corresponds to the noun "integer". It shouldn't be confused with the
<i>noun</i> "integral" that represents the area or volume under a curve
or surface. The adjective "integral" is widely used in the C++ world in
contexts such as "integral type", "integral constant", etc.)
In particular, in this program you will see types::global_dof_index in a couple of
places: an integer type that is used to denote the <i>global</i> index of a
degree of freedom, i.e., the index of a particular degree of freedom within the
DoFHandler object that is defined on top of a triangulation (as opposed to the
index of a particular degree of freedom within a particular cell). For the
current program (as well as almost all of the tutorial programs), you will have
a few thousand to maybe a few million unknowns globally (and, for $Q_1$
elements, you will have 4 <i>locally on each cell</i> in 2d and 8 in 3d).
Consequently, a data type that allows to store sufficiently large numbers for
global DoF indices is <code>unsigned int</code> given that it allows to store
numbers between 0 and slightly more than 4 billion (on most systems, where
integers are 32-bit). In fact, this is what types::global_dof_index is.

So, why not just use <code>unsigned int</code> right away? deal.II used to do
this until version 7.3. However, deal.II supports very large computations (via
the framework discussed in step-40) that may have more than 4 billion unknowns
when spread across a few thousand processors. Consequently, there are
situations where <code>unsigned int</code> is not sufficiently large and we
need a 64-bit unsigned integral type. To make this possible, we introduced
types::global_dof_index which by default is defined as simply <code>unsigned
int</code> whereas it is possible to define it as <code>unsigned long long
int</code> if necessary, by passing a particular flag during configuration
(see the ReadMe file).

This covers the technical aspect. But there is also a documentation purpose:
everywhere in the library and codes that are built on it, if you see a place
using the data type types::global_dof_index, you immediately know that the
quantity that is being referenced is, in fact, a global dof index. No such
meaning would be apparent if we had just used <code>unsigned int</code> (which
may also be a local index, a boundary indicator, a material id,
etc.). Immediately knowing what a variable refers to also helps avoid errors:
it's quite clear that there must be a bug if you see an object of type
types::global_dof_index being assigned to variable of type
types::subdomain_id, even though they are both represented by unsigned
integers and the compiler will, consequently, not complain.

In more practical terms what the presence of this type means is that during
assembly, we create a $4\times 4$ matrix (in 2d, using a $Q_1$ element) of the
contributions of the cell we are currently sitting on, and then we need to add
the elements of this matrix to the appropriate elements of the global (system)
matrix. For this, we need to get at the global indices of the degrees of
freedom that are local to the current cell, for which we will always use the
following piece of the code:
@code
  cell->get_dof_indices (local_dof_indices);
@endcode
where <code>local_dof_indices</code> is declared as
@code
  std::vector<types::global_dof_index> local_dof_indices (fe.n_dofs_per_cell());
@endcode
The name of this variable might be a bit of a misnomer -- it stands for "the
global indices of those degrees of freedom locally defined on the current
cell" -- but variables that hold this information are universally named this
way throughout the library.

@note types::global_dof_index is not the only type defined in this namespace.
Rather, there is a whole family, including types::subdomain_id,
types::boundary_id, and types::material_id. All of these are alias for integer
data types but, as explained above, they are used throughout the library so that
(i) the intent of a variable becomes more easily discerned, and (ii) so that it
becomes possible to change the actual type to a larger one if necessary without
having to go through the entire library and figure out whether a particular use
of <code>unsigned int</code> corresponds to, say, a material indicator.


examples/step-3/doc/results.dox
<h1>Results</h1>

The output of the program looks as follows:
@code
Number of active cells: 1024
Number of degrees of freedom: 1089
DEAL:cg::Starting value 0.121094
DEAL:cg::Convergence step 48 value 5.33692e-13
@endcode

The first two lines is what we wrote to <code>cout</code>. The last
two lines were generated without our intervention by the CG
solver. The first two lines state the residual at the start of the
iteration, while the last line tells us that the solver needed 47
iterations to bring the norm of the residual to 5.3e-13, i.e. below
the threshold 1e-12 which we have set in the `solve' function. We will
show in the next program how to suppress this output, which is
sometimes useful for debugging purposes, but often clutters up the
screen display.

Apart from the output shown above, the program generated the file
<code>solution.vtk</code>, which is in the VTK format that is widely
used by many visualization programs today -- including the two
heavy-weights <a href="https://www.llnl.gov/visit">VisIt</a> and
<a href="https://www.paraview.org">Paraview</a> that are the most
commonly used programs for this purpose today.

Using VisIt, it is not very difficult to generate a picture of the
solution like this:
<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-3.solution-3.png" alt="Visualization of the solution of step-3">
    </td>
  </tr>
</table>
It shows both the solution and the mesh, elevated above the $x$-$y$ plane
based on the value of the solution at each point. Of course the solution
here is not particularly exciting, but that is a result of both what the
Laplace equation represents and the right hand side $f(\mathbf x)=1$ we
have chosen for this program: The Laplace equation describes (among many
other uses) the vertical deformation of a membrane subject to an external
(also vertical) force. In the current example, the membrane's borders
are clamped to a square frame with no vertical variation; a constant
force density will therefore intuitively lead to a membrane that
simply bulges upward -- like the one shown above.

VisIt and Paraview both allow playing with various kinds of visualizations
of the solution. Several video lectures show how to use these programs.
@dealiiVideoLectureSeeAlso{11,32}



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

If you want to play around a little bit with this program, here are a few
suggestions:
</p>

<ul>
  <li>
  Change the geometry and mesh: In the program, we have generated a square
  domain and mesh by using the <code>GridGenerator::hyper_cube</code>
  function. However, the <code>GridGenerator</code> has a good number of other
  functions as well. Try an L-shaped domain, a ring, or other domains you find
  there.
  </li>

  <li>
  Change the boundary condition: The code uses the Functions::ZeroFunction
  function to generate zero boundary conditions. However, you may want to try
  non-zero constant boundary values using
  <code>ConstantFunction&lt;2&gt;(1)</code> instead of
  <code>ZeroFunction&lt;2&gt;()</code> to have unit Dirichlet boundary
  values. More exotic functions are described in the documentation of the
  Functions namespace, and you may pick one to describe your particular boundary
  values.
  </li>

  <li> Modify the type of boundary condition: Presently, what happens
  is that we use Dirichlet boundary values all around, since the
  default is that all boundary parts have boundary indicator zero, and
  then we tell the
  VectorTools::interpolate_boundary_values() function to
  interpolate boundary values to zero on all boundary components with
  indicator zero.  <p> We can change this behavior if we assign parts
  of the boundary different indicators. For example, try this
  immediately after calling GridGenerator::hyper_cube():
  @code
  triangulation.begin_active()->face(0)->set_boundary_id(1);
  @endcode

  What this does is it first asks the triangulation to
  return an iterator that points to the first active cell. Of course,
  this being the coarse mesh for the triangulation of a square, the
  triangulation has only a single cell at this moment, and it is
  active. Next, we ask the cell to return an iterator to its first
  face, and then we ask the face to reset the boundary indicator of
  that face to 1. What then follows is this: When the mesh is refined,
  faces of child cells inherit the boundary indicator of their
  parents, i.e. even on the finest mesh, the faces on one side of the
  square have boundary indicator 1. Later, when we get to
  interpolating boundary conditions, the
  VectorTools::interpolate_boundary_values() call will only produce boundary
  values for those faces that have zero boundary indicator, and leave
  those faces alone that have a different boundary indicator. What
  this then does is to impose Dirichlet boundary conditions on the
  former, and homogeneous Neumann conditions on the latter (i.e. zero
  normal derivative of the solution, unless one adds additional terms
  to the right hand side of the variational equality that deal with
  potentially non-zero Neumann conditions). You will see this if you
  run the program.

  An alternative way to change the boundary indicator is to label
  the boundaries based on the Cartesian coordinates of the face centers.
  For example, we can label all of the cells along the top and
  bottom boundaries with a boundary indicator 1 by checking to
  see if the cell centers' y-coordinates are within a tolerance
  (here 1e-12) of -1 and 1. Try this immediately after calling
  GridGenerator::hyper_cube(), as before:
  @code
  for (auto &face : triangulation.active_face_iterators())
    if (std::fabs(face->center()(1) - (-1.0)) < 1e-12 ||
        std::fabs(face->center()(1) - (1.0)) < 1e-12)
      face->set_boundary_id(1);
  @endcode
  Although this code is a bit longer than before, it is useful for
  complex geometries, as it does not require knowledge of face labels.

  <li>
  A slight variation of the last point would be to set different boundary
  values as above, but then use a different boundary value function for
  boundary indicator one. In practice, what you have to do is to add a second
  call to <code>interpolate_boundary_values</code> for boundary indicator one:
  @code
  VectorTools::interpolate_boundary_values(dof_handler,
					   1,
					   ConstantFunction<2>(1.),
					   boundary_values);
  @endcode
  If you have this call immediately after the first one to this function, then
  it will interpolate boundary values on faces with boundary indicator 1 to the
  unit value, and merge these interpolated values with those previously
  computed for boundary indicator 0. The result will be that we will get
  discontinuous boundary values, zero on three sides of the square, and one on
  the fourth.

  <li>
  Observe convergence: We will only discuss computing errors in norms in
  step-7, but it is easy to check that computations converge
  already here. For example, we could evaluate the value of the solution in a
  single point and compare the value for different %numbers of global
  refinement (the number of global refinement steps is set in
  <code>LaplaceProblem::make_grid</code> above). To evaluate the
  solution at a point, say at $(\frac 13, \frac 13)$, we could add the
  following code to the <code>LaplaceProblem::output_results</code> function:
  @code
    std::cout << "Solution at (1/3,1/3): "
              << VectorTools::point_value(dof_handler, solution,
                                          Point<2>(1./3, 1./3))
              << std::endl;
  @endcode
  For 1 through 9 global refinement steps, we then get the following sequence
  of point values:
  <table align="center" class="doxtable">
    <tr> <th># of refinements</th> <th>$u_h(\frac 13,\frac13)$</th> </tr>
    <tr> <td>1</td> <td>0.166667</td> </tr>
    <tr> <td>2</td> <td>0.227381</td> </tr>
    <tr> <td>3</td> <td>0.237375</td> </tr>
    <tr> <td>4</td> <td>0.240435</td> </tr>
    <tr> <td>5</td> <td>0.241140</td> </tr>
    <tr> <td>6</td> <td>0.241324</td> </tr>
    <tr> <td>7</td> <td>0.241369</td> </tr>
    <tr> <td>8</td> <td>0.241380</td> </tr>
    <tr> <td>9</td> <td>0.241383</td> </tr>
  </table>
  By noticing that the difference between each two consecutive values reduces
  by about a factor of 4, we can conjecture that the "correct" value may be
  $u(\frac 13, \frac 13)\approx 0.241384$. In fact, if we assumed this to be
  the correct value, we could show that the sequence above indeed shows ${\cal
  O}(h^2)$ convergence &mdash; theoretically, the convergence order should be
  ${\cal O}(h^2 |\log h|)$ but the symmetry of the domain and the mesh may lead
  to the better convergence order observed.

  A slight variant of this would be to repeat the test with quadratic
  elements. All you need to do is to set the polynomial degree of the finite
  element to two in the constructor
  <code>LaplaceProblem::LaplaceProblem</code>.

  <li>Convergence of the mean: A different way to see that the solution
  actually converges (to something &mdash; we can't tell whether it's really
  the correct value!) is to compute the mean of the solution. To this end, add
  the following code to <code>LaplaceProblem::output_results</code>:
  @code
    std::cout << "Mean value: "
              << VectorTools::compute_mean_value (dof_handler,
						  QGauss<2>(fe.degree + 1),
						  solution,
						  0)
              << std::endl;
  @endcode
  The documentation of the function explains what the second and fourth
  parameters mean, while the first and third should be obvious. Doing the same
  study again where we change the number of global refinement steps, we get
  the following result:
  <table align="center" class="doxtable">
    <tr> <th># of refinements</th> <th>$\int_\Omega u_h(x)\; dx$</th> </tr>
    <tr> <td>0</td> <td>0.09375000</td> </tr>
    <tr> <td>1</td> <td>0.12790179</td> </tr>
    <tr> <td>2</td> <td>0.13733440</td> </tr>
    <tr> <td>3</td> <td>0.13976069</td> </tr>
    <tr> <td>4</td> <td>0.14037251</td> </tr>
    <tr> <td>5</td> <td>0.14052586</td> </tr>
    <tr> <td>6</td> <td>0.14056422</td> </tr>
    <tr> <td>7</td> <td>0.14057382</td> </tr>
    <tr> <td>8</td> <td>0.14057622</td> </tr>
  </table>
  Again, the difference between two adjacent values goes down by about a
  factor of four, indicating convergence as ${\cal O}(h^2)$.
</ul>



<h3>Using %HDF5 to output the solution and additional data</h3>

%HDF5 is a commonly used format that can be read by many scripting
languages (e.g. R or Python). It is not difficult to get deal.II to
produce some %HDF5 files that can then be used in external scripts to
postprocess some of the data generated by this program. Here are some
ideas on what is possible.


<h4> Changing the output to .h5</h4>

To fully make use of the automation we first need to introduce a private variable for the number of
global refinement steps <code>unsigned int n_refinement_steps </code>, which will be used for the output filename.
In <code>make_grid()</code> we then replace <code>triangulation.refine_global(5);</code> with
@code
n_refinement_steps = 5;
triangulation.refine_global(n_refinement_steps);
@endcode
The deal.II library has two different %HDF5 bindings, one in the HDF5
namespace (for interfacing to general-purpose data files)
and another one in DataOut (specifically for writing files for the
visualization of solutions).
Although the HDF5 deal.II binding supports both serial and MPI, the %HDF5 DataOut binding
only supports parallel output.
For this reason we need to initialize an MPI
communicator with only one processor. This is done by adding the following code.
@code
int main(int argc, char* argv[])
{
  Utilities::MPI::MPI_InitFinalize mpi_initialization(argc, argv, 1);
  ...
}
@endcode
Next we change the `Step3::output_results()` output routine as
described in the DataOutBase namespace documentation:
@code
const std::string filename_h5 = "solution_" + std::to_string(n_refinement_steps) + ".h5";
DataOutBase::DataOutFilterFlags flags(true, true);
DataOutBase::DataOutFilter data_filter(flags);
data_out.write_filtered_data(data_filter);
data_out.write_hdf5_parallel(data_filter, filename_h5, MPI_COMM_WORLD);
@endcode
The resulting file can then be visualized just like the VTK file that
the original version of the tutorial produces; but, since %HDF5 is a
more general file format, it can also easily be processed in scripting
languages for other purposes.


<h4> Adding the point value and the mean (see extension above) into the .h5 file</h4>

After outputting the solution, the file can be opened again to include
more datasets.  This allows us to keep all the necessary information
of our experiment in a single result file, which can then be read and
processed by some postprocessing script.
(Have a look at HDF5::Group::write_dataset() for further
information on the possible output options.)

To make this happen, we first include the necessary header into our file:
@code
#include <deal.II/base/hdf5.h>
@endcode
Adding the following lines to the end
of our output routine adds the information about the value of the
solution at a particular point, as well as the mean value of the
solution, to our %HDF5 file:
@code
HDF5::File data_file(filename_h5, HDF5::File::FileAccessMode::open, MPI_COMM_WORLD);
Vector<double> point_value(1);
point_value[0] = VectorTools::point_value(dof_handler, solution,
                                          Point<2>(1./3, 1./3));
data_file.write_dataset("point_value", point_value);
Vector<double> mean_value(1);
mean_value[0] = VectorTools::compute_mean_value(dof_handler,
                                                QGauss<2>(fe.degree + 1),
                                                solution, 0);
data_file.write_dataset("mean_value",mean_value);
@endcode



<h3> Using R and ggplot2 to generate plots</h3>

The data put into %HDF5 files above can then be used from scripting
languages for further postprocessing. In the following, let us show
how this can, in particular, be done with the
<a href="https://en.wikipedia.org/wiki/R_(programming_language)">R
programming language</a>, a widely used language in statistical data
analysis. (Similar things can also be done in Python, for example.)
If you are unfamiliar with R and ggplot2 you could check out the data carpentry course on R
<a href="https://datacarpentry.org/R-ecology-lesson/index.html">here</a>.
Furthermore, since most search engines struggle with searches of the form "R + topic",
we recommend using the specializes service <a
href="http://rseek.org">RSeek </a> instead.

The most prominent difference between R and other languages is that
the assignment operator (`a = 5`) is typically written as
`a <- 5`. As the latter is considered standard we will use it in our examples as well.
To open the `.h5` file in R you have to install the <a href="https://bioconductor.org/packages/release/bioc/html/rhdf5.html">rhdf5</a> package, which is a part of the Bioconductor package.

First we will include all necessary packages and have a look at how the data is structured in our file.
@code{.r}
library(rhdf5)     # library for handling HDF5 files
library(ggplot2)   # main plotting library
library(grDevices) # needed for output to PDF
library(viridis)   # contains good colormaps for sequential data

refinement <- 5
h5f <- H5Fopen(paste("solution_",refinement,".h5",sep=""))
print(h5f)
@endcode
This gives the following output
@code{.unparsed}
HDF5 FILE
   name /
filename

    name       otype  dclass     dim
0 cells       H5I_DATASET INTEGER  x 1024
1 mean_value  H5I_DATASET FLOAT   1
2 nodes       H5I_DATASET FLOAT    x 1089
3 point_value H5I_DATASET FLOAT   1
4 solution    H5I_DATASET FLOAT    x 1089
@endcode
The datasets can be accessed by <code>h5f\$name</code>. The function
<code>dim(h5f\$cells)</code> gives us the dimensions of the matrix
that is used to store our cells.
We can see the following three matrices, as well as the two
additional data points we added.
<ul>
<li> <code>cells</code>: a 4x1024 matrix that stores the  (C++) vertex indices for each cell
<li> <code>nodes</code>: a 2x1089 matrix storing the position values (x,y) for our cell vertices
<li> <code>solution</code>: a 1x1089 matrix storing the values of our solution at each vertex
</ul>
Now we can use this data to generate various plots. Plotting with ggplot2 usually splits into two steps.
At first the data needs to be manipulated and added to a <code>data.frame</code>.
After that, a <code>ggplot</code> object is constructed and manipulated by adding plot elements to it.

<code>nodes</code> and <code>cells</code> contain all the information we need to plot our grid.
The following code wraps all the data into one dataframe for plotting our grid:
@code{.r}
# Counting in R starts at 1 instead of 0, so we need to increment all
# vertex indices by one:
cell_ids <- h5f$cells+1

# Store the x and y positions of each vertex in one big vector in a
# cell by cell fashion (every 4 entries belong to one cell):
cells_x <- h5f$nodes[1,][cell_ids]
cells_y <- h5f$nodes[2,][cell_ids]

# Construct a vector that stores the matching cell by cell grouping
# (1,1,1,1,2,2,2,2,...):
groups <- rep(1:ncol(cell_ids),each=4)

# Finally put everything into one dataframe:
meshdata <- data.frame(x = cells_x, y = cells_y, id = groups)
@endcode

With the finished dataframe we have everything we need to plot our grid:
@code{.r}
pdf (paste("grid_",refinement,".pdf",sep=""),width = 5,height = 5) # Open new PDF file
plt <- ggplot(meshdata,aes(x=x,y=y,group=id))                      # Construction of our plot
                                                                   # object, at first only data

plt <- plt + geom_polygon(fill="white",colour="black")             # Actual plotting of the grid as polygons
plt <- plt + ggtitle(paste("grid at refinement level #",refinement))

print(plt)                                                         # Show the current state of the plot/add it to the pdf
dev.off()                                                          # Close PDF file
@endcode

The contents of this file then look as follows (not very exciting, but
you get the idea):
<table width="60%" align="center">
  <tr>
   <td align="center">
     <img src="https://www.dealii.org/images/steps/developer/step-3.extensions.grid_5.png" alt="Grid after 5 refinement steps of step-3">
   </td>
  </tr>
</table>

We can also visualize the solution itself, and this is going to look
more interesting.
To make a 2D pseudocolor plot of our solution we will use <code>geom_raster</code>.
This function needs a structured grid, i.e. uniform in x and y directions.
Luckily our data at this point is structured in the right way.
The following code plots a pseudocolor representation of our surface into a new PDF:
@code{.r}
pdf (paste("pseudocolor_",refinement,".pdf",sep=""),width = 5,height = 4.2) # Open new PDF file
colordata <- data.frame(x = h5f$nodes[1,],y = h5f$nodes[2,] , solution = h5f$solution[1,])
plt <- ggplot(colordata,aes(x=x,y=y,fill=solution))
plt <- plt + geom_raster(interpolate=TRUE)
plt <- plt + scale_fill_viridis()
plt <- plt + ggtitle(paste("solution at refinement level #",refinement))

print(plt)
dev.off()
H5Fclose(h5f) # Close the HDF5 file
@endcode
This is now going to look as follows:
<table width="60%" align="center">
 <tr>
   <td align="center">
     <img src="https://www.dealii.org/images/steps/developer/step-3.extensions.pseudocolor_5.png" alt="Solution after 5 refinement steps of step-3">
   </td>
 </tr>
</table>

For plotting the converge curves we need to re-run the C++ code multiple times with different values for <code>n_refinement_steps</code>
starting from 1.
Since every file only contains a single data point we need to loop over them and concatenate the results into a single vector.
@code{.r}
n_ref <- 8   # Maximum refinement level for which results are existing

# First we initiate all vectors with the results of the first level
h5f   <- H5Fopen("solution_1.h5")
dofs  <- dim(h5f$solution)[2]
mean  <- h5f$mean_value
point <- h5f$point_value
H5Fclose(h5f)

for (reflevel in 2:n_ref)
{
   h5f   <- H5Fopen(paste("solution_",reflevel,".h5",sep=""))
   dofs  <- c(dofs,dim(h5f\$solution)[2])
   mean  <- c(mean,h5f\$mean_value)
   point <- c(point,h5f\$point_value)
   H5Fclose(h5f)
}
@endcode
As we are not interested in the values themselves but rather in the error compared to a "exact" solution we will
assume our highest refinement level to be that solution and omit it from the data.
@code{.r}
# Calculate the error w.r.t. our maximum refinement step
mean_error  <- abs(mean[1:n_ref-1]-mean[n_ref])
point_error <- abs(point[1:n_ref-1]-point[n_ref])

# Remove the highest value from our DoF data
dofs     <- dofs[1:n_ref-1]
convdata <- data.frame(dofs = dofs, mean_value= mean_error, point_value = point_error)
@endcode
Now we have all the data available to generate our plots.
It is often useful to plot errors on a log-log scale, which is
accomplished in the following code:
@code
pdf (paste("convergence.pdf",sep=""),width = 5,height = 4.2)
plt <- ggplot(convdata,mapping=aes(x = dofs, y = mean_value))
plt <- plt+geom_line()
plt <- plt+labs(x="#DoFs",y = "mean value error")
plt <- plt+scale_x_log10()+scale_y_log10()
print(plt)

plt <- ggplot(convdata,mapping=aes(x = dofs, y = point_value))
plt <- plt+geom_line()
plt <- plt+labs(x="#DoFs",y = "point value error")
plt <- plt+scale_x_log10()+scale_y_log10()
print(plt)

dev.off()
@endcode
This results in the following plot that shows how the errors in the
mean value and the solution value at the chosen point nicely converge
to zero:
<table style="width:50%" align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-3.extensions.convergence_mean.png" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-3.extensions.convergence_point.png" alt=""></td>
  </tr>
</table>


examples/step-30/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>


<h3>Overview</h3>

This example is devoted to <em>anisotropic refinement</em>, which extends to
possibilities of local refinement. In most parts, this is a modification of the
step-12 tutorial program, we use the same DG method for a linear transport
equation. This program will cover the following topics:
<ol>
  <li> <em>Anisotropic refinement</em>: What is the meaning of anisotropic refinement?
  <li> <em>Implementation</em>: Necessary modifications of code to work with anisotropically refined meshes.
  <li> <em>Jump indicator</em>: A simple indicator for anisotropic refinement in
  the context of DG methods.
</ol>
The discretization itself will not be discussed, and neither will
implementation techniques not specific to anisotropic refinement used
here. Please refer to step-12 for this.

Please note, at the moment of writing this tutorial program, anisotropic
refinement is only fully implemented for discontinuous Galerkin Finite
Elements. This may later change (or may already have).


@note While this program is a modification of step-12, it is an adaptation of
a version of step-12 written early on in the history of deal.II when the
MeshWorker framework wasn't available yet. Consequently, it bears little
resemblance to the step-12 as it exists now, apart from the fact that it
solves the same equation with the same discretization.



<h3>Anisotropic refinement</h3>

All the adaptive processes in the preceding tutorial programs were based on
<em>isotropic</em> refinement of cells, which cuts all edges in half and forms
new cells of these split edges (plus some additional edges, faces and vertices,
of course). In deal.II, <em>anisotropic refinement</em> refers to the process of
splitting only part of the edges while leaving the others unchanged. Consider a
simple square cell, for example:
@code
  *-------*
  |       |
  |       |
  |       |
  *-------*
@endcode
After the usual refinement it will consist of four children and look like this:
@code
  *---*---*
  |   |   |
  *---*---*     RefinementCase<2>::cut_xy
  |   |   |
  *---*---*
@endcode
The new anisotropic refinement may take two forms: either we can split the edges
which are parallel to the horizontal x-axis, resulting in these two child cells:
@code
  *---*---*
  |   |   |
  |   |   |     RefinementCase<2>::cut_x
  |   |   |
  *---*---*
@endcode
or we can split the two edges which run along the y-axis, resulting again in two
children, which look that way, however:
@code
  *-------*
  |       |
  *-------*     RefinementCase<2>::cut_y
  |       |
  *-------*
@endcode
All refinement cases of cells are described by an enumeration
RefinementPossibilities::Possibilities, and the above anisotropic
cases are called @p cut_x and @p cut_y for obvious reasons. The
isotropic refinement case is called @p cut_xy in 2D and can be
requested from the RefinementCase class via
RefinementCase<dim>::isotropic_refinement.

In 3D, there is a third axis which can be split, the z-axis, and thus we
have an additional refinement case @p cut_z here. Isotropic refinement will now
refine a cell along the x-, y- and z-axes and thus be referred to as @p
cut_xyz. Additional cases @p cut_xy, @p cut_xz and @p cut_yz exist, which refine
a cell along two of the axes, but not along the third one. Given a hex cell with
x-axis running to the right, y-axis 'into the page' and z-axis to the top,
@code
      *-----------*
     /           /|
    /           / |
   /           /  |
  *-----------*   |
  |           |   |
  |           |   *
  |           |  /
  |           | /
  |           |/
  *-----------*
@endcode
we have the isotropic refinement case,
@code
      *-----*-----*
     /     /     /|
    *-----*-----* |
   /     /     /| *
  *-----*-----* |/|
  |     |     | * |
  |     |     |/| *
  *-----*-----* |/
  |     |     | *
  |     |     |/
  *-----*-----*

  RefinementCase<3>::cut_xyz
@endcode
three anisotropic cases which refine only one axis:
@code
      *-----*-----*             *-----------*             *-----------*
     /     /     /|            /           /|            /           /|
    /     /     / |           *-----------* |           /           / |
   /     /     /  |          /           /| |          /           /  *
  *-----*-----*   |         *-----------* | |         *-----------*  /|
  |     |     |   |         |           | | |         |           | / |
  |     |     |   *         |           | | *         |           |/  *
  |     |     |  /          |           | |/          *-----------*  /
  |     |     | /           |           | *           |           | /
  |     |     |/            |           |/            |           |/
  *-----*-----*             *-----------*             *-----------*

  RefinementCase<3>::cut_x  RefinementCase<3>::cut_y  RefinementCase<3>::cut_z
@endcode
and three cases which refine two of the three axes:
@code
      *-----*-----*             *-----*-----*             *-----------*
     /     /     /|            /     /     /|            /           /|
    *-----*-----* |           /     /     / |           *-----------* |
   /     /     /| |          /     /     /  *          /           /| *
  *-----*-----* | |         *-----*-----*  /|         *-----------* |/|
  |     |     | | |         |     |     | / |         |           | * |
  |     |     | | *         |     |     |/  *         |           |/| *
  |     |     | |/          *-----*-----*  /          *-----------* |/
  |     |     | *           |     |     | /           |           | *
  |     |     |/            |     |     |/            |           |/
  *-----*-----*             *-----*-----*             *-----------*

  RefinementCase<3>::cut_xy RefinementCase<3>::cut_xz RefinementCase<3>::cut_yz
@endcode
For 1D problems, anisotropic refinement can make no difference, as there is only
one coordinate direction for a cell, so it is not possible to split it
in any other way than isotropically.

<h4>Motivation</h4>
Adaptive local refinement is used to obtain fine meshes which are well adapted
to solving the problem at hand efficiently. In short, the size of cells which
produce a large error is reduced to obtain a better approximation of the
solution to the problem at hand. However, a lot of problems contain anisotropic
features. Prominent examples are shocks or boundary layers in compressible
viscous flows. An efficient mesh approximates these features with cells of higher aspect ratio
which are oriented according to the mentioned features. Using only isotropic
refinement, the aspect ratios of the original mesh cells are preserved, as they
are inherited by the children of a cell. Thus, starting from an isotropic mesh, a
boundary layer will be refined in order to catch the rapid variation of the flow
field in the wall normal direction, thus leading to cells with very small edge
lengths both in normal and tangential direction. Usually, much higher edge
lengths in tangential direction and thus significantly less cells could be used
without a significant loss in approximation accuracy. An anisotropic
refinement process can modify the aspect ratio from mother to child cells by a
factor of two for each refinement step. In the course of several refinements,
the aspect ratio of the fine cells can be optimized, saving a considerable
number of cells and correspondingly degrees of freedom and thus computational
resources, memory as well as CPU time.

<h3>Implementation</h3>

Most of the time, when we do finite element computations, we only consider one
cell at a time, for example to calculate cell contributions to the global
matrix, or to interpolate boundary values. However, sometimes we have to look
at how cells are related in our algorithms. Relationships between cells come
in two forms: neighborship and mother-child relationship. For the case of
isotropic refinement, deal.II uses certain conventions (invariants) for cell
relationships that are always maintained. For example, a refined cell always
has exactly $2^{dim}$ children. And (except for the 1d case), two neighboring
cells may differ by at most one refinement level: they are equally often
refined or one of them is exactly once more refined, leaving exactly one
hanging node on the common face. Almost all of the time these invariants are
only of concern in the internal implementation of the library. However, there
are cases where knowledge of them is also relevant to an application program.

In the current context, it is worth noting that the kind of mesh refinement
affects some of the most fundamental assumptions. Consequently, some of the
usual code found in application programs will need modifications to exploit
the features of meshes which were created using anisotropic
refinement. For those interested in how deal.II evolved, it may be of
interest that the loosening of such invariants required some
incompatible changes. For example, the library used to have a member
GeometryInfo<dim>::children_per_cell that specified how many children
a cell has once it is refined. For isotropic refinement, this number
is equal to $2^{dim}$, as mentioned above. However, for anisotropic refinement, this number
does not exist, as is can be either two or four in 2D and two, four or eight in
3D, and the member GeometryInfo<dim>::children_per_cell has
consequently been removed. It has now been replaced by
GeometryInfo<dim>::max_children_per_cell which specifies the
<i>maximum</i> number of children a cell can have. How many children a
refined cell has was previously available as static information, but
now it depends on the actual refinement state of a cell and can be
retrieved using TriaAccessor::n_children(),
a call that works equally well for both isotropic and anisotropic
refinement. A very similar situation can be found for
faces and their subfaces: the pertinent information can be queried using
GeometryInfo<dim>::max_children_per_face or <code>face->n_children()</code>,
depending on the context.

Another important aspect, and the most important one in this tutorial, is
the treatment of neighbor-relations when assembling jump terms on the
faces between cells. Looking at the documentation of the
assemble_system functions in step-12 we notice, that we need to decide if a
neighboring cell is coarser, finer or on the same (refinement) level as our
current cell. These decisions do not work in the same way for anisotropic
refinement as the information given by the <em>level</em> of a cell is not
enough to completely characterize anisotropic cells; for example, are
the terminal children of a two-dimensional
cell that is first cut in $x$-direction and whose children are then
cut in $y$-direction on level 2, or are they on level 1 as they would
be if the cell would have been refined once isotropically, resulting
in the same set of finest cells?

After anisotropic refinement, a coarser neighbor is not necessarily
exactly one level below ours, but can pretty much have any level
relative to the current one; in fact, it can even be on a higher
level even though it is coarser. Thus the decisions
have to be made on a different basis, whereas the intention of the
decisions stays the same.

In the following, we will discuss the cases that can happen when we
want to compute contributions to the matrix (or right hand side) of
the form
@f[
  \int_{\partial K} \varphi_i(x) \varphi_j(x) \; dx
@f]
or similar; remember that we integrate terms like this using the
FEFaceValues and FESubfaceValues classes. We will also show how to
write code that works for both isotropic and anisotropic refinement:

<ul>

  <li> <em>Finer neighbor</em>: If we are on an active cell and want
  to integrate over a face $f\subset \partial K$, the first
  possibility is that the neighbor behind this face is more refined,
  i.e. has children occupying only part of the
  common face. In this case, the face
  under consideration has to be a refined one, which can determine by
  asking <code>if (face->has_children())</code>. If this is true, we need to
  loop over
  all subfaces and get the neighbors' child behind this subface, so that we can
  reinit an FEFaceValues object with the neighbor and an FESubfaceValues object
  with our cell and the respective subface.

  For isotropic refinement, this kind is reasonably simple because we
  know that an invariant of the isotropically refined adaptive meshes
  in deal.II is that neighbors can only differ by exactly one
  refinement level. However, this isn't quite true any more for
  anisotropically refined meshes, in particular in 3d; there,
  the active cell we are interested on the other side of $f$ might not
  actually be a child of our
  neighbor, but perhaps a grandchild or even a farther offspring. Fortunately,
  this complexity is hidden in the internals of the library. All we need to do
  is call the CellAccessor::neighbor_child_on_subface()
  function. Still, in 3D there are two cases which need special consideration:
  <ul>
    <li> If the neighbor is refined more than once anisotropically, it might be
  that here are not two or four but actually three subfaces to
  consider. Imagine
  the following refinement process of the (two-dimensional) face of
  the (three-dimensional) neighbor cell we are considering: first the
  face is refined along x, later on only the left subface is refined along y.
@code
   *-------*        *---*---*        *---*---*
   |       |        |   |   |        |   |   |
   |       |  --->  |   |   |  --->  *---*   |
   |       |        |   |   |        |   |   |
   *-------*        *---*---*        *---*---*
@endcode
     Here the number of subfaces is three. It is important to note the subtle
  differences between, for a face, TriaAccessor::n_children() and
  TriaAccessor::n_active_descendants(). The first function returns the number of
  immediate children, which would be two for the above example, whereas the
  second returns the number of active offspring (i.e., including children,
  grandchildren, and further descendants), which is the correct three in
  the example above. Using <code>face->n_active_descendants()</code> works for
  isotropic and anisotropic as well as 2D and 3D cases, so it should always be
  used. It should be noted that if any of the cells behind the two
  small subfaces on the left side of the rightmost image is further
  refined, then the current cell (i.e. the side from which we are
  viewing this common face) is going to be refined as well: this is so
  because otherwise the invariant of having only one hanging node per
  edge would be violated.

    <li> It might be, that the neighbor is coarser, but still has children which
  are finer than our current cell. This situation can occur if two equally
  coarse cells are refined, where one of the cells has two children at the face
  under consideration and the other one four. The cells in the next graphic are
  only separated from each other to show the individual refinement cases.
@code
      *-----------*     *-----------*
     /           /|    /           /|
    ############# |   +++++++++++++ |
   #           ## |  +           ++ *
  ############# # | +++++++++++++ +/|
  #           # # | +           + + |
  #           # # * +           +++ *
  #           # #/  +++++++++++++ +/
  #           # #   +           + +
  #           ##    +           ++
  #############     +++++++++++++
@endcode

  Here, the left two cells resulted from an anisotropic bisection of
  the mother cell in $y$-direction, whereas the right four cells
  resulted from a simultaneous anisotropic refinement in both the $y$-
  and $z$-directions.
  The left cell marked with # has two finer neighbors marked with +, but the
  actual neighbor of the left cell is the complete right mother cell, as the
  two cells marked with + are finer and their direct mother is the one
  large cell.
  </ul>

  However, fortunately, CellAccessor::neighbor_child_on_subface() takes care of
  these situations by itself, if you loop over the correct number of subfaces,
  in the above example this is two. The FESubfaceValues<dim>::reinit function
  takes care of this too, so that the resulting state is always correct. There
  is one little caveat, however: For reiniting the neighbors FEFaceValues object
  you need to know the index of the face that points toward the current
  cell. Usually you assume that the neighbor you get directly is as coarse or as
  fine as you, if it has children, thus this information can be obtained with
  CellAccessor::neighbor_of_neighbor(). If the neighbor is coarser, however, you
  would have to use the first value in CellAccessor::neighbor_of_coarser_neighbor()
  instead. In order to make this easy for you, there is
  CellAccessor::neighbor_face_no() which does the correct thing for you and
  returns the desired result.

  <li> <em>Neighbor is as fine as our cell</em>: After we ruled out all cases in
  which there are finer children, we only need to decide, whether the neighbor
  is coarser here. For this, there is the
  CellAccessor::neighbor_is_coarser() function which returns a boolean. In
  order to get the relevant case of a neighbor of the same coarseness we would
  use <code>else if (!cell->neighbor_is_coarser(face_no))</code>. The code inside this
  block can be left untouched. However, there is one thing to mention here: If
  we want to use a rule, which cell should assemble certain terms on a given
  face we might think of the rule presented in step-12. We know that we have to
  leave out the part about comparing our cell's level with that of the neighbor
  and replace it with the test for a coarser neighbor presented above. However,
  we also have to consider the possibility that neighboring cells of same
  coarseness have the same index (on different levels). Thus we have to include
  the case where the cells have the same index, and give an additional
  condition, which of the cells should assemble the terms, e.g. we can choose
  the cell with lower level. The details of this concept can be seen in the
  implementation below.

  <li> <em>Coarser neighbor</em>: The remaining case is obvious: If there are no
  refined neighbors and the neighbor is not as fine as the current cell, then it must
  be coarser. Thus we can leave the old condition phrase, simply using
  <code>else</code>. The CellAccessor::neighbor_of_coarser_neighbor()
  function takes care of all the complexity of anisotropic refinement combined
  with possible non standard face orientation, flip and rotation on general 3D meshes.

</ul>

<h4>Mesh smoothing</h4>
When a triangulation is refined, cells which were not flagged for refinement may
be refined nonetheless. This is due to additional smoothing algorithms which are
either necessary or requested explicitly. In particular, the restriction that there
be at most one hanging node on each edge frequently forces the refinement of additional
cells neighboring ones that are already finer and are flagged for
further refinement.

However, deal.II also implements a number of algorithms that make sure
that resulting meshes are smoother than just the bare minimum, for
example ensuring that there are no isolated refined cells surrounded
by non-refined ones, since the additional degrees of freedom on these
islands would almost all be constrained by hanging node
constraints. (See the documentation of the Triangulation class and its
Triangulation::MeshSmoothing member for more information on mesh
smoothing.)

Most of the smoothing algorithms that were originally developed for
the isotropic case have been adapted to work in a very similar
way for both anisotropic and isotropic refinement. There are two
algorithms worth mentioning, however:
<ol>
  <li> <code>MeshSmoothing::limit_level_difference_at_vertices</code>: In an isotropic environment,
  this algorithm tries to ensure a good approximation quality by reducing the
  difference in refinement level of cells meeting at a common vertex. However,
  there is no clear corresponding concept for anisotropic refinement, thus this
  algorithm may not be used in combination with anisotropic refinement. This
  restriction is enforced by an assertion which throws an error as soon as the
  algorithm is called on a triangulation which has been refined anisotropically.

  <li> <code>MeshSmoothing::allow_anisotropic_smoothing</code>: If refinement is introduced to
  limit the number of hanging nodes, the additional cells are often not needed
  to improve the approximation quality. This is especially true for DG
  methods. If you set the flag <code>allow_anisotropic_smoothing</code> the
  smoothing algorithm tries to minimize the number of probably unneeded
  additional cells by using anisotropic refinement for the smoothing. If you set
  this smoothing flag you might get anisotropically refined cells, even if you
  never set a single refinement flag to anisotropic refinement. Be aware that
  you should only use this flag, if your code respects the possibility of
  anisotropic meshes. Combined with a suitable anisotropic indicator this flag
  can help save additional cells and thus effort.
</ol>


<h3>Jump indicator</h3>

Using the benefits of anisotropic refinement requires an indicator to catch
anisotropic features of the solution and exploit them for the refinement
process. Generally the anisotropic refinement process will consist of several
steps:
<ol>
  <li> Calculate an error indicator.
  <li> Use the error indicator to flag cells for refinement, e.g. using a fixed
  number or fraction of cells. Those cells will be flagged for isotropic
  refinement automatically.
  <li> Evaluate a distinct anisotropic indicator only on the flagged cells.
  <li> Use the anisotropic indicator to set a new, anisotropic refinement flag
  for cells where this is appropriate, leave the flags unchanged otherwise.
  <li> Call Triangulation<dim>::execute_coarsening_and_refinement to perform the
  requested refinement, using the requested isotropic and anisotropic flags.
</ol>
This approach is similar to the one we have used in step-27
for hp-refinement and
has the great advantage of flexibility: Any error indicator can be
used in the anisotropic process, i.e. if you have quite involved a posteriori
goal-oriented error indicators available you can use them as easily as a simple
Kelly error estimator. The anisotropic part of the refinement process is not
influenced by this choice. Furthermore, simply leaving out the third and forth
steps leads to the same isotropic refinement you used to get before any
anisotropic changes in deal.II or your application program.
As a last advantage, working only
on cells flagged for refinement results in a faster evaluation of the
anisotropic indicator, which can become noticeable on finer meshes with a lot of
cells if the indicator is quite involved.

Here, we use a very simple approach which is only applicable to DG
methods. The general idea is quite simple: DG methods allow the discrete
solution to jump over the faces of a cell, whereas it is smooth within each
cell. Of course, in the limit we expect that the jumps tend to zero as
we refine the mesh and approximate the true solution better and better.
Thus, a large jump
across a given face indicates that the cell should be refined (at least)
orthogonally to that face, whereas a small jump does not lead to this
conclusion. It is possible, of course, that the exact solution is not smooth and
that it also features a jump. In that case, however, a large jump over one face
indicates, that this face is more or less parallel to the jump and in the
vicinity of it, thus again we would expect a refinement orthogonal to the face
under consideration to be effective.

The proposed indicator calculates the average jump $K_j$, i.e. the mean value of
the absolute jump $|[u]|$ of the discrete solution $u$ over the two faces
$f_i^j$, $i=1,2$, $j=1..d$ orthogonal to coordinate direction $j$ on the unit
cell.
@f[
K_j = \frac{\sum_{i=1}^2 \int_{f_i^j}|[u]| dx}{\sum_{i=1}^2 |f_i^j|} .
@f]
If the average jump in one direction is larger than the average of the
jumps in the other directions by a
certain factor $\kappa$, i.e. if
$K_i > \kappa \frac 1{d-1} \sum_{j=1, j\neq i}^d K_j$, the cell is refined only along that particular
direction $i$, otherwise the cell is refined isotropically.

Such a criterion is easily generalized to systems of equations: the
absolute value of the jump would be replaced by an appropriate norm of
the vector-valued jump.



<h3>The problem</h3>

We solve the linear transport equation presented in step-12. The domain is
extended to cover $[-1,1]\times[0,1]$ in 2D, where the flow field $\beta$ describes a
counterclockwise quarter circle around the origin in the right half of the
domain and is parallel to the x-axis in the left part of the domain. The inflow
boundary is again located at $x=1$ and along the positive part of the x-axis,
and the boundary conditions are chosen as in step-12.


examples/step-30/doc/results.dox
<h1>Results</h1>


The output of this program consist of the console output, the SVG
files containing the grids, and the solutions given in VTU format.
@code
Performing a 2D run with isotropic refinement...
------------------------------------------------
Cycle 0:
   Number of active cells:       128
   Number of degrees of freedom: 512
   Time of assemble_system: 0.092049
   Writing grid to <grid-0.iso.svg>...
   Writing solution to <sol-0.iso.vtu>...

Cycle 1:
   Number of active cells:       239
   Number of degrees of freedom: 956
   Time of assemble_system: 0.109519
   Writing grid to <grid-1.iso.svg>...
   Writing solution to <sol-1.iso.vtu>...

Cycle 2:
   Number of active cells:       491
   Number of degrees of freedom: 1964
   Time of assemble_system: 0.08303
   Writing grid to <grid-2.iso.svg>...
   Writing solution to <sol-2.iso.vtu>...

Cycle 3:
   Number of active cells:       1031
   Number of degrees of freedom: 4124
   Time of assemble_system: 0.278987
   Writing grid to <grid-3.iso.svg>...
   Writing solution to <sol-3.iso.vtu>...

Cycle 4:
   Number of active cells:       2027
   Number of degrees of freedom: 8108
   Time of assemble_system: 0.305869
   Writing grid to <grid-4.iso.svg>...
   Writing solution to <sol-4.iso.vtu>...

Cycle 5:
   Number of active cells:       4019
   Number of degrees of freedom: 16076
   Time of assemble_system: 0.47616
   Writing grid to <grid-5.iso.svg>...
   Writing solution to <sol-5.iso.vtu>...


Performing a 2D run with anisotropic refinement...
--------------------------------------------------
Cycle 0:
   Number of active cells:       128
   Number of degrees of freedom: 512
   Time of assemble_system: 0.052866
   Writing grid to <grid-0.aniso.svg>...
   Writing solution to <sol-0.aniso.vtu>...

Cycle 1:
   Number of active cells:       171
   Number of degrees of freedom: 684
   Time of assemble_system: 0.050917
   Writing grid to <grid-1.aniso.svg>...
   Writing solution to <sol-1.aniso.vtu>...

Cycle 2:
   Number of active cells:       255
   Number of degrees of freedom: 1020
   Time of assemble_system: 0.064132
   Writing grid to <grid-2.aniso.svg>...
   Writing solution to <sol-2.aniso.vtu>...

Cycle 3:
   Number of active cells:       394
   Number of degrees of freedom: 1576
   Time of assemble_system: 0.119849
   Writing grid to <grid-3.aniso.svg>...
   Writing solution to <sol-3.aniso.vtu>...

Cycle 4:
   Number of active cells:       648
   Number of degrees of freedom: 2592
   Time of assemble_system: 0.218244
   Writing grid to <grid-4.aniso.svg>...
   Writing solution to <sol-4.aniso.vtu>...

Cycle 5:
   Number of active cells:       1030
   Number of degrees of freedom: 4120
   Time of assemble_system: 0.128121
   Writing grid to <grid-5.aniso.svg>...
   Writing solution to <sol-5.aniso.vtu>...
@endcode

This text output shows the reduction in the number of cells which results from
the successive application of anisotropic refinement. After the last refinement
step the savings have accumulated so much that almost four times as many cells
and thus degrees of freedom are needed in the isotropic case. The time needed for assembly
scales with a similar factor.

The first interesting part is of course to see how the meshes look like.
On the left are the isotropically refined ones, on the right the
anisotropic ones (colors indicate the refinement level of cells):

<table width="80%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.grid-0.iso.9.2.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.grid-0.aniso.9.2.png" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.grid-1.iso.9.2.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.grid-1.aniso.9.2.png" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.grid-2.iso.9.2.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.grid-2.aniso.9.2.png" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.grid-3.iso.9.2.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.grid-3.aniso.9.2.png" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.grid-4.iso.9.2.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.grid-4.aniso.9.2.png" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.grid-5.iso.9.2.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.grid-5.aniso.9.2.png" alt="">
    </td>
  </tr>
</table>


The other interesting thing is, of course, to see the solution on these
two sequences of meshes. Here they are, on the refinement cycles 1 and 4,
clearly showing that the solution is indeed composed of <i>discontinuous</i> piecewise
polynomials:

<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.sol-1.iso.9.2.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.sol-1.aniso.9.2.png" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.sol-4.iso.9.2.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-30.sol-4.aniso.9.2.png" alt="">
    </td>
  </tr>
</table>

We see, that the solution on the anisotropically refined mesh is very similar to
the solution obtained on the isotropically refined mesh. Thus the anisotropic
indicator seems to effectively select the appropriate cells for anisotropic
refinement.

The pictures also explain why the mesh is refined as it is.
In the whole left part of the domain refinement is only performed along the
$y$-axis of cells. In the right part of the domain the refinement is dominated by
isotropic refinement, as the anisotropic feature of the solution - the jump from
one to zero - is not well aligned with the mesh where the advection direction
takes a turn. However, at the bottom and closest (to the observer) parts of the
quarter circle this jumps again becomes more and more aligned
with the mesh and the refinement algorithm reacts by creating anisotropic cells
of increasing aspect ratio.

It might seem that the necessary alignment of anisotropic features and the
coarse mesh can decrease performance significantly for real world
problems. That is not wrong in general: If one were, for example, to apply
anisotropic refinement to problems in which shocks appear (e.g., the
equations solved in step-69), then it many cases the shock is not aligned
with the mesh and anisotropic refinement will help little unless one also
introduces techniques to move the mesh in alignment with the shocks.
On the other hand, many steep features of solutions are due to boundary layers.
In those cases, the mesh is already aligned with the anisotropic features
because it is of course aligned with the boundary itself, and anisotropic
refinement will almost always increase the efficiency of computations on
adapted grids for these cases.


examples/step-31/doc/intro.dox
<br>

<i>This program was contributed by Martin Kronbichler and Wolfgang
Bangerth.
<br>
This material is based upon work partly supported by the National
Science Foundation under Award No. EAR-0426271 and The California Institute of
Technology. Any opinions, findings, and conclusions or recommendations
expressed in this publication are those of the author and do not
necessarily reflect the views of the National Science Foundation or of The
California Institute of Technology.
</i>


<a name="Intro"></a>
<h1>Introduction</h1>

<h3>The Boussinesq equations</h3>

This program deals with an interesting physical problem: how does a
fluid (i.e., a liquid or gas) behave if it experiences differences in
buoyancy caused by temperature differences? It is clear that those
parts of the fluid that are hotter (and therefore lighter) are going
to rise up and those that are cooler (and denser) are going to sink
down with gravity.

In cases where the fluid moves slowly enough such that inertial effects
can be neglected, the equations that describe such behavior are the
Boussinesq equations that read as follows:
@f{eqnarray*}
  -\nabla \cdot (2 \eta \varepsilon ({\mathbf u})) + \nabla p &=&
  -\rho\; \beta \; T\; \mathbf{g},
  \\
  \nabla \cdot {\mathbf u} &=& 0,
  \\
  \frac{\partial T}{\partial t}
  +
  {\mathbf u} \cdot \nabla T
  -
  \nabla \cdot \kappa \nabla T &=& \gamma.
@f}
These equations fall into the class of vector-valued problems (a
toplevel overview of this topic can be found in the @ref vector_valued module).
Here, $\mathbf u$ is the velocity field, $p$ the pressure, and $T$
the temperature of the fluid. $\varepsilon ({\mathbf u}) = \frac 12
[(\nabla{\mathbf u}) + (\nabla {\mathbf u})^T]$ is the symmetric
gradient of the velocity. As can be seen, velocity and pressure
solve a Stokes equation describing the motion of an incompressible
fluid, an equation we have previously considered in step-22; we
will draw extensively on the experience we have gained in that program, in
particular with regard to efficient linear Stokes solvers.

The forcing term of the fluid motion is the buoyancy of the
fluid, expressed as the product of the density $\rho$, the thermal expansion
coefficient $\beta$,
the temperature $T$ and the gravity vector $\mathbf{g}$ pointing
downward. (A derivation of why the right hand side looks like it looks
is given in the introduction of step-32.)
While the first two equations describe how the fluid reacts to
temperature differences by moving around, the third equation states
how the fluid motion affects the temperature field: it is an advection
diffusion equation, i.e., the temperature is attached to the fluid
particles and advected along in the flow field, with an additional
diffusion (heat conduction) term. In many applications, the diffusion
coefficient is fairly small, and the temperature equation is in fact
transport, not diffusion dominated and therefore in character more hyperbolic
than elliptic; we will have to take this into account when developing a stable
discretization.

In the equations above, the term $\gamma$ on the right hand side denotes the
heat sources and may be a spatially and temporally varying function. $\eta$
and $\kappa$ denote the viscosity and diffusivity coefficients, which we assume
constant for this tutorial program. The more general case when $\eta$ depends on
the temperature is an important factor in physical applications: Most materials
become more fluid as they get hotter (i.e., $\eta$ decreases with $T$);
sometimes, as in the case of rock minerals at temperatures close to their
melting point, $\eta$ may change by orders of magnitude over the typical range
of temperatures.

We note that the Stokes equation above could be nondimensionalized by
introducing the <a target="_top"
href="http://en.wikipedia.org/wiki/Rayleigh_number">Rayleigh
number</a> $\mathrm{Ra}=\frac{\|\mathbf{g}\| \beta \rho}{\eta \kappa} \delta T L^3$ using a
typical length scale $L$, typical temperature difference $\delta T$, density
$\rho$, thermal diffusivity $\eta$, and thermal conductivity $\kappa$.
$\mathrm{Ra}$ is a dimensionless number that describes the ratio of heat
transport due to convection induced by buoyancy changes from
temperature differences, and of heat transport due to thermal
diffusion. A small Rayleigh number implies that buoyancy is not strong
relative to viscosity and fluid motion $\mathbf{u}$ is slow enough so
that heat diffusion $\kappa\nabla T$ is the dominant heat transport
term. On the other hand, a fluid with a high Rayleigh number will show
vigorous convection that dominates heat conduction.

For most fluids for which we are interested in computing thermal
convection, the Rayleigh number is very large, often $10^6$ or
larger. From the structure of the equations, we see that this will
lead to large pressure differences and large velocities. Consequently,
the convection term in the convection-diffusion equation for $T$ will
also be very large and an accurate solution of this equation will
require us to choose small time steps. Problems with large Rayleigh
numbers are therefore hard to solve numerically for similar reasons
that make solving the <a
href="http://en.wikipedia.org/wiki/Navier-stokes_equations">Navier-Stokes
equations</a> hard to solve when the <a
href="http://en.wikipedia.org/wiki/Reynolds_number">Reynolds number
$\mathrm{Re}$</a> is large.

Note that a large Rayleigh number does not necessarily involve large
velocities in absolute terms. For example, the Rayleigh number in the
earth mantle is larger than $10^6$. Yet the
velocities are small: the material is in fact solid rock but it is so
hot and under pressure that it can flow very slowly, on the order of
at most a few centimeters per year. Nevertheless, this can lead to
mixing over time scales of many million years, a time scale much
shorter than for the same amount of heat to be distributed by thermal
conductivity and a time scale of relevance to affect the evolution of the
earth's interior and surface structure.

@note If you are interested in using the program as the basis for your own
experiments, you will also want to take a look at its continuation in
step-32. Furthermore, step-32 later was developed into the much larger open
source code ASPECT (see https://aspect.geodynamics.org/ ) that can solve realistic
problems and that you may want to investigate before trying to morph step-31
into something that can solve whatever you want to solve.


<h3>Boundary and initial conditions</h3>

Since the Boussinesq equations are derived under the assumption that inertia
of the fluid's motion does not play a role, the flow field is at each time
entirely determined by buoyancy difference at that time, not by the flow field
at previous times. This is reflected by the fact that the first two equations
above are the steady state Stokes equation that do not contain a time
derivative. Consequently, we do not need initial conditions for either
velocities or pressure. On the other hand, the temperature field does satisfy
an equation with a time derivative, so we need initial conditions for $T$.

As for boundary conditions: if $\kappa>0$ then the temperature
satisfies a second order differential equation that requires
boundary data all around the boundary for all times. These can either be a
prescribed boundary temperature $T|_{\partial\Omega}=T_b$ (Dirichlet boundary
conditions), or a prescribed thermal flux $\mathbf{n}\cdot\kappa\nabla
T|_{\partial\Omega}=\phi$; in this program, we will use an insulated boundary
condition, i.e., prescribe no thermal flux: $\phi=0$.

Similarly, the velocity field requires us to pose boundary conditions. These
may be no-slip no-flux conditions $\mathbf{u}=0$ on $\partial\Omega$ if the fluid
sticks to the boundary, or no normal flux conditions $\mathbf n \cdot \mathbf
u = 0$ if the fluid can flow along but not across the boundary, or any number
of other conditions that are physically reasonable. In this program, we will
use no normal flux conditions.


<h3>Solution approach</h3>

Like the equations solved in step-21, we here have a
system of differential-algebraic equations (DAE): with respect to the time
variable, only the temperature equation is a differential equation
whereas the Stokes system for $\mathbf{u}$ and $p$ has no
time-derivatives and is therefore of the sort of an algebraic
constraint that has to hold at each time instant. The main difference
to step-21 is that the algebraic constraint there was a
mixed Laplace system of the form
@f{eqnarray*}
  \mathbf u + {\mathbf K}\lambda \nabla p &=& 0, \\
  \nabla\cdot \mathbf u &=& f,
@f}
where now we have a Stokes system
@f{eqnarray*}
  -\nabla \cdot (2 \eta \varepsilon ({\mathbf u})) + \nabla p &=& f, \\
  \nabla\cdot \mathbf u &=& 0,
@f}
where $\nabla \cdot \eta \varepsilon (\cdot)$ is an operator similar to the
Laplacian $\Delta$ applied to a vector field.

Given the similarity to what we have done in step-21,
it may not come as a surprise that we choose a similar approach,
although we will have to make adjustments for the change in operator
in the top-left corner of the differential operator.


<h4>Time stepping</h4>

The structure of the problem as a DAE allows us to use the same strategy as
we have already used in step-21, i.e., we use a time lag
scheme: we first solve the temperature equation (using an extrapolated
velocity field), and then insert the new temperature solution into the right
hand side of the velocity equation. The way we implement this in our code
looks at things from a slightly different perspective, though. We first
solve the Stokes equations for velocity and pressure using the temperature
field from the previous time step, which means that we get the velocity for
the previous time step. In other words, we first solve the Stokes system for
time step $n - 1$ as
@f{eqnarray*}
  -\nabla \cdot (2\eta \varepsilon ({\mathbf u}^{n-1})) + \nabla p^{n-1} &=&
  -\rho\; \beta \; T^{n-1} \mathbf{g},
  \\
  \nabla \cdot {\mathbf u}^{n-1} &=& 0,
@f}
and then the temperature equation with an extrapolated velocity field to
time $n$.

In contrast to step-21, we'll use a higher order time
stepping scheme here, namely the <a
href="http://en.wikipedia.org/wiki/Backward_differentiation_formula">Backward
Differentiation Formula scheme of order 2 (BDF-2 in short)</a> that replaces
the time derivative $\frac{\partial T}{\partial t}$ by the (one-sided)
difference quotient $\frac{\frac 32 T^{n}-2T^{n-1}+\frac 12 T^{n-2}}{k}$
with $k$ the time step size. This gives the discretized-in-time
temperature equation
@f{eqnarray*}
  \frac 32 T^n
  -
  k\nabla \cdot \kappa \nabla T^n
  &=&
  2 T^{n-1}
  -
  \frac 12 T^{n-2}
  -
  k(2{\mathbf u}^{n-1} - {\mathbf u}^{n-2} ) \cdot \nabla (2T^{n-1}-T^{n-2})
  +
  k\gamma.
@f}
Note how the temperature equation is solved semi-explicitly: diffusion is
treated implicitly whereas advection is treated explicitly using an
extrapolation (or forward-projection) of temperature and velocity, including
the just-computed velocity ${\mathbf u}^{n-1}$. The forward-projection to
the current time level $n$ is derived from a Taylor expansion, $T^n
\approx T^{n-1} + k_n \frac{\partial T}{\partial t} \approx T^{n-1} + k_n
\frac{T^{n-1}-T^{n-2}}{k_n} = 2T^{n-1}-T^{n-2}$. We need this projection for
maintaining the order of accuracy of the BDF-2 scheme. In other words, the
temperature fields we use in the explicit right hand side are second order
approximations of the current temperature field &mdash; not quite an
explicit time stepping scheme, but by character not too far away either.

The introduction of the temperature extrapolation limits the time step by a
<a href="http://en.wikipedia.org/wiki/Courant–Friedrichs–Lewy_condition">
Courant-Friedrichs-Lewy (CFL) condition</a> just like it was in @ref step_21
"step-21". (We wouldn't have had that stability condition if we treated the
advection term implicitly since the BDF-2 scheme is A-stable, at the price
that we needed to build a new temperature matrix at each time step.) We will
discuss the exact choice of time step in the <a href="#Results">results
section</a>, but for the moment of importance is that this CFL condition
means that the time step size $k$ may change from time step to time
step, and that we have to modify the above formula slightly. If
$k_n,k_{n-1}$ are the time steps sizes of the current and previous time
step, then we use the approximations
@f{align*}{
\frac{\partial T}{\partial t} \approx
 \frac 1{k_n}
 \left(
       \frac{2k_n+k_{n-1}}{k_n+k_{n-1}} T^{n}
       -
       \frac{k_n+k_{n-1}}{k_{n-1}}T^{n-1}
       +
       \frac{k_n^2}{k_{n-1}(k_n+k_{n-1})} T^{n-2}
 \right)
 @f}
and
@f{align*}{
T^n \approx
   T^{n-1} + k_n \frac{\partial T}{\partial t}
   \approx
   T^{n-1} + k_n
   \frac{T^{n-1}-T^{n-2}}{k_{n-1}}
   =
   \left(1+\frac{k_n}{k_{n-1}}\right)T^{n-1}-\frac{k_n}{k_{n-1}}T^{n-2},
@f}
and above equation is generalized as follows:
@f{eqnarray*}
  \frac{2k_n+k_{n-1}}{k_n+k_{n-1}} T^n
  -
  k_n\nabla \cdot \kappa \nabla T^n
  &=&
  \frac{k_n+k_{n-1}}{k_{n-1}} T^{n-1}
  -
  \frac{k_n^2}{k_{n-1}(k_n+k_{n-1})} T^{n-2}
  -
  k_n{\mathbf u}^{*,n} \cdot \nabla T^{*,n}
  +
  k_n\gamma,
@f}

where ${(\cdot)}^{*,n} = \left(1+\frac{k_n}{k_{n-1}}\right)(\cdot)^{n-1} -
\frac{k_n}{k_{n-1}}(\cdot)^{n-2}$ denotes the extrapolation of velocity
$\mathbf u$ and temperature $T$ to time level $n$, using the values
at the two previous time steps. That's not an easy to read equation, but
will provide us with the desired higher order accuracy. As a consistency
check, it is easy to verify that it reduces to the same equation as above if
$k_n=k_{n-1}$.

As a final remark we note that the choice of a higher order time
stepping scheme of course forces us to keep more time steps in memory;
in particular, we here will need to have $T^{n-2}$ around, a vector
that we could previously discard. This seems like a nuisance that we
were able to avoid previously by using only a first order time
stepping scheme, but as we will see below when discussing the topic of
stabilization, we will need this vector anyway and so keeping it
around for time discretization is essentially for free and gives us
the opportunity to use a higher order scheme.


<h4>Weak form and space discretization for the Stokes part</h4>

Like solving the mixed Laplace equations, solving the Stokes equations
requires us to choose particular pairs of finite elements for
velocities and pressure variables. Because this has already been discussed in
step-22, we only cover this topic briefly:
Here, we use the
stable pair $Q_{p+1}^d \times Q_p, p\ge 1$. These are continuous
elements, so we can form the weak form of the Stokes equation without
problem by integrating by parts and substituting continuous functions
by their discrete counterparts:
@f{eqnarray*}
  (\nabla {\mathbf v}_h, 2\eta \varepsilon ({\mathbf u}^{n-1}_h))
  -
  (\nabla \cdot {\mathbf v}_h, p^{n-1}_h)
  &=&
  -({\mathbf v}_h, \rho\; \beta \; T^{n-1}_h \mathbf{g}),
  \\
  (q_h, \nabla \cdot {\mathbf u}^{n-1}_h) &=& 0,
@f}
for all test functions $\mathbf v_h, q_h$. The first term of the first
equation is considered as the inner product between tensors, i.e.
$(\nabla {\mathbf v}_h, \eta \varepsilon ({\mathbf u}^{n-1}_h))_\Omega
 = \int_\Omega \sum_{i,j=1}^d [\nabla {\mathbf v}_h]_{ij}
           \eta [\varepsilon ({\mathbf u}^{n-1}_h)]_{ij}\, dx$.
Because the second tensor in this product is symmetric, the
anti-symmetric component of $\nabla {\mathbf v}_h$ plays no role and
it leads to the entirely same form if we use the symmetric gradient of
$\mathbf v_h$ instead. Consequently, the formulation we consider and
that we implement is
@f{eqnarray*}
  (\varepsilon({\mathbf v}_h), 2\eta \varepsilon ({\mathbf u}^{n-1}_h))
  -
  (\nabla \cdot {\mathbf v}_h, p^{n-1}_h)
  &=&
  -({\mathbf v}_h, \rho\; \beta \; T^{n-1}_h \mathbf{g}),
  \\
  (q_h, \nabla \cdot {\mathbf u}^{n-1}_h) &=& 0.
@f}

This is exactly the same as what we already discussed in
step-22 and there is not much more to say about this here.


<h4>Stabilization, weak form and space discretization for the temperature equation</h4>

The more interesting question is what to do with the temperature
advection-diffusion equation. By default, not all discretizations of
this equation are equally stable unless we either do something like
upwinding, stabilization, or all of this. One way to achieve this is
to use discontinuous elements (i.e., the FE_DGQ class that we used, for
example, in the discretization of the transport equation in
step-12, or in discretizing the pressure in
step-20 and step-21) and to define a
flux at the interface between cells that takes into account
upwinding. If we had a pure advection problem this would probably be
the simplest way to go. However, here we have some diffusion as well,
and the discretization of the Laplace operator with discontinuous
elements is cumbersome because of the significant number of additional
terms that need to be integrated on each face between
cells. Discontinuous elements also have the drawback that the use of
numerical fluxes introduces an additional numerical diffusion that
acts everywhere, whereas we would really like to minimize the effect
of numerical diffusion to a minimum and only apply it where it is
necessary to stabilize the scheme.

A better alternative is therefore to add some nonlinear viscosity to
the model. Essentially, what this does is to transform the temperature
equation from the form
@f{eqnarray*}
  \frac{\partial T}{\partial t}
  +
  {\mathbf u} \cdot \nabla T
  -
  \nabla \cdot \kappa \nabla T &=& \gamma
@f}
to something like
@f{eqnarray*}
  \frac{\partial T}{\partial t}
  +
  {\mathbf u} \cdot \nabla T
  -
  \nabla \cdot (\kappa+\nu(T)) \nabla T &=& \gamma,
@f}
where $\nu(T)$ is an addition viscosity (diffusion) term that only
acts in the vicinity of shocks and other discontinuities. $\nu(T)$ is
chosen in such a way that if $T$ satisfies the original equations, the
additional viscosity is zero.

To achieve this, the literature contains a number of approaches. We
will here follow one developed by Guermond and Popov that builds on a
suitably defined residual and a limiting procedure for the additional
viscosity. To this end, let us define a residual $R_\alpha(T)$ as follows:
@f{eqnarray*}
  R_\alpha(T)
  =
  \left(
  \frac{\partial T}{\partial t}
  +
  {\mathbf u} \cdot \nabla T
  -
  \nabla \cdot \kappa \nabla T - \gamma
  \right)
  T^{\alpha-1}
@f}
where we will later choose the stabilization exponent $\alpha$ from
within the range $[1,2]$. Note that $R_\alpha(T)$ will be zero if $T$
satisfies the temperature equation, since then the term in parentheses
will be zero. Multiplying terms out, we get the following, entirely
equivalent form:
@f{eqnarray*}
  R_\alpha(T)
  =
  \frac 1\alpha
  \frac{\partial (T^\alpha)}{\partial t}
  +
  \frac 1\alpha
  {\mathbf u} \cdot \nabla (T^\alpha)
  -
  \frac 1\alpha
  \nabla \cdot \kappa \nabla (T^\alpha)
  +
  \kappa(\alpha-1)
  T^{\alpha-2} |\nabla T|^2
  -
  \gamma
  T^{\alpha-1}
@f}

With this residual, we can now define the artificial viscosity as
a piecewise constant function defined on each cell $K$ with diameter
$h_K$ separately as
follows:
@f{eqnarray*}
  \nu_\alpha(T)|_K
  =
  \beta
  \|\mathbf{u}\|_{L^\infty(K)}
  \min\left\{
    h_K,
    h_K^\alpha
    \frac{\|R_\alpha(T)\|_{L^\infty(K)}}{c(\mathbf{u},T)}
  \right\}
@f}

Here, $\beta$ is a stabilization constant (a dimensional analysis
reveals that it is unitless and therefore independent of scaling; we will
discuss its choice in the <a href="#Results">results section</a>) and
$c(\mathbf{u},T)$ is a normalization constant that must have units
$\frac{m^{\alpha-1}K^\alpha}{s}$. We will choose it as
$c(\mathbf{u},T) =
 c_R\ \|\mathbf{u}\|_{L^\infty(\Omega)} \ \mathrm{var}(T)
 \ |\mathrm{diam}(\Omega)|^{\alpha-2}$,
where $\mathrm{var}(T)=\max_\Omega T - \min_\Omega T$ is the range of present
temperature values (remember that buoyancy is driven by temperature
variations, not the absolute temperature) and $c_R$ is a dimensionless
constant. To understand why this method works consider this: If on a particular
cell $K$ the temperature field is smooth, then we expect the residual
to be small there (in fact to be on the order of ${\cal O}(h_K)$) and
the stabilization term that injects artificial diffusion will there be
of size $h_K^{\alpha+1}$ &mdash; i.e., rather small, just as we hope it to
be when no additional diffusion is necessary. On the other hand, if we
are on or close to a discontinuity of the temperature field, then the
residual will be large; the minimum operation in the definition of
$\nu_\alpha(T)$ will then ensure that the stabilization has size $h_K$
&mdash; the optimal amount of artificial viscosity to ensure stability of
the scheme.

Whether or not this scheme really works is a good question.
Computations by Guermond and Popov have shown that this form of
stabilization actually performs much better than most of the other
stabilization schemes that are around (for example streamline
diffusion, to name only the simplest one). Furthermore, for $\alpha\in
[1,2)$ they can even prove that it produces better convergence orders
for the linear transport equation than for example streamline
diffusion. For $\alpha=2$, no theoretical results are currently
available, but numerical tests indicate that the results
are considerably better than for $\alpha=1$.

A more practical question is how to introduce this artificial
diffusion into the equations we would like to solve. Note that the
numerical viscosity $\nu(T)$ is temperature-dependent, so the equation
we want to solve is nonlinear in $T$ &mdash; not what one desires from a
simple method to stabilize an equation, and even less so if we realize
that $\nu(T)$ is nondifferentiable in $T$. However, there is no
reason to despair: we still have to discretize in time and we can
treat the term explicitly.

In the definition of the stabilization parameter, we approximate the time
derivative by $\frac{\partial T}{\partial t} \approx
\frac{T^{n-1}-T^{n-2}}{k^{n-1}}$. This approximation makes only use
of available time data and this is the reason why we need to store data of two
previous time steps (which enabled us to use the BDF-2 scheme without
additional storage cost). We could now simply evaluate the rest of the
terms at $t_{n-1}$, but then the discrete residual would be nothing else than
a backward Euler approximation, which is only first order accurate. So, in
case of smooth solutions, the residual would be still of the order $h$,
despite the second order time accuracy in the outer BDF-2 scheme and the
spatial FE discretization. This is certainly not what we want to have
(in fact, we desired to have small residuals in regions where the solution
behaves nicely), so a bit more care is needed. The key to this problem
is to observe that the first derivative as we constructed it is actually
centered at $t_{n-\frac{3}{2}}$. We get the desired second order accurate
residual calculation if we evaluate all spatial terms at $t_{n-\frac{3}{2}}$
by using the approximation $\frac 12 T^{n-1}+\frac 12 T^{n-2}$, which means
that we calculate the nonlinear viscosity as a function of this
intermediate temperature, $\nu_\alpha =
\nu_\alpha\left(\frac 12 T^{n-1}+\frac 12 T^{n-2}\right)$. Note that this
evaluation of the residual is nothing else than a Crank-Nicholson scheme,
so we can be sure that now everything is alright. One might wonder whether
it is a problem that the numerical viscosity now is not evaluated at
time $n$ (as opposed to the rest of the equation). However, this offset
is uncritical: For smooth solutions, $\nu_\alpha$ will vary continuously,
so the error in time offset is $k$ times smaller than the nonlinear
viscosity itself, i.e., it is a small higher order contribution that is
left out. That's fine because the term itself is already at the level of
discretization error in smooth regions.

Using the BDF-2 scheme introduced above,
this yields for the simpler case of uniform time steps of size $k$:
@f{eqnarray*}
  \frac 32 T^n
  -
  k\nabla \cdot \kappa \nabla T^n
  &=&
  2 T^{n-1}
  -
  \frac 12 T^{n-2}
  \\
  &&
  +
  k\nabla \cdot
  \left[
    \nu_\alpha\left(\frac 12 T^{n-1}+\frac 12 T^{n-2}\right)
    \ \nabla (2T^{n-1}-T^{n-2})
  \right]
  \\
  &&
  -
  k(2{\mathbf u}^{n-1}-{\mathbf u}^{n-2}) \cdot \nabla (2T^{n-1}-T^{n-2})
  \\
  &&
  +
  k\gamma.
@f}
On the left side of this equation remains the term from the time
derivative and the original (physical) diffusion which we treat
implicitly (this is actually a nice term: the matrices that result
from the left hand side are the mass matrix and a multiple of the
Laplace matrix &mdash; both are positive definite and if the time step
size $k$ is small, the sum is simple to invert). On the right hand
side, the terms in the first line result from the time derivative; in
the second line is the artificial diffusion at time $t_{n-\frac
32}$; the third line contains the
advection term, and the fourth the sources. Note that the
artificial diffusion operates on the extrapolated
temperature at the current time in the same way as we have discussed
the advection works in the section on time stepping.

The form for nonuniform time steps that we will have to use in
reality is a bit more complicated (which is why we showed the simpler
form above first) and reads:
@f{eqnarray*}
  \frac{2k_n+k_{n-1}}{k_n+k_{n-1}} T^n
  -
  k_n\nabla \cdot \kappa \nabla T^n
  &=&
  \frac{k_n+k_{n-1}}{k_{n-1}} T^{n-1}
  -
  \frac{k_n^2}{k_{n-1}(k_n+k_{n-1})} T^{n-2}
  \\
  &&
  +
  k_n\nabla \cdot
  \left[
    \nu_\alpha\left(\frac 12 T^{n-1}+\frac 12 T^{n-2}\right)
    \ \nabla  \left[
    \left(1+\frac{k_n}{k_{n-1}}\right)T^{n-1}-\frac{k_n}{k_{n-1}}T^{n-2}
  \right]
  \right]
  \\
  &&
  -
  k_n
  \left[
    \left(1+\frac{k_n}{k_{n-1}}\right){\mathbf u}^{n-1} -
    \frac{k_n}{k_{n-1}}{\mathbf u}^{n-2}
  \right]
  \cdot \nabla
  \left[
    \left(1+\frac{k_n}{k_{n-1}}\right)T^{n-1}-\frac{k_n}{k_{n-1}}T^{n-2}
  \right]
  \\
  &&
  +
  k_n\gamma.
@f}

After settling all these issues, the weak form follows naturally from
the strong form shown in the last equation, and we immediately arrive
at the weak form of the discretized equations:
@f{eqnarray*}
  \frac{2k_n+k_{n-1}}{k_n+k_{n-1}} (\tau_h,T_h^n)
  +
  k_n (\nabla \tau_h, \kappa \nabla T_h^n)
  &=&
  \biggl(\tau_h,
  \frac{k_n+k_{n-1}}{k_{n-1}} T_h^{n-1}
  -
  \frac{k_n^2}{k_{n-1}(k_n+k_{n-1})} T_h^{n-2}
  \\
  &&\qquad
  -
  k_n
  \left[
    \left(1+\frac{k_n}{k_{n-1}}\right){\mathbf u}^{n-1} -
    \frac{k_n}{k_{n-1}}{\mathbf u}^{n-2}
  \right]
  \cdot \nabla
  \left[
    \left(1+\frac{k_n}{k_{n-1}}\right)T^{n-1}-\frac{k_n}{k_{n-1}}T^{n-2}
  \right]
  +
  k_n\gamma \biggr)
  \\
  &&
  -
  k_n \left(\nabla \tau_h,
    \nu_\alpha\left(\frac 12 T_h^{n-1}+\frac 12 T_h^{n-2}\right)
    \ \nabla \left[
    \left(1+\frac{k_n}{k_{n-1}}\right)T^{n-1}-\frac{k_n}{k_{n-1}}T^{n-2}
  \right]
  \right)
@f}
for all discrete test functions $\tau_h$. Here, the diffusion term has been
integrated by parts, and we have used that we will impose no thermal flux,
$\mathbf{n}\cdot\kappa\nabla T|_{\partial\Omega}=0$.

This then results in a
matrix equation of form
@f{eqnarray*}
  \left( \frac{2k_n+k_{n-1}}{k_n+k_{n-1}} M+k_n A_T\right) T_h^n
  = F(U_h^{n-1}, U_h^{n-2},T_h^{n-1},T_h^{n-2}),
@f}
which given the structure of matrix on the left (the sum of two
positive definite matrices) is easily solved using the Conjugate
Gradient method.



<h4>Linear solvers</h4>

As explained above, our approach to solving the joint system for
velocities/pressure on the one hand and temperature on the other is to use an
operator splitting where we first solve the Stokes system for the velocities
and pressures using the old temperature field, and then solve for the new
temperature field using the just computed velocity field. (A more
extensive discussion of operator splitting methods can be found in step-58.)


<h5>Linear solvers for the Stokes problem</h5>

Solving the linear equations coming from the Stokes system has been
discussed in great detail in step-22. In particular, in
the results section of that program, we have discussed a number of
alternative linear solver strategies that turned out to be more
efficient than the original approach. The best alternative
identified there we to use a GMRES solver preconditioned by a block
matrix involving the Schur complement. Specifically, the Stokes
operator leads to a block structured matrix
@f{eqnarray*}
  \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
@f}
and as discussed there a good preconditioner is
@f{eqnarray*}
  P
  =
  \left(\begin{array}{cc}
    A & 0 \\ B & -S
  \end{array}\right),
  \qquad
  \text{or equivalently}
  \qquad
  P^{-1}
  =
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ S^{-1} B A^{-1} & -S^{-1}
  \end{array}\right)
@f}
where $S$ is the Schur complement of the Stokes operator
$S=B^TA^{-1}B$. Of course, this preconditioner is not useful because we
can't form the various inverses of matrices, but we can use the
following as a preconditioner:
@f{eqnarray*}
  \tilde P^{-1}
  =
  \left(\begin{array}{cc}
    \tilde A^{-1} & 0 \\ \tilde S^{-1} B \tilde A^{-1} & -\tilde S^{-1}
  \end{array}\right)
@f}
where $\tilde A^{-1},\tilde S^{-1}$ are approximations to the inverse
matrices. In particular, it turned out that $S$ is spectrally
equivalent to the mass matrix and consequently replacing $\tilde
S^{-1}$ by a CG solver applied to the mass matrix on the pressure
space was a good choice. In a small deviation from step-22, we
here have a coefficient $\eta$ in the momentum equation, and by the same
derivation as there we should arrive at the conclusion that it is the weighted
mass matrix with entries $\tilde S_{ij}=(\eta^{-1}\varphi_i,\varphi_j)$ that
we should be using.

It was more complicated to come up with a good replacement $\tilde
A^{-1}$, which corresponds to the discretized symmetric Laplacian of
the vector-valued velocity field, i.e.
$A_{ij} = (\varepsilon {\mathbf v}_i, 2\eta \varepsilon ({\mathbf
v}_j))$.
In step-22 we used a sparse LU decomposition (using the
SparseDirectUMFPACK class) of $A$ for $\tilde A^{-1}$ &mdash; the
perfect preconditioner &mdash; in 2d, but for 3d memory and compute
time is not usually sufficient to actually compute this decomposition;
consequently, we only use an incomplete LU decomposition (ILU, using
the SparseILU class) in 3d.

For this program, we would like to go a bit further. To this end, note
that the symmetrized bilinear form on vector fields,
$(\varepsilon {\mathbf v}_i, 2 \eta \varepsilon ({\mathbf v}_j))$
is not too far away from the nonsymmetrized version,
$(\nabla {\mathbf v}_i, \eta \nabla {\mathbf v}_j)
= \sum_{k,l=1}^d
  (\partial_k ({\mathbf v}_i)_l, \eta \partial_k ({\mathbf v}_j)_l)
$ (note that the factor 2 has disappeared in this form). The latter,
however, has the advantage that the <code>dim</code> vector components
of the test functions are not coupled (well, almost, see below),
i.e., the resulting matrix is block-diagonal: one block for each vector
component, and each of these blocks is equal to the Laplace matrix for
this vector component. So assuming we order degrees of freedom in such
a way that first all $x$-components of the velocity are numbered, then
the $y$-components, and then the $z$-components, then the matrix
$\hat A$ that is associated with this slightly different bilinear form has
the form
@f{eqnarray*}
  \hat A =
  \left(\begin{array}{ccc}
    A_s & 0 & 0 \\ 0 & A_s & 0 \\ 0 & 0 & A_s
  \end{array}\right)
@f}
where $A_s$ is a Laplace matrix of size equal to the number of shape functions
associated with each component of the vector-valued velocity. With this
matrix, one could be tempted to define our preconditioner for the
velocity matrix $A$ as follows:
@f{eqnarray*}
  \tilde A^{-1} =
  \left(\begin{array}{ccc}
    \tilde A_s^{-1} & 0 & 0 \\
    0 & \tilde A_s^{-1} & 0 \\
    0 & 0 & \tilde A_s^{-1}
  \end{array}\right),
@f}
where $\tilde A_s^{-1}$ is a preconditioner for the Laplace matrix &mdash;
something where we know very well how to build good preconditioners!

In reality, the story is not quite as simple: To make the matrix
$\tilde A$ definite, we need to make the individual blocks $\tilde
A_s$ definite by applying boundary conditions. One can try to do so by
applying Dirichlet boundary conditions all around the boundary, and
then the so-defined preconditioner $\tilde A^{-1}$ turns out to be a
good preconditioner for $A$ if the latter matrix results from a Stokes
problem where we also have Dirichlet boundary conditions on the
velocity components all around the domain, i.e., if we enforce $\mathbf{u} =
0$.

Unfortunately, this "if" is an "if and only if": in the program below
we will want to use no-flux boundary conditions of the form $\mathbf u
\cdot \mathbf n = 0$ (i.e., flow %parallel to the boundary is allowed,
but no flux through the boundary). In this case, it turns out that the
block diagonal matrix defined above is not a good preconditioner
because it neglects the coupling of components at the boundary. A
better way to do things is therefore if we build the matrix $\hat A$
as the vector Laplace matrix $\hat A_{ij} = (\nabla {\mathbf v}_i,
\eta \nabla {\mathbf v}_j)$ and then apply the same boundary condition
as we applied to $A$. If this is a Dirichlet boundary condition all
around the domain, the $\hat A$ will decouple to three diagonal blocks
as above, and if the boundary conditions are of the form $\mathbf u
\cdot \mathbf n = 0$ then this will introduce a coupling of degrees of
freedom at the boundary but only there. This, in fact, turns out to be
a much better preconditioner than the one introduced above, and has
almost all the benefits of what we hoped to get.


To sum this whole story up, we can observe:
<ul>
  <li> Compared to building a preconditioner from the original matrix $A$
  resulting from the symmetric gradient as we did in step-22,
  we have to expect that the preconditioner based on the Laplace bilinear form
  performs worse since it does not take into account the coupling between
  vector components.

  <li>On the other hand, preconditioners for the Laplace matrix are typically
  more mature and perform better than ones for vector problems. For example,
  at the time of this writing, Algebraic %Multigrid (AMG) algorithms are very
  well developed for scalar problems, but not so for vector problems.

  <li>In building this preconditioner, we will have to build up the
  matrix $\hat A$ and its preconditioner. While this means that we
  have to store an additional matrix we didn't need before, the
  preconditioner $\tilde A_s^{-1}$ is likely going to need much less
  memory than storing a preconditioner for the coupled matrix
  $A$. This is because the matrix $A_s$ has only a third of the
  entries per row for all rows corresponding to interior degrees of
  freedom, and contains coupling between vector components only on
  those parts of the boundary where the boundary conditions introduce
  such a coupling. Storing the matrix is therefore comparatively
  cheap, and we can expect that computing and storing the
  preconditioner $\tilde A_s$ will also be much cheaper compared to
  doing so for the fully coupled matrix.
</ul>



<h5>Linear solvers for the temperature equation</h5>

This is the easy part: The matrix for the temperature equation has the form
$\alpha M + \beta A$, where $M,A$ are mass and stiffness matrices on the
temperature space, and $\alpha,\beta$ are constants related the time stepping
scheme and the current and previous time step. This being the sum of a
symmetric positive definite and a symmetric positive semidefinite matrix, the
result is also symmetric positive definite. Furthermore, $\frac\beta\alpha$ is
a number proportional to the time step, and so becomes small whenever the mesh
is fine, damping the effect of the then ill-conditioned stiffness matrix.

As a consequence, inverting this matrix with the Conjugate Gradient algorithm,
using a simple preconditioner, is trivial and very cheap compared to inverting
the Stokes matrix.



<h3>Implementation details</h3>

<h4>Using different DoFHandler objects</h4>

One of the things worth explaining up front about the program below is the use
of two different DoFHandler objects. If one looks at the structure of the
equations above and the scheme for their solution, one realizes that there is
little commonality that keeps the Stokes part and the temperature part
together. In all previous tutorial programs in which we have discussed @ref
vector_valued "vector-valued problems" we have always only used a single
finite element with several vector components, and a single DoFHandler object.
Sometimes, we have substructured the resulting matrix into blocks to
facilitate particular solver schemes; this was, for example, the case in the
step-22 program for the Stokes equations upon which the current
program is based.

We could of course do the same here. The linear system that we would get would
look like this:
@f{eqnarray*}
  \left(\begin{array}{ccc}
    A & B^T & 0 \\ B & 0 &0 \\ C & 0 & K
  \end{array}\right)
  \left(\begin{array}{ccc}
    U^{n-1} \\ P^{n-1} \\ T^n
  \end{array}\right)
  =
  \left(\begin{array}{ccc}
    F_U(T^{n-1}) \\ 0 \\ F_T(U^{n-1},U^{n-2},T^{n-1},T^{n-2})
  \end{array}\right).
@f}
The problem with this is: We never use the whole matrix at the same time. In
fact, it never really exists at the same time: As explained above, $K$ and
$F_T$ depend on the already computed solution $U^n$, in the first case through
the time step (that depends on $U^n$ because it has to satisfy a CFL
condition). So we can only assemble it once we've already solved the top left
$2\times 2$ block Stokes system, and once we've moved on to the temperature
equation we don't need the Stokes part any more; the fact that we
build an object for a matrix that never exists as a whole in memory at
any given time led us to jumping through some hoops in step-21, so
let's not repeat this sort of error. Furthermore, we don't
actually build the matrix $C$: Because by the time we get to the temperature
equation we already know $U^n$, and because we have to assemble the right hand
side $F_T$ at this time anyway, we simply move the term $CU^n$ to the right
hand side and assemble it along with all the other terms there. What this
means is that there does not remain a part of the matrix where temperature
variables and Stokes variables couple, and so a global enumeration of all
degrees of freedom is no longer important: It is enough if we have an
enumeration of all Stokes degrees of freedom, and of all temperature degrees
of freedom independently.

In essence, there is consequently not much use in putting <i>everything</i>
into a block matrix (though there are of course the same good reasons to do so
for the $2\times 2$ Stokes part), or, for that matter, in putting everything
into the same DoFHandler object.

But are there <i>downsides</i> to doing so? These exist, though they may not
be obvious at first. The main problem is that if we need to create one global
finite element that contains velocity, pressure, and temperature shape
functions, and use this to initialize the DoFHandler. But we also use this
finite element object to initialize all FEValues or FEFaceValues objects that
we use. This may not appear to be that big a deal, but imagine what happens
when, for example, we evaluate the residual
$
  R_\alpha(T)
  =
  \left(
  \frac{\partial T}{\partial t}
  +
  {\mathbf u} \cdot \nabla T
  -
  \nabla \cdot \kappa \nabla T - \gamma
  \right)
  T^{\alpha-1}
$
that we need to compute the artificial viscosity $\nu_\alpha(T)|_K$.  For
this, we need the Laplacian of the temperature, which we compute using the
tensor of second derivatives (Hessians) of the shape functions (we have to
give the <code>update_hessians</code> flag to the FEValues object for
this). Now, if we have a finite that contains the shape functions for
velocities, pressures, and temperatures, that means that we have to compute
the Hessians of <i>all</i> shape functions, including the many higher order
shape functions for the velocities. That's a lot of computations that we don't
need, and indeed if one were to do that (as we had in an early version of the
program), assembling the right hand side took about a quarter of the overall
compute time.

So what we will do is to use two different finite element objects, one for the
Stokes components and one for the temperatures. With this come two different
DoFHandlers, two sparsity patterns and two matrices for the Stokes and
temperature parts, etc. And whenever we have to assemble something that
contains both temperature and Stokes shape functions (in particular the right
hand sides of Stokes and temperature equations), then we use two FEValues
objects initialized with two cell iterators that we walk in %parallel through
the two DoFHandler objects associated with the same Triangulation object; for
these two FEValues objects, we use of course the same quadrature objects so
that we can iterate over the same set of quadrature points, but each FEValues
object will get update flags only according to what it actually needs to
compute. In particular, when we compute the residual as above, we only ask for
the values of the Stokes shape functions, but also the Hessians of the
temperature shape functions &mdash; much cheaper indeed, and as it turns out:
assembling the right hand side of the temperature equation is now a component
of the program that is hardly measurable.

With these changes, timing the program yields that only the following
operations are relevant for the overall run time:
<ul>
  <li>Solving the Stokes system: 72% of the run time.
  <li>Assembling the Stokes preconditioner and computing the algebraic
      multigrid hierarchy using the Trilinos ML package: 11% of the
      run time.
  <li>The function <code>BoussinesqFlowProblem::setup_dofs</code>: 7%
      of overall run time.
  <li>Assembling the Stokes and temperature right hand side vectors as
      well as assembling the matrices: 7%.
</ul>
In essence this means that all bottlenecks apart from the algebraic
multigrid have been removed.



<h4>Using Trilinos</h4>

In much the same way as we used PETSc to support our linear algebra needs in
step-17 and step-18, we use interfaces to the <a
href="http://trilinos.org">Trilinos</a> library (see the
deal.II README file for installation instructions) in this program. Trilinos
is a very large collection of
everything that has to do with linear and nonlinear algebra, as well as all
sorts of tools around that (and looks like it will grow in many other
directions in the future as well).

The main reason for using Trilinos, similar to our exploring PETSc, is that it
is a very powerful library that provides a lot more tools than deal.II's own
linear algebra library. That includes, in particular, the ability to work in
%parallel on a cluster, using MPI, and a wider variety of preconditioners. In
the latter class, one of the most interesting capabilities is the existence of
the Trilinos ML package that implements an Algebraic Multigrid (AMG)
method. We will use this preconditioner to precondition the second order
operator part of the momentum equation. The ability to solve problems in
%parallel will be explored in step-32, using the same problem as
discussed here.

PETSc, which we have used in step-17 and step-18, is certainly a powerful
library, providing a large number of functions that deal with matrices,
vectors, and iterative solvers and preconditioners, along with lots of other
stuff, most of which runs quite well in %parallel. It is, however, a few years
old already than Trilinos, written in C, and generally not quite as easy to
use as some other libraries. As a consequence, deal.II has also acquired
interfaces to Trilinos, which shares a lot of the same functionality with
PETSc. It is, however, a project that is several years younger, is written in
C++ and by people who generally have put a significant emphasis on software
design.


<h3>The testcase</h3>

The case we want to solve here is as follows: we solve the Boussinesq
equations described above with $\kappa=10^{-6}, \eta=1, \rho=1, \beta=10$,
i.e., a relatively slow moving fluid that has virtually no thermal diffusive
conductivity and transports heat mainly through convection. On the
boundary, we will require no-normal flux for the velocity
($\mathrm{n}\cdot\mathrm{u}=0$) and for the temperature
($\mathrm{n}\cdot\nabla T=0$). This is one of the cases discussed in the
introduction of step-22 and fixes one component of the velocity
while allowing flow to be %parallel to the boundary. There remain
<code>dim-1</code> components to be fixed, namely the tangential components of
the normal stress; for these, we choose homogeneous conditions which means that
we do not have to anything special. Initial conditions are only necessary for
the temperature field, and we choose it to be constant zero.

The evolution of the problem is then entirely driven by the right hand side
$\gamma(\mathrm{x},t)$ of the temperature equation, i.e., by heat sources and
sinks. Here, we choose a setup invented in advance of a Christmas lecture:
real candles are of course prohibited in U.S. class rooms, but virtual ones
are allowed. We therefore choose three spherical heat sources unequally spaced
close to the bottom of the domain, imitating three candles. The fluid located
at these sources, initially at rest, is then heated up and as the temperature
rises gains buoyancy, rising up; more fluid is dragged up and through the
sources, leading to three hot plumes that rise up until they are captured by
the recirculation of fluid that sinks down on the outside, replacing the air
that rises due to heating.


examples/step-31/doc/results.dox
<h1>Results</h1>

<h3> Results in 2d </h3>

When you run the program in 2d, the output will look something like
this:
<code>
<pre>
Number of active cells: 256 (on 5 levels)
Number of degrees of freedom: 3556 (2178+289+1089)

Timestep 0:  t=0
   Assembling...
   Rebuilding Stokes preconditioner...
   Solving...
   0 GMRES iterations for Stokes subsystem.
   Time step: 0.919118
   9 CG iterations for temperature.
   Temperature range: -0.16687 1.30011

Number of active cells: 280 (on 6 levels)
Number of degrees of freedom: 4062 (2490+327+1245)

Timestep 0:  t=0
   Assembling...
   Rebuilding Stokes preconditioner...
   Solving...
   0 GMRES iterations for Stokes subsystem.
   Time step: 0.459559
   9 CG iterations for temperature.
   Temperature range: -0.0982971 0.598503

Number of active cells: 520 (on 7 levels)
Number of degrees of freedom: 7432 (4562+589+2281)

Timestep 0:  t=0
   Assembling...
   Rebuilding Stokes preconditioner...
   Solving...
   0 GMRES iterations for Stokes subsystem.
   Time step: 0.229779
   9 CG iterations for temperature.
   Temperature range: -0.0551098 0.294493

Number of active cells: 1072 (on 8 levels)
Number of degrees of freedom: 15294 (9398+1197+4699)

Timestep 0:  t=0
   Assembling...
   Rebuilding Stokes preconditioner...
   Solving...
   0 GMRES iterations for Stokes subsystem.
   Time step: 0.11489
   9 CG iterations for temperature.
   Temperature range: -0.0273524 0.156861

Number of active cells: 2116 (on 9 levels)
Number of degrees of freedom: 30114 (18518+2337+9259)

Timestep 0:  t=0
   Assembling...
   Rebuilding Stokes preconditioner...
   Solving...
   0 GMRES iterations for Stokes subsystem.
   Time step: 0.0574449
   9 CG iterations for temperature.
   Temperature range: -0.014993 0.0738328

Timestep 1:  t=0.0574449
   Assembling...
   Solving...
   56 GMRES iterations for Stokes subsystem.
   Time step: 0.0574449
   9 CG iterations for temperature.
   Temperature range: -0.0273934 0.14488

...
</pre>
</code>

In the beginning we refine the mesh several times adaptively and
always return to time step zero to restart on the newly refined
mesh. Only then do we start the actual time iteration.

The program runs for a while. The temperature field for time steps 0,
500, 1000, 1500, 2000, 3000, 4000, and 5000 looks like this (note that
the color scale used for the temperature is not always the same):

<table align="center" class="doxtable">
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.solution.00.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.solution.01.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.solution.02.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.solution.03.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.solution.04.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.solution.05.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.solution.06.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.solution.07.png" alt="">
    </td>
  </tr>
</table>

The visualizations shown here were generated using a version of the example
which did not enforce the constraints after transferring the mesh.

As can be seen, we have three heat sources that heat fluid and
therefore produce a buoyancy effect that lets hots pockets of fluid
rise up and swirl around. By a chimney effect, the three streams are
pressed together by fluid that comes from the outside and wants to
join the updraft party. Note that because the fluid is initially at
rest, those parts of the fluid that were initially over the sources
receive a longer heating time than that fluid that is later dragged
over the source by the fully developed flow field. It is therefore
hotter, a fact that can be seen in the red tips of the three
plumes. Note also the relatively fine features of the flow field, a
result of the sophisticated transport stabilization of the temperature
equation we have chosen.

In addition to the pictures above, the following ones show the
adaptive mesh and the flow field at the same time steps:

<table align="center" class="doxtable">
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.grid.00.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.grid.01.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.grid.02.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.grid.03.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.grid.04.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.grid.05.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.grid.06.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.2d.grid.07.png" alt="">
    </td>
  </tr>
</table>


<h3> Results in 3d </h3>

The same thing can of course be done in 3d by changing the template
parameter to the BoussinesqFlowProblem object in <code>main()</code>
from 2 to 3, so that the output now looks like follows:

<code>
<pre>
Number of active cells: 64 (on 3 levels)
Number of degrees of freedom: 3041 (2187+125+729)

Timestep 0:  t=0
   Assembling...
   Rebuilding Stokes preconditioner...
   Solving...
   0 GMRES iterations for Stokes subsystem.
   Time step: 2.45098
   9 CG iterations for temperature.
   Temperature range: -0.675683 4.94725

Number of active cells: 288 (on 4 levels)
Number of degrees of freedom: 12379 (8943+455+2981)

Timestep 0:  t=0
   Assembling...
   Rebuilding Stokes preconditioner...
   Solving...
   0 GMRES iterations for Stokes subsystem.
   Time step: 1.22549
   9 CG iterations for temperature.
   Temperature range: -0.527701 2.25764

Number of active cells: 1296 (on 5 levels)
Number of degrees of freedom: 51497 (37305+1757+12435)

Timestep 0:  t=0
   Assembling...
   Rebuilding Stokes preconditioner...
   Solving...
   0 GMRES iterations for Stokes subsystem.
   Time step: 0.612745
   10 CG iterations for temperature.
   Temperature range: -0.496942 0.847395

Number of active cells: 5048 (on 6 levels)
Number of degrees of freedom: 192425 (139569+6333+46523)

Timestep 0:  t=0
   Assembling...
   Rebuilding Stokes preconditioner...
   Solving...
   0 GMRES iterations for Stokes subsystem.
   Time step: 0.306373
   10 CG iterations for temperature.
   Temperature range: -0.267683 0.497739

Timestep 1:  t=0.306373
   Assembling...
   Solving...
   27 GMRES iterations for Stokes subsystem.
   Time step: 0.306373
   10 CG iterations for temperature.
   Temperature range: -0.461787 0.958679

...
</pre>
</code>

Visualizing the temperature isocontours at time steps 0,
50, 100, 150, 200, 300, 400, 500, 600, 700, and 800 yields the
following plots:

<table align="center" class="doxtable">
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.3d.solution.00.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.3d.solution.01.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.3d.solution.02.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.3d.solution.03.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.3d.solution.04.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.3d.solution.05.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.3d.solution.06.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.3d.solution.07.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.3d.solution.08.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.3d.solution.09.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.3d.solution.10.png" alt="">
    </td>
    <td>
    </td>
  </tr>
</table>

That the first picture looks like three hedgehogs stems from the fact that our
scheme essentially projects the source times the first time step size onto the
mesh to obtain the temperature field in the first time step. Since the source
function is discontinuous, we need to expect over- and undershoots from this
project. This is in fact what happens (it's easier to check this in 2d) and
leads to the crumpled appearance of the isosurfaces.  The visualizations shown
here were generated using a version of the example which did not enforce the
constraints after transferring the mesh.



<h3> Numerical experiments to determine optimal parameters </h3>

The program as is has three parameters that we don't have much of a
theoretical handle on how to choose in an optimal way. These are:
<ul>
  <li>The time step must satisfy a CFL condition
      $k\le \min_K \frac{c_kh_K}{\|\mathbf{u}\|_{L^\infty(K)}}$. Here, $c_k$ is
      dimensionless, but what is the right value?
  <li>In the computation of the artificial viscosity,
@f{eqnarray*}
  \nu_\alpha(T)|_K
  =
  \beta
  \|\mathbf{u}\|_{L^\infty(K)}
  \min\left\{
    h_K,
    h_K^\alpha
    \frac{\|R_\alpha(T)\|_{L^\infty(K)}}{c(\mathbf{u},T)}
  \right\},
@f}
      with $c(\mathbf{u},T) =
      c_R\ \|\mathbf{u}\|_{L^\infty(\Omega)} \ \mathrm{var}(T)
      \ |\mathrm{diam}(\Omega)|^{\alpha-2}$.
      Here, the choice of the dimensionless %numbers $\beta,c_R$ is of
      interest.
</ul>
In all of these cases, we will have to expect that the correct choice of each
value depends on that of the others, and most likely also on the space
dimension and polynomial degree of the finite element used for the
temperature. Below we'll discuss a few numerical experiments to choose
constants $c_k$ and $\beta$.

Below, we will not discuss the choice of $c_R$. In the program, we set
it to $c_R=2^{\frac{4-2\alpha}{d}}$. The reason for this value is a
bit complicated and has more to do with the history of the program
than reasoning: while the correct formula for the global scaling
parameter $c(\mathbf{u},T)$ is shown above, the program (including the
version shipped with deal.II 6.2) initially had a bug in that we
computed
$c(\mathbf{u},T) =
      \|\mathbf{u}\|_{L^\infty(\Omega)} \ \mathrm{var}(T)
      \ \frac{1}{|\mathrm{diam}(\Omega)|^{\alpha-2}}$ instead, where
we had set the scaling parameter to one. Since we only computed on the
unit square/cube where $\mathrm{diam}(\Omega)=2^{1/d}$, this was
entirely equivalent to using the correct formula with
$c_R=\left(2^{1/d}\right)^{4-2\alpha}=2^{\frac{4-2\alpha}{d}}$. Since
this value for $c_R$ appears to work just fine for the current
program, we corrected the formula in the program and set $c_R$ to a
value that reproduces exactly the results we had before. We will,
however, revisit this issue again in step-32.

Now, however, back to the discussion of what values of $c_k$ and
$\beta$ to choose:


<h4> Choosing <i>c<sub>k</sub></i> and beta </h4>

These two constants are definitely linked in some way. The reason is easy to
see: In the case of a pure advection problem,
$\frac{\partial T}{\partial t} + \mathbf{u}\cdot\nabla T = \gamma$, any
explicit scheme has to satisfy a CFL condition of the form
$k\le \min_K \frac{c_k^a h_K}{\|\mathbf{u}\|_{L^\infty(K)}}$. On the other hand,
for a pure diffusion problem,
$\frac{\partial T}{\partial t} + \nu \Delta T = \gamma$,
explicit schemes need to satisfy a condition
$k\le \min_K \frac{c_k^d h_K^2}{\nu}$. So given the form of $\nu$ above, an
advection diffusion problem like the one we have to solve here will result in
a condition of the form
$
k\le \min_K \min \left\{
  \frac{c_k^a h_K}{\|\mathbf{u}\|_{L^\infty(K)}},
  \frac{c_k^d h_K^2}{\beta \|\mathbf{u}\|_{L^\infty(K)} h_K}\right\}
  =
  \min_K \left( \min \left\{
  c_k^a,
  \frac{c_k^d}{\beta}\right\}
  \frac{h_K}{\|\mathbf{u}\|_{L^\infty(K)}} \right)
$.
It follows that we have to face the fact that we might want to choose $\beta$
larger to improve the stability of the numerical scheme (by increasing the
amount of artificial diffusion), but we have to pay a price in the form of
smaller, and consequently more time steps. In practice, one would therefore
like to choose $\beta$ as small as possible to keep the transport problem
sufficiently stabilized while at the same time trying to choose the time step
as large as possible to reduce the overall amount of work.

The find the right balance, the only way is to do a few computational
experiments. Here's what we did: We modified the program slightly to allow
less mesh refinement (so we don't always have to wait that long) and to choose
$
  \nu(T)|_K
  =
  \beta
  \|\mathbf{u}\|_{L^\infty(K)} h_K
$ to eliminate the effect of the constant $c_R$ (we know that
solutions are stable by using this version of $\nu(T)$ as an artificial
viscosity, but that we can improve things -- i.e. make the solution
sharper -- by using the more complicated formula for this artificial
viscosity). We then run the program
for different values $c_k,\beta$ and observe maximal and minimal temperatures
in the domain. What we expect to see is this: If we choose the time step too
big (i.e. choose a $c_k$ bigger than theoretically allowed) then we will get
exponential growth of the temperature. If we choose $\beta$ too small, then
the transport stabilization becomes insufficient and the solution will show
significant oscillations but not exponential growth.


<h5>Results for Q<sub>1</sub> elements</h5>

Here is what we get for
$\beta=0.01, \beta=0.1$, and $\beta=0.5$, different choices of $c_k$, and
bilinear elements (<code>temperature_degree=1</code>) in 2d:

<table align="center" class="doxtable">
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.timestep.q1.beta=0.01.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.timestep.q1.beta=0.03.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.timestep.q1.beta=0.1.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.timestep.q1.beta=0.5.png" alt="">
    </td>
  </tr>
</table>

The way to interpret these graphs goes like this: for $\beta=0.01$ and
$c_k=\frac 12,\frac 14$, we see exponential growth or at least large
variations, but if we choose
$k=\frac 18\frac{h_K}{\|\mathbf{u}\|_{L^\infty(K)}}$
or smaller, then the scheme is
stable though a bit wobbly. For more artificial diffusion, we can choose
$k=\frac 14\frac{h_K}{\|\mathbf{u}\|_{L^\infty(K)}}$
or smaller for $\beta=0.03$,
$k=\frac 13\frac{h_K}{\|\mathbf{u}\|_{L^\infty(K)}}$
or smaller for $\beta=0.1$, and again need
$k=\frac 1{15}\frac{h_K}{\|\mathbf{u}\|_{L^\infty(K)}}$
for $\beta=0.5$ (this time because much diffusion requires a small time
step).

So how to choose? If we were simply interested in a large time step, then we
would go with $\beta=0.1$ and
$k=\frac 13\frac{h_K}{\|\mathbf{u}\|_{L^\infty(K)}}$.
On the other hand, we're also interested in accuracy and here it may be of
interest to actually investigate what these curves show. To this end note that
we start with a zero temperature and that our sources are positive &mdash; so
we would intuitively expect that the temperature can never drop below
zero. But it does, a consequence of Gibb's phenomenon when using continuous
elements to approximate a discontinuous solution. We can therefore see that
choosing $\beta$ too small is bad: too little artificial diffusion leads to
over- and undershoots that aren't diffused away. On the other hand, for large
$\beta$, the minimum temperature drops below zero at the beginning but then
quickly diffuses back to zero.

On the other hand, let's also look at the maximum temperature. Watching the
movie of the solution, we see that initially the fluid is at rest. The source
keeps heating the same volume of fluid whose temperature increases linearly at
the beginning until its buoyancy is able to move it upwards. The hottest part
of the fluid is therefore transported away from the solution and fluid taking
its place is heated for only a short time before being moved out of the source
region, therefore remaining cooler than the initial bubble. If $\kappa=0$
(in the program it is nonzero but very small) then the hottest part of the
fluid should be advected along with the flow with its temperature
constant. That's what we can see in the graphs with the smallest $\beta$: Once
the maximum temperature is reached, it hardly changes any more. On the other
hand, the larger the artificial diffusion, the more the hot spot is
diffused. Note that for this criterion, the time step size does not play a
significant role.

So to sum up, likely the best choice would appear to be $\beta=0.03$
and $k=\frac 14\frac{h_K}{\|\mathbf{u}\|_{L^\infty(K)}}$. The curve is
a bit wobbly, but overall pictures looks pretty reasonable with the
exception of some over and undershoots close to the start time due to
Gibb's phenomenon.


<h5>Results for Q<sub>2</sub> elements</h5>

One can repeat the same sequence of experiments for higher order
elements as well. Here are the graphs for bi-quadratic shape functions
(<code>temperature_degree=2</code>) for the temperature, while we
retain the $Q_2/Q_1$ stable Taylor-Hood element for the Stokes system:

<table align="center" class="doxtable">
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.timestep.q2.beta=0.01.png" alt="">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.timestep.q2.beta=0.03.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-31.timestep.q2.beta=0.1.png" alt="">
    </td>
  </tr>
</table>

Again, small values of $\beta$ lead to less diffusion but we have to
choose the time step very small to keep things under control. Too
large values of $\beta$ make for more diffusion, but again require
small time steps. The best value would appear to be $\beta=0.03$, as
for the $Q_1$ element, and then we have to choose
$k=\frac 18\frac{h_K}{\|\mathbf{u}\|_{L^\infty(K)}}$ &mdash; exactly
half the size for the $Q_1$ element, a fact that may not be surprising
if we state the CFL condition as the requirement that the time step be
small enough so that the distance transport advects in each time step
is no longer than one <i>grid point</i> away (which for $Q_1$ elements
is $h_K$, but for $Q_2$ elements is $h_K/2$). It turns out that $\beta$
needs to be slightly larger for obtaining stable results also late in
the simulation at times larger than 60, so we actually choose it as
$\beta = 0.034$ in the code.


<h5>Results for 3d</h5>

One can repeat these experiments in 3d and find the optimal time step
for each value of $\beta$ and find the best value of $\beta$. What one
finds is that for the same $\beta$ already used in 2d, the time steps
needs to be a bit smaller, by around a factor of 1.2 or so. This is
easily explained: the time step restriction is
$k=\min_K \frac{ch_K}{\|\mathbf{u}\|_{L^\infty(K)}}$ where $h_K$ is
the <i>diameter</i> of the cell. However, what is really needed is the
distance between mesh points, which is $\frac{h_K}{\sqrt{d}}$. So a
more appropriate form would be
$k=\min_K \frac{ch_K}{\|\mathbf{u}\|_{L^\infty(K)}\sqrt{d}}$.

The second find is that one needs to choose $\beta$ slightly bigger
(about $\beta=0.05$ or so). This then again reduces the time step we
can take.




<h5>Conclusions</h5>

Concluding, from the simple computations above, $\beta=0.034$ appears to be a
good choice for the stabilization parameter in 2d, and $\beta=0.05$ in 3d. In
a dimension independent way, we can model this as $\beta=0.017d$. If one does
longer computations (several thousand time steps) on finer meshes, one
realizes that the time step size is not quite small enough and that for
stability one will have to reduce the above values a bit more (by about a
factor of $\frac 78$).

As a consequence, a formula that reconciles 2d, 3d, and variable polynomial
degree and takes all factors in account reads as follows:
@f{eqnarray*}
  k =
  \frac 1{2 \cdot 1.7} \frac 1{\sqrt{d}}
  \frac 2d
  \frac 1{q_T}
  \frac{h_K}{\|\mathbf{u}\|_{L^\infty(K)}}
  =
  \frac 1{1.7 d\sqrt{d}}
  \frac 1{q_T}
  \frac{h_K}{\|\mathbf{u}\|_{L^\infty(K)}}.
@f}
In the first form (in the center of the equation), $\frac
1{2 \cdot 1.7}$ is a universal constant, $\frac 1{\sqrt{d}}$
is the factor that accounts for the difference between cell diameter
and grid point separation,
$\frac 2d$ accounts for the increase in $\beta$ with space dimension,
$\frac 1{q_T}$ accounts for the distance between grid points for
higher order elements, and $\frac{h_K}{\|\mathbf{u}\|_{L^\infty(K)}}$
for the local speed of transport relative to the cell size. This is
the formula that we use in the program.

As for the question of whether to use $Q_1$ or $Q_2$ elements for the
temperature, the following considerations may be useful: First,
solving the temperature equation is hardly a factor in the overall
scheme since almost the entire compute time goes into solving the
Stokes system in each time step. Higher order elements for the
temperature equation are therefore not a significant drawback. On the
other hand, if one compares the size of the over- and undershoots the
solution produces due to the discontinuous source description, one
notices that for the choice of $\beta$ and $k$ as above, the $Q_1$
solution dips down to around $-0.47$, whereas the $Q_2$ solution only
goes to $-0.13$ (remember that the exact solution should never become
negative at all. This means that the $Q_2$ solution is significantly
more accurate; the program therefore uses these higher order elements,
despite the penalty we pay in terms of smaller time steps.


<h3> Possibilities for extensions </h3>

There are various ways to extend the current program. Of particular interest
is, of course, to make it faster and/or increase the resolution of the
program, in particular in 3d. This is the topic of the step-32
tutorial program which will implement strategies to solve this problem in
%parallel on a cluster. It is also the basis of the much larger open
source code ASPECT (see https://aspect.geodynamics.org/ ) that can solve realistic
problems and that constitutes the further development of step-32.

Another direction would be to make the fluid flow more realistic. The program
was initially written to simulate various cases simulating the convection of
material in the earth's mantle, i.e. the zone between the outer earth core and
the solid earth crust: there, material is heated from below and cooled from
above, leading to thermal convection. The physics of this fluid are much more
complicated than shown in this program, however: The viscosity of mantle
material is strongly dependent on the temperature, i.e. $\eta=\eta(T)$, with
the dependency frequently modeled as a viscosity that is reduced exponentially
with rising temperature. Secondly, much of the dynamics of the mantle is
determined by chemical reactions, primarily phase changes of the various
crystals that make up the mantle; the buoyancy term on the right hand side of
the Stokes equations then depends not only on the temperature, but also on the
chemical composition at a given location which is advected by the flow field
but also changes as a function of pressure and temperature. We will
investigate some of these effects in later tutorial programs as well.


examples/step-32/doc/intro.dox
<br>

<i>This program was contributed by Martin Kronbichler, Wolfgang
Bangerth, and Timo Heister.

This material is based upon work partly supported by the National
Science Foundation under Award No. EAR-0426271 and The California Institute of
Technology; and in a continuation by the National Science
Foundation under Award No. EAR-0949446 and The University of California
&ndash; Davis. Any opinions, findings, and conclusions or recommendations
expressed in this publication are those of the author and do not
necessarily reflect the views of the National Science Foundation, The
California Institute of Technology, or of The University of California
&ndash; Davis.

The work discussed here is also presented in the following publication:
<b>
  M. Kronbichler, T. Heister, W. Bangerth:
  <i>High Accuracy Mantle Convection Simulation through Modern Numerical
  Methods</i>, Geophysical Journal International, 2012, 191, 12-29.
  <a href="http://dx.doi.org/10.1111/j.1365-246X.2012.05609.x">[DOI]</a>
</b>

The continuation of development of this program has led to the much larger open
source code <i>ASPECT</i> (see http://aspect.geodynamics.org/) which is much
more flexible in solving many kinds of related problems.
</i>


<a name="Intro"></a>
<h1>Introduction</h1>

This program does pretty much exactly what step-31 already does: it
solves the Boussinesq equations that describe the motion of a fluid
whose temperature is not in equilibrium. As such, all the equations we
have described in step-31 still hold: we solve the same general
partial differential equation (with only minor modifications to adjust
for more realism in the problem setting), using the same finite
element scheme, the same time stepping algorithm, and more or less the
same stabilization method for the temperature advection-diffusion
equation. As a consequence, you may first want to understand that
program &mdash; and its implementation &mdash; before you work on the
current one.

The difference between step-31 and the current program is that
here we want to do things in %parallel, using both the availability of many
machines in a cluster (with parallelization based on MPI) as well as many
processor cores within a single machine (with parallelization based on
threads). This program's main job is therefore to introduce the changes that are
necessary to utilize the availability of these %parallel compute
resources. In this regard, it builds on the step-40 program that first
introduces the necessary classes for much of the %parallel
functionality, and on step-55 that shows how this is done for a
vector-valued problem.

In addition to these changes, we also use a slightly different
preconditioner, and we will have to make a number of changes that have
to do with the fact that we want to solve a <i>realistic</i> problem
here, not a model problem. The latter, in particular, will require
that we think about scaling issues as well as what all those
parameters and coefficients in the equations under consideration
actually mean. We will discuss first the issues that affect changes in
the mathematical formulation and solver structure, then how to
parallelize things, and finally the actual testcase we will consider.


<h3> Using the "right" pressure </h3>

In step-31, we used the following Stokes model for the
velocity and pressure field:
@f{eqnarray*}
  -\nabla \cdot (2 \eta \varepsilon ({\mathbf u})) + \nabla p &=&
  -\rho \; \beta \; T \mathbf{g},
  \\
  \nabla \cdot {\mathbf u} &=& 0.
@f}
The right hand side of the first equation appears a wee bit
unmotivated. Here's how things should really be. We
need the external forces that act on the fluid, which we assume are
given by gravity only. In the current case, we assume that the fluid
does expand slightly for the purposes of this gravity force, but not
enough that we need to modify the incompressibility condition (the
second equation). What this means is that for the purpose of the right
hand side, we can assume that $\rho=\rho(T)$. An assumption that may
not be entirely justified is that we can assume that the changes of
density as a function of temperature are small, leading to an
expression of the form $\rho(T) = \rho_{\text{ref}}
[1-\beta(T-T_{\text{ref}})]$, i.e., the density equals
$\rho_{\text{ref}}$ at reference temperature and decreases linearly as
the temperature increases (as the material expands). The force balance
equation then looks properly written like this:
@f{eqnarray*}
  -\nabla \cdot (2 \eta \varepsilon ({\mathbf u})) + \nabla p &=&
  \rho_{\text{ref}} [1-\beta(T-T_{\text{ref}})] \mathbf{g}.
@f}
Now note that the gravity force results from a gravity potential as
$\mathbf g=-\nabla \varphi$, so that we can re-write this as follows:
@f{eqnarray*}
  -\nabla \cdot (2 \eta \varepsilon ({\mathbf u})) + \nabla p &=&
  -\rho_{\text{ref}} \; \beta\; T\; \mathbf{g}
  -\rho_{\text{ref}} [1+\beta T_{\text{ref}}] \nabla\varphi.
@f}
The second term on the right is time independent, and so we could
introduce a new "dynamic" pressure $p_{\text{dyn}}=p+\rho_{\text{ref}}
[1+\beta T_{\text{ref}}] \varphi=p_{\text{total}}-p_{\text{static}}$
with which the Stokes equations would read:
@f{eqnarray*}
  -\nabla \cdot (2 \eta \varepsilon ({\mathbf u})) + \nabla p_{\text{dyn}} &=&
  -\rho_{\text{ref}} \; \beta \; T \; \mathbf{g},
  \\
  \nabla \cdot {\mathbf u} &=& 0.
@f}
This is exactly the form we used in step-31, and it was
appropriate to do so because all changes in the fluid flow are only
driven by the dynamic pressure that results from temperature
differences. (In other words: Any contribution to the right hand side
that results from taking the gradient of a scalar field have no effect
on the velocity field.)

On the other hand, we will here use the form of the Stokes equations
that considers the total pressure instead:
@f{eqnarray*}
  -\nabla \cdot (2 \eta \varepsilon ({\mathbf u})) + \nabla p &=&
  \rho(T)\; \mathbf{g},
  \\
  \nabla \cdot {\mathbf u} &=& 0.
@f}
There are several advantages to this:

- This way we can plot the pressure in our program in such a way that it
  actually shows the total pressure that includes the effects of
  temperature differences as well as the static pressure of the
  overlying rocks. Since the pressure does not appear any further in any
  of the other equations, whether to use one or the other is more a
  matter of taste than of correctness. The flow field is exactly the
  same, but we get a pressure that we can now compare with values that
  are given in geophysical books as those that hold at the bottom of the
  earth mantle, for example.

- If we wanted to make the model even more realistic, we would have to take
  into account that many of the material parameters (e.g. the viscosity, the
  density, etc) not only depend on the temperature but also the
  <i>total</i> pressure.

- The model above assumed a linear dependence $\rho(T) = \rho_{\text{ref}}
  [1-\beta(T-T_{\text{ref}})]$ and assumed that $\beta$ is small. In
  practice, this may not be so. In fact, realistic models are
  certainly not linear, and $\beta$ may also not be small for at least
  part of the temperature range because the density's behavior is
  substantially dependent not only on thermal expansion but by phase
  changes.

- A final reason to do this is discussed in the results section and
  concerns possible extensions to the model we use here. It has to do
  with the fact that the temperature equation (see below) we use here does not
  include a term that contains the pressure. It should, however:
  rock, like gas, heats up as you compress it. Consequently,
  material that rises up cools adiabatically, and cold material that
  sinks down heats adiabatically. We discuss this further below.

@note There is, however, a downside to this procedure. In the earth,
the dynamic pressure is several orders of magnitude smaller than the
total pressure. If we use the equations above and solve all variables
to, say, 4 digits of accuracy, then we may be able to get the velocity
and the total pressure right, but we will have no accuracy at all if
we compute the dynamic pressure by subtracting from the total pressure
the static part $p_\text{static}=\rho_{\text{ref}}
[1+\beta T_{\text{ref}}] \varphi$. If, for example, the dynamic
pressure is six orders of magnitude smaller than the static pressure,
then we need to solve the overall pressure to at least seven digits of
accuracy to get anything remotely accurate. That said, in practice
this turns out not to be a limiting factor.



<h3> The scaling of discretized equations </h3>

Remember that we want to solve the following set of equations:
@f{eqnarray*}
  -\nabla \cdot (2 \eta \varepsilon ({\mathbf u})) + \nabla p &=&
  \rho(T) \mathbf{g},
  \\
  \nabla \cdot {\mathbf u} &=& 0,
  \\
  \frac{\partial T}{\partial t}
  +
  {\mathbf u} \cdot \nabla T
  -
  \nabla \cdot \kappa \nabla T &=& \gamma,
@f}
augmented by appropriate boundary and initial conditions. As discussed
in step-31, we will solve this set of equations by
solving for a Stokes problem first in each time step, and then moving
the temperature equation forward by one time interval.

The problem under consideration in this current section is with the
Stokes problem: if we discretize it as usual, we get a linear system
@f{eqnarray*}
  M \; X
  =
  \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
  \left(\begin{array}{c}
    U \\ P
  \end{array}\right)
  =
  \left(\begin{array}{c}
    F_U \\ 0
  \end{array}\right)
  =
  F
@f}
which in this program we will solve with a FGMRES solver. This solver
iterates until the residual of these linear equations is below a
certain tolerance, i.e., until
@f[
  \left\|
  \left(\begin{array}{c}
    F_U - A U^{(k)} - B P^{(k)}
    \\
    B^T U^{(k)}
  \end{array}\right)
  \right\|
  < \text{Tol}.
@f]
This does not make any sense from the viewpoint of physical units: the
quantities involved here have physical units so that the first part of
the residual has units $\frac{\text{Pa}}{\text{m}}
\text{m}^{\text{dim}}$ (most easily established by considering the
term $(\nabla \cdot \mathbf v, p)_{\Omega}$ and considering that the
pressure has units $\text{Pa}=\frac{\text{kg}}{\text{m}\;\text{s}^2}$ and
the integration yields a factor of $\text{m}^{\text{dim}}$), whereas
the second part of the residual has units
$\frac{\text{m}^{\text{dim}}}{\text{s}}$. Taking the norm
of this residual vector would yield a quantity with units
$\text{m}^{\text{dim}-1} \sqrt{\left(\text{Pa}\right)^2 +
       \left(\frac{\text{m}}{\text{s}}\right)^2}$. This,
quite obviously, does not make sense, and we should not be surprised
that doing so is eventually going to come back hurting us.

So why is this an issue here, but not in step-31? The
reason back there is that everything was nicely balanced: velocities
were on the order of one, the pressure likewise, the viscosity was
one, and the domain had a diameter of $\sqrt{2}$. As a result, while
nonsensical, nothing bad happened. On the other hand, as we will explain
below, things here will not be that simply scaled: $\eta$ will be around
$10^{21}$, velocities on the order of $10^{-8}$, pressure around $10^8$, and
the diameter of the domain is $10^7$. In other words, the order of magnitude
for the first equation is going to be
$\eta\text{div}\varepsilon(\mathbf u) \approx 10^{21} \frac{10^{-8}}{(10^7)^2}
\approx 10^{-1}$, whereas the second equation will be around
$\text{div}{\mathbf u}\approx \frac{10^{-8}}{10^7} \approx 10^{-15}$. Well, so
what this will lead to is this: if the solver wants to make the residual small,
it will almost entirely focus on the first set of equations because they are
so much bigger, and ignore the divergence equation that describes mass
conservation. That's exactly what happens: unless we set the tolerance to
extremely small values, the resulting flow field is definitely not divergence
free. As an auxiliary problem, it turns out that it is difficult to find a
tolerance that always works; in practice, one often ends up with a tolerance
that requires 30 or 40 iterations for most time steps, and 10,000 for some
others.

So what's a numerical analyst to do in a case like this? The answer is to
start at the root and first make sure that everything is mathematically
consistent first. In our case, this means that if we want to solve the system
of Stokes equations jointly, we have to scale them so that they all have the
same physical dimensions. In our case, this means multiplying the second
equation by something that has units $\frac{\text{Pa}\;\text{s}}{\text{m}}$; one
choice is to multiply with $\frac{\eta}{L}$ where $L$ is a typical lengthscale
in our domain (which experiments show is best chosen to be the diameter of
plumes &mdash; around 10 km &mdash; rather than the diameter of the
domain). Using these %numbers for $\eta$ and $L$, this factor is around
$10^{17}$. So, we now get this for the Stokes system:
@f{eqnarray*}
  -\nabla \cdot (2 \eta \varepsilon ({\mathbf u})) + \nabla p &=&
  \rho(T) \; \mathbf{g},
  \\
  \frac{\eta}{L} \nabla \cdot {\mathbf u} &=& 0.
@f}
The trouble with this is that the result is not symmetric any more (we have
$\frac{\eta}{L} \nabla \cdot$ at the bottom left, but not its transpose
operator at the top right). This, however, can be cured by introducing a
scaled pressure $\hat p = \frac{L}{\eta}p$, and we get the scaled equations
@f{eqnarray*}
  -\nabla \cdot (2 \eta \varepsilon ({\mathbf u})) +
  \nabla \left(\frac{\eta}{L} \hat p\right) &=&
  \rho(T) \; \mathbf{g},
  \\
  \frac{\eta}{L} \nabla \cdot {\mathbf u} &=& 0.
@f}
This is now symmetric. Obviously, we can easily recover the original pressure
$p$ from the scaled pressure $\hat p$ that we compute as a result of this
procedure.

In the program below, we will introduce a factor
<code>EquationData::pressure_scaling</code> that corresponds to
$\frac{\eta}{L}$, and we will use this factor in the assembly of the system
matrix and preconditioner. Because it is annoying and error prone, we will
recover the unscaled pressure immediately following the solution of the linear
system, i.e., the solution vector's pressure component will immediately be
unscaled to retrieve the physical pressure. Since the solver uses the fact that
we can use a good initial guess by extrapolating the previous solutions, we
also have to scale the pressure immediately <i>before</i> solving.



<h3> Changes to the Stokes preconditioner and solver </h3>

In this tutorial program, we apply a variant of the preconditioner used in
step-31. That preconditioner was built to operate on the
system matrix $M$ in block form such that the product matrix
@f{eqnarray*}
  P^{-1} M
  =
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ S^{-1} B A^{-1} & -S^{-1}
  \end{array}\right)
  \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
@f}
is of a form that Krylov-based iterative solvers like GMRES can solve in a
few iterations. We then replaced the exact inverse of $A$ by the action
of an AMG preconditioner $\tilde{A}$ based on a vector Laplace matrix,
approximated the Schur complement $S = B A^{-1} B^T$ by a mass matrix $M_p$
on the pressure space and wrote an <tt>InverseMatrix</tt> class for
implementing the action of $M_p^{-1}\approx S^{-1}$ on vectors. In the
InverseMatrix class, we used a CG solve with an incomplete Cholesky (IC)
preconditioner for performing the inner solves.

An observation one can make is that we use just the action of a
preconditioner for approximating the velocity inverse $A^{-1}$ (and the
outer GMRES iteration takes care of the approximate character of the
inverse), whereas we use a more or less <i>exact</i> inverse for $M_p^{-1}$,
realized by a fully converged CG solve. This appears unbalanced, but there's
system to this madness: almost all the effort goes into the upper left block
to which we apply the AMG preconditioner, whereas even an exact inversion of
the pressure mass matrix costs basically nothing. Consequently, if it helps us
reduce the overall number of iterations somewhat, then this effort is well
spent.

That said, even though the solver worked well for step-31, we have a problem
here that is a bit more complicated (cells are deformed, the pressure varies
by orders of magnitude, and we want to plan ahead for more complicated
physics), and so we'll change a few things slightly:

- For more complex problems, it turns out that using just a single AMG V-cycle
  as preconditioner is not always sufficient. The outer solver converges just
  fine most of the time in a reasonable number of iterations (say, less than
  50) but there are the occasional time step where it suddenly takes 700 or
  so. What exactly is going on there is hard to determine, but the problem can
  be avoided by using a more accurate solver for the top left
  block. Consequently, we'll want to use a CG iteration to invert the top left
  block of the preconditioner matrix, and use the AMG as a preconditioner for
  the CG solver.

- The downside of this is that, of course, the Stokes preconditioner becomes
  much more expensive (approximately 10 times more expensive than when we just
  use a single V-cycle). Our strategy then is this: let's do up to 30 GMRES
  iterations with just the V-cycle as a preconditioner and if that doesn't
  yield convergence, then take the best approximation of the Stokes solution
  obtained after this first round of iterations and use that as the starting
  guess for iterations where we use the full inner solver with a rather
  lenient tolerance as preconditioner. In all our experiments this leads to
  convergence in only a few additional iterations.

- One thing we need to pay attention to is that when using a CG with a lenient
  tolerance in the preconditioner, then $y = \tilde A^{-1} r$ is no longer a
  linear function of $r$ (it is, of course, if we have a very stringent
  tolerance in our solver, or if we only apply a single V-cycle). This is a
  problem since now our preconditioner is no longer a linear operator; in
  other words, every time GMRES uses it the preconditioner looks
  different. The standard GMRES solver can't deal with this, leading to slow
  convergence or even breakdown, but the F-GMRES variant is designed to deal
  with exactly this kind of situation and we consequently use it.

- On the other hand, once we have settled on using F-GMRES we can relax the
  tolerance used in inverting the preconditioner for $S$. In step-31, we ran a
  preconditioned CG method on $\tilde S$ until the residual had been reduced
  by 7 orders of magnitude. Here, we can again be more lenient because we know
  that the outer preconditioner doesn't suffer.

- In step-31, we used a left preconditioner in which we first invert the top
  left block of the preconditioner matrix, then apply the bottom left
  (divergence) one, and then invert the bottom right. In other words, the
  application of the preconditioner acts as a lower left block triangular
  matrix. Another option is to use a right preconditioner that here would be
  upper right block triangulation, i.e., we first invert the bottom right
  Schur complement, apply the top right (gradient) operator and then invert
  the elliptic top left block. To a degree, which one to choose is a matter of
  taste. That said, there is one significant advantage to a right
  preconditioner in GMRES-type solvers: the residual with which we determine
  whether we should stop the iteration is the true residual, not the norm of
  the preconditioned equations. Consequently, it is much simpler to compare it
  to the stopping criterion we typically use, namely the norm of the right
  hand side vector. In writing this code we found that the scaling issues we
  discussed above also made it difficult to determine suitable stopping
  criteria for left-preconditioned linear systems, and consequently this
  program uses a right preconditioner.

- In step-31, we used an IC (incomplete Cholesky) preconditioner for the
  pressure mass matrix in the Schur complement preconditioner and for the
  solution of the temperature system. Here, we could in principle do the same,
  but we do choose an even simpler preconditioner, namely a Jacobi
  preconditioner for both systems. This is because here we target at massively
  %parallel computations, where the decompositions for IC/ILU would have to be
  performed block-wise for the locally owned degrees of freedom on each
  processor. This means, that the preconditioner gets more like a Jacobi
  preconditioner anyway, so we rather start from that variant straight
  away. Note that we only use the Jacobi preconditioners for CG solvers with
  mass matrices, where they give optimal (<i>h</i>-independent) convergence
  anyway, even though they usually require about twice as many iterations as
  an IC preconditioner.

As a final note, let us remark that in step-31 we computed the
Schur complement $S=B A^{-1} B^T$ by approximating
$-\text{div}(-\eta\Delta)^{-1}\nabla \approx \frac 1{\eta} \mathbf{1}$. Now,
however, we have re-scaled the $B$ and $B^T$ operators. So $S$ should now
approximate
$-\frac{\eta}{L}\text{div}(-\eta\Delta)^{-1}\nabla \frac{\eta}{L} \approx
\left(\frac{\eta}{L}\right)^2 \frac 1{\eta} \mathbf{1}$.
We use the discrete form of the right hand side of this as our approximation
$\tilde S$ to $S$.


<h3> Changes to the artificial viscosity stabilization </h3>

Similarly to step-31, we will use an artificial viscosity for stabilization
based on a residual of the equation.  As a difference to step-31, we will
provide two slightly different definitions of the stabilization parameter. For
$\alpha=1$, we use the same definition as in step-31:
@f{eqnarray*}
  \nu_\alpha(T)|_K
  =
  \nu_1(T)|_K
  =
  \beta
  \|\mathbf{u}\|_{L^\infty(K)}
  h_K
  \min\left\{
    1,
    \frac{\|R_1(T)\|_{L^\infty(K)}}{c(\mathbf{u},T)}
  \right\}
@f}
where we compute the viscosity from a residual $\|R_1(T)\|_{L^\infty(K)}$ of
the equation, limited by a diffusion proportional to the mesh size $h_K$ in
regions where the residual is large (around steep gradients). This definition
has been shown to work well for the given case, $\alpha = 1$ in step-31, but
it is usually less effective as the diffusion for $\alpha=2$. For that case, we
choose a slightly more readable definition of the viscosity,
@f{eqnarray*}
  \nu_2(T)|_K = \min (\nu_h^\mathrm{max}|_K,\nu_h^\mathrm{E}|_K)
@f}
where the first term gives again the maximum dissipation (similarly to a first
order upwind scheme),
@f{eqnarray*}
  \nu^\mathrm{max}_h|_K = \beta h_K \|\mathbf {u}\|_{L^\infty(K)}
@f}
and the entropy viscosity is defined as
@f{eqnarray*}
  \nu^\mathrm{E}_h|_K = c_R \frac{h_K^2 \|R_\mathrm{2,E}(T)\|_{L^\infty(K)}}
  {\|E(T) - \bar{E}(T)\|_{L^\infty(\Omega)} }.
@f}

This formula is described in the article <i>J.-L. Guermond, R. Pasquetti, \&
B. Popov, 2011.  Entropy viscosity method for nonlinear conservation laws, J.
Comput. Phys., 230, 4248--4267.</i> Compared to the case $\alpha = 1$, the
residual is computed from the temperature entropy, $E(T) = \frac12 (T-T_m)^2$
with $T_m$ an average temperature (we choose the mean between the maximum and
minimum temperature in the computation), which gives the following formula
@f{eqnarray*}
 R_\mathrm{E}(T) = \frac{\partial E(T)}{\partial t} +
    (T-T_\mathrm{m}) \left(\mathbf{u} \cdot \nabla T -  \kappa \nabla^2 T - \gamma\right).
@f}
The denominator in the formula for $\nu^\mathrm{E}_h|_K$ is computed as the
global deviation of the entropy from the space-averaged entropy $\bar{E}(T) =
\int_\Omega E(T) d\mathbf{x}/\int_\Omega d\mathbf{x}$. As in step-31, we
evaluate the artificial viscosity from the temperature and velocity at two
previous time levels, in order to avoid a nonlinearity in its definition.

The above definitions of the viscosity are simple, but depend on two
parameters, namely $\beta$ and $c_R$.  For the current program, we want to go
about this issue a bit more systematically for both parameters in the case
$\alpha =1$, using the same line of reasoning with which we chose two other
parameters in our discretization, $c_k$ and $\beta$, in the results section of
step-31. In particular, remember that we would like to make the artificial
viscosity as small as possible while keeping it as large as necessary. In the
following, let us describe the general strategy one may follow. The
computations shown here were done with an earlier version of the program and
so the actual numerical values you get when running the program may no longer
match those shown here; that said, the general approach remains valid and has
been used to find the values of the parameters actually used in the program.

To see what is happening, note that below we will impose
boundary conditions for the temperature between 973 and 4273 Kelvin,
and initial conditions are also chosen in this range; for these
considerations, we run the program without %internal heat sources or sinks,
and consequently the temperature should
always be in this range, barring any %internal
oscillations. If the minimal temperature drops below 973 Kelvin, then
we need to add stabilization by either increasing $\beta$ or
decreasing $c_R$.

As we did in step-31, we first determine an optimal value of $\beta$
by using the "traditional" formula
@f{eqnarray*}
  \nu_\alpha(T)|_K
  =
  \beta
  \|\mathbf{u}\|_{L^\infty(K)}
    h_K,
@f}
which we know to be stable if only $\beta$ is large enough. Doing a
couple hundred time steps (on a coarser mesh than the one shown in the
program, and with a different viscosity that affects transport
velocities and therefore time step sizes) in 2d will produce the
following graph:

<img src="https://www.dealii.org/images/steps/developer/step-32.beta.2d.png" alt="">

As can be seen, values $\beta \le 0.05$ are too small whereas
$\beta=0.052$ appears to work, at least to the time horizon shown
here. As a remark on the side, there are at least two questions one
may wonder here: First, what happens at the time when the solution
becomes unstable? Looking at the graphical output, we can see that
with the unreasonably coarse mesh chosen for these experiments, around
time $t=10^{15}$ seconds the plumes of hot material that have been
rising towards the cold outer boundary and have then spread sideways
are starting to get close to each other, squeezing out the cold
material in-between. This creates a layer of cells into which fluids
flows from two opposite sides and flows out toward a third, apparently
a scenario that then produce these instabilities without sufficient
stabilization. Second: In step-31, we used
$\beta=0.015\cdot\text{dim}$; why does this not work here? The answer
to this is not entirely clear -- stabilization parameters are
certainly known to depend on things like the shape of cells, for which
we had squares in step-31 but have trapezoids in the current
program. Whatever the exact cause, we at least have a value of
$\beta$, namely 0.052 for 2d, that works for the current program.
A similar set of experiments can be made in 3d where we find that
$\beta=0.078$ is a good choice &mdash; neatly leading to the formula
$\beta=0.026 \cdot \textrm{dim}$.

With this value fixed, we can go back to the original formula for the
viscosity $\nu$ and play with the constant $c_R$, making it as large
as possible in order to make $\nu$ as small as possible. This gives us
a picture like this:

<img src="https://www.dealii.org/images/steps/developer/step-32.beta_cr.2d.png" alt="">

Consequently, $c_R=0.1$ would appear to be the right value here. While this
graph has been obtained for an exponent $\alpha=1$, in the program we use
$\alpha=2$ instead, and in that case one has to re-tune the parameter (and
observe that $c_R$ appears in the numerator and not in the denominator). It
turns out that $c_R=1$ works with $\alpha=2$.


<h3> Locally conservative Stokes discretization </h3>

The standard Taylor-Hood discretization for Stokes, using the $Q_{k+1}^d
\times Q_k$ element, is globally conservative, i.e. $\int_{\partial\Omega}
\mathbf n \cdot \mathbf u_h = 0$. This can easily be seen: the weak form of
the divergence equation reads $(q_h, \textrm{div}\; \mathbf u_h)=0, \forall
q_h\in Q_h$. Because the pressure space does contain the function $q_h=1$, we
get
@f{align*}
  0 = (1, \textrm{div}\; \mathbf u_h)_\Omega
  = \int_\Omega \textrm{div}\; \mathbf u_h
  = \int_{\partial\Omega} \mathbf n \cdot \mathbf u_h
@f}
by the divergence theorem. This property is important: if we want to use the
velocity field $u_h$ to transport along other quantities (such as the
temperature in the current equations, but it could also be concentrations of
chemical substances or entirely artificial tracer quantities) then the
conservation property guarantees that the amount of the quantity advected
remains constant.

That said, there are applications where this <i>global</i> property is not
enough. Rather, we would like that it holds <i>locally</i>, on every
cell. This can be achieved by using the space
$Q_{k+1}^d \times DGP_k$ for discretization, where we have replaced the
<i>continuous</i> space of tensor product polynomials of degree $k$ for the
pressure by the <i>discontinuous</i> space of the complete polynomials of the
same degree. (Note that tensor product polynomials in 2d contain the functions
$1, x, y, xy$, whereas the complete polynomials only have the functions $1,x,y$.)
This space turns out to be stable for the Stokes equation.

Because the space is discontinuous, we can now in particular choose the test
function $q_h(\mathbf x)=\chi_K(\mathbf x)$, i.e. the characteristic function
of cell $K$. We then get in a similar fashion as above
@f{align*}
  0
  = (q_h, \textrm{div}\; \mathbf u_h)_\Omega
  = (1, \textrm{div}\; \mathbf u_h)_K
  = \int_K \textrm{div}\; \mathbf u_h
  = \int_{\partial K} \mathbf n \cdot \mathbf u_h,
@f}
showing the conservation property for cell $K$. This clearly holds for each
cell individually.

There are good reasons to use this discretization. As mentioned above, this
element guarantees conservation of advected quantities on each cell
individually. A second advantage is that the pressure mass matrix we use as a
preconditioner in place of the Schur complement becomes block diagonal and
consequently very easy to invert. However, there are also downsides. For one,
there are now more pressure variables, increasing the overall size of the
problem, although this doesn't seem to cause much harm in practice. More
importantly, though, the fact that now the divergence integrated over each
cell is zero when it wasn't before does not guarantee that the divergence is
pointwise smaller. In fact, as one can easily verify, the $L_2$ norm of the
divergence is <i>larger</i> for this than for the standard Taylor-Hood
discretization. (However, both converge at the same rate to zero, since it is
easy to see that
$\|\textrm{div}\; u_h\|=
\|\textrm{div}\; (u-u_h)\|=
\|\textrm{trace}\; \nabla (u-u_h)\|\le
\|\nabla (u-u_h)\|={\cal O}(h^{k+2})$.) It is therefore not a priori clear
that the error is indeed smaller just because we now have more degrees of
freedom.

Given these considerations, it remains unclear which discretization one should
prefer. Consequently, we leave that up to the user and make it a parameter in
the input file which one to use.


<h3> Higher order mappings for curved boundaries </h3>

In the program, we will use a spherical shell as domain. This means
that the inner and outer boundary of the domain are no longer
"straight" (by which we usually mean that they are bilinear surfaces
that can be represented by the FlatManifold class). Rather, they
are curved and it seems prudent to use a curved approximation in the
program if we are already using higher order finite elements for the
velocity. Consequently, we will introduce a member variable of type
MappingQ that
denotes such a mapping (step-10 and step-11 introduce such mappings
for the first time) and that we will use in all computations on cells
that are adjacent to the boundary. Since this only affects a
relatively small fraction of cells, the additional effort is not very
large and we will take the luxury of using a quartic mapping for these
cells.


<h3> Parallelization on clusters </h3>

Running convection codes in 3d with significant Rayleigh numbers requires a lot
of computations &mdash; in the case of whole earth simulations on the order of
one or several hundred million unknowns. This can obviously not be done with a
single machine any more (at least not in 2010 when we started writing this
code). Consequently, we need to parallelize it.
Parallelization of scientific codes across multiple machines in a cluster of
computers is almost always done using the Message Passing Interface
(MPI). This program is no exception to that, and it follows the general spirit
of the step-17 and step-18 programs in this though in practice it borrows more
from step-40 in which we first introduced the classes and strategies we use
when we want to <i>completely</i> distribute all computations, and
step-55 that shows how to do that for
@ref vector_valued "vector-valued problems": including, for
example, splitting the mesh up into a number of parts so that each processor
only stores its own share plus some ghost cells, and using strategies where no
processor potentially has enough memory to hold the entries of the combined
solution vector locally. The goal is to run this code on hundreds or maybe
even thousands of processors, at reasonable scalability.

@note Even though it has a larger number, step-40 comes logically before the
current program. The same is true for step-55. You will probably want
to look at these programs before you try to understand what we do here.

MPI is a rather awkward interface to program with. It is a semi-object
oriented set of functions, and while one uses it to send data around a
network, one needs to explicitly describe the data types because the MPI
functions insist on getting the address of the data as <code>void*</code>
objects rather than deducing the data type automatically through overloading
or templates. We've already seen in step-17 and step-18 how to avoid almost
all of MPI by putting all the communication necessary into either the deal.II
library or, in those programs, into PETSc. We'll do something similar here:
like in step-40 and step-55, deal.II and the underlying p4est library are responsible for
all the communication necessary for distributing the mesh, and we will let the
Trilinos library (along with the wrappers in namespace TrilinosWrappers) deal
with parallelizing the linear algebra components. We have already used
Trilinos in step-31, and will do so again here, with the difference that we
will use its %parallel capabilities.

Trilinos consists of a significant number of packages, implementing basic
%parallel linear algebra operations (the Epetra package), different solver and
preconditioner packages, and on to things that are of less importance to
deal.II (e.g., optimization, uncertainty quantification, etc).
deal.II's Trilinos interfaces encapsulate many of the things Trilinos offers
that are of relevance to PDE solvers, and
provides wrapper classes (in namespace TrilinosWrappers) that make the
Trilinos matrix, vector, solver and preconditioner classes look very much the
same as deal.II's own implementations of this functionality. However, as
opposed to deal.II's classes, they can be used in %parallel if we give them the
necessary information. As a consequence, there are two Trilinos classes that
we have to deal with directly (rather than through wrappers), both of which
are part of Trilinos' Epetra library of basic linear algebra and tool classes:
<ul>
<li> The Epetra_Comm class is an abstraction of an MPI "communicator", i.e.,
  it describes how many and which machines can communicate with each other.
  Each distributed object, such as a sparse matrix or a vector for which we
  may want to store parts on different machines, needs to have a communicator
  object to know how many parts there are, where they can be found, and how
  they can be accessed.

  In this program, we only really use one communicator object -- based on the
  MPI variable <code>MPI_COMM_WORLD</code> -- that encompasses <i>all</i>
  processes that work together. It would be perfectly legitimate to start a
  process on $N$ machines but only store vectors on a subset of these by
  producing a communicator object that only encompasses this subset of
  machines; there is really no compelling reason to do so here, however.

<li> The IndexSet class is used to describe which elements of a vector or which
  rows of a matrix should reside on the current machine that is part of a
  communicator. To create such an object, you need to know (i) the total
  number of elements or rows, (ii) the indices of the elements you want to
  store locally. We will set up these <code>partitioners</code> in the
  <code>BoussinesqFlowProblem::setup_dofs</code> function below and then hand
  it to every %parallel object we create.

  Unlike PETSc, Trilinos makes no assumption that the elements of a vector
  need to be partitioned into contiguous chunks. At least in principle, we
  could store all elements with even indices on one processor and all odd ones
  on another. That's not very efficient, of course, but it's
  possible. Furthermore, the elements of these partitionings do not
  necessarily be mutually exclusive. This is important because when
  postprocessing solutions, we need access to all locally relevant or at least
  the locally active degrees of freedom (see the module on @ref distributed
  for a definition, as well as the discussion in step-40). Which elements the
  Trilinos vector considers as locally owned is not important to us then. All
  we care about is that it stores those elements locally that we need.
</ul>

There are a number of other concepts relevant to distributing the mesh
to a number of processors; you may want to take a look at the @ref
distributed module and step-40 or step-55 before trying to understand this
program.  The rest of the program is almost completely agnostic about
the fact that we don't store all objects completely locally. There
will be a few points where we have to limit loops over all cells to
those that are locally owned, or where we need to distinguish between
vectors that store only locally owned elements and those that store
everything that is locally relevant (see @ref GlossLocallyRelevantDof
"this glossary entry"), but by and large the amount of heavy lifting
necessary to make the program run in %parallel is well hidden in the
libraries upon which this program builds. In any case, we will comment
on these locations as we get to them in the program code.


<h3> Parallelization within individual nodes of a cluster </h3>

The second strategy to parallelize a program is to make use of the fact that
most computers today have more than one processor that all have access to the
same memory. In other words, in this model, we don't explicitly have to say
which pieces of data reside where -- all of the data we need is directly
accessible and all we have to do is split <i>processing</i> this data between
the available processors. We will then couple this with the MPI
parallelization outlined above, i.e., we will have all the processors on a
machine work together to, for example, assemble the local contributions to the
global matrix for the cells that this machine actually "owns" but not for
those cells that are owned by other machines. We will use this strategy for
four kinds of operations we frequently do in this program: assembly of the
Stokes and temperature matrices, assembly of the matrix that forms the Stokes
preconditioner, and assembly of the right hand side of the temperature system.

All of these operations essentially look as follows: we need to loop over all
cells for which <code>cell-@>subdomain_id()</code> equals the index our
machine has within the communicator object used for all communication
(i.e., <code>MPI_COMM_WORLD</code>, as explained above). The test we are
actually going to use for this, and which describes in a concise way why we
test this condition, is <code>cell-@>is_locally_owned()</code>. On each
such cell we need to assemble the local contributions to the global matrix or
vector, and then we have to copy each cell's contribution into the global
matrix or vector. Note that the first part of this (the loop) defines a range
of iterators on which something has to happen. The second part, assembly of
local contributions is something that takes the majority of CPU time in this
sequence of steps, and is a typical example of things that can be done in
%parallel: each cell's contribution is entirely independent of all other cells'
contributions. The third part, copying into the global matrix, must not happen
in %parallel since we are modifying one object and so several threads can not
at the same time read an existing matrix element, add their contribution, and
write the sum back into memory without danger of producing a <a
href="http://en.wikipedia.org/wiki/Race_condition">race condition</a>.

deal.II has a class that is made for exactly this workflow: WorkStream, first
discussed in step-9 and step-13. Its
use is also extensively documented in the module on @ref threads (in the section
on @ref MTWorkStream "the WorkStream class") and we won't repeat here the
rationale and detailed instructions laid out there, though you will want to
read through this module to understand the distinction between scratch space
and per-cell data. Suffice it to mention that we need the following:

- An iterator range for those cells on which we are supposed to work. This is
  provided by the FilteredIterator class which acts just like every other cell
  iterator in deal.II with the exception that it skips all cells that do not
  satisfy a particular predicate (i.e., a criterion that evaluates to true or
  false). In our case, the predicate is whether a cell is locally owned.

- A function that does the work on each cell for each of the tasks identified
  above, i.e., functions that assemble the local contributions to Stokes matrix
  and preconditioner, temperature matrix, and temperature right hand
  side. These are the
  <code>BoussinesqFlowProblem::local_assemble_stokes_system</code>,
  <code>BoussinesqFlowProblem::local_assemble_stokes_preconditioner</code>,
  <code>BoussinesqFlowProblem::local_assemble_temperature_matrix</code>, and
  <code>BoussinesqFlowProblem::local_assemble_temperature_rhs</code> functions in
  the code below. These four functions can all have several instances
  running in %parallel at the same time.

- %Functions that copy the result of the previous ones into the global object
  and that run sequentially to avoid race conditions. These are the
  <code>BoussinesqFlowProblem::copy_local_to_global_stokes_system</code>,
  <code>BoussinesqFlowProblem::copy_local_to_global_stokes_preconditioner</code>,
  <code>BoussinesqFlowProblem::copy_local_to_global_temperature_matrix</code>, and
  <code>BoussinesqFlowProblem::copy_local_to_global_temperature_rhs</code>
  functions.

We will comment on a few more points in the actual code, but in general
their structure should be clear from the discussion in @ref threads.

The underlying technology for WorkStream identifies "tasks" that need to be
worked on (e.g. assembling local contributions on a cell) and schedules
these tasks automatically to available processors. WorkStream creates these
tasks automatically, by splitting the iterator range into suitable chunks.

@note Using multiple threads within each MPI process only makes sense if you
have fewer MPI processes running on each node of your cluster than there are
processor cores on this machine. Otherwise, MPI will already keep your
processors busy and you won't get any additional speedup from using
threads. For example, if your cluster nodes have 8 cores as they often have at
the time of writing this, and if your batch scheduler puts 8 MPI processes on
each node, then using threads doesn't make the program any
faster. Consequently, you probably want to either configure your deal.II without
threads, or set the number of threads in Utilities::MPI::MPI_InitFinalize to 1
(third argument), or "export DEAL_II_NUM_THREADS=1" before running. That said, at
the time of writing this, we only use the WorkStream class for assembling
(parts of) linear systems, while 75% or more of the run time of the program is
spent in the linear solvers that are not parallelized &mdash; in other words,
the best we could hope is to parallelize the remaining 25%.


<h3> The testcase </h3>

The setup for this program is mildly reminiscent of the problem we wanted to
solve in the first place (see the introduction of step-31):
convection in the earth mantle. As a consequence, we choose the following
data, all of which appears in the program in units of meters and seconds (the
SI system) even if we list them here in other units. We do note,
however, that these choices are essentially still only exemplary, and
not meant to result in a completely realistic description of
convection in the earth mantle: for that, more and more difficult
physics would have to be implemented, and several other aspects are
currently missing from this program as well. We will come back to this
issue in the results section again, but state for now that providing a
realistic description is a goal of the <i>ASPECT</i> code in
development at the time of writing this.

As a reminder, let us again state the equations we want to solve are these:
@f{eqnarray*}
  -\nabla \cdot (2 \eta \varepsilon ({\mathbf u})) +
  \nabla \left( \frac{\eta}{L} \hat p\right) &=&
  \rho(T) \mathbf{g},
  \\
  \frac{\eta}{L} \nabla \cdot {\mathbf u} &=& 0,
  \\
  \frac{\partial T}{\partial t}
  +
  {\mathbf u} \cdot \nabla T
  -
  \nabla \cdot \kappa \nabla T &=& \gamma,
@f}
augmented by boundary and initial conditions. We then have to choose data for
the following quantities:
<ul>
  <li>The domain is an annulus (in 2d) or a spherical shell (in 3d) with inner
  and outer radii that match that of the earth: the total radius of the earth
  is 6371km, with the mantle starting at a depth of around 35km (just under
  the solid earth <a target="_top"
  href="http://en.wikipedia.org/wiki/Crust_(geology)">crust</a> composed of
  <a target="_top"
  href="http://en.wikipedia.org/wiki/Continental_crust">continental</a> and <a
  target="_top" href="http://en.wikipedia.org/wiki/Oceanic_crust">oceanic
  plates</a>) to a depth of 2890km (where the
  <a target="_top" href="http://en.wikipedia.org/wiki/Outer_core">outer earth
  core</a> starts). The radii are therefore $R_0=(6371-2890)\text{km},
  R_1=(6371-35)\text{km}$. This domain is conveniently generated using the
  GridGenerator::hyper_shell() function.

  <li>At the interface between crust and mantle, the temperature is between
  500 and 900 degrees Celsius, whereas at its bottom it is around 4000 degrees
  Celsius (see, for example, <a target="_top"
  href="http://en.wikipedia.org/wiki/Mantle_(geology)">this Wikipedia
  entry</a>). In Kelvin, we therefore choose $T_0=(4000+273)\text{K}$,
  $T_1=(500+273)\text{K}$ as boundary conditions at the inner and outer edge.

  In addition to this, we also have to specify some initial conditions for
  the temperature field. The real temperature field of the earth is quite
  complicated as a consequence of the convection that has been going on for
  more than four billion years -- in fact, it is the properties of this
  temperature distribution that we want to explore with programs like
  this. As a consequence, we
  don't really have anything useful to offer here, but we can hope that if we
  start with something and let things run for a while that the exact initial
  conditions don't matter that much any more &mdash; as is in fact suggested
  by looking at the pictures shown in the <a href="#Results">results section
  below</a>. The initial temperature field we use here is given in terms of
  the radius by
  @f{align*}
    s &= \frac{\|\mathbf x\|-R_0}{R_1-R_0}, \\
    \varphi &= \arctan \frac{y}{x}, \\
    \tau &= s + \frac 15 s(1-s) \sin(6\varphi) q(z), \\
    T(\mathbf x) &= T_0(1-\tau) + T_1\tau,
  @f}
  where
  @f{align*}
    q(z) = \left\{
    \begin{array}{ll}
      1 & \text{in 2d} \\
      \max\{0, \cos(\pi |z/R_1|)\} & \text{in 3d}
    \end{array}
    \right. .
  @f}
  This complicated function is essentially a perturbation of a linear profile
  between the inner and outer temperatures. In 2d, the function
  $\tau=\tau(\mathbf x)$ looks like this (I got the picture from
  <a
  href="http://www.wolframalpha.com/input/?i=plot+%28sqrt%28x^2%2By^2%29%2B0.2*%28sqrt%28x^2%2By^2%29*%281-sqrt%28x^2%2By^2%29%29*sin%286*atan2%28x%2Cy%29%29%29%2C+x%3D-1+to+1%2C+y%3D-1+to+1">this
  page</a>):

  <img src="https://www.dealii.org/images/steps/developer/step-32.2d-initial.png" alt="">

  The point of this profile is that if we had used $s$ instead of $\tau$ in
  the definition of $T(\mathbf x)$ then it would simply be a linear
  interpolation. $\tau$ has the same function values as $s$ on the inner and
  outer boundaries (zero and one, respectively), but it stretches the
  temperature profile a bit depending on the angle and the $z$ value in 3d,
  producing an angle-dependent perturbation of the linearly interpolating
  field. We will see in the results section that this is an
  entirely unphysical temperature field (though it will make for
  interesting images) as the equilibrium state for the temperature
  will be an almost constant temperature with boundary layers at the
  inner and outer boundary.

  <li>The right hand side of the temperature equation contains the rate of
  %internal heating $\gamma$. The earth does heat naturally through several mechanisms:
  radioactive decay, chemical separation (heavier elements sink to the bottom,
  lighter ones rise to the top; the countercurrents dissipate energy equal to
  the loss of potential energy by this separation process); heat release
  by crystallization of liquid metal as the solid inner core of the earth
  grows; and heat dissipation from viscous friction as the fluid moves.

  Chemical separation is difficult to model since it requires modeling mantle
  material as multiple phases; it is also a relatively small
  effect. Crystallization heat is even more difficult since it is confined to
  areas where temperature and pressure allow for phase changes, i.e., a
  discontinuous process. Given the difficulties in modeling these two
  phenomena, we will neglect them.

  The other two are readily handled and, given the way we scaled the
  temperature equation, lead to the equation
  @f[
    \gamma(\mathbf x)
     =
     \frac{\rho q+2\eta \varepsilon(\mathbf u):\varepsilon(\mathbf u)}
     {\rho c_p},
  @f]
  where $q$ is the radiogenic heating in $\frac{W}{kg}$, and the second
  term in the enumerator is viscous friction heating. $\rho$ is the density
  and $c_p$ is the specific heat. The literature provides the following
  approximate values: $c_p=1250 \frac{J}{kg\; K}, q=7.4\cdot 10^{-12}\frac{W}{kg}$.
  The other parameters are discussed elsewhere in this section.

  We neglect one internal heat source, namely adiabatic heating here,
  which will lead to a surprising temperature field. This point is
  commented on in detail in the results section below.

  <li>For the velocity we choose as boundary conditions $\mathbf{v}=0$ at the
  inner radius (i.e., the fluid sticks to the earth core) and
  $\mathbf{n}\cdot\mathbf{v}=0$ at the outer radius (i.e., the fluid flows
  tangentially along the bottom of the earth crust). Neither of these is
  physically overly correct: certainly, on both boundaries, fluids can flow
  tangentially, but they will incur a shear stress through friction against
  the medium at the other side of the interface (the metallic core and the
  crust, respectively). Such a situation could be modeled by a Robin-type
  boundary condition for the tangential velocity; in either case, the normal (vertical)
  velocity would be zero, although even that is not entirely correct since
  continental plates also have vertical motion (see, for example, the
  phenomenon of <a
  href="http://en.wikipedia.org/wiki/Postglacial_rebound">post-glacial
  rebound</a>). But to already make things worse for the tangential velocity,
  the medium on the other side is in motion as well, so the shear stress
  would, in the simplest case, be proportional to the <i>velocity
  difference</i>, leading to a boundary condition of the form
  @f{align*}
    \mathbf{n}\cdot [2\eta \varepsilon(\mathbf v)]
    &=
    s \mathbf{n} \times [\mathbf v - \mathbf v_0],
    \\
    \mathbf{n} \cdot \mathbf v &= 0,
  @f}
  with a proportionality constant $s$. Rather than going down this route,
  however, we go with the choice of zero (stick) and tangential
  flow boundary conditions.

  As a side note of interest, we may also have chosen tangential flow
  conditions on both inner and outer boundary. That has a significant
  drawback, however: it leaves the velocity not uniquely defined. The reason
  is that all velocity fields $\hat{\mathbf v}$ that correspond to a solid
  body rotation around the center of the domain satisfy $\mathrm{div}\;
  \varepsilon(\hat{\mathbf v})=0, \mathrm{div} \;\hat{\mathbf v} = 0$, and
  $\mathbf{n} \cdot \hat{\mathbf v} = 0$. As a consequence, if $\mathbf v$
  satisfies equations and boundary conditions, then so does $\mathbf v +
  \hat{\mathbf v}$. That's certainly not a good situation that we would like
  to avoid. The traditional way to work around this is to pick an arbitrary
  point on the boundary and call this your fixed point by choosing the
  velocity to be zero in all components there. (In 3d one has to choose two
  points.) Since this program isn't meant to be too realistic to begin with,
  we avoid this complication by simply fixing the velocity along the entire
  interior boundary.

  <li>To first order, the gravity vector always points downward. The question for
  a body as big as the earth is just: where is "up". The naive answer of course is
  "radially inward, towards the center of the earth". So at the surface of the
  earth, we have
  @f[
    \mathbf g
    =
    -9.81 \frac{\text{m}}{\text{s}^2} \frac{\mathbf x}{\|\mathbf x\|},
  @f]
  where $9.81 \frac{\text{m}}{\text{s}^2}$ happens to be the average gravity
  acceleration at the earth surface. But in the earth interior, the question
  becomes a bit more complicated: at the (bary-)center of the earth, for
  example, you have matter pulling equally hard in all directions, and so
  $\mathbf g=0$. In between, the net force is described as follows: let us
  define the <a target="_top"
  href="http://en.wikipedia.org/wiki/Potential_energy#Gravitational_potential_energy">gravity
  potential</a> by
  @f[
    \varphi(\mathbf x)
    =
    \int_{\text{earth}}
    -G \frac{\rho(\mathbf y)}{\|\mathbf x-\mathbf y\|}
    \ \text{d}y,
  @f]
  then $\mathbf g(\mathbf x) = -\nabla \varphi(\mathbf x)$. If we assume that
  the density $\rho$ is constant throughout the earth, we can produce an
  analytical expression for the gravity vector (don't try to integrate above
  equation somehow -- it leads to elliptic integrals; a simpler way is to
  notice that $-\Delta\varphi(\mathbf x) = -4\pi G \rho
  \chi_{\text{earth}}(\mathbf x)$ and solving this
  partial differential equation in all of ${\mathbb R}^3$ exploiting the
  radial symmetry):
  @f[
    \mathbf g(\mathbf x) =
    \left\{
      \begin{array}{ll}
        -\frac{4}{3}\pi G \rho \|\mathbf x\| \frac{\mathbf x}{\|\mathbf x\|}
        & \text{for} \ \|\mathbf x\|<R_1, \\
        -\frac{4}{3}\pi G \rho R^3 \frac{1}{\|\mathbf x\|^2}
        \frac{\mathbf x}{\|\mathbf x\|}
        & \text{for} \ \|\mathbf x\|\ge R_1.
      \end{array}
    \right.
  @f]
  The factor $-\frac{\mathbf x}{\|\mathbf x\|}$ is the unit vector pointing
  radially inward. Of course, within this problem, we are only interested in
  the branch that pertains to within the earth, i.e., $\|\mathbf
  x\|<R_1$. We would therefore only consider the expression
  @f[
    \mathbf g(\mathbf x) =
        -\frac{4}{3}\pi G \rho \|\mathbf x\| \frac{\mathbf x}{\|\mathbf x\|}
        =
        -\frac{4}{3}\pi G \rho \mathbf x
        =
        - 9.81 \frac{\mathbf x}{R_1} \frac{\text{m}}{\text{s}^2},
  @f]
  where we can infer the last expression because we know Earth's gravity at
  the surface (where $\|x\|=R_1$).

  One can derive a more general expression by integrating the
  differential equation for $\varphi(r)$ in the case that the density
  distribution is radially symmetric, i.e., $\rho(\mathbf
  x)=\rho(\|\mathbf x\|)=\rho(r)$. In that case, one would get
  @f[
    \varphi(r)
    = 4\pi G \int_0^r \frac 1{s^2} \int_0^s t^2 \rho(t) \; dt \; ds.
  @f]


  There are two problems with this, however: (i) The Earth is not homogeneous,
  i.e., the density $\rho$ depends on $\mathbf x$; in fact it is not even a
  function that only depends on the radius $r=\|\mathbf x\|$. In reality, gravity therefore
  does not always decrease as we get deeper: because the earth core is so much
  denser than the mantle, gravity actually peaks at around $10.7
  \frac{\text{m}}{\text{s}^2}$ at the core mantle boundary (see <a
  target="_top" href="http://en.wikipedia.org/wiki/Earth's_gravity">this
  article</a>). (ii) The density, and by
  consequence the gravity vector, is not even constant in time: after all, the
  problem we want to solve is the time dependent upwelling of hot, less dense
  material and the downwelling of cold dense material. This leads to a gravity
  vector that varies with space and time, and does not always point straight
  down.

  In order to not make the situation more complicated than necessary, we could
  use the approximation that at the inner boundary of the mantle,
  gravity is $10.7 \frac{\text{m}}{\text{s}^2}$ and at the outer
  boundary it is $9.81 \frac{\text{m}}{\text{s}^2}$, in each case
  pointing radially inward, and that in between gravity varies
  linearly with the radial distance from the earth center. That said, it isn't
  that hard to actually be slightly more realistic and assume (as we do below)
  that the earth mantle has constant density. In that case, the equation above
  can be integrated and we get an expression for $\|\mathbf{g}\|$ where we
  can fit constants to match the gravity at the top and bottom of the earth
  mantle to obtain
  @f[
    \|\mathbf{g}\|
    = 1.245\cdot 10^{-6} \frac{1}{\textrm{s}^2} r + 7.714\cdot 10^{13} \frac{\textrm{m}^3}{\textrm{s}^2}\frac{1}{r^2}.
  @f]

  <li>The density of the earth mantle varies spatially, but not by very
  much. $\rho_{\text{ref}}=3300 \frac{\text{kg}}{\text{m}^3}$ is a relatively good average
  value for the density at reference temperature $T_{\text{ref}}=293$ Kelvin.

  <li>The thermal expansion coefficient $\beta$ also varies with depth
  (through its dependence on temperature and pressure). Close to the surface,
  it appears to be on the order of $\beta=45\cdot 10^{-6} \frac 1{\text{K}}$,
  whereas at the core mantle boundary, it may be closer to $\beta=10\cdot
  10^{-6} \frac 1{\text{K}}$. As a reasonable value, let us choose
  $\beta=2\cdot 10^{-5} \frac 1{\text{K}}$. The density as a function
  of temperature is then
  $\rho(T)=[1-\beta(T-T_{\text{ref}})]\rho_{\text{ref}}$.

  <li>The second to last parameter we need to specify is the viscosity
  $\eta$. This is a tough one, because rocks at the temperatures and pressure
  typical for the earth mantle flow so slowly that the viscosity can not be
  determined accurately in the laboratory. So how do we know about the
  viscosity of the mantle? The most commonly used route is to consider that
  during and after ice ages, ice shields form and disappear on time scales
  that are shorter than the time scale of flow in the mantle. As a
  consequence, continents slowly sink into the earth mantle under the added
  weight of an ice shield, and they rise up again slowly after the ice shield
  has disappeared again (this is called <a target="_top"
  href="http://en.wikipedia.org/wiki/Postglacial_rebound"><i>postglacial
  rebound</i></a>). By measuring the speed of this rebound, we can infer the
  viscosity of the material that flows into the area vacated under the
  rebounding continental plates.

  Using this technique, values around $\eta=10^{21} \text{Pa}\;\text{s}
  = 10^{21} \frac{\text{N}\;\text{s}}{\text{m}^2}
  = 10^{21} \frac{\text{kg}}{\text{m}\;\text{s}}$ have been found as the most
  likely, though the error bar on this is at least one order of magnitude.

  While we will use this value, we again have to caution that there are many
  physical reasons to assume that this is not the correct value. First, it
  should really be made dependent on temperature: hotter material is most
  likely to be less viscous than colder material. In reality, however, the
  situation is even more complex. Most rocks in the mantle undergo phase
  changes as temperature and pressure change: depending on temperature and
  pressure, different crystal configurations are thermodynamically favored
  over others, even if the chemical composition of the mantle were
  homogeneous. For example, the common mantle material MgSiO<sub>3</sub> exists
  in its <a target="_top"
  href="http://en.wikipedia.org/wiki/Perovskite_(structure)">perovskite
  structure</a> throughout most of the mantle, but in the lower mantle the
  same substance is stable only as <a targe="_top"
  href="http://en.wikipedia.org/wiki/Postperovskite">post-perovskite</a>. Clearly,
  to compute realistic viscosities, we would not only need to know the exact
  chemical composition of the mantle and the viscosities of all materials, but
  we would also have to compute the thermodynamically most stable
  configurations for all materials at each quadrature point. This is at the
  time of writing this program not a feasible suggestion.

  <li>Our last material parameter is the thermal diffusivity $\kappa$, which
  is defined as $\kappa=\frac{k}{\rho c_p}$ where $k$ is the thermal
  conductivity, $\rho$ the density, and $c_p$ the specific heat. For
  this, the literature indicates that it increases from around $0.7$ in the
  upper mantle to around $1.7 \frac{\text{mm}^2}{\text{s}}$ in the lower
  mantle, though the exact value
  is not really all that important: heat transport through convection is
  several orders of magnitude more important than through thermal
  conduction. It may be of interest to know that perovskite, the most abundant
  material in the earth mantle, appears to become transparent at pressures
  above around 120 GPa (see, for example, J. Badro et al., Science 305,
  383-386 (2004)); in the lower mantle, it may therefore be that heat
  transport through radiative transfer is more efficient than through thermal
  conduction.

  In view of these considerations, let us choose
  $\kappa=1 \frac{\text{mm}^2}{\text{s}} =10^{-6} \frac{\text{m}^2}{\text{s}}$
  for the purpose of this program.
</ul>

All of these pieces of equation data are defined in the program in the
<code>EquationData</code> namespace. When run, the program produces
long-term maximal velocities around 10-40 centimeters per year (see
the results section below), approximately the physically correct order
of magnitude. We will set the end time to 1 billion years.

@note The choice of the constants and material parameters above follows in
large part the comprehensive book "Mantle Convection in the Earth and Planets,
Part 1" by G. Schubert and D. L. Turcotte and P. Olson (Cambridge, 2001). It
contains extensive discussion of ways to make the program more realistic.


<h3> Implementation details </h3>

Compared to step-31, this program has a number of noteworthy differences:

- The <code>EquationData</code> namespace is significantly larger, reflecting
  the fact that we now have much more physics to deal with. That said, most of
  this additional physical detail is rather self-contained in functions in
  this one namespace, and does not proliferate throughout the rest of the
  program.

- Of more obvious visibility is the fact that we have put a good number of
  parameters into an input file handled by the ParameterHandler class (see,
  for example, step-29, for ways to set up run-time parameter files with this
  class). This often makes sense when one wants to avoid re-compiling the
  program just because one wants to play with a single parameter (think, for
  example, of parameter studies determining the best values of the
  stabilization constants discussed above), in particular given that it takes
  a nontrivial amount of time to re-compile programs of the current size. To
  just give an overview of the kinds of parameters we have moved from fixed
  values into the input file, here is a listing of a typical
  <code>\step-32.prm</code> file:
  @code
# Listing of Parameters
# ---------------------
# The end time of the simulation in years.
set End time                            = 1e8

# Whether graphical output is to be generated or not. You may not want to get
# graphical output if the number of processors is large.
set Generate graphical output           = false

# The number of adaptive refinement steps performed after initial global
# refinement.
set Initial adaptive refinement         = 1

# The number of global refinement steps performed on the initial coarse mesh,
# before the problem is first solved there.
set Initial global refinement           = 1

# The number of time steps between each generation of graphical output files.
set Time steps between graphical output = 50

# The number of time steps after which the mesh is to be adapted based on
# computed error indicators.
set Time steps between mesh refinement  = 10


subsection Discretization
  # The polynomial degree to use for the velocity variables in the Stokes
  # system.
  set Stokes velocity polynomial degree       = 2

  # The polynomial degree to use for the temperature variable.
  set Temperature polynomial degree           = 2

  # Whether to use a Stokes discretization that is locally conservative at the
  # expense of a larger number of degrees of freedom, or to go with a cheaper
  # discretization that does not locally conserve mass (although it is
  # globally conservative.
  set Use locally conservative discretization = true
end


subsection Stabilization parameters
  # The exponent in the entropy viscosity stabilization.
  set alpha = 2

  # The beta factor in the artificial viscosity stabilization. An appropriate
  # value for 2d is 0.052 and 0.078 for 3d.
  set beta  = 0.078

  # The c_R factor in the entropy viscosity stabilization.
  set c_R   = 0.5
end
  @endcode

- There are, obviously, a good number of changes that have to do with the fact
  that we want to run our program on a possibly very large number of
  machines. Although one may suspect that this requires us to completely
  re-structure our code, that isn't in fact the case (although the classes
  that implement much of this functionality in deal.II certainly look very
  different from an implementation viewpoint, but this doesn't reflect in
  their public interface). Rather, the changes are mostly subtle, and the
  overall structure of the main class is pretty much unchanged. That said, the
  devil is in the detail: getting %parallel computing right, without
  deadlocks, ensuring that the right data is available at the right place
  (see, for example, the discussion on fully distributed vectors vs. vectors
  with ghost elements), and avoiding bottlenecks is difficult and discussions
  on this topic will appear in a good number of places in this program.


<h3> Outlook </h3>

This is a tutorial program. That means that at least most of its focus needs
to lie on demonstrating ways of using deal.II and associated libraries, and
not diluting this teaching lesson by focusing overly much on physical
details. Despite the lengthy section above on the choice of physical
parameters, the part of the program devoted to this is actually quite short
and self contained.

That said, both step-31 and the current step-32 have not come about by chance
but are certainly meant as wayposts along the path to a more comprehensive
program that will simulate convection in the earth mantle. We call this code
<i>ASPECT</i> (short for <i>Advanced %Solver for Problems in Earth's
ConvecTion</i>); its development is funded by
the <a href="http://www.geodynamics.org">Computational Infrastructure in
Geodynamics</a> initiative with support from the National Science
Foundation. More information on <i>ASPECT</i> is available at
its <a href="https://aspect.geodynamics.org/">homepage</a>.


examples/step-32/doc/results.dox
<h1>Results</h1>

When run, the program simulates convection in 3d in much the same way
as step-31 did, though with an entirely different testcase.


<h3>Comparison of results with \step-31</h3>

Before we go to this testcase, however, let us show a few results from a
slightly earlier version of this program that was solving exactly the
testcase we used in step-31, just that we now solve it in parallel and with
much higher resolution. We show these results mainly for comparison.

Here are two images that show this higher resolution if we choose a 3d
computation in <code>main()</code> and if we set
<code>initial_refinement=3</code> and
<code>n_pre_refinement_steps=4</code>. At the time steps shown, the
meshes had around 72,000 and 236,000 cells, for a total of 2,680,000
and 8,250,000 degrees of freedom, respectively, more than an order of
magnitude more than we had available in step-31:

<table align="center" class="doxtable">
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-32.3d.cube.0.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-32.3d.cube.1.png" alt="">
    </td>
  </tr>
</table>

The computation was done on a subset of 50 processors of the Brazos
cluster at Texas A&amp;M University.


<h3>Results for a 2d circular shell testcase</h3>

Next, we will run step-32 with the parameter file in the directory with one
change: we increase the final time to 1e9. Here we are using 16 processors. The
command to launch is (note that step-32.prm is the default):

<code>
<pre>
\$ mpirun -np 16 ./step-32
</pre>
</code>

Note that running a job on a cluster typically requires going through a job
scheduler, which we won't discuss here. The output will look roughly like
this:

<code>
<pre>
\$ mpirun -np 16 ./step-32
Number of active cells: 12,288 (on 6 levels)
Number of degrees of freedom: 186,624 (99,840+36,864+49,920)

Timestep 0:  t=0 years

   Rebuilding Stokes preconditioner...
   Solving Stokes system... 41 iterations.
   Maximal velocity: 60.4935 cm/year
   Time step: 18166.9 years
   17 CG iterations for temperature
   Temperature range: 973 4273.16

Number of active cells: 15,921 (on 7 levels)
Number of degrees of freedom: 252,723 (136,640+47,763+68,320)

Timestep 0:  t=0 years

   Rebuilding Stokes preconditioner...
   Solving Stokes system... 50 iterations.
   Maximal velocity: 60.3223 cm/year
   Time step: 10557.6 years
   19 CG iterations for temperature
   Temperature range: 973 4273.16

Number of active cells: 19,926 (on 8 levels)
Number of degrees of freedom: 321,246 (174,312+59,778+87,156)

Timestep 0:  t=0 years

   Rebuilding Stokes preconditioner...
   Solving Stokes system... 50 iterations.
   Maximal velocity: 57.8396 cm/year
   Time step: 5453.78 years
   18 CG iterations for temperature
   Temperature range: 973 4273.16

Timestep 1:  t=5453.78 years

   Solving Stokes system... 49 iterations.
   Maximal velocity: 59.0231 cm/year
   Time step: 5345.86 years
   18 CG iterations for temperature
   Temperature range: 973 4273.16

Timestep 2:  t=10799.6 years

   Solving Stokes system... 24 iterations.
   Maximal velocity: 60.2139 cm/year
   Time step: 5241.51 years
   17 CG iterations for temperature
   Temperature range: 973 4273.16

[...]

Timestep 100:  t=272151 years

   Solving Stokes system... 21 iterations.
   Maximal velocity: 161.546 cm/year
   Time step: 1672.96 years
   17 CG iterations for temperature
   Temperature range: 973 4282.57

Number of active cells: 56,085 (on 8 levels)
Number of degrees of freedom: 903,408 (490,102+168,255+245,051)



+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |       115s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble Stokes system          |       103 |      2.82s |       2.5% |
| Assemble temperature matrices   |        12 |     0.452s |      0.39% |
| Assemble temperature rhs        |       103 |      11.5s |        10% |
| Build Stokes preconditioner     |        12 |      2.09s |       1.8% |
| Solve Stokes system             |       103 |      90.4s |        79% |
| Solve temperature system        |       103 |      1.53s |       1.3% |
| Postprocessing                  |         3 |     0.532s |      0.46% |
| Refine mesh structure, part 1   |        12 |      0.93s |      0.81% |
| Refine mesh structure, part 2   |        12 |     0.384s |      0.33% |
| Setup dof systems               |        13 |      2.96s |       2.6% |
+---------------------------------+-----------+------------+------------+

[...]

+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |  9.14e+04s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble Stokes system          |     47045 |  2.05e+03s |       2.2% |
| Assemble temperature matrices   |      4707 |       310s |      0.34% |
| Assemble temperature rhs        |     47045 |   8.7e+03s |       9.5% |
| Build Stokes preconditioner     |      4707 |  1.48e+03s |       1.6% |
| Solve Stokes system             |     47045 |  7.34e+04s |        80% |
| Solve temperature system        |     47045 |  1.46e+03s |       1.6% |
| Postprocessing                  |      1883 |       222s |      0.24% |
| Refine mesh structure, part 1   |      4706 |       641s |       0.7% |
| Refine mesh structure, part 2   |      4706 |       259s |      0.28% |
| Setup dof systems               |      4707 |  1.86e+03s |         2% |
+---------------------------------+-----------+------------+------------+
</pre>
</code>

The simulation terminates when the time reaches the 1 billion years
selected in the input file.  You can extrapolate from this how long a
simulation would take for a different final time (the time step size
ultimately settles on somewhere around 20,000 years, so computing for
two billion years will take 100,000 time steps, give or take 20%).  As
can be seen here, we spend most of the compute time in assembling
linear systems and &mdash; above all &mdash; in solving Stokes
systems.


To demonstrate the output we show the output from every 1250th time step here:
<table>
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-000.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-050.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-100.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-150.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-200.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-250.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-300.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-350.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-400.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-450.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-500.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-550.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-time-600.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-cells.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-32-2d-partition.png" alt="">
    </td>
  </tr>
</table>

The last two images show the grid as well as the partitioning of the mesh for
the same computation with 16 subdomains and 16 processors. The full dynamics of
this simulation are really only visible by looking at an animation, for example
the one <a
href="https://www.dealii.org/images/steps/developer/step-32-2d-temperature.webm">shown
on this site</a>. This image is well worth watching due to its artistic quality
and entrancing depiction of the evolution of the magma plumes.

If you watch the movie, you'll see that the convection pattern goes
through several stages: First, it gets rid of the instable temperature
layering with the hot material overlain by the dense cold
material. After this great driver is removed and we have a sort of
stable situation, a few blobs start to separate from the hot boundary
layer at the inner ring and rise up, with a few cold fingers also
dropping down from the outer boundary layer. During this phase, the solution
remains mostly symmetric, reflecting the 12-fold symmetry of the
original mesh. In a final phase, the fluid enters vigorous chaotic
stirring in which all symmetries are lost. This is a pattern that then
continues to dominate flow.

These different phases can also be identified if we look at the
maximal velocity as a function of time in the simulation:

<img src="https://www.dealii.org/images/steps/developer/step-32.2d.t_vs_vmax.png" alt="">

Here, the velocity (shown in centimeters per year) becomes very large,
to the order of several meters per year) at the beginning when the
temperature layering is instable. It then calms down to relatively
small values before picking up again in the chaotic stirring
regime. There, it remains in the range of 10-40 centimeters per year,
quite within the physically expected region.


<h3>Results for a 3d spherical shell testcase</h3>

3d computations are very expensive computationally. Furthermore, as
seen above, interesting behavior only starts after quite a long time
requiring more CPU hours than is available on a typical
cluster. Consequently, rather than showing a complete simulation here,
let us simply show a couple of pictures we have obtained using the
successor to this program, called <i>ASPECT</i> (short for <i>Advanced
%Solver for Problems in Earth's ConvecTion</i>), that is being
developed independently of deal.II and that already incorporates some
of the extensions discussed below. The following two pictures show
isocontours of the temperature and the partition of the domain (along
with the mesh) onto 512 processors:

<p align="center">
<img src="https://www.dealii.org/images/steps/developer/step-32.3d-sphere.solution.png" alt="">

<img src="https://www.dealii.org/images/steps/developer/step-32.3d-sphere.partition.png" alt="">
</p>


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

There are many directions in which this program could be extended. As
mentioned at the end of the introduction, most of these are under active
development in the <i>ASPECT</i> (short for <i>Advanced %Solver for Problems
in Earth's ConvecTion</i>) code at the time this tutorial program is being
finished. Specifically, the following are certainly topics that one should
address to make the program more useful:

<ul>
  <li> <b>Adiabatic heating/cooling:</b>
  The temperature field we get in our simulations after a while
  is mostly constant with boundary layers at the inner and outer
  boundary, and streamers of cold and hot material mixing
  everything. Yet, this doesn't match our expectation that things
  closer to the earth core should be hotter than closer to the
  surface. The reason is that the energy equation we have used does
  not include a term that describes adiabatic cooling and heating:
  rock, like gas, heats up as you compress it. Consequently, material
  that rises up cools adiabatically, and cold material that sinks down
  heats adiabatically. The correct temperature equation would
  therefore look somewhat like this:
  @f{eqnarray*}
    \frac{D T}{Dt}
    -
    \nabla \cdot \kappa \nabla T &=& \gamma + \tau\frac{Dp}{Dt},
  @f}
  or, expanding the advected derivative $\frac{D}{Dt} =
  \frac{\partial}{\partial t} + \mathbf u \cdot \nabla$:
  @f{eqnarray*}
    \frac{\partial T}{\partial t}
    +
    {\mathbf u} \cdot \nabla T
    -
    \nabla \cdot \kappa \nabla T &=& \gamma +
    \tau\left\{\frac{\partial
    p}{\partial t} + \mathbf u \cdot \nabla p \right\}.
  @f}
  In other words, as pressure increases in a rock volume
  ($\frac{Dp}{Dt}>0$) we get an additional heat source, and vice
  versa.

  The time derivative of the pressure is a bit awkward to
  implement. If necessary, one could approximate using the fact
  outlined in the introduction that the pressure can be decomposed
  into a dynamic component due to temperature differences and the
  resulting flow, and a static component that results solely from the
  static pressure of the overlying rock. Since the latter is much
  bigger, one may approximate $p\approx p_{\text{static}}=-\rho_{\text{ref}}
  [1+\beta T_{\text{ref}}] \varphi$, and consequently
  $\frac{Dp}{Dt} \approx \left\{- \mathbf u \cdot \nabla \rho_{\text{ref}}
  [1+\beta T_{\text{ref}}]\varphi\right\} = \rho_{\text{ref}}
  [1+\beta T_{\text{ref}}] \mathbf u \cdot \mathbf g$.
  In other words, if the fluid is moving in the direction of gravity
  (downward) it will be compressed and because in that case $\mathbf u
  \cdot \mathbf g > 0$ we get a positive heat source. Conversely, the
  fluid will cool down if it moves against the direction of gravity.

<li> <b>Compressibility:</b>
  As already hinted at in the temperature model above,
  mantle rocks are not incompressible. Rather, given the enormous pressures in
  the earth mantle (at the core-mantle boundary, the pressure is approximately
  140 GPa, equivalent to 1,400,000 times atmospheric pressure), rock actually
  does compress to something around 1.5 times the density it would have
  at surface pressure. Modeling this presents any number of
  difficulties. Primarily, the mass conservation equation is no longer
  $\textrm{div}\;\mathbf u=0$ but should read
  $\textrm{div}(\rho\mathbf u)=0$ where the density $\rho$ is now no longer
  spatially constant but depends on temperature and pressure. A consequence is
  that the model is now no longer linear; a linearized version of the Stokes
  equation is also no longer symmetric requiring us to rethink preconditioners
  and, possibly, even the discretization. We won't go into detail here as to
  how this can be resolved.

<li> <b>Nonlinear material models:</b> As already hinted at in various places,
  material parameters such as the density, the viscosity, and the various
  thermal parameters are not constant throughout the earth mantle. Rather,
  they nonlinearly depend on the pressure and temperature, and in the case of
  the viscosity on the strain rate $\varepsilon(\mathbf u)$. For complicated
  models, the only way to solve such models accurately may be to actually
  iterate this dependence out in each time step, rather than simply freezing
  coefficients at values extrapolated from the previous time step(s).

<li> <b>Checkpoint/restart:</b> Running this program in 2d on a number of
  processors allows solving realistic models in a day or two. However, in 3d,
  compute times are so large that one runs into two typical problems: (i) On
  most compute clusters, the queuing system limits run times for individual
  jobs are to 2 or 3 days; (ii) losing the results of a computation due to
  hardware failures, misconfigurations, or power outages is a shame when
  running on hundreds of processors for a couple of days. Both of these
  problems can be addressed by periodically saving the state of the program
  and, if necessary, restarting the program at this point. This technique is
  commonly called <i>checkpoint/restart</i> and it requires that the entire
  state of the program is written to a permanent storage location (e.g. a hard
  drive). Given the complexity of the data structures of this program, this is
  not entirely trivial (it may also involve writing gigabytes or more of
  data), but it can be made easier by realizing that one can save the state
  between two time steps where it essentially only consists of the mesh and
  solution vectors; during restart one would then first re-enumerate degrees
  of freedom in the same way as done before and then re-assemble
  matrices. Nevertheless, given the distributed nature of the data structures
  involved here, saving and restoring the state of a program is not
  trivial. An additional complexity is introduced by the fact that one may
  want to change the number of processors between runs, for example because
  one may wish to continue computing on a mesh that is finer than the one used
  to precompute a starting temperature field at an intermediate time.

<li> <b>Predictive postprocessing:</b> The point of computations like this is
  not simply to solve the equations. Rather, it is typically the exploration
  of different physical models and their comparison with things that we can
  measure at the earth surface, in order to find which models are realistic
  and which are contradicted by reality. To this end, we need to compute
  quantities from our solution vectors that are related to what we can
  observe. Among these are, for example, heatfluxes at the surface of the
  earth, as well as seismic velocities throughout the mantle as these affect
  earthquake waves that are recorded by seismographs.

<li> <b>Better refinement criteria:</b> As can be seen above for the
3d case, the mesh in 3d is primarily refined along the inner
boundary. This is because the boundary layer there is stronger than
any other transition in the domain, leading us to refine there almost
exclusively and basically not at all following the plumes. One
certainly needs better refinement criteria to track the parts of the
solution we are really interested in better than the criterion used
here, namely the KellyErrorEstimator applied to the temperature, is
able to.
</ul>


There are many other ways to extend the current program. However, rather than
discussing them here, let us point to the much larger open
source code ASPECT (see https://aspect.geodynamics.org/ ) that constitutes the
further development of step-32 and that already includes many such possible
extensions.


examples/step-33/doc/intro.dox
<br>

<i>
This program was written for fun by David Neckels (NCAR) while working
at Sandia (on the Wyoming Express bus to and from Corrales each day).
The main purpose was to better understand Euler flow.
The code solves the basic Euler equations of gas dynamics, by using a
fully implicit Newton iteration (inspired by Sandia's Aria code).  The
code may be configured by an input file to run different simulations
on different meshes, with differing boundary conditions.
<br>
The original code and documentation was later slightly modified by Wolfgang
Bangerth to make it more modular and allow replacing the parts that are
specific to the Euler equations by other hyperbolic conservation laws without
too much trouble.
</i>

@note The program uses the <a
href="http://trilinos.org">Trilinos</a> linear solvers (these can be found
in Trilinos in the Aztec/Amesos packages) and an automatic
differentiation package, Sacado, also part of Trilinos. deal.II must
be configured to use Trilinos. Refer to the <a
href="../../readme.html#trilinos">ReadMe</a> file for instructions how to
do this.

@note While this program demonstrates the use of automatic differentiation
well, it does not express the state of the art in Euler equation solvers.
There are much faster and more accurate method for this equation, and
you should take a look at step-67 and step-69 to see how this equation
can be solved more efficiently.



<a name="Intro"></a> <h1>Introduction</h1>

<h3>Euler flow</h3>

The equations that describe the movement of a compressible, inviscid
gas (the so-called Euler equations of gas dynamics) are
a basic system of conservation laws. In spatial dimension $d$ they read
@f[
\partial_t \mathbf{w} + \nabla \cdot \mathbf{F}(\mathbf{w}) =
\mathbf{G}(\mathbf w),
@f]
with the solution $\mathbf{w}=(\rho v_1,\ldots,\rho v_d,\rho,
E)^{\top}$ consisting of $\rho$ the fluid density, ${\mathbf v}=(v_1,\ldots v_d)^T$ the
flow velocity (and thus $\rho\mathbf v$ being the linear momentum
density), and
$E$ the energy density of the gas. We interpret the equations above as
$\partial_t \mathbf{w}_i + \nabla \cdot \mathbf{F}_i(\mathbf{w}) = \mathbf
G_i(\mathbf w)$, $i=1,\ldots,dim+2$.

For the Euler equations, the flux matrix $\mathbf F$ (or system of flux functions)
is defined as (shown here for the case $d=3$)
@f{eqnarray*}
  \mathbf F(\mathbf w)
  =
  \left(
  \begin{array}{ccc}
    \rho v_1^2+p & \rho v_2v_1  & \rho v_3v_1 \\
    \rho v_1v_2  & \rho v_2^2+p & \rho v_3v_2 \\
    \rho v_1v_3  & \rho v_2v_3  & \rho v_3^2+p \\
    \rho v_1 & \rho v_2 & \rho v_3 \\
    (E+p) v_1 & (E+p) v_2 & (E+p) v_3
  \end{array}
  \right),
@f}
and we will choose as particular right hand side forcing only the effects of
gravity, described by
@f{eqnarray*}
  \mathbf G(\mathbf w)
  =
  \left(
  \begin{array}{c}
    g_1\rho \\
    g_2\rho \\
    g_3\rho \\
    0 \\
    \rho \mathbf g \cdot \mathbf v
  \end{array}
  \right),
@f}
where $\mathbf g=(g_1,g_2,g_3)^T$ denotes the gravity vector.
With this, the entire system of equations reads:
@f{eqnarray*}
  \partial_t (\rho v_i) + \sum_{s=1}^d \frac{\partial(\rho v_i v_s +
  \delta_{is} p)}{\partial x_s} &=& g_i \rho, \qquad i=1,\dots,d, \\
  \partial_t \rho + \sum_{s=1}^d \frac{\partial(\rho v_s)}{\partial x_s} &=& 0,  \\
  \partial_t E + \sum_{s=1}^d \frac{\partial((E+p)v_s)}{\partial x_s} &=&
  \rho \mathbf g \cdot \mathbf v.
@f}
These equations describe, respectively, the conservation of momentum,
mass, and energy.
The system is closed by a relation that defines the pressure: $p =
(\gamma -1)(E-\frac{1}{2} \rho |\mathbf v|^2)$. For the constituents
of air (mainly nitrogen and oxygen) and other diatomic gases, the ratio of
specific heats is $\gamma=1.4$.

This problem obviously falls into the class of vector-valued
problems. A general overview of how to deal with these problems in
deal.II can be found in the @ref vector_valued module.

<h3>Discretization</h3>

Discretization happens in the usual way, taking into account that this
is a hyperbolic problem in the same style as the simple one discussed
in step-12:
We choose a finite element space $V_h$, and integrate our conservation law against
our (vector-valued) test function $\mathbf{z} \in V_h$.  We then integrate by parts and approximate the
boundary flux with a <i> numerical </i> flux $\mathbf{H}$,
@f{eqnarray*}
&&\int_{\Omega} (\partial_t \mathbf{w}, \mathbf{z}) + (\nabla \cdot \mathbf{F}(\mathbf{w}), \mathbf{z}) \\
&\approx &\int_{\Omega} (\partial_t \mathbf{w}, \mathbf{z}) - (\mathbf{F}(\mathbf{w}), \nabla \mathbf{z}) + h^{\eta}(\nabla \mathbf{w} , \nabla \mathbf{z}) + \int_{\partial \Omega} (\mathbf{H}(\mathbf{w}^+, \mathbf{w}^-, \mathbf{n}), \mathbf{z}^+),
@f}
where a superscript $+$ denotes the interior trace of a function, and $-$ represents the outer trace.
The diffusion term $h^{\eta}(\nabla \mathbf{w} , \nabla \mathbf{z})$ is introduced strictly for stability,
 where $h$ is the mesh size and $\eta$ is a parameter prescribing how
 much diffusion to add.

On the boundary, we have to say what the outer trace $\mathbf{w}^-$ is.
Depending on the boundary condition, we prescribe either of the following:
<ul>
<li> Inflow boundary: $\mathbf{w}^-$ is prescribed to be the desired value.
<li> Supersonic outflow boundary: $\mathbf{w}^- = \mathbf{w}^+$
<li> Subsonic outflow boundary: $\mathbf{w}^- = \mathbf{w}^+$ except that the energy variable
is modified to support a prescribed pressure $p_o$, i.e.
$\mathbf{w}^- =(\rho^+, \rho v_1^+, \dots, \rho v_d^+, p_o/(\gamma -1) + 0.5 \rho |\mathbf{v}^+|^2)$
<li> Reflective boundary: we set $\mathbf{w}^-$ so that $(\mathbf{v}^+ + \mathbf{v}^-) \cdot \mathbf{n} = 0$ and
$\rho^- = \rho^+,E^-=E^+$.
</ul>

More information on these issues can be found, for example, in Ralf
Hartmann's PhD thesis ("Adaptive Finite Element Methods for the
Compressible Euler Equations", PhD thesis, University of Heidelberg, 2002).

We use a time stepping scheme to substitute the time derivative in the
above equations. For simplicity, we define $ \mathbf{B}({\mathbf{w}_{n}})(\mathbf z) $ as the spatial residual at time step $n$ :

@f{eqnarray*}
 \mathbf{B}(\mathbf{w}_{n})(\mathbf z)  &=&
- \int_{\Omega} \left(\mathbf{F}(\mathbf{w}_n),
\nabla\mathbf{z}\right) +  h^{\eta}(\nabla \mathbf{w}_n , \nabla \mathbf{z}) \\
&& +
\int_{\partial \Omega} \left(\mathbf{H}(\mathbf{w}_n^+,
\mathbf{w}^-(\mathbf{w}_n^+), \mathbf{n}), \mathbf{z}\right)
-
\int_{\Omega} \left(\mathbf{G}(\mathbf{w}_n),
\mathbf{z}\right) .
@f}

At each time step, our full discretization is thus
that the residual applied to any test
function $\mathbf z$ equals zero:
@f{eqnarray*}
R(\mathbf{W}_{n+1})(\mathbf z) &=&
\int_{\Omega} \left(\frac{{\mathbf w}_{n+1} - \mathbf{w}_n}{\delta t},
\mathbf{z}\right)+
\theta \mathbf{B}({\mathbf{w}}_{n+1}) +  (1-\theta) \mathbf{B}({\mathbf w}_{n}) \\
&=& 0
@f}
where $ \theta \in [0,1] $ and
$\mathbf{w}_i = \sum_k \mathbf{W}_i^k \mathbf{\phi}_k$. Choosing
$\theta=0$ results in the explicit (forward) Euler scheme, $\theta=1$
in the stable implicit (backward) Euler scheme, and $\theta=\frac 12$
in the Crank-Nicolson scheme.

In the implementation below, we choose the Lax-Friedrichs flux for the
function $\mathbf H$, i.e.  $\mathbf{H}(\mathbf{a},\mathbf{b},\mathbf{n}) =
\frac{1}{2}(\mathbf{F}(\mathbf{a})\cdot \mathbf{n} +
\mathbf{F}(\mathbf{b})\cdot \mathbf{n} + \alpha (\mathbf{a} - \mathbf{b}))$,
where $\alpha$ is either a fixed number specified in the input file, or where
$\alpha$ is a mesh dependent value. In the latter case, it is chosen as
$\frac{h}{2\delta T}$ with $h$ the diameter of the face to which the flux is
applied, and $\delta T$ the current time step.

With these choices, equating the residual to zero results in a
nonlinear system of equations $R(\mathbf{W}_{n+1})=0$. We solve this nonlinear system by a
Newton iteration (in the same way as explained in step-15), i.e. by iterating
@f{eqnarray*}
R'(\mathbf{W}^k_{n+1},\delta \mathbf{W}_{n+1}^k)(\mathbf z) & = & -
R(\mathbf{W}^{k}_{n+1})(\mathbf z) \qquad \qquad \forall \mathbf z\in V_h \\
\mathbf{W}^{k+1}_{n+1} &=& \mathbf{W}^k_{n+1} + \delta \mathbf{W}^k_{n+1},
@f}
until $|R(\mathbf{W}^k_{n+1})|$ (the residual) is sufficiently small. By
testing with the nodal basis of a finite element space instead of all
$\mathbf z$, we arrive at a linear system for $\delta \mathbf W$:
@f{eqnarray*}
\mathbf R'(\mathbf{W}^k_{n+1})\delta \mathbf{W}^k_{n+1} & = & -
\mathbf R(\mathbf{W}^{k}_{n+1}).
@f}
This linear system is, in general, neither symmetric nor has any
particular definiteness properties. We will either use a direct solver
or Trilinos' GMRES implementation to solve it. As will become apparent from
the <a href="#Results">results shown below</a>, this fully implicit iteration
converges very rapidly (typically in 3 steps) and with the quadratic
convergence order expected from a Newton method.


<h3> Automatic differentiation </h3>

Since computing the Jacobian matrix $\mathbf R'(\mathbf W^k)$ is a
terrible beast, we use an automatic differentiation package, Sacado,
to do this.  Sacado is a package within the <a
href="http://trilinos.org" target="_top">Trilinos</a> framework
and offers a C++ template class <code>Sacado::Fad::DFad</code>
(<code>Fad</code> standing for "forward automatic
differentiation") that supports basic arithmetic operators and
functions such as <code> sqrt, sin, cos, pow, </code> etc. In order to
use this feature, one declares a collection of variables of this type
and then denotes some of this collection as degrees of freedom, the rest of
the variables being functions of the independent variables.  These
variables are used in an algorithm, and as the variables are used,
their sensitivities with respect to the degrees of freedom are
continuously updated.

One can imagine that for the full Jacobian matrix as a whole,
this could be prohibitively expensive: the number of independent variables are
the $\mathbf W^k$, the dependent variables the elements of the vector $\mathbf
R(\mathbf W^k)$. Both of these vectors can easily have tens of thousands of
elements or more.  However, it is important to note that not all elements of
$\mathbf R$ depend on all elements of $\mathbf W^k$: in fact, an entry in
$\mathbf R$ only depends on an element of $\mathbf W^k$ if the two
corresponding shape functions overlap and couple in the weak form.

Specifically, it is wise to define a minimum set of
independent AD variables that the residual on the current cell may possibly
depend on: on every element, we define those variables as
independent that correspond to the degrees of freedom defined on this
cell (or, if we have to compute jump terms between cells, that
correspond to degrees of freedom defined on either of the two adjacent
cells), and the dependent variables are the elements of the local
residual vector. Not doing this, i.e. defining <i>all</i> elements of
$\mathbf W^k$ as independent, will result a very expensive computation
of a lot of zeros: the elements of the local residual vector are
independent of almost all elements of the solution vector, and
consequently their derivatives are zero; however, trying to compute
these zeros can easily take 90% or more of the compute time of the
entire program, as shown in an experiment inadvertently made by a student a few
years after this program was first written.


Coming back to the question of computing the Jacobian automatically:
The author has used this approach side by side with a hand coded Jacobian for
the incompressible Navier-Stokes problem and found the Sacado approach to be
just as fast as using a hand coded Jacobian, but infinitely simpler and less
error prone: Since using the auto-differentiation requires only that one code
the residual $R(\mathbf{W})$, ensuring code correctness and maintaining code
becomes tremendously more simple -- the Jacobian matrix $\mathbf R'$ is
computed by essentially the same code that also computes the residual $\mathbf
R$.

All this said, here's a very simple example showing how Sacado can be
used:

@code
#include <Sacado.hpp>
#include <iostream>

using fad_double = Sacado::Fad::DFad<double>;

main() {

  fad_double a,b,c;

  a = 1; b = 2;

  a.diff(0,2);  // Set a to be dof 0, in a 2-dof system.

  b.diff(1,2);  // Set b to be dof 1, in a 2-dof system.

  c = 2*a+cos(a*b);

  double *derivs = &c.fastAccessDx(0); // Access derivatives

  std::cout << "dc/da = " << derivs[0] << ", dc/db=" << derivs[1] << std::endl;

}
@endcode

The output are the derivatives $\frac{\partial c(a,b)}{\partial a},
\frac{\partial c(a,b)}{\partial b}$ of $c(a,b)=2a+\cos(ab)$ at $a=1,b=2$.

It should be noted that Sacado provides more auto-differentiation capabilities than the small subset
used in this program.  However, understanding the example above is
enough to understand the use of Sacado in this Euler flow program.

<h3> Trilinos solvers </h3>
The program uses either the Aztec iterative solvers, or the Amesos
sparse direct solver, both provided by
the Trilinos package.  This package is inherently designed to be used in a parallel program, however,
it may be used in serial just as easily, as is done here.  The Epetra package is the basic
vector/matrix library upon which the solvers are built.  This very powerful package can be used
to describe the parallel distribution of a vector, and to define sparse matrices that operate
on these vectors.  Please view the commented code for more details on how these solvers are used
within the example.

<h3> Adaptivity </h3>
The example uses an ad hoc refinement indicator that shows some usefulness in shock-type problems, and
in the downhill flow example included.  We refine according to the squared gradient of the density.
Hanging nodes are handled by computing the numerical flux across cells that are of differing
refinement levels, rather than using the AffineConstraints class as in
all other tutorial programs so far.  In this way, the example combines
the continuous and DG methodologies. It also simplifies the generation
of the Jacobian because we do not have to track constrained degrees of
freedom through the automatic differentiation used to compute it.

@note Whereas this program was written in 2008, we were unaware of any
publication that would actually have used this approach. However, a
more recent paper by A. Dedner, R. Kl&ouml;fkorn, and M. Kr&auml;nkel
("Continuous Finite-Elements on Non-Conforming Grids Using
Discontinuous Galerkin Stabilization", Proceedings of Finite Volumes
for Complex Applications VII - Methods and Theoretical Aspects,
Springer, 2014) comes close.

Further, we enforce a maximum number of refinement levels to keep refinement under check.  It is the
author's experience that for adaptivity for a time dependent problem, refinement can easily lead the simulation to
a screeching halt, because of time step restrictions if the mesh
becomes too fine in any part of the domain, if care is not taken.  The amount of refinement is
limited in the example by letting the user specify the
maximum level of refinement that will be present anywhere in the mesh.  In this way, refinement
tends not to slow the simulation to a halt.  This, of course, is purely a heuristic strategy, and
if the author's advisor heard about it, the author would likely be exiled forever from the finite
 element error estimation community.

<h3>Input deck, initial and boundary conditions</h3>

We use an input file deck to drive the simulation.  In this way, we can alter the boundary conditions
and other important properties of the simulation without having to recompile.  For more information on
the format, look at the <a href="#Results">results section</a>, where we
describe an example input file in more detail.

In previous example programs, we have usually hard-coded the initial
and boundary conditions. In this program, we instead use the
expression parser class FunctionParser so that we can specify a
generic expression in the input file and have it parsed at run time &mdash;
this way, we can change initial conditions without the need to
recompile the program. Consequently, no classes named
InitialConditions or BoundaryConditions will be declared in the
program below.


<h3>Implementation</h3>

The implementation of this program is split into three essential parts:
<ul>
  <li>The <code>EulerEquations</code> class that encapsulates everything that
  completely describes the specifics of the Euler equations. This includes the
  flux matrix $\mathbf F(\mathbf W)$, the numerical flux $\mathbf F(\mathbf
  W^+,\mathbf W^-,\mathbf n)$, the right hand side $\mathbf G(\mathbf W)$,
  boundary conditions, refinement indicators, postprocessing the output, and
  similar things that require knowledge of the meaning of the individual
  components of the solution vectors and the equations.

  <li>A namespace that deals with everything that has to do with run-time
  parameters.

  <li>The <code>ConservationLaw</code> class that deals with time stepping,
  outer nonlinear and inner linear solves, assembling the linear systems, and
  the top-level logic that drives all this.
</ul>

The reason for this approach is that it separates the various concerns in a
program: the <code>ConservationLaw</code> is written in such a way that it
would be relatively straightforward to adapt it to a different set of
equations: One would simply re-implement the members of the
<code>EulerEquations</code> class for some other hyperbolic equation, or
augment the existing equations by additional ones (for example by advecting
additional variables, or by adding chemistry, etc). Such modifications,
however, would not affect the time stepping, or the nonlinear solvers if
correctly done, and consequently nothing in the <code>ConservationLaw</code>
would have to be modified.

Similarly, if we wanted to improve on the linear or nonlinear solvers, or on
the time stepping scheme (as hinted at the end of the <a
href="#Results">results section</a>), then this would not require changes in
the <code>EulerEquations</code> at all.


examples/step-33/doc/results.dox
<a name="Results"></a>
<h1>Results</h1>

We run the problem with the mesh <code>slide.inp</code> (this file is in the
same directory as the source code for this program) and the following input
deck (available as <code>input.prm</code> in the same directory):
@verbatim
# Listing of Parameters
# ---------------------

# The input grid
set mesh = slide.inp

# Stabilization parameter
set diffusion power = 2.0

# --------------------------------------------------
# Boundary conditions
# We may specify boundary conditions for up to MAX_BD boundaries.
# Your .inp file should have these boundaries designated.
subsection boundary_1
  set no penetration = true # reflective boundary condition
end

subsection boundary_2
  # outflow boundary
  # set w_2 = pressure
  # set w_2 value = 1.5 - y
end

subsection boundary_3
  set no penetration = true # reflective
  # set w_3 = pressure
  # set w_3 value = 1.0
end

subsection boundary_4
  set no penetration = true #reflective
end

# --------------------------------------------------
# Initial Conditions
# We set the initial conditions of the conservative variables.  These lines
# are passed to the expression parsing function.  You should use x,y,z for
# the coordinate variables.

subsection initial condition
  set w_0 value = 0
  set w_1 value = 0
  set w_2 value = 10*(x<-0.7)*(y> 0.3)*(y< 0.45) + (1-(x<-0.7)*(y> 0.3)*(y< 0.45))*1.0
  set w_3 value = (1.5-(1.0*1.0*y))/0.4
end

# --------------------------------------------------
# Time stepping control
subsection time stepping
  set final time = 10.0 # simulation end time
  set time step  = 0.02 # simulation time step
  set theta scheme value = 0.5
end

subsection linear solver
  set output         = quiet
  set method         = gmres
  set ilut fill      = 1.5
  set ilut drop tolerance = 1e-6
  set ilut absolute tolerance = 1e-6
  set ilut relative tolerance = 1.0
end

# --------------------------------------------------
# Output frequency and kind
subsection output
  set step           = 0.01
  set schlieren plot = true
end

# --------------------------------------------------
# Refinement control
subsection refinement
  set refinement = true # none only other option
  set shock value = 1.5
  set shock levels = 1 # how many levels of refinement to allow
end

# --------------------------------------------------
# Flux parameters
subsection flux
 set stab = constant
 #set stab value = 1.0
end
@endverbatim

When we run the program, we get the following kind of output:
@verbatim
...
T=0.14
   Number of active cells:       1807
   Number of degrees of freedom: 7696

   NonLin Res     Lin Iter       Lin Res
   _____________________________________
   7.015e-03        0008        3.39e-13
   2.150e-05        0008        1.56e-15
   2.628e-09        0008        5.09e-20
   5.243e-16        (converged)

T=0.16
   Number of active cells:       1807
   Number of degrees of freedom: 7696

   NonLin Res     Lin Iter       Lin Res
   _____________________________________
   7.145e-03        0008        3.80e-13
   2.548e-05        0008        7.20e-16
   4.063e-09        0008        2.49e-19
   5.970e-16        (converged)

T=0.18
   Number of active cells:       1807
   Number of degrees of freedom: 7696

   NonLin Res     Lin Iter       Lin Res
   _____________________________________
   7.395e-03        0008        6.69e-13
   2.867e-05        0008        1.33e-15
   4.091e-09        0008        3.35e-19
   5.617e-16        (converged)
...
@endverbatim

This output reports the progress of the Newton iterations and the time
stepping. Note that our implementation of the Newton iteration indeed shows
the expected quadratic convergence order: the norm of the nonlinear residual
in each step is roughly the norm of the previous step squared. This leads to
the very rapid convergence we can see here. This holds until
times up to $t=1.9$ at which time the nonlinear iteration reports a
lack of convergence:
@verbatim
...

T=1.88
   Number of active cells:       2119
   Number of degrees of freedom: 9096

   NonLin Res     Lin Iter       Lin Res
   _____________________________________
   2.251e-01        0012        9.78e-12
   5.698e-03        0012        2.04e-13
   3.896e-05        0012        1.48e-15
   3.915e-09        0012        1.94e-19
   8.800e-16        (converged)

T=1.9
   Number of active cells:       2140
   Number of degrees of freedom: 9184

   NonLin Res     Lin Iter       Lin Res
   _____________________________________
   2.320e-01        0013        3.94e-12
   1.235e-01        0016        6.62e-12
   8.494e-02        0016        6.05e-12
   1.199e+01        0026        5.72e-10
   1.198e+03        0002        1.20e+03
   7.030e+03        0001        nan
   7.030e+03        0001        nan
   7.030e+03        0001        nan
   7.030e+03        0001        nan
   7.030e+03        0001        nan
   7.030e+03        0001        nan


----------------------------------------------------
Exception on processing:

--------------------------------------------------------
An error occurred in line <2476> of file <\step-33.cc> in function
    void Step33::ConservationLaw<dim>::run() [with int dim = 2]
The violated condition was:
    nonlin_iter <= 10
The name and call sequence of the exception was:
    ExcMessage ("No convergence in nonlinear solver")
Additional Information:
No convergence in nonlinear solver
--------------------------------------------------------

Aborting!
----------------------------------------------------
@endverbatim

We may find out the cause and possible remedies by looking at the animation of the solution.

The result of running these computations is a bunch of output files that we
can pass to our visualization program of choice. When we collate them into a
movie, the results of last several time steps looks like this:

<img src="https://www.dealii.org/images/steps/developer/step-33.oscillation.gif " alt="" height="300">

As we see, when the heavy mass of fluid hits the left bottom corner,
some oscillation occurs and lead to the divergence of the iteration. A lazy solution to
this issue is add more viscosity. If we set the diffusion power $\eta = 1.5$ instead of $2.0$,
the simulation would be able to survive this crisis. Then, the result looks like this:


<img src="https://www.dealii.org/images/steps/developer/step-33.slide.ed2.gif " alt="" height="300">

The heavy mass of fluid is drawn down the slope by gravity, where
it collides with the ski lodge and is flung into the air!  Hopefully everyone
escapes! And also, we can see the boundary between heavy mass and light mass blur quickly
due to the artificial viscosity.

We can also visualize the evolution of the adaptively refined grid:

<img src="https://www.dealii.org/images/steps/developer/step-33.slide.adapt.ed2.gif " alt="" height="300">

The adaptivity follows and precedes the flow pattern, based on the heuristic
refinement scheme discussed above.




<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

<h4>Stabilization</h4>

The numerical scheme we have chosen is not particularly
stable when the artificial viscosity is small while is too diffusive when
the artificial viscosity is large. Furthermore, it is known there are more
advanced techniques to stabilize the solution, for example streamline
diffusion, least-squares stabilization terms, entropy viscosity.



<h4>Better linear solvers</h4>

While the Newton method as a nonlinear solver appears to work very
well if the time step is small enough, the linear solver can be
improved. For example, in the current scheme whenever we use an
iterative solver, an ILU is computed anew for each Newton step;
likewise, for the direct solver, an LU decomposition of the Newton
matrix is computed in each step. This is obviously wasteful: from one
Newton step to another, and probably also between time steps, the
Newton matrix does not radically change: an ILU or a sparse LU
decomposition for one Newton step is probably still a very good
preconditioner for the next Newton or time step. Avoiding the
recomputation would therefore be a good way to reduce the amount of
compute time.

One could drive this a step further: since close to convergence the
Newton matrix changes only a little bit, one may be able to define a
quasi-Newton scheme where we only re-compute the residual (i.e. the
right hand side vector) in each Newton iteration, and re-use the
Newton matrix. The resulting scheme will likely not be of quadratic
convergence order, and we have to expect to do a few more nonlinear
iterations; however, given that we don't have to spend the time to
build the Newton matrix each time, the resulting scheme may well be
faster.


<h4>Cache the explicit part of residual</h4>

The residual calculated in ConservationLaw::assemble_cell_term function
reads
   $R_i = \left(\frac{\mathbf{w}^{k}_{n+1} - \mathbf{w}_n}{\delta t}
    , \mathbf{z}_i \right)_K  +
      \theta \mathbf{B}({\mathbf{w}^{k}_{n+1}})(\mathbf{z}_i)_K +
      (1-\theta) \mathbf{B}({\mathbf{w}_{n}}) (\mathbf{z}_i)_K $
This means that we calculate the spatial residual twice at one Newton
iteration step: once respect to the current solution $\mathbf{w}^{k}_{n+1}$
and once more respect to the last time step solution $\mathbf{w}_{n}$ which
remains the same during all Newton iterations through one timestep.
Cache up the explicit part of residual
 $ \mathbf{B}({\mathbf{w}_{n}}) (\mathbf{z}_i)_K$
during Newton iteration will save lots of labor.


<h4>Other conservation laws</h4>

Finally, as a direction beyond the immediate solution of the Euler
equations, this program tries very hard to separate the implementation
of everything that is specific to the Euler equations into one class
(the <code>EulerEquation</code> class), and everything that is
specific to assembling the matrices and vectors, nonlinear and linear
solvers, and the general top-level logic into another (the
<code>ConservationLaw</code> class).

By replacing the definitions of flux matrices and numerical fluxes in
this class, as well as the various other parts defined there, it
should be possible to apply the <code>ConservationLaw</code> class to
other hyperbolic conservation laws as well.


examples/step-34/doc/intro.dox
<br>

<i>This program was contributed by Luca Heltai (thanks to Michael
Gratton for pointing out what the exact solution should have been in
the three dimensional case).  </i>

@dealiiTutorialDOI{10.5281/zenodo.495473,https://zenodo.org/badge/DOI/10.5281/zenodo.495473.svg}

<a name="Intro"></a>

<h1>Introduction</h1>

<h3> Irrotational flow </h3>
The incompressible motion of an inviscid fluid past a body (for
example air past an airplane wing, or air or water past a propeller) is
usually modeled by the Euler equations of fluid dynamics:

\f{align*}
  \frac{\partial }{\partial t}\mathbf{v} + (\mathbf{v}\cdot\nabla)\mathbf{v}
  &=
  -\frac{1}{\rho}\nabla p + \mathbf{g}
  \qquad &\text{in } \mathbb{R}^n \backslash \Omega
  \\
  \nabla \cdot \mathbf{v}&=0
  &\text{in } \mathbb{R}^n\backslash\Omega
\f}
where the fluid density $\rho$ and the acceleration $\mathbf{g}$ due
to external forces are given and the velocity $\mathbf{v}$ and the
pressure $p$ are the unknowns. Here $\Omega$ is a closed bounded
region representing the body around which the fluid moves.

The above equations can be derived from Navier-Stokes equations
assuming that the effects due to viscosity are negligible compared to
those due to the pressure gradient, inertial forces and the external
forces. This is the opposite case of the Stokes equations discussed in
step-22 which are the limit case of dominant viscosity,
i.e. where the velocity is so small that inertia forces can be
neglected. On the other hand, owing to the assumed incompressibility,
the equations are not suited for very high speed gas flows where
compressibility and the equation of state of the gas have to be taken
into account, leading to the Euler equations of gas dynamics, a
hyperbolic system.

For the purpose of this tutorial program, we will consider only stationary
flow without external forces:
\f{align*}
  (\mathbf{v}\cdot\nabla)\mathbf{v}
  &=
  -\frac{1}{\rho}\nabla p
  \qquad &\text{in } \mathbb{R}^n \backslash \Omega
  \\
  \nabla \cdot \mathbf{v}&=0
  &\text{in } \mathbb{R}^n\backslash\Omega
\f}


Uniqueness of the solution of the Euler equations is ensured by adding the
boundary conditions
\f[
  \label{eq:boundary-conditions}
  \begin{aligned}
    \mathbf{n}\cdot\mathbf{v}& = 0 \qquad && \text{ on } \partial\Omega \\
    \mathbf{v}& = \mathbf{v}_\infty && \text{ when } |\mathbf{x}| \to \infty,
  \end{aligned}
\f]

which is to say that the body is at rest in our coordinate systems and
is not permeable, and that the fluid has (constant) velocity
$\mathbf{v}_\infty$ at infinity. An alternative viewpoint is that our
coordinate system moves along with the body whereas the background
fluid is at rest at infinity. Notice that we define the normal
$\mathbf{n}$ as the <i>outer</i> normal to the domain $\Omega$, which
is the opposite of the outer normal to the integration domain.

For both stationary and non stationary flow, the solution process
starts by solving for the velocity in the second equation and
substituting in the first equation in order to find the pressure.
The solution of the stationary Euler equations is typically performed
in order to understand the behavior of the given (possibly complex)
geometry when a prescribed motion is enforced on the system.

The first step in this process is to change the frame of reference from a
coordinate system moving along with the body to one in which the body moves
through a fluid that is at rest at infinity. This can be expressed by
introducing a new velocity $\mathbf{\tilde{v}}=\mathbf{v}-\mathbf{v}_\infty$ for
which we find that the same equations hold (because $\nabla\cdot
\mathbf{v}_\infty=0$) and we have boundary conditions
\f[
  \label{eq:boundary-conditions-tilde}
  \begin{aligned}
    \mathbf{n}\cdot\mathbf{\tilde{v}}& = -\mathbf{n}\cdot\mathbf{v}_\infty \qquad && \text{ on } \partial\Omega \\
    \mathbf{\tilde{v}}& = 0 && \text{ when } |\mathbf{x}| \to \infty,
  \end{aligned}
\f]

If we assume that the fluid is irrotational, i.e., $\nabla \times
\mathbf{v}=0$ in $\mathbb{R}^n\backslash\Omega$, we can represent the
velocity, and consequently also the perturbation velocity, as the
gradient of a scalar function:
\f[
  \mathbf{\tilde{v}}=\nabla\phi,
\f]
and so the second part of Euler equations above can be rewritten
as the homogeneous Laplace equation for the unknown $\phi$:
\f{align*}
\label{laplace}
\Delta\phi &= 0 \qquad &&\text{in}\ \mathbb{R}^n\backslash\Omega,
	   \\
	   \mathbf{n}\cdot\nabla\phi &= -\mathbf{n}\cdot\mathbf{v}_\infty
	   && \text{on}\ \partial\Omega
\f}
while the momentum equation reduces to Bernoulli's equation that expresses the
pressure $p$ as a function of the potential $\phi$:
\f[
\frac{p}{\rho} +\frac{1}{2} | \nabla \phi |^2 = 0 \in \Omega.
\f]

So we can solve the problem by solving the Laplace equation for the
potential.  We recall that the following functions, called fundamental
solutions of the Laplace equation,

\f[ \begin{aligned}
\label{eq:3} G(\mathbf{y}-\mathbf{x}) = &
-\frac{1}{2\pi}\ln|\mathbf{y}-\mathbf{x}| \qquad && \text{for } n=2 \\
G(\mathbf{y}-\mathbf{x}) = &
\frac{1}{4\pi}\frac{1}{|\mathbf{y}-\mathbf{x}|}&& \text{for } n=3,
\end{aligned}
\f]

satisfy in a distributional sense the equation:

\f[
-\Delta_y G(\mathbf{y}-\mathbf{x}) = \delta(\mathbf{y}-\mathbf{x}),
\f]

where the derivative is done in the variable $\mathbf{y}$. By using
the usual Green identities, our problem can be written on the boundary
$\partial\Omega = \Gamma$ only. We recall the general definition of
the second Green %identity:

\f[\label{green}
  \int_{\omega}
  (-\Delta u)v\,dx + \int_{\partial\omega} \frac{\partial u}{\partial \tilde{\mathbf{n}} }v \,ds
  =
  \int_{\omega}
  (-\Delta v)u\,dx + \int_{\partial\omega} u\frac{\partial v}{\partial \tilde{\mathbf{n}}} \,ds,
\f]

where $\tilde{\mathbf{n}}$ is the normal to the surface of $\omega$ pointing
outwards from the domain of integration $\omega$.

In our case the domain of integration is the domain
$\mathbb{R}^n\backslash\Omega$, whose boundary is $ \Gamma_\infty \cup
\Gamma$, where the "boundary" at infinity is defined as

\f[
\Gamma_\infty \dealcoloneq \lim_{r\to\infty} \partial B_r(0).
\f]

In our program the normals are defined as <i>outer</i> to the domain
$\Omega$, that is, they are in fact <i>inner</i> to the integration
domain, and some care is required in defining the various integrals
with the correct signs for the normals, i.e. replacing $\tilde{\mathbf{n}}$
by $-\mathbf{n}$.

If we substitute $u$ and $v$ in the Green %identity with the solution
$\phi$ and with the fundamental solution of the Laplace equation
respectively, as long as $\mathbf{x}$ is chosen in the region
$\mathbb{R}^n\backslash\Omega$, we obtain:
\f[
  \phi(\mathbf{x}) -
  \int_{\Gamma\cup\Gamma_\infty}\frac{\partial G(\mathbf{y}-\mathbf{x})}{\partial \mathbf{n}_y}\phi(\mathbf{y})\,ds_y
  =
  -\int_{\Gamma\cup\Gamma_\infty}G(\mathbf{y}-\mathbf{x})\frac{\partial \phi}{\partial \mathbf{n}_y}(\mathbf{y})\,ds_y
  \qquad \forall\mathbf{x}\in \mathbb{R}^n\backslash\Omega
\f]

where the normals are now pointing <i>inward</i> the domain of
integration.

Notice that in the above equation, we also have the integrals on the
portion of the boundary at $\Gamma_\infty$. Using the boundary
conditions of our problem, we have that $\nabla \phi$ is zero at
infinity (which simplifies the integral on $\Gamma_\infty$ on the
right hand side).

The integral on $\Gamma_\infty$ that appears on the left hand side can
be treated by observing that $\nabla\phi=0$ implies that $\phi$ at
infinity is necessarily constant. We define its value to be
$\phi_\infty$.  It is an easy exercise to prove that

\f[
-\int_{\Gamma_\infty} \frac{\partial G(\mathbf{y}-\mathbf{x})}
{\partial \mathbf{n}_y}\phi_\infty \,ds_y =
\lim_{r\to\infty} \int_{\partial B_r(0)} \frac{\mathbf{r}}{r} \cdot \nabla G(\mathbf{y}-\mathbf{x})
\phi_\infty \,ds_y = -\phi_\infty.
\f]

Using this result, we can reduce the above equation only on the
boundary $\Gamma$ using the so-called Single and Double Layer
Potential operators:

\f[\label{integral}
  \phi(\mathbf{x}) - (D\phi)(\mathbf{x}) = \phi_\infty
  -\left(S \frac{\partial \phi}{\partial n_y}\right)(\mathbf{x})
  \qquad \forall\mathbf{x}\in \mathbb{R}^n\backslash\Omega.
\f]

(The name of these operators comes from the fact that they describe the
electric potential in $\mathbb{R}^n$ due to a single thin sheet of charges
along a surface, and due to a double sheet of charges and anti-charges along
the surface, respectively.)

In our case, we know the Neumann values of $\phi$ on the boundary:
$\mathbf{n}\cdot\nabla\phi = -\mathbf{n}\cdot\mathbf{v}_\infty$.
Consequently,
\f[
  \phi(\mathbf{x}) - (D\phi)(\mathbf{x}) = \phi_\infty +
   \left(S[\mathbf{n}\cdot\mathbf{v}_\infty]\right)(\mathbf{x})
   \qquad \forall\mathbf{x} \in \mathbb{R}^n\backslash\Omega.
\f]
If we take the limit for $\mathbf{x}$ tending to $\Gamma$ of
the above equation, using well known properties of the single and double layer
operators, we obtain an equation for $\phi$ just on the boundary $\Gamma$ of
$\Omega$:

\f[\label{SD}
  \alpha(\mathbf{x})\phi(\mathbf{x}) - (D\phi)(\mathbf{x}) = \phi_\infty +
  \left(S [\mathbf{n}\cdot\mathbf{v}_\infty]\right)(\mathbf{x})
  \quad \mathbf{x}\in \partial\Omega,
\f]

which is the Boundary Integral Equation (BIE) we were looking for,
where the quantity $\alpha(\mathbf{x})$ is the fraction of angle or
solid angle by which the point $\mathbf{x}$ sees the domain of
integration $\mathbb{R}^n\backslash\Omega$.

In particular, at points $\mathbf{x}$ where the boundary
$\partial\Omega$ is differentiable (i.e. smooth) we have
$\alpha(\mathbf{x})=\frac 12$, but the value may be smaller or larger
at points where the boundary has a corner or an edge.

Substituting the single and double layer operators we get:
\f[
  \alpha(\mathbf{x}) \phi(\mathbf{x})
  + \frac{1}{2\pi}\int_{\partial \Omega}  \frac{
  (\mathbf{y}-\mathbf{x})\cdot\mathbf{n}_y  }{ |\mathbf{y}-\mathbf{x}|^2 }
  \phi(\mathbf{y}) \,ds_y
  = \phi_\infty
    -\frac{1}{2\pi}\int_{\partial \Omega}  \ln|\mathbf{y}-\mathbf{x}| \, \mathbf{n}\cdot\mathbf{v_\infty}\,ds_y
\f]
for two dimensional flows and
\f[
  \alpha(\mathbf{x}) \phi(\mathbf{x})
   + \frac{1}{4\pi}\int_{\partial \Omega} \frac{ (\mathbf{y}-\mathbf{x})\cdot\mathbf{n}_y  }{ |\mathbf{y}-\mathbf{x}|^3 }\phi(\mathbf{y})\,ds_y
  = \phi_\infty +
  \frac{1}{4\pi}\int_{\partial \Omega} \frac{1}{|\mathbf{y}-\mathbf{x}|} \, \mathbf{n}\cdot\mathbf{v_\infty}\,ds_y
\f]
for three dimensional flows, where the normal derivatives of the fundamental
solutions have been written in a form that makes computation easier. In either
case, $\phi$ is the solution of an integral equation posed entirely on the
boundary since both $\mathbf{x},\mathbf{y}\in\partial\Omega$.

Notice that the fraction of angle (in 2d) or solid angle (in 3d)
$\alpha(\mathbf{x})$ by which the point $\mathbf{x}$ sees the domain
$\Omega$ can be defined using the double layer potential itself:
\f[
\alpha(\mathbf{x}) \dealcoloneq 1 -
\frac{1}{2(n-1)\pi}\int_{\partial \Omega} \frac{ (\mathbf{y}-\mathbf{x})\cdot\mathbf{n}_y  }
{ |\mathbf{y}-\mathbf{x}|^{n} }\phi(\mathbf{y})\,ds_y = 1+
\int_{\partial \Omega} \frac{ \partial G(\mathbf{y}-\mathbf{x}) }{\partial \mathbf{n}_y} \, ds_y.
\f]

The reason why this is possible can be understood if we consider the
fact that the solution of a pure Neumann problem is known up to an
arbitrary constant $c$, which means that, if we set the Neumann data
to be zero, then any constant $\phi = \phi_\infty$ will be a solution.
Inserting the constant solution and the Neumann boundary condition in the
boundary integral equation, we have
@f{align*}
\alpha\left(\mathbf{x}\right)\phi\left(\mathbf{x}\right)
&=\int_{\Omega}\phi\left(\mathbf{y}\right)\delta\left(\mathbf{y}-\mathbf{x}\right)\, dy\\
\Rightarrow
\alpha\left(\mathbf{x}\right)\phi_\infty
&=\phi_\infty\int_{\Gamma\cup\Gamma_\infty}\frac{ \partial G(\mathbf{y}-\mathbf{x}) }{\partial \mathbf{n}_y} \, ds_y
=\phi_\infty\left[\int_{\Gamma_\infty}\frac{ \partial G(\mathbf{y}-\mathbf{x}) }{\partial \mathbf{n}_y} \, ds_y
+\int_{\Gamma}\frac{ \partial G(\mathbf{y}-\mathbf{x}) }{\partial \mathbf{n}_y} \, ds_y
\right]
@f}
The integral on $\Gamma_\infty$ is unity, see above, so division by the constant $\phi_\infty$ gives us the explicit
expression above for $\alpha(\mathbf{x})$.

While this example program is really only focused on the solution of the
boundary integral equation, in a realistic setup one would still need to solve
for the velocities. To this end, note that we have just computed
$\phi(\mathbf{x})$ for all $\mathbf{x}\in\partial\Omega$. In the next step, we
can compute (analytically, if we want) the solution $\phi(\mathbf{x})$ in all
of $\mathbb{R}^n\backslash\Omega$. To this end, recall that we had
\f[
  \phi(\mathbf{x})
  =
  \phi_\infty +
  (D\phi)(\mathbf{x})
  +
  \left(S[\mathbf{n}\cdot\mathbf{v}_\infty]\right)(\mathbf{x})
  \qquad \forall\mathbf{x}\in \mathbb{R}^n\backslash\Omega.
\f]
where now we have everything that is on the right hand side ($S$ and $D$ are
integrals we can evaluate, the normal velocity on the boundary is given, and
$\phi$ on the boundary we have just computed). Finally, we can then recover
the velocity as $\mathbf{\tilde v}=\nabla \phi$.

Notice that the evaluation of the above formula for $\mathbf{x} \in
\Omega$ should yield zero as a result, since the integration of the
Dirac delta $\delta(\mathbf{x})$ in the domain
$\mathbb{R}^n\backslash\Omega$ is always zero by definition.

As a final test, let us verify that this velocity indeed satisfies the
momentum balance equation for a stationary flow field, i.e., whether
$\mathbf{v}\cdot\nabla\mathbf{v} = -\frac 1\rho \nabla p$ where
$\mathbf{v}=\mathbf{\tilde
v}+\mathbf{v}_\infty=\nabla\phi+\mathbf{v}_\infty$ for some (unknown) pressure
$p$ and a given constant $\rho$. In other words, we would like to verify that
Bernoulli's law as stated above indeed holds. To show this, we use that
the left hand side of this equation equates to
@f{align*}
  \mathbf{v}\cdot\nabla\mathbf{v}
  &=
  [(\nabla\phi+\mathbf{v}_\infty)\cdot\nabla] (\nabla\phi+\mathbf{v}_\infty)
  \\
  &=
  [(\nabla\phi+\mathbf{v}_\infty)\cdot\nabla] (\nabla\phi)
@f}
where we have used that $\mathbf{v}_\infty$ is constant. We would like to
write this expression as the gradient of something (remember that $\rho$ is a
constant). The next step is more
convenient if we consider the components of the equation individually
(summation over indices that appear twice is implied):
@f{align*}
  [\mathbf{v}\cdot\nabla\mathbf{v}]_i
  &=
  (\partial_j\phi+v_{\infty,j}) \partial_j \partial_i\phi
  \\
  &=
  \partial_j [(\partial_j\phi+v_{\infty,j}) \partial_i\phi]
  -
  \partial_j [(\partial_j\phi+v_{\infty,j})] \partial_i\phi
  \\
  &=
  \partial_j [(\partial_j\phi+v_{\infty,j}) \partial_i\phi]
@f}
because $\partial_j \partial_j\phi = \Delta \phi = 0$ and $\textrm{div}
\ \mathbf{v}_\infty=0$. Next,
@f{align*}
  [\mathbf{v}\cdot\nabla\mathbf{v}]_i
  &=
  \partial_j [(\partial_j\phi+v_{\infty,j}) \partial_i\phi]
  \\
  &=
  \partial_j [(\partial_j\phi) (\partial_i\phi)]
  +
  \partial_j [v_{\infty,j} \partial_i\phi]
  \\
  &=
  \partial_j [(\partial_j\phi) (\partial_i\phi)]
  +
  \partial_j [v_{\infty,j}] \partial_i\phi
  +
  v_{\infty,j} \partial_j \partial_i\phi
  \\
  &=
  \partial_j [(\partial_j\phi) (\partial_i\phi)]
  +
  v_{\infty,j} \partial_j \partial_i\phi
  \\
  &=
  \partial_i \partial_j [(\partial_j\phi) \phi]
  -
  \partial_j [\partial_i (\partial_j\phi) \phi]
  +
  \partial_i [v_{\infty,j} \partial_j \phi]
  -
  \partial_i [v_{\infty,j}] \partial_j \phi
@f}
Again, the last term disappears because $\mathbf{v}_\infty$ is constant and we
can merge the first and third term into one:
@f{align*}
  [\mathbf{v}\cdot\nabla\mathbf{v}]_i
  &=
  \partial_i (\partial_j [(\partial_j\phi) \phi + v_{\infty,j} \partial_j \phi])
  -
  \partial_j [\partial_i (\partial_j\phi) \phi]
  \\
  &=
  \partial_i [(\partial_j\phi)(\partial_j \phi) + v_{\infty,j} \partial_j \phi]
  -
  \partial_j [\partial_i (\partial_j\phi) \phi]
@f}

We now only need to massage that last term a bit more. Using the product rule,
we get
@f{align*}
  \partial_j [\partial_i (\partial_j\phi) \phi]
  &=
  \partial_i [\partial_j \partial_j\phi] \phi
  +
  \partial_i [\partial_j \phi] (\partial_j \phi).
@f}
The first of these terms is zero (because, again, the summation over $j$ gives
$\Delta\phi$, which is zero). The last term can be written as $\frac 12
\partial_i [(\partial_j\phi)(\partial_j\phi)]$ which is in the desired gradient
form. As a consequence, we can now finally state that
@f{align*}
  [\mathbf{v}\cdot\nabla\mathbf{v}]_i
  &=
  \partial_i (\partial_j [(\partial_j\phi) \phi + v_{\infty,j} \partial_j \phi])
  -
  \partial_j [\partial_i (\partial_j\phi) \phi]
  \\
  &=
  \partial_i
  \left[
    (\partial_j\phi)(\partial_j \phi) + v_{\infty,j} \partial_j \phi
    -
    \frac 12 (\partial_j\phi)(\partial_j\phi)
  \right],
  \\
  &=
  \partial_i
  \left[
    \frac 12 (\partial_j\phi)(\partial_j \phi) + v_{\infty,j} \partial_j \phi
  \right],
@f}
or in vector form:
@f[
  \mathbf{v}\cdot\nabla\mathbf{v}
  =
  \nabla
  \left[
    \frac 12 \mathbf{\tilde v}^2
    + \mathbf{v}_{\infty} \cdot \mathbf{\tilde v}
  \right],
@f]
or in other words:
@f[
  p
  =
  -\rho
  \left[
    \frac 12 \mathbf{\tilde v}^2
    + \mathbf{v}_{\infty} \cdot \mathbf{\tilde v}
  \right]
  =
  -\rho
  \left[
    \frac 12 \mathbf{v}^2
    -
    \frac 12 \mathbf{v}_{\infty}^2
  \right]
  .
@f]
Because the pressure is only determined up to a constant (it appears only with
a gradient in the equations), an equally valid definition is
@f[
  p
  =
  -\frac 12 \rho \mathbf{v}^2
  .
@f]
This is exactly Bernoulli's law mentioned above.


<h3>The numerical approximation</h3>

Numerical approximations of Boundary Integral Equations (BIE) are commonly
referred to as the boundary element method or panel method (the latter
expression being used mostly in the computational fluid dynamics community).
The goal of the following test problem is to solve the integral
formulation of the Laplace equation with Neumann boundary conditions,
using a circle and a sphere respectively in two and three space
dimensions, illustrating along the way the features that allow one to
treat boundary element problems almost as easily as finite element
problems using the deal.II library.

To this end, let $\mathcal{T}_h = \bigcup_i K_i$ be a subdivision of the
manifold $\Gamma = \partial \Omega$ into $M$ line segments if $n=2$, or $M$
quadrilaterals if $n=3$. We will call each individual segment or
quadrilateral an <i>element</i> or <i>cell</i>, independently of the
dimension $n$ of the surrounding space $\mathbb{R}^n$.
We define the finite dimensional space $V_h$ as
\f[
  \label{eq:definition-Vh}
  V_h \dealcoloneq \{ v \in C^0(\Gamma) \text{ s.t. } v|_{K_i} \in \mathcal{Q}^1(K_i),
  \forall i\},
\f]
with basis functions $\psi_i(\mathbf{x})$ for which we will use the usual FE_Q
finite element, with the catch that this time it is defined on a manifold of
codimension one (which we do by using the second template argument that is
usually defaulted to equal the first; here, we will create objects
<code>FE_Q@<dim-1,dim@></code> to indicate that we have <code>dim-1</code>
dimensional cells in a <code>dim</code> dimensional space).
An element $\phi_h$ of $V_h$ is uniquely
identified by the vector $\boldsymbol{\phi}$ of its coefficients
$\phi_i$, that is:
\f[
  \label{eq:definition-of-element}
  \phi_h(\mathbf{x}) \dealcoloneq \phi_i \psi_i(\mathbf{x}), \qquad
  \boldsymbol{\phi} \dealcoloneq \{ \phi_i \},
\f]
where summation  is implied over repeated indexes. Note that we could use
discontinuous elements here &mdash; in fact, there is no real reason to use
continuous ones since the integral formulation does not
imply any derivatives on our trial functions so continuity is unnecessary,
and often in the literature only piecewise constant elements are used.

<h3> Collocation boundary element method </h3>

By far, the most common approximation of boundary integral equations
is by use of the collocation based boundary element method.

This method requires the evaluation of the boundary integral equation
at a number of collocation points which is equal to the number of
unknowns of the system. The choice of these points is a delicate
matter, that requires a careful study. Assume that these points are
known for the moment, and call them $\mathbf x_i$ with $i=0...n\_dofs$.

The problem then becomes:
Given the datum $\mathbf{v}_\infty$, find a function $\phi_h$ in $V_h$
such that the following $n\_dofs$ equations are satisfied:

\f{align*}
    \alpha(\mathbf{x}_i) \phi_h(\mathbf{x}_i)
    - \int_{\Gamma_y} \frac{ \partial G(\mathbf{y}-\mathbf{x}_i)}{\partial\mathbf{n}_y }
    \phi_h(\mathbf{y}) \,ds_y =
    \int_{\Gamma_y} G(\mathbf{y}-\mathbf{x}_i) \,
    \mathbf{n}_y\cdot\mathbf{v_\infty} \,ds_y
    ,
\f}

where the quantity $\alpha(\mathbf{x}_i)$ is the fraction of (solid)
angle by which the point $\mathbf{x}_i$ sees the domain $\Omega$, as
explained above, and we set $\phi_\infty$ to be zero.  If the support
points $\mathbf{x}_i$ are chosen appropriately, then the problem can
be written as the following linear system:

\f[
\label{eq:linear-system}
(\mathbf{A}+\mathbf{N})\boldsymbol\phi = \mathbf{b},
\f]

where

\f[
\begin{aligned}
\mathbf{A}_{ij}&=
\alpha(\mathbf{x}_i) \psi_j(\mathbf{x}_i)
= 1+\int_\Gamma
\frac{\partial G(\mathbf{y}-\mathbf{x}_i)}{\partial \mathbf{n}_y}\,ds_y
\psi_j(\mathbf{x}_i)
\\
\mathbf{N}_{ij}&= - \int_\Gamma
  \frac{\partial G(\mathbf{y}-\mathbf{x}_i)}{\partial \mathbf{n}_y}
  \psi_j(\mathbf{y}) \,ds_y
\\
\mathbf{b}_i&= \int_\Gamma
   G(\mathbf{y}-\mathbf{x}_i)  \, \mathbf{n}_y\cdot\mathbf{v_\infty}
   ds_y.
\end{aligned}
\f]

From a linear algebra point of view, the best possible choice of the
collocation points is the one that renders the matrix
$\mathbf{A}+\mathbf{N}$ the most diagonally dominant. A natural choice
is then to select the $\mathbf{x}_i$ collocation points to be the
support points of the nodal basis functions $\psi_i(\mathbf{x})$. In that
case, $\psi_j(\mathbf{x}_i)=\delta_{ij}$, and as a consequence the matrix
$\mathbf{A}$ is diagonal with entries
\f[
  \mathbf{A}_{ii}
  =
  1+\int_\Gamma
  \frac{\partial G(\mathbf{y}-\mathbf{x}_i)}{\partial \mathbf{n}_y}\,ds_y
  =
  1-\sum_j N_{ij},
\f]
where we have used that $\sum_j \psi_j(\mathbf{y})=1$ for the usual Lagrange
elements.
With this choice of collocation points, the computation of the entries
of the matrices $\mathbf{A}$, $\mathbf{N}$ and of the right hand side
$\mathbf{b}$ requires the evaluation of singular integrals on the
elements $K_i$ of the triangulation $\mathcal{T}_h$.
As usual in these cases, all integrations are performed on a reference
simple domain, i.e., we assume that each element $K_i$ of
$\mathcal{T}_h$ can be expressed as a linear (in two dimensions) or
bi-linear (in three dimensions) transformation of the reference
boundary element $\hat K \dealcoloneq [0,1]^{n-1}$, and we perform the integrations after a
change of variables from the real element $K_i$ to the reference
element $\hat K$.

<h3> Treating the singular integrals. </h3>

In two dimensions it is not necessary to compute the diagonal elements
$\mathbf{N}_{ii}$ of the system matrix, since, even if the denominator
goes to zero when $\mathbf{x}=\mathbf{y}$, the numerator is always
zero because $\mathbf{n}_y$ and $(\mathbf{y}-\mathbf{x})$ are
orthogonal (on our polygonal approximation of the boundary of $\Omega$), and
the only singular integral arises in the computation
of $\mathbf{b}_i$ on the i-th element of $\mathcal{T}_h$:
\f[
  \frac{1}{\pi}
  \int_{K_i}
  \ln|\mathbf{y}-\mathbf{x}_i| \, \mathbf{n}_y\cdot\mathbf{v_\infty} \,ds_y.
\f]

This can be easily treated by the QGaussLogR quadrature
formula.

Similarly, it is possible to use the QGaussOneOverR quadrature formula
to perform the singular integrations in three dimensions. The
interested reader will find detailed explanations on how these
quadrature rules work in their documentation.

The resulting matrix $\mathbf{A}+\mathbf{N}$ is full. Depending on its
size, it might be convenient to use a direct solver or an iterative
one. For the purpose of this example code, we chose to use only an
iterative solver, without providing any preconditioner.

If this were a production code rather than a demonstration of principles,
there are techniques that are available to not store full matrices but instead
store only those entries that are large and/or relevant. In the literature on
boundary element methods, a plethora of methods is available that allows to
determine which elements are important and which are not, leading to a
significantly sparser representation of these matrices that also facilitates
rapid evaluations of the scalar product between vectors and matrices. This not
being the goal of this program, we leave this for more sophisticated
implementations.


<h3>Implementation</h3>

The implementation is rather straight forward. The main point that hasn't been
used in any of the previous tutorial programs is that most classes in deal.II
are not only templated on the dimension, but in fact on the dimension of the
manifold on which we pose the differential equation as well as the dimension
of the space into which this manifold is embedded. By default, the second
template argument equals the first, meaning for example that we want to solve
on a two-dimensional region of two-dimensional space. The triangulation class
to use in this case would be <code>Triangulation@<2@></code>, which is an
equivalent way of writing <code>Triangulation@<2,2@></code>.

However, this doesn't have to be so: in the current example, we will for
example want to solve on the surface of a sphere, which is a two-dimensional
manifold embedded in a three-dimensional space. Consequently, the right class
will be <code>Triangulation@<2,3@></code>, and correspondingly we will use
<code>DoFHandler@<2,3@></code> as the DoF handler class and
<code>FE_Q@<2,3@></code> for finite elements.

Some further details on what one can do with things that live on
curved manifolds can be found in the report
<a target="_top"
href="http://www.dealii.org/reports/codimension-one/desimone-heltai-manigrasso.pdf"><i>Tools
for the Solution of PDEs Defined on Curved Manifolds with the deal.II
Library</i> by A. DeSimone, L. Heltai, C. Manigrasso</a>. In addition, the
step-38 tutorial program extends what we show here to cases where the equation
posed on the manifold is not an integral operator but in fact involves
derivatives.


<h3>Testcase</h3>

The testcase we will be solving is for a circular (in 2d) or spherical
(in 3d) obstacle. Meshes for these geometries will be read in from
files in the current directory and an object of type SphericalManifold
will then be attached to the triangulation to allow mesh refinement
that respects the continuous geometry behind the discrete initial
mesh.

For a sphere of radius $a$ translating at a velocity of $U$ in the $x$ direction, the potential reads
@f{align*}
\phi = -\frac{1}{2}U \left(\frac{a}{r}\right)3 r \cos\theta
@f}
see, e.g. J. N. Newman, <i>Marine Hydrodynamics</i>, 1977,
pp. 127. For unit speed and radius, and restricting $(x,y,z)$ to lie
on the surface of the sphere,
$\phi = -x/2$. In the test problem,
the flow is $(1,1,1)$, so the appropriate exact solution on the
surface of the sphere is the superposition of the above solution with
the analogous solution along the $y$ and $z$ axes, or $\phi =
\frac{1}{2}(x + y + z)$.


examples/step-34/doc/results.dox
<h1>Results</h1>

We ran the program using the following <code>parameters.prm</code> file (which
can also be found in the directory in which all the other source files are):
@verbatim
# Listing of Parameters
# ---------------------
set Extend solution on the -2,2 box = true
set External refinement             = 5
set Number of cycles                = 4
set Run 2d simulation               = true
set Run 3d simulation               = true


subsection Exact solution 2d
  # Any constant used inside the function which is not a variable name.
  set Function constants  =

  # Separate vector valued expressions by ';' as ',' is used internally by the
  # function parser.
  set Function expression = x+y   # default: 0

  # The name of the variables as they will be used in the function, separated
  # by ','.
  set Variable names      = x,y,t
end


subsection Exact solution 3d
  # Any constant used inside the function which is not a variable name.
  set Function constants  =

  # Separate vector valued expressions by ';' as ',' is used internally by the
  # function parser.
  set Function expression = .5*(x+y+z)   # default: 0

  # The name of the variables as they will be used in the function, separated
  # by ','.
  set Variable names      = x,y,z,t
end


subsection Quadrature rules
  set Quadrature order          = 4
  set Quadrature type           = gauss
  set Singular quadrature order = 5
end


subsection Solver
  set Log frequency = 1
  set Log history   = false
  set Log result    = true
  set Max steps     = 100
  set Tolerance     = 1.e-10
end


subsection Wind function 2d
  # Any constant used inside the function which is not a variable name.
  set Function constants  =

  # Separate vector valued expressions by ';' as ',' is used internally by the
  # function parser.
  set Function expression = 1; 1  # default: 0; 0

  # The name of the variables as they will be used in the function, separated
  # by ','.
  set Variable names      = x,y,t
end


subsection Wind function 3d
  # Any constant used inside the function which is not a variable name.
  set Function constants  =

  # Separate vector valued expressions by ';' as ',' is used internally by the
  # function parser.
  set Function expression = 1; 1; 1 # default: 0; 0; 0

  # The name of the variables as they will be used in the function, separated
  # by ','.
  set Variable names      = x,y,z,t
end
@endverbatim

When we run the program, the following is printed on screen:
@verbatim
DEAL::
DEAL::Parsing parameter file parameters.prm
DEAL::for a 2 dimensional simulation.
DEAL:GMRES::Starting value 2.21576
DEAL:GMRES::Convergence step 1 value 2.37635e-13
DEAL::Cycle 0:
DEAL::   Number of active cells:       20
DEAL::   Number of degrees of freedom: 20
DEAL:GMRES::Starting value 3.15543
DEAL:GMRES::Convergence step 1 value 2.89310e-13
DEAL::Cycle 1:
DEAL::   Number of active cells:       40
DEAL::   Number of degrees of freedom: 40
DEAL:GMRES::Starting value 4.46977
DEAL:GMRES::Convergence step 1 value 3.11815e-13
DEAL::Cycle 2:
DEAL::   Number of active cells:       80
DEAL::   Number of degrees of freedom: 80
DEAL:GMRES::Starting value 6.32373
DEAL:GMRES::Convergence step 1 value 3.22474e-13
DEAL::Cycle 3:
DEAL::   Number of active cells:       160
DEAL::   Number of degrees of freedom: 160
DEAL::
cycle cells dofs    L2(phi)     Linfty(alpha)
    0    20   20 4.465e-02    - 5.000e-02    -
    1    40   40 1.081e-02 2.05 2.500e-02 1.00
    2    80   80 2.644e-03 2.03 1.250e-02 1.00
    3   160  160 6.529e-04 2.02 6.250e-03 1.00
DEAL::
DEAL::Parsing parameter file parameters.prm
DEAL::for a 3 dimensional simulation.
DEAL:GMRES::Starting value 2.84666
DEAL:GMRES::Convergence step 3 value 8.68638e-18
DEAL::Cycle 0:
DEAL::   Number of active cells:       24
DEAL::   Number of degrees of freedom: 26
DEAL:GMRES::Starting value 6.34288
DEAL:GMRES::Convergence step 5 value 1.38740e-11
DEAL::Cycle 1:
DEAL::   Number of active cells:       96
DEAL::   Number of degrees of freedom: 98
DEAL:GMRES::Starting value 12.9780
DEAL:GMRES::Convergence step 5 value 3.29225e-11
DEAL::Cycle 2:
DEAL::   Number of active cells:       384
DEAL::   Number of degrees of freedom: 386
DEAL:GMRES::Starting value 26.0874
DEAL:GMRES::Convergence step 6 value 1.47271e-12
DEAL::Cycle 3:
DEAL::   Number of active cells:       1536
DEAL::   Number of degrees of freedom: 1538
DEAL::
cycle cells dofs    L2(phi)     Linfty(alpha)
    0    24   26 3.437e-01    - 2.327e-01    -
    1    96   98 9.794e-02 1.81 1.239e-01 0.91
    2   384  386 2.417e-02 2.02 6.319e-02 0.97
    3  1536 1538 5.876e-03 2.04 3.176e-02 0.99
@endverbatim

As we can see from the convergence table in 2d, if we choose
quadrature formulas which are accurate enough, then the error we
obtain for $\alpha(\mathbf{x})$ should be exactly the inverse of the
number of elements. The approximation of the circle with N segments of
equal size generates a regular polygon with N faces, whose angles are
exactly $\pi-\frac {2\pi}{N}$, therefore the error we commit should be
exactly $\frac 12 - (\frac 12 -\frac 1N) = \frac 1N$. In fact this is
a very good indicator that we are performing the singular integrals in
an appropriate manner.

The error in the approximation of the potential $\phi$ is largely due
to approximation of the domain. A much better approximation could be
obtained by using higher order mappings.

If we modify the main() function, setting fe_degree and mapping_degree
to two, and raise the order of the quadrature formulas  in
the parameter file, we obtain the following convergence table for the
two dimensional simulation

@verbatim
cycle cells dofs    L2(phi)     Linfty(alpha)
    0    20   40 5.414e-05    - 2.306e-04    -
    1    40   80 3.623e-06 3.90 1.737e-05 3.73
    2    80  160 2.690e-07 3.75 1.253e-05 0.47
    3   160  320 2.916e-08 3.21 7.670e-06 0.71
@endverbatim

and

@verbatim
cycle cells dofs    L2(phi)     Linfty(alpha)
    0    24   98 3.770e-03    - 8.956e-03    -
    1    96  386 1.804e-04 4.39 1.182e-03 2.92
    2   384 1538 9.557e-06 4.24 1.499e-04 2.98
    3  1536 6146 6.617e-07 3.85 1.892e-05 2.99
@endverbatim

for the three dimensional case. As we can see, convergence results are
much better with higher order mapping, mainly due to a better
resolution of the curved geometry. Notice that, given the same number
of degrees of freedom, for example in step 3 of the Q1 case and step 2
of Q2 case in the three dimensional simulation, the error is roughly
three orders of magnitude lower.

The result of running these computations is a bunch of output files that we
can pass to our visualization program of choice.
The output files are of two kind: the potential on the boundary
element surface, and the potential extended to the outer and inner
domain. The combination of the two for the two dimensional case looks
like

<img src="https://www.dealii.org/images/steps/developer/step-34_2d.png" alt="">

while in three dimensions we show first the potential on the surface,
together with a contour plot,

<img src="https://www.dealii.org/images/steps/developer/step-34_3d.png" alt="">

and then the external contour plot of the potential, with opacity set to 25%:

<img src="https://www.dealii.org/images/steps/developer/step-34_3d-2.png" alt="">


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

This is the first tutorial program that considers solving equations defined on
surfaces embedded in higher dimensional spaces. But the equation discussed
here was relatively simple because it only involved an integral operator, not
derivatives which are more difficult to define on the surface. The step-38
tutorial program considers such problems and provides the necessary tools.

From a practical perspective, the Boundary Element Method (BEM) used
here suffers from two bottlenecks. The first is that assembling the
matrix has a cost that is *quadratic* in the number of unknowns, that
is ${\cal O}(N^2)$ where $N$ is the total number of unknowns. This can
be seen by looking at the `assemble_system()` function, which has this
structure:
@code
    for (const auto &cell : dof_handler.active_cell_iterators())
      {
        ...

        for (unsigned int i = 0; i < dof_handler.n_dofs(); ++i)
          ...
@endcode
Here, the first loop walks over all cells (one factor of $N$) whereas
the inner loop contributes another factor of $N$.

This has to be contrasted with the finite element method for *local*
differential operators: There, we loop over all cells (one factor of
$N$) and on each cell do an amount of work that is independent of how
many cells or unknowns there are. This clearly presents a
bottleneck.

The second bottleneck is that the system matrix is dense (i.e., is of
type FullMatrix) because every degree of freedom couples with every
other degree of freedom. As pointed out above, just *computing* this
matrix with its $N^2$ nonzero entries necessarily requires at least
${\cal O}(N^2)$ operations, but it's worth pointing out that it also
costs this many operations to just do one matrix-vector product. If
the GMRES method used to solve the linear system requires a number of
iterations that grows with the size of the problem, as is typically
the case, then solving the linear system will require a number of
operations that grows even faster than just ${\cal O}(N^2)$.

"Real" boundary element methods address these issues by strategies
that determine which entries of the matrix will be small and can
consequently be neglected (at the cost of introducing an additional
error, of course). This is possible by recognizing that the matrix
entries decay with the (physical) distance between the locations where
degrees of freedom $i$ and $j$ are defined. This can be exploited in
methods such as the Fast Multipole Method (FMM) that control which
matrix entries must be stored and computed to achieve a certain
accuracy, and -- if done right -- result in methods in which both
assembly and solution of the linear system requires less than
${\cal O}(N^2)$ operations.

Implementing these methods clearly presents opportunities to extend
the current program.


examples/step-35/doc/intro.dox
<br>

<i>
This program grew out of a student project by Abner Salgado at Texas A&M
University. Most of the work for this program is by him.
</i>

<a name="Intro"></a>
<h1> Introduction </h1>

<a name="Motivation"></a>
<h3> Motivation </h3>
The purpose of this program is to show how to effectively solve the incompressible time-dependent
Navier-Stokes equations. These equations describe the flow of a viscous incompressible fluid and read
@f{align*}
  u_t + u \cdot \nabla u - \nu \Delta u + \nabla p = f, \\
  \nabla \cdot u = 0,
@f}
where $u$ represents the velocity of the flow and $p$ the pressure. This system of equations is supplemented by
the initial condition
@f[
  u |_{t=0} = u_0,
@f]
with $u_0$ sufficiently smooth and solenoidal, and suitable boundary conditions. For instance, an admissible boundary
condition, is
@f[
  u|_{\partial\Omega} = u_b.
@f]
It is possible to prescribe other boundary conditions as well. In the test case that we solve here the boundary
is partitioned into two disjoint subsets $\partial\Omega = \Gamma_1 \cup \Gamma_2$ and we have
@f[
  u|_{\Gamma_1} = u_b,
@f]
and
@f[
 u\times n|_{\Gamma_2} = 0, \quad p|_{\Gamma_2} = 0
@f]
where $n$ is the outer unit normal. The boundary conditions on $\Gamma_2$ are often
used to model outflow conditions.

In previous tutorial programs (see for instance step-20 and
step-22) we have seen
how to solve the time-independent Stokes equations using a Schur complement approach. For the
time-dependent case, after time discretization, we would arrive at a system like
@f{align*}
  \frac1\tau u^k - \nu \Delta u^k + \nabla p^k = F^k, \\
  \nabla \cdot u^k = 0,
@f}
where $\tau$ is the time-step. Although the structure of this system is similar to the Stokes system and thus
it could be solved using a Schur complement approach, it turns out that the condition number of the
Schur complement is proportional to $\tau^{-2}$. This makes the system very
difficult to solve, and means that for the Navier-Stokes equations, this is
not a useful avenue to the solution.

<a name="Projection"></a>
<h3> Projection methods </h3>

Rather, we need to come up with a different approach to solve the time-dependent Navier-Stokes
equations. The difficulty in their solution comes from the fact that the velocity and the pressure are coupled
through the constraint
@f[
  \nabla \cdot u = 0,
@f]
for which the pressure is the Lagrange multiplier.
Projection methods aim at decoupling this constraint from the diffusion (Laplace) operator.

Let us shortly describe how the projection methods look like in a semi-discrete setting. The objective is to
obtain a sequence of velocities $\{u^k\}$ and pressures $\{p^k\}$. We will
also obtain a sequence $\{\phi^k\}$ of auxiliary variables.
Suppose that from the initial conditions, and an application of a first order method we have found
$(u^0,p^0,\phi^0=0)$ and $(u^1,p^1,\phi^1=p^1-p^0)$. Then the projection method consists of the following steps:
<ul>
  <li> <b>Step 0</b>: Extrapolation. Define:
  @f[
    u^\star = 2u^k - u^{k-1}, \quad p^\sharp = p^k + \frac43 \phi^k - \frac13 \phi^{k-1}.
  @f]
  <li> <b>Step 1</b>: Diffusion step. We find $u^{k+1}$ that solves the single
  linear equation
  @f[
    \frac1{2\tau}\left( 3u^{k+1} - 4u^k + u^{k-1} \right)
    + u^\star \cdot\nabla u^{k+1} + \frac12 \left( \nabla \cdot u^\star \right) u^{k+1}
    -\nu \Delta u^{k+1} + \nabla p^\sharp
    = f^{k+1},
    \quad
    u^{k+1}|_{\Gamma_1} = u_b,
    \quad
    u^{k+1} \times n|_{\Gamma_2} = 0.
  @f]

  <li> <b>Step 2</b>: Projection. Find $\phi^{k+1}$ that solves
  @f[
    \Delta \phi^{k+1} = \frac3{2\tau} \nabla \cdot u^{k+1},
    \quad
    \partial_n \phi^{k+1}|_{\Gamma_1} = 0,
    \quad
    \phi^{k+1}|_{\Gamma_2} = 0
  @f]
  <li> <b>Step 3</b>: Pressure correction. Here we have two options:
    <ul>
      <li> <i>Incremental Method in Standard Form</i>. The pressure is updated by:
      @f[
        p^{k+1} = p^k + \phi^{k+1}.
      @f]
      <li> <i>Incremental Method in Rotational Form</i>. In this case
      @f[
        p^{k+1} = p^k + \phi^{k+1} - \nu \nabla \cdot u^{k+1}.
      @f]
    </ul>
</ul>

Without going into details, let us remark a few things about the projection methods that we have just described:
<ul>
  <li> The advection term $u\cdot\nabla u$ is replaced by its <i>skew symmetric form</i>
  @f[
    u \cdot \nabla u + \frac12 \left( \nabla\cdot u \right) u.
  @f]
  This is consistent with the continuous equation (because $\nabla\cdot u = 0$,
  though this is not true pointwise for the discrete solution) and it is needed to
  guarantee unconditional stability of the
  time-stepping scheme. Moreover, to linearize the term we use the second order extrapolation $u^\star$ of
  $u^{k+1}$.
  <li> The projection step is a realization of the Helmholtz decomposition
  @f[
    L^2(\Omega)^d = H \oplus \nabla H^1_{\Gamma_2}(\Omega),
  @f]
  where
  @f[
    H = \left\{ v \in L^2(\Omega)^d:\  \nabla\cdot v =0, \  v\cdot n|_{\Gamma_1} = 0 \right\},
  @f]
  and
  @f[
    H^1_{\Gamma_2}(\Omega) = \left\{ q \in H^1(\Omega):\ q|_{\Gamma_2} = 0 \right\}.
  @f]
  Indeed, if we use this decomposition on $u^{k+1}$ we obtain
  @f[
    u^{k+1} = v^{k+1} + \nabla \left( \frac{2\tau}{3}  \phi^{k+1} \right),
  @f]
  with $v^{k+1}\in H$. Taking the divergence of this equation we arrive at the projection equation.
  <li> The more accurate of the two variants outlined above is the rotational
  one. However, the program below implements both variants. Moreover, in the author's experience,
  the standard form is the one that should be used if, for instance, the viscosity $\nu$ is variable.
</ul>


<p>
The standard incremental scheme and the rotational incremental scheme were first considered by van Kan in
<ul>
  <li> J. van Kan, "A second-order accurate pressure-correction scheme for viscous incompressible flow",
       SIAM Journal on Scientific and Statistical Computing, vol. 7, no. 3, pp. 870–891, 1986
</ul>
and is analyzed by Guermond in
<ul>
  <li> J.-L. Guermond, "Un résultat de convergence d’ordre deux en temps pour
                        l’approximation des équations de Navier–Stokes par une technique de projection incrémentale",
       ESAIM: Mathematical Modelling and Numerical Analysis, vol. 33, no. 1, pp. 169–189, 1999
</ul>
for the case $\nu = 1$.
It turns out that this technique suffers from unphysical boundary conditions for the kinematic pressure that
lead to reduced rates of convergence. To prevent this, Timmermans et al. proposed in
<ul>
  <li> L. Timmermans, P. Minev, and F. Van De Vosse,
       "An approximate projection scheme for incompressible flow using spectral elements",
       International Journal for Numerical Methods in Fluids, vol. 22, no. 7, pp. 673–688, 1996
</ul>
the rotational pressure-correction projection method that uses a divergence correction for the kinematic pressure.
A thorough analysis for scheme has first been performed in
<ul>
  <li> J.-L. Guermond and J. Shen, "On the error estimates for the rotational pressure-correction projection methods",
       Mathematics of Computation, vol. 73, no. 248, pp. 1719–1737, 2004
</ul>
for the Stokes problem.
</p>

<a name ="fullydiscrete"></a>
<h3> The Fully Discrete Setting </h3>
To obtain a fully discrete setting of the method we, as always, need a variational formulation. There is one
subtle issue here given the nature of the boundary conditions. When we multiply the equation by a suitable test
function one of the term that arises is
@f[
  -\nu \int_\Omega \Delta u \cdot v.
@f]
If we, say, had Dirichlet boundary conditions on the whole boundary then after integration by parts we would
obtain
@f[
  -\nu \int_\Omega \Delta u \cdot v = \nu \int_\Omega \nabla u : \nabla v
                                    - \int_{\partial\Omega} \partial_n u \cdot v
                                    = \nu \int_\Omega \nabla u : \nabla v.
@f]
One of the advantages of this formulation is that it fully decouples the components of the velocity. Moreover,
they all share the same system matrix. This can be exploited in the program.

However, given the nonstandard boundary conditions, to be able to take them into account we need to use
the following %identity
@f[
  \Delta u = \nabla\nabla\cdot u - \nabla\times\nabla\times u,
@f]
so that when we integrate by parts and take into account the boundary conditions we obtain
@f[
  -\nu \int_\Omega \Delta u \cdot v = \nu \int_\Omega \left[ \nabla \cdot u \nabla \cdot v
                                    + \nabla \times u \nabla \times v \right],
@f]
which is the form that we would have to use. Notice that this couples the components of the velocity.
Moreover, to enforce the boundary condition on the pressure, we need to rewrite
@f[
  \int_\Omega \nabla p \cdot v = -\int_\Omega p \nabla \cdot v + \int_{\Gamma_1} p v\cdot n
                                + \int_{\Gamma_2} p v\cdot n
                               = -\int_\Omega p \nabla \cdot v,
@f]
where the boundary integral in $\Gamma_1$ equals zero given the boundary conditions for the velocity,
and the one in $\Gamma_2$ given the boundary conditions for the pressure.

In the simplified case where the boundary $\Gamma_2$ is %parallel to a coordinate axis, which holds for
the testcase that we carry out below, it can actually be shown that
@f[
  \nu \int_\Omega \nabla u : \nabla v = \nu \int_\Omega \left[ \nabla \cdot u \nabla \cdot v
                                    + \nabla \times u \nabla \times v \right].
@f]
This issue is not very often addressed in the literature. For more information the reader can consult, for
instance,
<ul>
  <li> J.-L. GUERMOND, L. QUARTAPELLE, On the approximation of the unsteady Navier-Stokes equations by
  finite element projection methods, Numer. Math., 80  (1998) 207-238
  <li> J.-L. GUERMOND, P. MINEV, J. SHEN, Error analysis of pressure-correction schemes for the
  Navier-Stokes equations with open boundary conditions, SIAM J. Numer. Anal., 43  1 (2005) 239--258.
</ul>



<a name = "implementation"></a>
<h3> Implementation </h3>

Our implementation of the projection methods follows <i>verbatim</i> the description given above. We must note,
however, that as opposed to most other problems that have several solution components, we do not use
vector-valued finite elements. Instead, we use separate finite elements the components of the velocity
and the pressure, respectively, and use different <code>DoFHandler</code>'s for those as well. The main
reason for doing this is that, as we see from the description of the scheme, the <code>dim</code> components
of the velocity and the pressure are decoupled. As a consequence, the equations for all the velocity components
look all the same, have the same system matrix, and can be solved in %parallel. Obviously, this approach
has also its disadvantages. For instance, we need to keep several <code>DoFHandler</code>s and iterators
synchronized when assembling matrices and right hand sides; obtaining quantities that are inherent to
vector-valued functions (e.g. divergences) becomes a little awkward, and others.

<a name ="testcase"></a>
<h3> The Testcase </h3>

The testcase that we use for this program consists of the flow around a square obstacle. The geometry is
as follows:

<img src="https://www.dealii.org/images/steps/developer/step-35.geometry.png" alt="">

with $H=4.1$, making the geometry slightly non-symmetric.

We impose no-slip boundary conditions on both the top and bottom walls and the obstacle. On the left side we
have the inflow boundary condition
@f[
  u =
  \left( \begin{array}{c} 4 U_m y (H-y)/H^2 \\ 0 \end{array} \right),
@f]
with $U_m = 1.5$, i.e. the inflow boundary conditions correspond to Poiseuille flow for this configuration.
Finally, on the right vertical wall we impose the condition that the vertical component of the velocity
and the pressure should both be zero.
The final time $T=10$.


examples/step-35/doc/results.dox
<a name="results"></a>
<h1>Results</h1>

<a name="Re100"></a>
<h3> Re = 100 </h3>

We run the code with the following <code>parameter-file.prm</code>, which can be found in the
same directory as the source:
@verbatim
  # First a global definition
  # the type of method we want to use
  set Method_Form = rotational

  subsection Physical data
    # In this subsection we declare the physical data
    # The initial and final time, and the Reynolds number
    set initial_time = 0.
    set final_time   = 25.
    set Reynolds     = 100
  end

  subsection Time step data
    # In this subsection we declare the data that is to be used for time discretization,
    # i.e. the time step dt
    set dt = 5e-3
  end

  subsection Space discretization
    # In this subsection we declare the data that is relevant to the space discretization
    # we set the number of global refines the triangulation must have
    # and the degree k of the pair Q_(k+1)--Q_k of velocity--pressure finite element spaces
    set n_of_refines = 3
    set pressure_fe_degree = 1
  end

  subsection Data solve velocity
    # In this section we declare the parameters that are going to control the solution process
    # for the velocity.
    set max_iterations = 1000  # maximal number of iterations that GMRES must make
    set eps            = 1e-6  # stopping criterion
    set Krylov_size    = 30    # size of the Krylov subspace to be used in GMRES
    set off_diagonals  = 60    # number of off diagonals that ILU must compute
    set diag_strength  = 0.01  # diagonal strengthening value
    set update_prec    = 10    # this number indicates how often the preconditioner must be updated
  end

  #The output frequency
  set output = 50

  #Finally we set the verbosity level
  set verbose = false
@endverbatim

Since the <code>verbose</code> parameter is set to <code>false</code>,
we do not get any kind of output besides the number of the time step
the program is currently working on.
If we were to set it to <code>true</code> we would get information on what the program is doing and
how many steps each iterative process had to make to converge, etc.

Let us plot the obtained results for $t=1,5,12,20,25$ (i.e. time steps
200, 1000, 2400, 4000, and 5000), where in the left column we show the
vorticity and in the right the velocity field:

<table>
  <tr>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_100.vorticity.0.9.3.png" alt="" width="400"> </td>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_100.velocity.0.9.3.png" alt="" width="400"> </td>
  </tr>
  <tr>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_100.vorticity.1.9.3.png" alt="" width="400"> </td>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_100.velocity.1.9.3.png" alt="" width="400"> </td>
  </tr>
  <tr>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_100.vorticity.2.9.3.png" alt="" width="400"> </td>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_100.velocity.2.9.3.png" alt="" width="400"> </td>
  </tr>
  <tr>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_100.vorticity.3.9.3.png" alt="" width="400"> </td>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_100.velocity.3.9.3.png" alt="" width="400"> </td>
  </tr>
  <tr>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_100.vorticity.4.9.3.png" alt="" width="400"> </td>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_100.velocity.4.9.3.png" alt="" width="400"> </td>
  </tr>
</table>

The images show nicely the development and extension of a vortex chain
behind the obstacles, with the sign of the vorticity indicating
whether this is a left or right turning vortex.


<a name="Re500"></a>
<h3> Re = 500 </h3>

We can change the Reynolds number, $Re$, in the parameter file to a
value of $500$. Doing so, and reducing the time step somewhat as well,
yields the following images at times $t=20,40$:

<table>
  <tr>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_500.vorticity.0.9.3.png" alt="" width="400"> </td>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_500.velocity.0.9.3.png" alt="" width="400"> </td>
  </tr>
  <tr>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_500.vorticity.1.9.3.png" alt="" width="400"> </td>
    <td> <img src="https://www.dealii.org/images/steps/developer/step-35.Re_500.velocity.1.9.3.png" alt="" width="400"> </td>
  </tr>
</table>

For this larger Reynolds number, we observe unphysical oscillations, especially
for the vorticity. The discretization scheme has now difficulties in correctly
resolving the flow, which should still be laminar and well-organized.
These phenomena are typical of discretization schemes that lack robustness
in under-resolved scenarios, where under-resolved means that the Reynolds
number computed with the mesh size instead of the physical dimensions of
the geometry is large. We look at a zoom at the region behind the obstacle, and
the mesh size we have there:


<img src="https://www.dealii.org/images/steps/developer/step-35.Re_500.zoom.9.3.png" alt="" width="400">

We can easily test our hypothesis by re-running the simulation with one more
mesh refinement set in the parameter file:

<img src="https://www.dealii.org/images/steps/developer/step-35.Re_500.zoom_2.9.3.png" alt="" width="400">

Indeed, the vorticity field now looks much smoother. While we can expect that
further refining the mesh will suppress the remaining oscillations as well,
one should take measures to obtain a robust scheme in the limit of coarse
resolutions, as described below.


<a name="extensions"></a>
<h3> Possibilities for extensions </h3>

This program can be extended in the following directions:
<ul>
  <li> Adaptive mesh refinement: As we have seen, we computed everything on a single fixed mesh.
  Using adaptive mesh refinement can lead to increased accuracy while not significantly increasing the
  computational time.

  <li> Adaptive time-stepping: Although there apparently is currently no theory about
  projection methods with variable time step,
  practice shows that they perform very well.

  <li> High Reynolds %numbers: As we can see from the results, increasing the Reynolds number changes significantly
  the behavior of the discretization scheme. Using well-known stabilization techniques we could be able to
  compute the flow in this, or many other problems, when the Reynolds number is very large and where computational
  costs demand spatial resolutions for which the flow is only marginally resolved, especially for 3D turbulent
  flows.

  <li> Variable density incompressible flows: There are projection-like methods for the case of incompressible
  flows with variable density. Such flows play a role if fluids of different
  density mix, for example fresh water and salt water, or alcohol and water.

  <li> Compressible Navier-Stokes equations: These equations are relevant for
  cases where
  velocities are high enough so that the fluid becomes compressible, but not
  fast enough that we get into a regime where viscosity becomes negligible
  and the Navier-Stokes equations need to be replaced by the hyperbolic Euler
  equations of gas dynamics. Compressibility starts to become a factor if the
  velocity becomes greater than about one third of the speed of sound, so it
  is not a factor for almost all terrestrial vehicles. On the other hand,
  commercial jetliners fly at about 85 per cent of the speed of sound, and
  flow over the wings becomes significantly supersonic, a regime in which the
  compressible Navier-Stokes equations are not applicable any more
  either. There are significant applications for the range in between,
  however, such as for small aircraft or the fast trains in many European and
  East Asian countries.
</ul>


examples/step-36/doc/intro.dox
<br>

<i>This program was contributed by Toby D. Young and Wolfgang
Bangerth.  </i>

<a name="Preamble"></a>
<h1>Preamble</h1>

The problem we want to solve in this example is an eigenspectrum
problem. Eigenvalue problems appear in a wide context of problems, for
example in the computation of electromagnetic standing waves in
cavities, vibration modes of drum membranes, or oscillations of lakes
and estuaries. One of the most enigmatic applications is probably the
computation of stationary or quasi-static wave functions in quantum
mechanics. The latter application is what we would like to investigate
here, though the general techniques outlined in this program are of
course equally applicable to the other applications above.

Eigenspectrum problems have the general form
@f{align*}
	L \Psi &= \varepsilon \Psi \qquad &&\text{in}\ \Omega\quad,
	\\
	\Psi &= 0 &&\text{on}\ \partial\Omega\quad,
@f}
where the Dirichlet boundary condition on $\Psi=\Psi(\mathbf x)$ could also be
replaced by Neumann or Robin conditions; $L$ is an operator that generally
also contains differential operators.

Under suitable conditions, the above equations have a set of solutions
$\Psi_\ell,\varepsilon_\ell$, $\ell\in {\cal I}$, where $\cal I$ can
be a finite or infinite set (and in the latter case it may be a discrete or
sometimes at least in part a continuous set). In either case, let us note that
there is
no longer just a single solution, but a set of solutions (the various
eigenfunctions and corresponding eigenvalues) that we want to
compute. The problem of numerically finding all eigenvalues
(eigenfunctions) of such eigenvalue problems is a formidable
challenge. In fact, if the set $\cal I$ is infinite, the challenge is
of course intractable.  Most of the time however we are really only
interested in a small subset of these values (functions); and
fortunately, the interface to the SLEPc library that we will use for
this tutorial program allows us to select which portion of the
eigenspectrum and how many solutions we want to solve for.

In this program, the eigenspectrum solvers we use are classes provided
by deal.II that wrap around the linear algebra implementation of the
<a href="http://www.grycap.upv.es/slepc/" target="_top">SLEPc</a>
library; SLEPc itself builds on the <a
href="http://www.mcs.anl.gov/petsc/" target="_top">PETSc</a> library
for linear algebra contents.

<a name="Intro"></a>
<h1>Introduction</h1>

The basic equation of stationary quantum mechanics is the
Schrödinger equation which models the motion of particles in an
external potential $V(\mathbf x)$. The particle is described by a wave
function $\Psi(\mathbf x)$ that satisfies a relation of the
(nondimensionalized) form
@f{align*} [-\Delta + V(\mathbf x)]
\Psi(\mathbf x) &= \varepsilon \Psi(\mathbf x) \qquad &&\text{in}\
\Omega\quad, \\ \Psi &= 0 &&\text{on}\ \partial\Omega\quad.
@f}
As a consequence, this particle can only exist in a certain number of
eigenstates that correspond to the energy eigenvalues
$\varepsilon_\ell$ admitted as solutions of this equation. The
orthodox (Copenhagen) interpretation of quantum mechanics posits that, if a
particle has energy $\varepsilon_\ell$ then the probability of finding
it at location $\mathbf x$ is proportional to $|\Psi_\ell(\mathbf
x)|^2$ where $\Psi_\ell$ is the eigenfunction that corresponds to this
eigenvalue.

In order to numerically find solutions to this equation, i.e. a set of
pairs of eigenvalues/eigenfunctions, we use the usual finite element
approach of multiplying the equation from the left with test functions,
integrating by parts, and searching for solutions in finite
dimensional spaces by approximating $\Psi(\mathbf
x)\approx\Psi_h(\mathbf x)=\sum_{j}\phi_j(\mathbf x)\tilde\psi_j$,
where $\tilde\psi$ is a vector of expansion coefficients. We then
immediately arrive at the following equation that discretizes the
continuous eigenvalue problem: @f[ \sum_j [(\nabla\phi_i,
\nabla\phi_j)+(V(\mathbf x)\phi_i,\phi_j)] \tilde{\psi}_j =
\varepsilon_h \sum_j (\phi_i, \phi_j) \tilde{\psi}_j\quad.  @f] In
matrix and vector notation, this equation then reads: @f[ A
\tilde{\Psi} = \varepsilon_h M \tilde{\Psi} \quad, @f] where $A$ is
the stiffness matrix arising from the differential operator $L$, and
$M$ is the mass matrix. The solution to the eigenvalue problem is an
eigenspectrum $\varepsilon_{h,\ell}$, with associated eigenfunctions
$\Psi_\ell=\sum_j \phi_j\tilde{\psi}_j$.


<h3>Eigenvalues and Dirichlet boundary conditions</h3>

In this program, we use Dirichlet boundary conditions for the wave
function $\Psi$. What this means, from the perspective of a finite
element code, is that only the interior degrees of freedom are real
degrees of <i>freedom</i>: the ones on the boundary are not free but
are forced to have a zero value, after all. On the other hand, the
finite element method gains much of its power and simplicity from
the fact that we just do the same thing on every cell, without
having to think too much about where a cell is, whether it bounds
on a less refined cell and consequently has a hanging node, or is
adjacent to the boundary. All such checks would make the assembly
of finite element linear systems unbearably difficult to write and
even more so to read.

Consequently, of course, when you distribute degrees of freedom with
your DoFHandler object, you don't care whether some of the degrees
of freedom you enumerate are at a Dirichlet boundary. They all get
numbers. We just have to take care of these degrees of freedom at a
later time when we apply boundary values. There are two basic ways
of doing this (either using MatrixTools::apply_boundary_values()
<i>after</i> assembling the linear system, or using
AffineConstraints::distribute_local_to_global() <i>during</i> assembly;
see the @ref constraints "constraints module" for more information),
but both result in the same: a linear system that has a total
number of rows equal to the number of <i>all</i> degrees of freedom,
including those that lie on the boundary. However, degrees of
freedom that are constrained by Dirichlet conditions are separated
from the rest of the linear system by zeroing out the corresponding
row and column, putting a single positive entry on the diagonal,
and the corresponding Dirichlet value on the right hand side.

If you assume for a moment that we had renumbered degrees of freedom
in such a way that all of those on the Dirichlet boundary come last,
then the linear system we would get when solving a regular PDE with
a right hand side would look like this:
@f{align*}
  \begin{pmatrix}
    A_i & 0 \\ 0 & D_b
  \end{pmatrix}
  \begin{pmatrix}
    U_i \\ U_b
  \end{pmatrix}
  =
  \begin{pmatrix}
    F_i \\ F_b
  \end{pmatrix}.
@f}
Here, subscripts $i$ and $b$ correspond to interior and boundary
degrees of freedom, respectively. The interior degrees of freedom
satisfy the linear system $A_i U_i=F_i$ which yields the correct
solution in the interior, and boundary values are determined by
$U_b = D_b^{-1} F_b$ where $D_b$ is a diagonal matrix that results
from the process of eliminating boundary degrees of freedom, and
$F_b$ is chosen in such a way that $U_{b,j}=D_{b,jj}^{-1} F_{b,j}$
has the correct boundary values for every boundary degree of freedom
$j$. (For the curious, the entries of the
matrix $D_b$ result from adding modified local contributions to the
global matrix where for the local matrices the diagonal elements, if non-zero,
are set to their absolute value; otherwise, they are set to the average of
absolute values of the diagonal. This process guarantees that the entries
of $D_b$ are positive and of a size comparable to the rest of the diagonal
entries, ensuring that the resulting matrix does not incur unreasonable
losses of accuracy due to roundoff involving matrix entries of drastically
different size. The actual values that end up on the diagonal are difficult
to predict and you should treat them as arbitrary and unpredictable, but
positive.)

For "regular" linear systems, this all leads to the correct solution.
On the other hand, for eigenvalue problems, this is not so trivial.
There, eliminating boundary values affects both matrices
$A$ and $M$ that we will solve with in the current tutorial program.
After elimination of boundary values, we then receive an eigenvalue
problem that can be partitioned like this:
@f{align*}
  \begin{pmatrix}
    A_i & 0 \\ 0 & D_A
  \end{pmatrix}
  \begin{pmatrix}
    \tilde\Psi_i \\ \tilde\Psi_b
  \end{pmatrix}
  =
  \epsilon_h
  \begin{pmatrix}
    M_i & 0 \\ 0 & D_M
  \end{pmatrix}
  \begin{pmatrix}
    \tilde\Psi_i \\ \tilde\Psi_b
  \end{pmatrix}.
@f}
This form makes it clear that there are two sets of eigenvalues:
the ones we care about, and spurious eigenvalues from the
separated problem
@f[
  D_A \tilde \Psi_b = \epsilon_h D_M \Psi_b.
@f]
These eigenvalues are spurious since they result from an eigenvalue
system that operates only on boundary nodes -- nodes that are not
real degrees of <i>freedom</i>.
Of course, since the two matrices $D_A,D_M$ are diagonal, we can
exactly quantify these spurious eigenvalues: they are
$\varepsilon_{h,j}=D_{A,jj}/D_{M,jj}$ (where the indices
$j$ corresponds exactly to the degrees of freedom that are constrained
by Dirichlet boundary values).

So how does one deal with them? The fist part is to recognize when our
eigenvalue solver finds one of them. To this end, the program computes
and prints an interval within which these eigenvalues lie, by computing
the minimum and maximum of the expression $\varepsilon_{h,j}=D_{A,jj}/D_{M,jj}$
over all constrained degrees of freedom. In the program below, this
already suffices: we find that this interval lies outside the set of
smallest eigenvalues and corresponding eigenfunctions we are interested
in and compute, so there is nothing we need to do here.

On the other hand, it may happen that we find that one of the eigenvalues
we compute in this program happens to be in this interval, and in that
case we would not know immediately whether it is a spurious or a true
eigenvalue. In that case, one could simply scale the diagonal elements of
either matrix after computing the two matrices,
thus shifting them away from the frequency of interest in the eigen-spectrum.
This can be done by using the following code, making sure that all spurious
eigenvalues are exactly equal to $1.234\cdot 10^5$:
@code
    for (unsigned int i = 0; i < dof_handler.n_dofs(); ++i)
      if (constraints.is_constrained(i))
        {
          stiffness_matrix.set(i, i, 1.234e5);
          mass_matrix.set(i, i, 1);
        }
@endcode
However, this strategy is not pursued here as the spurious eigenvalues
we get from our program as-is happen to be greater than the lowest
five that we will calculate and are interested in.


<h3>Implementation details</h3>

The program below is essentially just a slightly modified version of
step-4. The things that are different are the following:

<ul>

<li>The main class (named <code>EigenvalueProblem</code>) now no
longer has a single solution vector, but a whole set of vectors for
the various eigenfunctions we want to compute. Moreover, the
<code>main</code> function, which has the top-level control over
everything here, initializes and finalizes the interface to SLEPc and
PETSc simultaneously via <code>SlepcInitialize</code> and
<code>SlepFinalize</code>.</li>

<li>We use PETSc matrices and vectors as in step-17 and
step-18 since that is what the SLEPc eigenvalue solvers
require.</li>

<li>The function <code>EigenvalueProblem::solve</code> is entirely
different from anything seen so far in the tutorial, as it does not
just solve a linear system but actually solves the eigenvalue problem.
It is built on the SLEPc library, and more immediately on the deal.II
SLEPc wrappers in the class SLEPcWrappers::SolverKrylovSchur.</li>

<li>We use the ParameterHandler class to describe a few input
parameters, such as the exact form of the potential $V({\mathbf
x})$, the number of global refinement steps of the mesh,
or the number of eigenvalues we want to solve for. We could go much
further with this but stop at making only a few of the things that one
could select at run time actual input file parameters. In order to see
what could be done in this regard, take a look at @ref step_29
"step-29" and step-33.</li>

<li>We use the FunctionParser class to make the potential $V(\mathbf
x)$ a run-time parameter that can be specified in the input file as a
formula.</li>

</ul>

The rest of the program follows in a pretty straightforward way from
step-4.


examples/step-36/doc/results.dox
<h1>Results</h1>

<h3>Running the problem</h3>

The problem's input is parameterized by an input file <code>\step-36.prm</code>
which could, for example, contain the following text:

@code
set Global mesh refinement steps         = 5
set Number of eigenvalues/eigenfunctions = 5
set Potential                            = 0
@endcode

Here, the potential is zero inside the domain, and we know that the
eigenvalues are given by $\lambda_{(mn)}=\frac{\pi^2}{4}(m^2+n^2)$ where
$m,n\in{\mathbb N^+}$. Eigenfunctions are sines and cosines with $m$ and $n$
periods in $x$ and $y$ directions. This matches the output our program
generates:
@code
examples/\step-36> make run
============================ Running \step-36
   Number of active cells:       1024
   Number of degrees of freedom: 1089
   Solver converged in 67 iterations.

      Eigenvalue 0 : 4.93877
      Eigenvalue 1 : 12.3707
      Eigenvalue 2 : 12.3707
      Eigenvalue 3 : 19.8027
      Eigenvalue 4 : 24.837

   Job done.  @endcode These eigenvalues are exactly the ones that
correspond to pairs $(m,n)=(1,1)$, $(1,2)$ and $(2,1)$, $(2,2)$, and
$(3,1)$. A visualization of the corresponding eigenfunctions would
look like this:

<table width="80%">
<tr>
<td><img src="https://www.dealii.org/images/steps/developer/step-36.default.eigenfunction.0.png" alt=""></td>
<td><img src="https://www.dealii.org/images/steps/developer/step-36.default.eigenfunction.1.png" alt=""></td>
<td><img src="https://www.dealii.org/images/steps/developer/step-36.default.eigenfunction.2.png" alt=""></td>
</tr>
<tr>
<td><img src="https://www.dealii.org/images/steps/developer/step-36.default.eigenfunction.3.png" alt=""></td>
<td><img src="https://www.dealii.org/images/steps/developer/step-36.default.eigenfunction.4.png" alt=""></td>
<td></td>
</tr>
</table>

<h3>Possibilities for extensions</h3>

It is always worth playing a few games in the playground! So here goes
with a few suggestions:

<ul>

<li> The potential used above (called the <i>infinite well</i> because
it is a flat potential surrounded by infinitely high walls) is
interesting because it allows for analytically known solutions. Apart
from that, it is rather boring, however. That said, it is trivial to
play around with the potential by just setting it to something
different in the input file. For example, let us assume that we wanted
to work with the following potential in
2d:
@f[
  V(x,y) = \left\{
       \begin{array}{ll}
         -100 & \text{if}\ \sqrt{x^2+y^2}<\frac 34 \ \text{and}
                         \ xy>0
         \\
         -5 & \text{if}\ \sqrt{x^2+y^2}<\frac 34 \ \text{and}
                         \ xy\le 0
         \\
         0 & \text{otherwise}
      \end{array} \right.\quad.
@f]
In other words, the potential is -100 in two sectors of a circle of radius
0.75, -5 in the other two sectors, and zero outside the circle. We can achieve
this by using the following in the input file:
@code
set Potential = if (x^2 + y^2 < 0.75^2, if (x*y > 0, -100, -5), 0)
@endcode
If in addition we also increase the mesh refinement by one level, we get the
following results:
@code
examples/\step-36> make run
============================ Running \step-36
   Number of active cells:       4096
   Number of degrees of freedom: 4225

   Eigenvalue 0 : -74.2562
   Eigenvalue 1 : -72.7322
   Eigenvalue 2 : -42.7406
   Eigenvalue 3 : -42.2232
   Eigenvalue 4 : -37.0744
@endcode

The output file also contains an interpolated version of the potential, which
looks like this (note that as expected the lowest few eigenmodes have
probability densities $|\Psi(\mathbf x)|^2$ that are significant only where the
potential is the lowest, i.e. in the top right and bottom left sector of inner
circle of the potential):

<img src="https://www.dealii.org/images/steps/developer/step-36.mod.potential.png" alt="">

The first five eigenfunctions are now like this:

<table width="80%">
<tr>
<td><img src="https://www.dealii.org/images/steps/developer/step-36.mod.eigenfunction.0.png" alt=""></td>
<td><img src="https://www.dealii.org/images/steps/developer/step-36.mod.eigenfunction.1.png" alt=""></td>
<td><img src="https://www.dealii.org/images/steps/developer/step-36.mod.eigenfunction.2.png" alt=""></td>
</tr>
<tr>
<td><img src="https://www.dealii.org/images/steps/developer/step-36.mod.eigenfunction.3.png" alt=""></td>
<td><img src="https://www.dealii.org/images/steps/developer/step-36.mod.eigenfunction.4.png" alt=""></td>
<td></td>
</tr>
</table>

<li> In our derivation of the problem we have assumed that the
particle is confined to a domain $\Omega$ and that at the boundary of
this domain its probability $|\Psi|^2$ of being is zero. This is
equivalent to solving the eigenvalue problem on all of ${\mathbb R}^d$
and assuming that the energy potential is finite only inside a region
$\Omega$ and infinite outside. It is relatively easy to show that
$|\Psi(\mathbf x)|^2$ at all locations $\mathbf x$ where $V(\mathbf
x)=\infty$. So the question is what happens if our potential is not of
this form, i.e. there is no bounded domain outside of which the
potential is infinite? In that case, it may be worth to just consider
a very large domain at the boundary of which $V(\mathbf x)$ is at
least very large, if not infinite. Play around with a few cases like
this and explore how the spectrum and eigenfunctions change as we make
the computational region larger and larger.

<li> What happens if we investigate the simple harmonic oscillator
problem $V(\mathbf x)=c|\mathbf x|^2$? This potential is exactly of
the form discussed in the previous paragraph and has hyper spherical
symmetry. One may want to use a large spherical domain with a large
outer radius, to approximate the whole-space problem (say, by invoking
GridGenerator::hyper_ball).

<li> The plots above show the wave function $\Psi(\mathbf x)$, but the
physical quantity of interest is actually the probability density
$|\Psi(\mathbf x)|^2$ for the particle to be at location $\mathbf x$.
Some visualization programs can compute derived quantities from the data in
an input file, but we can also do so right away when creating the output
file. The facility to do that is the DataPostprocessor class that can
be used in conjunction with the DataOut class. Examples of how this
can be done can be found in step-29 and
step-33.

<li> What happens if the particle in the box has %internal degrees of
freedom? For example, if the particle were a spin-$1/2$ particle? In
that case, we may want to start solving a vector-valued problem
instead.

<li> Our implementation of the deal.II library here uses the
PETScWrappers and SLEPcWrappers and is suitable for running on serial
machine architecture. However, for larger grids and with a larger
number of degrees-of-freedom, we may want to run our application on
parallel architectures. A parallel implementation of the above code
can be particularly useful here since the generalized eigenspectrum
problem is somewhat more expensive to solve than the standard problems
considered in most of the earlier tutorials. Fortunately, modifying the above
program to be MPI compliant is a relatively straightforward
procedure. A sketch of how this can be done can be found in @ref
step_17 "step-17".

<li> Finally, there are alternatives to using the SLEPc eigenvalue
solvers. deal.II has interfaces to one of them, ARPACK (see <a
href="../../external-libs/arpack.html">the ARPACK configuration page</a> for
setup instructions), implemented in the ArpackSolver class. Here is a short and
quick overview of what one would need to change to use it, provided you have a
working installation of ARPACK and deal.II has been configured properly for it
(see the deal.II <a href="../../readme.html" target="body">README</a> file):

First, in order to use the ARPACK interfaces, we can go back to using standard
deal.II matrices and vectors, so we start by replacing the PETSc and SLEPc
headers
@code
#include <deal.II/lac/petsc_sparse_matrix.h>
#include <deal.II/lac/petsc_vector.h>
#include <deal.II/lac/slepc_solver.h>
@endcode
with these:
@code
#include <deal.II/lac/arpack_solver.h>
#include <deal.II/lac/sparse_direct.h>
#include <deal.II/lac/sparse_matrix.h>
#include <deal.II/lac/compressed_sparsity_pattern.h>
@endcode
ARPACK allows complex eigenvalues, so we will also need
@code
#include <complex>
@endcode

Secondly, we switch back to the deal.II matrix and vector definitions in the
main class:
@code
    SparsityPattern                     sparsity_pattern;
    SparseMatrix<double>                stiffness_matrix, mass_matrix;
    std::vector<Vector<double> >        eigenfunctions;
    std::vector<std::complex<double>>   eigenvalues;
@endcode
and initialize them as usual in <code>make_grid_and_dofs()</code>:
@code
    sparsity_pattern.reinit (dof_handler.n_dofs(),
                             dof_handler.n_dofs(),
                             dof_handler.max_couplings_between_dofs());

    DoFTools::make_sparsity_pattern (dof_handler, sparsity_pattern);
    constraints.condense (sparsity_pattern);
    sparsity_pattern.compress();

    stiffness_matrix.reinit (sparsity_pattern);
    mass_matrix.reinit (sparsity_pattern);
@endcode

For solving the eigenvalue problem with ARPACK, we finally need to modify
<code>solve()</code>:
@code
  template <int dim>
  unsigned int EigenvalueProblem<dim>::solve ()
  {
    SolverControl solver_control (dof_handler.n_dofs(), 1e-9);

    SparseDirectUMFPACK inverse;
    inverse.initialize (stiffness_matrix);

    const unsigned int num_arnoldi_vectors = 2*eigenvalues.size() + 2;
    ArpackSolver::AdditionalData additional_data(num_arnoldi_vectors);

    ArpackSolver eigensolver (solver_control, additional_data);
    eigensolver.solve (stiffness_matrix,
                       mass_matrix,
                       inverse,
                       eigenvalues,
                       eigenfunctions,
                       eigenvalues.size());

    for (unsigned int i=0; i<eigenfunctions.size(); ++i)
      eigenfunctions[i] /= eigenfunctions[i].linfty_norm ();

    return solver_control.last_step ();
  }
@endcode
Note how we have used an exact decomposition (using SparseDirectUMFPACK) as a
preconditioner to ARPACK.
</ul>


examples/step-37/doc/intro.dox
<br>

<i>
This program was contributed by Katharina Kormann and Martin
Kronbichler.

The algorithm for the matrix-vector product is based on the article <a
href="http://dx.doi.org/10.1016/j.compfluid.2012.04.012">A generic interface
for parallel cell-based finite element operator application</a> by Martin
Kronbichler and Katharina Kormann, Computers and Fluids 63:135&ndash;147,
2012, and the paper &quot;Parallel finite element operator application: Graph
partitioning and coloring&quot; by Katharina Kormann and Martin Kronbichler
in: Proceedings of the 7th IEEE International Conference on e-Science, 2011.

This work was partly supported by the German Research Foundation (DFG) through
the project "High-order discontinuous Galerkin for the exa-scale" (ExaDG)
within the priority program "Software for Exascale Computing" (SPPEXA). The
large-scale computations shown in the results section of this tutorial program
were supported by Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu)
by providing computing time on the GCS Supercomputer SuperMUC at Leibniz
Supercomputing Centre (LRZ, www.lrz.de) through project id pr83te. </i>

<a name="Intro"></a>
<h1>Introduction</h1>

This example shows how to implement a matrix-free method, that is, a method
that does not explicitly store the matrix elements, for a second-order Poisson
equation with variable coefficients on a hypercube. The linear system will be
solved with a multigrid method and uses large-scale parallelism with MPI.

The major motivation for matrix-free methods is the fact that on today's
processors access to main memory (i.e., for objects that do not fit in the
caches) has become the bottleneck in many solvers for partial differential equations: To perform a
matrix-vector product based on matrices, modern CPUs spend far more time
waiting for data to arrive from memory than on actually doing the floating
point multiplications and additions. Thus, if we could substitute looking up
matrix elements in memory by re-computing them &mdash; or rather, the operator
represented by these entries &mdash;, we may win in terms of overall run-time
even if this requires a significant number of additional floating point
operations. That said, to realize this with a trivial implementation is not
enough and one needs to really look at the details to gain in
performance. This tutorial program and the papers referenced above show how
one can implement such a scheme and demonstrates the speedup that can be
obtained.


<h3>The test case</h3>

In this example, we consider the Poisson problem @f{eqnarray*} -
\nabla \cdot a(\mathbf x) \nabla u &=& 1, \\ u &=& 0 \quad \text{on}\
\partial \Omega @f} where $a(\mathbf x)$ is a variable coefficient.
Below, we explain how to implement a matrix-vector product for this
problem without explicitly forming the matrix. The construction can,
of course, be done in a similar way for other equations as well.

We choose as domain $\Omega=[0,1]^3$ and $a(\mathbf x)=\frac{1}{0.05 +
2\|\mathbf x\|^2}$. Since the coefficient is symmetric around the
origin but the domain is not, we will end up with a non-symmetric
solution.


<h3>Matrix-vector product implementation</h3>

In order to find out how we can write a code that performs a matrix-vector
product, but does not need to store the matrix elements, let us start at
looking how a finite element matrix <i>A</i> is assembled:
@f{eqnarray*}
A = \sum_{\mathrm{cell}=1}^{\mathrm{n\_cells}}
P_{\mathrm{cell,{loc-glob}}}^T A_{\mathrm{cell}} P_{\mathrm{cell,{loc-glob}}}.
@f}
In this formula, the matrix <i>P</i><sub>cell,loc-glob</sub> is a rectangular
matrix that defines the index mapping from local degrees of freedom in the
current cell to the global degrees of freedom. The information from which this
operator can be built is usually encoded in the <code>local_dof_indices</code>
variable and is used in the assembly calls filling matrices in deal.II. Here,
<i>A</i><sub>cell</sub> denotes the cell matrix associated with <i>A</i>.

If we are to perform a matrix-vector product, we can hence use that
@f{eqnarray*}
y &=& A\cdot u = \left(\sum_{\text{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T
A_\mathrm{cell} P_\mathrm{cell,{loc-glob}}\right) \cdot u
\\
&=& \sum_{\mathrm{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T
A_\mathrm{cell} u_\mathrm{cell}
\\
&=& \sum_{\mathrm{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T
v_\mathrm{cell},
@f}
where <i>u</i><sub>cell</sub> are the values of <i>u</i> at the degrees of freedom
of the respective cell, and
<i>v</i><sub>cell</sub>=<i>A</i><sub>cell</sub><i>u</i><sub>cell</sub>
correspondingly for the result.
A naive attempt to implement the local action of the Laplacian would hence be
to use the following code:
@code
Matrixfree<dim>::vmult (Vector<double>       &dst,
                        const Vector<double> &src) const
{
  dst = 0;

  QGauss<dim>  quadrature_formula(fe.degree+1);
  FEValues<dim> fe_values (fe, quadrature_formula,
                           update_gradients | update_JxW_values|
                           update_quadrature_points);

  const unsigned int   dofs_per_cell = fe.n_dofs_per_cell();
  const unsigned int   n_q_points    = quadrature_formula.size();

  FullMatrix<double>   cell_matrix (dofs_per_cell, dofs_per_cell);
  Vector<double>       cell_src (dofs_per_cell),
                       cell_dst (dofs_per_cell);
  const Coefficient<dim> coefficient;
  std::vector<double> coefficient_values(n_q_points);

  std::vector<unsigned int> local_dof_indices (dofs_per_cell);

  for (const auto & cell : dof_handler.active_cell_iterators())
    {
      cell_matrix = 0;
      fe_values.reinit (cell);
      coefficient.value_list(fe_values.get_quadrature_points(),
                             coefficient_values);

      for (unsigned int q=0; q<n_q_points; ++q)
        for (unsigned int i=0; i<dofs_per_cell; ++i)
          for (unsigned int j=0; j<dofs_per_cell; ++j)
            cell_matrix(i,j) += (fe_values.shape_grad(i,q) *
                                 fe_values.shape_grad(j,q) *
                                 fe_values.JxW(q)*
                                 coefficient_values[q]);

      cell->get_dof_indices (local_dof_indices);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        cell_src(i) = src(local_dof_indices(i));

      cell_matrix.vmult (cell_dst, cell_src);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        dst(local_dof_indices(i)) += cell_dst;
    }
}
@endcode

Here we neglected boundary conditions as well as any hanging nodes we may
have, though neither would be very difficult to include using the
AffineConstraints class. Note how we first generate the local matrix in the
usual way as a sum over all quadrature points for each local matrix entry.
To form the actual product as expressed in the above formula, we
extract the values of <code>src</code> of the cell-related degrees of freedom
(the action of <i>P</i><sub>cell,loc-glob</sub>), multiply by the local matrix
(the action of <i>A</i><sub>cell</sub>), and finally add the result to the
destination vector <code>dst</code> (the action of
<i>P</i><sub>cell,loc-glob</sub><sup>T</sup>, added over all the elements). It
is not more difficult than that, in principle.

While this code is completely correct, it is very slow. For every cell, we
generate a local matrix, which takes three nested loops with loop length equal
to the number of local degrees of freedom to compute. The
multiplication itself is then done by two nested loops, which means that it
is much cheaper.

One way to improve this is to realize that conceptually the local
matrix can be thought of as the product of three matrices,
@f{eqnarray*}
A_\mathrm{cell} = B_\mathrm{cell}^T D_\mathrm{cell} B_\mathrm{cell},
@f}
where for the example of the Laplace operator the (<i>q</i>*dim+<i>d,i</i>)-th
element of <i>B</i><sub>cell</sub> is given by
<code>fe_values.shape_grad(i,q)[d]</code>. This matrix consists of
<code>dim*n_q_points</code> rows and @p dofs_per_cell columns. The matrix
<i>D</i><sub>cell</sub> is diagonal and contains the values
<code>fe_values.JxW(q) * coefficient_values[q]</code> (or, rather, @p
dim copies of each of these values). This kind of representation of
finite element matrices can often be found in the engineering literature.

When the cell matrix is applied to a vector,
@f{eqnarray*}
A_\mathrm{cell}\cdot u_\mathrm{cell} = B_\mathrm{cell}^T
D_\mathrm{cell} B_\mathrm{cell} \cdot u_\mathrm{cell},
@f}
one would then not form the matrix-matrix products, but rather multiply one
matrix at a time with a vector from right to left so that only three
successive matrix-vector products are formed. This approach removes the three
nested loops in the calculation of the local matrix, which reduces the
complexity of the work on one cell from something like $\mathcal
{O}(\mathrm{dofs\_per\_cell}^3)$ to $\mathcal
{O}(\mathrm{dofs\_per\_cell}^2)$. An interpretation of this algorithm is that
we first transform the vector of values on the local DoFs to a vector of
gradients on the quadrature points. In the second loop, we multiply these
gradients by the integration weight and the coefficient. The third loop applies
the second gradient (in transposed form), so that we get back to a vector of
(Laplacian) values on the cell dofs.

The bottleneck in the above code is the operations done by the call to
FEValues::reinit for every <code>cell</code>, which take about as much time as
the other steps together (at least if the mesh is unstructured; deal.II can
recognize that the gradients are often unchanged on structured meshes). That
is certainly not ideal and we would like to do better than this. What the
reinit function does is to calculate the gradient in real space by
transforming the gradient on the reference cell using the Jacobian of the
transformation from real to reference cell. This is done for each basis
function on the cell, for each quadrature point. The Jacobian does not depend
on the basis function, but it is different on different quadrature points in
general. If you only build the matrix once as we've done in all previous
tutorial programs, there is nothing to be optimized since FEValues::reinit
needs to be called on every cell. In this process, the transformation is
applied while computing the local matrix elements.

In a matrix-free implementation, however, we will compute those integrals very
often because iterative solvers will apply the matrix many times during the
solution process. Therefore, we need to think about whether we may be able to
cache some data that gets reused in the operator applications, i.e., integral
computations. On the other hand, we realize that we must not cache too much
data since otherwise we get back to the situation where memory access becomes
the dominating factor. Therefore, we will not store the transformed gradients
in the matrix <i>B</i>, as they would in general be different for each basis
function and each quadrature point on every element for curved meshes.

The trick is to factor out the Jacobian transformation and first apply the
gradient on the reference cell only. This operation interpolates the vector of
values on the local dofs to a vector of (unit-coordinate) gradients on the
quadrature points. There, we first apply the Jacobian that we factored out
from the gradient, then apply the weights of the quadrature, and finally apply
the transposed Jacobian for preparing the third loop which tests by the
gradients on the unit cell and sums over quadrature points.

Let us again write this in terms of matrices. Let the matrix
<i>B</i><sub>cell</sub> denote the cell-related gradient matrix, with each row
containing the values on the quadrature points. It is constructed by a
matrix-matrix product as @f{eqnarray*} B_\mathrm{cell} =
J_\mathrm{cell}^{-\mathrm T} B_\mathrm{ref\_cell}, @f} where
<i>B</i><sub>ref_cell</sub> denotes the gradient on the reference cell and
<i>J</i><sup>-T</sup><sub>cell</sub> denotes the inverse transpose Jacobian of
the transformation from unit to real cell (in the language of transformations,
the operation represented by <i>J</i><sup>-T</sup><sub>cell</sub> represents a
covariant transformation). <i>J</i><sup>-T</sup><sub>cell</sub> is
block-diagonal, and the blocks size is equal to the dimension of the
problem. Each diagonal block is the Jacobian transformation that goes from the
reference cell to the real cell.

Putting things together, we find that
@f{eqnarray*}
A_\mathrm{cell} = B_\mathrm{cell}^T D B_\mathrm{cell}
                = B_\mathrm{ref\_cell}^T J_\mathrm{cell}^{-1}
                  D_\mathrm{cell}
                  J_\mathrm{cell}^{-\mathrm T} B_\mathrm{ref\_cell},
@f}
so we calculate the product (starting the local product from the right)
@f{eqnarray*}
v_\mathrm{cell} = B_\mathrm{ref\_cell}^T J_\mathrm{cell}^{-1} D J_\mathrm{cell}^{-\mathrm T}
B_\mathrm{ref\_cell} u_\mathrm{cell}, \quad
v = \sum_{\mathrm{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T
v_\mathrm{cell}.
@f}
@code
  FEValues<dim> fe_values_reference (fe, quadrature_formula,
                                     update_gradients);
  Triangulation<dim> reference_cell;
  GridGenerator::hyper_cube(reference_cell, 0., 1.);
  fe_values_reference.reinit (reference_cell.begin());

  FEValues<dim> fe_values (fe, quadrature_formula,
                           update_inverse_jacobians | update_JxW_values |
                           update_quadrature_points);

  for (const auto & cell : dof_handler.active_cell_iterators())
    {
      fe_values.reinit (cell);
      coefficient.value_list(fe_values.get_quadrature_points(),
                             coefficient_values);

      cell->get_dof_indices (local_dof_indices);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        cell_src(i) = src(local_dof_indices(i));

      temp_vector = 0;
      for (unsigned int q=0; q<n_q_points; ++q)
        for (unsigned int d=0; d<dim; ++d)
          for (unsigned int i=0; i<dofs_per_cell; ++i)
            temp_vector(q*dim+d) +=
              fe_values_reference.shape_grad(i,q)[d] * cell_src(i);

      for (unsigned int q=0; q<n_q_points; ++q)
        {
          // apply the transposed inverse Jacobian of the mapping
          Tensor<1,dim> temp;
          for (unsigned int d=0; d<dim; ++d)
            temp[d] = temp_vector(q*dim+d);
          for (unsigned int d=0; d<dim; ++d)
            {
              double sum = 0;
              for (unsigned int e=0; e<dim; ++e)
                sum += fe_values.inverse_jacobian(q)[e][d] *
                               temp[e];
              temp_vector(q*dim+d) = sum;
            }

          // multiply by coefficient and integration weight
          for (unsigned int d=0; d<dim; ++d)
            temp_vector(q*dim+d) *= fe_values.JxW(q) * coefficient_values[q];

          // apply the inverse Jacobian of the mapping
          for (unsigned int d=0; d<dim; ++d)
            temp[d] = temp_vector(q*dim+d);
          for (unsigned int d=0; d<dim; ++d)
            {
              double sum = 0;
              for (unsigned int e=0; e<dim; ++e)
                sum += fe_values.inverse_jacobian(q)[d][e] *
                       temp[e];
              temp_vector(q*dim+d) = sum;
            }
        }

      cell_dst = 0;
      for (unsigned int i=0; i<dofs_per_cell; ++i)
        for (unsigned int q=0; q<n_q_points; ++q)
          for (unsigned int d=0; d<dim; ++d)
            cell_dst(i) += fe_values_reference.shape_grad(i,q)[d] *
                                   temp_vector(q*dim+d);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        dst(local_dof_indices(i)) += cell_dst(i);
    }
}
@endcode

Note how we create an additional FEValues object for the reference cell
gradients and how we initialize it to the reference cell. The actual
derivative data is then applied by the inverse, transposed Jacobians (deal.II
calls the Jacobian matrix from real to unit cell inverse_jacobian, as the
forward transformation is from unit to real cell). The factor
$J_\mathrm{cell}^{-1} D_\mathrm{cell} J_\mathrm{cell}^{-\mathrm T}$ is
block-diagonal over quadrature. In this form, one realizes that variable
coefficients (possibly expressed through a tensor) and general grid topologies
with Jacobian transformations have a similar effect on the coefficient
transforming the unit-cell derivatives.

At this point, one might wonder why we store the matrix
$J_\mathrm{cell}^{-\mathrm T}$ and the coefficient separately, rather than
only the complete factor $J_\mathrm{cell}^{-1} D_\mathrm{cell}
J_\mathrm{cell}^{-\mathrm T}$. The latter would use less memory because the
tensor is symmetric with six independent values in 3D, whereas for the former
we would need nine entries for the inverse transposed Jacobian, one for the
quadrature weight and Jacobian determinant, and one for the coefficient,
totaling to 11 doubles. The reason is that the former approach allows for
implementing generic differential operators through a common framework of
cached data, whereas the latter specifically stores the coefficient for the
Laplacian. In case applications demand for it, this specialization could pay
off and would be worthwhile to consider. Note that the implementation in
deal.II is smart enough to detect Cartesian or affine geometries where the
Jacobian is constant throughout the cell and needs not be stored for every
cell (and indeed often is the same over different cells as well).

The final optimization that is most crucial from an operation count point of
view is to make use of the tensor product structure in the basis
functions. This is possible because we have factored out the gradient from the
reference cell operation described by <i>B</i><sub>ref_cell</sub>, i.e., an
interpolation operation over the completely regular data fields of the
reference cell. We illustrate the process of complexity reduction in two space
dimensions, but the same technique can be used in higher dimensions. On the
reference cell, the basis functions are of the tensor product form
$\phi(x,y,z) = \varphi_i(x) \varphi_j(y)$. The part of the matrix
<i>B</i><sub>ref_cell</sub> that computes the first component has the form
$B_\mathrm{sub\_cell}^x = B_\mathrm{grad,x} \otimes B_\mathrm{val,y}$, where
<i>B</i><sub>grad,x</sub> and <i>B</i><sub>val,y</sub> contain the evaluation
of all the 1D basis functions on all the 1D quadrature points. Forming a
matrix <i>U</i> with <i>U(j,i)</i> containing the coefficient belonging to
basis function $\varphi_i(x) \varphi_j(y)$, we get $(B_\mathrm{grad,x} \otimes
B_\mathrm{val,y})u_\mathrm{cell} = B_\mathrm{val,y} U B_\mathrm{grad,x}$. This
reduces the complexity for computing this product from $p^4$ to $2 p^3$, where
<i>p</i>-1 is the degree of the finite element (i.e., equivalently, <i>p</i>
is the number of shape functions in each coordinate direction), or $p^{2d}$ to
$d p^{d+1}$ in general. The reason why we look at the complexity in terms of
the polynomial degree is since we want to be able to go to high degrees and
possibly increase the polynomial degree <i>p</i> instead of the grid
resolution. Good algorithms for moderate degrees like the ones used here are
linear in the polynomial degree independent on the dimension, as opposed to
matrix-based schemes or naive evaluation through FEValues. The techniques used
in the implementations of deal.II have been established in the spectral
element community since the 1980s.

Implementing a matrix-free and cell-based finite element operator requires a
somewhat different program design as compared to the usual matrix assembly
codes shown in previous tutorial programs. The data structures for doing this
are the MatrixFree class that collects all data and issues a (parallel) loop
over all cells and the FEEvaluation class that evaluates finite element basis
functions by making use of the tensor product structure.

The implementation of the matrix-free matrix-vector product shown in this
tutorial is slower than a matrix-vector product using a sparse matrix for
linear elements, but faster for all higher order elements thanks to the
reduced complexity due to the tensor product structure and due to less memory
transfer during computations. The impact of reduced memory transfer is
particularly beneficial when working on a multicore processor where several
processing units share access to memory. In that case, an algorithm which is
computation bound will show almost perfect parallel speedup (apart from
possible changes of the processor's clock frequency through turbo modes
depending on how many cores are active), whereas an algorithm that is bound by
memory transfer might not achieve similar speedup (even when the work is
perfectly parallel and one could expect perfect scaling like in sparse
matrix-vector products). An additional gain with this implementation is that
we do not have to build the sparse matrix itself, which can also be quite
expensive depending on the underlying differential equation. Moreover, the
above framework is simple to generalize to nonlinear operations, as we
demonstrate in step-48.


<h3>Combination with multigrid</h3>

Above, we have gone to significant lengths to implement a matrix-vector
product that does not actually store the matrix elements. In many user codes,
however, one wants more than just doing a few matrix-vector products &mdash;
one wants to do as few of these operations as possible when solving linear
systems. In theory, we could use the CG method without preconditioning;
however, that would not be very efficient for the Laplacian. Rather,
preconditioners are used for increasing the speed of
convergence. Unfortunately, most of the more frequently used preconditioners
such as SSOR, ILU or algebraic multigrid (AMG) cannot be used here because
their implementation requires knowledge of the elements of the system matrix.

One solution is to use geometric multigrid methods as shown in step-16. They
are known to be very fast, and they are suitable for our purpose since all
ingredients, including the transfer between different grid levels, can be
expressed in terms of matrix-vector products related to a collection of
cells. All one needs to do is to find a smoother that is based on
matrix-vector products rather than all the matrix entries. One such candidate
would be a damped Jacobi iteration that requires access to the matrix
diagonal, but it is often not sufficiently good in damping all high-frequency
errors. The properties of the Jacobi method can be improved by iterating it a
few times with the so-called Chebyshev iteration. The Chebyshev iteration is
described by a polynomial expression of the matrix-vector product where the
coefficients can be chosen to achieve certain properties, in this case to
smooth the high-frequency components of the error which are associated to the
eigenvalues of the Jacobi-preconditioned matrix. At degree zero, the Jacobi
method with optimal damping parameter is retrieved, whereas higher order
corrections are used to improve the smoothing properties. The effectiveness of
Chebyshev smoothing in multigrid has been demonstrated, e.g., in the article
<a href="http://www.sciencedirect.com/science/article/pii/S0021999103001943">
<i>M. Adams, M. Brezina, J. Hu, R. Tuminaro. Parallel multigrid smoothers:
polynomial versus Gauss&ndash;Seidel, J. Comput. Phys. 188:593&ndash;610,
2003</i></a>. This publication also identifies one more advantage of
Chebyshev smoothers that we exploit here, namely that they are easy to
parallelize, whereas SOR/Gauss&ndash;Seidel smoothing relies on substitutions,
for which a naive parallelization works on diagonal sub-blocks of the matrix,
thereby decreases efficiency (for more detail see e.g. Y. Saad, Iterative
Methods for Sparse Linear Systems, SIAM, 2nd edition, 2003, chapters 11 & 12).

The implementation into the multigrid framework is then straightforward. The
multigrid implementation in this program is similar to step-16 and includes
adaptivity.


<h3>Using CPU-dependent instructions (vectorization)</h3>

The computational kernels for evaluation in FEEvaluation are written in a way
to optimally use computational resources. To achieve this, they do not operate
on double data types, but something we call VectorizedArray (check e.g. the
return type of FEEvaluationBase::get_value, which is VectorizedArray for a
scalar element and a Tensor of VectorizedArray for a vector finite
element). VectorizedArray is a short array of doubles or float whose length
depends on the particular computer system in use. For example, systems based
on x86-64 support the streaming SIMD extensions (SSE), where the processor's
vector units can process two doubles (or four single-precision floats) by one
CPU instruction. Newer processors (from about 2012 and onwards) support the
so-called advanced vector extensions (AVX) with 256 bit operands, which can
use four doubles and eight floats, respectively. Vectorization is a
single-instruction/multiple-data (SIMD) concept, that is, one CPU instruction
is used to process multiple data values at once. Often, finite element
programs do not use vectorization explicitly as the benefits of this concept
are only in arithmetic intensive operations. The bulk of typical finite
element workloads are memory bandwidth limited (operations on sparse matrices
and vectors) where the additional computational power is useless.

Behind the scenes, optimized BLAS packages might heavily rely on
vectorization, though. Also, optimizing compilers might automatically
transform loops involving standard code into more efficient vectorized form
(deal.II uses OpenMP SIMD pragmas inside the regular loops of vector
updates). However, the data flow must be very regular in order for compilers
to produce efficient code. For example, already the automatic vectorization of
the prototype operation that benefits from vectorization, matrix-matrix
products, fails on most compilers (as of writing this tutorial in early 2012
and updating in late 2016, neither gcc nor the Intel compiler manage to
produce useful vectorized code for the FullMatrix::mmult function, and not
even on the simpler case where the matrix bounds are compile-time constants
instead of run-time constants as in FullMatrix::mmult). The main reason for
this is that the information to be processed at the innermost loop (that is
where vectorization is applied) is not necessarily a multiple of the vector
length, leaving parts of the resources unused. Moreover, the data that can
potentially be processed together might not be laid out in a contiguous way in
memory or not with the necessary alignment to address boundaries that are
needed by the processor. Or the compiler might not be able to prove that data
arrays do not overlap when loading several elements at once.

In the matrix-free implementation in deal.II, we have therefore chosen to
apply vectorization at the level which is most appropriate for finite element
computations: The cell-wise computations are typically exactly the same for
all cells (except for indices in the indirect addressing used while reading
from and writing to vectors), and hence SIMD can be used to process several
cells at once. In all what follows, you can think of a VectorizedArray to hold
data from several cells. Remember that it is not related to the spatial
dimension and the number of elements e.g. in a Tensor or Point.

Note that vectorization depends on the CPU the code is running on and for
which the code is compiled. In order to generate the fastest kernels of
FEEvaluation for your computer, you should compile deal.II with the so-called
<i>native</i> processor variant. When using the gcc compiler, it can be
enabled by setting the variable <tt>CMAKE_CXX_FLAGS</tt> to
<tt>"-march=native"</tt> in the cmake build settings (on the command line,
specify <tt>-DCMAKE_CXX_FLAGS="-march=native"</tt>, see the deal.II README for
more information). Similar options exist for other compilers. We output
the current vectorization length in the run() function of this example.


<h3>Running multigrid on large-scale parallel computers</h3>

As mentioned above, all components in the matrix-free framework can easily be
parallelized with MPI using domain decomposition. Thanks to the easy access to
large-scale parallel meshes through p4est (see step-40 for details) in
deal.II, and the fact that cell-based loops with matrix-free evaluation
<i>only</i> need a decomposition of the mesh into chunks of roughly equal size
on each processor, there is relatively little to do to write a parallel
program working with distributed memory. While other tutorial programs using
MPI have relied on either PETSc or Trilinos, this program uses deal.II's own
parallel vector facilities.

The deal.II parallel vector class, LinearAlgebra::distributed::Vector, holds
the processor-local part of the solution as well as data fields for ghosted
DoFs, i.e. DoFs that are owned by a remote processor but accessed by cells
that are owned by the present processor. In the @ref GlossLocallyActiveDof
"glossary" these degrees of freedom are referred to as locally active degrees
of freedom. The function MatrixFree::initialize_dof_vector() provides a method
that sets this design. Note that hanging nodes can relate to additional
ghosted degrees of freedom that must be included in the distributed vector but
are not part of the locally active DoFs in the sense of the @ref
GlossLocallyActiveDof "glossary". Moreover, the distributed vector holds the
MPI metadata for DoFs that are owned locally but needed by other
processors. A benefit of the design of this vector class is the way ghosted
entries are accessed. In the storage scheme of the vector, the data array
extends beyond the processor-local part of the solution with further vector
entries available for the ghosted degrees of freedom. This gives a contiguous
index range for all locally active degrees of freedom. (Note that the index
range depends on the exact configuration of the mesh.) Since matrix-free
operations can be thought of doing linear algebra that is performance
critical, and performance-critical code cannot waste time on doing MPI-global
to MPI-local index translations, the availability of an index spaces local to
one MPI rank is fundamental. The way things are accessed here is a direct
array access. This is provided through
LinearAlgebra::distributed::Vector::local_element(), but it is actually rarely
needed because all of this happens internally in FEEvaluation.

The design of LinearAlgebra::distributed::Vector is similar to the
PETScWrappers::MPI::Vector and TrilinosWrappers::MPI::Vector data types we
have used in step-40 and step-32 before, but since we do not need any other
parallel functionality of these libraries, we use the
LinearAlgebra::distributed::Vector class of deal.II instead of linking in another
large library in this tutorial program. Also note that the PETSc and Trilinos
vectors do not provide the fine-grained control over ghost entries with direct
array access because they abstract away the necessary implementation details.


examples/step-37/doc/results.dox
<h1>Results</h1>

<h3>Program output</h3>

Since this example solves the same problem as step-5 (except for
a different coefficient), there is little to say about the
solution. We show a picture anyway, illustrating the size of the
solution through both isocontours and volume rendering:

<img src="https://www.dealii.org/images/steps/developer/step-37.solution.png" alt="">

Of more interest is to evaluate some aspects of the multigrid solver.
When we run this program in 2D for quadratic ($Q_2$) elements, we get the
following output (when run on one core in release mode):
@code
Vectorization over 2 doubles = 128 bits (SSE2)
Cycle 0
Number of degrees of freedom: 81
Total setup time               (wall) 0.00159788s
Time solve (6 iterations)  (CPU/wall) 0.000951s/0.000951052s

Cycle 1
Number of degrees of freedom: 289
Total setup time               (wall) 0.00114608s
Time solve (6 iterations)  (CPU/wall) 0.000935s/0.000934839s

Cycle 2
Number of degrees of freedom: 1089
Total setup time               (wall) 0.00244665s
Time solve (6 iterations)  (CPU/wall) 0.00207s/0.002069s

Cycle 3
Number of degrees of freedom: 4225
Total setup time               (wall) 0.00678205s
Time solve (6 iterations)  (CPU/wall) 0.005616s/0.00561595s

Cycle 4
Number of degrees of freedom: 16641
Total setup time               (wall) 0.0241671s
Time solve (6 iterations)  (CPU/wall) 0.019543s/0.0195441s

Cycle 5
Number of degrees of freedom: 66049
Total setup time               (wall) 0.0967851s
Time solve (6 iterations)  (CPU/wall) 0.07457s/0.0745709s

Cycle 6
Number of degrees of freedom: 263169
Total setup time               (wall) 0.346374s
Time solve (6 iterations)  (CPU/wall) 0.260042s/0.265033s
@endcode

As in step-16, we see that the number of CG iterations remains constant with
increasing number of degrees of freedom. A constant number of iterations
(together with optimal computational properties) means that the computing time
approximately quadruples as the problem size quadruples from one cycle to the
next. The code is also very efficient in terms of storage. Around 2-4 million
degrees of freedom fit into 1 GB of memory, see also the MPI results below. An
interesting fact is that solving one linear system is cheaper than the setup,
despite not building a matrix (approximately half of which is spent in the
DoFHandler::distribute_dofs() and DoFHandler::distribute_mg_dofs()
calls). This shows the high efficiency of this approach, but also that the
deal.II data structures are quite expensive to set up and the setup cost must
be amortized over several system solves.

Not much changes if we run the program in three spatial dimensions. Since we
use uniform mesh refinement, we get eight times as many elements and
approximately eight times as many degrees of freedom with each cycle:

@code
Vectorization over 2 doubles = 128 bits (SSE2)
Cycle 0
Number of degrees of freedom: 125
Total setup time               (wall) 0.00231099s
Time solve (6 iterations)  (CPU/wall) 0.000692s/0.000922918s

Cycle 1
Number of degrees of freedom: 729
Total setup time               (wall) 0.00289083s
Time solve (6 iterations)  (CPU/wall) 0.001534s/0.0024128s

Cycle 2
Number of degrees of freedom: 4913
Total setup time               (wall) 0.0143182s
Time solve (6 iterations)  (CPU/wall) 0.010785s/0.0107841s

Cycle 3
Number of degrees of freedom: 35937
Total setup time               (wall) 0.087064s
Time solve (6 iterations)  (CPU/wall) 0.063522s/0.06545s

Cycle 4
Number of degrees of freedom: 274625
Total setup time               (wall) 0.596306s
Time solve (6 iterations)  (CPU/wall) 0.427757s/0.431765s

Cycle 5
Number of degrees of freedom: 2146689
Total setup time               (wall) 4.96491s
Time solve (6 iterations)  (CPU/wall) 3.53126s/3.56142s
@endcode

Since it is so easy, we look at what happens if we increase the polynomial
degree. When selecting the degree as four in 3D, i.e., on $\mathcal Q_4$
elements, by changing the line <code>const unsigned int
degree_finite_element=4;</code> at the top of the program, we get the
following program output:

@code
Vectorization over 2 doubles = 128 bits (SSE2)
Cycle 0
Number of degrees of freedom: 729
Total setup time               (wall) 0.00633097s
Time solve (6 iterations)  (CPU/wall) 0.002829s/0.00379395s

Cycle 1
Number of degrees of freedom: 4913
Total setup time               (wall) 0.0174279s
Time solve (6 iterations)  (CPU/wall) 0.012255s/0.012254s

Cycle 2
Number of degrees of freedom: 35937
Total setup time               (wall) 0.082655s
Time solve (6 iterations)  (CPU/wall) 0.052362s/0.0523629s

Cycle 3
Number of degrees of freedom: 274625
Total setup time               (wall) 0.507943s
Time solve (6 iterations)  (CPU/wall) 0.341811s/0.345788s

Cycle 4
Number of degrees of freedom: 2146689
Total setup time               (wall) 3.46251s
Time solve (7 iterations)  (CPU/wall) 3.29638s/3.3265s

Cycle 5
Number of degrees of freedom: 16974593
Total setup time               (wall) 27.8989s
Time solve (7 iterations)  (CPU/wall) 26.3705s/27.1077s
@endcode

Since $\mathcal Q_4$ elements on a certain mesh correspond to $\mathcal Q_2$
elements on half the mesh size, we can compare the run time at cycle 4 with
fourth degree polynomials with cycle 5 using quadratic polynomials, both at
2.1 million degrees of freedom. The surprising effect is that the solver for
$\mathcal Q_4$ element is actually slightly faster than for the quadratic
case, despite using one more linear iteration. The effect that higher-degree
polynomials are similarly fast or even faster than lower degree ones is one of
the main strengths of matrix-free operator evaluation through sum
factorization, see the <a
href="http://dx.doi.org/10.1016/j.compfluid.2012.04.012">matrix-free
paper</a>. This is fundamentally different to matrix-based methods that get
more expensive per unknown as the polynomial degree increases and the coupling
gets denser.

In addition, also the setup gets a bit cheaper for higher order, which is
because fewer elements need to be set up.

Finally, let us look at the timings with degree 8, which corresponds to
another round of mesh refinement in the lower order methods:

@code
Vectorization over 2 doubles = 128 bits (SSE2)
Cycle 0
Number of degrees of freedom: 4913
Total setup time               (wall) 0.0842004s
Time solve (8 iterations)  (CPU/wall) 0.019296s/0.0192959s

Cycle 1
Number of degrees of freedom: 35937
Total setup time               (wall) 0.327048s
Time solve (8 iterations)  (CPU/wall) 0.07517s/0.075999s

Cycle 2
Number of degrees of freedom: 274625
Total setup time               (wall) 2.12335s
Time solve (8 iterations)  (CPU/wall) 0.448739s/0.453698s

Cycle 3
Number of degrees of freedom: 2146689
Total setup time               (wall) 16.1743s
Time solve (8 iterations)  (CPU/wall) 3.95003s/3.97717s

Cycle 4
Number of degrees of freedom: 16974593
Total setup time               (wall) 130.8s
Time solve (8 iterations)  (CPU/wall) 31.0316s/31.767s
@endcode

Here, the initialization seems considerably slower than before, which is
mainly due to the computation of the diagonal of the matrix, which actually
computes a 729 x 729 matrix on each cell and throws away everything but the
diagonal. The solver times, however, are again very close to the quartic case,
showing that the linear increase with the polynomial degree that is
theoretically expected is almost completely offset by better computational
characteristics and the fact that higher order methods have a smaller share of
degrees of freedom living on several cells that add to the evaluation
complexity.

<h3>Comparison with a sparse matrix</h3>

In order to understand the capabilities of the matrix-free implementation, we
compare the performance of the 3d example above with a sparse matrix
implementation based on TrilinosWrappers::SparseMatrix by measuring both the
computation times for the initialization of the problem (distribute DoFs,
setup and assemble matrices, setup multigrid structures) and the actual
solution for the matrix-free variant and the variant based on sparse
matrices. We base the preconditioner on float numbers and the actual matrix
and vectors on double numbers, as shown above. Tests are run on an Intel Core
i7-5500U notebook processor (two cores and <a
href="http://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX</a>
support, i.e., four operations on doubles can be done with one CPU
instruction, which is heavily used in FEEvaluation), optimized mode, and two
MPI ranks.

<table align="center" class="doxtable">
  <tr>
    <th>&nbsp;</th>
    <th colspan="2">Sparse matrix</th>
    <th colspan="2">Matrix-free implementation</th>
  </tr>
  <tr>
    <th>n_dofs</th>
    <th>Setup + assemble</th>
    <th>&nbsp;Solve&nbsp;</th>
    <th>Setup + assemble</th>
    <th>&nbsp;Solve&nbsp;</th>
  </tr>
  <tr>
    <td align="right">125</td>
    <td align="center">0.0042s</td>
    <td align="center">0.0012s</td>
    <td align="center">0.0022s</td>
    <td align="center">0.00095s</td>
  </tr>
  <tr>
    <td align="right">729</td>
    <td align="center">0.012s</td>
    <td align="center">0.0040s</td>
    <td align="center">0.0027s</td>
    <td align="center">0.0021s</td>
  </tr>
  <tr>
    <td align="right">4,913</td>
    <td align="center">0.082s</td>
    <td align="center">0.012s</td>
    <td align="center">0.011s</td>
    <td align="center">0.0057s</td>
  </tr>
  <tr>
    <td align="right">35,937</td>
    <td align="center">0.73s</td>
    <td align="center">0.13s</td>
    <td align="center">0.048s</td>
    <td align="center">0.040s</td>
  </tr>
  <tr>
    <td align="right">274,625</td>
    <td align="center">5.43s</td>
    <td align="center">1.01s</td>
    <td align="center">0.33s</td>
    <td align="center">0.25s</td>
  </tr>
  <tr>
    <td align="right">2,146,689</td>
    <td align="center">43.8s</td>
    <td align="center">8.24s</td>
    <td align="center">2.42s</td>
    <td align="center">2.06s</td>
  </tr>
</table>

The table clearly shows that the matrix-free implementation is more than twice
as fast for the solver, and more than six times as fast when it comes to
initialization costs. As the problem size is made a factor 8 larger, we note
that the times usually go up by a factor eight, too (as the solver iterations
are constant at six). The main deviation is in the sparse matrix between 5k
and 36k degrees of freedom, where the time increases by a factor 12. This is
the threshold where the (L3) cache in the processor can no longer hold all
data necessary for the matrix-vector products and all matrix elements must be
fetched from main memory.

Of course, this picture does not necessarily translate to all cases, as there
are problems where knowledge of matrix entries enables much better solvers (as
happens when the coefficient is varying more strongly than in the above
example). Moreover, it also depends on the computer system. The present system
has good memory performance, so sparse matrices perform comparably
well. Nonetheless, the matrix-free implementation gives a nice speedup already
for the <i>Q</i><sub>2</sub> elements used in this example. This becomes
particularly apparent for time-dependent or nonlinear problems where sparse
matrices would need to be reassembled over and over again, which becomes much
easier with this class. And of course, thanks to the better complexity of the
products, the method gains increasingly larger advantages when the order of the
elements increases (the matrix-free implementation has costs
4<i>d</i><sup>2</sup><i>p</i> per degree of freedom, compared to
2<i>p<sup>d</sup></i> for the sparse matrix, so it will win anyway for order 4
and higher in 3d).

<h3> Results for large-scale parallel computations on SuperMUC</h3>

As explained in the introduction and the in-code comments, this program can be
run in parallel with MPI. It turns out that geometric multigrid schemes work
really well and can scale to very large machines. To the authors' knowledge,
the geometric multigrid results shown here are the largest computations done
with deal.II as of late 2016, run on up to 147,456 cores of the <a
href="https://www.lrz.de/services/compute/supermuc/systemdescription/">complete
SuperMUC Phase 1</a>. The ingredients for scalability beyond 1000 cores are
that no data structure that depends on the global problem size is held in its
entirety on a single processor and that the communication is not too frequent
in order not to run into latency issues of the network.  For PDEs solved with
iterative solvers, the communication latency is often the limiting factor,
rather than the throughput of the network. For the example of the SuperMUC
system, the point-to-point latency between two processors is between 1e-6 and
1e-5 seconds, depending on the proximity in the MPI network. The matrix-vector
products with @p LaplaceOperator from this class involves several
point-to-point communication steps, interleaved with computations on each
core. The resulting latency of a matrix-vector product is around 1e-4
seconds. Global communication, for example an @p MPI_Allreduce operation that
accumulates the sum of a single number per rank over all ranks in the MPI
network, has a latency of 1e-4 seconds. The multigrid V-cycle used in this
program is also a form of global communication. Think about the coarse grid
solve that happens on a single processor: It accumulates the contributions
from all processors before it starts. When completed, the coarse grid solution
is transferred to finer levels, where more and more processors help in
smoothing until the fine grid. Essentially, this is a tree-like pattern over
the processors in the network and controlled by the mesh. As opposed to the
@p MPI_Allreduce operations where the tree in the reduction is optimized to the
actual links in the MPI network, the multigrid V-cycle does this according to
the partitioning of the mesh. Thus, we cannot expect the same
optimality. Furthermore, the multigrid cycle is not simply a walk up and down
the refinement tree, but also communication on each level when doing the
smoothing. In other words, the global communication in multigrid is more
challenging and related to the mesh that provides less optimization
opportunities. The measured latency of the V-cycle is between 6e-3 and 2e-2
seconds, i.e., the same as 60 to 200 MPI_Allreduce operations.

The following figure shows a scaling experiments on $\mathcal Q_3$
elements. Along the lines, the problem size is held constant as the number of
cores is increasing. When doubling the number of cores, one expects a halving
of the computational time, indicated by the dotted gray lines. The results
show that the implementation shows almost ideal behavior until an absolute
time of around 0.1 seconds is reached. The solver tolerances have been set
such that the solver performs five iterations. This way of plotting data is
the <b>strong scaling</b> of the algorithm. As we go to very large core
counts, the curves flatten out a bit earlier, which is because of the
communication network in SuperMUC where communication between processors
farther away is slightly slower.

<img src="https://www.dealii.org/images/steps/developer/step-37.scaling_strong.png" alt="">

In addition, the plot also contains results for <b>weak scaling</b> that lists
how the algorithm behaves as both the number of processor cores and elements
is increased at the same pace. In this situation, we expect that the compute
time remains constant. Algorithmically, the number of CG iterations is
constant at 5, so we are good from that end. The lines in the plot are
arranged such that the top left point in each data series represents the same
size per processor, namely 131,072 elements (or approximately 3.5 million
degrees of freedom per core). The gray lines indicating ideal strong scaling
are by the same factor of 8 apart. The results show again that the scaling is
almost ideal. The parallel efficiency when going from 288 cores to 147,456
cores is at around 75% for a local problem size of 750,000 degrees of freedom
per core which takes 1.0s on 288 cores, 1.03s on 2304 cores, 1.19s on 18k
cores, and 1.35s on 147k cores. The algorithms also reach a very high
utilization of the processor. The largest computation on 147k cores reaches
around 1.7 PFLOPs/s on SuperMUC out of an arithmetic peak of 3.2 PFLOPs/s. For
an iterative PDE solver, this is a very high number and significantly more is
often only reached for dense linear algebra. Sparse linear algebra is limited
to a tenth of this value.

As mentioned in the introduction, the matrix-free method reduces the memory
consumption of the data structures. Besides the higher performance due to less
memory transfer, the algorithms also allow for very large problems to fit into
memory. The figure below shows the computational time as we increase the
problem size until an upper limit where the computation exhausts memory. We do
this for 1k cores, 8k cores, and 65k cores and see that the problem size can
be varied over almost two orders of magnitude with ideal scaling. The largest
computation shown in this picture involves 292 billion ($2.92 \cdot 10^{11}$)
degrees of freedom. On a DG computation of 147k cores, the above algorithms
were also run involving up to 549 billion (2^39) DoFs.

<img src="https://www.dealii.org/images/steps/developer/step-37.scaling_size.png" alt="">

Finally, we note that while performing the tests on the large-scale system
shown above, improvements of the multigrid algorithms in deal.II have been
developed. The original version contained the sub-optimal code based on
MGSmootherPrecondition where some MPI_Allreduce commands (checking whether
all vector entries are zero) were done on each smoothing
operation on each level, which only became apparent on 65k cores and
more. However, the following picture shows that the improvement already pay
off on a smaller scale, here shown on computations on up to 14,336 cores for
$\mathcal Q_5$ elements:

<img src="https://www.dealii.org/images/steps/developer/step-37.scaling_oldnew.png" alt="">


<h3> Adaptivity</h3>

As explained in the code, the algorithm presented here is prepared to run on
adaptively refined meshes. If only part of the mesh is refined, the multigrid
cycle will run with local smoothing and impose Dirichlet conditions along the
interfaces which differ in refinement level for smoothing through the
MatrixFreeOperators::Base class. Due to the way the degrees of freedom are
distributed over levels, relating the owner of the level cells to the owner of
the first descendant active cell, there can be an imbalance between different
processors in MPI, which limits scalability by a factor of around two to five.

<h3> Possibilities for extensions</h3>

<h4> Kelly error estimator </h4>

As mentioned above the code is ready for locally adaptive h-refinement.
For the Poisson equation one can employ the Kelly error indicator,
implemented in the KellyErrorEstimator class. However one needs to be careful
with the ghost indices of parallel vectors.
In order to evaluate the jump terms in the error indicator, each MPI process
needs to know locally relevant DoFs.
However MatrixFree::initialize_dof_vector() function initializes the vector only with
some locally relevant DoFs.
The ghost indices made available in the vector are a tight set of only those indices
that are touched in the cell integrals (including constraint resolution).
This choice has performance reasons, because sending all locally relevant degrees
of freedom would be too expensive compared to the matrix-vector product.
Consequently the solution vector as-is is
not suitable for the KellyErrorEstimator class.
The trick is to change the ghost part of the partition, for example using a
temporary vector and LinearAlgebra::distributed::Vector::copy_locally_owned_data_from()
as shown below.

@code
IndexSet locally_relevant_dofs;
DoFTools::extract_locally_relevant_dofs(dof_handler, locally_relevant_dofs);
LinearAlgebra::distributed::Vector<double> copy_vec(solution);
solution.reinit(dof_handler.locally_owned_dofs(),
                locally_relevant_dofs,
                triangulation.get_communicator());
solution.copy_locally_owned_data_from(copy_vec);
constraints.distribute(solution);
solution.update_ghost_values();
@endcode

<h4> Shared-memory parallelization</h4>

This program is parallelized with MPI only. As an alternative, the MatrixFree
loop can also be issued in hybrid mode, for example by using MPI parallelizing
over the nodes of a cluster and with threads through Intel TBB within the
shared memory region of one node. To use this, one would need to both set the
number of threads in the MPI_InitFinalize data structure in the main function,
and set the MatrixFree::AdditionalData::tasks_parallel_scheme to
partition_color to actually do the loop in parallel. This use case is
discussed in step-48.

<h4> Inhomogeneous Dirichlet boundary conditions </h4>

The presented program assumes homogeneous Dirichlet boundary conditions. When
going to non-homogeneous conditions, the situation is a bit more intricate. To
understand how to implement such a setting, let us first recall how these
arise in the mathematical formulation and how they are implemented in a
matrix-based variant. In essence, an inhomogeneous Dirichlet condition sets
some of the nodal values in the solution to given values rather than
determining them through the variational principles,
@f{eqnarray*}
u_h(\mathbf{x}) = \sum_{i\in \mathcal N} \varphi_i(\mathbf{x}) u_i =
\sum_{i\in \mathcal N \setminus \mathcal N_D} \varphi_i(\mathbf{x}) u_i +
\sum_{i\in \mathcal N_D} \varphi_i(\mathbf{x}) g_i,
@f}
where $u_i$ denotes the nodal values of the solution and $\mathcal N$ denotes
the set of all nodes. The set $\mathcal N_D\subset \mathcal N$ is the subset
of the nodes that are subject to Dirichlet boundary conditions where the
solution is forced to equal $u_i = g_i = g(\mathbf{x}_i)$ as the interpolation
of boundary values on the Dirichlet-constrained node points $i\in \mathcal
N_D$. We then insert this solution
representation into the weak form, e.g. the Laplacian shown above, and move
the known quantities to the right hand side:
@f{eqnarray*}
(\nabla \varphi_i, \nabla u_h)_\Omega &=& (\varphi_i, f)_\Omega \quad \Rightarrow \\
\sum_{j\in \mathcal N \setminus \mathcal N_D}(\nabla \varphi_i,\nabla \varphi_j)_\Omega \, u_j &=&
(\varphi_i, f)_\Omega
-\sum_{j\in \mathcal N_D} (\nabla \varphi_i,\nabla\varphi_j)_\Omega\, g_j.
@f}
In this formula, the equations are tested for all basis functions $\varphi_i$
with $i\in N \setminus \mathcal N_D$ that are not related to the nodes
constrained by Dirichlet conditions.

In the implementation in deal.II, the integrals $(\nabla \varphi_i,\nabla \varphi_j)_\Omega$
on the right hand side are already contained in the local matrix contributions
we assemble on each cell. When using
AffineConstraints::distribute_local_to_global() as first described in the
step-6 and step-7 tutorial programs, we can account for the contribution of
inhomogeneous constraints <i>j</i> by multiplying the columns <i>j</i> and
rows <i>i</i> of the local matrix according to the integrals $(\varphi_i,
\varphi_j)_\Omega$ by the inhomogeneities and subtracting the resulting from
the position <i>i</i> in the global right-hand-side vector, see also the @ref
constraints module. In essence, we use some of the integrals that get
eliminated from the left hand side of the equation to finalize the right hand
side contribution. Similar mathematics are also involved when first writing
all entries into a left hand side matrix and then eliminating matrix rows and
columns by MatrixTools::apply_boundary_values().

In principle, the components that belong to the constrained degrees of freedom
could be eliminated from the linear system because they do not carry any
information. In practice, in deal.II we always keep the size of the linear
system the same to avoid handling two different numbering systems and avoid
confusion about the two different index sets. In order to ensure that the
linear system does not get singular when not adding anything to constrained
rows, we then add dummy entries to the matrix diagonal that are otherwise
unrelated to the real entries.

In a matrix-free method, we need to take a different approach, since the @p
LaplaceOperator class represents the matrix-vector product of a
<b>homogeneous</b> operator (the left-hand side of the last formula).  It does
not matter whether the AffineConstraints object passed to the
MatrixFree::reinit() contains inhomogeneous constraints or not, the
MatrixFree::cell_loop() call will only resolve the homogeneous part of the
constraints as long as it represents a <b>linear</b> operator.

In our matrix-free code, the right hand side computation where the
contribution of inhomogeneous conditions ends up is completely decoupled from
the matrix operator and handled by a different function above. Thus, we need
to explicitly generate the data that enters the right hand side rather than
using a byproduct of the matrix assembly. Since we already know how to apply
the operator on a vector, we could try to use those facilities for a vector
where we only set the Dirichlet values:
@code
  // interpolate boundary values on vector solution
  std::map<types::global_dof_index, double> boundary_values;
  VectorTools::interpolate_boundary_values(mapping,
                                           dof_handler,
                                           0,
                                           BoundaryValueFunction<dim>(),
                                           boundary_values);
  for (const std::pair<const types::global_dof_index, double> &pair : boundary_values)
    if (solution.locally_owned_elements().is_element(pair.first))
      solution(pair.first) = pair.second;
@endcode
or, equivalently, if we already had filled the inhomogeneous constraints into
an AffineConstraints object,
@code
  solution = 0;
  constraints.distribute(solution);
@endcode

We could then pass the vector @p solution to the @p
LaplaceOperator::vmult_add() function and add the new contribution to the @p
system_rhs vector that gets filled in the @p LaplaceProblem::assemble_rhs()
function. However, this idea does not work because the
FEEvaluation::read_dof_values() call used inside the vmult() functions assumes
homogeneous values on all constraints (otherwise the operator would not be a
linear operator but an affine one). To also retrieve the values of the
inhomogeneities, we could select one of two following strategies.

<h5> Use FEEvaluation::read_dof_values_plain() to avoid resolving constraints </h5>

The class FEEvaluation has a facility that addresses precisely this
requirement: For non-homogeneous Dirichlet values, we do want to skip the
implicit imposition of homogeneous (Dirichlet) constraints upon reading the
data from the vector @p solution. For example, we could extend the @p
LaplaceProblem::assemble_rhs() function to deal with inhomogeneous Dirichlet
values as follows, assuming the Dirichlet values have been interpolated into
the object @p constraints:
@code
template <int dim>
void LaplaceProblem<dim>::assemble_rhs()
{
  solution = 0;
  constraints.distribute(solution);
  solution.update_ghost_values();
  system_rhs = 0;

  const Table<2, VectorizedArray<double>> &coefficient = system_matrix.get_coefficient();
  FEEvaluation<dim, degree_finite_element> phi(*system_matrix.get_matrix_free());
  for (unsigned int cell = 0;
       cell < system_matrix.get_matrix_free()->n_cell_batches();
       ++cell)
    {
      phi.reinit(cell);
      phi.read_dof_values_plain(solution);
      phi.evaluate(EvaluationFlags::gradients);
      for (unsigned int q = 0; q < phi.n_q_points; ++q)
        {
          phi.submit_gradient(-coefficient(cell, q) * phi.get_gradient(q), q);
          phi.submit_value(make_vectorized_array<double>(1.0), q);
        }
      phi.integrate(EvaluationFlags::values|EvaluationFlags::gradients);
      phi.distribute_local_to_global(system_rhs);
    }
  system_rhs.compress(VectorOperation::add);
}
@endcode

In this code, we replaced the FEEvaluation::read_dof_values() function for the
tentative solution vector by FEEvaluation::read_dof_values_plain() that
ignores all constraints. Due to this setup, we must make sure that other
constraints, e.g. by hanging nodes, are correctly distributed to the input
vector already as they are not resolved as in
FEEvaluation::read_dof_values_plain(). Inside the loop, we then evaluate the
Laplacian and repeat the second derivative call with
FEEvaluation::submit_gradient() from the @p LaplaceOperator class, but with the
sign switched since we wanted to subtract the contribution of Dirichlet
conditions on the right hand side vector according to the formula above. When
we invoke the FEEvaluation::integrate() call, we then set both arguments
regarding the value slot and first derivative slot to true to account for both
terms added in the loop over quadrature points. Once the right hand side is
assembled, we then go on to solving the linear system for the homogeneous
problem, say involving a variable @p solution_update. After solving, we can
add @p solution_update to the @p solution vector that contains the final
(inhomogeneous) solution.

Note that the negative sign for the Laplacian alongside with a positive sign
for the forcing that we needed to build the right hand side is a more general
concept: We have implemented nothing else than Newton's method for nonlinear
equations, but applied to a linear system. We have used an initial guess for
the variable @p solution in terms of the Dirichlet boundary conditions and
computed a residual $r = f - Au_0$. The linear system was then solved as
$\Delta u = A^{-1} (f-Au)$ and we finally computed $u = u_0 + \Delta u$. For a
linear system, we obviously reach the exact solution after a single
iteration. If we wanted to extend the code to a nonlinear problem, we would
rename the @p assemble_rhs() function into a more descriptive name like @p
assemble_residual() that computes the (weak) form of the residual, whereas the
@p LaplaceOperator::apply_add() function would get the linearization of the
residual with respect to the solution variable.

<h5> Use LaplaceOperator with a second AffineConstraints object without Dirichlet conditions </h5>

A second alternative to get the right hand side that re-uses the @p
LaplaceOperator::apply_add() function is to instead add a second LaplaceOperator
that skips Dirichlet constraints. To do this, we initialize a second MatrixFree
object which does not have any boundary value constraints. This @p matrix_free
object is then passed to a @p LaplaceOperator class instance @p
inhomogeneous_operator that is only used to create the right hand side:
@code
template <int dim>
void LaplaceProblem<dim>::assemble_rhs()
{
  system_rhs = 0;
  AffineConstraints<double> no_constraints;
  no_constraints.close();
  LaplaceOperator<dim, degree_finite_element, double> inhomogeneous_operator;

  typename MatrixFree<dim, double>::AdditionalData additional_data;
  additional_data.mapping_update_flags =
    (update_gradients | update_JxW_values | update_quadrature_points);
  std::shared_ptr<MatrixFree<dim, double>> matrix_free(
    new MatrixFree<dim, double>());
  matrix_free->reinit(dof_handler,
                      no_constraints,
                      QGauss<1>(fe.degree + 1),
                      additional_data);
  inhomogeneous_operator.initialize(matrix_free);

  solution = 0.0;
  constraints.distribute(solution);
  inhomogeneous_operator.evaluate_coefficient(Coefficient<dim>());
  inhomogeneous_operator.vmult(system_rhs, solution);
  system_rhs *= -1.0;

  FEEvaluation<dim, degree_finite_element> phi(
    *inhomogeneous_operator.get_matrix_free());
  for (unsigned int cell = 0;
       cell < inhomogeneous_operator.get_matrix_free()->n_cell_batches();
       ++cell)
    {
      phi.reinit(cell);
      for (unsigned int q = 0; q < phi.n_q_points; ++q)
        phi.submit_value(make_vectorized_array<double>(1.0), q);
      phi.integrate(EvaluationFlags::values);
      phi.distribute_local_to_global(system_rhs);
    }
  system_rhs.compress(VectorOperation::add);
}
@endcode

A more sophisticated implementation of this technique could reuse the original
MatrixFree object. This can be done by initializing the MatrixFree object with
multiple blocks, where each block corresponds to a different AffineConstraints
object. Doing this would require making substantial modifications to the
LaplaceOperator class, but the MatrixFreeOperators::LaplaceOperator class that
comes with the library can do this. See the discussion on blocks in
MatrixFreeOperators::Base for more information on how to set up blocks.


examples/step-38/doc/intro.dox
<br>

<i>This program was contributed by Andrea Bonito and M. Sebastian Pauletti,
with editing and writing by Wolfgang Bangerth.
<br>
This material is based upon work supported by the National Science
Foundation under Grant No. DMS-0914977. Any opinions, findings and conclusions
or recommendations expressed in this material are those of the author(s) and
do not necessarily reflect the views of the National Science Foundation
(NSF).
</i>

<a name="Intro"></a>

<h1>Introduction</h1>

In this example, we show how to solve a partial differential equation (PDE)
on a codimension one surface $\Gamma \subset \mathbb R^3$
made of quadrilaterals, i.e. on a surface in 3d or a line in 2d.
We focus on the following elliptic second order PDE
@f{align*}
-\Delta_\Gamma u &= f \qquad \text{on } \qquad \Gamma,\\
u  &= g \qquad \text{on} \qquad \partial \Gamma,
@f}
which generalized the Laplace equation we have previously solved in several of
the early tutorial programs. Our implementation is based on step-4. step-34
also solves problems on lower dimensional surfaces; however, there we only
consider integral equations that do not involve derivatives on the solution
variable, while here we actually have to investigate what it means to take
derivatives of a function only defined on a (possibly curved) surface.

In order to define the above operator, we start by introducing some notations.
Let $\mathbf x_S:\hat S \rightarrow S$ be a parameterization of
a surface $S$ from a reference element $\hat S \subset \mathbb R^2$,
i.e. each point $\hat{\mathbf x}\in\hat S$ induces a point ${\mathbf
  x}_S(\hat{\mathbf x}) \in S$. Then let
@f[
G_S\dealcoloneq (D \mathbf{x}_S)^T \ D \mathbf{x}_S
@f]
denotes the corresponding first fundamental form, where $D
\mathbf{x}_S=\left(\frac{\partial x_{S,i}(\hat{\mathbf x})}{\partial \hat x_j}\right)_{ij}$ is the
derivative (Jacobian) of the mapping.
In the following, $S$ will be either the entire surface $\Gamma$ or,
more convenient for the finite element method, any face $S \in
{\mathbb T}$, where ${\mathbb T}$ is a partition (triangulation) of $\Gamma$
constituted of quadrilaterals.
We are now in position to define the tangential gradient of a function $v : S \rightarrow \mathbb
R$ by
@f[
(\nabla_S v)\circ \mathbf x_S \dealcoloneq  D \mathbf x_S \ G_S^{-1} \ \nabla (v \circ \mathbf x_S).
@f]
The surface Laplacian (also called the Laplace-Beltrami operator) is then
defined as  $\Delta_S \dealcoloneq \nabla_S \cdot \nabla_S$.
Note that an alternate way to compute the surface gradient on smooth surfaces $\Gamma$ is
@f[
\nabla_S v = \nabla \tilde v - \mathbf n (\mathbf n \cdot \nabla \tilde v),
@f]
where $\tilde v$ is a "smooth" extension of $v$ in a tubular neighborhood of $\Gamma$ and
$\mathbf n$ is the normal of $\Gamma$.
Since $\Delta_S = \nabla_S \cdot \nabla_S$, we deduce
@f[
\Delta_S v = \Delta \tilde v - \mathbf n^T \ D^2 \tilde v \ \mathbf n - (\mathbf n \cdot \nabla \tilde v) (\nabla \cdot \mathbf n - \mathbf n^T \ D \mathbf n \ \mathbf n ).
@f]
Worth mentioning, the term $\nabla \cdot \mathbf n - \mathbf n \ D \mathbf n \ \mathbf n$ appearing in the above expression is the total curvature of the surface (sum of principal curvatures).

As usual, we are only interested in weak solutions for which we can use $C^0$
finite elements (rather than requiring $C^1$ continuity as for strong
solutions). We therefore resort to the weak formulation
@f[
\int_\Gamma \nabla_\Gamma u \cdot
\nabla_\Gamma v = \int_\Gamma f \ v  \qquad \forall v \in H^1_0(\Gamma)
@f]
and take advantage of the partition ${\mathbb T}$ to further write
@f[
\sum_{K\in  {\mathbb T}}\int_K \nabla_{K} u \cdot \nabla_{K} v = \sum_{K\in
  {\mathbb T}} \int_K f \ v  \qquad \forall v \in H^1_0(\Gamma).
@f]
Moreover, each integral in the above expression is computed in the reference
element $\hat K \dealcoloneq [0,1]^2$
so that
@f{align*}
\int_{K} \nabla_{K} u \cdot \nabla_{K} v
&=
\int_{\hat K} \nabla (u \circ \mathbf x_K)^T G_K^{-1} (D \mathbf
  x_K)^T D \mathbf x_K G_K^{-1} \nabla (v \circ \mathbf x_K) \sqrt{\det
    (G_K)}
\\
&=
\int_{\hat K} \nabla (u \circ \mathbf x_K)^T G_K^{-1} \nabla (v \circ \mathbf x_K) \sqrt{\det
    (G_K)}
@f}
and
@f[
\int_{K} f \ v = \int_{\hat K} (f \circ \mathbf x_K) (v \circ \mathbf
x_K)  \sqrt{\det
    (G_K)}.
@f]
Finally, we use a quadrature formula defined by points $\{p_l\}_{l=1}^N\subset
\hat K$ and weights $\{w_l\}_{l=1}^N \subset \mathbb R^+_*$ to
evaluate the above integrals and
obtain
@f[\int_{K} \nabla_{K} u \cdot \nabla_{K} v \approx \sum_{l=1}^N
 (\nabla (u \circ \mathbf x_K)(p_l))^T G^{-1}(p_l)  \nabla (v \circ \mathbf x_K)
(p_l) \sqrt{\det (G(p_l))} \ w_l
@f]
and
@f[
\int_{K} f \ v \approx \sum_{l=1}^N (f \circ \mathbf x_K)(p_l) \ (v \circ \mathbf x_K)(p_l) \sqrt{\det (G(p_l))} \ w_l.
@f]


Fortunately, deal.II has already all the tools to compute the above
expressions.
In fact, they barely differ from the ways in which we solve the usual
Laplacian, only requiring the surface coordinate mapping to be provided in the
constructor of the FEValues class.
This surface description given, in the codimension one surface case, the two
routines FEValues::shape_grad and FEValues::JxW
return
@f{align*}
\text{FEValues::shape\_grad}(i,l)&=D \mathbf x_K(p_l) G^{-1}(p_l)D(\varphi_i \circ \mathbf x_K)
  (p_l)
\\
\text{FEValues::JxW}(l) &=  \sqrt{\det (G(p_l))} \ w_l.
@f}
This provides exactly the terms we need for our computations.

On a more general note, details for the finite element approximation on
surfaces can be found for instance in
[Dziuk, in Partial differential equations and calculus of
variations 1357, Lecture Notes in Math., 1988],
[Demlow, SIAM J. Numer. Anal.  47(2), 2009]
and
[Bonito, Nochetto, and Pauletti, SIAM J. Numer. Anal. 48(5), 2010].



<h3>Testcase</h3>

In general when you want to test numerically the accuracy and/or order of
convergence of an algorithm you need to provide an exact solution. The usual
trick is to pick a function that we want to be the solution, then apply the
differential operator to it that defines a forcing term for the right hand
side. This is what we do in this example. In the current case, the form of the
domain is obviously also essential.

We produce one test case for a 2d problem and another one for 3d:

<ul>
<li>
  In 2d, let's choose as domain a half circle. On this domain, we choose the
  function $u(\mathbf x)=-2x_1x_2$ as the solution. To compute the right hand
  side, we have to compute the surface Laplacian of the
  solution function. There are (at least) two ways to do that. The first one
  is to project away the normal derivative as described above using the natural extension of $u(\mathbf x)$ (still denoted by $u$) over $\mathbb R^d$, i.e. to compute
  @f[
    -\Delta_\Gamma u =  \Delta u - \mathbf n^T \ D^2 u \ \mathbf n - (\mathbf n \cdot \nabla u)\ \kappa,
  @f]
  where $\kappa$ is the total curvature of $\Gamma$.
  Since we are on the unit circle, $\mathbf n=\mathbf x$ and $\kappa = 1$ so that
  @f[
    -\Delta_\Gamma u = -8 x_1x_2.
  @f]

  A somewhat simpler way, at least for the current case of a curve in
  two-dimensional space, is to note that we can map the interval $t \in
  [0,\pi]$ onto the domain $\Omega$ using the transformation
  $\mathbf x(t)= \left(\begin{array}{c} \cos t \\ \sin t \end{array}\right)$.
  At position $\mathbf x=\mathbf x(t)$, the value of the solution is then
  $u(\mathbf x(t)) = -2\cos t \sin t$.
  Taking into account that the transformation is length preserving, i.e. a
  segment of length $dt$ is mapped onto a piece of curve of exactly the same
  length, the tangential Laplacian then satisfies
  @f{align*}
    \Delta_\Gamma u
    &= \frac{d^2}{dt^2}(-2\cos t \sin t)
    = -2 \frac{d}{dt}(-\sin^2 t + \cos^2 t)
    = -2 (-2 \sin t \cos t - 2 \cos t \sin t)
    \\
    &= 8 \sin t \cos t
    \\
    &= 8 x_1x_2,
  @f}
  which is of course the same result as we had above.
</li>
<li>
  In 3d, the domain is again half of the surface of the unit ball, i.e. a half
  sphere or dome. We choose $u(\mathbf x)=-2\sin(\pi x_1)\cos(\pi x_2)e^z$ as
  the solution. We can compute the right hand side of the
  equation, $f=-\Delta_\Gamma u$, in the same way as the method above (with $\kappa = 2$), yielding an
  awkward and lengthy expression. You can find the full expression in the
  source code.
</li>
</ul>

In the program, we will also compute the $H^1$ seminorm error of the
solution. Since the solution function and its numerical approximation are only
defined on the manifold, the obvious definition of this error functional is
$| e |_{H^1(\Gamma)}
  = | \nabla_\Gamma e |_{L_2(\Gamma)}
  = \left( \int_\Gamma | \nabla_\Gamma (u-u_h) |^2 \right)^{1/2}$. This requires us to provide the
<i>tangential</i> gradient $\nabla_\Gamma u$ to the function VectorTools::integrate_difference
(first introduced in step-7), which we
will do by implementing the function <code>Solution::gradient</code> in the
program below.


<h3>Implementation</h3>

If you've read through step-4 and understand the discussion above of how
solution and right hand side correspond to each other, you will be immediately
familiar with this program as well. In fact, there are only two things that
are of significance:

- The way we generate the mesh that triangulates the computational domain.

- The way we use Mapping objects to describe that the domain on which we solve
  the partial differential equation is not planar but in fact curved.

Mapping objects were already introduced in step-10 and step-11 and as
explained there, there is usually not a whole lot you have to know about how
they work as long as you have a working description of how the boundary
looks. In essence, we will simply declare an appropriate object of type
MappingQ that will automatically obtain the boundary description from the
Triangulation. The mapping object will then be passed to the appropriate
functions, and we will get a boundary description for half circles or half
spheres that is predefined in the library.

The rest of the program follows closely step-4 and, as far as computing the
error, step-7. Some aspects of this program, in particular the use of two
template arguments on the classes Triangulation, DoFHandler, and similar, are
already described in detail in step-34; you may wish to read through this
tutorial program as well.


examples/step-38/doc/results.dox
<h1>Results</h1>

When you run the program, the following output should be printed on screen:

@verbatim
Surface mesh has 1280 cells.
Surface mesh has 5185 degrees of freedom.
H1 error = 0.0217136
@endverbatim


By playing around with the number of global refinements in the
<code>LaplaceBeltrami::make_grid_and_dofs</code> function you increase or decrease mesh
refinement. For example, doing one more refinement and only running the 3d surface
problem yields the following
output:

@verbatim
Surface mesh has 5120 cells.
Surface mesh has 20609 degrees of freedom.
H1 error = 0.00543481
@endverbatim

This is what we expect: make the mesh size smaller by a factor of two and the
error goes down by a factor of four (remember that we use bi-quadratic
elements). The full sequence of errors from one to five refinements looks like
this, neatly following the theoretically predicted pattern:
@verbatim
0.339438
0.0864385
0.0217136
0.00543481
0.00135913
@endverbatim

Finally, the program produces graphical output that we can visualize. Here is
a plot of the results:

<img src="https://www.dealii.org/images/steps/developer/step-38.solution-3d.png" alt="">

The program also works for 1d curves in 2d, not just 2d surfaces in 3d. You
can test this by changing the template argument in <code>main()</code> like
so:
@code
      LaplaceBeltramiProblem<2> laplace_beltrami;
@endcode
The domain is a curve in 2d, and we can visualize the solution by using the
third dimension (and color) to denote the value of the function $u(x)$. This
then looks like so (the white curve is the domain, the colored curve is the
solution extruded into the third dimension, clearly showing the change in sign
as the curve moves from one quadrant of the domain into the adjacent one):

<img src="https://www.dealii.org/images/steps/developer/step-38.solution-2d.png" alt="">


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

Computing on surfaces only becomes interesting if the surface is more
interesting than just a half sphere. To achieve this, deal.II can read
meshes that describe surfaces through the usual GridIn class. Or, in case you
have an analytic description, a simple mesh can sometimes be stretched and
bent into a shape we are interested in.

Let us consider a relatively simple example: we take the half sphere we used
before, we stretch it by a factor of 10 in the z-direction, and then we jumble
the x- and y-coordinates a bit. Let's show the computational domain and the
solution first before we go into details of the implementation below:

<img src="https://www.dealii.org/images/steps/developer/step-38.warp-1.png" alt="">

<img src="https://www.dealii.org/images/steps/developer/step-38.warp-2.png" alt="">

The way to produce such a mesh is by using the GridTools::transform()
function. It needs a way to transform each individual mesh point to a
different position. Let us here use the following, rather simple function
(remember: stretch in one direction, jumble in the other two):

@code
template <int spacedim>
Point<spacedim> warp(const Point<spacedim> &p)
{
  Point<spacedim> q = p;
  q[spacedim-1] *= 10;

  if (spacedim >= 2)
    q[0] += 2*std::sin(q[spacedim-1]);
  if (spacedim >= 3)
    q[1] += 2*std::cos(q[spacedim-1]);

  return q;
}
@endcode

If we followed the <code>LaplaceBeltrami::make_grid_and_dofs</code> function, we would
extract the half spherical surface mesh as before, warp it into the shape we
want, and refine as often as necessary. This is not quite as simple as we'd
like here, though: refining requires that we have an appropriate manifold
object attached to the triangulation that describes where new vertices of the
mesh should be located upon refinement. I'm sure it's possible to describe
this manifold in a not-too-complicated way by simply undoing the
transformation above (yielding the spherical surface again), finding the
location of a new point on the sphere, and then re-warping the result. But I'm
a lazy person, and since doing this is not really the point here, let's just
make our lives a bit easier: we'll extract the half sphere, refine it as
often as necessary, get rid of the object that describes the manifold since we
now no longer need it, and then finally warp the mesh. With the function
above, this would look as follows:

@code
template <int spacedim>
void LaplaceBeltrami<spacedim>::make_grid_and_dofs()
{
  {
    Triangulation<spacedim> volume_mesh;
    GridGenerator::half_hyper_ball(volume_mesh);

    volume_mesh.refine_global(4);

    std::set<types::boundary_id> boundary_ids;
    boundary_ids.insert(0);

    GridGenerator::extract_boundary_mesh(volume_mesh, triangulation,
                                         boundary_ids);
    GridTools::transform(&warp<spacedim>, triangulation);       /* ** */
    std::ofstream x("x"), y("y");
    GridOut().write_gnuplot(volume_mesh, x);
    GridOut().write_gnuplot(triangulation, y);
  }

  std::cout << "Surface mesh has " << triangulation.n_active_cells()
            << " cells."
            << std::endl;
  ...
}
@endcode

Note that the only essential addition is the line marked with
asterisks. It is worth pointing out one other thing here, though: because we
detach the manifold description from the surface mesh, whenever we use a
mapping object in the rest of the program, it has no curves boundary
description to go on any more. Rather, it will have to use the implicit,
FlatManifold class that is used on all parts of the domain not
explicitly assigned a different manifold object. Consequently, whether we use
MappingQ(2), MappingQ(15) or MappingQ1, each cell of our mesh will be mapped
using a bilinear approximation.

All these drawbacks aside, the resulting pictures are still pretty. The only
other differences to what's in step-38 is that we changed the right hand side
to $f(\mathbf x)=\sin x_3$ and the boundary values (through the
<code>Solution</code> class) to $u(\mathbf x)|_{\partial\Omega}=\cos x_3$. Of
course, we now no longer know the exact solution, so the computation of the
error at the end of <code>LaplaceBeltrami::run</code> will yield a meaningless
number.


examples/step-39/doc/intro.dox
<a name="Intro"></a>

In this program, we use the interior penalty method and Nitsche's weak
boundary conditions to solve Poisson's equation. We use multigrid
methods on locally refined meshes, which are generated using a bulk
criterion and a standard error estimator based on cell and face
residuals. All operators are implemented using the MeshWorker interface.

Like in step-12, the discretization relies on finite element spaces,
which are polynomial inside the mesh cells $K\in \mathbb T_h$, but
have no continuity between cells. Since such functions have two values
on each interior face $F\in \mathbb F_h^i$, one from each side, we
define mean value and jump operators as follows: let
<i>K</i><sub>1</sub> and <i>K</i><sub>2</sub> be the two cells sharing
a face, and let the traces of functions <i>u<sub>i</sub></i> and the
outer normal vectors <b>n</b><i><sub>i</sub></i> be labeled
accordingly. Then, on the face, we let
@f[
	\average{ u } = \frac{u_1 + u_2}2
@f]

Note, that if such an expression contains a normal vector, the
averaging operator turns into a jump. The interior penalty method for the problem
@f[
  -\Delta u = f \text{ in }\Omega \qquad u = u^D \text{ on } \partial\Omega
@f]
becomes
@f{multline*}
  \sum_{K\in \mathbb T_h} (\nabla u, \nabla v)_K
  \\
  + \sum_{F \in F_h^i} \biggl\{4\sigma_F (\average{ u \mathbf n}, \average{ v \mathbf n })_F
  - 2 (\average{ \nabla u },\average{ v\mathbf n })_F
  - 2 (\average{ \nabla v },\average{ u\mathbf n })_F
  \biggr\}
  \\
  + \sum_{F \in F_h^b} \biggl\{2\sigma_F (u, v)_F
  - (\partial_n u,v)_F
  - (\partial_n v,u)_F
  \biggr\}
  \\
  = (f, v)_\Omega + \sum_{F \in F_h^b} \biggl\{
  2\sigma_F (u^D, v)_F - (\partial_n v,u^D)_F
  \biggr\}.
@f}

Here, $\sigma_F$ is the penalty parameter, which is chosen as follows:
for a face <i>F</i> of a cell <i>K</i>, compute the value
@f[
\sigma_{F,K} = p(p+1) \frac{|F|_{d-1}}{|K|_d},
@f]
where <i>p</i> is the polynomial degree of the finite element
functions and $|\cdot|_d$ and $|\cdot|_{d-1}$ denote the $d$ and $d-1$
dimensional Hausdorff measure of the corresponding
object. If the face is at the boundary, choose $\sigma_F = \sigma_{F,K}$.
For an interior face, we take the average of the two values at this face.

In our finite element program, we distinguish three different
integrals, corresponding to the sums over cells, interior faces and
boundary faces above. Since the MeshWorker::loop organizes the sums
for us, we only need to implement the integrals over each mesh
element. The class MatrixIntegrator below has these three functions
for the left hand side of the formula, the class RHSIntegrator for the
right.

As we will see below, even the error estimate is of the same
structure, since it can be written as
@f{align*}
  \eta^2 &= \eta_K^2 + \eta_F^2 + \eta_B^2
  \\
  \eta_K^2 &= \sum_{K\in \mathbb T_h} h^2 \|f + \Delta u_h\|^2
  \\
  \eta_F^2 &= \sum_{F \in F_h^i} \biggl\{
    4 \sigma_F \| \average{u_h\mathbf n} \|^2 + h \|\average{\partial_n u_h}\|^2 \biggr\}
  \\
  \eta_B^2 &= \sum_{F \in F_h^b} 2\sigma_F \| u_h-u^D \|^2.
@f}

Thus, the functions for assembling matrices, right hand side and error
estimates below exhibit that these loops are all generic and can be
programmed in the same way.

This program is related to step-12b, in that it uses MeshWorker and
discontinuous Galerkin methods. While there, we solved an advection
problem, here it is a diffusion problem. Here, we also use multigrid
preconditioning and a theoretically justified error estimator, see
Karakashian and Pascal (2003). The multilevel scheme was discussed in
detail in Kanschat (2004). The adaptive iteration and its convergence
have been discussed (for triangular meshes) in Hoppe, Kanschat, and
Warburton (2009).


examples/step-39/doc/results.dox
<h1>Results</h1>

<h3>Logfile output</h3>
First, the program produces the usual logfile here stored in <tt>deallog</tt>. It reads (with omission of intermediate steps)

@code
DEAL::Element: FE_DGQ<2>(3)
DEAL::Step 0
DEAL::Triangulation 16 cells, 2 levels
DEAL::DoFHandler 256 dofs, level dofs 64 256
DEAL::Assemble matrix
DEAL::Assemble multilevel matrix
DEAL::Assemble right hand side
DEAL::Solve
DEAL:cg::Starting value 37.4071
DEAL:cg::Convergence step 13 value 1.64974e-13
DEAL::energy-error: 0.297419
DEAL::L2-error:     0.00452447
DEAL::Estimate 0.990460
DEAL::Writing solution to <sol-00.gnuplot>...
DEAL::
DEAL::Step 1
DEAL::Triangulation 25 cells, 3 levels
DEAL::DoFHandler 400 dofs, level dofs 64 256 192
DEAL::Assemble matrix
DEAL::Assemble multilevel matrix
DEAL::Assemble right hand side
DEAL::Solve
DEAL:cg::Starting value 37.4071
DEAL:cg::Convergence step 14 value 3.72262e-13
DEAL::energy-error: 0.258559
DEAL::L2-error:     0.00288510
DEAL::Estimate 0.738624
DEAL::Writing solution to <sol-01.gnuplot>...
DEAL::
DEAL::Step 2
DEAL::Triangulation 34 cells, 4 levels
DEAL::DoFHandler 544 dofs, level dofs 64 256 256 128
DEAL::Assemble matrix
DEAL::Assemble multilevel matrix
DEAL::Assemble right hand side
DEAL::Solve
DEAL:cg::Starting value 37.4071
DEAL:cg::Convergence step 15 value 1.91610e-13
DEAL::energy-error: 0.189234
DEAL::L2-error:     0.00147954
DEAL::Estimate 0.657507
DEAL::Writing solution to <sol-02.gnuplot>...

...

DEAL::Step 10
DEAL::Triangulation 232 cells, 11 levels
DEAL::DoFHandler 3712 dofs, level dofs 64 256 896 768 768 640 512 256 256 256 256
DEAL::Assemble matrix
DEAL::Assemble multilevel matrix
DEAL::Assemble right hand side
DEAL::Solve
DEAL:cg::Starting value 51.1571
DEAL:cg::Convergence step 15 value 7.19599e-13
DEAL::energy-error: 0.0132475
DEAL::L2-error:     1.00423e-05
DEAL::Estimate 0.0470724
DEAL::Writing solution to <sol-10.gnuplot>...
DEAL::
DEAL::Step 11
DEAL::Triangulation 322 cells, 12 levels
DEAL::DoFHandler 5152 dofs, level dofs 64 256 1024 1024 896 768 768 640 448 320 320 320
DEAL::Assemble matrix
DEAL::Assemble multilevel matrix
DEAL::Assemble right hand side
DEAL::Solve
DEAL:cg::Starting value 52.2226
DEAL:cg::Convergence step 15 value 8.15195e-13
DEAL::energy-error: 0.00934891
DEAL::L2-error:     5.41095e-06
DEAL::Estimate 0.0329102
DEAL::Writing solution to <sol-11.gnuplot>...
DEAL::
@endcode

This log for instance shows that the number of conjugate gradient
iteration steps is constant at approximately 15.

<h3>Postprocessing of the logfile</h3>

<img src="https://www.dealii.org/images/steps/developer/step-39-convergence.svg" alt="">
Using the perl script <tt>postprocess.pl</tt>, we extract relevant
data into <tt>output.dat</tt>, which can be used to plot graphs with
<tt>gnuplot</tt>. The graph above for instance was produced using the gnuplot
script <tt>plot_errors.gpl</tt> via

@code
perl postprocess.pl deallog &> output.dat
gnuplot plot_errors.gpl
@endcode

Reference data can be found in <tt>output.reference.dat</tt>.


examples/step-4/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

@dealiiVideoLecture{12,13}

deal.II has a unique feature which we call
``dimension independent programming''. You may have noticed in the
previous examples that many classes had a number in angle brackets
suffixed to them. This is to indicate that for example the
triangulation in two and three space dimensions are different, but
related data %types. We could as well have called them
<code>Triangulation2d</code> and <code>Triangulation3d</code> instead
of <code>Triangulation@<2@></code> and
<code>Triangulation@<3@></code> to name the two classes, but this
has an important drawback: assume you have a function which does
exactly the same functionality, but on 2d or 3d triangulations,
depending on which dimension we would like to solve the equation in
presently (if you don't believe that it is the common case that a
function does something that is the same in all dimensions, just take
a look at the code below - there are almost no distinctions between 2d
and 3d!). We would have to write the same function twice, once
working on <code>Triangulation2d</code> and once working with a
<code>Triangulation3d</code>. This is an unnecessary obstacle in
programming and leads to a nuisance to keep the two function in sync
(at best) or difficult to find errors if the two versions get out of
sync (at worst; this would probably the more common case).




Such obstacles can be circumvented by using some template magic as
provided by the C++ language: templatized classes and functions are
not really classes or functions but only a pattern depending on an
as-yet undefined data type parameter or on a numerical value which is
also unknown at the point of definition. However, the compiler can
build proper classes or functions from these templates if you provide
it with the information that is needed for that. Of course, parts of
the template can depend on the template parameters, and they will be
resolved at the time of compilation for a specific template
parameter. For example, consider the following piece of code:
@code
  template <int dim>
  void make_grid (Triangulation<dim> &triangulation)
  {
    GridGenerator::hyper_cube (triangulation, -1, 1);
  };
@endcode



At the point where the compiler sees this function, it does not know
anything about the actual value of <code>dim</code>. The only thing the compiler has is
a template, i.e. a blueprint, to generate
functions <code>make_grid</code> if given a particular value of
<code>dim</code>. Since <code>dim</code> has an unknown value, there is no
code the compiler can generate for the moment.



However, if later down the compiler would encounter code that looks, for
example, like this,
@code
  Triangulation<2> triangulation;
  make_grid (triangulation);
@endcode
then the compiler will deduce that the function <code>make_grid</code> for
<code>dim==2</code> was
requested and will compile the template above into a function with dim replaced
by 2 everywhere, i.e. it will compile the function as if it were defined
as
@code
  void make_grid (Triangulation<2> &triangulation)
  {
    GridGenerator::hyper_cube (triangulation, -1, 1);
  };
@endcode



However, it is worth to note that the function
<code>GridGenerator::hyper_cube</code> depends on the dimension as
well, so in this case, the compiler will call the function
<code>GridGenerator::hyper_cube@<2@></code> while if dim were 3,
it would call <code>GridGenerator::hyper_cube@<3@></code> which
might be (and actually is) a totally unrelated  function.



The same can be done with member variables. Consider the following
function, which might in turn call the above one:
@code
  template <int dim>
  void make_grid_and_dofs (Triangulation<dim> &triangulation)
  {
    make_grid (triangulation);

    DoFHandler<dim> dof_handler(triangulation);
    ...
  };
@endcode
This function has a member variable of type
<code>DoFHandler@<dim@></code>. Again, the compiler can't
compile this function until it knows for which dimension. If you call
this function for a specific dimension as above, the compiler will
take the template, replace all occurrences of dim by the dimension for
which it was called, and compile it. If you call the function several
times for different dimensions, it will compile it several times, each
time calling the right <code>make_grid</code> function and reserving the right
amount of memory for the member variable; note that the size of a
<code>DoFHandler</code> might, and indeed does, depend on the space dimension.



The deal.II library is built around this concept
of dimension-independent programming, and therefore allows you to program in
a way that will not need to
distinguish between the space dimensions. It should be noted that in
only a very few places is it necessary to actually compare the
dimension using <code>if</code>s or <code>switch</code>es. However, since the compiler
has to compile each function for each dimension separately, even there
it knows the value of <code>dim</code> at the time of compilation and will
therefore be able to optimize away the <code>if</code> statement along with the
unused branch.



In this example program, we will show how to program dimension
independently (which in fact is even simpler than if you had to take
care about the dimension) and we will extend the Laplace problem of
the last example to a program that runs in two and three space
dimensions at the same time. Other extensions are the use of a
non-constant right hand side function and of non-zero boundary values.


@note When using templates, C++ imposes all sorts of syntax constraints that
make it sometimes a bit difficult to understand why exactly something has to
be written this way. A typical example is the need to use the keyword
<code>typename</code> in so many places. If you are not entirely familiar with
this already, then several of these difficulties are explained in the deal.II
Frequently Asked Questions (FAQ) linked to from the <a
href="http://www.dealii.org/">deal.II homepage</a>.

<!--We need a blank line to end the above block properly.-->


examples/step-4/doc/results.dox
<h1>Results</h1>


The output of the program looks as follows (the number of iterations
may vary by one or two, depending on your computer, since this is
often dependent on the round-off accuracy of floating point
operations, which differs between processors):
@code
Solving problem in 2 space dimensions.
   Number of active cells: 256
   Total number of cells: 341
   Number of degrees of freedom: 289
   26 CG iterations needed to obtain convergence.
Solving problem in 3 space dimensions.
   Number of active cells: 4096
   Total number of cells: 4681
   Number of degrees of freedom: 4913
   30 CG iterations needed to obtain convergence.
@endcode
It is obvious that in three spatial dimensions the number of cells and
therefore also the number of degrees of freedom is
much higher. What cannot be seen here, is that besides this higher
number of rows and columns in the matrix, there are also significantly
more entries per row of the matrix in three space
dimensions. Together, this leads to a much higher numerical effort for
solving the system of equation, which you can feel in the run time of the two
solution steps when you actually run the program.



The program produces two files: <code>solution-2d.vtk</code> and
<code>solution-3d.vtk</code>, which can be viewed using the programs
VisIt or Paraview (in case you do not have these programs, you can easily
change the
output format in the program to something which you can view more
easily). Visualizing solutions is a bit of an art, but it can also be fun, so
you should play around with your favorite visualization tool to get familiar
with its functionality. Here's what I have come up with for the 2d solution:

<p align="center">
  <img src="https://www.dealii.org/images/steps/developer/step-4.solution-2d.png" alt="">
</p>

(@dealiiVideoLectureSeeAlso{11,32})
The picture shows the solution of the problem under consideration as
a 3D plot. As can be seen, the solution is almost flat in the interior
of the domain and has a higher curvature near the boundary. This, of
course, is due to the fact that for Laplace's equation the curvature
of the solution is equal to the right hand side and that was chosen as
a quartic polynomial which is nearly zero in the interior and is only
rising sharply when approaching the boundaries of the domain; the
maximal values of the right hand side function are at the corners of
the domain, where also the solution is moving most rapidly.
It is also nice to see that the solution follows the desired quadratic
boundary values along the boundaries of the domain.
It can also be useful to verify a computed solution against an analytical
solution. For an explanation of this technique, see step-7.

On the other hand, even though the picture does not show the mesh lines
explicitly, you can see them as little kinks in the solution. This clearly
indicates that the solution hasn't been computed to very high accuracy and
that to get a better solution, we may have to compute on a finer mesh.

In three spatial dimensions, visualization is a bit more difficult. The left
picture shows the solution and the mesh it was computed on on the surface of
the domain. This is nice, but it has the drawback that it completely hides
what is happening on the inside. The picture on the right is an attempt at
visualizing the interior as well, by showing surfaces where the solution has
constant values (as indicated by the legend at the top left). Isosurface
pictures look best if one makes the individual surfaces slightly transparent
so that it is possible to see through them and see what's behind.

<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-4.solution-3d.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-4.contours-3d.png" alt="">
    </td>
  </tr>
</table>

@note
A final remark on visualization: the idea of visualization is to give insight,
which is not the same as displaying information. In particular, it is easy to
overload a picture with information, but while it shows more information it
makes it also more difficult to glean insight. As an example, the program I
used to generate these pictures, VisIt, by default puts tick marks on every
axis, puts a big fat label "X Axis" on the $x$ axis and similar for the other
axes, shows the file name from which the data was taken in the top left and
the name of the user doing so and the time and date on the bottom right. None
of this is important
here: the axes are equally easy to make out because the tripod at the bottom
left is still visible, and we know from the program that the domain is
$[-1,1]^3$, so there is no need for tick marks. As a consequence, I have
switched off all the extraneous stuff in the picture: the art of visualization
is to reduce the picture to those parts that are important to see what one
wants to see, but no more.



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>


Essentially the possibilities for playing around with the program are the same
as for the previous one, except that they will now also apply to the 3d
case. For inspiration read up on <a href="step_3.html#extensions"
target="body">possible extensions in the documentation of step 3</a>.


examples/step-40/doc/intro.dox
<br>

<i>This program was contributed by Timo Heister, Martin Kronbichler and Wolfgang
Bangerth.
<br>
This material is based upon work partly supported by the National
Science Foundation under Award No. EAR-0426271 and The California Institute of
Technology. Any opinions, findings, and conclusions or recommendations
expressed in this publication are those of the author and do not
necessarily reflect the views of the National Science Foundation or of The
California Institute of Technology.
</i>


@note As a prerequisite of this program, you need to have both PETSc and the
p4est library installed. The installation of deal.II
together with these two additional libraries is described in the <a
href="../../readme.html" target="body">README</a> file. Note also that
to work properly, this program needs access to the Hypre
preconditioner package implementing algebraic multigrid; it can be
installed as part of PETSc but has to be explicitly enabled during
PETSc configuration; see the page linked to from the installation
instructions for PETSc.


<a name="Intro"></a>
<h1>Introduction</h1>

@dealiiVideoLecture{41.5,41.75}

Given today's computers, most finite element computations can be done on
a single machine. The majority of previous tutorial programs therefore
shows only this, possibly splitting up work among a number of
processors that, however, can all access the same, shared memory
space. That said, there are problems that are simply too big for a
single machine and in that case the problem has to be split up in a
suitable way among multiple machines each of which contributes its
part to the whole. A simple way to do that was shown in step-17 and
step-18, where we show how a program can use <a
href="http://www.mpi-forum.org/" target="_top">MPI</a> to parallelize
assembling the linear system, storing it, solving it, and computing
error estimators. All of these operations scale relatively trivially
(for a definition of what it means for an operation to "scale", see
@ref GlossParallelScaling "this glossary entry"),
but there was one significant drawback: for this to be moderately
simple to implement, each MPI processor had to keep its own copy of
the entire Triangulation and DoFHandler objects. Consequently, while
we can suspect (with good reasons) that the operations listed above
can scale to thousands of computers and problem sizes of billions of
cells and billions of degrees of freedom, building the one big mesh for the
entire problem these thousands of computers are solving on every last
processor is clearly not going to scale: it is going to take forever,
and maybe more importantly no single machine will have enough memory
to store a mesh that has a billion cells (at least not at the time of
writing this). In reality, programs like step-17 and step-18 can
therefore not be run on more than maybe 100 or 200 processors and even
there storing the Triangulation and DoFHandler objects consumes the
vast majority of memory on each machine.

Consequently, we need to approach the problem differently: to scale to
very large problems each processor can only store its own little piece
of the Triangulation and DoFHandler objects. deal.II implements such a
scheme in the parallel::distributed namespace and the classes
therein. It builds on an external library, <a
href="http://www.p4est.org/">p4est</a> (a play on the expression
<i>parallel forest</i> that describes the parallel storage of a
hierarchically constructed mesh as a forest of quad- or
oct-trees). You need to <a
href="../../external-libs/p4est.html">install and configure p4est</a>
but apart from that all of its workings are hidden under the surface
of deal.II.

In essence, what the parallel::distributed::Triangulation class and
code inside the DoFHandler class do is to split
the global mesh so that every processor only stores a small bit it
"owns" along with one layer of "ghost" cells that surround the ones it
owns. What happens in the rest of the domain on which we want to solve
the partial differential equation is unknown to each processor and can
only be inferred through communication with other machines if such
information is needed. This implies that we also have to think about
problems in a different way than we did in, for example, step-17 and
step-18: no processor can have the entire solution vector for
postprocessing, for example, and every part of a program has to be
parallelized because no processor has all the information necessary
for sequential operations.

A general overview of how this parallelization happens is described in
the @ref distributed documentation module. You should read it for a
top-level overview before reading through the source code of this
program. A concise discussion of many terms we will use in the program
is also provided in the @ref distributed_paper "Distributed Computing paper".
It is probably worthwhile reading it for background information on how
things work internally in this program.


<h3>The testcase</h3>

This program essentially re-solves what we already do in
step-6, i.e. it solves the Laplace equation
@f{align*}
  -\Delta u &= f \qquad &&\text{in}\ \Omega=[0,1]^2, \\
  u &= 0 \qquad &&\text{on}\ \partial\Omega.
@f}
The difference of course is now that we want to do so on a mesh that
may have a billion cells, with a billion or so degrees of
freedom. There is no doubt that doing so is completely silly for such
a simple problem, but the point of a tutorial program is, after all,
not to do something useful but to show how useful programs can be
implemented using deal.II. Be that as it may, to make things at least
a tiny bit interesting, we choose the right hand side as a
discontinuous function,
@f{align*}
  f(x,y)
  =
  \left\{
  \begin{array}{ll}
    1 & \text{if}\ y > \frac 12 + \frac 14 \sin(4\pi x), \\
    -1 & \text{otherwise},
  \end{array}
  \right.
@f}
so that the solution has a singularity along the sinusoidal line
snaking its way through the domain. As a consequence, mesh refinement
will be concentrated along this line. You can see this in the mesh
picture shown below in the results section.

Rather than continuing here and giving a long introduction, let us go
straight to the program code. If you have read through step-6 and the
@ref distributed documentation module, most of things that are going
to happen should be familiar to you already. In fact, comparing the two
programs you will notice that the additional effort necessary to make things
work in %parallel is almost insignificant: the two programs have about the
same number of lines of code (though step-6 spends more space on dealing with
coefficients and output). In either case, the comments below will only be on
the things that set step-40 apart from step-6 and that aren't already covered
in the @ref distributed documentation module.


@note This program will be able to compute on as many processors as you want
to throw at it, and for as large a problem as you have the memory and patience
to solve. However, there <i>is</i> a limit: the number of unknowns can not
exceed the largest number that can be stored with an object of type
types::global_dof_index. By default, this is an alias for <code>unsigned
int</code>, which on most machines today is a 32-bit integer, limiting you to
some 4 billion (in reality, since this program uses PETSc, you will be limited
to half that as PETSc uses signed integers). However, this can be changed
during configuration to use 64-bit integers, see the ReadMe file. This will
give problem sizes you are unlikely to exceed anytime soon.


examples/step-40/doc/results.dox
<h1>Results</h1>

When you run the program, on a single processor or with your local MPI
installation on a few, you should get output like this:
@code
Cycle 0:
   Number of active cells:       1024
   Number of degrees of freedom: 4225
   Solved in 10 iterations.


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |     0.176s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| assembly                        |         1 |    0.0209s |        12% |
| output                          |         1 |    0.0189s |        11% |
| setup                           |         1 |    0.0299s |        17% |
| solve                           |         1 |    0.0419s |        24% |
+---------------------------------+-----------+------------+------------+


Cycle 1:
   Number of active cells:       1954
   Number of degrees of freedom: 8399
   Solved in 10 iterations.


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |     0.327s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| assembly                        |         1 |    0.0368s |        11% |
| output                          |         1 |    0.0208s |       6.4% |
| refine                          |         1 |     0.157s |        48% |
| setup                           |         1 |    0.0452s |        14% |
| solve                           |         1 |    0.0668s |        20% |
+---------------------------------+-----------+------------+------------+


Cycle 2:
   Number of active cells:       3664
   Number of degrees of freedom: 16183
   Solved in 11 iterations.

...
@endcode

The exact numbers differ, depending on how many processors we use;
this is due to the fact that the preconditioner depends on the
partitioning of the problem, the solution then differs in the last few
digits, and consequently the mesh refinement differs slightly.
The primary thing to notice here, though, is that the number of
iterations does not increase with the size of the problem. This
guarantees that we can efficiently solve even the largest problems.

When run on a sufficiently large number of machines (say a few
thousand), this program can relatively easily solve problems with well
over one billion unknowns in less than a minute. On the other hand,
such big problems can no longer be visualized, so we also ran the
program on only 16 processors. Here are a mesh, along with its
partitioning onto the 16 processors, and the corresponding solution:

<table width="100%">
<tr>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-40.mesh.png" alt="">
</td>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-40.solution.png" alt="">
</td>
</tr>
</table>

The mesh on the left has a mere 7,069 cells. This is of course a
problem we would easily have been able to solve already on a single
processor using step-6, but the point of the program was to show how
to write a program that scales to many more machines. For example,
here are two graphs that show how the run time of a large number of parts
of the program scales on problems with around 52 and 375 million degrees of
freedom if we take more and more processors (these and the next couple of
graphs are taken from an earlier version of the
@ref distributed_paper "Distributed Computing paper"; updated graphs showing
data of runs on even larger numbers of processors, and a lot
more interpretation can be found in the final version of the paper):

<table width="100%">
<tr>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-40.strong2.png" alt="">
</td>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-40.strong.png" alt="">
</td>
</tr>
</table>

As can clearly be seen, the program scales nicely to very large
numbers of processors.
(For a discussion of what we consider "scalable" programs, see
@ref GlossParallelScaling "this glossary entry".)
The curves, in particular the linear solver, become a
bit wobbly at the right end of the graphs since each processor has too little
to do to offset the cost of communication (the part of the whole problem each
processor has to solve in the above two examples is only 13,000 and 90,000
degrees of freedom when 4,096 processors are used; a good rule of thumb is that
parallel programs work well if each processor has at least 100,000 unknowns).

While the strong scaling graphs above show that we can solve a problem of
fixed size faster and faster if we take more and more processors, the more
interesting question may be how big problems can become so that they can still
be solved within a reasonable time on a machine of a particular size. We show
this in the following two graphs for 256 and 4096 processors:

<table width="100%">
<tr>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-40.256.png" alt="">
</td>
<td>
  <img src="https://www.dealii.org/images/steps/developer/step-40.4096.png" alt="">
</td>
</tr>
</table>

What these graphs show is that all parts of the program scale linearly with
the number of degrees of freedom. This time, lines are wobbly at the left as
the size of local problems is too small. For more discussions of these results
we refer to the @ref distributed_paper "Distributed Computing paper".

So how large are the largest problems one can solve? At the time of writing
this problem, the
limiting factor is that the program uses the BoomerAMG algebraic
multigrid method from the <a
href="http://acts.nersc.gov/hypre/" target="_top">Hypre package</a> as
a preconditioner, which unfortunately uses signed 32-bit integers to
index the elements of a %distributed matrix. This limits the size of
problems to $2^{31}-1=2,147,483,647$ degrees of freedom. From the graphs
above it is obvious that the scalability would extend beyond this
number, and one could expect that given more than the 4,096 machines
shown above would also further reduce the compute time. That said, one
can certainly expect that this limit will eventually be lifted by the
hypre developers.

On the other hand, this does not mean that deal.II cannot solve bigger
problems. Indeed, step-37 shows how one can solve problems that are not
just a little, but very substantially larger than anything we have shown
here.



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

In a sense, this program is the ultimate solver for the Laplace
equation: it can essentially solve the equation to whatever accuracy
you want, if only you have enough processors available. Since the
Laplace equation by itself is not terribly interesting at this level
of accuracy, the more interesting possibilities for extension
therefore concern not so much this program but what comes beyond
it. For example, several of the other programs in this tutorial have
significant run times, especially in 3d. It would therefore be
interesting to use the techniques explained here to extend other
programs to support parallel distributed computations. We have done
this for step-31 in the step-32 tutorial program, but the same would
apply to, for example, step-23 and step-25 for hyperbolic time
dependent problems, step-33 for gas dynamics, or step-35 for the
Navier-Stokes equations.

Maybe equally interesting is the problem of postprocessing. As
mentioned above, we only show pictures of the solution and the mesh
for 16 processors because 4,096 processors solving 1 billion unknowns
would produce graphical output on the order of several 10
gigabyte. Currently, no program is able to visualize this amount of
data in any reasonable way unless it also runs on at least several
hundred processors. There are, however, approaches where visualization
programs directly communicate with solvers on each processor with each
visualization process rendering the part of the scene computed by the
solver on this processor. Implementing such an interface would allow
to quickly visualize things that are otherwise not amenable to
graphical display.


examples/step-41/doc/intro.dox
<br>

<i>This program was contributed by Jörg Frohne (University of Siegen,
Germany) while on a long-term visit to Texas A&amp;M University.
<br>
This material is based upon work partly supported by ThyssenKrupp Steel Europe.
</i>


<a name="Intro"></a>
<h3>Introduction</h3>

This example is based on the Laplace equation in 2d and deals with the
question what happens if a membrane is deflected by some external force but is
also constrained by an obstacle. In other words, think of a elastic membrane
clamped at the boundary to a rectangular frame (we choose $\Omega =
\left[-1,1\right]^2$) and that sags through due to gravity acting on it. What
happens now if there is an obstacle under the membrane that prevents it from
reaching its equilibrium position if gravity was the only existing force? In
the current example program, we will consider that under the membrane is a
stair step obstacle against which gravity pushes the membrane.

This problem is typically called the "obstacle problem" (see also <a
href="http://en.wikipedia.org/wiki/Obstacle_problem">this Wikipedia article</a>), and it results in a
variational inequality, rather than a variational equation when put into the
weak form. We will below derive it from the classical formulation, but before we
go on to discuss the mathematics let us show how the solution of the problem we
will consider in this tutorial program looks to gain some intuition of what
we should expect:

<table align="center" class="tutorial" cellspacing="3" cellpadding="3">
  <tr>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.png" alt="">
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-41.active-set.png" alt="">
    </td>
  </tr>
</table>

Here, at the left, we see the displacement of the membrane. The shape
of the obstacle underneath is clearly visible. On the right, we overlay which
parts of the membrane are in contact with the obstacle. We will later call
this set of points the "active set" to indicate that an inequality constraint
is active there.


<h3>Classical formulation</h3>

The classical formulation of the problem possesses the following form:
@f{align*}
 -\textrm{div}\ \sigma &\geq f & &\quad\text{in } \Omega,\\
 \sigma &= \nabla u & &\quad\text{in } \Omega,\\
 u(\mathbf x) &= 0 & &\quad\text{on }\partial\Omega,\\
(-\Delta u - f)(u - g) &= 0 & &\quad\text{in } \Omega,\\
 u(\mathbf x) &\geq g(\mathbf x) & &\quad\text{in } \Omega
@f}
with $u\in H^2(\Omega)$.  $u$ is a scalar valued function that denotes the
vertical displacement of the membrane. The first equation is called equilibrium
condition with a force of areal density $f$. Here, we will consider this force
to be gravity. The second one is known as Hooke's Law that says that the stresses
$\sigma$ are proportional to the gradient of the displacements $u$ (the
proportionality constant, often denoted by $E$, has been set to one here,
without loss of generality; if it is constant, it can be put into the right
hand side function). At the boundary we have zero Dirichlet
conditions. Obviously, the first two equations can be combined to yield
$-\Delta u \ge f$.

Intuitively, gravity acts downward and so $f(\mathbf x)$ is a negative
function (we choose $f=-10$ in this program). The first condition then means
that the total force acting on the membrane is gravity plus something
positive: namely the upward force that the obstacle exerts on the membrane at
those places where the two of them are in contact. How big is this additional
force? We don't know yet (and neither do we know "where" it actually acts) but
it must be so that the membrane doesn't penetrate the obstacle.

The fourth equality above together with the last inequality forms the obstacle
condition which has to hold at every point of the whole domain. The latter of
these two means that the membrane must be above the obstacle $g(\mathbf x)$
everywhere. The second to last equation, often called the "complementarity
condition" says that where the membrane is not in contact with the obstacle
(i.e., those $\mathbf x$ where $u(\mathbf x) - g(\mathbf x) \neq 0$), then
$-\Delta u=f$ at these locations; in other words, no additional forces act
there, as expected. On the other hand, where $u=g$ we can have $-\Delta u-f
\neq 0$, i.e., there can be additional forces (though there don't have to be:
it is possible for the membrane to just touch, not press against, the
obstacle).


<h3>Derivation of the variational inequality</h3>

An obvious way to obtain the variational formulation of the obstacle problem is to consider the total potential energy:
@f{equation*}
 E(u) \dealcoloneq \dfrac{1}{2}\int\limits_{\Omega} \nabla u \cdot \nabla u - \int\limits_{\Omega} fu.
@f}
We have to find a solution $u\in G$ of the following minimization problem:
@f{equation*}
 E(u)\leq E(v)\quad \forall v\in G,
@f}
with the convex set of admissible displacements:
@f{equation*}
 G \dealcoloneq \lbrace v\in V: v\geq g \text{ a.e. in } \Omega\rbrace,\quad V\dealcoloneq H^1_0(\Omega).
@f}
This set takes care of the third and fifth conditions above (the boundary
values and the complementarity condition).

Consider now the minimizer $u\in G$ of $E$ and any other function $v\in
G$. Then the function
@f{equation*}
 F(\varepsilon) \dealcoloneq E(u+\varepsilon(v-u)),\quad\varepsilon\in\left[0,1\right],
@f}
takes its minimum at $\varepsilon = 0$ (because $u$ is a minimizer of the
energy functional $E(\cdot)$), so that $F'(0)\geq 0$ for any choice
of $v$. Note that
$u+\varepsilon(v-u) = (1-\varepsilon)u+\varepsilon v\in G$ because of the
convexity of $G$. If we compute $F'(\varepsilon)\vert_{\varepsilon=0}$ it
yields the variational formulation we are searching for:

<i>Find a function $u\in G$ with</i>
@f{equation*}
 \left(\nabla u, \nabla(v-u)\right) \geq \left(f,v-u\right) \quad \forall v\in G.
@f}

This is the typical form of variational inequalities, where not just $v$
appears in the bilinear form but in fact $v-u$. The reason is this: if $u$ is
not constrained, then we can find test functions $v$ in $G$ so that $v-u$ can have
any sign. By choosing test functions $v_1,v_2$ so that $v_1-u = -(v_2-u)$ it
follows that the inequality can only hold for both $v_1$ and $v_2$ if the two
sides are in fact equal, i.e., we obtain a variational equality.

On the other hand, if $u=g$ then $G$ only allows test functions $v$ so that in fact
$v-u\ge 0$. This means that we can't test the equation with both $v-u$ and
$-(v-u)$ as above, and so we can no longer conclude that the two sides are in
fact equal. Thus, this mimics the way we have discussed the complementarity
condition above.



<h3>Formulation as a saddle point problem</h3>

The variational inequality above is awkward to work with. We would therefore
like to reformulate it as an equivalent saddle point problem. We introduce a
Lagrange multiplier $\lambda$ and the convex cone $K\subset V'$, $V'$
dual space of $V$, $K \dealcoloneq \{\mu\in V': \langle\mu,v\rangle\geq 0,\quad \forall
v\in V, v \le 0 \}$ of
Lagrange multipliers, where $\langle\cdot,\cdot\rangle$ denotes the duality
pairing between $V'$ and $V$. Intuitively, $K$ is the cone of all "non-positive
functions", except that $K\subset (H_0^1)'$ and so contains other objects
besides regular functions as well.
This yields:

<i>Find $u\in V$ and $\lambda\in K$ such that</i>
@f{align*}
 a(u,v) + b(v,\lambda) &= f(v),\quad &&v\in V\\
 b(u,\mu - \lambda) &\leq \langle g,\mu - \lambda\rangle,\quad&&\mu\in K,
@f}
<i>with</i>
@f{align*}
 a(u,v) &\dealcoloneq \left(\nabla u, \nabla v\right),\quad &&u,v\in V\\
 b(u,\mu) &\dealcoloneq \langle u,\mu\rangle,\quad &&u\in V,\quad\mu\in V'.
@f}
In other words, we can consider $\lambda$ as the negative of the additional, positive force that the
obstacle exerts on the membrane. The inequality in the second line of the
statement above only appears to have the wrong sign because we have
$\mu-\lambda<0$ at points where $\lambda=0$, given the definition of $K$.

The existence and uniqueness of $(u,\lambda)\in V\times K$ of this saddle
point problem has been stated in Glowinski, Lions and Tr&eacute;moli&egrave;res: Numerical Analysis of Variational
Inequalities, North-Holland, 1981.



<h3>Active Set methods to solve the saddle point problem</h3>

There are different methods to solve the variational inequality. As one
possibility you can understand the saddle point problem as a convex quadratic program (QP) with
inequality constraints.

To get there, let us assume that we discretize both $u$ and $\lambda$ with the
same finite element space, for example the usual $Q_k$ spaces. We would then
get the equations
@f{eqnarray*}
 &A U + B\Lambda = F,&\\
 &[BU-G]_i \geq 0, \quad \Lambda_i \leq 0,\quad \Lambda_i[BU-G]_i = 0
\qquad \forall i.&
@f}
where $B$ is the mass matrix on the chosen finite element space and the
indices $i$ above are for all degrees of freedom in the set $\cal S$ of degrees of
freedom located in the interior of the domain
(we have Dirichlet conditions on the perimeter). However, we
can make our life simpler if we use a particular quadrature rule when
assembling all terms that yield this mass matrix, namely a quadrature formula
where quadrature points are only located at the interpolation points at
which shape functions are defined; since all but one shape function are zero
at these locations, we get a diagonal mass matrix with
@f{align*}
  B_{ii} = \int_\Omega \varphi_i(\mathbf x)^2\ \textrm{d}x,
  \qquad
  B_{ij}=0 \ \text{for } i\neq j.
@f}
To define $G$ we use the same technique as for $B$. In other words, we
define
@f{align*}
  G_{i} = \int_\Omega g_h(x) \varphi_i(\mathbf x)\ \textrm{d}x,
@f}
where $g_h$ is a suitable approximation of $g$. The integral in the definition
of $B_{ii}$ and $G_i$ are then approximated by the trapezoidal rule.
With this, the equations above can be restated as
@f{eqnarray*}
 &A U + B\Lambda = F,&\\
 &U_i-B_{ii}^{-1}G_i \ge 0, \quad \Lambda_i \leq 0,\quad \Lambda_i[U_i-B_{ii}^{-1}G_i] = 0
\qquad \forall i\in{\cal S}.&
@f}

Now we define for each degree of freedom $i$ the function
@f{equation*}
 C([BU]_i,\Lambda_i) \dealcoloneq -\Lambda_i + \min\lbrace 0, \Lambda_i + c([BU]_i - G_i) \rbrace,
@f}
with some $c>0$. (In this program we choose $c = 100$. It is a kind of a
penalty parameter which depends on the problem itself and needs to be chosen
large enough; for example there is no convergence for $c = 1$ using the
current program if we use 7 global refinements.)

After some head-scratching one can then convince oneself that the inequalities
above can equivalently be rewritten as
@f{equation*}
 C([BU]_i,\Lambda_i) = 0, \qquad \forall i\in{\cal S}.
@f}
The primal-dual active set strategy we will use here is an iterative scheme which is based on
this condition to predict the next active and inactive sets $\mathcal{A}_k$ and
$\mathcal{F}_k$ (that is, those complementary sets of indices $i$ for which
$U_i$ is either equal to or not equal to the value of the obstacle
$B^{-1}G$). For a more in depth treatment of this approach, see Hintermueller, Ito, Kunisch: The primal-dual active set
strategy as a semismooth newton method, SIAM J. OPTIM., 2003, Vol. 13, No. 3,
pp. 865-888.

<h3>The primal-dual active set algorithm</h3>

The algorithm for the primal-dual active set method works as follows (NOTE: $B = B^T$):

1. Initialize $\mathcal{A}_k$ and $\mathcal{F}_k$, such that
 $\mathcal{S}=\mathcal{A}_k\cup\mathcal{F}_k$ and
 $\mathcal{A}_k\cap\mathcal{F}_k=\emptyset$ and set $k=1$.
2. Find the primal-dual pair $(U^k,\Lambda^k)$ that satisfies
 @f{align*}
  AU^k + B\Lambda^k &= F,\\
  [BU^k]_i &= G_i\quad&&\forall i\in\mathcal{A}_k,\\
  \Lambda_i^k &= 0\quad&&\forall i\in\mathcal{F}_k.
 @f}
 Note that the second and third conditions imply that exactly $|S|$ unknowns
 are fixed, with the first condition yielding the remaining $|S|$ equations
 necessary to determine both $U$ and $\Lambda$.
3. Define the new active and inactive sets by
 @f{equation*}
 \begin{split}
  \mathcal{A}_{k+1} \dealcoloneq \lbrace i\in\mathcal{S}:\Lambda^k_i + c([BU^k]_i - G_i)< 0\rbrace,\\
  \mathcal{F}_{k+1} \dealcoloneq \lbrace i\in\mathcal{S}:\Lambda^k_i + c([BU^k]_i - G_i)\geq 0\rbrace.
 \end{split}
 @f}
4. If $\mathcal{A}_{k+1}=\mathcal{A}_k$ (and then, obviously, also
 $\mathcal{F}_{k+1}=\mathcal{F}_k$) then stop, else set $k=k+1$ and go to step
 (2).

The method is called "primal-dual" because it uses both primal (the
displacement $U$) as well as dual variables (the Lagrange multiplier
$\Lambda$) to determine the next active set.

At the end of this section, let us add two observations. First,
for any primal-dual pair $(U^k,\Lambda^k)$ that satisfies these
condition, we can distinguish the following cases:

1. $\Lambda^k_i + c([BU^k]_i - G_i) < 0$ (i active):
  <br>
  Then either $[BU^k]_i<G_i$ and $\Lambda^k_i=0$ (penetration) or $\Lambda^k_i<0$ and $[BU^k]_i=G_i$ (pressing load).
2. $\Lambda^k_i + c([BU^k]_i - G_i)\geq 0$ (i inactive):
  <br>
  Then either $[BU^k]_i\geq G_i$ and $\Lambda^k_i=0$ (no contact) or $\Lambda^k_i\geq0$ and $[BU^k]_i=G_i$ (unpressing load).

Second, the method above appears intuitively correct and useful but a bit ad
hoc. However, it can be derived in a concisely in the following way. To this
end, note that we'd like to solve the nonlinear system
@f{eqnarray*}
 &A U + B\Lambda = F,&\\
 &C([BU-G]_i, \Lambda_i) = 0,
\qquad \forall i.&
@f}
We can iteratively solve this by always linearizing around the previous
iterate (i.e., applying a Newton method), but for this we need to linearize
the function $C(\cdot,\cdot)$ that is not differentiable. That said, it is
slantly differentiable, and in fact we have
@f{equation*}
 \dfrac{\partial}{\partial U^k_i}C([BU^k]_i,\Lambda^k_i) = \begin{cases}
                                   cB_{ii},& \text{if}\ \Lambda^k_i + c([BU^k]_i - G_i)< 0\\
                                   0,& \text{if}\ \Lambda^k_i + c([BU^k]_i - G_i)\geq 0.
                                  \end{cases}
@f}
@f{equation*}
 \dfrac{\partial}{\partial\Lambda^k_i}C([BU^k]_i,\Lambda^k_i) = \begin{cases}
                                   0,& \text{if}\ \Lambda^k_i + c([BU^k]_i - G_i)< 0\\
                                   -1,& \text{if}\ \Lambda^k_i + c([BU^k]_i - G_i)\geq 0.
                                  \end{cases}
@f}
This suggest a semismooth Newton step of the form
@f{equation*}
 \begin{pmatrix}
 A_{\mathcal{F}_k\mathcal{F}_k} & A_{\mathcal{F}_k\mathcal{A}_k} & B_{\mathcal{F}_k} & 0\\
 A_{\mathcal{A}_k\mathcal{F}_k} & A_{\mathcal{A}_k\mathcal{A}_k} & 0 & B_{\mathcal{A}_k}\\
 0 & 0 & -Id_{\mathcal{F}_k} & 0\\
 0 & cB_{\mathcal{A}_k} & 0 & 0
\end{pmatrix}
\begin{pmatrix}
 \delta U^k_{\mathcal{F}_k}\\ \delta U^k_{\mathcal{A}_k}\\ \delta \Lambda^k_{\mathcal{F}_k}\\ \delta \Lambda^k_{\mathcal{A}_k}
\end{pmatrix}
=
-\begin{pmatrix}
 (AU^k + \Lambda^k - F)_{\mathcal{F}_k}\\ (AU^k + \Lambda^k - F)_{\mathcal{A}_k}\\ -\Lambda^k_{\mathcal{F}_k}\\ c(B_{\mathcal{A}_k} U^k - G)_{\mathcal{A}_k}
\end{pmatrix},
@f}
where we have split matrices $A,B$ as well as vectors in the natural way into
rows and columns whose indices belong to either the active set
${\mathcal{A}_k}$ or the inactive set ${\mathcal{F}_k}$.

Rather than solving for updates $\delta U, \delta \Lambda$, we can also solve
for the variables we are interested in right away by setting $\delta U^k \dealcoloneq
U^{k+1} - U^k$ and $\delta \Lambda^k \dealcoloneq \Lambda^{k+1} - \Lambda^k$ and
bringing all known terms to the right hand side. This yields
@f{equation*}
\begin{pmatrix}
 A_{\mathcal{F}_k\mathcal{F}_k} & A_{\mathcal{F}_k\mathcal{A}_k} & B_{\mathcal{F}_k} & 0\\
 A_{\mathcal{A}_k\mathcal{F}_k} & A_{\mathcal{A}_k\mathcal{A}_k} & 0 & B_{\mathcal{A}_k}\\
 0 & 0 & Id_{\mathcal{F}_k} & 0\\
 0 & B_{\mathcal{A}_k} & 0 & 0
\end{pmatrix}
\begin{pmatrix}
 U^k_{\mathcal{F}_k}\\ U^k_{\mathcal{A}_k}\\ \Lambda^k_{\mathcal{F}_k}\\ \Lambda^k_{\mathcal{A}_k}
\end{pmatrix}
=
\begin{pmatrix}
 F_{\mathcal{F}_k}\\ F_{\mathcal{A}_k}\\ 0\\ G_{\mathcal{A}_k}
\end{pmatrix}.
@f}
These are the equations outlined above in the description of the basic algorithm.

We could even drive this a bit further.
It's easy to see that we can eliminate the third row and the third column
because it implies $\Lambda_{\mathcal{F}_k} = 0$:
@f{equation*}
\begin{pmatrix}
 A_{\mathcal{F}_k\mathcal{F}_k} & A_{\mathcal{F}_k\mathcal{A}_k} & 0\\
 A_{\mathcal{A}_k\mathcal{F}_k} & A_{\mathcal{A}_k\mathcal{A}_k} & B_{\mathcal{A}_k}\\
 0 & B_{\mathcal{A}_k} & 0
\end{pmatrix}
\begin{pmatrix}
 U^k_{\mathcal{F}_k}\\ U^k_{\mathcal{A}_k}\\ \Lambda^k_{\mathcal{A}_k}
\end{pmatrix}
=
\begin{pmatrix}
 F_{\mathcal{F}_k}\\ F_{\mathcal{A}_k}\\ G_{\mathcal{A}_k}
\end{pmatrix}.
@f}
This shows that one in fact only needs to solve for the Lagrange multipliers
located on the active set. By considering the second row one would then recover
the full Lagrange multiplier vector through
@f{equation*}
 \Lambda^k_S = B^{-1}\left(f_{\mathcal{S}} - A_{\mathcal{S}}U^k_{\mathcal{S}}\right).
@f}
Because of the third row and the fact that $B_{\mathcal{A}_k}$ is a diagonal matrix we are able
to calculate $U^k_{\mathcal{A}_k}=B^{-1}_{\mathcal{A}_k}G_{\mathcal{A}_k}$ directly. We can therefore also write the
linear system as follows:
@f{equation*}
\begin{pmatrix}
 A_{\mathcal{F}_k\mathcal{F}_k} & 0\\
 0 & Id_{\mathcal{A}_k} \\
\end{pmatrix}
\begin{pmatrix}
 U^k_{\mathcal{F}_k}\\ U^k_{\mathcal{A}_k}
\end{pmatrix}
=
\begin{pmatrix}
 F_{\mathcal{F}_k} - A_{\mathcal{F}_k\mathcal{A}_k}B^{-1}_{\mathcal{A}_k}G_{\mathcal{A}_k}
 \\
 B_{\mathcal{A}_k}^{-1}G_{\mathcal{A}_k}
\end{pmatrix}.
@f}
Fortunately, this form is easy to arrive at: we simply build the usual Laplace
linear system
@f{equation*}
\begin{pmatrix}
 A_{\mathcal{F}_k\mathcal{F}_k} & A_{\mathcal{F}_k\mathcal{A}_k} \\
 A_{\mathcal{A}_k\mathcal{F}_k} & A_{\mathcal{A}_k\mathcal{A}_k}
\end{pmatrix}
\begin{pmatrix}
 U^k_{\mathcal{F}_k}\\ U^k_{\mathcal{A}_k}
\end{pmatrix}
=
\begin{pmatrix}
 F_{\mathcal{F}_k}\\ F_{\mathcal{A}_k}
\end{pmatrix},
@f}
and then let the AffineConstraints class eliminate all constrained degrees of
freedom, namely $U^k_{\mathcal{A}_k}=B^{-1}_{\mathcal{A}_k}G_{\mathcal{A}_k}$,
in the same way as if the dofs in $\mathcal{A}_k$ were Dirichlet data. The
result linear system (the second to last one above) is symmetric and positive
definite and we solve it with a CG-method
and the AMG preconditioner from Trilinos.


<h3>Implementation</h3>

This tutorial is quite similar to step-4. The general structure of the program
follows step-4 with minor differences:
- We need two new methods, <code>assemble_mass_matrix_diagonal</code> and
  <code>update_solution_and_constraints</code>.
- We need new member variables that denote the constraints we have here.
- We change the preconditioner for the solver.


You may want to read up on step-4 if you want to understand the
current program.


examples/step-41/doc/results.dox
<h1>Results</h1>

Running the program produces output like this:
@code
Number of active cells: 16384
Total number of cells: 21845
Number of degrees of freedom: 16641

Newton iteration 0
   Assembling system...
   Solving system...
      Error: 0.310059 -> 5.16619e-05 in 5 CG iterations.
   Updating active set...
      Size of active set: 13164
   Residual of the non-contact part of the system: 1.61863e-05
   Writing graphical output...

Newton iteration 1
   Assembling system...
   Solving system...
      Error: 1.11987 -> 0.00109377 in 6 CG iterations.
   Updating active set...
      Size of active set: 12363
   Residual of the non-contact part of the system: 3.9373
   Writing graphical output...

...

Newton iteration 17
   Assembling system...
   Solving system...
      Error: 0.00713308 -> 2.29249e-06 in 4 CG iterations.
   Updating active set...
      Size of active set: 5399
   Residual of the non-contact part of the system: 0.000957525
   Writing graphical output...

Newton iteration 18
   Assembling system...
   Solving system...
      Error: 0.000957525 -> 2.8033e-07 in 4 CG iterations.
   Updating active set...
      Size of active set: 5399
   Residual of the non-contact part of the system: 2.8033e-07
   Writing graphical output...
@endcode

The iterations end once the active set doesn't change any more (it has
5,399 constrained degrees of freedom at that point). The algebraic
precondition is apparently working nicely since we only need 4-6 CG
iterations to solve the linear system (although this also has a lot to
do with the fact that we are not asking for very high accuracy of the
linear solver).

More revealing is to look at a sequence of graphical output files
(every third step is shown, with the number of the iteration in the
leftmost column):

<table align="center">
  <tr>
    <td valign="top">
      0 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.00.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.active-set.00.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.3d.00.png" alt="">
    </td>
  </tr>
  <tr>
    <td valign="top">
      3 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.03.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.active-set.03.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.3d.03.png" alt="">
    </td>
  </tr>
  <tr>
    <td valign="top">
      6 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.06.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.active-set.06.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.3d.06.png" alt="">
    </td>
  </tr>
  <tr>
    <td valign="top">
      9 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.09.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.active-set.09.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.3d.09.png" alt="">
    </td>
  </tr>
  <tr>
    <td valign="top">
      12 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.12.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.active-set.12.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.3d.12.png" alt="">
    </td>
  </tr>
  <tr>
    <td valign="top">
      15 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.15.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.active-set.15.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.3d.15.png" alt="">
    </td>
  </tr>
  <tr>
    <td valign="top">
      18 &nbsp;
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.18.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.active-set.18.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.displacement.3d.18.png" alt="">
    </td>
  </tr>
</table>

The pictures show that in the first step, the solution (which has been
computed without any of the constraints active) bends through so much
that pretty much every interior point has to be bounced back to the
stairstep function, producing a discontinuous solution. Over the
course of the active set iterations, this unphysical membrane shape is
smoothed out, the contact with the lower-most stair step disappears,
and the solution stabilizes.

In addition to this, the program also outputs the values of the
Lagrange multipliers. Remember that these are the contact forces and
so should only be positive on the contact set, and zero outside. If,
on the other hand, a Lagrange multiplier is negative in the active
set, then this degree of freedom must be removed from the active
set. The following pictures show the multipliers in iterations 1, 9
and 18, where we use red and browns to indicate positive values, and
blue for negative values.

<table align="center">
  <tr>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.forces.01.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.forces.09.png" alt="">
    </td>
    <td valign="top">
      <img src="https://www.dealii.org/images/steps/developer/step-41.forces.18.png" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      Iteration 1
    </td>
    <td align="center">
      Iteration 9
    </td>
    <td align="center">
      Iteration 18
    </td>
  </tr>
</table>

It is easy to see that the positive values converge nicely to moderate
values in the interior of the contact set and large upward forces at
the edges of the steps, as one would expect (to support the large
curvature of the membrane there); at the fringes of the active set,
multipliers are initially negative, causing the set to shrink until,
in iteration 18, there are no more negative multipliers and the
algorithm has converged.



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

As with any of the programs of this tutorial, there are a number of
obvious possibilities for extensions and experiments. The first one is
clear: introduce adaptivity. Contact problems are prime candidates for
adaptive meshes because the solution has lines along which it is less
regular (the places where contact is established between membrane and
obstacle) and other areas where the solution is very smooth (or, in
the present context, constant wherever it is in contact with the
obstacle). Adding this to the current program should not pose too many
difficulties, but it is not trivial to find a good error estimator for
that purpose.

A more challenging task would be an extension to 3d. The problem here
is not so much to simply make everything run in 3d. Rather, it is that
when a 3d body is deformed and gets into contact with an obstacle,
then the obstacle does not act as a constraining body force within the
domain as is the case here. Rather, the contact force only acts on the
boundary of the object. The inequality then is not in the differential
equation but in fact in the (Neumann-type) boundary conditions, though
this leads to a similar kind of variational
inequality. Mathematically, this means that the Lagrange multiplier
only lives on the surface, though it can of course be extended by zero
into the domain if that is convenient. As in the current program, one
does not need to form and store this Lagrange multiplier explicitly.

A further interesting problem for the 3d case is to consider contact problems
with friction. In almost every mechanical process friction has a big influence.
For the modelling we have to take into account tangential stresses at the contact
surface. Also we have to observe that friction adds another nonlinearity to
our problem.

Another nontrivial modification is to implement a more complex constitutive
law like nonlinear elasticity or elasto-plastic  material behavior.
The difficulty here is to handle the additional nonlinearity arising
through the nonlinear constitutive law.


examples/step-42/doc/intro.dox
<br>

<i>This program was contributed by Jörg Frohne (University of Siegen,
Germany) while on a long-term visit to Texas A&amp;M University, with significant
contributions by Timo Heister and Wolfgang Bangerth.
<br>
<br>
The code described here provides the basis for the numerical experiments shown
in the following paper:
<br>
  J. Frohne, T. Heister, W. Bangerth: <b>Efficient numerical methods for the large-scale, parallel
                  solution of elastoplastic contact problems</b>.
  Accepted for publication in International Journal for Numerical Methods in Engineering, 2015.
</i>



<a name="Intro"></a>
<h3>Introduction</h3>

This example is an extension of step-41, considering a 3d contact problem with an
elasto-plastic material behavior with isotropic hardening in three dimensions.
In other words, it considers how a three-dimensional body deforms if one pushes
into it a rigid obstacle (the contact problem) where deformation is governed
by an elasto-plastic material law (a material that can only accommodate a certain
maximal stress) that hardens as deformation accumulates. To show what we intend to
do before going into too many details, let us just show a picture of what the
solution will look like (the deformable body is a cube - only half of
which is actually shown -, the obstacle corresponds
to a Chinese character that is discussed below):

<img src="https://www.dealii.org/images/steps/developer/step-42.CellConstitutionLi2.png" alt="">


This problem description implies that we have to take care of an additional
nonlinearity compared to step-41: the
material behavior. Since we consider a three dimensional problem here, we also
have to account for the fact that the contact area is at the boundary of
the deformable body now, rather than in the interior. Finally, compared to
step-41, we also have to deal with hanging nodes in both the handling of the linear
system as well as of the inequality constraints as we would like to use an
adaptive mesh; in the latter case, we will
have to deal with prioritizing whether the constraints from the hanging nodes
or from the inequalities are more important.

Since you can very easily reach a few million degrees of freedom in three
dimensions, even with adaptive mesh refinement, we decided to use Trilinos and
p4est to run our code in parallel, building on the framework of step-40 for
the parallelization. Additional pointers for parallelization can be found in
step-32.


<h3>Classical formulation</h3>

The classical formulation of the problem possesses the following form:
@f{align*}
 \varepsilon(\mathbf u) &= A\sigma + \varepsilon^p & &\quad\text{in } \Omega,\\
  -\textrm{div}\ \sigma &= \mathbf f & &\quad\text{in } \Omega,\\
  \varepsilon^p:(\tau - \sigma) &\geq 0\quad\forall\tau\text{ with
  }\mathcal{F}(\tau)\leq 0 & &\quad\text{in } \Omega,\\
  \mathbf u &= 0 & &\quad\text{on }\Gamma_D,\\
  \sigma \mathbf n - [\mathbf n \cdot(\sigma \mathbf n)]\mathbf n &= 0,
  \quad \mathbf n \cdot (\sigma
  \mathbf n) \leq 0 & &\quad\text{on }\Gamma_C,\\
  (\mathbf n \cdot (\sigma
  \mathbf n))(\mathbf n \cdot \mathbf u - g) &= 0,\quad \mathbf n
  \cdot \mathbf u - g \leq 0 & &\quad\text{on } \Gamma_C.
@f}
Here, the first of these equations defines the
relationship between strain $\varepsilon(\mathbf u)=\frac{1}{2}\left(\nabla \mathbf u
  + \nabla \mathbf u^T\right)$ and stress $\sigma$ via
the fourth-order compliance tensor $A$; $\varepsilon^p$ provides the plastic
component of the strain to ensure that the stress does not exceed the yield
stress. We will only consider isotropic
materials for which $A$ can be expressed in terms of the Lam&eacute; moduli
$\lambda$ and $\mu$ or alternatively in terms of the bulk modulus
$\kappa$ and $\mu$.
The second equation is the force balance; we will here
not consider any body forces and henceforth assume that $\mathbf f=0$. The
complementarity condition in the third line implies that $\varepsilon^p=0$ if
$\mathcal{F}(\sigma)< 0$ but that $\varepsilon^p$ may be a nonzero tensor if and
only if $\mathcal{F}(\sigma) = 0$, and in particular that in this case
$\varepsilon^p$ must point in the direction $\partial
\mathcal{F}(\sigma)/\partial \sigma$. The inequality $\mathcal{F}(\sigma)\le 0$ is
a statement of the fact that plastic materials can only support a finite amount
of stress; in other words, they react with plastic deformations $\varepsilon^p$
if external forces would result in a stress $\sigma$ for which $\mathcal{F}(\sigma)> 0$
would result. A typical form for this <i>yield function</i> is
$\mathcal{F}(\sigma)=|\sigma^D|-\sigma_{\text{yield}}$ where $\tau^D
= \tau - \dfrac{1}{3}tr(\tau)I$ is the deviatoric part of a tensor
and $|\cdot|$ denotes the Frobenius norm.

Further equations describe a
fixed, zero displacement on $\Gamma_D$ and
that on the surface $\Gamma_C=\partial\Omega\backslash\Gamma_D$ where contact may appear, the normal
force $\sigma_n=\mathbf n \cdot (\sigma(\mathbf u)
  \mathbf n)$ exerted by the obstacle is inward (no "pull" by the obstacle on our
body) and with zero tangential component $\mathbf \sigma_t= \sigma \mathbf n - \mathbf \sigma_n \mathbf n
= \sigma \mathbf n - [\mathbf n \cdot(\sigma \mathbf n)]\mathbf n$.
The last condition is again a complementarity condition that
implies that on $\Gamma_C$, the normal
force can only be nonzero if the body is in contact with the obstacle; the
second part describes the impenetrability of the obstacle and the body.
The last two equations are commonly referred to as the Signorini contact
conditions.

Most materials - especially metals - have the property that they show some hardening as a result of
deformation. In other words, $\sigma_{\text{yield}}$ increases with deformation.
In practice, it is not the elastic deformation that results in hardening,
but the plastic component.
There are different constitutive laws to describe those material behaviors. The
simplest one is called linear isotropic hardening described by the flow function
$\mathcal{F}(\sigma,\varepsilon^p) = \vert\sigma^D\vert - (\sigma_0 +
\gamma^{\text{iso}}|\varepsilon^p|)$.


<h3>Reformulation as a variational inequality</h3>

It is generally rather awkward to deal with inequalities. Here, we have to deal with
two: plasticity and the contact problem.
As described in more detail in the paper mentioned at the top of this page, one
can at least reformulate the plasticity in a way that makes it look like a
nonlinearity that we can then treat with Newton's method. This is slightly
tricky mathematically since the nonlinearity is not just some smooth
function but instead has kinks where the stress reaches the yield stress;
however, it can be shown for such <i>semismooth</i> functions that Newton's
method still converges.

Without going into details, we will also get rid of the stress as an independent
variable and instead work exclusively with the displacements $\mathbf u$. Ultimately,
the goal of this reformulation is that we will want to end up with a symmetric,
positive definite problem - such as a linearized elasticity problem with spatially
variable coefficients resulting from the plastic behavior - that needs to be solved
in each Newton step. We want this because there are efficient and scalable methods
for the solution of such linear systems, such as CG preconditioned with an
algebraic multigrid. This is opposed to the saddle point problem akin to the mixed
Laplace (see step-20) we would get were we to continue with the mixed formulation
containing both displacements and stresses, and for which step-20 already gives a
hint at how difficult it is to construct good solvers and preconditioners.

With this said, let us simply state the problem we obtain after reformulation
(again, details can be found in the paper): Find a displacement $\mathbf u \in
V^+$ so that
@f{align*}
\left(P_{\Pi}(C\varepsilon(\mathbf u)),\varepsilon(\varphi) - \varepsilon(\mathbf u)\right) \geq 0,\quad \forall \varphi\in V^+.
@f}
where the projector $P_\Pi$ is defined as
@f{align*}
 P_{\Pi}(\tau) \dealcoloneq \begin{cases}
    \tau, & \text{if }\vert\tau^D\vert \leq \sigma_0,\\
    \left[
      \dfrac{\gamma^{\text{iso}}}{2\mu + \gamma^{\text{iso}}} +
      \left(1-\dfrac{\gamma^{\text{iso}}}{2\mu + \gamma^{\text{iso}}}\right)\dfrac{\sigma_0}{\vert\tau^D\vert}
    \right]\tau^D
    + \dfrac{1}{3}\text{trace}(\tau) I, & \text{if }\vert\tau^D\vert >
    \sigma_0,
  \end{cases}
@f}
and the space $V^+$ is the space of all displacements that satisfy the contact
condition:
@f{align*}
  V
  &=
  \left\{ \mathbf u\in \left[H^1(\Omega)\right]^{d}:
    \mathbf u = 0 \text{ on } \Gamma_D\right\},
  \\
  V^+
  &=
  \left\{ \mathbf u\in V: \mathbf n \cdot \mathbf u\leq g \text{ on } \Gamma_C \right\}.
@f}

In the actual code, we will use the abbreviation $\gamma=\dfrac{\gamma^{\text{iso}}}{2\mu + \gamma^{\text{iso}}}$.

Given this formulation, we will apply two techniques:
- Run a Newton method to iterate out the nonlinearity in the projector.
- Run an active set method for the contact condition, in much the same
  way as we did in step-41.

A strict approach would keep the active set fixed while we iterate
the Newton method to convergence (or maybe the other way around: find the
final active set before moving on to the next Newton iteration).
In practice, it turns out that it is sufficient to do only a single
Newton step per active set iteration, and so we will iterate over them
concurrently. We will also, every once in a while, refine the mesh.


<h3>A Newton method for the plastic nonlinearity</h3>

As mentioned, we will treat the nonlinearity of the operator $P_\Pi$ by
applying a Newton method, despite the fact that the operator is not differentiable
in the strict sense. However, it satisfies the conditions of <i>slant</i>
differentiability and this turns out to be enough for Newton's method to work.
The resulting method then goes by the name <i>semi-smooth Newton method</i>,
which sounds impressive but is, in reality, just a Newton method applied to
a semi-smooth function with an appropriately chosen "derivative".

In the current case, we will run our iteration by solving in each iteration $i$
the following equation (still an inequality, but linearized):
@f{align*}
  \label{eq:linearization}
  \left(I_{\Pi}\varepsilon(\tilde {\mathbf u}^{i}),
    \varepsilon(\varphi) - \varepsilon(\tilde {\mathbf u}^{i})\right) \geq
  \left(\left(I_{\Pi}\varepsilon({\mathbf u}^{i-1}),
    \varepsilon(\varphi) - \varepsilon(\tilde {\mathbf u}^{i})\right) -
  \left(P_{\Pi}(C\varepsilon({\mathbf u}^{i-1})),
    \varepsilon(\varphi) - \varepsilon(\tilde {\mathbf u}^{i})\right)\right),
  \quad \forall \varphi\in V^+,
@f}
where the rank-4 tensor $I_\Pi=I_\Pi(\varepsilon^D(\mathbf u^{i-1}))$ given by
@f{align}
  I_\Pi = \begin{cases}
    C_{\mu} + C_{\kappa}, & \hspace{-8em} \text{if } \vert C\varepsilon^D(\mathbf u^{i-1}) \vert \leq \sigma_0,
    \\
    \frac{\gamma^{\text{iso}}}{2\mu + \gamma^{\text{iso}}} C_{\mu} + \frac{\left(1-\frac{\gamma^{\text{iso}}}{2\mu + \gamma^{\text{iso}}}\right)\sigma_0}{\vert C\varepsilon^D(\mathbf u^{i-1}) \vert}\left(C_{\mu} -
      2\mu\dfrac{C\varepsilon^D(\mathbf u^{i-1})\otimes C\varepsilon^D(\mathbf
        u^{i-1})}{\vert C\varepsilon^D(\mathbf u^{i-1})\vert^2}\right) + C_{\kappa}, & \text{ else.}
\end{cases}
@f}
This tensor is the (formal) linearization of $P_\Pi(C\cdot)$ around $\varepsilon^D(\mathbf u^{i-1})$.
For the linear isotropic material we consider here,
the bulk and shear components of the projector are given by
@f{gather*}
  C_{\kappa} = \kappa I\otimes I,
  \qquad\qquad\qquad\qquad
  C_{\mu} = 2\mu\left(\mathbb{I}  - \dfrac{1}{3} I\otimes
    I\right),
@f}
where $I$
and $\mathbb{I}$ are the identity tensors of rank 2 and 4, respectively.

Note that this problem corresponds to a linear elastic contact problem
where $I_\Pi$ plays the role of the elasticity tensor $C=A^{-1}$. Indeed,
if the material is not plastic at a point, then $I_\Pi=C$. However, at
places where the material is plastic, $I_\Pi$ is a spatially varying
function. In any case, the system we have to solve for the Newton iterate
$\tilde {\mathbf u}^{i}$ gets us closer to the goal of rewriting our problem in
a way that allows us to use well-known solvers and preconditioners for
elliptic systems.

As a final note about the Newton method let us mention that as is common with
Newton methods we need to globalize it by controlling the step length. In
other words, while the system above solves for $\tilde {\mathbf u}^{i}$, the final
iterate will rather be
@f{align*}
  {\mathbf u}^{i} = {\mathbf u}^{i-1} + \alpha_i (\tilde {\mathbf u}^{i} - {\mathbf u}^{i-1})
@f}
where the difference in parentheses on the right takes the role of the
traditional Newton direction, $\delta {\mathbf u}^{i}$. We will determine
$\alpha^i$ using a standard line search.


<h3>Active Set methods to solve the saddle point problem</h3>

This linearized problem to be solved in each Newton step is essentially like
in step-41. The only difference consists in the fact that the contact area
is at the boundary instead of in the domain. But this has no further consequence
so that we refer to the documentation of step-41 with the only hint that
$\mathcal{S}$ contains all the vertices at the contact boundary $\Gamma_C$ this
time. As there, what we need to do is keep a subset of degrees of freedom fixed,
leading to additional constraints that one can write as a saddle point problem.
However, as discussed in the paper, by writing these constraints in an
appropriate way that removes the coupling between degrees of freedom,
we end up with a set of nodes that essentially just have Dirichlet values
attached to them.


<h3>Overall algorithm</h3>

The algorithm outlined above combines the damped semismooth Newton-method,
which we use for the nonlinear constitutive law, with the semismooth Newton
method for the contact. It works as follows:
<ol>
 <li> Initialize the active and inactive sets $\mathcal{A}_i$ and $\mathcal{F}_i$
 such that $\mathcal{S} = \mathcal{A}_i \cup \mathcal{F}_i$ and $\mathcal{A}_i \cap
 \mathcal{F}_i = \emptyset$ and set $i = 1$. Here, $\mathcal{S}$ is the set of
 all degrees of freedom located at the surface of the domain where contact
 may happen.
 The start value $\hat U^0 \dealcoloneq
 P_{\mathcal{A}_k}(0)$ fulfills our obstacle condition, i.e., we project an
 initial zero displacement onto the set of feasible displacements.

 <li> Assemble the Newton matrix $A_{pq} \dealcoloneq a'(
 U^{i-1};\varphi_p,\varphi_q)$ and the right-hand-side $F(\hat U^{i-1})$.
 These correspond to the linearized Newton step, ignoring for the moment
 the contact inequality.

 <li> Find the primal-dual pair $(\tilde U^i,\Lambda^i)$ that satisfies
 @f{align*}
 A\tilde U^i + B\Lambda^i & = F, &\\
 \left[B^T\tilde U^i\right]_p & = G_p & \forall p\in\mathcal{A}_i,\\
 \Lambda^i_p & = 0 & \forall p\in\mathcal{F}_i.
 @f}
 As in step-41, we can obtain the solution to this problem by eliminating
 those degrees of freedom in ${\cal A}_i$ from the first equation and
 obtain a linear system $\hat {\hat A}(U^{i-1}) \tilde U^i = \hat {\hat H}(U^{i-1})$.



 <li> Damp the Newton iteration for $i>2$ by applying a line search and
 calculating a linear combination of $U^{i-1}$ and $\tilde U^i$. This
 requires finding an
 $\alpha^i_l \dealcoloneq 2^{-l},(l=0,\ldots,10)$ so that
 @f{gather*}U^i \dealcoloneq \alpha^i_l\bar U^i +
 (1-\alpha^i_l)U^{i-1}@f}
 satisfies
 @f{gather*}
   \vert {\hat R}\left({\mathbf u}^{i}\right) \vert < \vert {\hat R}\left({\mathbf u}^{i-1}\right) \vert.
 \f}
 with ${\hat R}\left({\mathbf u}\right)=\left(P_{Pi}(C\varepsilon(u)),\varepsilon(\varphi^{i}_p\right)$ with
 the exceptions of (i) elements $p\in\mathcal{A}_i$ where we set ${\hat R}\left({\mathbf u}\right)=0$,
 and (ii) elements that correspond to hanging nodes, which we eliminate in the usual manner.

 <li> Define the new active and inactive sets by
 @f{gather*}\mathcal{A}_{i+1} \dealcoloneq \lbrace p\in\mathcal{S}:\Lambda^i_p +
 c\left(\left[B^TU^i\right]_p - G_p\right) > 0\rbrace,@f}
 @f{gather*}\mathcal{F}_{i+1} \dealcoloneq \lbrace p\in\mathcal{S}:\Lambda^i_p +
 c\left(\left[B^TU^i\right]_p - G_p\right) \leq 0\rbrace.@f}

 <li>Project $U^i$ so that it satisfies the contact inequality,
 @f{gather*}\hat U^i \dealcoloneq P_{\mathcal{A}_{i+1}}(U^i).@f}
 Here,
 $P_{\mathcal{A}}(U)$ is the projection of the active
 components in $\mathcal{A}$ to the gap
 @f{gather*}P_{\mathcal{A}}(U)_p \dealcoloneq \begin{cases}
 U_p, & \textrm{if}\quad p\notin\mathcal{A}\\
 g_{h,p}, & \textrm{if}\quad
 p\in\mathcal{A},
 \end{cases}@f}
 where $g_{h,p}$ is the <i>gap</i> denoting the distance of the obstacle
 from the undisplaced configuration of the body.

 <li> If $\mathcal{A}_{i+1} = \mathcal{A}_k$ and $\left\|
 {\hat R}\left({\mathbf u}^{i}\right) \right\|_{\ell_2} < \delta$ then stop, else set $i=i+1$ and go to
 step (1). This step ensures that we only stop iterations if both the correct
 active set has been found and the plasticity has been iterated to sufficient
 accuracy.
</ol>

In step 3 of this algorithm,
the matrix $B\in\mathbb{R}^{n\times m}$, $n>m$ describes the coupling of the
bases for the displacements and Lagrange multiplier (contact forces)
and it is not quadratic in our situation since $\Lambda^k$ is only defined on
$\Gamma_C$, i.e., the surface where contact may happen. As shown in the paper,
we can choose $B$ to be a matrix that has only one entry per row,
(see also H&uuml;eber, Wohlmuth: A primal-dual active
set strategy for non-linear multibody contact problems, Comput. Methods Appl. Mech. Engrg.
194, 2005, pp. 3147-3166).
The vector $G$ is defined by a suitable approximation $g_h$ of the gap $g$
@f{gather*}G_p = \begin{cases}
g_{h,p}, & \text{if}\quad p\in\mathcal{S}\\
0, & \text{if}\quad p\notin\mathcal{S}.
\end{cases}@f}


<h3>Adaptive mesh refinement</h3>

Since we run our program in 3d, the computations the program performs are
expensive. Consequently using adaptive mesh refinement is an important step towards
staying within acceptable run-times. To make our lives easier we simply choose the
KellyErrorEstimator that is already implemented in deal.II. We hand the
solution vector to it which contains the displacement $u$. As we will see in the
results it yields a quite reasonable adaptive mesh for the contact zone as well
as for plasticity.


<h3>Implementation</h3>

This tutorial is essentially a mixture of step-40 and step-41 but instead of
PETSc we let the Trilinos library deal with parallelizing the linear algebra
(like in step-32). Since we are trying to solve a similar problem like in
step-41 we will use the same methods but now in parallel.

A difficulty is handling of the constraints from
the Dirichlet conditions, hanging nodes and the inequality condition that
arises from the contact. For this purpose we create three objects of type
AffineConstraints that describe the various constraints and that we will
combine as appropriate in each iteration.

Compared to step-41, the programs has a few new classes:

<ul>
<li> <code>ConstitutiveLaw</code> describes the plastic behavior of the
  material

<li> <code>SphereObstacle</code> describes a sphere that serves as the
  obstacle that is pushed into the deformable, elastoplastic body.
  Whether this or the next class is used to describe the obstacle is
  determined from the input parameter file.

<li> <code>ChineseObstacle</code> (and a helper class) is a class that
  allows us to read in an obstacle from a file. In the example we
  will show in the results section, this file will be
  <code>'obstacle_file.dat'</code> and will correspond to data that shows the
  Chinese, Japanese or
  Korean symbol for force or power (see http://www.orientaloutpost.com/ :
  "This word can be used for motivation - it
  can also mean power/motion/propulsion/force. It can be anything
  internal or external that keeps you going. This is the safest way to express
  motivation in Chinese. If your audience is Japanese, please see the other entry
  for motivation. This is a word in Japanese and Korean, but it means "motive
  power" or "kinetic energy" (without the motivation meaning that you are
  probably looking for)"). In essence, we will pretend that we have a stamp
  (i.e., a mask that corresponds to a flat bottomed obstacle with no pieces
  of intermediate height) that we press into the body. The symbol in question
  looks as follows (see also the picture at
  the top of this section on how the end result looks like):

  <img src="https://www.dealii.org/images/steps/developer/step-42.character.png" alt="" width="25%">
</ul>

Other than that, let us comment only on the following aspects:
<ul>
<li> The program allows you to select from two different coarse meshes
  through the parameter file. These are either a cube $[0,1]^3$ or
  a half sphere with the open side facing the positive $z$ direction.

<li>In either case, we will assume the convention that the part of the
  boundary that may be in contact with the obstacle has boundary
  indicator one. For both kinds of meshes, we assume that this is a free
  surface, i.e., the body is either in contact there or there is no force
  acting on it. For the half sphere, the curved part has boundary
  indicator zero and we impose zero displacement there. For the box,
  we impose zero displacement along the bottom but allow vertical
  displacement along the sides (though no horizontal displacement).
</ul>


examples/step-42/doc/results.dox
<h1>Results</h1>

The directory that contains this program also contains a number of input
parameter files that can be used to create various different
simulations. For example, running the program with the
<code>p1_adaptive.prm</code> parameter file (using a ball as obstacle and the
box as domain) on 16 cores produces output like this:
@code
    Using output directory 'p1adaptive/'
    FE degree 1
    transfer solution false

Cycle 0:
   Number of active cells: 512
   Number of degrees of freedom: 2187

  Newton iteration 1
      Updating active set...
         Size of active set: 1
      Assembling system...
      Solving system...
         Error: 173.076 -> 1.64265e-06 in 7 Bicgstab iterations.
      Accepting Newton solution with residual: 1.64265e-06

   Newton iteration 2
      Updating active set...
         Size of active set: 1
      Assembling system...
      Solving system...
         Error: 57.3622 -> 3.23721e-07 in 8 Bicgstab iterations.
      Accepting Newton solution with residual: 24.9028
      Active set did not change!

   Newton iteration 3
      Updating active set...
         Size of active set: 1
      Assembling system...
      Solving system...
         Error: 24.9028 -> 9.94326e-08 in 7 Bicgstab iterations.
      Residual of the non-contact part of the system: 1.63333
         with a damping parameter alpha = 1
      Active set did not change!

...

  Newton iteration 6
      Updating active set...
         Size of active set: 1
      Assembling system...
      Solving system...
         Error: 1.43188e-07 -> 3.56218e-16 in 8 Bicgstab iterations.
      Residual of the non-contact part of the system: 4.298e-14
         with a damping parameter alpha = 1
      Active set did not change!
      Writing graphical output... p1_adaptive/solution-00.pvtu


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |      1.13s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assembling                      |         6 |     0.463s |        41% |
| Graphical output                |         1 |    0.0257s |       2.3% |
| Residual and lambda             |         4 |    0.0754s |       6.7% |
| Setup                           |         1 |     0.227s |        20% |
| Setup: constraints              |         1 |    0.0347s |       3.1% |
| Setup: distribute DoFs          |         1 |    0.0441s |       3.9% |
| Setup: matrix                   |         1 |    0.0119s |       1.1% |
| Setup: vectors                  |         1 |   0.00155s |      0.14% |
| Solve                           |         6 |     0.246s |        22% |
| Solve: iterate                  |         6 |    0.0631s |       5.6% |
| Solve: setup preconditioner     |         6 |     0.167s |        15% |
| update active set               |         6 |    0.0401s |       3.6% |
+---------------------------------+-----------+------------+------------+

Peak virtual memory used, resident in kB: 541884 77464
Contact force = 37.3058

...

Cycle 3:
   Number of active cells: 14652
   Number of degrees of freedom: 52497

   Newton iteration 1
      Updating active set...
         Size of active set: 145
      Assembling system...
      Solving system...
         Error: 296.309 -> 2.72484e-06 in 10 Bicgstab iterations.
      Accepting Newton solution with residual: 2.72484e-06

...

   Newton iteration 10
      Updating active set...
         Size of active set: 145
      Assembling system...
      Solving system...
         Error: 2.71541e-07 -> 1.5428e-15 in 27 Bicgstab iterations.
      Residual of the non-contact part of the system: 1.89261e-13
         with a damping parameter alpha = 1
      Active set did not change!
      Writing graphical output... p1_adaptive/solution-03.pvtu


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |      38.4s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assembling                      |        10 |      22.5s |        58% |
| Graphical output                |         1 |     0.327s |      0.85% |
| Residual and lambda             |         9 |      3.75s |       9.8% |
| Setup                           |         1 |      4.83s |        13% |
| Setup: constraints              |         1 |     0.578s |       1.5% |
| Setup: distribute DoFs          |         1 |      0.71s |       1.8% |
| Setup: matrix                   |         1 |     0.111s |      0.29% |
| Setup: refine mesh              |         1 |      4.83s |        13% |
| Setup: vectors                  |         1 |   0.00548s |     0.014% |
| Solve                           |        10 |      5.49s |        14% |
| Solve: iterate                  |        10 |       3.5s |       9.1% |
| Solve: setup preconditioner     |        10 |      1.84s |       4.8% |
| update active set               |        10 |     0.662s |       1.7% |
+---------------------------------+-----------+------------+------------+

Peak virtual memory used, resident in kB: 566052 105788
Contact force = 56.794

...
@endcode

The tables at the end of each cycle show information about computing time
(these numbers are of course specific to the machine on which this output
was produced)
and the number of calls of different parts of the program like assembly or
calculating the residual, for the most recent mesh refinement cycle. Some of
the numbers above can be improved by transferring the solution from one mesh to
the next, an option we have not exercised here. Of course, you can also make
the program run faster, especially on the later refinement cycles, by just
using more processors: the accompanying paper shows good scaling to at least
1000 cores.

In a typical run, you can observe that for every refinement step, the active
set - the contact points - are iterated out at first. After that the Newton
method has only to resolve the plasticity. For the finer meshes,
quadratic convergence can be observed for the last 4 or 5 Newton iterations.

We will not discuss here in all detail what happens with each of the input
files. Rather, let us just show pictures of the solution (the left half of the
domain is omitted if cells have zero quadrature points at which the plastic
inequality is active):

<table align="center">
  <tr>
    <td>
    <img src="https://www.dealii.org/images/steps/developer/step-42.CellConstitutionColorbar.png">
    </td>
    <td>
    <img src="https://www.dealii.org/images/steps/developer/step-42.CellConstitutionBall2.png" alt="" width="70%">
    </td>
    <td valign="top">
      &nbsp;
    </td>
    <td>
    <img src="https://www.dealii.org/images/steps/developer/step-42.CellConstitutionLi2.png" alt="" alt="" width="70%">
    </td>
  </tr>
</table>

The picture shows the adaptive refinement and as well how much a cell is
plastified during the contact with the ball. Remember that we consider the
norm of the deviator part of the stress in each quadrature point to
see if there is elastic or plastic behavior.
The blue
color means that this cell contains only elastic quadrature points in
contrast to the red cells in which all quadrature points are plastified.
In the middle of the top surface -
where the mesh is finest - a very close look shows the dimple caused by the
obstacle. This is the result of the <code>move_mesh()</code>
function. However, because the indentation of the obstacles we consider here
is so small, it is hard to discern this effect; one could play with displacing
vertices of the mesh by a multiple of the computed displacement.

Further discussion of results that can be obtained using this program is
provided in the publication mentioned at the very top of this page.


<a name="extensions"></a>
<h1>Possibilities for extensions</h1>

There are, as always, multiple possibilities for extending this program. From
an algorithmic perspective, this program goes about as far as one can at the
time of writing, using the best available algorithms for the contact
inequality, the plastic nonlinearity, and the linear solvers. However, there
are things one would like to do with this program as far as more realistic
situations are concerned:
<ul>
<li> Extend the program from a static to a quasi-static situation, perhaps by
choosing a backward-Euler-scheme for the time discretization. Some theoretical
results can be found in the PhD thesis by Jörg Frohne, <i>FEM-Simulation
der Umformtechnik metallischer Oberfl&auml;chen im Mikrokosmos</i>, University
of Siegen, Germany, 2011.

<li> It would also be an interesting advance to consider a contact problem
with friction. In almost every mechanical process friction has a big
influence.  To model this situation, we have to take into account tangential
stresses at the contact surface. Friction also adds another inequality to
our problem since body and obstacle will typically stick together as long as
the tangential stress does not exceed a certain limit, beyond which the two
bodies slide past each other.

<li> If we already simulate a frictional contact, the next step to consider
is heat generation over the contact zone. The heat that is
caused by friction between two bodies raises the temperature in the
deformable body and entails an change of some material parameters.

<li> It might be of interest to implement more accurate, problem-adapted error
estimators for contact as well as for the plasticity.
</ul>


examples/step-43/doc/intro.dox
<br>

<i>
This program was contributed by Chih-Che Chueh (University of Victoria) and
Wolfgang Bangerth. Results from this program are used and discussed in the
following publications (in particular in the second one):
- Chih-Che Chueh, Marc Secanell, Wolfgang Bangerth, Ned Djilali. Multi-level
  adaptive simulation of transient two-phase flow in heterogeneous porous
  media. Computers &amp; Fluids, 39:1585-1596, 2010
- Chih-Che Chueh, Ned Djilali, Wolfgang Bangerth. An h-adaptive operator
  splitting method for two-phase flow in 3D heterogeneous porous
  media. SIAM Journal on Scientific Computing, 35:B149-B175, 2013.

The implementation discussed here uses and extends
parts of the step-21 and step-31 tutorial programs.

The work of the Chih-Che Chueh was funded through the Canada Research Chairs
Program and the MITACS Network of Centres of Excellence. Parts of the work by
Wolfgang Bangerth were funded through Award No. KUS-C1-016-04, made by the King
Abdullah University of Science and Technology, and through an Alfred P. Sloan
Research Fellowship.
This material is also in parts based upon work supported by the National
Science Foundation under Award No. EAR-0426271 and The California Institute of
Technology; and in a continuation by the National Science
Foundation under Award No. EAR-0949446 and The University of California
&ndash; Davis. Any opinions, findings, and conclusions or recommendations
expressed in this publication are those of the author and do not
necessarily reflect the views of the National Science Foundation, The
California Institute of Technology, or of The University of California
&ndash; Davis.
</i>


<a name="Intro"></a> <h1>Introduction</h1>

The simulation of multiphase flow in porous media is a ubiquitous problem, and
we have previously addressed it already in some form in step-20 and
step-21. However, as was easy to see there, it faces two major difficulties:
numerical accuracy and efficiency. The first is easy to see in the stationary
solver step-20: using lowest order Raviart-Thomas elements can not be expected
to yield highly accurate solutions. We need more accurate methods. The second
reason is apparent from the time dependent step-21: that program is
excruciatingly slow, and there is no hope to get highly accurate solutions in
3d within reasonable time frames.

In this
program, in order to overcome these two problems, there are five areas which
we are trying to improve for a high performance simulator:

<ul>
<li> Higher order spatial discretizations
<li> Adaptive mesh refinement
<li> Adaptive time stepping
<li> Operator splitting
<li> Efficient solver and preconditioning
</ul>

Much inspiration for this program comes from step-31 but several of the
techniques discussed here are original.


<h3>Advection-dominated two-phase flow mathematical model.</h3>

We consider the flow of a two-phase immiscible, incompressible
fluid. Capillary and gravity effects are neglected, and viscous
effects are assumed dominant. The governing equations for such a
flow that are identical to those used in step-21 and are
@f{align*}
  \mathbf{u}_t &= - \mathbf{K} \lambda_t \left(S\right) \nabla p, \\
  \nabla \cdot \mathbf{u}_t &= q, \\
  \epsilon \frac{\partial S}{\partial t} + \nabla \cdot \left( \mathbf{u}_t  F\left( S \right) \right)&=0,
@f}
where $S$ is the saturation (volume fraction between zero and one) of the second (wetting) phase, $p$ is the pressure, $\mathbf{K}$ is the permeability tensor, $\lambda_t$ is the total mobility, $\epsilon$ is the porosity, $F$ is the fractional flow of the wetting phase, $q$ is the source term and $\mathbf{u}_t$ is the total velocity. The total mobility, fractional flow of the wetting phase and total velocity are respectively given by
@f{align*}
   \lambda_t(S)&= \lambda_w + \lambda_{nw} = \frac{k_{rw}(S)}{\mu_w} + \frac{k_{rnw}(S)}{\mu_{nw}}, \\
   F(S) &= \frac{\lambda_w}{\lambda_t} = \frac{\lambda_w}{\lambda_w + \lambda_{nw}} = \frac{k_{rw}(S)/\mu_w}{k_{rw}(S)/\mu_w + k_{rnw}(S)/\mu_{nw}}, \\
   \mathbf{u}_t &= \mathbf{u}_w + \mathbf{u}_{nw} = -\lambda_t(S)\mathbf{K} \cdot \nabla p,
@f}
where subscripts $w, nw$ represent the wetting and non-wetting phases,
respectively.

For convenience, the
porosity $\epsilon$ in the saturation equation, which can be considered a
scaling factor for the time variable, is set to
one. Following a commonly used prescription for the dependence of the relative
permeabilities $k_{rw}$ and $k_{rnw}$ on saturation, we use
@f{align*}
   k_{rw}  &= S^2, \qquad&\qquad
   k_{rnw} &= \left( 1-S \right)^2.
@f}

The porous media equations above are
augmented by initial conditions for the saturation and boundary conditions for
the pressure. Since saturation and the gradient of the pressure uniquely
determine the velocity, no boundary conditions are necessary for the velocity.
Since the flow equations do not contain time derivatives, initial conditions for the velocity and pressure
variables are not required. The flow field separates the boundary into inflow or outflow
parts. Specifically,
@f[
   \mathbf{\Gamma}_{in}(t) = \left\{\mathbf{x} \in \partial \Omega:\mathbf{n} \cdot \mathbf{u}_t<0\right\},
@f]
and we arrive at a complete model by also imposing boundary values for the
saturation variable on the inflow boundary $\mathbf{\Gamma}_{in}$.


<h3>Adaptive operator splitting and time stepping.</h3>

As seen in step-21, solving the flow equations for velocity and pressure are
the parts of the program that take far longer than the (explicit) updating
step for the saturation variable once we know the flow variables. On the other
hand,  the pressure and velocity depend only weakly on saturation, so one may
think about only solving for pressure and velocity every few time steps while
updating the saturation in every step. If we can find a criterion for when the
flow variables need to be updated, we call this splitting an "adaptive
operator splitting" scheme.

Here, we use the following a posteriori criterion to decide when to re-compute
pressure and velocity variables
(detailed derivations and descriptions can be found in [Chueh, Djilali
and Bangerth 2011]):
@f{align*}
  \theta(n,n_p)
  =
    \max_{\kappa\in{\mathbb T}}
    \left(
    \left\|
      \frac 1{\lambda_t\left(S^{(n-1)}\right)}
      - \frac 1{\lambda_t\left(S^{(n_p)}\right)} \right\|_{L^\infty(\kappa)}
    \left\|\|\mathbf{K}^{-1}\|_1\right\|_{L^\infty(\kappa)}
    \right).
@f}
where superscripts in parentheses denote the number of the saturation time
step at which any quantity is defined and $n_p<n$ represents the last step
where we actually computed the pressure and velocity. If $\theta(n,n_p)$
exceeds a certain threshold we re-compute the flow variables; otherwise, we
skip this computation in time step $n$ and only move the saturation variable
one time step forward.

In short, the algorithm allows us to perform a number of
saturation time steps of length $\Delta t_c^{(n)}=t^{(n)}_c-t^{(n-1)}_c$ until
the criterion above tells us to re-compute velocity and pressure
variables, leading to a macro time step of length
@f[
   \Delta t_p^{(n)} = \sum_{i=n_p+1}^{n} \Delta t_c^{(i)}.
@f]
We choose the length of (micro) steps subject to the Courant-Friedrichs-Lewy
(CFL) restriction according to the criterion
@f[
  \Delta t_c = \frac{\textrm{min}_{K}h_{K}}{7 \|\mathbf{u}_t\|_{L^{\infty}\left(\Omega\right)}},
@f]
which we have confirmed to be stable for the choice of finite element and time
stepping scheme for the saturation equation discussed below ($h_K$ denotes the
diameter of cell $K$).
The result is a scheme where neither micro nor macro time
steps are of uniform length, and both are chosen adaptively.

<h3>Time discretization.</h3>
Using this time discretization, we obtain the following set of equations for
each time step from the IMPES approach (see step-21):
@f{align*}
   \mathbf{u}^{(n)}_t + \lambda_t\left(S^{(n-1)}\right) \mathbf{K} \nabla p^{(n)} =0, \\
   \nabla \cdot \mathbf{u}^{(n)}_t = q, \\
   \epsilon \left( \frac{S^{(n-1)}-S^{(n)}}{\Delta t^{(n)}_c} \right) + \mathbf{u}^{(n)}_t \cdot \nabla F\left(S^{(n-1)}\right) + F\left(S^{(n-1)}\right) \nabla \cdot \mathbf{u}^{(n)}_t =0.
@f}


Using the fact that $\nabla \cdot \mathbf{u}_t = q$, the time discrete
saturation equation becomes
@f{align*}
  &\epsilon \left( \frac{S^{(n)}-S^{(n-1)}}{\Delta t^{(n)}_c} \right) + \mathbf{u}^{(n)}_t \cdot \nabla F\left(S^{(n-1)}\right) + F\left(S^{(n-1)}\right)q=0.
@f}

<h3>Weak form, space discretization for the pressure-velocity part.</h3>

By multiplying the equations defining the total velocity $\mathbf u_t^{(n)}$ and
the equation that expresses its divergence in terms of source terms, with test
functions $\mathbf{v}$ and $w$
respectively and then integrating terms by parts as necessary, the weak form
of the problem reads: Find $\mathbf u, p$ so that for all test functions
$\mathbf{v}, w$ there holds
@f{gather*}
   \left( \left( \mathbf{K} \lambda_t\left(S^{(n-1)}\right) \right)^{-1} \mathbf{u}^{(n)}_t, \mathbf{v}\right)_{\Omega} - \left(p^{(n)}, \nabla \cdot \mathbf{v}\right)_{\Omega} = -\left(p^{(n)}, \mathbf{n} \cdot \mathbf{v} \right)_{\partial \Omega}, \\
   - \left( \nabla \cdot \mathbf{u}^{(n)}_t,w\right)_{\Omega} = - \big(q,w\big)_{\Omega}.
@f}
Here, $\mathbf{n}$ represents the unit outward normal vector to $\partial
\Omega$ and the pressure $p^{(n)}$ can be prescribed weakly on the open part
of the boundary $\partial \Omega$ whereas on those parts where a velocity is
prescribed (for example impermeable boundaries with $\mathbf n \cdot \mathbf
u=0$ the term disappears altogether because $\mathbf n \cdot \mathbf
v=0$.

We use continuous finite elements to discretize the velocity and pressure
equations. Specifically, we use mixed finite elements to ensure high order approximation
for both vector (e.g. a fluid velocity) and scalar variables (e.g. pressure)
simultaneously. For saddle point problems, it is well established that
the so-called Babuska-Brezzi or Ladyzhenskaya-Babuska-Brezzi (LBB) conditions
[Brezzi 1991, Chen 2005] need to be satisfied to ensure stability of
the pressure-velocity system. These stability conditions are satisfied in the
present work by using elements for velocity that are one order higher than for
the pressure, i.e. $u_h \in Q^d_{p+1}$ and $p_h \in Q_p$, where $p=1$, $d$ is
the space dimension, and $Q_s$ denotes the space of tensor product Lagrange
polynomials of degree $s$ in each variable.

<h3>Stabilization, weak form and space discretization for the saturation transport equation.</h3>
The chosen $Q_1$ elements for the saturation equation do not lead to a stable
discretization without upwinding or other kinds of stabilization, and spurious
oscillations will appear in the numerical solution. Adding an artificial
diffusion term is one approach to eliminating these oscillations
[Chen 2005]. On the other hand, adding too much diffusion smears sharp
fronts in the solution and suffers from grid-orientation difficulties
[Chen 2005]. To avoid these effects, we use the artificial diffusion
term proposed by [Guermond and Pasquetti 2008] and
validated in [Chueh, Djilali, Bangerth 2011] and
[Kronbichler, Heister and Bangerth, 2011], as well as in step-31.

This method modifies the (discrete) weak form of the saturation equation
to read
@f{align*}
  \left(\epsilon \frac{\partial S_h}{\partial t},\sigma_h\right)
  -
  \left(\mathbf{u}_t  F\left( S_h \right),
    \nabla \sigma_h\right)
  +
  \left(\mathbf n \cdot \mathbf{u}_t  \hat F\left( S_h \right),
    \sigma_h\right)_{\partial\Omega}
  +
  (\nu(S_h) \nabla S_h, \nabla \sigma_h)
  &=0
  \qquad
  \forall \sigma_h,
@f}
where $\nu$ is the artificial diffusion parameter and $\hat F$ is an
appropriately chosen numerical flux on the boundary of the domain (we choose
the obvious full upwind flux for this).

Following [Guermond and Pasquetti 2008] (and as detailed in
[Chueh, Djilali and Bangerth 2011]), we use
the parameter as a piecewise
constant function set on each cell $K$ with the diameter $h_{K}$ as
@f[
   \nu(S_h)|_{K} = \beta \| \mathbf{u}_t \max\{F'(S_h),1\} \|_{L^{\infty}(K)} \textrm{min} \left\{ h_{K},h^{\alpha}_{K} \frac{\|\textrm{Res}(S_h)\|_{L^{\infty}(K)}}{c(\mathbf{u}_t,S)} \right\}
@f]
where $\alpha$ is a stabilization exponent and $\beta$ is a dimensionless
user-defined stabilization constant. Following [Guermond and Pasquetti 2008]
as well as the implementation in step-31, the velocity and saturation global
normalization constant, $c(\mathbf{u}_t,S)$, and the residual $\textrm{Res}(S)$
are respectively given by
@f[
   c(\mathbf{u}_t,S) = c_R \|\mathbf{u}_t \max\{F'(S),1\}\|_{L^{\infty}(\Omega)} \textrm{var}(S)^\alpha | \textrm{diam} (\Omega) |^{\alpha - 2}
@f]
and
@f[
   \textrm{Res}(S) = \left( \epsilon \frac{\partial S}{\partial t} + \mathbf{u}_t \cdot \nabla F(S) + F(S)q \right) \cdot S^{\alpha - 1}
@f]
where $c_R$ is a second dimensionless user-defined constant,
$\textrm{diam}(\Omega)$ is the diameter of the domain and $\textrm{var}(S) =
\textrm{max}_{\Omega} S - \textrm{min}_{\Omega} S$ is the range of the present
saturation values in the entire computational domain $\Omega$.

This stabilization scheme has a number of advantages over simpler schemes such
as finite volume (or discontinuous Galerkin) methods or streamline upwind
Petrov Galerkin (SUPG) discretizations. In particular, the artificial
diffusion term acts primarily in the vicinity of discontinuities
since the residual is small in areas where the saturation is smooth. It
therefore provides for a higher degree of accuracy. On the other hand, it is
nonlinear since $\nu$ depends on the saturation $S$. We avoid this difficulty
by treating all nonlinear terms explicitly, which leads to the following
fully discrete problem at time step $n$:
@f{align*}
   &\left( \epsilon S_h^{(n)},\sigma_h\right)_{\Omega} - \Delta t^{(n)}_c \Big(F\left(S_h^{(n-1)}\right)\mathbf{u}^{*}_t,\nabla\sigma_h\Big)_{\Omega} + \Delta t^{(n)}_c \Big(F\left(S_h^{(n-1)}\right)\left(\mathbf{n}\cdot\mathbf{u}^{*}_t\right),\sigma_h\Big)_{\partial\Omega} \nonumber \\
   & \quad = \left( \epsilon S_h^{(n-1)},\sigma_h\right)_{\Omega} - \Delta t^{(n)}_c \bigg(\nu\left(S_h^{(n-1)}\right)\nabla S_h^{(n-1)},\nabla\sigma_h\bigg)_{\Omega} \nonumber \\
   & \qquad + \Delta t^{(n)}_c \bigg(\mathbf{n}\cdot\nu\left(S_h^{(n-1)}\right)\nabla S^{(n-1)},\sigma_h\bigg)_{\partial\Omega}
@f}
where $\mathbf{u}_t^{*}$ is the velocity linearly extrapolated from
$\mathbf{u}^{(n_p)}_t$ and $\mathbf{u}^{(n_{pp})}_t$ to the current time $t^{(n)}$ if $\theta<\theta^*$ while $\mathbf{u}_t^{*}$ is $\mathbf{u}^{(n_p)}_t$ if $\theta>\theta^*$.
Consequently, the equation is linear in $S_h^{(n)}$ and all that is required
is to solve with a mass matrix on the saturation space.

Since the Dirichlet boundary conditions for saturation are only imposed on the
inflow boundaries, the third term on the left hand side of the equation above
needs to be split further into two parts:
@f{align*}
  &\Delta t^{(n)}_c \Big(F\left(S_h^{(n-1)}\right)\left(\mathbf{n}\cdot\mathbf{u}^{(n)}_t\right),\sigma_h\Big)_{\partial\Omega} \nonumber \\
  &\qquad= \Delta t^{(n)}_c \Big(F\left(S^{(n-1)}_{(+)}\right)\left(\mathbf{n}\cdot\mathbf{u}^{(n)}_{t(+)}\right),\sigma_h\Big)_{\partial\Omega_{(+)}} + \Delta t^{(n)}_c \Big(F\left(S^{(n-1)}_{(-)}\right)\left(\mathbf{n}\cdot\mathbf{u}^{(n)}_{t(-)}\right),\sigma_h\Big)_{\partial\Omega_{(-)}}
@f}
where $\partial\Omega_{(-)} = \left\{\mathbf{x} \in \partial\Omega : \mathbf{n}
  \cdot \mathbf{u}_t<0\right\}$ and
$\partial\Omega_{(+)} = \left\{\mathbf{x} \in \partial\Omega : \mathbf{n} \cdot
  \mathbf{u}_t>0\right\}$ represent inflow and outflow boundaries,
respectively. We choose values using an
upwind formulation, i.e. $S^{(n-1)}_{(+)}$ and $\mathbf{u}^{(n)}_{t(+)}$
correspond to the values taken from the present cell, while the values of
$S^{(n-1)}_{(-)}$ and $\mathbf{u}^{(n)}_{t(-)}$ are those taken from the
neighboring boundary $\partial\Omega_{(-)}$.


<h3>Adaptive mesh refinement.</h3>

Choosing meshes adaptively to resolve sharp
saturation fronts is an essential ingredient to achieve efficiency in our
algorithm. Here, we use the same shock-type refinement approach used in
[Chueh, Djilali and Bangerth 2011] to select those cells that should be refined or
coarsened. The refinement indicator for each cell $K$ of the triangulation is
computed by
@f[
   \eta_{K} = |\nabla S_h(\mathbf x_K)|
@f]
where $\nabla S_h(\mathbf x_K)$ is the gradient of the discrete saturation
variable evaluated at the center $\mathbf x_K$ of cell $K$. This approach is
analogous to ones frequently used in compressible flow problems, where density
gradients are used to indicate refinement. That said, as we will
discuss at the end of the <a href="#Results">results section</a>, this turns
out to not be a very useful criterion since it leads to refinement basically
everywhere. We only show it here for illustrative purposes.


<h3>Linear system and its preconditioning.</h3>

Following the discretization of the governing equations
discussed above, we
obtain a linear system of equations in time step $(n)$ of the following form:
@f[
 \left(
  \begin{array}{ccc}
   \mathbf{M}^{\mathbf{u}} & \mathbf{B}^{T} & \mathbf{0}  \\
   \mathbf{B}           & \mathbf{0}     & \mathbf{0}   \\
   \mathbf{H}           & \mathbf{0}     & \mathbf{M}^{S}
  \end{array}
 \right)
 \left(
  \begin{array}{c}
   \mathbf{U}^{(n)} \\
   \mathbf{P}^{(n)} \\
   \mathbf{S}^{(n)}
  \end{array}
 \right)
 =
 \left(
  \begin{array}{c}
   0 \\
   \mathbf{F}_{2} \\
   \mathbf{F}_{3}
  \end{array}
 \right)
@f]
where the individual matrices and vectors are defined as follows using shape functions $\mathbf{v}_i$ for velocity, and $\phi_i$ for both pressure and saturation:
@f{align*}
  \mathbf{M}^{\mathbf{u}}_{ij}
  &= \left( \left( \mathbf{K} \lambda_t\left(S^{(n-1)}\right) \right)^{-1}
  \mathbf{v}_{i},\mathbf{v}_{j}\right)_{\Omega},
  &
  \mathbf{M}^{S}_{ij}           &= \left(\epsilon \phi_i,\phi_j\right)_{\Omega}
  \\
  \mathbf{B}_{ij}
  &= - \left( \nabla \cdot \mathbf{v}_{j},\phi_{i}\right)_{\Omega},
  &
  \mathbf{H}_{ij}
  &= - \Delta t^{(n)}_c \Big( F\left(S^{(n-1)}\right) \mathbf{v}_i,\nabla\phi_j\Big)_{\Omega}
  \\
  \left(\mathbf{F}_{2}\right)_i
  &= - \big(F\left(S^{(n-1)}\right)q,\phi_i\big)_{\Omega},
@f}
and $\mathbf{F}_{3}$ as given in the definition of the stabilized transport
equation.

The linear system above is of block triangular form if we consider the top
left $2\times 2$ panel of matrices as one block. We can therefore first solve
for the velocity and pressure (unless we decide to use $\mathbf U^{(n_p)}$ in
place of the velocity)
followed by a solve for the saturation variable. The first of these steps
requires us to solve
@f[
 \left(
  \begin{array}{cc}
   \mathbf{M}^{\mathbf{u}} & \mathbf{B}^{T}  \\
   \mathbf{B}           & \mathbf{0}
  \end{array}
 \right)
 \left(
  \begin{array}{c}
   \mathbf{U}^{(n)} \\
   \mathbf{P}^{(n)}
  \end{array}
 \right)
 =
 \left(
  \begin{array}{c}
   0 \\
   \mathbf{F}_{2}
  \end{array}
 \right)
@f]
We apply the Generalized Minimal Residual (GMRES) method [Saad and Schultz
1986] to this linear system. The ideal preconditioner for the
velocity-pressure system is
@f{align*}
\mathbf{P} =
 \left(
  \begin{array}{cc}
   \mathbf{M}^{\mathbf{u}} &  \mathbf{0}  \\
   \mathbf{B}           & -\mathbf{S}
  \end{array}
 \right),
 & \qquad
 \mathbf{P}^{-1} =
 \left(
  \begin{array}{cc}
   \left(\mathbf{M}^{\mathbf{u}}\right)^{-1}                              &  \mathbf{0}  \\
   \mathbf{S}^{-1} \mathbf{B} \left(\mathbf{M}^{\mathbf{u}}\right)^{-1}   & -\mathbf{S}^{-1}
  \end{array}
 \right)
 @f}
where
$\mathbf{S}=\mathbf{B}\left(\mathbf{M}^{\mathbf{u}}\right)^{-1}\mathbf{B}^T$ is
the Schur complement [Zhang 2005] of the system. This preconditioner is
optimal since
@f{align*}
 \mathbf{P}^{-1}
 \left(
  \begin{array}{cc}
   \mathbf{M}^{\mathbf{u}} & \mathbf{B}^{T}  \\
   \mathbf{B}           & \mathbf{0}
  \end{array}
 \right)
 =
  \left(
  \begin{array}{cc}
   \mathbf{I}         &  \left(\mathbf{M}^{\mathbf{u}}\right)^{-1} \mathbf{B}^{T}  \\
   \mathbf{0}         &  \mathbf{I}
  \end{array}
 \right),
@f}
for which it can be shown that GMRES converges in two iterations.

However, we cannot of course expect to use exact inverses of the
velocity mass matrix and the Schur complement. We therefore follow the
approach by [Silvester and Wathen 1994] originally proposed for
the Stokes system. Adapting it to the current set of equations yield the
preconditioner
@f{align*}
 \mathbf{\tilde{P}}^{-1} =
 \left(
  \begin{array}{cc}
   \widetilde{\left(\mathbf{{M}}^{\mathbf{u}}\right)^{-1}}
                              &  \mathbf{0}  \\
   \widetilde{\mathbf{{S}}^{-1}} \mathbf{B} \widetilde{\left(\mathbf{{M}}^{\mathbf{u}}\right)^{-1}}   & -\widetilde{\mathbf{{S}}^{-1}}
  \end{array}
 \right)
@f}
where a tilde indicates an approximation of the exact inverse matrix. In
particular, since $\left(\mathbf{{M}}^{\mathbf{u}}\right)^{-1}=\left( \left(
    \mathbf{K} \lambda_t \right)^{-1}
  \mathbf{v}_{i},\mathbf{v}_{j}\right)_{\Omega}$
is a sparse symmetric and positive definite matrix, we choose for
$\widetilde{\left(\mathbf{{M}}^{\mathbf{u}}\right)^{-1}}$ a single application of
a sparse incomplete Cholesky decomposition of this matrix
[Golub and Van Loan 1996].
We note that the Schur complement that corresponds to the porous
media flow operator in non-mixed form, $-\nabla \cdot [\mathbf K
\lambda_t(S)]\nabla$ and
$\mathbf{\tilde {S}} = \left( \left( \mathbf{K} \lambda_t \right) \nabla \phi_{i},\nabla \phi_{j}\right)_{\Omega}$
should be a good approximation of the actual Schur complement matrix $\mathbf
S$. Since both of these matrices are again symmetric and positive definite, we
use an incomplete Cholesky decomposition of $\mathbf{\tilde S}$ for $\widetilde
{\mathbf{{S}}^{-1}}$. It is important to note that $\mathbf{\tilde S}$ needs
to be built with Dirichlet boundary conditions to ensure its invertibility.

Once the velocity $\mathbf{U}^{(n)} \equiv \mathbf{u}^*_t$  is available, we
can assemble $\mathbf{H}$ and
$\mathbf{F}_{3}$ and solve for the saturations using
@f{align*}
  \mathbf{M}^{S} \mathbf{S}^{(n)} = \mathbf{F}_{3} - \mathbf{H} \mathbf{U}^{(n)}.
@f}
where the mass matrix $\mathbf{M}^{S}$ is solved by the conjugate gradient
method, using an incomplete Cholesky decomposition as preconditioner once
more.

<h3>The test cases.</h3>

@note
The implementation discussed here uses and extends
parts of the step-21, step-31 and step-33 tutorial programs of this
library. In particular, if you want to understand how it works, please
consult step-21 for a discussion of the mathematical problem, and
step-31 from which most of the implementation is derived. We will not
discuss aspects of the implementation that have already been discussed
in step-31.

We show numerical results for some two-phase flow equations augmented by
appropriate initial and boundary conditions in conjunction with two different
choices of the permeability model. In the problems considered, there is no
internal source term ($q=0$). As mentioned above, quantitative numerical
results are presented in [Chueh, Djilali and Bangerth 2011].

For simplicity, we choose $\Omega=[0,1]^d,d=2,3$, though all methods (as well
as our implementation) should work equally well on general unstructured meshes.

Initial conditions are only required for the saturation variable, and we
choose $S(\mathbf{x},0)=0.2$, i.e. the porous medium is initially filled by a
mixture of the non-wetting (80%) and wetting (20%) phases. This differs from
the initial condition in step-21 where we had taken $S(\mathbf{x},0)=0$, but
for complicated mathematical reasons that are mentioned there in a longish
remark, the current method using an entropy-based artificial diffusion term
does not converge to the viscosity solution with this initial condition
without additional modifications to the method. We therefore choose this
modified version for the current program.

Furthermore, we prescribe a linear pressure on
the boundaries:
@f[
   p(\mathbf{x},t) = 1 - x \qquad
   \textrm{on} \quad \partial \Omega \times [0,T].
@f]
Pressure and saturation uniquely
determine a velocity, and the velocity determines whether a boundary segment
is an inflow or outflow boundary. On the inflow part of the boundary,
$\mathbf{\Gamma}_{in}(t)$, we impose
@f{align*}
   S(\mathbf{x},t) = 1 \qquad & \textrm{on} \quad \mathbf{\Gamma}_{in}(t) \cap \left\{x = 0\right\}, \\
   S(\mathbf{x},t) = 0 \qquad & \textrm{on} \quad \mathbf{\Gamma}_{in}(t) \backslash \left\{x = 0\right\}.
@f}
In other words, the domain is flooded by the wetting phase from the left.
No boundary conditions for the saturation are required for the outflow parts
of the boundary.

All the numerical and physical parameters used for the 2D/3D
cases are listed in the following table:

<table align="center" class="tutorial" width="50%">
<tr>
    <th>Parameter                           </th><th>Symbol          </th><th>Value               </th><th>units     </th></tr><tr>
    <td>Porosity                            </td><td>$\epsilon$      </td><td>1.0                 </td><td>-                   </td></tr><tr>
    <td>Viscosity (wetting)                 </td><td>$\mu_w$         </td><td>0.2                 </td><td>$kg \cdot m^{-1} \cdot sec^{-1}$   </td></tr><tr>
    <td>Viscosity (nonwetting)              </td><td>$\mu_{nw}$      </td><td>1.0                 </td><td>$kg \cdot m^{-1} \cdot sec^{-1}$      </td></tr><tr>
    <td>Stabilization exponent              </td><td>$\alpha$        </td><td>1.0                 </td><td>-     </td></tr><tr>
    <td>Stabilization constant              </td><td>$\beta$         </td><td>2D: 0.3; 3D: 0.27   </td><td>- </td></tr><tr>
    <td>Normalization constant              </td><td>$c_R$           </td><td>1.0                 </td><td>- </td></tr><tr>
    <td>Number of high-permeability regions </td><td>$N$             </td><td>50; 200             </td><td>- </td></tr><tr>
    <td>Operator splitting threshold        </td><td>$\theta^\ast$   </td><td>5.0              </td><td>- </td></tr>
</table>


<h3>List of references</h3>


<ol>
<li>
CC Chueh, N Djilali and W Bangerth.
<br> An h-adaptive operator splitting method for two-phase flow in 3D
  heterogeneous porous media.
<br> SIAM Journal on Scientific Computing, vol. 35 (2013), pp. B149-B175

<li>
M. Kronbichler, T. Heister, and W. Bangerth
<br> High Accuracy Mantle Convection Simulation through Modern Numerical
Methods.
<br> Geophysics Journal International, vol. 191 (2012), pp. 12-29

<li>
F Brezzi and M Fortin.
<br> <i>Mixed and Hybrid Finite Element Methods</i>.
<br> Springer-Verlag, 1991.

<li>
Z Chen.
<br> <i>Finite Element Methods and Their Applications</i>.
<br> Springer, 2005.

<li>
JL Guermond and R Pasquetti.
<br> Entropy-based nonlinear viscosity for Fourier approximations of
  conservation laws.
<br> <i>Comptes Rendus Mathematique</i>, 346(13-14):801-806, 2008.

<li>
CC Chueh, M Secanell, W Bangerth, and N Djilali.
<br> Multi-level adaptive simulation of transient two-phase flow in
  heterogeneous porous media.
<br> <i>Computers and Fluids</i>, 39:1585-1596, 2010.

<li>
Y Saad and MH Schultz.
<br> Gmres: A generalized minimal residual algorithm for solving
  nonsymmetric linear systems.
<br> <i>SIAM Journal on Scientific and Statistical Computing</i>,
  7(3):856-869, 1986.

<li>
F Zhang.
<br> <i>The Schur Complement and its Applications</i>.
<br> Springer, 2005.

<li>
D Silvester and A Wathen.
<br> Fast iterative solution of stabilised Stokes systems part ii: Using
  general block preconditioners.
<br> <i>SIAM Journal on Numerical Analysis</i>, 31(5):1352-1367, 1994.

<li>
GH Golub and CF van Loan.
<br> <i>Matrix Computations</i>.
<br> 3rd Edition, Johns Hopkins, 1996.

<li>
SE Buckley and MC Leverett.
<br> Mechanism of fluid displacements in sands.
<br> <i>AIME Trans.</i>, 146:107-116, 1942.

</ol>


examples/step-43/doc/results.dox
<h1>Results</h1>


The output of this program is not really much different from that of
step-21: it solves the same problem, after all. Of more importance are
quantitative metrics such as the accuracy of the solution as well as
the time needed to compute it. These are documented in detail in the
two publications listed at the top of this page and we won't repeat
them here.

That said, no tutorial program is complete without a couple of good
pictures, so here is some output of a run in 3d:

<table align="center" class="tutorial" cellspacing="3" cellpadding="3">
  <tr>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-43.3d.velocity.png" alt="">
	<p align="center">
        Velocity vectors of flow through the porous medium with random
        permeability model. Streaming paths of high permeability and resulting
        high velocity are clearly visible.
	</p>
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-43.3d.streamlines.png" alt="">
	<p align="center">
        Streamlines colored by the saturation along the streamline path. Blue
        streamlines indicate low saturations, i.e., the flow along these
	streamlines must be slow or else more fluid would have been
        transported along them. On the other hand, green paths indicate high
        velocities since the fluid front has already reached further into the
        domain.
	</p>
    </td>
  </tr>
  <tr>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-43.3d.saturation.png" alt="">
	<p align="center">
        Streamlines with a volume rendering of the saturation, showing how far
        the fluid front has advanced at this time.
	</p>
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-43.3d.mesh.png" alt="">
	<p align="center">
	Surface of the mesh showing the adaptive refinement along the front.
	</p>
    </td>
  </tr>
</table>


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

The primary objection one may have to this program is that it is still too
slow: 3d computations on reasonably fine meshes are simply too expensive to be
done routinely and with reasonably quick turn-around. This is similar to the
situation we were in when we wrote step-31, from which this program has taken
much inspiration. The solution is similar as it was there as well: We need to
parallelize the program in a way similar to how we derived step-32 out of
step-31. In fact, all of the techniques used in step-32 would be transferable
to this program as well, making the program run on dozens or hundreds of
processors immediately.

A different direction is to make the program more relevant to many other
porous media applications. Specifically, one avenue is to go to the primary
user of porous media flow simulators, namely the oil industry. There,
applications in this area are dominated by multiphase flow (i.e., more than
the two phases we have here), and the reactions they may have with each other
(or any other way phases may exchange mass, such as through dissolution in and
bubbling out of gas from the oil phase). Furthermore, the presence of gas
often leads to compressibility effects of the fluid. Jointly, these effects
are typically formulated in the widely-used "black oil model". True reactions
between multiple phases also play a role in oil reservoir modeling when
considering controlled burns of oil in the reservoir to raise pressure and
temperature. These are much more complex problems, though, and left for future
projects.

Finally, from a mathematical perspective, we have derived the
criterion for re-computing the velocity/pressure solution at a given
time step under the assumption that we want to compare the solution we
would get at the current time step with that computed the last time we
actually solved this system. However, in the program, whenever we did
not re-compute the solution, we didn't just use the previously
computed solution but instead extrapolated from the previous two times
we solved the system. Consequently, the criterion was pessimistically
stated: what we should really compare is the solution we would get at
the current time step with the extrapolated one. Re-stating the
theorem in this regard is left as an exercise.

There are also other ways to extend the mathematical foundation of
this program; for example, one may say that it isn't the velocity we
care about, but in fact the saturation. Thus, one may ask whether the
criterion we use here to decide whether $\mathbf u$ needs to be
recomputed is appropriate; one may, for example, suggest that it is
also important to decide whether (and by how much) a wrong velocity
field in fact affects the solution of the saturation equation. This
would then naturally lead to a sensitivity analysis.

From an algorithmic viewpoint, we have here used a criterion for refinement
that is often used in engineering, namely by looking at the gradient of
the solution. However, if you inspect the solution, you will find that
it quickly leads to refinement almost everywhere, even in regions where it
is clearly not necessary: frequently used therefore does not need to imply
that it is a useful criterion to begin with. On the other hand, replacing
this criterion by a different and better one should not be very difficult.
For example, the KellyErrorEstimator class used in many other programs
should certainly be applicable to the current problem as well.


examples/step-44/doc/intro.dox
<br>

<i>This program was contributed by Jean-Paul Pelteret and Andrew McBride.
<br>
This material is based upon work supported by  the German Science Foundation (Deutsche
Forschungsgemeinschaft, DFG), grant STE 544/39-1,  and the National Research Foundation of South Africa.
</i>

@dealiiTutorialDOI{10.5281/zenodo.439772,https://zenodo.org/badge/DOI/10.5281/zenodo.439772.svg}

<a name="Intro"></a>
<h1>Introduction</h1>

The subject of this tutorial is nonlinear solid mechanics.
Classical single-field approaches (see e.g. step-18) can not correctly describe the response of quasi-incompressible materials.
The response is overly stiff; a phenomenon known as locking.
Locking problems can be circumvented using a variety of alternative strategies.
One such strategy is the  three-field formulation.
It is used here  to model the three-dimensional, fully-nonlinear (geometrical and material) response of an isotropic continuum body.
The material response is approximated as hyperelastic.
Additionally, the three-field formulation employed is valid for quasi-incompressible as well as compressible materials.

The objective of this presentation is to provide a basis for using deal.II for problems in nonlinear solid mechanics.
The linear problem was addressed in step-8.
A non-standard, hypoelastic-type form of the geometrically nonlinear problem was partially considered in step-18: a rate form of the linearised constitutive relations is used and the problem domain evolves with the motion.
Important concepts surrounding the nonlinear kinematics are absent in the theory and implementation.
Step-18 does, however, describe many of the key concepts to implement elasticity within the framework of deal.II.

We begin with a crash-course in nonlinear kinematics.
For the sake of simplicity, we restrict our attention to the quasi-static problem.
Thereafter, various key stress measures are introduced and the constitutive model described.
We then describe the three-field formulation in detail prior to explaining the structure of the class used to manage the material.
The setup of the example problem is then presented.

@note This tutorial has been developed (and is described in the introduction) for the problem of elasticity in three dimensions.
 While the space dimension could be changed in the main() routine, care needs to be taken.
 Two-dimensional elasticity problems, in general, exist only as idealizations of three-dimensional ones.
 That is, they are either plane strain or plane stress.
 The assumptions that follow either of these choices needs to be consistently imposed.
 For more information see the note in step-8.

<h3>List of references</h3>

The three-field formulation implemented here was pioneered by Simo et al. (1985) and is known as the mixed Jacobian-pressure formulation.
Important related contributions include those by Simo and Taylor (1991), and Miehe (1994).
The notation adopted here draws heavily on the excellent overview of the theoretical aspects of nonlinear solid mechanics by Holzapfel (2001).
A nice overview of issues pertaining to incompressible elasticity (at small strains) is given in Hughes (2000).

<ol>
	<li> J.C. Simo, R.L. Taylor and K.S. Pister (1985),
		Variational and projection methods for the volume constraint in finite deformation elasto-plasticity,
		<em> Computer Methods in Applied Mechanics and Engineering </em>,
		<strong> 51 </strong>, 1-3,
		177-208.
		DOI: <a href="http://doi.org/10.1016/0045-7825(85)90033-7">10.1016/0045-7825(85)90033-7</a>;
	<li> J.C. Simo and R.L. Taylor (1991),
  		Quasi-incompressible finite elasticity in principal stretches. Continuum
			basis and numerical algorithms,
		<em> Computer Methods in Applied Mechanics and Engineering </em>,
		<strong> 85 </strong>, 3,
		273-310.
		DOI: <a href="http://doi.org/10.1016/0045-7825(91)90100-K">10.1016/0045-7825(91)90100-K</a>;
	<li> C. Miehe (1994),
		Aspects of the formulation and finite element implementation of large strain isotropic elasticity
		<em> International Journal for Numerical Methods in Engineering </em>
		<strong> 37 </strong>, 12,
		1981-2004.
		DOI: <a href="http://doi.org/10.1002/nme.1620371202">10.1002/nme.1620371202</a>;
	<li> G.A. Holzapfel (2001),
		Nonlinear Solid Mechanics. A Continuum Approach for Engineering,
		John Wiley & Sons.
		ISBN: 0-471-82304-X;
	<li> T.J.R. Hughes (2000),
		The Finite Element Method: Linear Static and Dynamic Finite Element Analysis,
		Dover.
		ISBN: 978-0486411811
</ol>

An example where this three-field formulation is used in a coupled problem is documented in
<ol>
	<li> J-P. V. Pelteret, D. Davydov, A. McBride, D. K. Vu, and P. Steinmann (2016),
		Computational electro- and magneto-elasticity for quasi-incompressible media immersed in free space,
		<em> International Journal for Numerical Methods in Engineering </em>.
		DOI: <a href="http://doi.org/10.1002/nme.5254">10.1002/nme.5254</a>
</ol>

<h3> Notation </h3>

One can think of fourth-order tensors as linear operators mapping second-order
tensors (matrices) onto themselves in much the same way as matrices map
vectors onto vectors.
There are various fourth-order unit tensors that will be required in the forthcoming presentation.
The fourth-order unit tensors $\mathcal{I}$ and $\overline{\mathcal{I}}$ are defined by
@f[
	\mathbf{A} = \mathcal{I}:\mathbf{A}
		\qquad \text{and} \qquad
	\mathbf{A}^T = \overline{\mathcal{I}}:\mathbf{A} \, .
@f]
Note $\mathcal{I} \neq \overline{\mathcal{I}}^T$.
Furthermore, we define the symmetric and skew-symmetric fourth-order unit tensors by
@f[
	\mathcal{S} \dealcoloneq \dfrac{1}{2}[\mathcal{I} + \overline{\mathcal{I}}]
		\qquad \text{and} \qquad
	\mathcal{W} \dealcoloneq \dfrac{1}{2}[\mathcal{I} - \overline{\mathcal{I}}] \, ,
@f]
such that
@f[
	\dfrac{1}{2}[\mathbf{A} + \mathbf{A}^T] = \mathcal{S}:\mathbf{A}
		\qquad \text{and} \qquad
	\dfrac{1}{2}[\mathbf{A} - \mathbf{A}^T] = \mathcal{W}:\mathbf{A} \, .
@f]
The fourth-order <code>SymmetricTensor</code> returned by identity_tensor() is $\mathcal{S}$.


<h3>Kinematics</h3>

Let the time domain be denoted $\mathbb{T} = [0,T_{\textrm{end}}]$, where $t \in \mathbb{T}$ and $T_{\textrm{end}}$ is the total problem duration.
Consider a continuum body that occupies the reference configuration $\Omega_0$ at time $t=0$.
%Particles in the reference configuration are identified by the position vector $\mathbf{X}$.
The configuration of the body at a later time $t>0$ is termed the current configuration, denoted $\Omega$, with particles identified by the vector $\mathbf{x}$.
The nonlinear map between the reference and current configurations, denoted $\boldsymbol{\varphi}$, acts as follows:
@f[
	\mathbf{x} = \boldsymbol{\varphi}(\mathbf{X},t) \, .
@f]
The material description of the displacement of a particle is defined by
@f[
	\mathbf{U}(\mathbf{X},t) = \mathbf{x}(\mathbf{X},t) - \mathbf{X} \, .
@f]

The deformation gradient $\mathbf{F}$ is defined as the material gradient of the motion:
@f[
	\mathbf{F}(\mathbf{X},t)
		\dealcoloneq \dfrac{\partial \boldsymbol{\varphi}(\mathbf{X},t)}{\partial \mathbf{X}}
		= \textrm{Grad}\ \mathbf{x}(\mathbf{X},t)
		= \mathbf{I} + \textrm{Grad}\ \mathbf{U} \, .
@f]
The determinant of the of the deformation gradient
$J(\mathbf{X},t) \dealcoloneq \textrm{det}\ \mathbf{F}(\mathbf{X},t) > 0$
maps corresponding volume elements in the reference and current configurations, denoted
$\textrm{d}V$ and $\textrm{d}v$,
respectively, as
@f[
	\textrm{d}v = J(\mathbf{X},t)\; \textrm{d}V \, .
@f]

Two important measures of the deformation in terms of the spatial and material coordinates are the left and right Cauchy-Green tensors, respectively,
and denoted $\mathbf{b} \dealcoloneq \mathbf{F}\mathbf{F}^T$ and $\mathbf{C} \dealcoloneq \mathbf{F}^T\mathbf{F}$.
They are both symmetric and positive definite.

The Green-Lagrange strain tensor is defined by
@f[
	\mathbf{E} \dealcoloneq \frac{1}{2}[\mathbf{C} - \mathbf{I} ]
		= \underbrace{\frac{1}{2}[\textrm{Grad}^T \mathbf{U} +	\textrm{Grad}\mathbf{U}]}_{\boldsymbol{\varepsilon}}
			+ \frac{1}{2}[\textrm{Grad}^T\ \mathbf{U}][\textrm{Grad}\ \mathbf{U}] \, .
@f]
If the assumption of infinitesimal deformations is made, then the second term
on the right can be neglected, and $\boldsymbol{\varepsilon}$ (the linearised
strain tensor) is the only component of the strain tensor.
This assumption is, looking at the setup of the problem, not valid in step-18,
making the use of the linearized $\boldsymbol{\varepsilon}$ as the strain
measure in that tutorial program questionable.

In order to handle the different response that materials exhibit when subjected to bulk and shear type deformations we consider the following decomposition of the deformation gradient $\mathbf{F}$  and the left Cauchy-Green tensor $\mathbf{b}$ into volume-changing (volumetric) and volume-preserving (isochoric) parts:
@f[
	\mathbf{F}
		= (J^{1/3}\mathbf{I})\overline{\mathbf{F}}
	\qquad \text{and} \qquad
	\mathbf{b}
        = (J^{2/3}\mathbf{I})\overline{\mathbf{F}}\,\overline{\mathbf{F}}^T
		=  (J^{2/3}\mathbf{I})\overline{\mathbf{b}} \, .
@f]
Clearly, $\textrm{det}\ \mathbf{F} = \textrm{det}\ (J^{1/3}\mathbf{I}) = J$.

The spatial velocity field is denoted $\mathbf{v}(\mathbf{x},t)$.
The derivative of the spatial velocity field with respect to the spatial coordinates gives the spatial velocity gradient $\mathbf{l}(\mathbf{x},t)$, that is
@f[
	\mathbf{l}(\mathbf{x},t)
		\dealcoloneq \dfrac{\partial \mathbf{v}(\mathbf{x},t)}{\partial \mathbf{x}}
		= \textrm{grad}\ \mathbf{v}(\mathbf{x},t) \, ,
@f]
where $\textrm{grad} \{\bullet \}
= \frac{\partial \{ \bullet \} }{ \partial \mathbf{x}}
= \frac{\partial \{ \bullet \} }{ \partial \mathbf{X}}\frac{\partial \mathbf{X} }{ \partial \mathbf{x}}
= \textrm{Grad} \{ \bullet \} \mathbf{F}^{-1}$.


<h3>Kinetics</h3>

Cauchy's stress theorem equates the Cauchy traction $\mathbf{t}$ acting on an infinitesimal surface element in the current configuration $\mathrm{d}a$ to the product of the Cauchy stress tensor $\boldsymbol{\sigma}$ (a spatial quantity)  and the outward unit normal to the surface $\mathbf{n}$ as
@f[
	\mathbf{t}(\mathbf{x},t, \mathbf{n}) = \boldsymbol{\sigma}\mathbf{n} \, .
@f]
The Cauchy stress is symmetric.
Similarly,  the first Piola-Kirchhoff traction $\mathbf{T}$ which acts on an infinitesimal surface element in the reference configuration $\mathrm{d}A$ is the product of the first Piola-Kirchhoff stress tensor $\mathbf{P}$ (a two-point tensor)  and the outward unit normal to the surface $\mathbf{N}$ as
@f[
	\mathbf{T}(\mathbf{X},t, \mathbf{N}) = \mathbf{P}\mathbf{N} \, .
@f]
The Cauchy traction $\mathbf{t}$ and the first Piola-Kirchhoff traction $\mathbf{T}$ are related as
@f[
	\mathbf{t}\mathrm{d}a = \mathbf{T}\mathrm{d}A \, .
@f]
This can be demonstrated using <a href="http://en.wikipedia.org/wiki/Finite_strain_theory">Nanson's formula</a>.

The first Piola-Kirchhoff stress tensor is related to the Cauchy stress as
@f[
	\mathbf{P} = J \boldsymbol{\sigma}\mathbf{F}^{-T} \, .
@f]
Further important stress measures are the (spatial) Kirchhoff stress  $\boldsymbol{\tau} = J \boldsymbol{\sigma}$
and the (referential) second Piola-Kirchhoff stress
$\mathbf{S} = {\mathbf{F}}^{-1} \boldsymbol{\tau} {\mathbf{F}}^{-T}$.


<h3> Push-forward and pull-back operators </h3>

Push-forward and pull-back operators allow one to transform various measures between the material and spatial settings.
The stress measures used here are contravariant, while the strain measures are covariant.

The push-forward and-pull back operations for second-order covariant tensors $(\bullet)^{\text{cov}}$ are respectively given by:
@f[
	\chi_{*}(\bullet)^{\text{cov}} \dealcoloneq \mathbf{F}^{-T} (\bullet)^{\text{cov}} \mathbf{F}^{-1}
	\qquad \text{and} \qquad
	\chi^{-1}_{*}(\bullet)^{\text{cov}} \dealcoloneq \mathbf{F}^{T} (\bullet)^{\text{cov}} \mathbf{F} \, .
@f]

The push-forward and pull back operations for second-order contravariant tensors $(\bullet)^{\text{con}}$ are respectively given by:
@f[
	\chi_{*}(\bullet)^{\text{con}} \dealcoloneq \mathbf{F} (\bullet)^{\text{con}} \mathbf{F}^T
	\qquad \text{and} \qquad
	\chi^{-1}_{*}(\bullet)^{\text{con}} \dealcoloneq \mathbf{F}^{-1} (\bullet)^{\text{con}} \mathbf{F}^{-T} \, .
@f]
For example $\boldsymbol{\tau} = \chi_{*}(\mathbf{S})$.


<h3>Hyperelastic materials</h3>

A hyperelastic material response is governed by a Helmholtz free energy function $\Psi = \Psi(\mathbf{F}) = \Psi(\mathbf{C}) = \Psi(\mathbf{b})$ which serves as a potential for the stress.
For example, if the Helmholtz free energy depends on the right Cauchy-Green tensor $\mathbf{C}$ then the isotropic hyperelastic response is
@f[
	\mathbf{S}
		= 2 \dfrac{\partial \Psi(\mathbf{C})}{\partial \mathbf{C}} \, .
@f]
If the Helmholtz free energy depends on the left Cauchy-Green tensor $\mathbf{b}$ then the isotropic hyperelastic response is
@f[
	\boldsymbol{\tau}
		= 2 \dfrac{\partial \Psi(\mathbf{b})}{\partial \mathbf{b}} \mathbf{b}
		=  2 \mathbf{b} \dfrac{\partial \Psi(\mathbf{b})}{\partial \mathbf{b}} \, .
@f]

Following the multiplicative decomposition of the deformation gradient, the Helmholtz free energy can be decomposed as
@f[
	\Psi(\mathbf{b}) = \Psi_{\text{vol}}(J) + \Psi_{\text{iso}}(\overline{\mathbf{b}}) \, .
@f]
Similarly, the Kirchhoff stress can be decomposed into volumetric and isochoric parts as $\boldsymbol{\tau} = \boldsymbol{\tau}_{\text{vol}} + \boldsymbol{\tau}_{\text{iso}}$ where:
@f{align*}
	\boldsymbol{\tau}_{\text{vol}} &=
		2 \mathbf{b} \dfrac{\partial \Psi_{\textrm{vol}}(J)}{\partial \mathbf{b}}
		\\
		&= p J\mathbf{I} \, ,
		\\
	\boldsymbol{\tau}_{\text{iso}} &=
		2 \mathbf{b} \dfrac{\partial \Psi_{\textrm{iso}} (\overline{\mathbf{b}})}{\partial \mathbf{b}}
		\\
		&= \underbrace{( \mathcal{I} - \dfrac{1}{3} \mathbf{I} \otimes \mathbf{I})}_{\mathbb{P}} : \overline{\boldsymbol{\tau}} \, ,
@f}
where
$p \dealcoloneq \dfrac{\partial \Psi_{\text{vol}}(J)}{\partial J}$ is the pressure response.
$\mathbb{P}$ is the projection tensor which provides the deviatoric operator in the Eulerian setting.
The fictitious Kirchhoff stress tensor $\overline{\boldsymbol{\tau}}$ is defined by
@f[
	\overline{\boldsymbol{\tau}}
		\dealcoloneq 2 \overline{\mathbf{b}} \dfrac{\partial \Psi_{\textrm{iso}}(\overline{\mathbf{b}})}{\partial \overline{\mathbf{b}}} \, .
@f]


@note The pressure response as defined above differs from the widely-used definition of the
pressure in solid mechanics as
$p = - 1/3 \textrm{tr} \boldsymbol{\sigma} = - 1/3 J^{-1} \textrm{tr} \boldsymbol{\tau}$.
Here $p$ is the hydrostatic pressure.
We make use of the pressure response throughout this tutorial (although we refer to it as the pressure).

<h4> Neo-Hookean materials </h4>

The Helmholtz free energy corresponding to a compressible <a href="http://en.wikipedia.org/wiki/Neo-Hookean_solid">neo-Hookean material</a> is given by
@f[
    \Psi \equiv
        \underbrace{\kappa [ \mathcal{G}(J) ] }_{\Psi_{\textrm{vol}}(J)}
        + \underbrace{\bigl[c_1 [ \overline{I}_1 - 3] \bigr]}_{\Psi_{\text{iso}}(\overline{\mathbf{b}})} \, ,
@f]
where $\kappa \dealcoloneq \lambda + 2/3 \mu$ is the bulk modulus ($\lambda$ and $\mu$ are the Lam&eacute; parameters)
and $\overline{I}_1 \dealcoloneq \textrm{tr}\ \overline{\mathbf{b}}$.
The function $\mathcal{G}(J)$ is required to be strictly convex and satisfy the condition $\mathcal{G}(1) = 0$,
among others, see Holzapfel (2001) for further details.
In this work $\mathcal{G} \dealcoloneq \frac{1}{4} [ J^2 - 1 - 2\textrm{ln}J ]$.

Incompressibility imposes the isochoric constraint that $J=1$ for all motions $\boldsymbol{\varphi}$.
The Helmholtz free energy corresponding to an incompressible neo-Hookean material is given by
@f[
    \Psi \equiv
        \underbrace{\bigl[ c_1 [ I_1 - 3] \bigr] }_{\Psi_{\textrm{iso}}(\mathbf{b})} \, ,
@f]
where $ I_1 \dealcoloneq \textrm{tr}\mathbf{b} $.
Thus, the incompressible response is obtained by removing the volumetric component from the compressible free energy and enforcing $J=1$.


<h3>Elasticity tensors</h3>

We will use a Newton-Raphson strategy to solve the nonlinear boundary value problem.
Thus, we will need to linearise the constitutive relations.

The fourth-order elasticity tensor in the material description is defined by
@f[
	\mathfrak{C}
		= 2\dfrac{\partial \mathbf{S}(\mathbf{C})}{\partial \mathbf{C}}
		= 4\dfrac{\partial^2 \Psi(\mathbf{C})}{\partial \mathbf{C} \partial \mathbf{C}} \, .
@f]
The fourth-order elasticity tensor in the spatial description $\mathfrak{c}$ is obtained from the push-forward of $\mathfrak{C}$ as
@f[
	\mathfrak{c} = J^{-1} \chi_{*}(\mathfrak{C})
		\qquad \text{and thus} \qquad
	J\mathfrak{c} = 4 \mathbf{b} \dfrac{\partial^2 \Psi(\mathbf{b})} {\partial \mathbf{b} \partial \mathbf{b}} \mathbf{b}	\, .
@f]
The fourth-order elasticity tensors (for hyperelastic materials) possess both major and minor symmetries.

The fourth-order spatial elasticity tensor can be written in the following decoupled form:
@f[
	\mathfrak{c} = \mathfrak{c}_{\text{vol}} + \mathfrak{c}_{\text{iso}} \, ,
@f]
where
@f{align*}
	J \mathfrak{c}_{\text{vol}}
		&= 4 \mathbf{b} \dfrac{\partial^2 \Psi_{\text{vol}}(J)} {\partial \mathbf{b} \partial \mathbf{b}} \mathbf{b}
		\\
		&= J[\widehat{p}\, \mathbf{I} \otimes \mathbf{I} - 2p \mathcal{I}]
			\qquad \text{where} \qquad
		\widehat{p} \dealcoloneq p + \dfrac{\textrm{d} p}{\textrm{d}J} \, ,
		\\
	J \mathfrak{c}_{\text{iso}}
		&=  4 \mathbf{b} \dfrac{\partial^2 \Psi_{\text{iso}}(\overline{\mathbf{b}})} {\partial \mathbf{b} \partial \mathbf{b}} \mathbf{b}
		\\
		&= \mathbb{P} : \mathfrak{\overline{c}} : \mathbb{P}
			+ \dfrac{2}{3}[\overline{\boldsymbol{\tau}}:\mathbf{I}]\mathbb{P}
			- \dfrac{2}{3}[ \mathbf{I}\otimes\boldsymbol{\tau}_{\text{iso}}
				+ \boldsymbol{\tau}_{\text{iso}} \otimes \mathbf{I} ] \, ,
@f}
where the fictitious elasticity tensor $\overline{\mathfrak{c}}$ in the spatial description is defined by
@f[
	\overline{\mathfrak{c}}
		= 4 \overline{\mathbf{b}} \dfrac{ \partial^2 \Psi_{\textrm{iso}}(\overline{\mathbf{b}})} {\partial \overline{\mathbf{b}} \partial \overline{\mathbf{b}}} \overline{\mathbf{b}} \, .
@f]

<h3>Principle of stationary potential energy and the three-field formulation</h3>

The total potential energy of the system $\Pi$ is the sum of the internal and external potential energies, denoted $\Pi_{\textrm{int}}$ and $\Pi_{\textrm{ext}}$, respectively.
We wish to find the equilibrium configuration by minimising the potential energy.

As mentioned above, we adopt a three-field formulation.
We denote the set of primary unknowns by
$\mathbf{\Xi} \dealcoloneq \{ \mathbf{u}, \widetilde{p}, \widetilde{J} \}$.
The independent kinematic variable $\widetilde{J}$ enters the formulation as a constraint on $J$ enforced by the Lagrange multiplier $\widetilde{p}$ (the pressure, as we shall see).

The three-field variational principle used here is given by
@f[
	\Pi(\mathbf{\Xi}) \dealcoloneq \int_\Omega \bigl[
		\Psi_{\textrm{vol}}(\widetilde{J})
		+ \widetilde{p}\,[J(\mathbf{u}) - \widetilde{J}]
		+ \Psi_{\textrm{iso}}(\overline{\mathbf{b}}(\mathbf{u}))
		\bigr] \textrm{d}v
	+ 	\Pi_{\textrm{ext}} \, ,
@f]
where the external potential is defined by
@f[
	\Pi_{\textrm{ext}}
		= - \int_\Omega \mathbf{b}^\text{p} \cdot \mathbf{u}~\textrm{d}v
			- \int_{\partial \Omega_{\sigma}} \mathbf{t}^\text{p} \cdot \mathbf{u}~\textrm{d}a \, .
@f]
The boundary of the current configuration  $\partial \Omega$ is composed into two parts as
$\partial \Omega = \partial \Omega_{\mathbf{u}} \cup \partial \Omega_{\sigma}$,
where
$\partial \Omega_{\mathbf{u}} \cap \partial \Omega_{\boldsymbol{\sigma}} = \emptyset$.
The prescribed Cauchy traction, denoted $\mathbf{t}^\text{p}$, is applied to $ \partial \Omega_{\boldsymbol{\sigma}}$ while the motion is prescribed on the remaining portion of the boundary $\partial \Omega_{\mathbf{u}}$.
The body force per unit current volume is denoted $\mathbf{b}^\text{p}$.



The stationarity of the potential follows as
@f{align*}
	R(\mathbf\Xi;\delta \mathbf{\Xi})
		&= D_{\delta \mathbf{\Xi}}\Pi(\mathbf{\Xi})
		\\
		&= \dfrac{\partial \Pi(\mathbf{\Xi})}{\partial \mathbf{u}} \cdot \delta \mathbf{u}
			+ \dfrac{\partial \Pi(\mathbf{\Xi})}{\partial \widetilde{p}} \delta \widetilde{p}
			+ \dfrac{\partial \Pi(\mathbf{\Xi})}{\partial \widetilde{J}} \delta \tilde{J}
			\\
		&= \int_{\Omega_0}  \left[
			\textrm{grad}\ \delta\mathbf{u} : [ \underbrace{[\widetilde{p} J \mathbf{I}]}_{\equiv \boldsymbol{\tau}_{\textrm{vol}}}
            +  \boldsymbol{\tau}_{\textrm{iso}}]
			+ \delta \widetilde{p}\, [ J(\mathbf{u}) - \widetilde{J}]
			+ \delta \widetilde{J}\left[ \dfrac{\textrm{d} \Psi_{\textrm{vol}}(\widetilde{J})}{\textrm{d} \widetilde{J}}
            -\widetilde{p}\right]
			\right]~\textrm{d}V
			\\
		&\quad - \int_{\Omega_0} \delta \mathbf{u} \cdot \mathbf{B}^\text{p}~\textrm{d}V
			- \int_{\partial \Omega_{0,\boldsymbol{\sigma}}} \delta \mathbf{u} \cdot \mathbf{T}^\text{p}~\textrm{d}A
			\\
		&=0 \, ,
@f}
for all virtual displacements $\delta \mathbf{u} \in H^1(\Omega)$ subject to the constraint that $\delta \mathbf{u} = \mathbf{0}$ on $\partial \Omega_{\mathbf{u}}$, and all virtual pressures $\delta \widetilde{p} \in L^2(\Omega)$ and virtual dilatations $\delta \widetilde{J} \in L^2(\Omega)$.

One should note that the definitions of the volumetric Kirchhoff stress in the three field formulation
$\boldsymbol{\tau}_{\textrm{vol}} \equiv \widetilde{p} J \mathbf{I}$
 and the subsequent volumetric tangent differs slightly from the general form given in the section on hyperelastic materials where
$\boldsymbol{\tau}_{\textrm{vol}} \equiv p J\mathbf{I}$.
This is because the pressure $\widetilde{p}$ is now a primary field as opposed to a constitutively derived quantity.
One needs to carefully distinguish between the primary fields and those obtained from the constitutive relations.

@note Although the variables are all expressed in terms of spatial quantities, the domain of integration is the initial configuration.
This approach is called a <em> total-Lagrangian formulation </em>.
The approach given in step-18, where the domain of integration is the current configuration, could be called an <em> updated Lagrangian formulation </em>.
The various merits of these two approaches are discussed widely in the literature.
It should be noted however that they are equivalent.


The Euler-Lagrange equations corresponding to the residual are:
@f{align*}
	&\textrm{div}\ \boldsymbol{\sigma} + \mathbf{b}^\text{p} = \mathbf{0} && \textrm{[equilibrium]}
		\\
	&J(\mathbf{u}) = \widetilde{J} 		&& \textrm{[dilatation]}
		\\
	&\widetilde{p} = \dfrac{\textrm{d} \Psi_{\textrm{vol}}(\widetilde{J})}{\textrm{d} \widetilde{J}} && \textrm{[pressure]} \, .
@f}
The first equation is the (quasi-static) equilibrium equation in the spatial setting.
The second is the constraint that $J(\mathbf{u}) = \widetilde{J}$.
The third is the definition of the pressure $\widetilde{p}$.

@note The simplified single-field derivation ($\mathbf{u}$ is the only primary variable) below makes it clear how we transform the limits of integration to the reference domain:
@f{align*}
\int_{\Omega}\delta \mathbf{u} \cdot [\textrm{div}\ \boldsymbol{\sigma} + \mathbf{b}^\text{p}]~\mathrm{d}v
&=
\int_{\Omega} [-\mathrm{grad}\delta \mathbf{u}:\boldsymbol{\sigma} + \delta \mathbf{u} \cdot\mathbf{b}^\text{p}]~\mathrm{d}v
  + \int_{\partial \Omega} \delta \mathbf{u} \cdot \mathbf{t}^\text{p}~\mathrm{d}a \\
&=
- \int_{\Omega_0} \mathrm{grad}\delta \mathbf{u}:\boldsymbol{\tau}~\mathrm{d}V
+ \int_{\Omega_0} \delta \mathbf{u} \cdot J\mathbf{b}^\text{p}~\mathrm{d}V
 + \int_{\partial \Omega_0} \delta \mathbf{u} \cdot \mathbf{T}^\text{p}~\mathrm{d}A \\
&=
- \int_{\Omega_0} \mathrm{grad}\delta \mathbf{u}:\boldsymbol{\tau}~\mathrm{d}V
+ \int_{\Omega_0} \delta \mathbf{u} \cdot \mathbf{B}^\text{p}~\mathrm{d}V
 + \int_{\partial \Omega_{0,\sigma}} \delta \mathbf{u} \cdot \mathbf{T}^\text{p}~\mathrm{d}A \\
&=
- \int_{\Omega_0} [\mathrm{grad}\delta\mathbf{u}]^{\text{sym}} :\boldsymbol{\tau}~\mathrm{d}V
+ \int_{\Omega_0} \delta \mathbf{u} \cdot \mathbf{B}^\text{p}~\mathrm{d}V
 + \int_{\partial \Omega_{0,\sigma}} \delta \mathbf{u} \cdot \mathbf{T}^\text{p}~\mathrm{d}A \, ,
@f}
where
$[\mathrm{grad}\delta\mathbf{u}]^{\text{sym}} = 1/2[ \mathrm{grad}\delta\mathbf{u} + [\mathrm{grad}\delta\mathbf{u}]^T] $.

We will use an iterative Newton-Raphson method to solve the nonlinear residual equation $R$.
For the sake of simplicity we assume dead loading, i.e. the loading does not change due to the deformation.

The change in a quantity between the known state at $t_{\textrm{n}-1}$
and the currently unknown state at $t_{\textrm{n}}$ is denoted
$\varDelta \{ \bullet \} = { \{ \bullet \} }^{\textrm{n}} - { \{ \bullet \} }^{\textrm{n-1}}$.
The value of a quantity at the current iteration $\textrm{i}$ is denoted
${ \{ \bullet \} }^{\textrm{n}}_{\textrm{i}} = { \{ \bullet \} }_{\textrm{i}}$.
The incremental change between iterations $\textrm{i}$ and $\textrm{i}+1$ is denoted
$d \{ \bullet \} \dealcoloneq \{ \bullet \}_{\textrm{i}+1} - \{ \bullet \}_{\textrm{i}}$.

Assume that the state of the system is known for some iteration $\textrm{i}$.
The linearised approximation to nonlinear governing equations to be solved using the  Newton-Raphson method is:
Find $d \mathbf{\Xi}$ such that
@f[
	R(\mathbf{\Xi}_{\mathsf{i}+1}) =
		R(\mathbf{\Xi}_{\mathsf{i}})
		+ D^2_{d \mathbf{\Xi}, \delta \mathbf{\Xi}} \Pi(\mathbf{\Xi_{\mathsf{i}}}) \cdot d \mathbf{\Xi} \equiv 0 \, ,
@f]
then set
$\mathbf{\Xi}_{\textrm{i}+1} = \mathbf{\Xi}_{\textrm{i}}
+ d \mathbf{\Xi}$.
The tangent is given by

@f[
	D^2_{d \mathbf{\Xi}, \delta \mathbf{\Xi}} \Pi( \mathbf{\Xi}_{\mathsf{i}} )
		= D_{d \mathbf{\Xi}} R( \mathbf{\Xi}_{\mathsf{i}}; \delta \mathbf{\Xi})
		=: K(\mathbf{\Xi}_{\mathsf{i}}; d \mathbf{\Xi}, \delta \mathbf{\Xi}) \, .
@f]
Thus,
@f{align*}
 	K(\mathbf{\Xi}_{\mathsf{i}}; d \mathbf{\Xi}, \delta \mathbf{\Xi})
 		&=
 			D_{d \mathbf{u}} R( \mathbf{\Xi}_{\mathsf{i}}; \delta \mathbf{\Xi}) \cdot d \mathbf{u}
 			\\
 				&\quad +
 			 	D_{d \widetilde{p}} R( \mathbf{\Xi}_{\mathsf{i}}; \delta \mathbf{\Xi})  d \widetilde{p}
 			 \\
 			 	&\quad +
 			  D_{d \widetilde{J}} R( \mathbf{\Xi}_{\mathsf{i}}; \delta \mathbf{\Xi})  d \widetilde{J} \, ,
@f}
where
@f{align*}
	D_{d \mathbf{u}} R( \mathbf{\Xi}; \delta \mathbf{\Xi})
 	&=
 	\int_{\Omega_0} \bigl[ \textrm{grad}\ \delta \mathbf{u} :
 			\textrm{grad}\ d \mathbf{u} [\boldsymbol{\tau}_{\textrm{iso}} + \boldsymbol{\tau}_{\textrm{vol}}]
 			+ \textrm{grad}\ \delta \mathbf{u} :[
             \underbrace{[\widetilde{p}J[\mathbf{I}\otimes\mathbf{I} - 2 \mathcal{I}]}_{\equiv J\mathfrak{c}_{\textrm{vol}}} +
             J\mathfrak{c}_{\textrm{iso}}] :\textrm{grad} d \mathbf{u}
 		\bigr]~\textrm{d}V \, ,
 		\\
 	&\quad + \int_{\Omega_0} \delta \widetilde{p} J \mathbf{I} : \textrm{grad}\ d \mathbf{u} ~\textrm{d}V
 	\\
 	D_{d \widetilde{p}} R( \mathbf{\Xi}; \delta \mathbf{\Xi})
 	&=
 	\int_{\Omega_0} \textrm{grad}\ \delta \mathbf{u} : J \mathbf{I} d \widetilde{p} ~\textrm{d}V
 		-  \int_{\Omega_0} \delta \widetilde{J} d \widetilde{p}  ~\textrm{d}V \, ,
 	\\
 	D_{d \widetilde{J}} R( \mathbf{\Xi}; \delta \mathbf{\Xi})
 	&=  -\int_{\Omega_0} \delta \widetilde{p} d \widetilde{J}~\textrm{d}V
 	 + \int_{\Omega_0} \delta \widetilde{J}  \dfrac{\textrm{d}^2 \Psi_{\textrm{vol}}(\widetilde{J})}{\textrm{d} \widetilde{J}\textrm{d}\widetilde{J}} d \widetilde{J} ~\textrm{d}V \, .
@f}

Note that the following terms are termed the geometrical stress and  the material contributions to the tangent matrix:
@f{align*}
& \int_{\Omega_0} \textrm{grad}\ \delta \mathbf{u} :
 			\textrm{grad}\ d \mathbf{u} [\boldsymbol{\tau}_{\textrm{iso}} +  \boldsymbol{\tau}_{\textrm{vol}}]~\textrm{d}V
 			&& \quad {[\textrm{Geometrical stress}]} \, ,
 		\\
& \int_{\Omega_0} \textrm{grad} \delta \mathbf{u} :
 			[J\mathfrak{c}_{\textrm{vol}} + J\mathfrak{c}_{\textrm{iso}}] :\textrm{grad}\ d \mathbf{u}
 		~\textrm{d}V
 		&& \quad {[\textrm{Material}]} \, .
@f}


<h3> Discretization of governing equations </h3>

The three-field formulation used here is effective for quasi-incompressible materials,
that is where $\nu \rightarrow 0.5$ (where $\nu$ is <a
href="http://en.wikipedia.org/wiki/Poisson's_ratio">Poisson's ratio</a>), subject to a good choice of the interpolation fields
for $\mathbf{u},~\widetilde{p}$ and $\widetilde{J}$.
Typically a choice of $Q_n \times DGPM_{n-1} \times DGPM_{n-1}$ is made.
Here $DGPM$ is the FE_DGPMonomial class.
A popular choice is $Q_1 \times DGPM_0 \times DGPM_0$ which is known as the mean dilatation method (see Hughes (2000) for an intuitive discussion).
This code can accommodate a $Q_n \times DGPM_{n-1} \times DGPM_{n-1}$ formulation.
The discontinuous approximation
allows $\widetilde{p}$ and $\widetilde{J}$ to be condensed out
and a classical displacement based method is recovered.

For fully-incompressible materials $\nu = 0.5$ and the three-field formulation will still exhibit
locking behavior.
This can be overcome by introducing an additional constraint into the free energy of the form
$\int_{\Omega_0} \Lambda [ \widetilde{J} - 1]~\textrm{d}V$.
Here $\Lambda$ is a Lagrange multiplier to enforce the isochoric constraint.
For further details see Miehe (1994).

The linearised problem can be written as
@f[
	\mathbf{\mathsf{K}}( \mathbf{\Xi}_{\textrm{i}}) d\mathbf{\Xi}
	=
	\mathbf{ \mathsf{F}}(\mathbf{\Xi}_{\textrm{i}})
@f]
where
@f{align*}
		\underbrace{\begin{bmatrix}
			\mathbf{\mathsf{K}}_{uu}	&	\mathbf{\mathsf{K}}_{u\widetilde{p}}	& \mathbf{0}
			\\
			\mathbf{\mathsf{K}}_{\widetilde{p}u}	&	\mathbf{0}	&	\mathbf{\mathsf{K}}_{\widetilde{p}\widetilde{J}}
			\\
			\mathbf{0}	& 	\mathbf{\mathsf{K}}_{\widetilde{J}\widetilde{p}}		& \mathbf{\mathsf{K}}_{\widetilde{J}\widetilde{J}}
		\end{bmatrix}}_{\mathbf{\mathsf{K}}(\mathbf{\Xi}_{\textrm{i}})}
		\underbrace{\begin{bmatrix}
			d \mathbf{\mathsf{u}}\\
            d \widetilde{\mathbf{\mathsf{p}}} \\
            d \widetilde{\mathbf{\mathsf{J}}}
		\end{bmatrix}}_{d \mathbf{\Xi}}
        =
        \underbrace{\begin{bmatrix}
			-\mathbf{\mathsf{R}}_{u}(\mathbf{u}_{\textrm{i}}) \\
            -\mathbf{\mathsf{R}}_{\widetilde{p}}(\widetilde{p}_{\textrm{i}}) \\
           -\mathbf{\mathsf{R}}_{\widetilde{J}}(\widetilde{J}_{\textrm{i}})
		\end{bmatrix}}_{ -\mathbf{\mathsf{R}}(\mathbf{\Xi}_{\textrm{i}}) }
=
        \underbrace{\begin{bmatrix}
			\mathbf{\mathsf{F}}_{u}(\mathbf{u}_{\textrm{i}}) \\
            \mathbf{\mathsf{F}}_{\widetilde{p}}(\widetilde{p}_{\textrm{i}}) \\
           \mathbf{\mathsf{F}}_{\widetilde{J}}(\widetilde{J}_{\textrm{i}})
		\end{bmatrix}}_{ \mathbf{\mathsf{F}}(\mathbf{\Xi}_{\textrm{i}}) } \, .
@f}

There are no derivatives of the pressure and dilatation (primary) variables present in the formulation.
Thus the discontinuous finite element interpolation of the pressure and dilatation yields a block
diagonal matrix for
$\mathbf{\mathsf{K}}_{\widetilde{p}\widetilde{J}}$,
$\mathbf{\mathsf{K}}_{\widetilde{J}\widetilde{p}}$ and
$\mathbf{\mathsf{K}}_{\widetilde{J}\widetilde{J}}$.
Therefore we can easily express the fields $\widetilde{p}$ and $\widetilde{J}$ on each cell simply
by inverting a local matrix and multiplying it by the local right hand
side. We can then insert the result into the remaining equations and recover
a classical displacement-based method.
In order to condense out the pressure and dilatation contributions at the element level we need the following results:
@f{align*}
		d \widetilde{\mathbf{\mathsf{p}}}
		& = \mathbf{\mathsf{K}}_{\widetilde{J}\widetilde{p}}^{-1} \bigl[
			 \mathbf{\mathsf{F}}_{\widetilde{J}}
			 - \mathbf{\mathsf{K}}_{\widetilde{J}\widetilde{J}} d \widetilde{\mathbf{\mathsf{J}}} \bigr]
			\\
		d \widetilde{\mathbf{\mathsf{J}}}
		& = \mathbf{\mathsf{K}}_{\widetilde{p}\widetilde{J}}^{-1} \bigl[
			\mathbf{\mathsf{F}}_{\widetilde{p}}
			- \mathbf{\mathsf{K}}_{\widetilde{p}u} d \mathbf{\mathsf{u}}
			\bigr]
		\\
		 \Rightarrow d \widetilde{\mathbf{\mathsf{p}}}
		&=  \mathbf{\mathsf{K}}_{\widetilde{J}\widetilde{p}}^{-1} \mathbf{\mathsf{F}}_{\widetilde{J}}
		- \underbrace{\bigl[\mathbf{\mathsf{K}}_{\widetilde{J}\widetilde{p}}^{-1} \mathbf{\mathsf{K}}_{\widetilde{J}\widetilde{J}}
		\mathbf{\mathsf{K}}_{\widetilde{p}\widetilde{J}}^{-1}\bigr]}_{\overline{\mathbf{\mathsf{K}}}}\bigl[ \mathbf{\mathsf{F}}_{\widetilde{p}}
 		- \mathbf{\mathsf{K}}_{\widetilde{p}u} d \mathbf{\mathsf{u}} \bigr]
@f}
and thus
@f[
		\underbrace{\bigl[ \mathbf{\mathsf{K}}_{uu} + \overline{\overline{\mathbf{\mathsf{K}}}}~ \bigr]
		}_{\mathbf{\mathsf{K}}_{\textrm{con}}} d \mathbf{\mathsf{u}}
		=
        \underbrace{
		\Bigl[
		\mathbf{\mathsf{F}}_{u}
			- \mathbf{\mathsf{K}}_{u\widetilde{p}} \bigl[ \mathbf{\mathsf{K}}_{\widetilde{J}\widetilde{p}}^{-1} \mathbf{\mathsf{F}}_{\widetilde{J}}
			- \overline{\mathbf{\mathsf{K}}}\mathbf{\mathsf{F}}_{\widetilde{p}} \bigr]
		\Bigr]}_{\mathbf{\mathsf{F}}_{\textrm{con}}}
@f]
where
@f[
		\overline{\overline{\mathbf{\mathsf{K}}}} \dealcoloneq
			\mathbf{\mathsf{K}}_{u\widetilde{p}} \overline{\mathbf{\mathsf{K}}} \mathbf{\mathsf{K}}_{\widetilde{p}u} \, .
@f]
Note that due to the choice of $\widetilde{p}$ and $\widetilde{J}$ as discontinuous at the element level, all matrices that need to be inverted are defined at the element level.

The procedure to construct the various contributions is as follows:
- Construct $\mathbf{\mathsf{K}}$.
- Form $\mathbf{\mathsf{K}}_{\widetilde{p}\widetilde{J}}^{-1}$ for element and store where $\mathbf{\mathsf{K}}_{\widetilde{p}\widetilde{J}}$ was stored in $\mathbf{\mathsf{K}}$.
- Form $\overline{\overline{\mathbf{\mathsf{K}}}}$ and add to $\mathbf{\mathsf{K}}_{uu}$ to get $\mathbf{\mathsf{K}}_{\textrm{con}}$
- The modified system matrix is called ${\mathbf{\mathsf{K}}}_{\textrm{store}}$.
  That is
  @f[
        \mathbf{\mathsf{K}}_{\textrm{store}}
\dealcoloneq
        \begin{bmatrix}
			\mathbf{\mathsf{K}}_{\textrm{con}}	&	\mathbf{\mathsf{K}}_{u\widetilde{p}}	& \mathbf{0}
			\\
			\mathbf{\mathsf{K}}_{\widetilde{p}u}	&	\mathbf{0}	&	\mathbf{\mathsf{K}}_{\widetilde{p}\widetilde{J}}^{-1}
			\\
			\mathbf{0}	& 	\mathbf{\mathsf{K}}_{\widetilde{J}\widetilde{p}}		& \mathbf{\mathsf{K}}_{\widetilde{J}\widetilde{J}}
		\end{bmatrix} \, .
  @f]


<h3> The material class </h3>

A good object-oriented design of a Material class would facilitate the extension of this tutorial to a wide range of material types.
In this tutorial we simply have one Material class named Material_Compressible_Neo_Hook_Three_Field.
Ideally this class would derive from a class HyperelasticMaterial which would derive from the base class Material.
The three-field nature of the formulation used here also complicates the matter.

The Helmholtz free energy function for the three field formulation is $\Psi = \Psi_\text{vol}(\widetilde{J}) + \Psi_\text{iso}(\overline{\mathbf{b}})$.
The isochoric part of the Kirchhoff stress ${\boldsymbol{\tau}}_{\text{iso}}(\overline{\mathbf{b}})$ is identical to that obtained using a one-field formulation for a hyperelastic material.
However, the volumetric part of the free energy is now a function of the primary variable $\widetilde{J}$.
Thus, for a three field formulation the constitutive response for the volumetric part of the Kirchhoff stress ${\boldsymbol{\tau}}_{\text{vol}}$ (and the tangent) is not given by the hyperelastic constitutive law as in a one-field formulation.
One can label the term
$\boldsymbol{\tau}_{\textrm{vol}} \equiv \widetilde{p} J \mathbf{I}$
as the volumetric Kirchhoff stress, but the pressure $\widetilde{p}$ is not derived from the free energy; it is a primary field.

In order to have a flexible approach, it was decided that the Material_Compressible_Neo_Hook_Three_Field would still be able to calculate and return a volumetric Kirchhoff stress and tangent.
In order to do this, we choose to store the interpolated primary fields $\widetilde{p}$ and $\widetilde{J}$ in the Material_Compressible_Neo_Hook_Three_Field class associated with the quadrature point.
This decision should be revisited at a later stage when the tutorial is extended to account for other materials.


<h3> Numerical example </h3>

The numerical example considered here is a nearly-incompressible block under compression.
This benchmark problem is taken from
- S. Reese, P. Wriggers, B.D. Reddy (2000),
  A new locking-free brick element technique for large deformation problems in elasticity,
  <em> Computers and Structures </em>,
  <strong> 75 </strong>,
  291-304.
  DOI: <a href="http://doi.org/10.1016/S0045-7949(99)00137-6">10.1016/S0045-7949(99)00137-6</a>

 <img src="https://www.dealii.org/images/steps/developer/step-44.setup.png" alt="">

The material is quasi-incompressible neo-Hookean with <a href="http://en.wikipedia.org/wiki/Shear_modulus">shear modulus</a> $\mu = 80.194e6$ and $\nu = 0.4999$.
For such a choice of material properties a conventional single-field $Q_1$ approach would lock.
That is, the response would be overly stiff.
The initial and final configurations are shown in the image above.
Using symmetry, we solve for only one quarter of the geometry (i.e. a cube with dimension $0.001$).
The inner-quarter of the upper surface of the domain is subject to a load of $p_0$.


examples/step-44/doc/results.dox
<h1>Results</h1>

Firstly, we present a comparison of a series of 3-d results with those
in the literature (see Reese et al (2000)) to demonstrate that the program works as expected.

We begin with a comparison of the convergence with mesh refinement for the $Q_1-DGPM_0-DGPM_0$ and
$Q_2-DGPM_1-DGPM_1$ formulations, as summarised in the figure below.
The vertical displacement of the midpoint of the upper surface of the block is used to assess convergence.
Both schemes demonstrate good convergence properties for varying values of the load parameter $p/p_0$.
The results agree with those in the literature.
The lower-order formulation typically overestimates the displacement for low levels of refinement,
while the higher-order interpolation scheme underestimates it, but be a lesser degree.
This benchmark, and a series of others not shown here, give us confidence that the code is working
as it should.

<table align="center" class="tutorial" cellspacing="3" cellpadding="3">
  <tr>
     <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-44.Q1-P0_convergence.png" alt="">
	<p align="center">
        Convergence of the $Q_1-DGPM_0-DGPM_0$ formulation in 3-d.
	</p>
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-44.Q2-P1_convergence.png" alt="">
	<p align="center">
        Convergence of the $Q_2-DGPM_1-DGPM_1$ formulation in 3-d.
	</p>
    </td>
  </tr>
</table>


A typical screen output generated by running the problem is shown below.
The particular case demonstrated is that of the $Q_2-DGPM_1-DGPM_1$ formulation.
It is clear that, using the Newton-Raphson method, quadratic convergence of the solution is obtained.
Solution convergence is achieved within 5 Newton increments for all time-steps.
The converged displacement's $L_2$-norm is several orders of magnitude less than the geometry scale.

@code
Grid:
	 Reference volume: 1e-09
Triangulation:
	 Number of active cells: 64
	 Number of degrees of freedom: 2699
    Setting up quadrature point data...

Timestep 1 @ 0.1s
___________________________________________________________________________________________________________________________________________________________
                 SOLVER STEP                   |  LIN_IT   LIN_RES    RES_NORM     RES_U     RES_P      RES_J     NU_NORM      NU_U       NU_P       NU_J
___________________________________________________________________________________________________________________________________________________________
  0  ASM_R  ASM_K  CST  ASM_SC  SLV  PP  UQPH  |     786  2.118e-06  1.000e+00  1.000e+00  0.000e+00  0.000e+00  1.000e+00  1.000e+00  1.000e+00  1.000e+00
  1  ASM_R  ASM_K  CST  ASM_SC  SLV  PP  UQPH  |     552  1.031e-03  8.563e-02  8.563e-02  9.200e-13  3.929e-08  1.060e-01  3.816e-02  1.060e-01  1.060e-01
  2  ASM_R  ASM_K  CST  ASM_SC  SLV  PP  UQPH  |     667  5.602e-06  2.482e-03  2.482e-03  3.373e-15  2.982e-10  2.936e-03  2.053e-04  2.936e-03  2.936e-03
  3  ASM_R  ASM_K  CST  ASM_SC  SLV  PP  UQPH  |     856  6.469e-10  2.129e-06  2.129e-06  2.245e-19  1.244e-13  1.887e-06  7.289e-07  1.887e-06  1.887e-06
  4  ASM_R  CONVERGED!
___________________________________________________________________________________________________________________________________________________________
Relative errors:
Displacement:	7.289e-07
Force: 		2.451e-10
Dilatation:	1.353e-07
v / V_0:	1.000e-09 / 1.000e-09 = 1.000e+00


[...]

Timestep 10 @ 1.000e+00s
___________________________________________________________________________________________________________________________________________________________
                 SOLVER STEP                   |  LIN_IT   LIN_RES    RES_NORM     RES_U     RES_P      RES_J     NU_NORM      NU_U       NU_P       NU_J
___________________________________________________________________________________________________________________________________________________________
  0  ASM_R  ASM_K  CST  ASM_SC  SLV  PP  UQPH  |     874  2.358e-06  1.000e+00  1.000e+00  1.000e+00  1.000e+00  1.000e+00  1.000e+00  1.000e+00  1.000e+00
  1  ASM_R  ASM_K  CST  ASM_SC  SLV  PP  UQPH  |     658  2.942e-04  1.544e-01  1.544e-01  1.208e+13  1.855e+06  6.014e-02  7.398e-02  6.014e-02  6.014e-02
  2  ASM_R  ASM_K  CST  ASM_SC  SLV  PP  UQPH  |     790  2.206e-06  2.908e-03  2.908e-03  7.302e+10  2.067e+03  2.716e-03  1.433e-03  2.716e-03  2.717e-03
  3  ASM_R  ASM_K  CST  ASM_SC  SLV  PP  UQPH  |     893  2.374e-09  1.919e-06  1.919e-06  4.527e+07  4.100e+00  1.672e-06  6.842e-07  1.672e-06  1.672e-06
  4  ASM_R  CONVERGED!
___________________________________________________________________________________________________________________________________________________________
Relative errors:
Displacement:	6.842e-07
Force: 		8.995e-10
Dilatation:	1.528e-06
v / V_0:	1.000e-09 / 1.000e-09 = 1.000e+00
@endcode



Using the Timer class, we can discern which parts of the code require the highest computational expense.
For a case with a large number of degrees-of-freedom (i.e. a high level of refinement), a typical output of the Timer is given below.
Much of the code in the tutorial has been developed based on the optimizations described,
discussed and demonstrated in Step-18 and others.
With over 93% of the time being spent in the linear solver, it is obvious that it may be necessary
to invest in a better solver for large three-dimensional problems.
The SSOR preconditioner is not multithreaded but is effective for this class of solid problems.
It may be beneficial to investigate the use of another solver such as those available through the Trilinos library.


@code
+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    | 9.874e+02s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble system right-hand side |        53 | 1.727e+00s |  1.75e-01% |
| Assemble tangent matrix         |        43 | 2.707e+01s |  2.74e+00% |
| Linear solver                   |        43 | 9.248e+02s |  9.37e+01% |
| Linear solver postprocessing    |        43 | 2.743e-02s |  2.78e-03% |
| Perform static condensation     |        43 | 1.437e+01s |  1.46e+00% |
| Setup system                    |         1 | 3.897e-01s |  3.95e-02% |
| Update QPH data                 |        43 | 5.770e-01s |  5.84e-02% |
+---------------------------------+-----------+------------+------------+
@endcode


We then used ParaView to visualize the results for two cases.
The first was for the coarsest grid and the lowest-order interpolation method: $Q_1-DGPM_0-DGPM_0$.
The second was on a refined grid using a $Q_2-DGPM_1-DGPM_1$ formulation.
The vertical component of the displacement, the pressure $\widetilde{p}$ and the dilatation $\widetilde{J}$ fields
are shown below.


For the first case it is clear that the coarse spatial discretization coupled with large displacements leads to a low quality solution
(the loading ratio is  $p/p_0=80$).
Additionally, the pressure difference between elements is very large.
The constant pressure field on the element means that the large pressure gradient is not captured.
However, it should be noted that locking, which would be present in a standard $Q_1$ displacement formulation does not arise
even in this poorly discretised case.
The final vertical displacement of the tracked node on the top surface of the block is still within 12.5% of the converged solution.
The pressure solution is very coarse and has large jumps between adjacent cells.
It is clear that the volume nearest to the applied traction undergoes compression while the outer extents
of the domain are in a state of expansion.
The dilatation solution field and pressure field are clearly linked,
with positive dilatation indicating regions of positive pressure and negative showing regions placed in compression.
As discussed in the Introduction, a compressive pressure has a negative sign
while an expansive pressure takes a positive sign.
This stems from the definition of the volumetric strain energy function
and is opposite to the physically realistic interpretation of pressure.


<table align="center" class="tutorial" cellspacing="3" cellpadding="3">
  <tr>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-44.Q1-P0_gr_1_p_ratio_80-displacement.png" alt="">
	<p align="center">
        Z-displacement solution for the 3-d problem.
	</p>
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-44.Q1-P0_gr_1_p_ratio_80-pressure.png" alt="">
	<p align="center">
        Discontinuous piece-wise constant pressure field.
	</p>
    </td>
     <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-44.Q1-P0_gr_1_p_ratio_80-dilatation.png" alt="">
	<p align="center">
        Discontinuous piece-wise constant dilatation field.
	</p>
    </td>
  </tr>
</table>

Combining spatial refinement and a higher-order interpolation scheme results in a high-quality solution.
Three grid refinements coupled with a $Q_2-DGPM_1-DGPM_1$ formulation produces
a result that clearly captures the mechanics of the problem.
The deformation of the traction surface is well resolved.
We can now observe the actual extent of the applied traction, with the maximum force being applied
at the central point of the surface causing the largest compression.
Even though very high strains are experienced in the domain,
especially at the boundary of the region of applied traction,
the solution remains accurate.
The pressure field is captured in far greater detail than before.
There is a clear distinction and transition between regions of compression and expansion,
and the linear approximation of the pressure field allows a refined visualization
of the pressure at the sub-element scale.
It should however be noted that the pressure field remains discontinuous
and could be smoothed on a continuous grid for the post-processing purposes.



<table align="center" class="tutorial" cellspacing="3" cellpadding="3">
  <tr>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-44.Q2-P1_gr_3_p_ratio_80-displacement.png" alt="">
	<p align="center">
        Z-displacement solution for the 3-d problem.
	</p>
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-44.Q2-P1_gr_3_p_ratio_80-pressure.png" alt="">
	<p align="center">
        Discontinuous linear pressure field.
	</p>
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-44.Q2-P1_gr_3_p_ratio_80-dilatation.png" alt="">
	<p align="center">
        Discontinuous linear dilatation field.
	</p>
    </td>
  </tr>
</table>

This brief analysis of the results demonstrates that the three-field formulation is effective
in circumventing volumetric locking for highly-incompressible media.
The mixed formulation is able to accurately simulate the displacement of a
near-incompressible block under compression.
The command-line output indicates that the volumetric change under extreme compression resulted in
less than 0.01% volume change for a Poisson's ratio of 0.4999.

In terms of run-time, the $Q_2-DGPM_1-DGPM_1$ formulation tends to be more computationally expensive
than the $Q_1-DGPM_0-DGPM_0$ for a similar number of degrees-of-freedom
(produced by adding an extra grid refinement level for the lower-order interpolation).
This is shown in the graph below for a batch of tests run consecutively on a single 4-core (8-thread) machine.
The increase in computational time for the higher-order method is likely due to
the increased band-width required for the higher-order elements.
As previously mentioned, the use of a better solver and preconditioner may mitigate the
expense of using a higher-order formulation.
It was observed that for the given problem using the multithreaded Jacobi preconditioner can reduce the
computational runtime by up to 72% (for the worst case being a higher-order formulation with a large number
of degrees-of-freedom) in comparison to the single-thread SSOR preconditioner.
However, it is the author's experience that the Jacobi method of preconditioning may not be suitable for
some finite-strain problems involving alternative constitutive models.


<table align="center" class="tutorial" cellspacing="3" cellpadding="3">
  <tr>
     <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-44.Normalised_runtime.png" alt="">
	<p align="center">
        Runtime on a 4-core machine, normalised against the lowest grid resolution $Q_1-DGPM_0-DGPM_0$ solution that utilised a SSOR preconditioner.
	</p>
    </td>
  </tr>
</table>


Lastly, results for the displacement solution for the 2-d problem are showcased below for
two different levels of grid refinement.
It is clear that due to the extra constraints imposed by simulating in 2-d that the resulting
displacement field, although qualitatively similar, is different to that of the 3-d case.


<table align="center" class="tutorial" cellspacing="3" cellpadding="3">
  <tr>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-44.2d-gr_2.png" alt="">
	<p align="center">
        Y-displacement solution in 2-d for 2 global grid refinement levels.
	</p>
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-44.2d-gr_5.png" alt="">
	<p align="center">
        Y-displacement solution in 2-d for 5 global grid refinement levels.
	</p>
    </td>
  </tr>
</table>

<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

There are a number of obvious extensions for this work:

- Firstly, an additional constraint could be added to the free-energy
  function in order to enforce a high degree of incompressibility in
  materials. An additional Lagrange multiplier would be introduced,
  but this could most easily be dealt with using the principle of
  augmented Lagrange multipliers. This is demonstrated in <em>Simo and
  Taylor (1991) </em>.
- The constitutive relationship used in this
  model is relatively basic. It may be beneficial to split the material
  class into two separate classes, one dealing with the volumetric
  response and the other the isochoric response, and produce a generic
  materials class (i.e. having abstract virtual functions that derived
  classes have to implement) that would allow for the addition of more complex
  material models. Such models could include other hyperelastic
  materials, plasticity and viscoelastic materials and others.
- The program has been developed for solving problems on single-node
  multicore machines. With a little effort, the program could be
  extended to a large-scale computing environment through the use of
  Petsc or Trilinos, using a similar technique to that demonstrated in
  step-40. This would mostly involve changes to the setup, assembly,
  <code>PointHistory</code> and linear solver routines.
- As this program assumes quasi-static equilibrium, extensions to
  include dynamic effects would be necessary to study problems where
  inertial effects are important, e.g. problems involving impact.
- Load and solution limiting procedures may be necessary for highly
  nonlinear problems. It is possible to add a linesearch algorithm to
  limit the step size within a Newton increment to ensure optimum
  convergence. It may also be necessary to use a load limiting method,
  such as the Riks method, to solve unstable problems involving
  geometric instability such as buckling and snap-through.
- Many physical problems involve contact. It is possible to include
  the effect of frictional or frictionless contact between objects
  into this program. This would involve the addition of an extra term
  in the free-energy functional and therefore an addition to the
  assembly routine. One would also need to manage the contact problem
  (detection and stress calculations) itself. An alternative to
  additional penalty terms in the free-energy functional would be to
  use active set methods such as the one used in step-41.
- The complete condensation procedure using LinearOperators has been
  coded into the linear solver routine. This could also have been
  achieved through the application of the schur_complement()
  operator to condense out one or more of the fields in a more
  automated manner.
- Finally, adaptive mesh refinement, as demonstrated in step-6 and
  step-18, could provide additional solution accuracy.


examples/step-45/doc/intro.dox
<br>

<i>This program was contributed by Daniel Arndt and Matthias Maier.</i>
<a name="Intro"></a>
<h1>Introduction</h1>

In this example we present how to use periodic boundary conditions in
deal.II. Periodic boundary conditions are algebraic constraints that
typically occur in computations on representative regions of a larger
domain that repeat in one or more directions.

An example is the simulation of the electronic structure of photonic
crystals, because they have a lattice-like structure and, thus, it often
suffices to do the actual computation on only one box of the lattice. To
be able to proceed this way one has to assume that the model can be
periodically extended to the other boxes; this requires the solution to
have a periodic structure.

<a name="Procedure"></a>
<h1>Procedure</h1>

deal.II provides a number of high level entry points to impose periodic
boundary conditions.
The general approach to apply periodic boundary conditions consists of
three steps (see also the
@ref GlossPeriodicConstraints "Glossary entry on periodic boundary conditions"):
-# Create a mesh
-# Identify those pairs of faces on different parts of the boundary across which
   the solution should be symmetric, using GridTools::collect_periodic_faces()
-# Add the periodicity information to the mesh
   using parallel::distributed::Triangulation::add_periodicity()
-# Add periodicity constraints using DoFTools::make_periodicity_constraints()

The second and third step are necessary for parallel meshes using the
parallel::distributed::Triangulation class
to ensure that cells on opposite sides of the domain but connected by periodic
faces are part of the ghost layer if one of them is stored on the local processor.
If the Triangulation is not a parallel::distributed::Triangulation,
these steps are not necessary.

The first step consists of collecting matching periodic faces and storing them in
a <code>std::vector</code> of GridTools::PeriodicFacePair. This is done with the
function GridTools::collect_periodic_faces() that can be invoked for example
like this:
@code
GridTools::collect_periodic_faces(dof_handler,
                                  b_id1,
                                  b_id2,
                                  direction,
                                  matched_pairs,
                                  offset = <default value>,
                                  matrix = <default value>,
                                  first_vector_components = <default value>);
@endcode

This call loops over all faces of the container dof_handler on the periodic
boundaries with boundary indicator @p b_id1 and @p b_id2,
respectively. (You can assign these boundary indicators by hand after
creating the coarse mesh, see
@ref GlossBoundaryIndicator "Boundary indicator". Alternatively, you
can also let many of the functions in namespace GridGenerator do this
for if you specify the "colorize" flag; in that case, these functions
will assign different boundary indicators to different parts of the
boundary, with the details typically spelled out in the documentation
of these functions.)

Concretely, if $\text{vertices}_{1/2}$ are the vertices of two faces
$\text{face}_{1/2}$, then the function call above will match pairs of
faces (and dofs) such that the difference between $\text{vertices}_2$
and $matrix\cdot \text{vertices}_1+\text{offset}$ vanishes in every
component apart from direction and stores the resulting pairs with
associated data in @p matched_pairs. (See
GridTools::orthogonal_equality() for detailed information about the
matching process.)

Consider, for example, the colored unit square $\Omega=[0,1]^2$ with boundary
indicator 0 on the left, 1 on the right, 2 on the bottom and 3 on the top
faces. (See the documentation of GridGenerator::hyper_cube() for this
convention on how boundary indicators are assigned.) Then,
@code
GridTools::collect_periodic_faces(dof_handler,
                                  /*b_id1*/ 0,
                                  /*b_id2*/ 1,
                                  /*direction*/ 0,
                                  matched_pairs);
@endcode
would yield periodicity constraints such that $u(0,y)=u(1,y)$ for all
$y\in[0,1]$.

If we instead consider the parallelogram given by the convex hull of
$(0,0)$, $(1,1)$, $(1,2)$, $(0,1)$ we can achieve the constraints
$u(0,y)=u(1,y+1)$ by specifying an @p offset:
@code
GridTools::collect_periodic_faces(dof_handler,
                                  /*b_id1*/ 0,
                                  /*b_id2*/ 1,
                                  /*direction*/ 0,
                                  matched_pairs,
                                  Tensor<1, 2>(0.,1.));
@endcode
or
@code
GridTools::collect_periodic_faces(dof_handler,
                                  /*b_id1*/ 0,
                                  /*b_id2*/ 1,
                                  /*arbitrary direction*/ 0,
                                  matched_pairs,
                                  Tensor<1, 2>(1.,1.));
@endcode
Here, again, the assignment of boundary indicators 0 and 1 stems from
what GridGenerator::parallelogram() documents.

The resulting @p matched_pairs can be used in
DoFTools::make_periodicity_constraints for populating an AffineConstraints
object with periodicity constraints:
@code
DoFTools::make_periodicity_constraints(matched_pairs, constraints);
@endcode

Apart from this high level interface there are also variants of
DoFTools::make_periodicity_constraints available that combine those two
steps (see the variants of DofTools::make_periodicity_constraints).

There is also a low level interface to
DoFTools::make_periodicity_constraints if more flexibility is needed. The
low level variant allows to directly specify two faces that shall be
constrained:
@code
using namespace DoFTools;
make_periodicity_constraints(face_1,
                             face_2,
                             affine_constraints,
                             component_mask = <default value>;
                             face_orientation = <default value>,
                             face_flip = <default value>,
                             face_rotation = <default value>,
                             matrix = <default value>);
@endcode
Here, we need to specify the orientation of the two faces using
@p face_orientation, @p face_flip and @p face_orientation. For a closer description
have a look at the documentation of DoFTools::make_periodicity_constraints.
The remaining parameters are the same as for the high level interface apart
from the self-explaining @p component_mask and @p affine_constraints.


<a name="problem"></a>
<h1>A practical example</h1>

In the following, we show how to use the above functions in a more involved
example. The task is to enforce rotated periodicity constraints for the
velocity component of a Stokes flow.

On a quarter-circle defined by $\Omega=\{{\bf x}\in(0,1)^2:\|{\bf x}\|\in (0.5,1)\}$ we are
going to solve the Stokes problem
@f{eqnarray*}
  -\Delta \; \textbf{u} + \nabla p &=& (\exp(-100\|{\bf x}-(.75,0.1)^T\|^2),0)^T, \\
  -\textrm{div}\;  \textbf{u}&=&0,\\
  \textbf{u}|_{\Gamma_1}&=&{\bf 0},
@f}
where the boundary $\Gamma_1$ is defined as $\Gamma_1 \dealcoloneq \{x\in \partial\Omega: \|x\|\in\{0.5,1\}\}$.
For the remaining parts of the boundary we are going to use periodic boundary conditions, i.e.
@f{align*}
  u_x(0,\nu)&=-u_y(\nu,0)&\nu&\in[0,1]\\
  u_y(0,\nu)&=u_x(\nu,0)&\nu&\in[0,1].
@f}

The mesh will be generated by GridGenerator::quarter_hyper_shell(),
which also documents how it assigns boundary indicators to its various
boundaries if its `colorize` argument is set to `true`.


examples/step-45/doc/results.dox
<h1>Results</h1>

The created output is not very surprising. We simply see that the solution is
periodic with respect to the left and lower boundary:

<img src="https://www.dealii.org/images/steps/developer/step-45.periodic.png" alt="">

Without the periodicity constraints we would have ended up with the following solution:

<img src="https://www.dealii.org/images/steps/developer/step-45.non_periodic.png" alt="">


examples/step-46/doc/intro.dox
<br>

<i>This program was contributed by Wolfgang Bangerth.
<br>
This material is based upon work partly supported by the National Science
Foundation under Award No. EAR-0949446 and The University of California
&ndash; Davis. Any opinions, findings, and conclusions or recommendations
expressed in this publication are those of the author and do not necessarily
reflect the views of the National Science Foundation or of The University of
California &ndash; Davis.  </i>


<a name="Intro"></a>
<h1>Introduction</h1>

This program deals with the problem of coupling different physics in different
parts of the domain. Specifically, let us consider the following
situation that couples a Stokes fluid with an elastic solid (these two
problems were previously discussed separately in step-22 and step-8,
where you may want to read up on the individual equations):

- In a part $\Omega_f$ of $\Omega$, we have a fluid flowing that satisfies the
  time independent Stokes equations (in the form that involves the strain
  tensor):
  @f{align*}
    -2\eta\nabla \cdot \varepsilon(\mathbf v) + \nabla p &= 0,
          \qquad \qquad && \text{in}\ \Omega_f\\
    -\nabla \cdot \mathbf v &= 0  && \text{in}\ \Omega_f.
  @f}
  Here, $\mathbf v, p$ are the fluid velocity and pressure, respectively.
  We prescribe the velocity on part of the external boundary,
  @f{align*}
    \mathbf v = \mathbf v_0 \qquad\qquad
     \text{on}\ \Gamma_{f,1} \subset \partial\Omega \cap \partial\Omega_f
  @f}
  while we assume free-flow conditions on the remainder of the external
  boundary,
  @f{align*}
    (2\eta \varepsilon(\mathbf v) - p \mathbf 1) \cdot \mathbf n = 0
     \qquad\qquad
     \text{on}\ \Gamma_{f,2} = \partial\Omega \cap \partial\Omega_f \backslash
     \Gamma_{f,1}.
  @f}
- The remainder of the domain, $\Omega_s = \Omega \backslash \Omega_f$ is
  occupied by a solid whose deformation field $\mathbf u$ satisfies the
  elasticity equation,
  @f{align*}
    -\nabla \cdot C \varepsilon(\mathbf u) = 0 \qquad\qquad
    & \text{in}\ \Omega_s,
  @f}
  where $C$ is the rank-4 elasticity tensor (for which we will use a
  particularly simple form by assuming that the solid is isotropic).
  It deforms in reaction to the forces exerted by the
  fluid flowing along the boundary of the solid. We assume this deformation to
  be so small that it has no feedback effect on the fluid, i.e. the coupling
  is only in one direction. For simplicity, we will assume that the
  solid's external boundary is clamped, i.e.
  @f{align*}
    \mathbf u = \mathbf 0 \qquad\qquad
     \text{on}\ \Gamma_{s,1} = \partial\Omega \cap \partial\Omega_s
  @f}
- As a consequence of the small displacement assumption, we will pose the
  following boundary conditions on the interface between the fluid and solid:
  first, we have no slip boundary conditions for the fluid,
  @f{align*}
    \mathbf v = \mathbf 0 \qquad\qquad
     \text{on}\ \Gamma_{i} = \partial\Omega_s \cap \partial\Omega_f.
  @f}
  Secondly, the forces (traction) on the solid equal the normal stress from the fluid,
  @f{align*}
    (C \varepsilon(\mathbf u)) \mathbf n =
    (2 \eta \varepsilon(\mathbf v) - p \mathbf 1) \mathbf n \qquad\qquad
     \text{on}\ \Gamma_{i} = \partial\Omega_s \cap \partial\Omega_f,
  @f}
  where $\mathbf{n}$ is the normal vector on $\Gamma_{i}$ pointing from
  the solid to the fluid.

We get a weak formulation of this problem by following our usual rule of
multiplying from the left by a test function and integrating over the
domain. It then looks like this: Find $y = \{\mathbf v, p,
\mathbf u\} \in Y \subset H^1(\Omega_f)^d \times L_2(\Omega_f) \times
H^1(\Omega_s)^d$ such that
@f{align*}
	2 \eta (\varepsilon(\mathbf a), \varepsilon(\mathbf v))_{\Omega_f}
	- (\nabla \cdot \mathbf a, p)_{\Omega_f}
	- (q, \nabla \cdot \mathbf v)_{\Omega_f} &
	\\
	+ (\varepsilon(\mathbf b), C \varepsilon(\mathbf u))_{\Omega_s} &
	\\
	- (\mathbf b,
           (2 \eta \varepsilon(\mathbf v) - p \mathbf 1) \mathbf n)_{\Gamma_i}
	&=
	0,
@f}
for all test functions $\mathbf a, q, \mathbf b$; the first, second, and
third lines correspond to the fluid, solid, and interface
contributions, respectively.
Note that $Y$ is only a subspace of the spaces listed above to accommodate for
the various Dirichlet boundary conditions.

This sort of coupling is of course possible by simply having two Triangulation
and two DoFHandler objects, one each for each of the two subdomains. On the
other hand, deal.II is much simpler to use if there is a single DoFHandler
object that knows about the discretization of the entire problem.

This program is about how this can be achieved. Note that the goal is not to
present a particularly useful physical model (a realistic fluid-structure
interaction model would have to take into account the finite deformation of
the solid and the effect this has on the fluid): this is, after all, just a
tutorial program intended to demonstrate techniques, not to solve actual
problems. Furthermore, we will make the assumption that the interface between
the subdomains is aligned with coarse mesh cell faces.


<h3>The general idea</h3>

Before going into more details let us state the obvious: this is a
problem with multiple solution variables; for this, you will probably
want to read the @ref vector_valued documentation module first, which
presents the basic philosophical framework in which we address
problems with more than one solution variable. But back to the problem
at hand:

The fundamental idea to implement these sort of problems in deal.II goes as
follows: in the problem formulation, the velocity and pressure variables
$\mathbf v, p$ only live in the fluid subdomain $\Omega_f$. But let's assume
that we extend them by zero to the entire domain $\Omega$ (in the general case
this means that they will be discontinuous along $\Gamma_i$). So what is the
appropriate function space for these variables? We know that on $\Omega_f$ we
should require $\mathbf v \in H^1(\Omega_f)^d, p \in L_2(\Omega_f)$, so for
the extensions $\tilde{\mathbf v}, \tilde p$ to the whole domain the following
appears a useful set of function spaces:
@f{align*}
  \tilde {\mathbf v} &\in V
   = \{\tilde {\mathbf v}|_{\Omega_f} \in H^1(\Omega_f)^d, \quad
       \tilde {\mathbf v}|_{\Omega_s} = 0 \}
  \\
  \tilde p &\in P
  = \{\tilde p|_{\Omega_f} \in L_2(\Omega_f), \quad
       \tilde p|_{\Omega_s} = 0 \}.
@f}
(Since this is not important for the current discussion, we have omitted the
question of boundary values from the choice of function spaces; this question
also affects whether we can choose $L_2$ for the pressure or whether we have
to choose the space $L_{2,0}(\Omega_f)=\{q\in L_2(\Omega_f): \int_{\Omega_f} q
= 0\}$ for the pressure. None of these questions are relevant to the following
discussion, however.)

Note that these are indeed a linear function spaces with obvious norm. Since no
confusion is possible in practice, we will henceforth omit the tilde again to
denote the extension of a function to the whole domain and simply refer by
$\mathbf v, p$ to both the original and the extended function.

For discretization, we need finite dimensional subspaces $V_h,P_h$ of
$V, P$. For Stokes, we know from step-22 that an appropriate choice is
$Q_{p+1}^d\times Q_P$ but this only holds for that part of the domain
occupied by the fluid. For the extended field, let's use the following
subspaces defined on the triangulation $\mathbb T$:
@f{align*}
  V_h
   &= \{{\mathbf v}_h \quad | \quad
       \forall K \in {\mathbb T}:
       {\mathbf v}_h|_K \in Q_{p+1}^d\  \text{if}\ K\subset {\Omega_f}, \quad
       {\mathbf v}_h|_{\Omega_f}\ \text{is continuous}, \quad
       {\mathbf v}_h|_K = 0\ \text{if}\ K\subset {\Omega_s}\}
   && \subset V
  \\
  P_h
  &= \{ p_h \quad | \quad
       \forall K \in {\mathbb T}:
       p_h|_K \in Q_p\  \text{if}\ K\subset {\Omega_f}, \quad
       p_h|_{\Omega_f}\ \text{is continuous}, \quad
       p_h|_K = 0\ \text{if}\ K\subset {\Omega_s}\ \}
   && \subset P.
@f}
In other words, on $\Omega_f$ we choose the usual discrete spaces but
we keep the (discontinuous) extension by zero. The point to make is
that we now need a description of a finite element space for functions
that are zero on a cell &mdash; and this is where the FE_Nothing class
comes in: it describes a finite dimensional function space of
functions that are constant zero. A particular property of this
peculiar linear vector space is that it has no degrees of freedom: it
isn't just finite dimensional, it is in fact zero dimensional, and
consequently for objects of this type, FiniteElement::n_dofs_per_cell()
will return zero. For discussion below, let us give this space a
proper symbol:
@f[
  Z = \{ \varphi: \varphi(x)=0 \}.
@f]
The symbol $Z$ reminds of the fact that functions in this space are
zero. Obviously, we choose $Z_h=Z$.

This entire discussion above can be repeated for the variables we use to
describe the elasticity equation. Here, for the extended variables, we
have
@f{align*}
  \tilde {\mathbf u} &\in U
   = \{\tilde {\mathbf u}|_{\Omega_s} \in H^1(\Omega_f)^d, \quad
       \tilde {\mathbf u}|_{\Omega_f} \in Z(\Omega_s)^d \},
@f}
and we will typically use a finite element space of the kind
@f{align*}
  U_h
   &= \{{\mathbf u}_h \quad | \quad
       \forall K \in {\mathbb T}:
       {\mathbf u}_h|_K \in Q_r^d\  \text{if}\ K\subset {\Omega_s}, \quad
       {\mathbf u}_h|_{\Omega_f}\ \text{is continuous}, \quad
       {\mathbf u}_h|_K \in Z^d\ \text{if}\ K\subset {\Omega_f}\}
   && \subset U
@f}
of polynomial degree $r$.

So to sum up, we are going to look for a discrete vector-valued
solution $y_h = \{\mathbf v_h, p_h, \mathbf u_h\}$ in the following
space:
@f{align*}
  Y_h = \{
      & y_h = \{\mathbf v_h, p_h, \mathbf u_h\} : \\
      & y_h|_{\Omega_f} \in Q_{p+1}^d \times Q_p \times Z^d, \\
      & y_h|_{\Omega_s} \in Z^d \times Z \times Q_r^d \}.
@f}



<h3>Implementation</h3>

So how do we implement this sort of thing? First, we realize that the discrete
space $Y_h$ essentially calls for two different finite elements: First, on the
fluid subdomain, we need the element $Q_{p+1}^d \times Q_p \times Z^d$ which
in deal.II is readily implemented by
@code
  FESystem<dim> (FE_Q<dim>(p+1), dim,
		 FE_Q<dim>(p), 1,
		 FE_Nothing<dim>(), dim),
@endcode
where <code>FE_Nothing</code> implements the space of functions that are
always zero. Second, on the solid subdomain, we need the element
$\in Z^d \times Z \times Q_r^d$, which we get using
@code
  FESystem<dim> (FE_Nothing<dim>(), dim,
		 FE_Nothing<dim>(), 1,
		 FE_Q<dim>(r), dim),
@endcode

The next step is that we associate each of these two elements with the cells
that occupy each of the two subdomains. For this we realize that in a sense
the two elements are just variations of each other in that they have the same
number of vector components but have different polynomial degrees &mdash; this
smells very much like what one would do in $hp$ finite element methods, and it
is exactly what we are going to do here: we are going to (ab)use the classes
and facilities of the hp-namespace to assign different elements to different
cells. In other words, we will use collect the two finite elements in an
hp::FECollection, will integrate with an appropriate hp::QCollection using an
hp::FEValues object, and our DoFHandler will be in <i>hp</i>-mode. You
may wish to take a look at step-27 for an overview of all of these concepts.

Before going on describing the testcase, let us clarify a bit <i>why</i> this
approach of extending the functions by zero to the entire domain and then
mapping the problem on to the hp-framework makes sense:

- It makes things uniform: On all cells, the number of vector components is
  the same (here, <code>2*dim+1</code>). This makes all sorts of
  things possible since a uniform description allows for code
  re-use. For example, counting degrees of freedom per vector
  component (DoFTools::count_dofs_per_fe_component), sorting degrees of
  freedom by component (DoFRenumbering::component_wise), subsequent
  partitioning of matrices and vectors into blocks and many other
  functions work as they always did without the need to add special
  logic to them that describes cases where some of the variables only
  live on parts of the domain. Consequently, you have all sorts of
  tools already available to you in programs like the current one that
  weren't originally written for the multiphysics case but work just
  fine in the current context.

- It allows for easy graphical output: All graphical output formats we support
  require that each field in the output is defined on all nodes of the
  mesh. But given that now all solution components live everywhere,
  our existing DataOut routines work as they always did, and produce
  graphical output suitable for visualization -- the fields will
  simply be extended by zero, a value that can easily be filtered out
  by visualization programs if not desired.

- There is essentially no cost: The trick with the FE_Nothing does not add any
  degrees of freedom to the overall problem, nor do we ever have to handle a
  shape function that belongs to these components &mdash; the FE_Nothing has
  no degrees of freedom, not does it have shape functions, all it does is take
  up vector components.


<h3> Specifics of the implementation </h3>

More specifically, in the program we have to address the following
points:
- Implementing the bilinear form, and in particular dealing with the
  interface term, both in the matrix and the sparsity pattern.
- Implementing Dirichlet boundary conditions on the external and
  internal parts of the boundaries
  $\partial\Omega_f,\partial\Omega_s$.


<h4>Dealing with the interface terms</h4>

Let us first discuss implementing the bilinear form, which at the
discrete level we recall to be
@f{align*}
	2 \eta (\varepsilon(\mathbf a_h), \varepsilon(\mathbf v_h))_{\Omega_f}
	- (\nabla \cdot \mathbf a_h, p_h)_{\Omega_f}
	- (q_h, \nabla \cdot \mathbf v_h)_{\Omega_f} &
	\\
	+ (\varepsilon(\mathbf b_h), C \varepsilon(\mathbf u_h))_{\Omega_s} &
	\\
	- (\mathbf b_h,
           (2 \eta \varepsilon(\mathbf v_h) - p \mathbf 1) \mathbf n)_{\Gamma_i}
	&=
	0,
@f}
Given that we have extended the fields by zero, we could in principle
write the integrals over subdomains to the entire domain $\Omega$,
though it is little additional effort to first ask whether a cell is
part of the elastic or fluid region before deciding which terms to
integrate. Actually integrating these terms is not very difficult; for
the Stokes equations, the relevant steps have been shown in step-22,
whereas for the elasticity equation we take essentially the form shown
in the @ref vector_valued module (rather than the one from step-8).

The term that is of more interest is the interface term,
@f[
	-(\mathbf b_h,
           (2 \eta \varepsilon(\mathbf v_h) - p \mathbf 1) \mathbf n)_{\Gamma_i}.
@f]
Based on our assumption that the interface $\Gamma_i$ coincides with
cell boundaries, this can in fact be written as a set of face
integrals. If we denote the velocity, pressure and displacement
components of shape function $\psi_i\in Y_h$ using the extractor
notation $\psi_i[\mathbf v],\psi_i[p], \psi_i[\mathbf u]$, then the
term above yields the following contribution to the global matrix
entry $i,j$:
@f[
	-\sum_K (\psi_i[\mathbf u],
           (2 \eta \varepsilon(\psi_j[\mathbf v]) - \psi_j[p] \mathbf 1)
	   \mathbf n)_{\partial K \cap \Gamma_i}.
@f]
Although it isn't immediately obvious, this term presents a slight
complication: while $\psi_i[\mathbf u]$ and $\mathbf n$ are evaluated
on the solid side of the interface (they are test functions for the
displacement and the normal vector to $\Omega_s$, respectively, we
need to evaluate $\psi_j[\mathbf v],\psi_j[p]$ on the fluid
side of the interface since they correspond to the stress/force
exerted by the fluid. In other words, in our implementation, we will
need FEFaceValue objects for both sides of the interface. To make
things slightly worse, we may also have to deal with the fact that one
side or the other may be refined, leaving us with the need to
integrate over parts of a face. Take a look at the implementation
below on how to deal with this.

As an additional complication, the matrix entries that result from this term
need to be added to the sparsity pattern of the matrix somehow. This is the
realm of various functions in the DoFTools namespace like
DoFTools::make_sparsity_pattern and
DoFTools::make_flux_sparsity_pattern. Essentially, what these functions do is
simulate what happens during assembly of the system matrix: whenever assembly
would write a nonzero entry into the global matrix, the functions in DoFTools
would add an entry to the sparsity pattern. We could therefore do the
following: let DoFTools::make_sparsity_pattern add all those entries to the
sparsity pattern that arise from the regular cell-by-cell integration, and
then do the same by hand that arise from the interface terms. If you look at
the implementation of the interface integrals in the program below, it should
be obvious how to do that and would require no more than maybe 100 lines of
code at most.

But we're lazy people: the interface term couples degrees of freedom from two
adjacent cells along a face, which is exactly the kind of thing one would do
in discontinuous Galerkin schemes for which the function
DoFTools::make_flux_sparsity_pattern was written. This is a superset of matrix
entries compared to the usual DoFTools::make_sparsity_pattern: it will also
add all entries that result from computing terms coupling the degrees of
freedom from both sides of all faces. Unfortunately, for the simplest version
of this function, this is a pretty big superset. Consider for example the
following mesh with two cells and a $Q_1$ finite element:
@code
  2---3---5
  |   |   |
  0---1---4
@endcode
Here, the sparsity pattern produced by DoFTools::make_sparsity_pattern will
only have entries for degrees of freedom that couple on a cell. However, it
will not have sparsity pattern entries $(0,4),(0,5),(2,4),(2,5)$. The sparsity
pattern generated by DoFTools::make_flux_sparsity_pattern will have these
entries, however: it assumes that you want to build a sparsity pattern for a
bilinear form that couples <i>all</i> degrees of freedom from adjacent
cells. This is not what we want: our interface term acts only on a small
subset of cells, and we certainly don't need all the extra couplings between
two adjacent fluid cells, or two adjacent solid cells. Furthermore, the fact that we
use higher order elements means that we would really generate many many more
entries than we actually need: on the coarsest mesh, in 2d, 44,207 nonzero
entries instead of 16,635 for DoFTools::make_sparsity_pattern, leading to
plenty of zeros in the matrix we later build (of course, the 16,635 are not
enough since they don't include the interface entries). This ratio would be
even worse in 3d.

So being extremely lazy comes with a cost: too many entries in the matrix. But
we can get away with being moderately lazy: there is a variant of
DoFTools::make_flux_sparsity_pattern that allows us
to specify which vector components of the finite element couple with which
other components, both in cell terms as well as in face terms. For cells that
are in the solid subdomain, we couple all displacements with each other; for
fluid cells, all velocities with all velocities and the pressure, but not the
pressure with itself. Since no cell has both sets of
variables, there is no need to distinguish between the two kinds of cells, so
we can write the mask like this:
@code
    Table<2,DoFTools::Coupling> cell_coupling (fe_collection.n_components(),
					       fe_collection.n_components());

    for (unsigned int c=0; c<fe_collection.n_components(); ++c)
      for (unsigned int d=0; d<fe_collection.n_components(); ++d)
	if (((c<dim+1) && (d<dim+1)
	     && !((c==dim) && (d==dim)))
	    ||
	    ((c>=dim+1) && (d>=dim+1)))
	  cell_coupling[c][d] = DoFTools::Coupling::always;
@endcode
Here, we have used the fact that the first <code>dim</code> components of the
finite element are the velocities, then the pressure, and then the
<code>dim</code> displacements. (We could as well have stated that the
velocities/pressure also couple with the displacements since no cell ever has
both sets of variables.) On the other hand, the interface terms require a mask
like this:
@code
    Table<2,DoFTools::Coupling> face_coupling (fe_collection.n_components(),
					       fe_collection.n_components());

    for (unsigned int c=0; c<fe_collection.n_components(); ++c)
      for (unsigned int d=0; d<fe_collection.n_components(); ++d)
	if ((c>=dim+1) && (d<dim+1))
	  face_coupling[c][d] = DoFTools::Coupling::always;
@endcode
In other words, all displacement test functions (components
<code>c@>=dim+1</code>) couple with all velocity and pressure shape functions
on the other side of an interface. This is not entirely true, though close: in
fact, the exact form of the interface term only those pressure displacement
shape functions that are indeed nonzero on the common interface, which is not
true for all shape functions; on the other hand, it really couples all
velocities (since the integral involves gradients of the velocity shape
functions, which are all nonzero on all faces of the cell). However, the mask we
build above, is not capable of these subtleties. Nevertheless, through these
masks we manage to get the number of sparsity pattern entries down to 21,028
&mdash; good enough for now.



<h4>Velocity boundary conditions on the interface</h4>

The second difficulty is that while we know how to enforce a zero
velocity or stress on the external boundary (using
VectorTools::interpolate_boundary_values, called with an appropriate
component mask and setting different boundary indicators for solid and
fluid external boundaries), we now also needed the velocity to be zero
on the interior interface, i.e. $\mathbf v|_{\Gamma_i}=0$. At the time
of writing this, there is no function in deal.II that handles this
part, but it isn't particularly difficult to implement by hand:
essentially, we just have to loop over all cells, and if it is a fluid
cell and its neighbor is a solid cell, then add constraints that
ensure that the velocity degrees of freedom on this face are
zero. Some care is necessary to deal with the case that the adjacent
solid cell is refined, yielding the following code:
@code
std::vector<unsigned int> local_face_dof_indices (stokes_fe.dofs_per_face);
for (const auto &cell: dof_handler.active_cell_iterators())
  if (cell_is_in_fluid_domain (cell))
    for (const auto f : cell->face_indices())
      if (!cell->at_boundary(f))
        {
          bool face_is_on_interface = false;

          if ((cell->neighbor(f)->has_children() == false)
	          &&
	          (cell_is_in_solid_domain (cell->neighbor(f))))
	        face_is_on_interface = true;
          else if (cell->neighbor(f)->has_children() == true)
	        {
              // The neighbor does have children. See if any of the cells
              // on the other side are elastic
	          for (unsigned int sf=0; sf<cell->face(f)->n_children(); ++sf)
	            if (cell_is_in_solid_domain (cell->neighbor_child_on_subface(f, sf)))
	              {
                   face_is_on_interface = true;
		            break;
	              }
	        }

          if (face_is_on_interface)
           {
             cell->face(f)->get_dof_indices (local_face_dof_indices, 0);
             for (unsigned int i=0; i<local_face_dof_indices.size(); ++i)
             if (stokes_fe.face_system_to_component_index(i).first < dim)
               constraints.add_line (local_face_dof_indices[i]);
           }
        }
@endcode

The call <code>constraints.add_line(t)</code> tells the
AffineConstraints to start a new constraint for degree of freedom
<code>t</code> of the form $x_t=\sum_{l=0}^{N-1} c_{tl} x_l +
b_t$. Typically, one would then proceed to set individual coefficients
$c_{tl}$ to nonzero values (using AffineConstraints::add_entry) or set
$b_t$ to something nonzero (using
AffineConstraints::set_inhomogeneity); doing nothing as above, funny as
it looks, simply leaves the constraint to be $x_t=0$, which is exactly
what we need in the current context. The call to
FiniteElement::face_system_to_component_index makes sure that we only set
boundary values to zero for velocity but not pressure components.

Note that there are cases where this may yield incorrect results:
notably, once we find a solid neighbor child to a current fluid cell,
we assume that all neighbor children on the common face are in the
solid subdomain. But that need not be so; consider, for example, the
following mesh:
@code
+---------+----+----+
|         | f  |    |
|    f    +----+----+
|         | s  |    |
+---------+----+----+
@endcode

In this case, we would set all velocity degrees of freedom on the
right face of the left cell to zero, which is incorrect for the top
degree of freedom on that face. That said, that can only happen if the
fluid and solid subdomains do not coincide with a set of complete
coarse mesh cells &mdash; but this is a contradiction to the
assumption stated at the end of the first section of this
introduction.



<h3>The testcase</h3>

We will consider the following situation as a testcase:

<img src="https://www.dealii.org/images/steps/developer/step-46.layout.png" alt="">

As discussed at the top of this document, we need to assume in a few places
that a cell is either entirely in the fluid or solid part of the domain and,
furthermore, that all children of an inactive cell also belong to the same
subdomain. This can definitely be ensured if the coarse mesh already
subdivides the mesh into solid and fluid coarse mesh cells; given the geometry
outlined above, we can do that by using an $8\times 8$ coarse mesh,
conveniently provided by the GridGenerator::subdivided_hyper_rectangle
function.

The fixed boundary at the bottom implies $\mathbf u=0$, and we also
prescribe Dirichlet conditions for the flow at the top so that we get
inflow at the left and outflow at the right. At the left and right
boundaries, no boundary conditions are imposed explicitly for the
flow, yielding the implicit no-stress condition $(2\eta
\varepsilon(\mathbf v) - p \mathbf 1) \cdot \mathbf n = 0$.
The conditions on the interface between the two domains has already been
discussed above.

For simplicity, we choose the material parameters to be
$\eta=\lambda=\mu=1$. In the results section below, we will also show
a 3d simulation that can be obtained from the same program. The
boundary conditions and geometry are defined nearly analogously to the
2d situation above.


<h4>Identifying which subdomain a cell is in</h4>

In the program, we need a way to identify which part of the domain a cell is
in. There are many different ways of doing this. A typical way would be to use
the @ref GlossSubdomainId "subdomain_id" tag available with each cell, though
this field has a special meaning in %parallel computations. An alternative
is the @ref GlossMaterialId "material_id" field also available with
every cell. It has the additional advantage that it is inherited from the
mother to the child cell upon mesh refinement; in other words, we would set
the material id once upon creating the mesh and it will be correct for all
active cells even after several refinement cycles. We therefore go with this
alternative: we define an <code>enum</code> with symbolic names for
material_id numbers and will use them to identify which part of the domain a
cell is on.

Secondly, we use an object of type DoFHandler operating in <i>hp</i>-mode. This
class needs to know which cells will use the Stokes and which the elasticity
finite element. At the beginning of each refinement cycle we will therefore
have to walk over all cells and set the (in hp-parlance) active FE index to
whatever is appropriate in the current situation. While we can use symbolic
names for the material id, the active FE index is in fact a number that will
frequently be used to index into collections of objects (e.g. of type
hp::FECollection and hp::QCollection); that means that the active FE index
actually has to have value zero for the fluid and one for the elastic part of
the domain.


<h4>Linear solvers</h4>

This program is primarily intended to show how to deal with different
physics in different parts of the domain, and how to implement such
models in deal.II. As a consequence, we won't bother coming up with a
good solver: we'll just use the SparseDirectUMFPACK class which always
works, even if not with optimal complexity. We will, however, comment
on possible other solvers in the <a href="#Results">results</a> section.


<h4>Mesh refinement</h4>

One of the trickier aspects of this program is how to estimate the
error. Because it works on almost any program, we'd like to use the
KellyErrorEstimator, and we can relatively easily do that here as well using
code like the following:
@code
  Vector<float> stokes_estimated_error_per_cell (triangulation.n_active_cells());
  Vector<float> elasticity_estimated_error_per_cell (triangulation.n_active_cells());

  std::vector<bool> stokes_component_mask (dim+1+dim, false);
  for (unsigned int d=0; d<dim; ++d)
    stokes_component_mask[d] = true;
  KellyErrorEstimator<dim>::estimate (dof_handler,
                                      face_q_collection,
                                      std::map<types::boundary_id, const Function<dim>*>(),
                                      solution,
                                      stokes_estimated_error_per_cell,
                                      stokes_component_mask);

  std::vector<bool> elasticity_component_mask (dim+1+dim, false);
  for (unsigned int d=0; d<dim; ++d)
    elasticity_component_mask[dim+1+d] = true;
  KellyErrorEstimator<dim>::estimate (dof_handler,
                                      face_q_collection,
                                      std::map<types::boundary_id, const Function<dim>*>(),
                                      solution,
                                      elasticity_estimated_error_per_cell,
                                      elasticity_component_mask);
@endcode
This gives us two sets of error indicators for each cell. We would then
somehow combine them into one for mesh refinement, for example using something
like the following (note that we normalize the squared error indicator in the
two vectors because error quantities have physical units that do not match in
the current situation, leading to error indicators that may differ by orders
of magnitude between the two subdomains):
@code
  stokes_estimated_error_per_cell /= stokes_estimated_error_per_cell.l2_norm();
  elasticity_estimated_error_per_cell /= elasticity_estimated_error_per_cell.l2_norm();

  Vector<float> estimated_error_per_cell (triangulation.n_active_cells());
  estimated_error_per_cell += stokes_estimated_error_per_cell;
  estimated_error_per_cell += elasticity_estimated_error_per_cell;
@endcode
(In the code, we actually weigh the error indicators 4:1 in favor of the ones
computed on the Stokes subdomain since refinement is otherwise heavily biased
towards the elastic subdomain, but this is just a technicality. The factor 4
has been determined heuristically to work reasonably well.)

While this principle is sound, it doesn't quite work as expected. The reason
is that the KellyErrorEstimator class computes error indicators by integrating
the jump in the solution's gradient around the faces of each cell. This jump
is likely to be very large at the locations where the solution is
discontinuous and extended by zero; it also doesn't become smaller as the mesh
is refined. The KellyErrorEstimator class can't just ignore the interface
because it essentially only sees a DoFHandler in <i>hp</i>-mode where the element
type changes from one cell to another &mdash; precisely the thing that the
<i>hp</i>-mode was designed for, the interface in the current program looks no
different than the interfaces in step-27, for example, and certainly no less
legitimate. Be that as it may, the end results is that there is a layer of
cells on both sides of the interface between the two subdomains where error
indicators are irrationally large. Consequently, most of the mesh refinement
is focused on the interface.

This clearly wouldn't happen if we had a refinement indicator that actually
understood something about the problem and simply ignore the interface between
subdomains when integrating jump terms.
On the other hand, this program is
about showing how to represent problems where we have different physics in
different subdomains, not about the peculiarities of the KellyErrorEstimator,
and so we resort to the big hammer called "heuristics": we simply set the
error indicators of cells at the interface to zero. This cuts off the spikes
in the error indicators. At first sight one would also think that it prevents
the mesh from being refined at the interface, but the requirement that
neighboring cells may only differ by one level of refinement will still lead
to a reasonably refined mesh.

While this is clearly a suboptimal solution, it works for now and leaves room
for future improvement.


examples/step-46/doc/results.dox
<a name="Results"></a>
<h1>Results</h1>

<h3>2d results</h3>


When running the program, you should get output like the following:
@code
Refinement cycle 0
   Number of active cells: 64
   Number of degrees of freedom: 531
   Assembling...
   Solving...
   Writing output...

Refinement cycle 1
   Number of active cells: 136
   Number of degrees of freedom: 1260
   Assembling...
   Solving...
   Writing output...

Refinement cycle 2
   Number of active cells: 436
   Number of degrees of freedom: 3723
   Assembling...
   Solving...
   Writing output...

Refinement cycle 3
   Number of active cells: 1072
   Number of degrees of freedom: 7493
   Assembling...
   Solving...
   Writing output...

Refinement cycle 4
   Number of active cells: 2632
   Number of degrees of freedom: 15005
   Assembling...
   Solving...
   Writing output...

Refinement cycle 5
   Number of active cells: 5944
   Number of degrees of freedom: 29437
   Assembling...
   Solving...
   Writing output...
@endcode

The results are easily visualized:

<table width="80%" align="center">
  <tr valign="top">
    <td valign="top" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-46.9.2.velocity-2d.png" alt="">
      <p align="center">
        Magnitude and vectors for the fluid velocity.
      </p>
    </td>
    <td valign="top" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-46.9.2.pressure-2d.png" alt="">
      <p align="center">
        Fluid pressure. The dynamic range has been truncated to cut off the
        pressure singularities at the top left and right corners of the domain
        as well as the top corners of the solid that forms re-entrant corners
        into the fluid domain.
      </p>
    </td>
    <td valign="top" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-46.9.2.displacement-2d.png" alt="">
      <p align="center">
        Magnitude and vectors for the solid displacement.
      </p>
    </td>
  </tr>
</table>

The plots are easily interpreted: as the flow drives down on the left side and
up on the right side of the upright part of the solid, it produces a
pressure that is high on the left and low on the right, and these
forces bend the vertical part of the solid to the right.


<h3>3d results</h3>

By changing the dimension of the <code>FluidStructureProblem</code>
class in <code>main()</code> to 3, we can also run the same problem
3d. You'd get output along the following lines:
@code
Refinement cycle 0
   Number of active cells: 512
   Number of degrees of freedom: 11631
   Assembling...
   Solving...
   Writing output...

Refinement cycle 1
   Number of active cells: 1716
   Number of degrees of freedom: 48984
   Assembling...
   Solving...
   Writing output...

Refinement cycle 2
   Number of active cells: 8548
   Number of degrees of freedom: 245746
   Assembling...
   Solving...
@endcode
You'll notice that the big bottleneck is the solver: SparseDirectUmfpack needs
nearly 5 hours and some 80 GB of memory to solve the last iteration of
this problem on a 2016 workstation (the second to last iteration took only 16
minutes). Clearly a better solver is needed here, a topic discussed below.

The results can also be visualized and yield good pictures as
well. Here is one, showing both a vector plot for the velocity (in
oranges), the solid displacement (in blues), and shading the solid region:

<p align="center">
  <img src="https://www.dealii.org/images/steps/developer/step-46.9.2.3d.png" alt="">
</p>

In addition to the lack of a good solver, the mesh is a bit
unbalanced: mesh refinement heavily favors the fluid subdomain (in 2d,
it was the other way around, prompting us to weigh the fluid error
indicators higher). Clearly, some tweaking of the relative importance
of error indicators in the two subdomains is important if one wanted
to go on doing more 3d computations.


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

<h4>Linear solvers and preconditioners</h4>

An obvious place to improve the program would be to use a more
sophisticated solver &mdash; in particular one that scales well and
will also work for realistic 3d problems. This shouldn't actually be
too hard to achieve here, because of the one-way coupling from fluid
into solid. To this end, assume we had re-ordered degrees of freedom
in such a way that we first have all velocity and pressure degrees of
freedom, and then all displacements (this is easily possible using
DoFRenumbering::component_wise). Then the system matrix could be split
into the following block form:
@f[
  A_\text{global}
  =
  \begin{pmatrix}
    A_{\text{fluid}} & 0 \\
    B & A_{\text{solid}}
  \end{pmatrix}
@f]
where $A_{\text{fluid}}$ is the Stokes matrix for velocity and pressure (it
could be further subdivided into a $2\times 2$ matrix as in step-22, though
this is immaterial for the current purpose),
$A_{\text{solid}}$ results from the elasticity equations for the
displacements, and $B$ is the matrix that comes from the interface
conditions. Now notice that the matrix
@f[
  A_\text{global}^{-1}
  =
  \begin{pmatrix}
    A_{\text{fluid}}^{-1} & 0 \\
    -A_\text{solid}^{-1} B
      A_\text{fluid}^{-1} & A_{\text{solid}}^{-1}
  \end{pmatrix}
@f]
is the inverse of $A_\text{global}$. Applying this matrix requires
only one solve with $A_\text{fluid}$ and $A_\text{solid}$ each since
@f[
  \begin{pmatrix}
    p_x \\ p_y
  \end{pmatrix}
  =
  \begin{pmatrix}
    A_{\text{fluid}}^{-1} & 0 \\
    -A_\text{solid}^{-1} B
      A_\text{fluid}^{-1} & A_{\text{solid}}^{-1}
  \end{pmatrix}
  \begin{pmatrix}
    x \\ y
  \end{pmatrix}
@f]
can be computed as $p_x = A_{\text{fluid}}^{-1} x$ followed by
$p_y = A_{\text{solid}}^{-1} (y-Bp_x)$.

One can therefore expect that
@f[
  \widetilde{A_\text{global}^{-1}}
  =
  \begin{pmatrix}
    \widetilde{A_{\text{fluid}}^{-1}} & 0 \\
    -\widetilde{A_\text{solid}^{-1}} B
      \widetilde{A_\text{fluid}^{-1}} & \widetilde{A_{\text{solid}}^{-1}}
  \end{pmatrix}
@f]
would be a good preconditioner if $\widetilde{A_{\text{fluid}}^{-1}}
\approx A_{\text{fluid}}^{-1}, \widetilde{A_{\text{solid}}^{-1}}
\approx A_{\text{solid}}^{-1}$.

That means, we only need good preconditioners for Stokes and the
elasticity equations separately. These are well known: for
Stokes, we can use the preconditioner discussed in the results section
of step-22; for elasticity, a good preconditioner would be a single
V-cycle of a geometric or algebraic multigrid. There are more open
questions, however: For an "optimized" solver block-triangular
preconditioner built from two sub-preconditioners, one point that
often comes up is that, when choosing parameters for the
sub-preconditioners, values that work well when solving the two
problems separately may not be optimal when combined into a
multiphysics preconditioner.  In particular, when solving just a solid
or fluid mechanics problem separately, the balancing act between the
number of iterations to convergence and the cost of applying the
preconditioner on a per iteration basis may lead one to choose an
expensive preconditioner for the Stokes problem and a cheap
preconditioner for the elasticity problem (or vice versa).  When
combined, however, there is the additional constraint that you want
the two sub-preconditioners to converge at roughly the same rate, or
else the cheap one may drive up the global number of iterations while
the expensive one drives up the cost-per-iteration. For example, while a single AMG
V-cycle is a good approach for elasticity by itself, when combined
into a multiphysics problem there may be an incentive to using a full
W-cycle or multiple cycles to help drive down the total solve time.


<h4>Refinement indicators</h4>

As mentioned in the introduction, the refinement indicator we use for this
program is rather ad hoc. A better one would understand that the jump in the
gradient of the solution across the interface is not indicative of the error
but to be expected and ignore the interface when integrating the jump
terms. Nevertheless, this is not what the KellyErrorEstimator class
does. Another, bigger question, is whether this kind of estimator is a good
strategy in the first place: for example, if we want to have maximal accuracy
in one particular aspect of the displacement (e.g. the displacement at the top
right corner of the solid), then is it appropriate to scale the error
indicators for fluid and solid to the same magnitude? Maybe it is necessary to
solve the fluid problem with more accuracy than the solid because the fluid
solution directly affects the solids solution? Maybe the other way around?

Consequently, an obvious possibility for improving the program would be to
implement a better refinement criterion. There is some literature on this
topic; one of a variety of possible starting points would be the paper by
Thomas Wick on "Adaptive finite elements for monolithic fluid-structure
interaction on a prolongated domain: Applied to an heart valve simulation",
Proceedings of the Computer Methods in Mechanics Conference 2011 (CMM-2011),
9-12 May 2011, Warszaw, Poland.


<h4>Verification</h4>

The results above are purely qualitative as there is no evidence that our
scheme in fact converges. An obvious thing to do would therefore be to add
some quantitative measures to check that the scheme at least converges to
<i>something</i>. For example, we could output for each refinement cycle the
deflection of the top right corner of the part of the solid that protrudes
into the fluid subdomain. Or we could compute the net force vector or torque
the fluid exerts on the solid.


<h4>Better models</h4>

In reality, most fluid structure interaction problems are so that the movement
of the solid does affect the flow of the fluid. For example, the forces of the
air around an air foil cause it to flex and to change its shape. Likewise, a
flag flaps in the wind, completely changing its shape.

Such problems where the coupling goes both ways are typically handled in an
Arbitrary Lagrangian Eulerian (ALE) framework, in which the displacement of
the solid is extended into the fluid domain in some smooth way, rather than by
zero as we do here. The extended displacement field is then used to deform the
mesh on which we compute the fluid flow. Furthermore, the boundary conditions
for the fluid on the interface are no longer that the velocity is zero;
rather, in a time dependent program, the fluid velocity must be equal to the
time derivative of the displacement along the interface.


examples/step-47/doc/intro.dox
<br>

<i>
This program was contributed by Natasha Sharma, Guido Kanschat, Timo
Heister, Wolfgang Bangerth, and Zhuoran Wang.

The first author would like to acknowledge the support of NSF Grant
No. DMS-1520862.
Timo Heister and Wolfgang Bangerth acknowledge support through NSF
awards DMS-1821210, EAR-1550901, and OAC-1835673.
</i>

<a name="Intro"></a>
<h1>Introduction</h1>

This program deals with the <a
href="https://en.wikipedia.org/wiki/Biharmonic_equation">biharmonic
equation</a>,
@f{align*}{
  \Delta^2 u(\mathbf x) &= f(\mathbf x)
  \qquad \qquad &&\forall \mathbf x \in \Omega.
@f}
This equation appears in the modeling of thin structures such as roofs
of stadiums. These objects are of course in reality
three-dimensional with a large aspect ratio of lateral extent to
perpendicular thickness, but one can often very accurately model these
structures as two dimensional by making assumptions about how internal
forces vary in the perpendicular direction. These assumptions lead to the
equation above.

The model typically comes in two different kinds, depending on what
kinds of boundary conditions are imposed. The first case,
@f{align*}{
  u(\mathbf x) &= g(\mathbf x) \qquad \qquad
  &&\forall \mathbf x \in \partial\Omega, \\
  \Delta u(\mathbf x) &= h(\mathbf x) \qquad \qquad
  &&\forall \mathbf x \in \partial\Omega,
@f}
corresponds to the edges of the thin structure attached to the top of
a wall of height $g(\mathbf x)$ in such a way that the bending forces
that act on the structure are $h(\mathbf x)$; in most physical
situations, one will have $h=0$, corresponding to the structure simply
sitting atop the wall.

In the second possible case of boundary values, one would have
@f{align*}{
  u(\mathbf x) &= g(\mathbf x) \qquad \qquad
  &&\forall \mathbf x \in \partial\Omega, \\
  \frac{\partial u(\mathbf x)}{\partial \mathbf n} &= j(\mathbf x) \qquad \qquad
  &&\forall \mathbf x \in \partial\Omega.
@f}
This corresponds to a "clamped" structure for which a nonzero
$j(\mathbf x)$ implies a certain angle against the horizontal.

As with Dirichlet and Neumann boundary conditions for the Laplace
equation, it is of course possible to have one kind of boundary
conditions on one part of the boundary, and the other on the
remainder.


<h3> What's the issue? </h3>

The fundamental issue with the equation is that it takes four
derivatives of the solution. In the case of the Laplace equation
we treated in step-3, step-4, and several other tutorial programs,
one multiplies by a test function, integrates, integrates by parts,
and ends up with only one derivative on both the test function and
trial function -- something one can do with functions that are
continuous globally, but may have kinks at the interfaces between
cells: The derivative may not be defined at the interfaces, but
that is on a lower-dimensional manifold (and so doesn't show up
in the integrated value).

But for the biharmonic equation, if one followed the same procedure
using integrals over the entire domain (i.e., the union of all cells),
one would end up with two derivatives on the test functions and trial
functions each. If one were to use the usual piecewise polynomial
functions with their kinks on cell interfaces, the first derivative
would yield a discontinuous gradient, and the second derivative with
delta functions on the interfaces -- but because both the second
derivatives of the test functions and of the trial functions yield a
delta function, we would try to integrate the product of two delta
functions. For example, in 1d, where $\varphi_i$ are the usual
piecewise linear "hat functions", we would get integrals of the sort
@f{align*}{
  \int_0^L (\Delta \varphi_i) (\Delta \varphi_j)
  =
  \int_0^L
  \frac 1h \left[\delta(x-x_{i-1}) - 2\delta(x-x_i) + \delta(x-x_{i+1})\right]
  \frac 1h \left[\delta(x-x_{j-1}) - 2\delta(x-x_j) + \delta(x-x_{j+1})\right]
@f}
where $x_i$ is the node location at which the shape function
$\varphi_i$ is defined, and $h$ is the mesh size (assumed
uniform). The problem is that delta functions in integrals are defined
using the relationship
@f{align*}{
  \int_0^L \delta(x-\hat x) f(x) \; dx
  =
  f(\hat x).
@f}
But that only works if (i) $f(\cdot)$ is actually well defined at
$\hat x$, and (ii) if it is finite. On the other hand, an integral of
the form
@f{align*}{
\int_0^L \delta(x-x_i) \delta (x-x_i)
@f}
does not make sense. Similar reasoning can be applied for 2d and 3d
situations.

In other words: This approach of trying to integrate over the entire
domain and then integrating by parts can't work.

Historically, numerical analysts have tried to address this by
inventing finite elements that are "C<sup>1</sup> continuous", i.e., that use
shape functions that are not just continuous but also have continuous
first derivatives. This is the realm of elements such as the Argyris
element, the Clough-Tocher element and others, all developed in the
late 1960s. From a twenty-first century perspective, they can only be
described as bizarre in their construction. They are also exceedingly
cumbersome to implement if one wants to use general meshes. As a
consequence, they have largely fallen out of favor and deal.II currently
does not contain implementations of these shape functions.


<h3> What to do instead? </h3>

So how does one approach solving such problems then? That depends a
bit on the boundary conditions. If one has the first set of boundary
conditions, i.e., if the equation is
@f{align*}{
  \Delta^2 u(\mathbf x) &= f(\mathbf x)
  \qquad \qquad &&\forall \mathbf x \in \Omega, \\
  u(\mathbf x) &= g(\mathbf x) \qquad \qquad
  &&\forall \mathbf x \in \partial\Omega, \\
  \Delta u(\mathbf x) &= h(\mathbf x) \qquad \qquad
  &&\forall \mathbf x \in \partial\Omega,
@f}
then the following trick works (at least if the domain is convex, see
below): In the same way as we obtained the
mixed Laplace equation of step-20 from the regular Laplace equation by
introducing a second variable, we can here introduce a variable
$v=\Delta u$ and can then replace the equations above by the
following, "mixed" system:
@f{align*}{
  -\Delta u(\mathbf x) +v(\mathbf x) &= 0
  \qquad \qquad &&\forall \mathbf x \in \Omega, \\
  -\Delta v(\mathbf x) &= -f(\mathbf x)
  \qquad \qquad &&\forall \mathbf x \in \Omega, \\
  u(\mathbf x) &= g(\mathbf x) \qquad \qquad
  &&\forall \mathbf x \in \partial\Omega, \\
  v(\mathbf x) &= h(\mathbf x) \qquad \qquad
  &&\forall \mathbf x \in \partial\Omega.
@f}
In other words, we end up with what is in essence a system of two
coupled Laplace equations for $u,v$, each with Dirichlet-type boundary
conditions. We know how to solve such problems, and it should not be
very difficult to construct good solvers and preconditioners for this
system either using the techniques of step-20 or step-22. So this
case is pretty simple to deal with.

@note It is worth pointing out that this only works for domains whose
  boundary has corners if the domain is also convex -- in other words,
  if there are no re-entrant corners.
  This sounds like a rather random condition, but it makes
  sense in view of the following two facts: The solution of the
  original biharmonic equation must satisfy $u\in H^2(\Omega)$. On the
  other hand, the mixed system reformulation above suggests that both
  $u$ and $v$ satisfy $u,v\in H^1(\Omega)$ because both variables only
  solve a Poisson equation. In other words, if we want to ensure that
  the solution $u$ of the mixed problem is also a solution of the
  original biharmonic equation, then we need to be able to somehow
  guarantee that the solution of $-\Delta u=v$ is in fact more smooth
  than just $H^1(\Omega)$. This can be argued as follows: For convex
  domains,
  <a href="https://en.wikipedia.org/wiki/Elliptic_operator#Elliptic_regularity_theorem">"elliptic
  regularity"</a> implies that if the right hand side $v\in H^s$, then
  $u\in H^{s+2}$ if the domain is convex and the boundary is smooth
  enough. (This could also be guaranteed if the domain boundary is
  sufficiently smooth -- but domains whose boundaries have no corners
  are not very practical in real life.)
  We know that $v\in H^1$ because it solves the equation
  $-\Delta v=f$, but we are still left with the condition on convexity
  of the boundary; one can show that polygonal, convex domains are
  good enough to guarantee that $u\in H^2$ in this case (smoothly
  bounded, convex domains would result in $u\in H^3$, but we don't
  need this much regularity). On the other hand, if the domain is not
  convex, we can not guarantee that the solution of the mixed system
  is in $H^2$, and consequently may obtain a solution that can't be
  equal to the solution of the original biharmonic equation.

The more complicated situation is if we have the "clamped" boundary
conditions, i.e., if the equation looks like this:
@f{align*}{
  \Delta^2 u(\mathbf x) &= f(\mathbf x)
  \qquad \qquad &&\forall \mathbf x \in \Omega, \\
  u(\mathbf x) &= g(\mathbf x) \qquad \qquad
  &&\forall \mathbf x \in \partial\Omega, \\
  \frac{\partial u(\mathbf x)}{\partial \mathbf n} &= j(\mathbf x) \qquad \qquad
  &&\forall \mathbf x \in \partial\Omega.
@f}
The same trick with the mixed system does not work here, because we
would end up with <i>both</i> Dirichlet and Neumann boundary conditions for
$u$, but none for $v$.


The solution to this conundrum arrived with the Discontinuous Galerkin
method wave in the 1990s and early 2000s: In much the same way as one
can use <i>discontinuous</i> shape functions for the Laplace equation
by penalizing the size of the discontinuity to obtain a scheme for an
equation that has one derivative on each shape function, we can use a
scheme that uses <i>continuous</i> (but not $C^1$ continuous) shape
functions and penalize the jump in the derivative to obtain a scheme
for an equation that has two derivatives on each shape function. In
analogy to the Interior Penalty (IP) method for the Laplace equation,
this scheme for the biharmonic equation is typically called the $C^0$ IP
(or C0IP) method, since it uses $C^0$ (continuous but not continuously
differentiable) shape functions with an interior penalty formulation.


<h3> Derivation of the C0IP method </h3>

We base this program on the $C^0$ IP method presented by Susanne
Brenner and Li-Yeng Sung in the paper "C$^0$ Interior Penalty Method
for Linear Fourth Order Boundary Value Problems on polygonal
domains'' @cite Brenner2005 , where the method is
derived for the biharmonic equation with "clamped" boundary
conditions.

As mentioned, this method relies on the use of $C^0$ Lagrange finite
elements where the $C^1$ continuity requirement is relaxed and has
been replaced with interior penalty techniques. To derive this method,
we consider a $C^0$ shape function $v_h$ which vanishes on
$\partial\Omega$. We introduce notation $ \mathbb{F} $ as the set of
all faces of $\mathbb{T}$, $ \mathbb{F}^b $ as the set of boundary faces,
and $ \mathbb{F}^i $ as the set of interior faces for use further down below.
Since the higher order derivatives of $v_h$ have two
values on each interface $e\in \mathbb{F}$ (shared by the two cells
$K_{+},K_{-} \in \mathbb{T}$), we cope with this discontinuity by
defining the following single-valued functions on $e$:
@f{align*}{
  \jump{\frac{\partial^k v_h}{\partial \mathbf n^k}}
  &=
  \frac{\partial^k v_h|_{K_+}}{\partial \mathbf n^k} \bigg |_e
  - \frac{\partial^k v_h|_{K_-}}{\partial \mathbf n^k} \bigg |_e,
  \\
  \average{\frac{\partial^k v_h}{\partial \mathbf n^k}}
  &=
  \frac{1}{2}
  \bigg( \frac{\partial^k v_h|_{K_+}}{\partial \mathbf n^k} \bigg |_e
  + \frac{\partial^k v_h|_{K_-}}{\partial \mathbf n^k} \bigg |_e \bigg )
@f}
for $k =1,2$ (i.e., for the gradient and the matrix of second
derivatives), and where $\mathbf n$ denotes a unit vector normal to
$e$ pointing from $K_+$ to $K_-$. In the
literature, these functions are referred to as the "jump" and
"average" operations, respectively.

To obtain the $C^0$ IP approximation $u_h$, we left multiply the
biharmonic equation by $v_h$, and then integrate over $\Omega$. As
explained above, we can't do the integration by parts on all of
$\Omega$ with these shape functions, but we can do it on each cell
individually since the shape functions are just polynomials on each
cell. Consequently, we start by using the following
integration-by-parts formula on each mesh cell $K \in {\mathbb{T}}$:
@f{align*}{
  \int_K v_h (\Delta^2 w_h)
  &= \int_K v_h (\nabla\cdot\nabla) (\Delta w_h)
  \\
  &= -\int_K \nabla v_h \cdot (\nabla \Delta w_h)
     +\int_{\partial K} v_h (\nabla \Delta w_h \cdot \mathbf n).
@f}
At this point, we have two options: We can integrate the domain term's
$\nabla\Delta w_h$ one more time to obtain
@f{align*}{
  \int_K v_h (\Delta^2 w_h)
  &= \int_K (\Delta v_h) (\Delta w_h)
     +\int_{\partial K} v_h (\nabla \Delta w_h \cdot \mathbf n)
     -\int_{\partial K} (\nabla v_h \cdot \mathbf n) \Delta w_h.
@f}
For a variety of reasons, this turns out to be a variation that is not
useful for our purposes.

Instead, what we do is recognize that
$\nabla\Delta w_h = \text{grad}\,(\text{div}\,\text{grad}\, w_h)$, and we
can re-sort these operations as
$\nabla\Delta w_h = \text{div}\,(\text{grad}\,\text{grad}\, w_h)$ where we
typically write $\text{grad}\,\text{grad}\, w_h = D^2 w_h$ to indicate
that this is the "Hessian" matrix of second derivatives. With this
re-ordering, we can now integrate the divergence, rather than the
gradient operator, and we get the following instead:
@f{align*}{
  \int_K v_h (\Delta^2 w_h)
  &= \int_K (\nabla \nabla v_h) : (\nabla \nabla w_h)
     +\int_{\partial K} v_h (\nabla \Delta w_h \cdot \mathbf n)
     -\int_{\partial K} (\nabla v_h \otimes \mathbf n) : (\nabla\nabla w_h)
  \\
  &= \int_K (D^2 v_h) : (D^2 w_h)
     +\int_{\partial K} v_h (\nabla \Delta w_h \cdot \mathbf n)
     -\int_{\partial K} (\nabla v_h) \cdot (D^2 w_h \mathbf n).
@f}
Here, the colon indicates a double-contraction over the indices of the
matrices to its left and right, i.e., the scalar product between two
tensors. The outer product of two vectors $a \otimes b$ yields the
matrix $(a \otimes b)_{ij} = a_i b_j$.

Then, we sum over all cells $K \in  \mathbb{T}$, and take into account
that this means that every interior face appears twice in the
sum. If we therefore split everything into a sum of integrals over
cell interiors and a separate sum over cell interfaces, we can use
the jump and average operators defined above. There are two steps
left: First, because our shape functions are continuous, the gradients
of the shape functions may be discontinuous, but the continuity
guarantees that really only the normal component of the gradient is
discontinuous across faces whereas the tangential component(s) are
continuous. Second, the discrete formulation that results is not
stable as the mesh size goes to zero, and to obtain a stable
formulation that converges to the correct solution, we need to add
the following terms:
@f{align*}{
-\sum_{e \in \mathbb{F}} \int_{e}
  \average{\frac{\partial^2 v_h}{\partial \mathbf n^2}}
  \jump{\frac{\partial u_h}{\partial \mathbf n}}
+ \sum_{e \in \mathbb{F}}
  \frac{\gamma}{h_e}\int_e
  \jump{\frac{\partial v_h}{\partial \mathbf n}}
  \jump{\frac{\partial u_h}{\partial \mathbf n}}.
@f}
Then, after making cancellations that arise, we arrive at the following
C0IP formulation of the biharmonic equation: find $u_h$ such that $u_h =
g$ on $\partial \Omega$ and
@f{align*}{
\mathcal{A}(v_h,u_h)&=\mathcal{F}(v_h) \quad \text{holds for all test functions } v_h,
@f}
where
@f{align*}{
\mathcal{A}(v_h,u_h):=&\sum_{K \in \mathbb{T}}\int_K D^2v_h:D^2u_h \ dx
\\
&
 -\sum_{e \in \mathbb{F}} \int_{e}
  \jump{\frac{\partial v_h}{\partial \mathbf n}}
  \average{\frac{\partial^2 u_h}{\partial \mathbf n^2}} \ ds
 -\sum_{e \in \mathbb{F}} \int_{e}
 \average{\frac{\partial^2 v_h}{\partial \mathbf n^2}}
 \jump{\frac{\partial u_h}{\partial \mathbf n}} \ ds
\\
&+ \sum_{e \in \mathbb{F}}
 \frac{\gamma}{h_e}
 \int_e
 \jump{\frac{\partial v_h}{\partial \mathbf n}}
 \jump{\frac{\partial u_h}{\partial \mathbf n}} \ ds,
@f}
and
@f{align*}{
\mathcal{F}(v_h)&:=\sum_{K \in \mathbb{T}}\int_{K} v_h f \ dx
-
\sum_{e \in \mathbb{F}, e\subset\partial\Omega}
\int_e \average{\frac{\partial^2 v_h}{\partial \mathbf n^2}} j \ ds
+
\sum_{e \in \mathbb{F}, e\subset\partial\Omega}
\frac{\gamma}{h_e}
\int_e \jump{\frac{\partial v_h}{\partial \mathbf n}} j \ ds.
@f}
Here, $\gamma$ is the penalty parameter which both weakly enforces the
boundary condition
@f{align*}{
\frac{\partial u(\mathbf x)}{\partial \mathbf n} = j(\mathbf x)
@f}
on the boundary interfaces $e \in \mathbb{F}^b$, and also ensures that
in the limit $h\rightarrow 0$, $u_h$ converges to a $C^1$ continuous
function. $\gamma$ is chosen to be large enough to guarantee the
stability of the method. We will discuss our choice in the program below.


<h4>Convergence Rates </h4>
On polygonal domains, the weak solution $u$ to the biharmonic equation
lives in $H^{2 +\alpha}(\Omega)$ where $\alpha \in(1/2, 2]$ is
determined by the interior angles at the corners of $\Omega$. For
instance, whenever $\Omega$ is convex, $\alpha=1$; $\alpha$ may be less
than one if the domain has re-entrant corners but
$\alpha$ is close to $1$ if one of all interior angles is close to
$\pi$.

Now suppose that the $C^0$ IP solution $u_h$ is approximated by $C^0$
shape functions with polynomial degree $p \ge 2$. Then the
discretization outlined above yields the convergence rates as
discussed below.


<b>Convergence in the $C^0$ IP-norm</b>

Ideally, we would like to measure convergence in the "energy norm"
$\|D^2(u-u_h)\|$. However, this does not work because, again, the
discrete solution $u_h$ does not have two (weak) derivatives. Instead,
one can define a discrete ($C^0$ IP) seminorm that is "equivalent" to the
energy norm, as follows:
@f{align*}{
 |u_h|_{h}^2 :=
 \sum\limits_{K \in \mathbb{T}} \big|u_h\big|_{H^2(K)}^2
 +
 \sum\limits_{e \in \mathbb{F} }
 \frac{\gamma }{h_e} \left\|
 \jump{\frac{\partial u_h}{\partial \mathbf n}} \right\|_{L^2(e)}^2.
@f}

In this seminorm, the theory in the paper mentioned above yields that we
can expect
@f{align*}{
 |u-u_h|_{h}^2 = {\cal O}(h^{p-1}),
@f}
much as one would expect given the convergence rates we know are true
for the usual discretizations of the Laplace equation.

Of course, this is true only if the exact solution is sufficiently
smooth. Indeed, if $f \in H^m(\Omega)$ with $m \ge 0$,
$u \in H^{2+\alpha}(\Omega)$ where $ 2 < 2+\alpha  \le m+4$,
then the convergence rate of the $C^0$ IP method is
$\mathcal{O}(h^{\min\{p-1, \alpha\}})$. In other words, the optimal
convergence rate can only be expected if the solution is so smooth
that $\alpha\ge p-1$; this can
only happen if (i) the domain is convex with a sufficiently smooth
boundary, and (ii) $m\ge p-3$. In practice, of course, the solution is
what it is (independent of the polynomial degree we choose), and the
last condition can then equivalently be read as saying that there is
definitely no point in choosing $p$ large if $m$ is not also
large. In other words, the only reasonably choices for $p$ are $p\le
m+3$ because larger polynomial degrees do not result in higher
convergence orders.

For the purposes of this program, we're a bit too lazy to actually
implement this equivalent seminorm -- though it's not very difficult and
would make for a good exercise. Instead, we'll simply check in the
program what the "broken" $H^2$ seminorm
@f{align*}{
 \left(|u_h|^\circ_{H^2}\right)^2
 :=
 \sum\limits_{K \in \mathbb{T}} \big|u_h\big|_{H^2(K)}^2
 =
 \sum\limits_{K \in \mathbb{T}} \big|D^2 u_h\big|_{L_2}^2
@f}
yields. The convergence rate in this norm can, from a theoretical
perspective, of course not be <i>worse</i> than the one for
$|\cdot|_h$ because it contains only a subset of the necessary terms,
but it could at least conceivably be better. It could also be the case that
we get the optimal convergence rate even though there is a bug in the
program, and that that bug would only show up in sub-optimal rates for
the additional terms present in $|\cdot|_h$. But, one might hope
that if we get the optimal rate in the broken norm and the norms
discussed below, then the program is indeed correct. The results
section will demonstrate that we obtain optimal rates in all norms
shown.


<b>Convergence in the $L_2$-norm</b>

The optimal convergence rate in the $L_2$-norm is $\mathcal{O}(h^{p+1})$
provided $p \ge 3$. More details can be found in Theorem 4.6 of
@cite Engel2002 .

The default in the program below is to choose $p=2$. In that case, the
theorem does not apply, and indeed one only gets $\mathcal{O}(h^2)$
instead of $\mathcal{O}(h^3)$ as we will show in the results section.


<b>Convergence in the $H^1$-seminorm</b>

Given that we expect
$\mathcal{O}(h^{p-1})$ in the best of cases for a norm equivalent to
the $H^2$ seminorm, and $\mathcal{O}(h^{p+1})$ for the $L_2$ norm, one
may ask about what happens in the $H^1$ seminorm that is intermediate
to the two others. A reasonable guess is that one should expect
$\mathcal{O}(h^{p})$. There is probably a paper somewhere that proves
this, but we also verify that this conjecture is experimentally true
below.



<h3>Other Boundary Conditions</h3>

We remark that the derivation of the $C^0$ IP method for the
biharmonic equation with other boundary conditions -- for instance,
for the first set of boundary conditions namely $u(\mathbf x) =
g(\mathbf x)$ and $\Delta u(\mathbf x)= h(\mathbf x)$ on
$\partial\Omega$ -- can be obtained with suitable modifications to
$\mathcal{A}(\cdot,\cdot)$ and $\mathcal{F}(\cdot)$ described in
the book chapter @cite Brenner2011 .


<h3>The testcase</h3>

The last step that remains to describe is what this program solves
for. As always, a trigonometric function is both a good and a bad
choice because it does not lie in any polynomial space in which we may
seek the solution while at the same time being smoother than real
solutions typically are (here, it is in $C^\infty$ while real
solutions are typically only in $H^3$ or so on convex polygonal
domains, or somewhere between $H^2$ and $H^3$ if the domain is not
convex). But, since we don't have the means to describe solutions of
realistic problems in terms of relatively simple formulas, we just go
with the following, on the unit square for the domain $\Omega$:
@f{align*}{
  u = \sin(\pi x) \sin(\pi y).
@f}
As a consequence, we then need choose as boundary conditions the following:
@f{align*}{
  g &= u|_{\partial\Omega} = \sin(\pi x) \sin(\pi y)|_{\partial\Omega},
  \\
  j &= \frac{\partial u}{\partial\mathbf n}|_{\partial\Omega}
  \\
    &= \left.\begin{pmatrix}
                \pi\cos(\pi x) \sin(\pi y) \\
                \pi\sin(\pi x) \cos(\pi y)
             \end{pmatrix}\right|_{\partial\Omega} \cdot \mathbf n.
@f}
The right hand side is easily computed as
@f{align*}{
  f = \Delta^2 u = 4 \pi^4 \sin(\pi x) \sin(\pi y).
@f}
The program has classes `ExactSolution::Solution` and
`ExactSolution::RightHandSide` that encode this information.


examples/step-47/doc/results.dox
<h1>Results</h1>

We run the program with right hand side and boundary values as
discussed in the introduction. These will produce the
solution $u = \sin(\pi x) \sin(\pi y)$ on the domain $\Omega = (0,1)^2$.
We test this setup using $Q_2$, $Q_3$, and $Q_4$ elements, which one can
change via the `fe_degree` variable in the `main()` function. With mesh
refinement, the $L_2$ convergence rates, $H^1$-seminorm rate,
and $H^2$-seminorm convergence of $u$
should then be around 2, 2, 1 for $Q_2$ (with the $L_2$ norm
sub-optimal as discussed in the introduction); 4, 3, 2 for
$Q_3$; and 5, 4, 3 for $Q_4$, respectively.

From the literature, it is not immediately clear what
the penalty parameter $\gamma$ should be. For example,
@cite Brenner2009 state that it needs to be larger than one, and
choose $\gamma=5$. The FEniCS/Dolphin tutorial chooses it as
$\gamma=8$, see
https://fenicsproject.org/docs/dolfin/1.6.0/python/demo/documented/biharmonic/python/documentation.html
. @cite Wells2007 uses a value for $\gamma$ larger than the
number of edges belonging to an element for Kirchhoff plates (see
their Section 4.2). This suggests that maybe
$\gamma = 1$, $2$, are too small; on the other hand, a value
$p(p+1)$ would be reasonable,
where $p$ is the degree of polynomials. The last of these choices is
the one one would expect to work by comparing
to the discontinuous Galerkin formulations for the Laplace equation
(see, for example, the discussions in step-39 and step-74),
and it will turn out to also work here.
But we should check what value of $\gamma$ is right, and we will do so
below; changing $\gamma$ is easy in the two `face_worker` and
`boundary_worker` functions defined in `assemble_system()`.


<h3>Test results on <i>Q<sub>2</sub></i> with <i>&gamma; = p(p+1)</i> </h3>

We run the code with differently refined meshes
and get the following convergence rates.

<table align="center" class="doxtable">
  <tr>
   <th>Number of refinements </th><th>  $\|u-u_h^\circ\|_{L_2}$ </th><th>  Conv. rates  </th><th>  $|u-u_h|_{H^1}$ </th><th> Conv. rates </th><th> $|u-u_h|_{H^2}$ </th><th> Conv. rates </th>
  </tr>
  <tr>
   <td>   2                  </td><td>   8.780e-03 </td><td>       </td><td>  7.095e-02   </td><td>           </td><td>  1.645 </td><td>   </td>
  </tr>
  <tr>
   <td>   3                  </td><td>   3.515e-03   </td><td>  1.32 </td><td> 2.174e-02  </td><td>     1.70     </td><td> 8.121e-01  </td><td>  1.018  </td>
  </tr>
  <tr>
   <td>   4                  </td><td>   1.103e-03   </td><td>  1.67   </td><td> 6.106e-03    </td><td>  1.83        </td><td>   4.015e-01 </td><td> 1.016  </td>
  </tr>
  <tr>
   <td>   5                  </td><td>  3.084e-04  </td><td>  1.83   </td><td>  1.622e-03   </td><td>    1.91        </td><td> 1.993e-01 </td><td>  1.010   </td>
  </tr>
</table>
We can see that the $L_2$ convergence rates are around 2,
$H^1$-seminorm convergence rates are around 2,
and $H^2$-seminorm convergence rates are around 1. The latter two
match the theoretically expected rates; for the former, we have no
theorem but are not surprised that it is sub-optimal given the remark
in the introduction.


<h3>Test results on <i>Q<sub>3</sub></i> with <i>&gamma; = p(p+1)</i> </h3>


<table align="center" class="doxtable">
  <tr>
   <th>Number of refinements </th><th>  $\|u-u_h^\circ\|_{L_2}$ </th><th>  Conv. rates  </th><th>  $|u-u_h|_{H^1}$ </th><th> Conv. rates </th><th> $|u-u_h|_{H^2}$ </th><th> Conv. rates </th>
  </tr>
  <tr>
   <td>   2                  </td><td>    2.045e-04 </td><td>       </td><td>   4.402e-03   </td><td>           </td><td> 1.641e-01 </td><td>   </td>
  </tr>
  <tr>
   <td>   3                  </td><td>   1.312e-05   </td><td> 3.96  </td><td>  5.537e-04  </td><td>   2.99     </td><td> 4.096e-02 </td><td>  2.00  </td>
  </tr>
  <tr>
   <td>   4                  </td><td>   8.239e-07 </td><td>  3.99  </td><td> 6.904e-05   </td><td> 3.00     </td><td> 1.023e-02 </td><td> 2.00 </td>
  </tr>
  <tr>
   <td>   5                  </td><td>   5.158e-08  </td><td>  3.99 </td><td> 8.621e-06 </td><td>  3.00      </td><td> 2.558e-03  </td><td>  2.00  </td>
  </tr>
</table>
We can see that the $L_2$ convergence rates are around 4,
$H^1$-seminorm convergence rates are around 3,
and $H^2$-seminorm convergence rates are around 2.
This, of course, matches our theoretical expectations.


<h3>Test results on <i>Q<sub>4</sub></i> with <i>&gamma; = p(p+1)</i> </h3>

<table align="center" class="doxtable">
  <tr>
   <th>Number of refinements </th><th>  $\|u-u_h^\circ\|_{L_2}$ </th><th>  Conv. rates  </th><th>  $|u-u_h|_{H^1}$ </th><th> Conv. rates </th><th> $|u-u_h|_{H^2}$ </th><th> Conv. rates </th>
  </tr>
  <tr>
   <td>   2                  </td><td>    6.510e-06 </td><td>       </td><td> 2.215e-04   </td><td>           </td><td>  1.275e-02 </td><td>   </td>
  </tr>
  <tr>
   <td>   3                  </td><td>   2.679e-07  </td><td>  4.60  </td><td> 1.569e-05  </td><td>   3.81    </td><td> 1.496e-03 </td><td>  3.09  </td>
  </tr>
  <tr>
   <td>   4                  </td><td>   9.404e-09  </td><td> 4.83   </td><td> 1.040e-06    </td><td> 3.91       </td><td> 1.774e-04 </td><td> 3.07 </td>
  </tr>
  <tr>
   <td>   5                  </td><td>   7.943e-10 </td><td>  3.56  </td><td>   6.693e-08 </td><td> 3.95     </td><td> 2.150e-05  </td><td> 3.04    </td>
  </tr>
</table>
We can see that the $L_2$ norm convergence rates are around 5,
$H^1$-seminorm convergence rates are around 4,
and $H^2$-seminorm convergence rates are around 3.
On the finest mesh, the $L_2$ norm convergence rate
is much smaller than our theoretical expectations
because the linear solver becomes the limiting factor due
to round-off. Of course the $L_2$ error is also very small already in
that case.


<h3>Test results on <i>Q<sub>2</sub></i> with <i>&gamma; = 1</i> </h3>

For comparison with the results above, let us now also consider the
case where we simply choose $\gamma=1$:

<table align="center" class="doxtable">
  <tr>
   <th>Number of refinements </th><th>  $\|u-u_h^\circ\|_{L_2}$ </th><th>  Conv. rates  </th><th>  $|u-u_h|_{H^1}$ </th><th> Conv. rates </th><th> $|u-u_h|_{H^2}$ </th><th> Conv. rates </th>
  </tr>
  <tr>
   <td>   2                  </td><td>   7.350e-02 </td><td>       </td><td>   7.323e-01   </td><td>           </td><td> 10.343 </td><td>   </td>
  </tr>
  <tr>
   <td>   3                  </td><td>   6.798e-03   </td><td> 3.43  </td><td> 1.716e-01   </td><td>   2.09    </td><td>4.836 </td><td>  1.09 </td>
  </tr>
  <tr>
   <td>   4                  </td><td>  9.669e-04   </td><td> 2.81   </td><td> 6.436e-02    </td><td> 1.41      </td><td>  3.590 </td><td> 0.430 </td>
  </tr>
  <tr>
   <td>   5                  </td><td>   1.755e-04 </td><td> 2.46 </td><td>  2.831e-02  </td><td>    1.18      </td><td>3.144  </td><td>  0.19  </td>
  </tr>
</table>
Although $L_2$ norm convergence rates of $u$ more or less
follows the theoretical expectations,
the $H^1$-seminorm and $H^2$-seminorm do not seem to converge as expected.
Comparing results from $\gamma = 1$ and $\gamma = p(p+1)$, it is clear that
$\gamma = p(p+1)$ is a better penalty.
Given that $\gamma=1$ is already too small for $Q_2$ elements, it may not be surprising that if one repeated the
experiment with a $Q_3$ element, the results are even more disappointing: One again only obtains convergence
rates of 2, 1, zero -- i.e., no better than for the $Q_2$ element (although the errors are smaller in magnitude).
Maybe surprisingly, however, one obtains more or less the expected convergence orders when using $Q_4$
elements. Regardless, this uncertainty suggests that $\gamma=1$ is at best a risky choice, and at worst an
unreliable one and that we should choose $\gamma$ larger.


<h3>Test results on <i>Q<sub>2</sub></i> with <i>&gamma; = 2</i> </h3>

Since $\gamma=1$ is clearly too small, one might conjecture that
$\gamma=2$ might actually work better. Here is what one obtains in
that case:

<table align="center" class="doxtable">
  <tr>
   <th>Number of refinements </th><th>  $\|u-u_h^\circ\|_{L_2}$ </th><th>  Conv. rates  </th><th>  $|u-u_h|_{H^1}$ </th><th> Conv. rates </th><th> $|u-u_h|_{H^2}$ </th><th> Conv. rates </th>
  </tr>
  <tr>
   <td>   2                  </td><td>   4.133e-02 </td><td>       </td><td>  2.517e-01   </td><td>           </td><td> 3.056 </td><td>   </td>
  </tr>
  <tr>
   <td>   3                  </td><td>  6.500e-03   </td><td>2.66  </td><td> 5.916e-02  </td><td>  2.08    </td><td>1.444 </td><td>  1.08 </td>
  </tr>
  <tr>
   <td>   4                  </td><td> 6.780e-04   </td><td> 3.26  </td><td> 1.203e-02    </td><td> 2.296      </td><td> 6.151e-01 </td><td> 1.231 </td>
  </tr>
  <tr>
   <td>   5                  </td><td> 1.622e-04 </td><td> 2.06 </td><td>  2.448e-03  </td><td>   2.297     </td><td> 2.618e-01  </td><td> 1.232  </td>
  </tr>
</table>
In this case, the convergence rates more or less follow the
theoretical expectations, but, compared to the results from $\gamma =
p(p+1)$, are more variable.
Again, we could repeat this kind of experiment for $Q_3$ and $Q_4$ elements. In both cases, we will find that we
obtain roughly the expected convergence rates. Of more interest may then be to compare the absolute
size of the errors. While in the table above, for the $Q_2$ case, the errors on the finest grid are comparable between
the $\gamma=p(p+1)$ and $\gamma=2$ case, for $Q_3$ the errors are substantially larger for $\gamma=2$ than for
$\gamma=p(p+1)$. The same is true for the $Q_4$ case.


<h3> Conclusions for the choice of the penalty parameter </h3>

The conclusions for which of the "reasonable" choices one should use for the penalty parameter
is that $\gamma=p(p+1)$ yields the expected results. It is, consequently, what the code
uses as currently written.


<h3> Possibilities for extensions </h3>

There are a number of obvious extensions to this program that would
make sense:

- The program uses a square domain and a uniform mesh. Real problems
  don't come this way, and one should verify convergence also on
  domains with other shapes and, in particular, curved boundaries. One
  may also be interested in resolving areas of less regularity by
  using adaptive mesh refinement.

- From a more theoretical perspective, the convergence results above
  only used the "broken" $H^2$ seminorm $|\cdot|^\circ_{H^2}$ instead
  of the "equivalent" norm $|\cdot|_h$. This is good enough to
  convince ourselves that the program isn't fundamentally
  broken. However, it might be interesting to measure the error in the
  actual norm for which we have theoretical results. Implementing this
  addition should not be overly difficult using, for example, the
  FEInterfaceValues class combined with MeshWorker::mesh_loop() in the
  same spirit as we used for the assembly of the linear system.


  <h4> Derivation for the simply supported plates </h4>

  Similar to the "clamped" boundary condition addressed in the implementation,
  we will derive the $C^0$ IP finite element scheme for simply supported plates:
  @f{align*}{
    \Delta^2 u(\mathbf x) &= f(\mathbf x)
    \qquad \qquad &&\forall \mathbf x \in \Omega,
    u(\mathbf x) &= g(\mathbf x) \qquad \qquad
    &&\forall \mathbf x \in \partial\Omega, \\
    \Delta u(\mathbf x) &= h(\mathbf x) \qquad \qquad
    &&\forall \mathbf x \in \partial\Omega.
  @f}
  We multiply the biharmonic equation by the test function $v_h$ and integrate over $ K $ and get:
  @f{align*}{
    \int_K v_h (\Delta^2 u_h)
     &= \int_K (D^2 v_h) : (D^2 u_h)
       + \int_{\partial K} v_h \frac{\partial (\Delta u_h)}{\partial \mathbf{n}}
       -\int_{\partial K} (\nabla v_h) \cdot (\frac{\partial \nabla u_h}{\partial \mathbf{n}}).
  @f}

  Summing up over all cells $K \in  \mathbb{T}$,since normal directions of $\Delta u_h$ are pointing at
  opposite directions on each interior edge shared by two cells and $v_h = 0$ on $\partial \Omega$,
  @f{align*}{
  \sum_{K \in \mathbb{T}} \int_{\partial K} v_h \frac{\partial (\Delta u_h)}{\partial \mathbf{n}} = 0,
  @f}
  and by the definition of jump over cell interfaces,
  @f{align*}{
  -\sum_{K \in \mathbb{T}} \int_{\partial K} (\nabla v_h) \cdot (\frac{\partial \nabla u_h}{\partial \mathbf{n}}) = -\sum_{e \in \mathbb{F}} \int_{e} \jump{\frac{\partial v_h}{\partial \mathbf{n}}} (\frac{\partial^2 u_h}{\partial \mathbf{n^2}}).
  @f}
  We separate interior faces and boundary faces of the domain,
  @f{align*}{
  -\sum_{K \in \mathbb{T}} \int_{\partial K} (\nabla v_h) \cdot (\frac{\partial \nabla u_h}{\partial \mathbf{n}}) = -\sum_{e \in \mathbb{F}^i} \int_{e} \jump{\frac{\partial v_h}{\partial \mathbf{n}}} (\frac{\partial^2 u_h}{\partial \mathbf{n^2}})
  - \sum_{e \in \partial \Omega} \int_{e} \jump{\frac{\partial v_h}{\partial \mathbf{n}}} h,
  @f}
  where $\mathbb{F}^i$ is the set of interior faces.
  This leads us to
  @f{align*}{
  \sum_{K \in \mathbb{T}} \int_K (D^2 v_h) : (D^2 u_h) \ dx - \sum_{e \in \mathbb{F}^i} \int_{e} \jump{\frac{\partial v_h}{\partial \mathbf{n}}} (\frac{\partial^2 u_h}{\partial \mathbf{n^2}}) \ ds
  = \sum_{K \in \mathbb{T}}\int_{K} v_h f  \ dx + \sum_{e\subset\partial\Omega} \int_{e} \jump{\frac{\partial v_h}{\partial \mathbf{n}}} h \ ds.
  @f}

  In order to symmetrize and stabilize the discrete problem,
  we add symmetrization and stabilization term.
  We finally get the $C^0$ IP finite element scheme for the biharmonic equation:
  find $u_h$ such that $u_h =g$ on $\partial \Omega$ and
  @f{align*}{
  \mathcal{A}(v_h,u_h)&=\mathcal{F}(v_h) \quad \text{holds for all test functions } v_h,
  @f}
  where
  @f{align*}{
  \mathcal{A}(v_h,u_h):=&\sum_{K \in \mathbb{T}}\int_K D^2v_h:D^2u_h \ dx
  \\
  &
   -\sum_{e \in \mathbb{F}^i} \int_{e}
    \jump{\frac{\partial v_h}{\partial \mathbf n}}
    \average{\frac{\partial^2 u_h}{\partial \mathbf n^2}} \ ds
   -\sum_{e \in \mathbb{F}^i} \int_{e}
   \average{\frac{\partial^2 v_h}{\partial \mathbf n^2}}
   \jump{\frac{\partial u_h}{\partial \mathbf n}} \ ds
  \\
  &+ \sum_{e \in \mathbb{F}^i}
   \frac{\gamma}{h_e}
   \int_e
   \jump{\frac{\partial v_h}{\partial \mathbf n}}
   \jump{\frac{\partial u_h}{\partial \mathbf n}} \ ds,
  @f}
  and
  @f{align*}{
  \mathcal{F}(v_h)&:=\sum_{K \in \mathbb{T}}\int_{K} v_h f \ dx
  +
  \sum_{e\subset\partial\Omega}
  \int_e \jump{\frac{\partial v_h}{\partial \mathbf n}} h \ ds.
  @f}
  The implementation of this boundary case is similar to the "clamped" version
  except that `boundary_worker` is no longer needed for system assembling
  and the right hand side is changed according to the formulation.


examples/step-48/doc/intro.dox

<i>
This program was contributed by Katharina Kormann and Martin
Kronbichler.

The algorithm for the matrix-vector product is based on the article <a
href="http://dx.doi.org/10.1016/j.compfluid.2012.04.012">A generic
interface for parallel cell-based finite element operator
application</a> by Martin Kronbichler and Katharina Kormann, Computers
and Fluids 63:135&ndash;147, 2012, and the paper &quot;Parallel finite element operator
application: Graph partitioning and coloring&quot; by Katharina
Kormann and Martin Kronbichler in: Proceedings of the 7th IEEE
International Conference on e-Science, 2011.  </i>

<a name="Intro"></a>
<h1>Introduction</h1>

This program demonstrates how to use the cell-based implementation of finite
element operators with the MatrixFree class, first introduced in step-37, to
solve nonlinear partial differential equations. Moreover, we have another look
at the handling of constraints within the matrix-free framework.
Finally, we will use an explicit time-stepping
method to solve the problem and introduce Gauss-Lobatto finite elements that
are very convenient in this case since their mass matrix can be accurately
approximated by a diagonal, and thus trivially invertible, matrix. The two
ingredients to this property are firstly a distribution of the nodal points of
Lagrange polynomials according to the point distribution of the Gauss-Lobatto
quadrature rule. Secondly, the quadrature is done with the same Gauss-Lobatto
quadrature rule. In this formula, the integrals $\int_K \varphi_i \varphi_j
dx\approx \sum_q \varphi_i \varphi_j \mathrm{det}(J) \big |_{x_q}$ become
zero whenever $i\neq j$, because exactly one function $\varphi_j$ is one and
all others zero in the points defining the Lagrange polynomials.
Moreover, the Gauss-Lobatto distribution of nodes of Lagrange
polynomials clusters the nodes towards the element boundaries. This results in
a well-conditioned polynomial basis for high-order discretization
methods. Indeed, the condition number of an FE_Q elements with equidistant
nodes grows exponentially with the degree, which destroys any benefit for
orders of about five and higher. For this reason, Gauss-Lobatto points are the
default distribution for the FE_Q element (but at degrees one and two, those
are equivalent to the equidistant points).

<h3> Problem statement and discretization </h3>

As an example, we choose to solve the sine-Gordon soliton equation
\f{eqnarray*}
u_{tt} &=& \Delta u -\sin(u) \quad\mbox{for}\quad (x,t) \in
\Omega \times (t_0,t_f],\\
{\mathbf n} \cdot \nabla u &=& 0
\quad\mbox{for}\quad (x,t) \in \partial\Omega \times (t_0,t_f],\\
u(x,t_0) &=& u_0(x).
\f}

that was already introduced in step-25. As a simple explicit time
integration method, we choose leap frog scheme using the second-order
formulation of the equation. With this time stepping, the scheme reads in
weak form

\f{eqnarray*}
(v,u^{n+1}) = (v,2 u^n-u^{n-1} -
(\Delta t)^2 \sin(u^n)) - (\nabla v, (\Delta t)^2 \nabla u^n),
\f}
where <i> v</i> denotes a test function and the index <i>n</i> stands for
the time step number.

For the spatial discretization, we choose FE_Q elements
with basis functions defined to interpolate the support points of the
Gauss-Lobatto quadrature rule. Moreover, when we compute the integrals
over the basis functions to form the mass matrix and the operator on
the right hand side of the equation above, we use the
Gauss-Lobatto quadrature rule with the same support points as the
node points of the finite element to evaluate the integrals. Since the
finite element is Lagrangian, this will yield a diagonal mass matrix
on the left hand side of the equation, making the solution of the
linear system in each time step trivial.

Using this quadrature rule, for a <i>p</i>th order finite element, we use a
<i>(2p-1)</i>th order accurate formula to evaluate the integrals. Since the
product of two <i>p</i>th order basis functions when computing a mass matrix
gives a function with polynomial degree <i>2p</i> in each direction, the
integrals are not computed exactly.  However, the overall convergence
properties are not disturbed by the quadrature error on meshes with affine
element shapes with L2 errors proportional to <i>h<sup>p+1</sup></i>. Note
however that order reduction with sub-optimal convergence rates of the L2
error of <i>O(h<sup>p</sup>)</i> or even <i>O(h<sup>p-1</sup>)</i> for some 3D
setups has been reported <a href="https://dx.doi.org/10.1002/num.20353">in
literature</a> on deformed (non-affine) element shapes for wave equations
when the integrand is not a polynomial any more.

Apart from the fact that we avoid solving linear systems with this
type of elements when using explicit time-stepping, they come with two
other advantages. When we are using the sum-factorization approach to
evaluate the finite element operator (cf. step-37), we have to
evaluate the function at the quadrature points. In the case of
Gauss-Lobatto elements, where quadrature points and node points of the
finite element coincide, this operation is trivial since the value
of the function at the quadrature points is given by its one-dimensional
coefficients. In this way, the arithmetic work for the finite element operator
evaluation is reduced by approximately a factor of two compared to the generic
Gaussian quadrature.

To sum up the discussion, by using the right finite element and
quadrature rule combination, we end up with a scheme where we
only need to compute the right hand side vector corresponding
to the formulation above and then multiply it by the inverse of the
diagonal mass matrix in each time step. In practice, of course, we extract
the diagonal elements and invert them only once at the beginning of the
program.

<h3>Implementation of constraints</h3>

The usual way to handle constraints in <code>deal.II</code> is to use
the AffineConstraints class that builds a sparse matrix storing
information about which degrees of freedom (DoF) are constrained and
how they are constrained. This format uses an unnecessarily large
amount of memory since there are not so many different types of
constraints: for example, in the case of hanging nodes when using
linear finite element on every cell, most constraints have the form
$x_k = \frac 12 x_i + \frac 12 x_j$ where the coefficients $\frac 12$
are always the same and only $i,j,k$ are different. While storing this
redundant information is not a problem in general because it is only
needed once during matrix and right hand side assembly, it becomes a
bottleneck in the matrix-free approach since there this
information has to be accessed every time we apply the operator, and the
remaining components of the operator evaluation are so fast. Thus,
instead of an AffineConstraints object, MatrixFree uses a variable that
we call <code>constraint_pool</code> that collects the weights of the
different constraints. Then, only an identifier of each constraint in the
mesh instead of all the weights have to be stored. Moreover,
the constraints are not applied in a pre- and postprocessing step
but rather as we evaluate the finite element
operator. Therefore, the constraint information is embedded into the
variable <code>indices_local_to_global</code> that is used to extract
the cell information from the global vector. If a DoF is constrained,
the <code>indices_local_to_global</code> variable contains the global
indices of the DoFs that it is constrained to. Then, we have another
variable <code>constraint_indicator</code> at hand that holds, for
each cell, the local indices of DoFs that are constrained as well as
the identifier of the type of constraint. Fortunately, you will not see
these data structures in the example program since the class
<code>FEEvaluation</code> takes care of the constraints without user
interaction.

In the presence of hanging nodes, the diagonal mass matrix obtained on the
element level via the Gauss-Lobatto quadrature/node point procedure does not
directly translate to a diagonal global mass matrix, as following the
constraints on rows and columns would also add off-diagonal entries. As
explained in <a href="https://dx.doi.org/10.4208/cicp.101214.021015a">Kormann
(2016)</a>, interpolating constraints on a vector, which maintains the
diagonal shape of the mass matrix, is consistent with the equations up to an
error of the same magnitude as the quadrature error. In the program below, we
will simply assemble the diagonal of the mass matrix as if it were a vector to
enable this approximation.


<h3> Parallelization </h3>

The MatrixFree class comes with the option to be parallelized on three levels:
MPI parallelization on clusters of distributed nodes, thread parallelization
scheduled by the Threading Building Blocks library, and finally with a
vectorization by working on a batch of two (or more) cells via SIMD data type
(sometimes called cross-element or external vectorization).
As we have already discussed in step-37, you will
get best performance by using an instruction set specific to your system,
e.g. with the cmake variable <tt>-DCMAKE_CXX_FLAGS="-march=native"</tt>. The
MPI parallelization was already exploited in step-37. Here, we additionally
consider thread parallelization with TBB. This is fairly simple, as all we
need to do is to tell the initialization of the MatrixFree object about the
fact that we want to use a thread parallel scheme through the variable
MatrixFree::AdditionalData::thread_parallel_scheme. During setup, a dependency
graph is set up similar to the one described in the @ref workstream_paper ,
which allows to schedule the work of the @p local_apply function on chunks of
cells without several threads accessing the same vector indices. As opposed to
the WorkStream loops, some additional clever tricks to avoid global
synchronizations as described in <a
href="https://dx.doi.org/10.1109/eScience.2011.53">Kormann and Kronbichler
(2011)</a> are also applied.

Note that this program is designed to be run with a distributed triangulation
(parallel::distributed::Triangulation), which requires deal.II to be
configured with <a href="http://www.p4est.org/">p4est</a> as described
in the <a href="../../readme.html">deal.II ReadMe</a> file. However, a
non-distributed triangulation is also supported, in which case the
computation will be run in serial.

<h3> The test case </h3>

In our example, we choose the initial value to be \f{eqnarray*} u(x,t) =
\prod_{i=1}^{d} -4 \arctan \left(
\frac{m}{\sqrt{1-m^2}}\frac{\sin\left(\sqrt{1-m^2} t +c_2\right)}{\cosh(mx_i+c_1)}\right)
\f} and solve the equation over the time interval [-10,10]. The
constants are chosen to be $c_1=c_1=0$ and <i> m=0.5</i>. As mentioned
in step-25, in one dimension <i>u</i> as a function of <i>t</i> is the exact
solution of the sine-Gordon equation. For higher dimension, this is however
not the case.


examples/step-48/doc/results.dox
<h1>Results</h1>

<h3>Comparison with a sparse matrix</h3>

In order to demonstrate the gain in using the MatrixFree class instead of
the standard <code>deal.II</code> assembly routines for evaluating the
information from old time steps, we study a simple serial run of the code on a
nonadaptive mesh. Since much time is spent on evaluating the sine function, we
do not only show the numbers of the full sine-Gordon equation but also for the
wave equation (the sine-term skipped from the sine-Gordon equation). We use
both second and fourth order elements. The results are summarized in the
following table.

<table align="center" class="doxtable">
  <tr>
    <th>&nbsp;</th>
    <th colspan="3">wave equation</th>
    <th colspan="2">sine-Gordon</th>
  </tr>
  <tr>
    <th>&nbsp;</th>
    <th>MF</th>
    <th>SpMV</th>
    <th>dealii</th>
    <th>MF</th>
    <th>dealii</th>
  </tr>
  <tr>
    <td>2D, $\mathcal{Q}_2$</td>
    <td align="right"> 0.0106</td>
    <td align="right"> 0.00971</td>
    <td align="right"> 0.109</td>
    <td align="right"> 0.0243</td>
    <td align="right"> 0.124</td>
  </tr>
  <tr>
    <td>2D, $\mathcal{Q}_4$</td>
    <td align="right"> 0.0328</td>
    <td align="right"> 0.0706</td>
    <td align="right"> 0.528</td>
    <td align="right"> 0.0714</td>
    <td align="right"> 0.502</td>
   </tr>
   <tr>
    <td>3D, $\mathcal{Q}_2$</td>
    <td align="right"> 0.0151</td>
    <td align="right"> 0.0320</td>
    <td align="right"> 0.331</td>
    <td align="right"> 0.0376</td>
    <td align="right"> 0.364</td>
   </tr>
   <tr>
    <td>3D, $\mathcal{Q}_4$</td>
    <td align="right"> 0.0918</td>
    <td align="right"> 0.844</td>
    <td align="right"> 6.83</td>
    <td align="right"> 0.194</td>
    <td align="right"> 6.95</td>
   </tr>
</table>

It is apparent that the matrix-free code outperforms the standard assembly
routines in deal.II by far. In 3D and for fourth order elements, one operator
evaluation is also almost ten times as fast as a sparse matrix-vector
product.

<h3>Parallel run in 2D and 3D</h3>

We start with the program output obtained on a workstation with 12 cores / 24
threads (one Intel Xeon E5-2687W v4 CPU running at 3.2 GHz, hyperthreading
enabled), running the program in release mode:
@code
\$ make run
Number of MPI ranks:            1
Number of threads on each rank: 24
Vectorization over 4 doubles = 256 bits (AVX)

   Number of global active cells: 15412
   Number of degrees of freedom: 249065
   Time step size: 0.00292997, finest cell: 0.117188

   Time:     -10, solution norm:  9.5599
   Time:   -9.41, solution norm:  17.678
   Time:   -8.83, solution norm:  23.504
   Time:   -8.24, solution norm:    27.5
   Time:   -7.66, solution norm:  29.513
   Time:   -7.07, solution norm:  29.364
   Time:   -6.48, solution norm:   27.23
   Time:    -5.9, solution norm:  23.527
   Time:   -5.31, solution norm:  18.439
   Time:   -4.73, solution norm:  11.935
   Time:   -4.14, solution norm:  5.5284
   Time:   -3.55, solution norm:  8.0354
   Time:   -2.97, solution norm:  14.707
   Time:   -2.38, solution norm:      20
   Time:    -1.8, solution norm:  22.834
   Time:   -1.21, solution norm:  22.771
   Time:  -0.624, solution norm:  20.488
   Time: -0.0381, solution norm:  16.697
   Time:   0.548, solution norm:  11.221
   Time:    1.13, solution norm:  5.3912
   Time:    1.72, solution norm:  8.4528
   Time:    2.31, solution norm:  14.335
   Time:    2.89, solution norm:  18.555
   Time:    3.48, solution norm:  20.894
   Time:    4.06, solution norm:  21.305
   Time:    4.65, solution norm:  19.903
   Time:    5.24, solution norm:  16.864
   Time:    5.82, solution norm:  12.223
   Time:    6.41, solution norm:   6.758
   Time:    6.99, solution norm:  7.2423
   Time:    7.58, solution norm:  12.888
   Time:    8.17, solution norm:  17.273
   Time:    8.75, solution norm:  19.654
   Time:    9.34, solution norm:  19.838
   Time:    9.92, solution norm:  17.964
   Time:      10, solution norm:  17.595

   Performed 6826 time steps.
   Average wallclock time per time step: 0.0013453s
   Spent 14.976s on output and 9.1831s on computations.
@endcode

In 3D, the respective output looks like
@code
\$ make run
Number of MPI ranks:            1
Number of threads on each rank: 24
Vectorization over 4 doubles = 256 bits (AVX)

   Number of global active cells: 17592
   Number of degrees of freedom: 1193881
   Time step size: 0.0117233, finest cell: 0.46875

   Time:     -10, solution norm:  29.558
   Time:   -7.66, solution norm:  129.13
   Time:   -5.31, solution norm:  67.753
   Time:   -2.97, solution norm:  79.245
   Time:  -0.621, solution norm:  123.52
   Time:    1.72, solution norm:  43.525
   Time:    4.07, solution norm:  93.285
   Time:    6.41, solution norm:  97.722
   Time:    8.76, solution norm:  36.734
   Time:      10, solution norm:  94.115

   Performed 1706 time steps.
   Average wallclock time per time step: 0.0084542s
   Spent 16.766s on output and 14.423s on computations.
@endcode

It takes 0.008 seconds for one time step with more than a million
degrees of freedom (note that we would need many processors to reach such
numbers when solving linear systems).

If we replace the thread-parallelization by a pure MPI parallelization, the
timings change into:
@code
\$ mpirun -n 24 ./step-48
Number of MPI ranks:            24
Number of threads on each rank: 1
Vectorization over 4 doubles = 256 bits (AVX)
...
   Performed 1706 time steps.
   Average wallclock time per time step: 0.0051747s
   Spent 2.0535s on output and 8.828s on computations.
@endcode

We observe a dramatic speedup for the output (which makes sense, given that
most code of the output is not parallelized via threads, whereas it is for
MPI), but less than the theoretical factor of 12 we would expect from the
parallelism. More interestingly, the computations also get faster when
switching from the threads-only variant to the MPI-only variant. This is a
general observation for the MatrixFree framework (as of updating this data in
2019). The main reason is that the decisions regarding work on conflicting
cell batches made to enable execution in parallel are overly pessimistic:
While they ensure that no work on neighboring cells is done on different
threads at the same time, this conservative setting implies that data from
neighboring cells is also evicted from caches by the time neighbors get
touched. Furthermore, the current scheme is not able to provide a constant
load for all 24 threads for the given mesh with 17,592 cells.

The current program allows to also mix MPI parallelization with thread
parallelization. This is most beneficial when running programs on clusters
with multiple nodes, using MPI for the inter-node parallelization and threads
for the intra-node parallelization. On the workstation used above, we can run
threads in the hyperthreading region (i.e., using 2 threads for each of the 12
MPI ranks). An important setting for mixing MPI with threads is to ensure
proper binning of tasks to CPUs. On many clusters the placing is either
automatically via the `mpirun/mpiexec` environment, or there can be manual
settings. Here, we simply report the run times the plain version of the
program (noting that things could be improved towards the timings of the
MPI-only program when proper pinning is done):
@code
\$ mpirun -n 12 ./step-48
Number of MPI ranks:            12
Number of threads on each rank: 2
Vectorization over 4 doubles = 256 bits (AVX)
...
   Performed 1706 time steps.
   Average wallclock time per time step: 0.0056651s
   Spent 2.5175s on output and 9.6646s on computations.
@endcode



<h3>Possibilities for extensions</h3>

There are several things in this program that could be improved to make it
even more efficient (besides improved boundary conditions and physical
stuff as discussed in step-25):

<ul> <li> <b>Faster evaluation of sine terms:</b> As becomes obvious
  from the comparison of the plain wave equation and the sine-Gordon
  equation above, the evaluation of the sine terms dominates the total
  time for the finite element operator application. There are a few
  reasons for this: Firstly, the deal.II sine computation of a
  VectorizedArray field is not vectorized (as opposed to the rest of
  the operator application). This could be cured by handing the sine
  computation to a library with vectorized sine computations like
  Intel's math kernel library (MKL). By using the function
  <code>vdSin</code> in MKL, the program uses half the computing time
  in 2D and 40 percent less time in 3D. On the other hand, the sine
  computation is structurally much more complicated than the simple
  arithmetic operations like additions and multiplications in the rest
  of the local operation.

  <li> <b>Higher order time stepping:</b> While the implementation allows for
  arbitrary order in the spatial part (by adjusting the degree of the finite
  element), the time stepping scheme is a standard second-order leap-frog
  scheme. Since solutions in wave propagation problems are usually very
  smooth, the error is likely dominated by the time stepping part. Of course,
  this could be cured by using smaller time steps (at a fixed spatial
  resolution), but it would be more efficient to use higher order time
  stepping as well. While it would be straight-forward to do so for a
  first-order system (use some Runge&ndash;Kutta scheme of higher order,
  probably combined with adaptive time step selection like the <a
  href="http://en.wikipedia.org/wiki/Dormand%E2%80%93Prince_method">Dormand&ndash;Prince
  method</a>), it is more challenging for the second-order formulation. At
  least in the finite difference community, people usually use the PDE to find
  spatial correction terms that improve the temporal error.

</ul>


examples/step-49/doc/intro.dox
<i>This program was contributed by Timo Heister. Parts of the results section
were contributed by Yuhan Zhou, Wolfgang Bangerth, and David Wells.</i>

<a name="Intro"></a>
<h1> Introduction </h1>
This tutorial is an extension to step-1 and demonstrates several ways to
obtain more involved meshes than the ones shown there.

@note This tutorial is also available as a Jupyter Python notebook that
  uses the deal.II python interface. The notebook is available in the
  same directory as the original C++ program.

Generating complex geometries is a challenging task, especially in three space
dimensions. We will discuss several ways to do this, but this list is not
exhaustive. Additionally, there is not one approach that fits all problems.

This example program shows some of ways to create and modify meshes for
computations and outputs them as <code>.vtu</code> files in much the same way
as we do in step-1. No other computations or adaptive
refinements are done; the idea is that you can use the techniques used here as
building blocks in other, more involved simulators. Please note that the
example program does not show all the ways to generate meshes that are
discussed in this introduction.


<h3>General concerns about meshes</h3>

When you use adaptive mesh refinement, you definitely want the initial mesh to
be as coarse as possible. The reason is that you can make it as fine as you
want using adaptive refinement as long as you have memory and CPU time
available. However, this requires that you don't waste mesh cells in parts of
the domain where they don't pay off. As a consequence, you don't want to start
with a mesh that is too fine to start with, because that takes up a good part
of your cell budget already, and because you can't coarsen away cells that are
in the initial mesh.

That said, your mesh needs to capture the given geometry adequately.


<h3>How to create meshes</h3>

There are several ways to create an initial mesh. Meshes can be modified or
combined in many ways as discussed later on.

<h4>Using GridGenerator</h4>

The easiest way to generate meshes is to use the functions in namespace
GridGenerator, as already discussed in step-1.  There are many different
helper functions
available, including GridGenerator::hyper_cube(),
GridGenerator::hyper_shell(), GridGenerator::hyper_ball(),
and GridGenerator::hyper_cube_with_cylindrical_hole().


<h4>Constructing your own mesh programmatically</h4>

If there is no good fit in the GridGenerator namespace for what you want to
do, you can always create a
Triangulation in your program "by hand". For that, you need a list of vertices
with their coordinates and a list of cells referencing those vertices. You can
find an example in the function <tt>create_coarse_grid()</tt> in step-14.
All the functions in GridGenerator are implemented in this fashion.

We are happy to accept more functions to be added to GridGenerator. So, if
you end up writing a function that might be useful for a larger audience,
please contribute it.


<h4>Importing from external programs</h4>

The class GridIn can read many different mesh formats from a file from
disk. How this is done is explained in step-5 and can be seen in the function
<code>grid_1</code> in this example, see the code below.

Meshes can be generated from different tools like <a
href="http://gmsh.info" target="_top">gmsh</a>, <a
href="https://lagrit.lanl.gov/" target="_top">lagrit</a> and <a
href="http://cubit.sandia.gov/" target="_top">cubit</a>. See the
documentation of GridIn for more information. The problem is that deal.II
needs meshes that only consist of quadrilaterals and hexahedra -- tetrahedral
meshes won't work (this means tools like tetgen can not be used directly).

We will describe a possible workflow using %Gmsh. %Gmsh is the smallest and
most quickly set up open source tool we are aware of. It can generate
unstructured 2d quad meshes. In 3d, it can extrude 2d meshes to
get hexahedral meshes; 3D meshing of unstructured geometry into hexahedra is
possible, though there are some issues with the quality of these meshes
that imply that these meshes only sometimes work in deal.II.

In %Gmsh, a mesh is fundamentally described in a text-based
<code>.geo</code> file whose format can
contain computations, loops, variables, etc. This format is quite flexible
in allowing the description of complex geometries. The mesh is then
generated from a surface representation, which is built from a list of line
loops, which is built from a list of lines, which are in turn built from
points. The <code>.geo</code> script can be written and edited by hand or it
can be generated automatically by creating objects graphically inside %Gmsh. In
many cases it is best to combine both approaches. The file can be easily
reloaded by pressing "reload" under the "Geometry" tab if you want to write
it by hand and see the effects in the graphical user interface of gmsh.

This tutorial contains an example <code>.geo</code> file that describes a box
with two objects cut out in the interior. This is how
<code>example.geo</code> looks like in %Gmsh (displaying the boundary
indicators as well as the mesh discussed further down below):

<img src="https://www.dealii.org/images/steps/developer/step-49.gmsh_picture.png" alt="">

You might want to open the <code>example.geo</code> file in a text editor (it
is located in the same directory as the <tt>step-49.cc</tt> source file) to
see how it is structured. You can see how the boundary of the domain is
composed of a number of lines and how later on we combine several lines into
"physical lines" (or "physical surfaces") that list the logical lines'
numbers. "Physical" object are the ones that carry information about the
boundary indicator (see @ref GlossBoundaryIndicator "this glossary entry").

@note It is important that this file contain "physical lines" and "physical
  surfaces". These give the boundary indicators and material ids for use
  in deal.II. Without these physical entities, nothing will be imported into
  deal.II.

deal.II's GridIn class can read the <code>.msh</code> format written by
%Gmsh and that contains a mesh created for the geometry described by the
<code>.geo</code> file. You generate the <code>.msh</code> from the
<code>.geo</code> by running the commands

@code
gmsh -2 example.geo
@endcode

on the command line, or by clicking "Mesh" and then "2D" inside %Gmsh after
loading the file.  Now this is the mesh read from the <code>.msh</code> file
and saved again by deal.II as an image (see the <code>grid_1</code> function
of the current program):

<img src="https://www.dealii.org/images/steps/developer/step-49.grid-1.png" alt="">

@note %Gmsh has a number of other interfaces by which one can describe
  geometries to it. In particular, it has the ability to interface with
  scripting languages like Python and Julia, but it can also be scripted
  from C++. These interfaces are useful if one doesn't just want to generate
  a mesh for a single geometry (in which case the graphical interface or,
  in simple cases, a hand-written `.geo` file is probably the simplest
  approach), but instead wants to do parametric studies over the geometry
  for which it is necessary to generate many meshes for geometries that
  differ in certain parameters. Another case where this is useful is if there
  is already a CAD geometry for which one only needs a mesh; indeed, this
  can be done from within deal.II using the
  Gmsh::create_triangulation_from_boundary_curve() function.


<h3>Modifying a Mesh</h3>

After acquiring one (or several) meshes in the ways described above, there are
many ways to manipulate them before using them in a finite element
computation.


<h4>Transformations</h4>

The GridTools namespace contains a collection of small functions to transform
a given mesh in various ways. The usage of the functions GridTools::shift,
GridTools::rotate, GridTools::scale is fairly obvious, so we won't discuss
those functions here.

The function GridTools::transform allows you to transform the vertices of a
given mesh using a smooth function. An example of its use is also given in the
results section of step-38 but let us show a simpler example here:
In the function <code>grid_5()</code> of the current program, we perturb the y
coordinate of a mesh with a sine curve:

<table width="60%" align="center">
  <tr>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-49.grid-5a.png" alt=""> regular input mesh
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-49.grid-5.png" alt=""> output mesh
    </td>
  </tr>
</table>

Similarly, we can transform a regularly refined
unit square to a wall-adapted mesh in y direction using the formula
$(x,y) \mapsto (x,\tanh(2 y)/\tanh(2))$. This is done in <code>grid_6()</code>
of this tutorial:
<table width="60%" align="center">
  <tr>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-49.grid-6a.png" alt=""> regular input mesh
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-49.grid-6.png" alt=""> wall-adapted output mesh
    </td>
  </tr>
</table>

Finally, the function GridTools::distort_random allows you to move vertices in the
mesh (optionally ignoring boundary nodes) by a random amount. This is
demonstrated in <code>grid_7()</code> and the result is as follows:

<table width="60%" align="center">
  <tr>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-49.grid-7a.png" alt=""> regular input mesh
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-49.grid-7.png" alt=""> perturbed output mesh
    </td>
  </tr>
</table>

This function is primarily intended to negate some of the superconvergence
effects one gets when studying convergence on regular meshes, as well as to
suppress some optimizations in deal.II that can exploit the fact that cells
are similar in shape. (Superconvergence refers to the fact that if a mesh
has certain symmetries -- for example, if the edges running into a vertex
are symmetric to this vertex, and if this is so for all vertices of a cell
-- that the solution is then often convergent with a higher order than one
would have expected from the usual error analysis. In the end, this
is a result of the fact that if one were to make a Taylor expansion of the
error, the symmetry leads to the fact that the expected next term of the
expansion happens to be zero, and the error order is determined by the
*second next* term. A distorted mesh does not have these symmetries and
consequently the error reflects what one will see when solving the equation
on *any* kind of mesh, rather than showing something that is only reflective
of a particular situation.)


<h4>Merging Meshes</h4>

The function GridGenerator::merge_triangulations() allows you to merge two
given Triangulation objects into a single one.  For this to work, the vertices
of the shared edge or face have to match exactly.  Lining up the two meshes
can be achieved using GridTools::shift and GridTools::scale.  In the function
<code>grid_2()</code> of this tutorial, we merge a square with a round hole
(generated with GridGenerator::hyper_cube_with_cylindrical_hole()) and a
rectangle (generated with GridGenerator::subdivided_hyper_rectangle()). The
function GridGenerator::subdivided_hyper_rectangle() allows you to specify the
number of repetitions and the positions of the corners, so there is no need to
shift the triangulation manually here. You should inspect the mesh graphically
to make sure that cells line up correctly and no unpaired nodes exist in the
merged Triangulation.

These are the input meshes and the output mesh:

<table width="80%" align="center">
  <tr>
    <td align="center"><img src="https://www.dealii.org/images/steps/developer/step-49.grid-2a.png" alt="" height="200px">input mesh 1</td>
    <td align="center"><img src="https://www.dealii.org/images/steps/developer/step-49.grid-2b.png" alt="" height="200px">input mesh 2</td>
    <td align="center"><img src="https://www.dealii.org/images/steps/developer/step-49.grid-2.png" alt="" height="200px">merged mesh</td>
  </tr>
</table>


<h4>Moving Vertices</h4>

The function <code>grid_3()</code> demonstrates the ability to pick individual vertices and
move them around in an existing mesh. Note that this has the potential to produce degenerate
or inverted cells and you shouldn't expect anything useful to come of using
such meshes. Here, we create a box with a cylindrical hole that is not exactly
centered by moving the top vertices upwards:

<table width="60%" align="center">
  <tr>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-49.grid-3a.png" alt="" height="200px"> input mesh
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-49.grid-3.png" alt="" height="200px"> top vertices moved upwards
    </td>
  </tr>
</table>

For the exact way how this is done, see the code below.


<h4>Extruding Meshes</h4>

If you need a 3d mesh that can be created by extruding a given 2d mesh (that
can be created in any of the ways given above), you can use the function
GridGenerator::extrude_triangulation(). See the <code>grid_4()</code> function
in this tutorial for an example. Note that for this particular case, the given
result could also be achieved using the 3d version of
GridGenerator::hyper_cube_with_cylindrical_hole(). The main usage is a 2d
mesh, generated for example with %Gmsh, that is read in from a
<code>.msh</code> file as described above. This is the output from grid_4():

<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-49.grid-4base.png" alt=""> input mesh
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-49.grid-4.png" alt=""> extruded output mesh
    </td>
  </tr>
</table>


<h3> After you have a coarse mesh </h3>

Creating a coarse mesh using the methods discussed above is only the first
step. When you have it, it will typically serve as the basis for further mesh
refinement. This is not difficult &mdash; in fact, there is nothing else to do
&mdash; if your geometry consists of only straight faces. However, this is
often not the case if you have a more complex geometry and more steps than
just creating the mesh are necessary. We will go over some of these steps in
the <a href="#Results">results section</a> below.


examples/step-49/doc/results.dox
<h1>Results</h1>

The program produces a series of <code>.vtu</code> files of the
triangulations. The methods are discussed above.


<h3>Next steps: Curved Cells</h3>

As mentioned in the introduction, creating a coarse mesh using the methods
discussed here is only the first step. In order to refine a mesh, the
Triangulation needs to know where to put new vertices on the mid-points of
edges, faces, and cells. By default, these new points will be placed at the
arithmetic mean of the surrounding points, but this isn't what you want if you
need curved boundaries that aren't already adequately resolved by the coarse
mesh. For example, for this mesh the central hole is supposed to be round:

<img src="https://www.dealii.org/images/steps/developer/step-49.grid-2a.png" alt="" height="200px">

If you simply refine it, the Triangulation class can not know whether you wanted
the hole to be round or to be an octagon. The default is to place new points
along existing straight lines. After two mesh refinement steps, this would yield
the following mesh, which is not what we wanted:

<img src="https://www.dealii.org/images/steps/developer/step-49.grid-2d-refined.png" alt="" height="200px">

What needs to happen is that you tell the triangulation that you in fact want
to use a curved geometry. The way to do this requires three steps:
- Create an object that describes the desired geometry. This object will be
  queried when refining the Triangulation for new point placement. It will also
  be used to calculate shape function values if a high degree mapping, like
  MappingQ or MappingQGeneric, is used during system assembly.
  In deal.II the Manifold class and classes inheriting from it (e.g.,
  PolarManifold and FlatManifold) perform these calculations.
- Notify the Triangulation object which Manifold classes to use. By default, a
  Triangulation uses FlatManifold to do all geometric calculations,
  which assumes that all cell edges are straight lines and all quadrilaterals
  are flat. You can attach Manifold classes to a Triangulation by calling
  Triangulation::set_manifold function, which associates a
  <code>manifold_id</code> with a Manifold object. For more information on this
  see the @ref GlossManifoldIndicator "glossary entry on this topic".
- Finally, you must mark cells and cell faces with the correct
  <code>manifold_id</code>. For example, you could get an annular sector with
  curved cells in Cartesian coordinates (but rectangles in polar coordinates)
  by doing the following:
  @code
  Triangulation<2> tria;
  GridGenerator::hyper_cube(tria);
  const auto cell = tria.begin_active();
  cell->vertex(2) = Point<2>(-0.5, 1.0);
  cell->vertex(3) = Point<2>(1.5, 1.0);
  tria.set_all_manifold_ids(42);
  tria.set_manifold(42, PolarManifold<2>(Point<2>(0.5, -1.0)));
  tria.refine_global(3);
  @endcode
  Now, when the grid is refined, all cell splitting calculations will be done in
  polar coordinates.

All functions in the GridGenerator namespace which create a mesh where some
cells should be curved also attach the correct Manifold object to the provided
Triangulation: i.e., for those functions we get the correct behavior by
default. For a hand-generated mesh, however, the situation is much more
interesting.

To illustrate this process in more detail, let us consider an example created
by Yuhan Zhou as part of a 2013 semester project at Texas A&amp;M University.
The goal was to generate (and use) a geometry that describes a
microstructured electric device. In a CAD program, the geometry looks like
this:

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.1.png" alt="">

In the following, we will walk you through the entire process of creating a
mesh for this geometry, including a number of common pitfalls by showing the
things that can go wrong.

The first step in getting there was to create a coarse mesh, which was done by
creating a 2d coarse mesh for each of cross sections, extruding them into the
third direction, and gluing them together. The following code does this, using
the techniques previously described:

@code
// Given a list of points and how vertices connect to cells, create a
// mesh. This is in the same way as we do in step 14.
void create_2d_grid(
  const std::vector<Point<2>> &vertices,
  const std::vector<
    std::array<unsigned int, GeometryInfo<2>::vertices_per_cell>>
    &               vertex_indices,
  Triangulation<2> &coarse_grid)
{
  std::vector<CellData<2>> cells(vertex_indices.size());
  for (unsigned int i = 0; i < cells.size(); ++i)
    {
      for (unsigned int j = 0; j < vertex_indices[i].size(); ++j)
        cells[i].vertices[j] = vertex_indices[i][j];
    }

  coarse_grid.create_triangulation(vertices, cells, SubCellData());
}


// Create a triangulation that covers the entire volume
void create_3d_grid(Triangulation<3> &triangulation)
{
  // Generate first cross section
  const std::vector<Point<2>> vertices_1{{-1.5, 0.},
                                         {-0.5, 0.},
                                         {0.5, 0.},
                                         {1.5, 0.},

                                         {-1.5, 1.5},
                                         {-0.5, 1.5},
                                         {0.5, 1.5},
                                         {1.5, 1.5},

                                         {-1.5, 3.},
                                         {-0.5, 3.},
                                         {0.5, 3.},
                                         {1.5, 3.},

                                         {-0.5, 3 + 0.5 * sqrt(3)},
                                         {0.5, 3 + 0.5 * sqrt(3)},

                                         {-0.75, 3 + 0.75 * sqrt(3)},
                                         {0.75, 3 + 0.75 * sqrt(3)}};

  const std::vector<std::array<unsigned int, GeometryInfo<2>::vertices_per_cell>>
    cell_vertices_1 = {{{0, 1, 4, 5}},
                       {{1, 2, 5, 6}},
                       {{3, 7, 2, 6}},
                       {{4, 5, 8, 9}},
                       {{5, 6, 9, 10}},
                       {{7, 11, 6, 10}},
                       {{8, 9, 14, 12}},
                       {{9, 10, 12, 13}},
                       {{11, 15, 10, 13}},
                       {{14, 12, 15, 13}}};

  // Copy vertices into a 2d triangulation
  Triangulation<2> triangulation_2d_1;
  create_2d_grid(vertices_1, cell_vertices_1, triangulation_2d_1);

  // Then extrude it into a 3d piece
  Triangulation<3> triangulation_3d_1;
  GridGenerator::extrude_triangulation(triangulation_2d_1,
                                       5,
                                       2.5,
                                       triangulation_3d_1);

  // Now do the same with the second volume
  const std::vector<Point<2>> vertices_2{{-2.5, 0.},
                                         {-1.5, 0.},
                                         {-0.5, 0.},
                                         {0.5, 0.},
                                         {1.5, 0.},
                                         {2.5, 0.},

                                         {-2.5, 1.5},
                                         {-1.5, 1.5},
                                         {-0.5, 1.5},
                                         {0.5, 1.5},
                                         {1.5, 1.5},
                                         {2.5, 1.5},

                                         {-2.5, 3.},
                                         {-1.5, 3.},
                                         {-0.5, 3.},
                                         {0.5, 3.},
                                         {1.5, 3.},
                                         {2.5, 3.},

                                         {-0.5, 3. + 0.5 * sqrt(3)},
                                         {0.5, 3. + 0.5 * sqrt(3)},

                                         {-0.75, 3. + 0.75 * sqrt(3)},
                                         {0.75, 3. + 0.75 * sqrt(3)},

                                         {-1.25, 3. + 1.25 * sqrt(3)},
                                         {1.25, 3. + 1.25 * sqrt(3)}};

  const std::vector<std::array<unsigned int, GeometryInfo<2>::vertices_per_cell>>
    cell_vertices_2 = {{{0, 1, 6, 7}},
                       {{1, 2, 7, 8}},
                       {{2, 3, 8, 9}},
                       {{4, 10, 3, 9}},
                       {{5, 11, 4, 10}},
                       {{6, 7, 12, 13}},
                       {{7, 8, 13, 14}},
                       {{8, 9, 14, 15}},
                       {{10, 16, 9, 15}},
                       {{11, 17, 10, 16}},
                       {{12, 13, 22, 20}},
                       {{13, 14, 20, 18}},
                       {{14, 15, 18, 19}},
                       {{16, 21, 15, 19}},
                       {{17, 23, 16, 21}},
                       {{20, 18, 21, 19}},
                       {{22, 20, 23, 21}}};

  Triangulation<2> triangulation_2d_2;
  create_2d_grid(vertices_2, cell_vertices_2, triangulation_2d_2);

  Triangulation<3> triangulation_3d_2;
  GridGenerator::extrude_triangulation(triangulation_2d_2,
                                       5,
                                       2.5,
                                       triangulation_3d_2);

  // Also shift this triangulation in the z-direction so that it matches the
  // end face of the first part
  GridTools::shift(Point<3>(0, 0, 2.5), triangulation_3d_2);

  // Now first merge these two pieces, then shift the first piece in
  // z-direction beyond the second, and merge the shifted piece with the two
  // previously merged one into the final one:
  Triangulation<3> triangulation_3d_tmp;
  GridGenerator::merge_triangulations(triangulation_3d_1,
                                      triangulation_3d_2,
                                      triangulation_3d_tmp);

  GridTools::shift(Point<3>(0, 0, 5), triangulation_3d_1);

  GridGenerator::merge_triangulations(triangulation_3d_tmp,
                                      triangulation_3d_1,
                                      triangulation);
}
@endcode

This creates the following mesh:

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.8.png"
     alt="" width="400" height="355">

This mesh has the right general shape, but the top cells are now polygonal: their
edges are no longer along circles and we do not have a very accurate
representation of the original geometry. The next step is to teach the top part
of the domain that it should be curved. Put another way, all calculations done
on the top boundary cells should be done in cylindrical coordinates rather than
Cartesian coordinates. We can do this by creating a CylindricalManifold object
and associating it with the cells above $y = 3$. This way, when we refine the
cells on top, we will place new points along concentric circles instead of
straight lines.

In deal.II we describe all geometries with classes that inherit from
Manifold. The default geometry is Cartesian and is implemented in the
FlatManifold class. As the name suggests, Manifold and its inheriting classes
provide a way to describe curves and curved cells in a general way with ideas
and terminology from differential geometry: for example, CylindricalManifold
inherits from ChartManifold, which describes a geometry through pull backs
and push forwards. In general, one should think that the Triangulation class
describes the topology of a domain (in addition, of course, to storing the
locations of the vertices) while the Manifold classes describe the geometry of a
domain (e.g., whether or not a pair of vertices lie along a circular arc or a
straight line). A Triangulation will refine cells by doing computations with the
Manifold associated with that cell regardless of whether or not the cell is on
the boundary. Put another way: the Manifold classes do not need any information
about where the boundary of the Triangulation actually is: it is up to the
Triangulation to query the right Manifold for calculations on a cell. Most
Manifold functions (e.g., Manifold::get_intermediate_point) know nothing about
the domain itself and just assume that the points given to it lie along a
geodesic. In this case, with the CylindricalManifold constructed below, the
geodesics are arcs along circles orthogonal to the $z$-axis centered along the
line $(0, 3, z)$.

Since all three top parts of the domain use the same geodesics, we will
mark all cells with centers above the $y = 3$ line as being cylindrical in
nature:

@code
const Tensor<1, 3>           axis({0.0, 0.0, 1.0});
const Point<3>               axial_point(0, 3.0, 0.0);
const CylindricalManifold<3> cylinder(axis, axial_point);
const types::manifold_id     cylinder_id = 8;

Triangulation<3> triangulation;
create_3d_grid(triangulation);
triangulation.set_manifold(cylinder_id, cylinder);

for (auto &cell : triangulation.active_cell_iterators())
  if (cell->center()[1] >= 3.0)
    cell->set_all_manifold_ids(cylinder_id);

triangulation.refine_global(1);
@endcode

With this code, we get a mesh that looks like this:

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.9.png"
     alt="" width="400" height="355">

This change fixes the boundary but creates a new problem: the cells adjacent to
the cylinder's axis are badly distorted. We should use Cartesian coordinates for
calculations on these central cells to avoid this issue. The cells along the
center line all have a face that touches the line $(0, 3, z)$ so, to implement
this, we go back and overwrite the <code>manifold_id</code>s on these cells to
be zero (which is the default):

@code
const Tensor<1, 3>           axis({0.0, 0.0, 1.0});
const Point<3>               axial_point(0, 3.0, 0.0);
const CylindricalManifold<3> cylinder(axis, axial_point);
const types::manifold_id     cylinder_id = 8;

Triangulation<3> triangulation;
create_3d_grid(triangulation);
triangulation.set_manifold(cylinder_id, cylinder);

for (auto &cell : triangulation.active_cell_iterators())
  if (cell->center()[1] >= 3.0)
    cell->set_all_manifold_ids(cylinder_id);

for (const auto &cell : triangulation.active_cell_iterators())
  for (const auto &face : cell->face_iterators())
    {
      const Point<3> face_center = face->center();
      if (std::abs(face_center[0]) < 1.0e-5 &&
          std::abs(face_center[1] - 3.0) < 1.0e-5)
        cell->set_all_manifold_ids(numbers::flat_manifold_id);
    }

triangulation.refine_global(1);
@endcode

This gives us the following grid:

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.10.png"
     alt="" width="400" height="355">

This gives us a good mesh, where cells at the center of each circle are still
Cartesian and cells around the boundary lie along a circle. We can really see
the nice detail of the boundary fitted mesh if we refine two more times:

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.11.png"
     alt="" width="400" height="355">


<h3> Possibilities for extensions </h3>

<h4> Assigning different boundary ids </h4>

It is often useful to assign different boundary ids to a mesh that is
generated in one form or another as described in this tutorial to apply
different boundary conditions.

For example, you might want to apply a different boundary condition for the
right boundary of the first grid in this program. To do this, iterate over the
cells and their faces and identify the correct faces (for example using
`cell->center()` to query the coordinates of the center of a cell as we
do in step-1, or using `cell->face(f)->get_boundary_id()` to query the current
boundary indicator of the $f$th face of the cell). You can then use
`cell->face(f)->set_boundary_id()` to set the boundary id to something different.
You can take a look back at step-1 how iteration over the meshes is done there.

<h4> Extracting a boundary mesh </h4>

Computations on manifolds, like they are done in step-38, require a surface
mesh embedded into a higher dimensional space. While some can be constructed
using the GridGenerator namespace or loaded from a file, it is sometimes
useful to extract a surface mesh from a volume mesh.

Use the function GridGenerator::extract_boundary_mesh() to extract the surface
elements of a mesh. Using the function on a 3d mesh (a `Triangulation<3,3>`, for
example from `grid_4()`), this will return a `Triangulation<2,3>` that you can use
in step-38.  Also try extracting the boundary mesh of a `Triangulation<2,2>`.


<!--

Possible Extensions for this tutorial:

- Database of unstructured meshes for convergence studies
- how to remove or disable a cell inside a mesh
-->


examples/step-5/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

@dealiiVideoLecture{14}

This example does not show revolutionary new things, but it shows many
small improvements over the previous examples, and also many small
things that can usually be found in finite element programs. Among
them are:
<ul>
  <li> Computations on successively refined grids. At least in the
       mathematical sciences, it is common to compute solutions on
       a hierarchy of grids, in order to get a feeling for the accuracy
       of the solution; if you only have one solution on a single grid, you
       usually can't guess the accuracy of the
       solution. Furthermore, deal.II is designed to support adaptive
       algorithms where iterative solution on successively refined
       grids is at the heart of algorithms. Although adaptive grids
       are not used in this example, the foundations for them is laid
       here.
  <li> In practical applications, the domains are often subdivided
       into triangulations by automatic mesh generators. In order to
       use them, it is important to read coarse grids from a file. In
       this example, we will read a coarse grid in UCD (unstructured
       cell data) format. When this program was first written around
       2000, UCD format was what the AVS Explorer used -- a program
       reasonably widely used at the time but now no longer of
       importance. (Nonetheless, the file format has survived and is
       still understood by a number of programs.)
  <li> Finite element programs usually use extensive amounts of
       computing time, so some optimizations are sometimes
       necessary. We will show some of them.
  <li> On the other hand, finite element programs tend to be rather
       complex, so debugging is an important aspect. We support safe
       programming by using assertions that check the validity of
       parameters and %internal states in a debug mode, but are removed
       in optimized mode. (@dealiiVideoLectureSeeAlso{18})
  <li> Regarding the mathematical side, we show how to support a
       variable coefficient in the elliptic operator and how to use
       preconditioned iterative solvers for the linear systems of
       equations.
</ul>

The equation to solve here is as follows:
@f{align*}
  -\nabla \cdot a(\mathbf x) \nabla u(\mathbf x) &= 1 \qquad\qquad & \text{in}\ \Omega,
  \\
  u &= 0 \qquad\qquad & \text{on}\ \partial\Omega.
@f}
If $a(\mathbf x)$ was a constant coefficient, this would simply be the Poisson
equation. However, if it is indeed spatially variable, it is a more complex
equation (often referred to as the "extended Poisson equation"). Depending on
what the variable $u$ refers to it models a variety of situations with wide
applicability:

- If $u$ is the electric potential, then $-a\nabla u$ is the electric current
  in a medium and the coefficient $a$ is the conductivity of the medium at any
  given point. (In this situation, the right hand side of the equation would
  be the electric source density and would usually be zero or consist of
  localized, Delta-like, functions.)
- If $u$ is the vertical deflection of a thin membrane, then $a$ would be a
  measure of the local stiffness. This is the interpretation that will allow
  us to interpret the images shown in the results section below.

Since the Laplace/Poisson equation appears in so many contexts, there are many
more interpretations than just the two listed above.

When assembling the linear system for this equation, we need the weak form
which here reads as follows:
@f{align*}
  (a \nabla \varphi, \nabla u) &= (\varphi, 1) \qquad \qquad \forall \varphi.
@f}
The implementation in the <code>assemble_system</code> function follows
immediately from this.


examples/step-5/doc/results.dox
<h1>Results</h1>


Here is the console output:
@code
Cycle 0:
   Number of active cells: 20
   Total number of cells: 20
   Number of degrees of freedom: 25
   13 CG iterations needed to obtain convergence.
Cycle 1:
   Number of active cells: 80
   Total number of cells: 100
   Number of degrees of freedom: 89
   18 CG iterations needed to obtain convergence.
Cycle 2:
   Number of active cells: 320
   Total number of cells: 420
   Number of degrees of freedom: 337
   29 CG iterations needed to obtain convergence.
Cycle 3:
   Number of active cells: 1280
   Total number of cells: 1700
   Number of degrees of freedom: 1313
   52 CG iterations needed to obtain convergence.
Cycle 4:
   Number of active cells: 5120
   Total number of cells: 6820
   Number of degrees of freedom: 5185
   95 CG iterations needed to obtain convergence.
Cycle 5:
   Number of active cells: 20480
   Total number of cells: 27300
   Number of degrees of freedom: 20609
   182 CG iterations needed to obtain convergence.
@endcode



In each cycle, the number of cells quadruples and the number of CG
iterations roughly doubles.
Also, in each cycle, the program writes one output graphic file in VTU
format. They are depicted in the following:

<table width="100%">
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-5.solution-0-r9.2.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-5.solution-1-r9.2.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-5.solution-2-r9.2.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-5.solution-3-r9.2.png" alt="">
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-5.solution-4-r9.2.png" alt="">
    </td>
    <td>
      <img src="https://www.dealii.org/images/steps/developer/step-5.solution-5-r9.2.png" alt="">
    </td>
  </tr>
</table>



Due to the variable coefficient (the curvature there is reduced by the
same factor by which the coefficient is increased), the top region of
the solution is flattened. The gradient of the solution is
discontinuous along the interface, although this is not very clearly
visible in the pictures above. We will look at this in more detail in
the next example.

The pictures also show that the solution computed by this program is
actually pretty wrong on a very coarse mesh (its magnitude is
wrong). That's because no numerical method guarantees that the solution
on a coarse mesh is particularly accurate -- but we know that the
solution <i>converges</i> to the exact solution, and indeed you can
see how the solutions from one mesh to the next seem to not change
very much any more at the end.


examples/step-50/doc/intro.dox
<br>

<i>
This program was contributed by Thomas C. Clevenger and Timo Heister.
<br>
This material is based upon work partly supported by the National
Science Foundation Award DMS-2028346, OAC-2015848, EAR-1925575, by the Computational
Infrastructure in Geodynamics initiative (CIG), through the NSF under Award
EAR-0949446 and EAR-1550901 and The University of California -- Davis.
</i>

@dealiiTutorialDOI{10.5281/zenodo.4004166,https://zenodo.org/badge/DOI/10.5281/zenodo.4004166.svg}

@note As a prerequisite of this program, you need to have both p4est and either the PETSc
or Trilinos library installed. The installation of deal.II together with these additional
libraries is described in the <a href="../../readme.html" target="body">README</a> file.


<a name="Intro"></a>
<h1>Introduction</h1>


This example shows the usage of the multilevel functions in deal.II on
parallel, distributed
meshes and gives a comparison between geometric and algebraic multigrid methods.
The algebraic multigrid (AMG) preconditioner is the same used in step-40. Two geometric
multigrid (GMG) preconditioners are considered: a matrix-based version similar to that
in step-16 (but for parallel computations) and a matrix-free version
discussed in step-37. The goal is to find out which approach leads to
the best solver for large parallel computations.

This tutorial is based on one of the numerical examples in
@cite clevenger_par_gmg. Please see that publication for a detailed background
on the multigrid implementation in deal.II. We will summarize some of the
results in the following text.

Algebraic multigrid methods are obviously the easiest to implement
with deal.II since classes such as TrilinosWrappers::PreconditionAMG
and PETScWrappers::PreconditionBoomerAMG are, in essence, black box
preconditioners that require only a couple of lines to set up even for
parallel computations. On the other hand, geometric multigrid methods
require changes throughout a code base -- not very many, but one has
to know what one is doing.

What the results of this program will show
is that algebraic and geometric multigrid methods are roughly
comparable in performance <i>when using matrix-based formulations</i>,
and that matrix-free geometric multigrid methods are vastly better for
the problem under consideration here. A secondary conclusion will be
that matrix-based geometric multigrid methods really don't scale well
strongly when the number of unknowns per processor becomes smaller than
20,000 or so.


<h3>The testcase</h3>

We consider the variable-coefficient Laplacian weak formulation
@f{align*}
 (\epsilon \nabla u, \nabla v) = (f,v) \quad \forall v \in V_h
@f}
on the domain $\Omega = [-1,1]^\text{dim} \setminus [0,1]^\text{dim}$ (an L-shaped domain
for 2D and a Fichera corner for 3D) with $\epsilon = 1$ if $\min(x,y,z)>-\frac{1}{2}$ and
$\epsilon = 100$ otherwise. In other words, $\epsilon$ is small along the edges
or faces of the domain that run into the reentrant corner, as will be visible
in the figure below.

The boundary conditions are $u=0$ on the whole boundary and
the right-hand side is $f=1$. We use continuous $Q_2$ elements for the
discrete finite element space $V_h$, and use a
residual-based, cell-wise a posteriori error estimator
$e(K) = e_{\text{cell}}(K) + e_{\text{face}}(K)$ from @cite karakashian2003posteriori with
@f{align*}
 e_{\text{cell}}(K) &= h^2 \| f + \epsilon \triangle u \|_K^2, \\
 e_{\text{face}}(K) &= \sum_F h_F \| \jump{ \epsilon \nabla u \cdot n } \|_F^2,
@f}
to adaptively refine the mesh. (This is a generalization of the Kelly
error estimator used in the KellyErrorEstimator class that drives mesh
refinement in most of the other tutorial programs.)
The following figure visualizes the solution and refinement for 2D:
<img width="400px" src="https://www.dealii.org/images/steps/developer/step-50-2d-solution.png" alt="">
In 3D, the solution looks similar (see below). On the left you can see the solution and on the right we show a slice for $x$ close to the
center of the domain showing the adaptively refined mesh.
<table width="60%" align="center">
  <tr>
    <td align="center">
      <img width="400px" src="https://www.dealii.org/images/steps/developer/step-50-3d-solution.png" alt="">
    </td>
    <td align="center">
      <img width="400px" src="https://www.dealii.org/images/steps/developer/step-50-refinement.png" alt="">
    </td>
  </tr>
</table>
Both in 2D and 3D you can see the adaptive refinement picking up the corner singularity and the inner singularity where the viscosity jumps, while the interface along the line that separates the two viscosities is (correctly) not refined as it is resolved adequately.
This is because the kink in the solution that results from the jump
in the coefficient is aligned with cell interfaces.


<h3>Workload imbalance for geometric multigrid methods</h3>

As mentioned above, the purpose of this program is to demonstrate the
use of algebraic and geometric multigrid methods for this problem, and
to do so for parallel computations. An important component of making
algorithms scale to large parallel machines is ensuring that every
processor has the same amount of work to do. (More precisely, what
matters is that there are no small fraction of processors that have
substantially more work than the rest since, if that were so, a large
fraction of processors will sit idle waiting for the small fraction to
finish. Conversely, a small fraction of processors having
substantially <i>less</i> work is not a problem because the majority
of processors continues to be productive and only the small fraction
sits idle once finished with their work.)

For the active mesh, we use the parallel::distributed::Triangulation class as done
in step-40 which uses functionality in the external library
<a href="http://www.p4est.org/">p4est</a> for the distribution of the active cells
among processors. For the non-active cells in the multilevel hierarchy, deal.II
implements what we will refer to as the "first-child rule" where, for each cell
in the hierarchy, we recursively assign the parent of a cell to the owner of the
first child cell. The following figures give an example of such a distribution. Here
the left image represents the active cells for a sample 2D mesh partitioned using a
space-filling curve (which is what p4est uses to partition cells);
the center image gives the tree representation
of the active mesh; and the right image gives the multilevel hierarchy of cells. The
colors and numbers represent the different processors. The circular nodes in the tree
are the non-active cells which are distributed using the "first-child rule".

<img width="800px" src="https://www.dealii.org/images/steps/developer/step-50-workload-example.png" alt="">

Included among the output to screen in this example is a value "Partition efficiency"
given by one over MGTools::workload_imbalance(). This value, which will be denoted
by $\mathbb{E}$,  quantifies the overhead produced by not having a perfect work balance
on each level of the multigrid hierarchy. This imbalance is evident from the
example above: while level $\ell=2$ is about as well balanced as is possible
with four cells among three processors, the coarse
level $\ell=0$ has work for only one processor, and level $\ell=1$ has work
for only two processors of which one has three times as much work as
the other.

For defining $\mathbb{E}$, it is important to note that, as we are using local smoothing
to define the multigrid hierarchy (see the @ref mg_paper "multigrid paper" for a description of
local smoothing), the refinement level of a cell corresponds to that cell's multigrid
level. Now, let $N_{\ell}$ be the number of cells on level $\ell$
(both active and non-active cells) and $N_{\ell,p}$ be the subset owned by process
$p$. We will also denote by $P$ the total number of processors.
Assuming that the workload for any one processor is proportional to the number
of cells owned by that processor, the optimal workload per processor is given by
@f{align*}
W_{\text{opt}} = \frac1{P}\sum_{\ell} N_{\ell} = \sum_{\ell}\left(\frac1{P}\sum_{p}N_{\ell,p}\right).
@f}
Next, assuming a synchronization of work on each level (i.e., on each level of a V-cycle,
work must be completed by all processors before moving on to the next level), the
limiting effort on each level is given by
@f{align*}
W_\ell = \max_{p} N_{\ell,p},
@f}
and the total parallel complexity
@f{align*}
W = \sum_{\ell} W_\ell.
@f}
Then we define $\mathbb{E}$ as a ratio of the optimal partition to the parallel
complexity of the current partition
@f{align*}
  \mathbb{E} = \frac{W_{\text{opt}}}{W}.
@f}
For the example distribution above, we have
@f{align*}
W_{\text{opt}}&=\frac{1}{P}\sum_{\ell} N_{\ell} = \frac{1}{3} \left(1+4+4\right)= 3 \qquad
\\
W &= \sum_\ell W_\ell = 1 + 2 + 3 = 6
\\
\mathbb{E} &= \frac{W_{\text{opt}}}{W} = \frac12.
@f}
The value MGTools::workload_imbalance()  
$= 1/\mathbb{E}$ then represents the factor increase
in timings we expect for GMG methods (vmults, assembly, etc.) due to the imbalance of the
mesh partition compared to a perfectly load-balanced workload. We will
report on these in the results section below for a sequence of meshes,
and compare with the observed slow-downs as we go to larger and larger
processor numbers (where, typically, the load imbalance becomes larger
as well).

These sorts of considerations are considered in much greater detail in
@cite clevenger_par_gmg, which contains a full discussion of the partition efficiency model
and the effect the imbalance has on the GMG V-cycle timing. In summary, the value
of $\mathbb{E}$ is highly dependent on the degree of local mesh refinement used and has
an optimal value $\mathbb{E} \approx 1$ for globally refined meshes. Typically for adaptively
refined meshes, the number of processors used to distribute a single mesh has a
negative impact on $\mathbb{E}$ but only up to a leveling off point, where the imbalance
remains relatively constant for an increasing number of processors, and further refinement
has very little impact on $\mathbb{E}$. Finally, $1/\mathbb{E}$ was shown to give an
accurate representation of the slowdown in parallel scaling expected for the timing of
a V-cycle.

It should be noted that there is potential for some asynchronous work between multigrid
levels, specifically with purely nearest neighbor MPI communication, and an adaptive mesh
could be constructed such that the efficiency model would far overestimate the V-cycle slowdown
due to the asynchronous work "covering up" the imbalance (which assumes synchronization over levels).
However, for most realistic adaptive meshes the expectation is that this asynchronous work will
only cover up a very small portion of the imbalance and the efficiency model will describe the
slowdown very well.


<h3>Workload imbalance for algebraic multigrid methods</h3>

The considerations above show that one has to expect certain limits on
the scalability of the geometric multigrid algorithm as it is implemented in deal.II because even in cases
where the finest levels of a mesh are perfectly load balanced, the
coarser levels may not be. At the same time, the coarser levels are
weighted less (the contributions of $W_\ell$ to $W$ are small) because
coarser levels have fewer cells and, consequently, do not contribute
to the overall run time as much as finer levels. In other words,
imbalances in the coarser levels may not lead to large effects in the
big picture.

Algebraic multigrid methods are of course based on an entirely
different approach to creating a hierarchy of levels. In particular,
they create these purely based on analyzing the system matrix, and
very sophisticated algorithms for ensuring that the problem is well
load-balanced on every level are implemented in both the hypre and
ML/MueLu packages that underly the TrilinosWrappers::PreconditionAMG
and PETScWrappers::PreconditionBoomerAMG classes. In some sense, these
algorithms are simpler than for geometric multigrid methods because
they only deal with the matrix itself, rather than all of the
connotations of meshes, neighbors, parents, and other geometric
entities. At the same time, much work has also been put into making
algebraic multigrid methods scale to very large problems, including
questions such as reducing the number of processors that work on a
given level of the hierarchy to a subset of all processors, if
otherwise processors would spend less time on computations than on
communication. (One might note that it is of course possible to
implement these same kinds of ideas also in geometric multigrid
algorithms where one purposefully idles some processors on coarser
levels to reduce the amount of communication. deal.II just doesn't do
this at this time.)

These are not considerations we typically have to worry about here,
however: For most purposes, we use algebraic multigrid methods as
black-box methods.



<h3>Running the program</h3>

As mentioned above, this program can use three different ways of
solving the linear system: matrix-based geometric multigrid ("MB"),
matrix-free geometric multigrid ("MF"), and algebraic multigrid
("AMG"). The directory in which this program resides has input files
with suffix `.prm` for all three of these options, and for both 2d and
3d.

You can execute the program as in
@code
  ./step-50 gmg_mb_2d.prm
@endcode
and this will take the run-time parameters from the given input
file (here, `gmg_mb_2d.prm`).

The program is intended to be run in parallel, and you can achieve
this using a command such as
@code
  mpirun -np 4 ./step-50 gmg_mb_2d.prm
@endcode
if you want to, for example, run on four processors. (That said, the
program is also ready to run with, say, `-np 28672` if that's how many
processors you have available.)


examples/step-50/doc/results.dox
<h1>Results</h1>

When you run the program using the following command
@code
mpirun -np 16 ./step-50  gmg_mf_2d.prm
@endcode
the screen output should look like the following:
@code
Cycle 0:
   Number of active cells:       12 (2 global levels)
   Partition efficiency:         0.1875
   Number of degrees of freedom: 65 (by level: 21, 65)
   Number of CG iterations:      10
   Global error estimate:        0.355373
   Wrote solution_00.pvtu


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |    0.0163s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble right-hand side        |         1 |  0.000374s |       2.3% |
| Estimate                        |         1 |  0.000724s |       4.4% |
| Output results                  |         1 |   0.00277s |        17% |
| Setup                           |         1 |   0.00225s |        14% |
| Setup multigrid                 |         1 |   0.00181s |        11% |
| Solve                           |         1 |   0.00364s |        22% |
| Solve: 1 multigrid V-cycle      |         1 |  0.000354s |       2.2% |
| Solve: CG                       |         1 |   0.00151s |       9.3% |
| Solve: Preconditioner setup     |         1 |   0.00125s |       7.7% |
+---------------------------------+-----------+------------+------------+

Cycle 1:
   Number of active cells:       24 (3 global levels)
   Partition efficiency:         0.276786
   Number of degrees of freedom: 139 (by level: 21, 65, 99)
   Number of CG iterations:      10
   Global error estimate:        0.216726
   Wrote solution_01.pvtu


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |    0.0169s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble right-hand side        |         1 |  0.000309s |       1.8% |
| Estimate                        |         1 |   0.00156s |       9.2% |
| Output results                  |         1 |   0.00222s |        13% |
| Refine grid                     |         1 |   0.00278s |        16% |
| Setup                           |         1 |   0.00196s |        12% |
| Setup multigrid                 |         1 |    0.0023s |        14% |
| Solve                           |         1 |   0.00565s |        33% |
| Solve: 1 multigrid V-cycle      |         1 |  0.000349s |       2.1% |
| Solve: CG                       |         1 |   0.00285s |        17% |
| Solve: Preconditioner setup     |         1 |   0.00195s |        12% |
+---------------------------------+-----------+------------+------------+

Cycle 2:
   Number of active cells:       51 (4 global levels)
   Partition efficiency:         0.41875
   Number of degrees of freedom: 245 (by level: 21, 65, 225, 25)
   Number of CG iterations:      11
   Global error estimate:        0.112098
   Wrote solution_02.pvtu


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |    0.0183s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble right-hand side        |         1 |  0.000274s |       1.5% |
| Estimate                        |         1 |   0.00127s |       6.9% |
| Output results                  |         1 |   0.00227s |        12% |
| Refine grid                     |         1 |    0.0024s |        13% |
| Setup                           |         1 |   0.00191s |        10% |
| Setup multigrid                 |         1 |   0.00295s |        16% |
| Solve                           |         1 |   0.00702s |        38% |
| Solve: 1 multigrid V-cycle      |         1 |  0.000398s |       2.2% |
| Solve: CG                       |         1 |   0.00376s |        21% |
| Solve: Preconditioner setup     |         1 |   0.00238s |        13% |
+---------------------------------+-----------+------------+------------+
.
.
.
@endcode
Here, the timing of the `solve()` function is split up in 3 parts: setting
up the multigrid preconditioner, execution of a single multigrid V-cycle, and
the CG solver. The V-cycle that is timed is unnecessary for the overall solve
and only meant to give an insight at the different costs for AMG and GMG.
Also it should be noted that when using the AMG solver, "Workload imbalance"
is not included in the output since the hierarchy of coarse meshes is not
required.

All results in this section are gathered on Intel Xeon Platinum 8280 (Cascade
Lake) nodes which have 56 cores and 192GB per node and support AVX-512 instructions,
allowing for vectorization over 8 doubles (vectorization used only in the matrix-free
computations). The code is compiled using gcc 7.1.0 with intel-mpi 17.0.3. Trilinos
12.10.1 is used for the matrix-based GMG/AMG computations.

We can then gather a variety of information by calling the program
with the input files that are provided in the directory in which
step-50 is located. Using these, and adjusting the number of mesh
refinement steps, we can produce information about how well the
program scales.

The following table gives weak scaling timings for this program on up to 256M DoFs
and 7,168 processors. (Recall that weak scaling keeps the number of
degrees of freedom per processor constant while increasing the number of
processors; i.e., it considers larger and larger problems.)
Here, $\mathbb{E}$ is the partition efficiency from the
 introduction (also equal to 1.0/workload imbalance), "Setup" is a combination
of setup, setup multigrid, assemble, and assemble multigrid from the timing blocks,
and "Prec" is the preconditioner setup. Ideally all times would stay constant
over each problem size for the individual solvers, but since the partition
efficiency decreases from 0.371 to 0.161 from largest to smallest problem size,
we expect to see an approximately $0.371/0.161=2.3$ times increase in timings
for GMG. This is, in fact, pretty close to what we really get:

<table align="center" class="doxtable">
<tr>
  <th colspan="4"></th>
  <th></th>
  <th colspan="4">MF-GMG</th>
  <th></th>
  <th colspan="4">MB-GMG</th>
  <th></th>
  <th colspan="4">AMG</th>
</tr>
<tr>
  <th align="right">Procs</th>
  <th align="right">Cycle</th>
  <th align="right">DoFs</th>
  <th align="right">$\mathbb{E}$</th>
  <th></th>
  <th align="right">Setup</th>
  <th align="right">Prec</th>
  <th align="right">Solve</th>
  <th align="right">Total</th>
  <th></th>
  <th align="right">Setup</th>
  <th align="right">Prec</th>
  <th align="right">Solve</th>
  <th align="right">Total</th>
  <th></th>
  <th align="right">Setup</th>
  <th align="right">Prec</th>
  <th align="right">Solve</th>
  <th align="right">Total</th>
</tr>
<tr>
  <td align="right">112</th>
  <td align="right">13</th>
  <td align="right">4M</th>
  <td align="right">0.37</th>
  <td></td>
  <td align="right">0.742</th>
  <td align="right">0.393</th>
  <td align="right">0.200</th>
  <td align="right">1.335</th>
  <td></td>
  <td align="right">1.714</th>
  <td align="right">2.934</th>
  <td align="right">0.716</th>
  <td align="right">5.364</th>
  <td></td>
  <td align="right">1.544</th>
  <td align="right">0.456</th>
  <td align="right">1.150</th>
  <td align="right">3.150</th>
</tr>
<tr>
  <td align="right">448</th>
  <td align="right">15</th>
  <td align="right">16M</th>
  <td align="right">0.29</th>
  <td></td>
  <td align="right">0.884</th>
  <td align="right">0.535</th>
  <td align="right">0.253</th>
  <td align="right">1.672</th>
  <td></td>
  <td align="right">1.927</th>
  <td align="right">3.776</th>
  <td align="right">1.190</th>
  <td align="right">6.893</th>
  <td></td>
  <td align="right">1.544</th>
  <td align="right">0.456</th>
  <td align="right">1.150</th>
  <td align="right">3.150</th>
</tr>
<tr>
  <td align="right">1,792</th>
  <td align="right">17</th>
  <td align="right">65M</th>
  <td align="right">0.22</th>
  <td></td>
  <td align="right">1.122</th>
  <td align="right">0.686</th>
  <td align="right">0.309</th>
  <td align="right">2.117</th>
  <td></td>
  <td align="right">2.171</th>
  <td align="right">4.862</th>
  <td align="right">1.660</th>
  <td align="right">8.693</th>
  <td></td>
  <td align="right">1.654</th>
  <td align="right">0.546</th>
  <td align="right">1.460</th>
  <td align="right">3.660</th>
</tr>
<tr>
  <td align="right">7,168</th>
  <td align="right">19</th>
  <td align="right">256M</th>
  <td align="right">0.16</th>
  <td></td>
  <td align="right">1.214</th>
  <td align="right">0.893</th>
  <td align="right">0.521</th>
  <td align="right">2.628</th>
  <td></td>
  <td align="right">2.386</th>
  <td align="right">7.260</th>
  <td align="right">2.560</th>
  <td align="right">12.206</th>
  <td></td>
  <td align="right">1.844</th>
  <td align="right">1.010</th>
  <td align="right">1.890</th>
  <td align="right">4.744</th>
</tr>
</table>

On the other hand, the algebraic multigrid in the last set of columns
is relatively unaffected by the increasing imbalance of the mesh
hierarchy (because it doesn't use the mesh hierarchy) and the growth
in time is rather driven by other factors that are well documented in
the literature (most notably that the algorithmic complexity of
some parts of algebraic multigrid methods appears to be ${\cal O}(N
\log N)$ instead of ${\cal O}(N)$ for geometric multigrid).

The upshort of the table above is that the matrix-free geometric multigrid
method appears to be the fastest approach to solving this equation if
not by a huge margin. Matrix-based methods, on the other hand, are
consistently the worst.

The following figure provides strong scaling results for each method, i.e.,
we solve the same problem on more and more processors. Specifically,
we consider the problems after 16 mesh refinement cycles
(32M DoFs) and 19 cycles (256M DoFs), on between 56 to 28,672 processors:

<img width="600px" src="https://www.dealii.org/images/steps/developer/step-50-strong-scaling.png" alt="">

While the matrix-based GMG solver and AMG scale similarly and have a
similar time to solution (at least as long as there is a substantial
number of unknowns per processor -- say, several 10,000), the
matrix-free GMG solver scales much better and solves the finer problem
in roughly the same time as the AMG solver for the coarser mesh with
only an eighth of the number of processors. Conversely, it can solve the
same problem on the same number of processors in about one eighth the
time.


<h3> Possibilities for extensions </h3>

<h4> Testing convergence and higher order elements </h4>

The finite element degree is currently hard-coded as 2, see the template
arguments of the main class. It is easy to change. To test, it would be
interesting to switch to a test problem with a reference solution. This way,
you can compare error rates.

<h4> Coarse solver </h4>

A more interesting example would involve a more complicated coarse mesh (see
step-49 for inspiration). The issue in that case is that the coarsest
level of the mesh hierarchy is actually quite large, and one would
have to think about ways to solve the coarse level problem
efficiently. (This is not an issue for algebraic multigrid methods
because they would just continue to build coarser and coarser levels
of the matrix, regardless of their geometric origin.)

In the program here, we simply solve the coarse level problem with a
Conjugate Gradient method without any preconditioner. That is acceptable
if the coarse problem is really small -- for example, if the coarse
mesh had a single cell, then the coarse mesh problems has a $9\times 9$
matrix in 2d, and a $27\times 27$ matrix in 3d; for the coarse mesh we
use on the $L$-shaped domain of the current program, these sizes are
$21\times 21$ in 2d and $117\times 117$ in 3d. But if the coarse mesh
consists of hundreds or thousands of cells, this approach will no
longer work and might start to dominate the overall run-time of each V-cyle.
A common approach is then to solve the coarse mesh problem using an
algebraic multigrid preconditioner; this would then, however, require
assembling the coarse matrix (even for the matrix-free version) as
input to the AMG implementation.


examples/step-51/doc/intro.dox
<br>

<i>
This program was contributed by Martin Kronbichler and Scott Miller.
</i>

<a name="Intro"></a>
<h1>Introduction</h1>

This tutorial program presents the implementation of a hybridizable
discontinuous Galkerin method for the convection-diffusion equation.

<h3> Hybridizable discontinuous Galerkin methods </h3>

One common argument against the use of discontinuous Galerkin elements
is the large number of globally coupled degrees of freedom that one
must solve in an implicit system.  This is because, unlike continuous finite
elements, in typical discontinuous elements there is one degree of freedom at
each vertex <i>for each of the adjacent elements</i>, rather than just one,
and similarly for edges and faces.  As an example of how fast the number of
unknowns grows, consider the FE_DGPMonomial basis: each
scalar solution component is represented by polynomials of degree $p$
with $(1/\text{dim}!) \prod_{i=1}^{\text{dim}}(p+i)$ degrees of freedom per
element. Typically, all degrees of freedom in an element are coupled
to all of the degrees of freedom in the adjacent elements.  The resulting
discrete equations yield very large linear systems very quickly, especially
for systems of equations in 2 or 3 dimensions.

<h4> Reducing the size of the linear system </h4>
To alleviate the computational cost of solving such large linear systems,
the hybridizable discontinuous Galerkin (HDG) methodology was introduced
by Cockburn and co-workers (see the references in the recent HDG overview
article by Nguyen and Peraire @cite Ngu2012).

The HDG method achieves this goal by formulating the mathematical problem using
Dirichlet-to-Neumann mappings.  The partial differential equations are first
written as a first order system, and each field is then discretized via a DG
method.  At this point, the single-valued "trace" values on the skeleton of the
mesh, i.e., element faces, are taken to be independent unknown quantities.
This yields unknowns in the discrete formulation that fall into two categories:
- Face unknowns that only couple with the cell unknowns from both sides of the face;
- Cell unknowns that only couple with the cell and face unknowns
  defined within the same cell. Crucially, no cell interior degree of freedom
  on one cell ever couples to any interior cell degree of freedom of a
  different cell.

The Dirichlet-to-Neumann map concept then permits the following solution procedure:
<ol>
  <li>  Use local element interior data to enforce a Neumann condition on the
skeleton of the triangulation.  The global problem is then to solve for the
trace values, which are the only globally coupled unknowns.
  <li>  Use the known skeleton values as Dirichlet data for solving local
element-level solutions.  This is known as the 'local solver', and is an
<i>embarrassingly parallel</i> element-by-element solution process.
</ol>

<h4> Relation with Static Condensation </h4>
The above procedure also has a linear algebra interpretation---referred to
as <i>static condensation</i>---that was exploited to reduce the size of the
global linear system by Guyan in the context of continuous Finite Elements
@cite G65, and by Fraeijs de Veubeke for mixed methods @cite F65. In the
latter case (mixed formulation), the system reduction was achieved through the
use of discontinuous fluxes combined with the introduction of an additional
auxiliary <i>hybrid</i> variable that approximates the trace of the unknown
at the boundary of every element. This procedure became known as hybridization
and---by analogy---is the reason why the local discontinuous Galerkin method
introduced by Cockburn, Gopalakrishnan, and Lazarov in 2009 @cite CGL2009, and
subsequently developed by their collaborators, eventually came to be known as
the <i>hybridizable discontinuous Galerkin</i> (HDG) method.

Let us write the complete linear system associated to the HDG problem as a
block system with the discrete DG (cell interior) variables $U$ as first block
and the skeleton (face) variables $\Lambda$ as the second block:
@f{eqnarray*}
\begin{pmatrix} A & B \\ C & D \end{pmatrix}
\begin{pmatrix} U \\ \Lambda \end{pmatrix}
=
\begin{pmatrix} F \\ G \end{pmatrix}.
@f}
Our aim is now to eliminate the $U$ block with a Schur complement
approach similar to step-20, which results in the following two steps:
@f{eqnarray*}
(D - C A^{-1} B) \Lambda &=& G - C A^{-1} F, \\
A U &=& F - B \Lambda.
@f}
The point is that the presence of $A^{-1}$ is not a problem because $A$ is a
block diagonal matrix where each block corresponds to one cell and is
therefore easy enough to invert.
The coupling to other cells is introduced by the matrices
$B$ and $C$ over the skeleton variable. The block-diagonality of
$A$ and the structure in $B$ and $C$ allow us to invert the
matrix $A$ element by element (the local solution of the Dirichlet
problem) and subtract $CA^{-1}B$ from $D$. The steps in the Dirichlet-to-Neumann
map concept hence correspond to
<ol>
  <li> constructing the Schur complement matrix $D-C A^{-1} B$ and right hand
    side $G - C A^{-1} F$  <i>locally on each cell</i>
    and inserting the contribution into the global trace matrix in the usual way,
  <li> solving the Schur complement system for $\Lambda$, and
  <li> solving for $U$ using the second equation, given $\Lambda$.
</ol>


<h4> Solution quality and rates of convergence</h4>
Another criticism of traditional DG methods is that the approximate fluxes
converge suboptimally.  The local HDG solutions can be shown to converge
as $\mathcal{O}(h^{p+1})$, i.e., at optimal order.  Additionally, a
super-convergence property can be used to post-process a new approximate
solution that converges at the rate $\mathcal{O}(h^{p+2})$.


<h4> Alternative approaches </h4>

The hybridizable discontinuous Galerkin method is only one way in
which the problems of the discontinuous Galerkin method can be
addressed. Another idea is what is called the "weak Galerkin"
method. It is explored in step-61.


<h3> HDG applied to the convection-diffusion problem </h3>

The HDG formulation used for this example is taken from
<br>
<b>
  N.C. Nguyen, J. Peraire, B. Cockburn:
  <i>An implicit high-order hybridizable discontinuous Galerkin method
  for linear convection–diffusion equations</i>,
  Journal of Computational Physics, 2009, 228:9, 3232-3254.
  <a href="http://dx.doi.org/10.1016/j.jcp.2009.01.030">[DOI]</a>
</b>

We consider the convection-diffusion equation over the domain $\Omega$
with Dirichlet boundary $\partial \Omega_D$ and Neumann boundary
$\partial \Omega_N$:
@f{eqnarray*}
	\nabla \cdot (\mathbf{c} u) - \nabla \cdot (\kappa \nabla u) &=& f,
	\quad \text{ in } \Omega, \\
	u &=& g_D, \quad \text{ on } \partial \Omega_D, \\
	(\mathbf{c} u - \kappa \nabla u)\cdot \mathbf{n} &=& g_N,
	\quad \text{ on }  \partial \Omega_N.
@f}

Introduce the auxiliary variable $\mathbf{q}=-\kappa \nabla u$ and rewrite
the above equation as the first order system:
@f{eqnarray*}
  \mathbf{q} + \kappa \nabla u &=& 0, \quad \text{ in } \Omega, \\
  \nabla \cdot (\mathbf{c} u + \mathbf{q}) &=& f, \quad \text{ in } \Omega, \\
  u &=& g_D, \quad \text{ on } \partial \Omega_D, \\
  (\mathbf{q} + \mathbf{c}u)\cdot\mathbf{n}  &=& g_N,
	\quad \text{ on }  \partial \Omega_N.
@f}

We multiply these equations by the weight functions $\mathbf{v}, w$
and integrate by parts over every element $K$ to obtain:
@f{eqnarray*}
  (\mathbf{v}, \kappa^{-1} \mathbf{q})_K - (\nabla\cdot\mathbf{v}, u)_K
    + \left<\mathbf{v}\cdot\mathbf{n}, {\hat{u}}\right>_{\partial K} &=& 0, \\
  - (\nabla w, \mathbf{c} u + \mathbf{q})_K
    + \left<w, (\widehat{\mathbf{c} u}+{\hat{\mathbf{q}}})\cdot\mathbf{n}\right>_{\partial K}
    &=& (w,f)_K.
@f}

The terms decorated with a hat denote the numerical traces (also commonly referred
to as numerical fluxes).  They are approximations
to the interior values on the boundary of the element.  To ensure conservation,
these terms must be single-valued on any given element edge $\partial K$ even
though, with discontinuous shape functions, there may of course be multiple
values coming from the cells adjacent to an interface.
We eliminate the numerical trace $\hat{\mathbf{q}}$ by using traces of the form:
@f{eqnarray*}
  \widehat{\mathbf{c} u}+\hat{\mathbf{q}} = \mathbf{c}\hat{u} + \mathbf{q}
  + \tau(u - \hat{u})\mathbf{n} \quad \text{ on } \partial K.
@f}

The variable $\hat {u}$ is introduced as an additional independent variable
and is the one for which we finally set up a globally coupled linear
system. As mentioned above, it is defined on the element faces and
discontinuous from one face to another wherever faces meet (at
vertices in 2d, and at edges and vertices in 3d).
Values for $u$ and $\mathbf{q}$ appearing in the numerical trace function
are taken to be the cell's interior solution restricted
to the boundary $\partial K$.

The local stabilization parameter $\tau$ has effects on stability and accuracy
of HDG solutions; see the literature for a further discussion. A stabilization
parameter of unity is reported to be the choice which gives best results. A
stabilization parameter $\tau$ that tends to infinity prohibits jumps in the
solution over the element boundaries, making the HDG solution approach the
approximation with continuous finite elements. In the program below, we choose
the stabilization parameter as
@f{eqnarray*}
  \tau = \frac{\kappa}{\ell} + |\mathbf{c} \cdot \mathbf{n}|
@f}
where we set the diffusion $\kappa=1$ and the diffusion length scale to
$\ell = \frac{1}{5}$.

The trace/skeleton variables in HDG methods are single-valued on element
faces.  As such, they must strongly represent the Dirichlet data on
$\partial\Omega_D$.  This means that
@f{equation*}
  \hat{u}|_{\partial \Omega_D} = g_D,
@f}
where the equal sign actually means an $L_2$ projection of the boundary
function $g$ onto the space of the face variables (e.g. linear functions on
the faces). This constraint is then applied to the skeleton variable $\hat{u}$
using inhomogeneous constraints by the method
VectorTools::project_boundary_values.

Summing the elemental
contributions across all elements in the triangulation, enforcing the normal
component of the numerical flux, and integrating by parts
on the equation weighted by $w$, we arrive at the final form of the problem:
Find $(\mathbf{q}_h, u_h, \hat{u}_h) \in
\mathcal{V}_h^p \times \mathcal{W}_h^p \times \mathcal{M}_h^p$ such that
@f{align*}
  (\mathbf{v}, \kappa^{-1} \mathbf{q}_h)_{\mathcal{T}}
    - ( \nabla\cdot\mathbf{v}, u_h)_{\mathcal{T}}
    + \left<\mathbf{v}\cdot\mathbf{n}, \hat{u}_h\right>_{\partial\mathcal{T}}
    &= 0,
    \quad &&\forall \mathbf{v} \in \mathcal{V}_h^p,
\\
   - (\nabla w, \mathbf{c} u_h)_{\mathcal{T}}
   + (w, \nabla \cdot \mathbf{q}_h)_{\mathcal{T}}
   + (w, (\mathbf{c}\cdot\mathbf{n}) \hat{u}_h)_{\partial \mathcal{T}}
    + \left<w, \tau (u_h - \hat{u}_h)\right>_{\partial \mathcal{T}}
    &=
    (w, f)_{\mathcal{T}},
    \quad &&\forall w \in \mathcal{W}_h^p,
\\
  \left< \mu, \hat{u}_h\mathbf{c} \cdot \mathbf{n}
  		+ \mathbf{q}_h\cdot \mathbf{n}
  	    + \tau (u_h - \hat{u}_h)\right>_{\partial \mathcal{T}}
    &=
    \left<\mu, g_N\right>_{\partial\Omega_N},
    \quad &&\forall \mu \in \mathcal{M}_h^p.
@f}

The unknowns $(\mathbf{q}_h, u_h)$ are referred to as local variables; they are
represented as standard DG variables.  The unknown $\hat{u}_h$ is the skeleton
variable which has support on the codimension-1 surfaces (faces) of the mesh.

We use the notation $(\cdot, \cdot)_{\mathcal{T}} = \sum_K (\cdot, \cdot)_K$
to denote the sum of integrals over all cells and $\left<\cdot,
\cdot\right>_{\partial \mathcal{T}} = \sum_K \left<\cdot,
\cdot\right>_{\partial K}$ to denote integration over all faces of all cells,
i.e., interior faces are visited twice, once from each side and with
the corresponding normal vectors. When combining the contribution from
both elements sharing a face, the above equation yields terms familiar
from the DG method, with jumps of the solution over the cell boundaries.

In the equation above, the space $\mathcal {W}_h^{p}$ for the scalar variable
$u_h$ is defined as the space of functions that are tensor
product polynomials of degree $p$ on each cell and discontinuous over the
element boundaries $\mathcal Q_{-p}$, i.e., the space described by
<code>FE_DGQ<dim>(p)</code>. The space for the gradient or flux variable
$\mathbf{q}_i$ is a vector element space where each component is
a locally polynomial and discontinuous $\mathcal Q_{-p}$. In the code below,
we collect these two local parts together in one FESystem where the first @p
dim components denote the gradient part and the last scalar component
corresponds to the scalar variable. For the skeleton component $\hat{u}_h$, we
define a space that consists of discontinuous tensor product polynomials that
live on the element faces, which in deal.II is implemented by the class
FE_FaceQ. This space is otherwise similar to FE_DGQ, i.e., the solution
function is not continuous between two neighboring faces, see also the results
section below for an illustration.

In the weak form given above, we can note the following coupling patterns:
<ol>
  <li> The matrix $A$ consists of local-local coupling terms.  These arise when the
  local weighting functions $(\mathbf{v}, w)$ multiply the local solution terms
  $(\mathbf{q}_h, u_h)$. Because the elements are discontinuous, $A$
  is block diagonal.
  <li> The matrix $B$ represents the local-face coupling.  These are the terms
  with weighting functions $(\mathbf{v}, w)$ multiplying the skeleton variable
  $\hat{u}_h$.
  <li> The matrix $C$ represents the face-local coupling, which involves the
  weighting function $\mu$ multiplying the local solutions $(\mathbf{q}_h, u_h)$.
  <li>  The matrix $D$ is the face-face coupling;
  terms involve both $\mu$ and $\hat{u}_h$.
</ol>

<h4> Post-processing and super-convergence </h4>

One special feature of the HDG methods is that they typically allow for
constructing an enriched solution that gains accuracy. This post-processing
takes the HDG solution in an element-by-element fashion and combines it such
that one can get $\mathcal O(h^{p+2})$ order of accuracy when using
polynomials of degree $p$. For this to happen, there are two necessary
ingredients:
<ol>
  <li> The computed solution gradient $\mathbf{q}_h$ converges at optimal rate,
   i.e., $\mathcal{O}(h^{p+1})$.
  <li> The cell-wise average of the scalar part of the solution,
   $\frac{(1,u_h)_K}{\text{vol}(K)}$, super-converges at rate
   $\mathcal{O}(h^{p+2})$.
</ol>

We now introduce a new variable $u_h^* \in \mathcal{V}_h^{p+1}$, which we find
by minimizing the expression $|\kappa \nabla u_h^* + \mathbf{q}_h|^2$ over the cell
$K$ under the constraint $\left(1, u_h^*\right)_K = \left(1,
u_h\right)_K$. The constraint is necessary because the minimization
functional does not determine the constant part of $u_h^*$. This
translates to the following system of equations:
@f{eqnarray*}
\left(1, u_h^*\right)_K &=& \left(1, u_h\right)_K\\
\left(\nabla w_h^*, \kappa \nabla u_h^*\right)_K &=&
-\left(\nabla w_h^*, \mathbf{q}_h\right)_K
\quad \text{for all } w_h^* \in \mathcal Q^{p+1}.
@f}

Since we test by the whole set of basis functions in the space of tensor
product polynomials of degree $p+1$ in the second set of equations, this
is an overdetermined system with one more equation than unknowns. We fix this
in the code below by omitting one of these equations (since the rows in the
Laplacian are linearly dependent when representing a constant function). As we
will see below, this form of the post-processing gives the desired
super-convergence result with rate $\mathcal {O}(h^{p+2})$.  It should be
noted that there is some freedom in constructing $u_h^*$ and this minimization
approach to extract the information from the gradient is not the only one. In
particular, the post-processed solution defined here does not satisfy the
convection-diffusion equation in any sense. As an alternative, the paper by
Nguyen, Peraire and Cockburn cited above suggests another somewhat more
involved formula for convection-diffusion that can also post-process the flux
variable into an $H(\Omega,\mathrm{div})$-conforming variant and better
represents the local convection-diffusion operator when the diffusion is
small. We leave the implementation of a more sophisticated post-processing as
a possible extension to the interested reader.

Note that for vector-valued problems, the post-processing works similarly. One
simply sets the constraint for the mean value of each vector component
separately and uses the gradient as the main source of information.

<h3> Problem specific data </h3>

For this tutorial program, we consider almost the same test case as in
step-7. The computational domain is $\Omega \dealcoloneq [-1,1]^d$ and the exact
solution corresponds to the one in step-7, except for a scaling. We use the
following source centers $x_i$ for the exponentials
<ul>
  <li> 1D:  $\{x_i\}^1 = \{ -\frac{1}{3}, 0, \frac{1}{3} \}$,
  <li> 2D: $\{\mathbf{x}_i\}^2 = \{ (-\frac{1}{2},\frac{1}{2}),
                        		 (-\frac{1}{2},-\frac{1}{2}),
  					 (\frac{1}{2},-\frac{1}{2})
  				   \}$,
  <li> 3D: $\{\mathbf{x}_i\}^3 = \{ (-\frac{1}{2},\frac{1}{2}, \frac{1}{4}),
  				      (-\frac{3}{5},-\frac{1}{2}, -\frac{1}{8}),
  				      (\frac{1}{2},-\frac{1}{2}, \frac{1}{2})
  				   \}$.
</ul>

With the exact solution given, we then choose the forcing on the right hand
side and the Neumann boundary condition such that we obtain this solution
(manufactured solution technique). In this example, we choose the diffusion
equal to one and the convection as
\f[
\mathbf{c} = \begin{cases}
1, & \textrm{dim}=1 \\
(y, -x), & \textrm{dim}=2 \\
(y, -x, 1), & \textrm{dim}=3
\end{cases}
\f]
Note that the convection is divergence-free, $\nabla \cdot c = 0$.

<h3> Implementation </h3>

Besides implementing the above equations, the implementation below provides
the following features:
<ul>
  <li> WorkStream to parallelize local solvers. Workstream has been presented
  in detail in step-9.
  <li> Reconstruct the local DG solution from the trace.
  <li> Post-processing the solution for superconvergence.
  <li> DataOutFaces for direct output of the global skeleton solution.
</ul>


examples/step-51/doc/results.dox
<h1>Results</h1>

<h3>Program output</h3>

We first have a look at the output generated by the program when run in 2D. In
the four images below, we show the solution for polynomial degree $p=1$
and cycles 2, 3, 4, and 8 of the program. In the plots, we overlay the data
generated from the internal data (DG part) with the skeleton part ($\hat{u}$)
into the same plot. We had to generate two different data sets because cells
and faces represent different geometric entities, the combination of which (in
the same file) is not supported in the VTK output of deal.II.

The images show the distinctive features of HDG: The cell solution (colored
surfaces) is discontinuous between the cells. The solution on the skeleton
variable sits on the faces and ties together the local parts. The skeleton
solution is not continuous on the vertices where the faces meet, even though
its values are quite close along lines in the same coordinate direction. The
skeleton solution can be interpreted as a rubber spring between the two sides
that balances the jumps in the solution (or rather, the flux $\kappa \nabla u
+ \mathbf{c} u$). From the picture at the top left, it is clear that
the bulk solution frequently over- and undershoots and that the
skeleton variable in indeed a better approximation to the exact
solution; this explains why we can get a better solution using a
postprocessing step.

As the mesh is refined, the jumps between the cells get
small (we represent a smooth solution), and the skeleton solution approaches
the interior parts. For cycle 8, there is no visible difference in the two
variables. We also see how boundary conditions are implemented weakly and that
the interior variables do not exactly satisfy boundary conditions. On the
lower and left boundaries, we set Neumann boundary conditions, whereas we set
Dirichlet conditions on the right and top boundaries.

<table align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.sol_2.png" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.sol_3.png" alt=""></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.sol_4.png" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.sol_8.png" alt=""></td>
  </tr>
</table>

Next, we have a look at the post-processed solution, again at cycles 2, 3, 4,
and 8. This is a discontinuous solution that is locally described by second
order polynomials. While the solution does not look very good on the mesh of
cycle two, it looks much better for cycles three and four. As shown by the
convergence table below, we find that is also converges more quickly to the
analytical solution.

<table align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.post_2.png" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.post_3.png" alt=""></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.post_4.png" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.post_8.png" alt=""></td>
  </tr>
</table>

Finally, we look at the solution for $p=3$ at cycle 2. Despite the coarse
mesh with only 64 cells, the post-processed solution is similar in quality
to the linear solution (not post-processed) at cycle 8 with 4,096
cells. This clearly shows the superiority of high order methods for smooth
solutions.

<table align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.sol_q3_2.png" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.post_q3_2.png" alt=""></td>
  </tr>
</table>

<h4>Convergence tables</h4>

When the program is run, it also outputs information about the respective
steps and convergence tables with errors in the various components in the
end. In 2D, the convergence tables look the following:

@code
Q1 elements, adaptive refinement:
cells dofs   val L2    grad L2  val L2-post
   16    80 1.804e+01 2.207e+01   1.798e+01
   31   170 9.874e+00 1.322e+01   9.798e+00
   61   314 7.452e-01 3.793e+00   4.891e-01
  121   634 3.240e-01 1.511e+00   2.616e-01
  238  1198 8.585e-02 8.212e-01   1.808e-02
  454  2290 4.802e-02 5.178e-01   2.195e-02
  898  4378 2.561e-02 2.947e-01   4.318e-03
 1720  7864 1.306e-02 1.664e-01   2.978e-03
 3271 14638 7.025e-03 9.815e-02   1.075e-03
 6217 27214 4.119e-03 6.407e-02   9.975e-04

Q1 elements, global refinement:
cells dofs      val L2        grad L2      val L2-post
   16    80 1.804e+01    - 2.207e+01    - 1.798e+01    -
   36   168 6.125e+00 2.66 9.472e+00 2.09 6.084e+00 2.67
   64   288 9.785e-01 6.38 4.260e+00 2.78 7.102e-01 7.47
  144   624 2.730e-01 3.15 1.866e+00 2.04 6.115e-02 6.05
  256  1088 1.493e-01 2.10 1.046e+00 2.01 2.880e-02 2.62
  576  2400 6.965e-02 1.88 4.846e-01 1.90 9.204e-03 2.81
 1024  4224 4.018e-02 1.91 2.784e-01 1.93 4.027e-03 2.87
 2304  9408 1.831e-02 1.94 1.264e-01 1.95 1.236e-03 2.91
 4096 16640 1.043e-02 1.96 7.185e-02 1.96 5.306e-04 2.94
 9216 37248 4.690e-03 1.97 3.228e-02 1.97 1.599e-04 2.96

Q3 elements, global refinement:
cells dofs      val L2        grad L2      val L2-post
   16   160 3.613e-01    - 1.891e+00    - 3.020e-01    -
   36   336 6.411e-02 4.26 5.081e-01 3.24 3.238e-02 5.51
   64   576 3.480e-02 2.12 2.533e-01 2.42 5.277e-03 6.31
  144  1248 8.297e-03 3.54 5.924e-02 3.58 6.330e-04 5.23
  256  2176 2.254e-03 4.53 1.636e-02 4.47 1.403e-04 5.24
  576  4800 4.558e-04 3.94 3.277e-03 3.96 1.844e-05 5.01
 1024  8448 1.471e-04 3.93 1.052e-03 3.95 4.378e-06 5.00
 2304 18816 2.956e-05 3.96 2.104e-04 3.97 5.750e-07 5.01
 4096 33280 9.428e-06 3.97 6.697e-05 3.98 1.362e-07 5.01
 9216 74496 1.876e-06 3.98 1.330e-05 3.99 1.788e-08 5.01
@endcode


One can see the error reduction upon grid refinement, and for the cases where
global refinement was performed, also the convergence rates. The quadratic
convergence rates of Q1 elements in the $L_2$ norm for both the scalar
variable and the gradient variable is apparent, as is the cubic rate for the
postprocessed scalar variable in the $L_2$ norm. Note this distinctive
feature of an HDG solution. In typical continuous finite elements, the
gradient of the solution of order $p$ converges at rate $p$ only, as
opposed to $p+1$ for the actual solution. Even though superconvergence
results for finite elements are also available (e.g. superconvergent patch
recovery first introduced by Zienkiewicz and Zhu), these are typically limited
to structured meshes and other special cases. For Q3 HDG variables, the scalar
variable and gradient converge at fourth order and the postprocessed scalar
variable at fifth order.

The same convergence rates are observed in 3d.
@code
Q1 elements, adaptive refinement:
cells   dofs    val L2    grad L2  val L2-post
     8     144 7.122e+00 1.941e+01   6.102e+00
    29     500 3.309e+00 1.023e+01   2.145e+00
   113    1792 2.204e+00 1.023e+01   1.912e+00
   379    5732 6.085e-01 5.008e+00   2.233e-01
  1317   19412 1.543e-01 1.464e+00   4.196e-02
  4579   64768 5.058e-02 5.611e-01   9.521e-03
 14596  199552 2.129e-02 3.122e-01   4.569e-03
 46180  611400 1.033e-02 1.622e-01   1.684e-03
144859 1864212 5.007e-03 8.371e-02   7.364e-04
451060 5684508 2.518e-03 4.562e-02   3.070e-04

Q1 elements, global refinement:
cells   dofs       val L2          grad L2       val L2-post
     8     144 7.122e+00    - 1.941e+01     - 6.102e+00    -
    27     432 5.491e+00 0.64 2.184e+01 -0.29 4.448e+00 0.78
    64     960 3.646e+00 1.42 1.299e+01  1.81 3.306e+00 1.03
   216    3024 1.595e+00 2.04 8.550e+00  1.03 1.441e+00 2.05
   512    6912 6.922e-01 2.90 5.306e+00  1.66 2.511e-01 6.07
  1728   22464 2.915e-01 2.13 2.490e+00  1.87 8.588e-02 2.65
  4096   52224 1.684e-01 1.91 1.453e+00  1.87 4.055e-02 2.61
 13824  172800 7.972e-02 1.84 6.861e-01  1.85 1.335e-02 2.74
 32768  405504 4.637e-02 1.88 3.984e-01  1.89 5.932e-03 2.82
110592 1354752 2.133e-02 1.92 1.830e-01  1.92 1.851e-03 2.87

Q3 elements, global refinement:
cells   dofs       val L2        grad L2      val L2-post
     8     576 5.670e+00    - 1.868e+01    - 5.462e+00    -
    27    1728 1.048e+00 4.16 6.988e+00 2.42 8.011e-01 4.73
    64    3840 2.831e-01 4.55 2.710e+00 3.29 1.363e-01 6.16
   216   12096 7.883e-02 3.15 7.721e-01 3.10 2.158e-02 4.55
   512   27648 3.642e-02 2.68 3.305e-01 2.95 5.231e-03 4.93
  1728   89856 8.546e-03 3.58 7.581e-02 3.63 7.640e-04 4.74
  4096  208896 2.598e-03 4.14 2.313e-02 4.13 1.783e-04 5.06
 13824  691200 5.314e-04 3.91 4.697e-03 3.93 2.355e-05 4.99
 32768 1622016 1.723e-04 3.91 1.517e-03 3.93 5.602e-06 4.99
110592 5419008 3.482e-05 3.94 3.055e-04 3.95 7.374e-07 5.00
@endcode

<h3>Comparison with continuous finite elements</h3>

<h4>Results for 2D</h4>

The convergence tables verify the expected convergence rates stated in the
introduction. Now, we want to show a quick comparison of the computational
efficiency of the HDG method compared to a usual finite element (continuous
Galkerin) method on the problem of this tutorial. Of course, stability aspects
of the HDG method compared to continuous finite elements for
transport-dominated problems are also important in practice, which is an
aspect not seen on a problem with smooth analytic solution. In the picture
below, we compare the $L_2$ error as a function of the number of degrees of
freedom (left) and of the computing time spent in the linear solver (right)
for two space dimensions of continuous finite elements (CG) and the hybridized
discontinuous Galerkin method presented in this tutorial. As opposed to the
tutorial where we only use unpreconditioned BiCGStab, the times shown in the
figures below use the Trilinos algebraic multigrid preconditioner in
TrilinosWrappers::PreconditionAMG. For the HDG part, a wrapper around
ChunkSparseMatrix for the trace variable has been used in order to utilize the
block structure in the matrix on the finest level.

<table align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.2d_plain.png" width="400" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.2dt_plain.png" width="400" alt=""></td>
  </tr>
</table>

The results in the graphs show that the HDG method is slower than continuous
finite elements at $p=1$, about equally fast for cubic elements and
faster for sixth order elements. However, we have seen above that the HDG
method actually produces solutions which are more accurate than what is
represented in the original variables. Therefore, in the next two plots below
we instead display the error of the post-processed solution for HDG (denoted
by $p=1^*$ for example). We now see a clear advantage of HDG for the same
amount of work for both $p=3$ and $p=6$, and about the same quality
for $p=1$.

<table align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.2d_post.png" width="400" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.2dt_post.png" width="400" alt=""></td>
  </tr>
</table>

Since the HDG method actually produces results converging as
$h^{p+2}$, we should compare it to a continuous Galerkin
solution with the same asymptotic convergence behavior, i.e., FE_Q with degree
$p+1$. If we do this, we get the convergence curves below. We see that
CG with second order polynomials is again clearly better than HDG with
linears. However, the advantage of HDG for higher orders remains.

<table align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.2d_postb.png" width="400" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.2dt_postb.png" width="400" alt=""></td>
  </tr>
</table>

The results are in line with properties of DG methods in general: Best
performance is typically not achieved for linear elements, but rather at
somewhat higher order, usually around $p=3$. This is because of a
volume-to-surface effect for discontinuous solutions with too much of the
solution living on the surfaces and hence duplicating work when the elements
are linear. Put in other words, DG methods are often most efficient when used
at relatively high order, despite their focus on a discontinuous (and hence,
seemingly low accurate) representation of solutions.

<h4>Results for 3D</h4>

We now show the same figures in 3D: The first row shows the number of degrees
of freedom and computing time versus the $L_2$ error in the scalar variable
$u$ for CG and HDG at order $p$, the second row shows the
post-processed HDG solution instead of the original one, and the third row
compares the post-processed HDG solution with CG at order $p+1$. In 3D,
the volume-to-surface effect makes the cost of HDG somewhat higher and the CG
solution is clearly better than HDG for linears by any metric. For cubics, HDG
and CG are of similar quality, whereas HDG is again more efficient for sixth
order polynomials. One can alternatively also use the combination of FE_DGP
and FE_FaceP instead of (FE_DGQ, FE_FaceQ), which do not use tensor product
polynomials of degree $p$ but Legendre polynomials of <i>complete</i>
degree $p$. There are fewer degrees of freedom on the skeleton variable
for FE_FaceP for a given mesh size, but the solution quality (error vs. number
of DoFs) is very similar to the results for FE_FaceQ.

<table align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.3d_plain.png" width="400" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.3dt_plain.png" width="400" alt=""></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.3d_post.png" width="400" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.3dt_post.png" width="400" alt=""></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.3d_postb.png" width="400" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-51.3dt_postb.png" width="400" alt=""></td>
  </tr>
</table>

One final note on the efficiency comparison: We tried to use general-purpose
sparse matrix structures and similar solvers (optimal AMG preconditioners for
both without particular tuning of the AMG parameters on any of them) to give a
fair picture of the cost versus accuracy of two methods, on a toy example. It
should be noted however that geometric multigrid (GMG) for continuous finite
elements is about a factor four to five faster for $p=3$ and $p=6$. As of
2019, optimal-complexity iterative solvers for HDG are still under development
in the research community. Also, there are other implementation aspects for CG
available such as fast matrix-free approaches as shown in step-37 that make
higher order continuous elements more competitive. Again, it is not clear to
the authors of the tutorial whether similar improvements could be made for
HDG. We refer to <a href="https://dx.doi.org/10.1137/16M110455X">Kronbichler
and Wall (2018)</a> for a recent efficiency evaluation.


<h3>Possibilities for improvements</h3>

As already mentioned in the introduction, one possibility is to implement
another post-processing technique as discussed in the literature.

A second item that is not done optimally relates to the performance of this
program, which is of course an issue in practical applications (weighing in
also the better solution quality of (H)DG methods for transport-dominated
problems). Let us look at
the computing time of the tutorial program and the share of the individual
components:

<table align="center" class="doxtable">
  <tr>
    <th>&nbsp;</th>
    <th>&nbsp;</th>
    <th>Setup</th>
    <th>Assemble</th>
    <th>Solve</th>
    <th>Trace reconstruct</th>
    <th>Post-processing</th>
    <th>Output</th>
  </tr>
  <tr>
    <th>&nbsp;</th>
    <th>Total time</th>
    <th colspan="6">Relative share</th>
  </tr>
  <tr>
    <td align="left">2D, Q1, cycle 9, 37,248 dofs</td>
    <td align="center">5.34s</td>
    <td align="center">0.7%</td>
    <td align="center">1.2%</td>
    <td align="center">89.5%</td>
    <td align="center">0.9%</td>
    <td align="center">2.3%</td>
    <td align="center">5.4%</td>
  </tr>
  <tr>
    <td align="left">2D, Q3, cycle 9, 74,496 dofs</td>
    <td align="center">22.2s</td>
    <td align="center">0.4%</td>
    <td align="center">4.3%</td>
    <td align="center">84.1%</td>
    <td align="center">4.1%</td>
    <td align="center">3.5%</td>
    <td align="center">3.6%</td>
  </tr>
  <tr>
    <td align="left">3D, Q1, cycle 7, 172,800 dofs</td>
    <td align="center">9.06s</td>
    <td align="center">3.1%</td>
    <td align="center">8.9%</td>
    <td align="center">42.7%</td>
    <td align="center">7.0%</td>
    <td align="center">20.6%</td>
    <td align="center">17.7%</td>
  </tr>
  <tr>
    <td align="left">3D, Q3, cycle 7, 691,200 dofs</td>
    <td align="center">516s</td>
    <td align="center">0.6%</td>
    <td align="center">34.5%</td>
    <td align="center">13.4%</td>
    <td align="center">32.8%</td>
    <td align="center">17.1%</td>
    <td align="center">1.5%</td>
  </tr>
</table>

As can be seen from the table, the solver and assembly calls dominate the
runtime of the program. This also gives a clear indication of where
improvements would make the most sense:

<ol>
  <li> Better linear solvers: We use a BiCGStab iterative solver without
  preconditioner, where the number of iteration increases with increasing
  problem size (the number of iterations for Q1 elements and global
  refinements starts at 35 for the small sizes but increase up to 701 for the
  largest size). To do better, one could for example use an algebraic
  multigrid preconditioner from Trilinos, or some more advanced variants as
  the one discussed in <a
  href="https://dx.doi.org/10.1137/16M110455X">Kronbichler and Wall
  (2018)</a>. For diffusion-dominated problems such as the problem at hand
  with finer meshes, such a solver can be designed that uses the matrix-vector
  products from the more efficient ChunkSparseMatrix on the finest level, as
  long as we are not working in parallel with MPI. For MPI-parallelized
  computations, a standard TrilinosWrappers::SparseMatrix can be used.

  <li> Speed up assembly by pre-assembling parts that do not change from one
  cell to another (those that do neither contain variable coefficients nor
  mapping-dependent terms).
</ol>


examples/step-52/doc/intro.dox
<br>

<i>This program was contributed by Bruno Turcksin and Damien Lebrun-Grandie.</i>

@note In order to run this program, deal.II must be configured to use
the UMFPACK sparse direct solver. Refer to the <a
href="../../readme.html#umfpack">ReadMe</a> for instructions how to do this.

<a name="Intro"></a>
<h1>Introduction</h1>

This program shows how to use Runge-Kutta methods to solve a time-dependent
problem. It solves a small variation of the heat equation discussed first in
step-26 but, since the purpose of this program is only to demonstrate using
more advanced ways to interface with deal.II's time stepping algorithms, only
solves a simple problem on a uniformly refined mesh.


<h3>Problem statement</h3>

In this example, we solve the one-group time-dependent diffusion
approximation of the neutron transport equation (see step-28 for the
time-independent multigroup diffusion). This is a model for how neutrons move
around highly scattering media, and consequently it is a variant of the
time-dependent diffusion equation -- which is just a different name for the
heat equation discussed in step-26, plus some extra terms.
We assume that the medium is not
fissible and therefore, the neutron flux satisfies the following equation:
@f{eqnarray*}
\frac{1}{v}\frac{\partial \phi(x,t)}{\partial t} = \nabla \cdot D(x) \nabla \phi(x,t)
- \Sigma_a(x) \phi(x,t) + S(x,t)
@f}
augmented by appropriate boundary conditions. Here, $v$ is the velocity of
neutrons (for simplicity we assume it is equal to 1 which can be achieved by
simply scaling the time variable), $D$ is the diffusion coefficient,
$\Sigma_a$ is the absorption cross section, and $S$ is a source. Because we are
only interested in the time dependence, we assume that $D$ and $\Sigma_a$ are
constant.

Since this program only intends to demonstrate how to use advanced time
stepping algorithms, we will only look for the solutions of relatively simple
problems. Specifically, we are looking for a solution on a square domain
$[0,b]\times[0,b]$ of the form
@f{eqnarray*}
\phi(x,t) = A\sin(\omega t)(bx-x^2).
@f}
By using quadratic finite elements, we can represent this function exactly at
any particular time, and all the error will be due to the time
discretization. We do this because it is then easy to observe the order of
convergence of the various time stepping schemes we will consider, without
having to separate spatial and temporal errors.

We impose the following boundary conditions: homogeneous Dirichlet for $x=0$ and
$x=b$ and homogeneous Neumann conditions for $y=0$ and $y=b$. We choose the
source term so that the corresponding solution is
in fact of the form stated above:
@f{eqnarray*}
S=A\left(\frac{1}{v}\omega \cos(\omega t)(bx -x^2) + \sin(\omega t)
\left(\Sigma_a (bx-x^2)+2D\right) \right).
@f}
Because the solution is a sine in time, we know that the exact solution
satisfies $\phi\left(x,\frac{\pi}{\omega}\right) = 0$.
Therefore, the error at time $t=\frac{\pi}{\omega}$ is simply the norm of the numerical
solution, i.e., $\|e(\cdot,t=\frac{\pi}{\omega})\|_{L_2} = \|\phi_h(\cdot,t=\frac{\pi}{\omega})\|_{L_2}$,
and is particularly easily evaluated. In the code, we evaluate the $l_2$ norm
of the vector of nodal values of $\phi_h$ instead of the $L_2$ norm of the
associated spatial function, since the former is simpler to compute; however,
on uniform meshes, the two are just related by a constant and we can
consequently observe the temporal convergence order with either.


<h3>Runge-Kutta methods</h3>

The Runge-Kutta methods implemented in deal.II assume that the equation to be
solved can be written as:
@f{eqnarray*}
\frac{dy}{dt} = g(t,y).
@f}
On the other hand, when using finite elements, discretized time derivatives always result in the
presence of a mass matrix on the left hand side. This can easily be seen by
considering that if the solution vector $y(t)$ in the equation above is in fact the vector
of nodal coefficients $U(t)$ for a variable of the form
@f{eqnarray*}
  u_h(x,t) = \sum_j U_j(t) \varphi_j(x)
@f}
with spatial shape functions $\varphi_j(x)$, then multiplying an equation of
the form
@f{eqnarray*}
  \frac{\partial u(x,t)}{\partial t} = q(t,u(x,t))
@f}
by test functions, integrating over $\Omega$, substituting $u\rightarrow u_h$
and restricting the test functions to the $\varphi_i(x)$ from above, then this
spatially discretized equation has the form
@f{eqnarray*}
M\frac{dU}{dt} = f(t,U),
@f}
where $M$ is the mass matrix and $f(t,U)$ is the spatially discretized version
of $q(t,u(x,t))$ (where $q$ is typically the place where spatial
derivatives appear, but this is not of much concern for the moment given that
we only consider time derivatives). In other words, this form fits the general
scheme above if we write
@f{eqnarray*}
\frac{dy}{dt} = g(t,y) = M^{-1}f(t,y).
@f}

Runke-Kutta methods are time stepping schemes that approximate $y(t_n)\approx
y_{n}$ through a particular one-step approach. They are typically written in the form
@f{eqnarray*}
y_{n+1} = y_n + \sum_{i=1}^s b_i k_i
@f}
where for the form of the right hand side above
@f{eqnarray*}
k_i = h M^{-1} f\left(t_n+c_ih,y_n+\sum_{j=1}^sa_{ij}k_j\right).
@f}
Here $a_{ij}$, $b_i$, and $c_i$ are known coefficients that identify which
particular Runge-Kutta scheme you want to use, and $h=t_{n+1}-t_n$ is the time step
used. Different time stepping methods of the Runge-Kutta class differ in the
number of stages $s$ and the values they use for the coefficients $a_{ij}$,
$b_i$, and $c_i$ but are otherwise easy to implement since one can look up
tabulated values for these coefficients. (These tables are often called
Butcher tableaus.)

At the time of the writing of this tutorial, the methods implemented in
deal.II can be divided in three categories:
<ol>
<li> Explicit Runge-Kutta; in order for a method to be explicit, it is
necessary that in the formula above defining $k_i$, $k_i$ does not appear
on the right hand side. In other words, these methods have to satisfy
$a_{ii}=0, i=1,\ldots,s$.
<li> Embedded (or adaptive) Runge-Kutta; we will discuss their properties below.
<li> Implicit Runge-Kutta; this class of methods require the solution of a
possibly nonlinear system the stages $k_i$ above, i.e., they have
$a_{ii}\neq 0$ for at least one of the stages $i=1,\ldots,s$.
</ol>
Many well known time stepping schemes that one does not typically associate
with the names Runge or Kutta can in fact be written in a way so that they,
too, can be expressed in these categories. They oftentimes represent the
lowest-order members of these families.


<h4>Explicit Runge-Kutta methods</h4>

These methods, only require a function to evaluate $M^{-1}f(t,y)$ but not
(as implicit methods) to solve an equation that involves
$f(t,y)$ for $y$. As all explicit time stepping methods, they become unstable
when the time step chosen is too large.

Well known methods in this class include forward Euler, third order
Runge-Kutta, and fourth order Runge-Kutta (often abbreviated as RK4).


<h4>Embedded Runge-Kutta methods</h4>

These methods use both a lower and a higher order method to
estimate the error and decide if the time step needs to be shortened or can be
increased. The term "embedded" refers to the fact that the lower-order method
does not require additional evaluates of the function $M^{-1}f(\cdot,\cdot)$
but reuses data that has to be computed for the high order method anyway. It
is, in other words, essentially free, and we get the error estimate as a side
product of using the higher order method.

This class of methods include Heun-Euler, Bogacki-Shampine, Dormand-Prince (ode45 in
Matlab and often abbreviated as RK45 to indicate that the lower and higher order methods
used here are 4th and 5th order Runge-Kutta methods, respectively), Fehlberg,
and Cash-Karp.

At the time of the writing, only embedded explicit methods have been implemented.


<h4>Implicit Runge-Kutta methods</h4>

Implicit methods require the solution of (possibly nonlinear) systems of the
form $\alpha y = f(t,y)$
for $y$ in each (sub-)timestep. Internally, this is
done using a Newton-type method and, consequently, they require that the user
provide functions that can evaluate $M^{-1}f(t,y)$ and
$\left(I-\tau M^{-1} \frac{\partial f}{\partial y}\right)^{-1}$ or equivalently
$\left(M - \tau \frac{\partial f}{\partial y}\right)^{-1} M$.

The particular form of this operator results from the fact that each Newton
step requires the solution of an equation of the form
@f{align*}
  \left(M - \tau \frac{\partial f}{\partial y}\right) \Delta y
  = -M h(t,y)
@f}
for some (given) $h(t,y)$. Implicit methods are
always stable, regardless of the time step size, but too large time steps of
course affect the <i>accuracy</i> of the solution, even if the numerical
solution remains stable and bounded.

Methods in this class include backward Euler, implicit midpoint,
Crank-Nicolson, and the two stage SDIRK method (short for "singly diagonally
implicit Runge-Kutta", a term coined to indicate that the diagonal elements
$a_{ii}$ defining the time stepping method are all equal; this property
allows for the Newton matrix $I-\tau M^{-1}\frac{\partial f}{\partial y}$ to
be re-used between stages because $\tau$ is the same every time).


<h3>Spatially discrete formulation</h3>

By expanding the solution of our model problem
as always using shape functions $\psi_j$ and writing
@f{eqnarray*}
\phi_h(x,t) = \sum_j U_j(t) \psi_j(x),
@f}
we immediately get the spatially discretized version of the diffusion equation as
@f{eqnarray*}
  M \frac{dU(t)}{dt}
  = -{\cal D} U(t) - {\cal A} U(t) + {\cal S}(t)
@f}
where
@f{eqnarray*}
  M_{ij}  &=& (\psi_i,\psi_j), \\
  {\cal D}_{ij}  &=& (D\nabla\psi_i,\nabla\psi_j)_\Omega, \\
  {\cal A}_{ij}  &=& (\Sigma_a\psi_i,\psi_j)_\Omega, \\
  {\cal S}_{i}(t)  &=& (\psi_i,S(x,t))_\Omega.
@f}
See also step-24 and step-26 to understand how we arrive here.
Boundary terms are not necessary due to the chosen boundary conditions for
the current problem. To use the Runge-Kutta methods, we recast this
as follows:
@f{eqnarray*}
f(y) = -{\cal D}y - {\cal A}y + {\cal S}.
@f}
In the code, we will need to be able to evaluate this function $f(U)$ along
with its derivative,
@f{eqnarray*}
\frac{\partial f}{\partial y} = -{\cal D} - {\cal A}.
@f}


<h3>Notes on the testcase</h3>

To simplify the problem, the domain is two dimensional and the mesh is
uniformly refined (there is no need to adapt the mesh since we use quadratic
finite elements and the exact solution is quadratic). Going from a two
dimensional domain to a three dimensional domain is not very
challenging. However if you intend to solve more complex problems where the
mesh must be adapted (as is done, for example, in step-26), then it is
important to remember the following issues:

<ol>
<li> You will need to project the solution to the new mesh when the mesh is changed. Of course,
     the mesh
     used should be the same from the beginning to the end of each time step,
     a question that arises because Runge-Kutta methods use multiple
     evaluations of the equations within each time step.
<li> You will need to update the mass matrix and its inverse every time the
     mesh is changed.
</ol>
The techniques for these steps are readily available by looking at step-26.


examples/step-52/doc/results.dox
<h1>Results</h1>

The point of this program is less to show particular results, but instead to
show how it is done. This we have already demonstrated simply by discussing
the code above. Consequently, the output the program yields is relatively
sparse and consists only of the console output and the solutions given in VTU
format for visualization.

The console output contains both errors and, for some of the methods, the
number of steps they performed:
@code
Explicit methods:
   Forward Euler:            error=1.00883
   Third order Runge-Kutta:  error=0.000227982
   Fourth order Runge-Kutta: error=1.90541e-06

Implicit methods:
   Backward Euler:           error=1.03428
   Implicit Midpoint:        error=0.00862702
   Crank-Nicolson:           error=0.00862675
   SDIRK:                    error=0.0042349

Embedded explicit methods:
   Heun-Euler:               error=0.0073012
                   steps performed=284
   Bogacki-Shampine:         error=0.000408407
                   steps performed=181
   Dopri:                    error=0.000836695
                   steps performed=120
   Fehlberg:                 error=0.00248922
                   steps performed=106
   Cash-Karp:                error=0.0787735
                   steps performed=106
@endcode

As expected the higher order methods give (much) more accurate solutions. We
also see that the (rather inaccurate) Heun-Euler method increased the number of
time steps in order to satisfy the tolerance. On the other hand, the other
embedded methods used a lot less time steps than what was prescribed.


examples/step-53/doc/intro.dox
<br>

<i>This program was contributed by Wolfgang Bangerth and Luca Heltai, using
data provided by D. Sarah Stamps.</i>

@note This program elaborates on concepts of geometry and the classes that
implement it. These classes are grouped into the documentation module on @ref
manifold "Manifold description for triangulations". See there for additional
information.

@note This tutorial is also available as a Jupyter Python notebook that
  uses the deal.II python interface. The notebook is available in the
  same directory as the original C++ program. Rendered notebook can also
  be viewed on the <a
href="https://github.com/dealii/dealii/blob/master/examples/step-53/step-53.ipynb">github</a>.


<a name="Intro"></a>
<h1>Introduction</h1>

Partial differential equations for realistic problems are often posed on
domains with complicated geometries. To provide just a few examples, consider
these cases:
- Among the two arguably most important industrial applications for the finite
  element method, aerodynamics and more generally fluid dynamics is
  one. Computer simulations today are used in the design of every airplane,
  car, train and ship. The domain in which the partial differential equation
  is posed is, in these cases, the air surrounding the plane with its wings,
  flaps and engines; the air surrounding the car with its wheel, wheel wells,
  mirrors and, in the case of race cars, all sorts of aerodynamic equipment;
  the air surrounding the train with its wheels and gaps between cars. In the
  case of ships, the domain is the water surrounding the ship with its rudders
  and propellers.
- The other of the two big applications of the finite element method is
  structural engineering in which the domains are bridges, airplane nacelles
  and wings, and other solid bodies of often complicated shapes.
- Finite element modeling is also often used to describe the generation and
  propagation of earthquake waves. In these cases, one needs to accurately
  represent the geometry of faults in the Earth crust. Since faults intersect,
  dip at angles, and are often not completely straight, domains are frequently
  very complex.
One could cite many more examples of complicated geometries in which one wants
to pose and solve a partial differential equation. What this shows is that the
"real" world is much more complicated than what we have shown in almost all of
the tutorial programs preceding this one.

This program is therefore devoted to showing how one deals with complex
geometries using a concrete application. In particular, what it shows is how
we make a mesh fit the domain we want to solve on. On the other hand, what the
program does not show is how to create a coarse for a domain. The process to
arrive at a coarse mesh is called "mesh generation" and there are a number of
high-quality programs that do this much better than we could ever
implement. However, deal.II does have the ability to read in meshes in many
formats generated by mesh generators and then make them fit a given shape,
either by deforming a mesh or refining it a number of times until it fits. The
deal.II Frequently Asked Questions page referenced from http://www.dealii.org/
provides resources to mesh generators.


<h3>Where geometry and meshes intersect</h3>

Let us assume that you have a complex domain and that you already have a
coarse mesh that somehow represents the general features of the domain. Then
there are two situations in which it is necessary to describe to a deal.II
program the details of your geometry:

- Mesh refinement: Whenever a cell is refined, it is necessary to introduce
  new vertices in the Triangulation. In the simplest case, one assumes that
  the objects that make up the Triangulation are straight line segments, a
  bi-linear surface or a tri-linear volume. The next vertex is then simply put
  into the middle of the old ones. However, for curved boundaries or if we
  want to solve a PDE on a curved, lower-dimensional manifold embedded in a
  higher-dimensional space, this is insufficient since it will not respect the
  actual geometry. We will therefore have to tell Triangulation where to put
  new points.

- Integration: When using higher order finite element methods, it is often
  necessary to compute integrals using curved approximations of the boundary,
  i.e., describe each edge or face of cells as curves, instead of straight
  line segments or bilinear patches. The same is, of course, true when
  integrating boundary terms (e.g., inhomogeneous Neumann boundary
  conditions). For the purpose of integration, the various Mapping classes
  then provide the transformation from the reference cell to the actual cell.

In both cases, we need a way to provide information about the geometry of the
domain at the level of an individual cell, its faces and edges. This is where
the Manifold class comes into play. Manifold is an abstract base class that
only defines an interface by which the Triangulation and Mapping classes can
query geometric information about the domain. Conceptually, Manifold sees the
world in a way not dissimilar to how the mathematical subdiscipline geometry
sees it: a domain is essentially just a collection of points that is somehow
equipped with the notion of a distance between points so that we can obtain a
point "in the middle" of some other points.

deal.II provides a number of classes that implement the interface provided by
Manifold for a variety of common geometries. On the other hand, in this
program we will consider only a very common and much simpler case, namely the
situation where (a part of) the domain we want to solve on can be described by
transforming a much simpler domain (we will call this the "reference domain").
In the language of mathematics, this means
that the (part of the) domain is a <a
href="http://en.wikipedia.org/wiki/Chart_%28topology%29">chart</a>. Charts are
described by a smooth function that maps from the simpler domain to the chart
(the "push-forward" function) and its inverse (the "pull-back" function). If
the domain as a whole is not a chart (e.g., the surface of a sphere), then it
can often be described as a collection of charts (e.g., the northern
hemisphere and the southern hemisphere are each charts) and the domain can then
be describe by an <a
href="http://en.wikipedia.org/wiki/Atlas_%28topology%29">atlas</a>.

If a domain can be decomposed into an atlas, all we need to do is provide the
pull-back and push-forward functions for each of the charts. In deal.II, this
means providing a class derived from ChartManifold, and this is precisely what
we will do in this program.


<h3>The example case</h3>

To illustrate how one describes geometries using charts in deal.II, we will
consider a case that originates in an application of the <a
href="https://aspect.geodynamics.org">ASPECT mantle convection code</a>, using a
data set provided by D. Sarah Stamps. In the concrete application, we were
interested in describing flow in the Earth mantle under the <a
href="http://en.wikipedia.org/wiki/East_African_rift">East African Rift</a>, a
zone where two continental plates drift apart. Not to beat around the bush,
the geometry we want to describe looks like this:

<img src="https://www.dealii.org/images/steps/developer/step-53.topo.png" alt="">

In particular, though you cannot see this here, the top surface is not
just colored by the elevation but is, in fact, deformed to follow the
correct topography.
While the actual application is not relevant here, the geometry is. The domain
we are interested in is a part of the Earth that ranges from the surface to a
depth of 500km, from 26 to 35 degrees East of the Greenwich meridian, and from
5 degrees North of the equator to 10 degrees South.

This description of the geometry suggests to start with a box
$\hat U=[26,35]\times[-10,5]\times[-500000,0]$ (measured in degrees,
degrees, and meters) and to provide a map $\varphi$ so
that $\varphi^{-1}(\hat U)=\Omega$ where $\Omega$ is the domain we
seek. $(\Omega,\varphi)$ is then a chart, $\varphi$ the pull-back operator, and
$\varphi^{-1}$ the push-forward operator. If we need a point $q$ that is the
"average" of other points $q_i\in\Omega$, the ChartManifold class then first
applies the pull-back to obtain $\hat q_i=\varphi(q_i)$, averages these to a
point $\hat p$ and then computes $p=\varphi^{-1}(\hat p)$.

Our goal here is therefore to implement a class that describes $\varphi$ and
$\varphi^{-1}$. If Earth was a sphere, then this would not be difficult: if we
denote by $(\hat \phi,\hat \theta,\hat d)$ the points of $\hat U$ (i.e.,
longitude counted eastward, latitude counted northward, and elevation relative
to zero depth), then
@f[
  \mathbf x = \varphi^{-1}(\hat \phi,\hat \theta,\hat d)
  = (R+\hat d) (\cos\hat \phi\cos\hat \theta, \sin\hat \phi\cos\hat \theta, \sin\hat \theta)^T
@f]
provides coordinates in a Cartesian coordinate system, where $R$ is the radius
of the sphere. However, the Earth is not a sphere:

<ol>
<li> It is flattened at the poles and larger at the equator: the semi-major axis
  is approximately 22km longer than the semi-minor axis. We will account for
  this using the <a href="http://en.wikipedia.org/wiki/WGS84">WGS 84</a>
  reference standard for the Earth shape. The formula used in WGS 84 to obtain
  a position in Cartesian coordinates from longitude, latitude, and elevation
  is
@f[
  \mathbf x = \varphi_\text{WGS84}^{-1}(\phi,\theta,d)
  = \left(
    \begin{array}{c}
     (\bar R(\theta)+d) \cos\phi\cos\theta, \\
     (\bar R(\theta)+d) \sin\phi\cos\theta, \\
     ((1-e^2)\bar R(\theta)+d) \sin\theta
    \end{array}
    \right),
@f]
  where $\bar R(\theta)=\frac{R}{\sqrt{1-(e \sin\theta)^2}}$, and radius and
  ellipticity are given by $R=6378137\text{m}, e=0.081819190842622$. In this formula,
  we assume that the arguments to sines and cosines are evaluated in degree, not
  radians (though we will have to change this assumption in the code).

<li> It has topography in the form of mountains and valleys. We will account for
  this using real topography data (see below for a description of where
  this data comes from). Using this data set, we can look up elevations on a
  latitude-longitude mesh laid over the surface of the Earth. Starting with
  the box $\hat U=[26,35]\times[-10,5]\times[-500000,0]$, we will therefore
  first stretch it in vertical direction before handing it off to the WGS 84
  function: if $h(\hat\phi,\hat\theta)$ is the height at longitude $\hat\phi$
  and latitude $\hat\theta$, then we define
@f[
  (\phi,\theta,d) =
  \varphi_\text{topo}^{-1}(\hat\phi,\hat\theta,\hat d)
  = \left(
      \hat\phi,
      \hat\theta,
      \hat d + \frac{\hat d+500000}{500000}h(\hat\phi,\hat\theta)
    \right).
@f]
  Using this function, the top surface of the box $\hat U$ is displaced to the
  correct topography, the bottom surface remains where it was, and points in
  between are linearly interpolated.
</ol>

Using these two functions, we can then define the entire push-forward function
$\varphi^{-1}: \hat U \rightarrow \Omega$ as
@f[
  \mathbf x
  =
  \varphi^{-1}(\hat\phi,\hat\theta,\hat d)
  =
  \varphi_\text{WGS84}^{-1}(\varphi_\text{topo}^{-1}(\hat\phi,\hat\theta,\hat d)).
@f]
In addition, we will have to define the inverse of this function, the
pull-back operation, which we can write as
@f[
  (\hat\phi,\hat\theta,\hat d)
  =
  \varphi(\mathbf x)
  =
  \varphi_\text{topo}(\varphi_\text{WGS84}(\mathbf x)).
@f]
We can obtain one of the components of this function by inverting the formula above:
@f[
  (\hat\phi,\hat\theta,\hat d) =
  \varphi_\text{topo}(\phi,\theta,d)
  = \left(
      \phi,
      \theta,
      500000\frac{d-h(\phi,\theta)}{500000+h(\phi,\theta)}
    \right).
@f]
Computing $\varphi_\text{WGS84}(\mathbf x)$ is also possible though a lot more
awkward. We won't show the formula here but instead only provide the implementation
in the program.


<h3>Implementation</h3>

There are a number of issues we need to address in the program. At the largest scale,
we need to write a class that implements the interface of ChartManifold. This involves
a function <code>push_forward()</code> that takes a point
in the reference domain $\hat U$ and transform it into real space using the function
$\varphi^{-1}$ outlined above, and its inverse function <code>pull_back()</code>
implementing $\varphi$. We will do so in the <code>AfricaGeometry</code> class below
that looks, in essence, like this:
@code
  class AfricaGeometry : public ChartManifold<3,3>
  {
  public:
    virtual
    Point<3>
    pull_back(const Point<3> &space_point) const;

    virtual
    Point<3>
    push_forward(const Point<3> &chart_point) const;

  private:
    ... some member variables and other member functions...;
  };
@endcode

The transformations above have two parts: the WGS 84 transformations and the topography
transformation. Consequently, the <code>AfricaGeometry</code> class will have
additional (non-virtual) member functions
<code>AfricaGeometry::push_forward_wgs84()</code> and
<code>AfricaGeometry::push_forward_topo()</code> that implement these two pieces, and
corresponding pull back functions.

The WGS 84 transformation functions are not particularly interesting (even though the
formulas they implement are impressive). The more interesting part is the topography
transformation. Recall that for this, we needed to evaluate the elevation function
$h(\hat\phi,\hat\theta)$. There is of course no formula for this: Earth is what it is,
the best one can do is look up the altitude from some table. This is, in fact what we
will do.

The data we use was originally created by the  <a
href="http://en.wikipedia.org/wiki/Shuttle_Radar_Topography_Mission">Shuttle
Radar Topography Mission</a>, was downloaded from the US Geologic Survey
(USGS) and processed by D. Sarah Stamps who also wrote the initial version of
the WGS 84 transformation functions. The topography data so processed is
stored in a file <code>topography.txt.gz</code> that, when unpacked
looks like this:
@code
6.983333 25.000000 700
6.983333 25.016667 692
6.983333 25.033333 701
6.983333 25.050000 695
6.983333 25.066667 710
6.983333 25.083333 702
...
-11.983333 35.950000 707
-11.983333 35.966667 687
-11.983333 35.983333 659
@endcode
The data is formatted as <code>latitude longitude elevation</code> where the first two
columns are provided in degrees North of the equator and degrees East of the Greenwich
meridian. The final column is given in meters above the WGS 84 zero elevation.

In the transformation functions, we need to evaluate $h(\hat\phi,\hat\theta)$ for a given
longitude $\hat\phi$ and latitude $\hat\theta$. In general, this data point will not be
available and we will have to interpolate between adjacent data points. Writing such an
interpolation routine is not particularly difficult, but it is a bit tedious and error
prone. Fortunately, we can somehow shoehorn this data set into an existing class:
Functions::InterpolatedUniformGridData . Unfortunately, the class does not fit the bill
quite exactly and so we need to work around it a bit. The problem comes from the way
we initialize this class: in its simplest form, it takes a stream of values that it
assumes form an equispaced mesh in the $x-y$ plane (or, here, the $\phi-\theta$ plane).
Which is what they do here, sort of: they are ordered latitude first, longitude second;
and more awkwardly, the first column starts at the largest values and counts down,
rather than the usual other way around.

Now, while tutorial programs are meant to illustrate how to code with deal.II, they do
not necessarily have to satisfy the same quality standards as one would have to do
with production codes. In a production code, we would write a function that reads the
data and (i) automatically determines the extents of the first and second column,
(ii) automatically determines the number of data points in each direction, (iii) does
the interpolation regardless of the order in which data is arranged, if necessary
by switching the order between reading and presenting it to the
Functions::InterpolatedUniformGridData class.

On the other hand, tutorial programs are best if they are short and demonstrate key
points rather than dwell on unimportant aspects and, thereby, obscure what we really
want to show. Consequently, we will allow ourselves a bit of leeway:
- since this program is intended solely for a particular geometry around the area
  of the East-African rift and since this is precisely the area described by the data
  file, we will hardcode in the program that there are
  $1139\times 660$ pieces of data;
- we will hardcode the boundaries of the data
  $[-11.98333^\circ,6.983333^\circ]\times[25^\circ,35.98333^\circ]$;
- we will lie to the Functions::InterpolatedUniformGridData class: the class will
  only see the data in the last column of this data file, and we will pretend that
  the data is arranged in a way that there are 1139 data points in the first
  coordinate direction that are arranged in <i>ascending</i> order but in an
  interval $[-6.983333^\circ,11.98333^\circ]$ (not the negated bounds). Then,
  when we need to look something up for a latitude $\hat\theta$, we can ask the
  interpolating table class for a value at $-\hat\theta$. With this little
  trick, we can avoid having to switch around the order of data as read from
  file.

All of this then calls for a class that essentially looks like this:
@code
  class AfricaTopography
  {
  public:
    AfricaTopography ()
      :
      topography_data (...initialize somehow...)
    {}

    double value (const double lon, const double lat) const
    {
      return topography_data.value (Point<2>(-lat * 180/numbers::PI,
                                             lon * 180/numbers::PI));
    }

  private:
    const Functions::InterpolatedUniformGridData<2> topography_data;
  };
@endcode

Note how the <code>value()</code> function negates the latitude. It also switches
from the format $\phi,\theta$ that we use everywhere else to the latitude-longitude
format used in the table. Finally, it takes its arguments in radians as that is what
we do everywhere else in the program, but then converts them to the degree-based
system used for table lookup. As you will see in the implementation below, the function
has a few more (static) member functions that we will call in the initialization
of the <code>topography_data</code> member variable: the class type of this variable
has a constructor that allows us to set everything right at construction time,
rather than having to fill data later on, but this constructor takes a number of
objects that can't be constructed in-place (at least not in C++98). Consequently,
the construction of each of the objects we want to pass in the initialization happens
in a number of static member functions.

Having discussed the general outline of how we want to implement things, let us go
to the program and show how it is done in practice.


examples/step-53/doc/results.dox
<h1>Results</h1>

Running the program produces a mesh file <code>mesh.vtu</code> that we can
visualize with any of the usual visualization programs that can read the VTU
file format. If one just looks at the mesh itself, it is actually very difficult
to see anything that doesn't just look like a perfectly round piece of a
sphere (though if one modified the program so that it does produce a sphere and
looked at them at the same time, the difference between the overall sphere and
WGS 84 shape is quite apparent). Apparently, Earth is actually quite a flat place.
Of course we already know this from satellite pictures.
However, we can tease out something more by
coloring cells by their volume. This both produces slight variations in hue
along the top surface and something for the visualization programs to apply
their shading algorithms to (because the top surfaces of the cells are now no
longer just tangential to a sphere but tilted):

<img src="https://www.dealii.org/images/steps/developer/step-53.mesh.png" alt="">

Yet, at least as far as visualizations are concerned, this is still not too
impressive. Rather, let us visualize things in a way so that we show the
actual elevation along the top surface. In other words, we want a picture like
this, with an incredible amount of detail:

<img src="https://www.dealii.org/images/steps/developer/step-53.topo.png" alt="">

A zoom-in of this picture shows the vertical displacement quite clearly (here,
looking from the West-Northwest over the rift valley, the triple peaks
of
<a href="http://en.wikipedia.org/wiki/Mount_Stanley">Mount Stanley</a>,
<a href="http://en.wikipedia.org/wiki/Mount_Speke">Mount Speke</a>, and
<a href="http://en.wikipedia.org/wiki/Mount_Baker_%28Uganda%29">Mount Baker</a>
in the
<a href="http://en.wikipedia.org/wiki/Rwenzori_Mountains">Rwenzori Range</a>,
<a href="http://en.wikipedia.org/wiki/Lake_George_%28Uganda%29">Lake
George</a>
and toward the great flatness of
<a href="http://en.wikipedia.org/wiki/Lake_Victoria">Lake Victoria</a>):

<img src="https://www.dealii.org/images/steps/developer/step-53.topozoom.png" alt="">


These image were produced with three small modifications:
<ol>
  <li> An additional seventh mesh refinement towards the top surface for the
  first of these two pictures, and a total of nine for the second. In the
  second image, the horizontal mesh size is approximately 1.5km, and just
  under 1km in vertical direction. (The picture was also created using a
  more resolved data set; however, it is too big to distribute as part of
  the tutorial.)

  <li> The addition of the following function that, given a point
  <code>x</code> computes the elevation by converting the point to
  reference WGS 84 coordinates and only keeping the depth variable (the
  function is, consequently, a simplified version of the
  <code>AfricaGeometry::pull_back_wgs84()</code> function):

@code
#include <deal.II/fe/fe_q.h>
#include <deal.II/dofs/dof_handler.h>
#include <deal.II/numerics/data_out.h>
#include <deal.II/numerics/vector_tools.h>


double get_elevation (const Point<3> &x)
  {
    const double R           = 6378137;
    const double ellipticity = 8.1819190842622e-2;

    const double b     = std::sqrt(R * R * (1 - ellipticity * ellipticity));
    const double ep    = std::sqrt((R * R - b * b) / (b * b));
    const double p     = std::sqrt(x(0) * x(0) + x(1) * x(1));
    const double th    = std::atan2(R * x(2), b * p);
    const double theta = std::atan2((x(2) + ep * ep * b * std::sin(th) * std::sin(th) * std::sin(th)),
                                      (p - (ellipticity * ellipticity * R  * (std::cos(th) * std::cos(th) * std::cos(th)))));
    const double R_bar = R / (std::sqrt(1 - ellipticity * ellipticity * std::sin(theta) * std::sin(theta)));
    const double R_plus_d = p / std::cos(theta);

    return R_plus_d - R_bar;
  }
@endcode

  <li>Adding the following piece to the bottom of the <code>run()</code> function:

@code
      FE_Q<3>       fe(1);
      DoFHandler<3> dof_handler (triangulation);
      dof_handler.distribute_dofs(fe);

      Vector<double> elevation (dof_handler.n_dofs());
      {
        std::map<unsigned int,double> boundary_values;
        VectorTools::interpolate_boundary_values(dof_handler,
                                                 5,
                                                 ScalarFunctionFromFunctionObject<3>(get_elevation),
                                                 boundary_values);
        for (std::map<unsigned int,double>::const_iterator p = boundary_values.begin();
             p!=boundary_values.end(); ++p)
          elevation[p->first] = p->second;
      }

      DataOut<3>    data_out;
      data_out.attach_dof_handler(dof_handler);
      data_out.add_data_vector (elevation, "elevation");
      data_out.build_patches();

      std::ofstream out ("data.vtu");
      data_out.write_vtu (out);
@endcode
</ol>
This last piece of code first creates a $Q_1$ finite element space on the mesh.
It then (ab)uses VectorTools::interpolate_boundary_values() to evaluate the
elevation function for every node at the top boundary (the one with boundary
indicator 5). We here wrap the call to <code>get_elevation()</code> with the
ScalarFunctionFromFunctionObject class to make a regular C++ function look
like an object of a class derived from the Function class that we want
to use in VectorTools::interpolate_boundary_values(). Having so gotten a list
of degrees of freedom located at the top boundary and corresponding elevation
values, we just go down this list and set these elevations in the
<code>elevation</code> vector (leaving all interior degrees of freedom at
their original zero value). This vector is then output using DataOut as
usual and can be visualized as shown above.


<h3>Issues with adaptively refined meshes generated this way</h3>

If you zoomed in on the mesh shown above and looked closely enough, you would
find that at hanging nodes, the two small edges connecting to the hanging
nodes are not in exactly the same location as the large edge of the
neighboring cell. This can be shown more clearly by using a different surface
description in which we enlarge the vertical topography to enhance the effect
(courtesy of Alexander Grayver):

<img src="https://www.dealii.org/images/steps/developer/step-53.smooth-geometry.png" alt="">

So what is happening here? Partly, this is only a result of visualization, but
there is an underlying real cause as well:

<ul>
  <li>When you visualize a mesh using any of the common visualization
  programs, what they really show you is just a set of edges that are plotted
  as straight lines in three-dimensional space. This is so because almost all
  data file formats for visualizing data only describe hexahedral cells as a
  collection of eight vertices in 3d space, and do not allow to any more
  complicated descriptions. (This is the main reason why
  DataOut::build_patches() takes an argument that can be set to something
  larger than one.) These linear edges may be the edges of the cell you do
  actual computations on, or they may not, depending on what kind of mapping
  you use when you do your integrations using FEValues. By default, of course,
  FEValues uses a linear mapping (i.e., an object of class MappingQ1) and in
  that case a 3d cell is indeed described exclusively by its 8 vertices and
  the volume it fills is a trilinear interpolation between these points,
  resulting in linear edges. But, you could also have used tri-quadratic,
  tri-cubic, or even higher order mappings and in these cases the volume of
  each cell will be bounded by quadratic, cubic or higher order polynomial
  curves. Yet, you only get to see these with linear edges in the
  visualization program because, as mentioned, file formats do not allow to
  describe the real geometry of cells.

  <li>That said, let us for simplicity assume that you are indeed using a
  trilinear mapping, then the image shown above is a faithful representation
  of the cells on which you form your integrals. In this case, indeed the
  small cells at a hanging nodes do not, in general, snugly fit against the
  large cell but leave a gap or may intersect the larger cell. Why is this?
  Because when the triangulation needs a new vertex on an edge it wants to
  refine, it asks the manifold description where this new vertex is supposed
  to be, and the manifold description duly returns such a point by (in the
  case of a geometry derived from ChartManifold) pulling the adjacent points
  of the line back to the reference domain, averaging their locations, and
  pushing forward this new location to the real domain. But this new location
  is not usually along a straight line (in real space) between the adjacent
  vertices and consequently the two small straight lines forming the refined
  edge do not lie exactly on the one large straight line forming the unrefined
  side of the hanging node.
</ul>

The situation is slightly more complicated if you use a higher order mapping
using the MappingQ class, but not fundamentally different. Let's take a
quadratic mapping for the moment (nothing fundamental changes with even higher
order mappings). Then you need to imagine each edge of the cells you integrate
on as a quadratic curve despite the fact that you will never actually see it
plotted that way by a visualization program. But imagine it that way for a
second. So which quadratic curve does MappingQ take? It is the quadratic curve
that goes through the two vertices at the end of the edge as well as a point
in the middle that it queries from the manifold. In the case of the long edge
on the unrefined side, that's of course exactly the location of the hanging
node, so the quadratic curve describing the long edge does go through the
hanging node, unlike in the case of the linear mapping. But the two small
edges are also quadratic curves; for example, the left small edge will go
through the left vertex of the long edge and the hanging node, plus a point it
queries halfway in between from the manifold. Because, as before, the point
the manifold returns halfway along the left small edge is rarely exactly on
the quadratic curve describing the long edge, the quadratic short edge will
typically not coincide with the left half of the quadratic long edge, and the
same is true for the right short edge. In other words, again, the geometries
of the large cell and its smaller neighbors at hanging nodes do not touch
snuggly.

This all begs two questions: first, does it matter, and second, could this be
fixed. Let us discuss these in the following:

<ul>
  <li>Does it matter? It is almost certainly true that this depends on the
  equation you are solving. For example, it is known that solving the Euler
  equations of gas dynamics on complex geometries requires highly accurate
  boundary descriptions to ensure convergence of quantities that are measure
  the flow close to the boundary. On the other hand, equations with elliptic
  components (e.g., the Laplace or Stokes equations) are typically rather
  forgiving of these issues: one does quadrature anyway to approximate
  integrals, and further approximating the geometry may not do as much harm as
  one could fear given that the volume of the overlaps or gaps at every
  hanging node is only ${\cal O}(h^d)$ even with a linear mapping and ${\cal
  O}(h^{d+p-1})$ for a mapping of degree $p$. (You can see this by considering
  that in 2d the gap/overlap is a triangle with base $h$ and height ${\cal
  O}(h)$; in 3d, it is a pyramid-like structure with base area $h^2$ and
  height ${\cal O}(h)$. Similar considerations apply for higher order mappings
  where the height of the gaps/overlaps is ${\cal O}(h^p)$.) In other words,
  if you use a linear mapping with linear elements, the error in the volume
  you integrate over is already at the same level as the integration error
  using the usual Gauss quadrature. Of course, for higher order elements one
  would have to choose matching mapping objects.

  Another point of view on why it is probably not worth worrying too much
  about the issue is that there is certainly no narrative in the community of
  numerical analysts that these issues are a major concern one needs to watch
  out for when using complex geometries. If it does not seem to be discussed
  often among practitioners, if ever at all, then it is at least not something
  people have identified as a common problem.

  This issue is not dissimilar to having hanging nodes at curved boundaries
  where the geometry description of the boundary typically pulls a hanging
  node onto the boundary whereas the large edge remains straight, making the
  adjacent small and large cells not match each other. Although this behavior
  existed in deal.II since its beginning, 15 years before manifold
  descriptions became available, it did not ever come up in mailing list
  discussions or conversations with colleagues.

  <li>Could it be fixed? In principle, yes, but it's a complicated
  issue. Let's assume for the moment that we would only ever use the MappingQ1
  class, i.e., linear mappings. In that case, whenever the triangulation class
  requires a new vertex along an edge that would become a hanging node, it
  would just take the mean value of the adjacent vertices <i>in real
  space</i>, i.e., without asking the manifold description. This way, the
  point lies on the long straight edge and the two short straight edges would
  match the one long edge. Only when all adjacent cells have been refined and
  the point is no longer a hanging node would we replace its coordinates by
  coordinates we get by a manifold. This may be awkward to implement, but it
  would certainly be possible.

  The more complicated issue arises because people may want to use a higher
  order MappingQ object. In that case, the Triangulation class may freely
  choose the location of the hanging node (because the quadratic curve for the
  long edge can be chosen in such a way that it goes through the hanging node)
  but the MappingQ class, when determining the location of mid-edge points
  must make sure that if the edge is one half of a long edge of a neighboring
  coarser cell, then the midpoint cannot be obtained from the manifold but
  must be chosen along the long quadratic edge. For cubic (and all other odd)
  mappings, the matter is again a bit complicated because one typically
  arranges the cubic edge to go through points 1/3 and 2/3 along the edge, and
  thus necessarily through the hanging node, but this could probably be worked
  out. In any case, even then, there are two problems with this:

  - When refining the triangulation, the Triangulation class can not know what
    mapping will be used. In fact it is not uncommon for a triangulation to be
    used differently in different contexts within the same program. If the
    mapping used determines whether we can freely choose a point or not, how,
    then, should the triangulation locate new vertices?
  - Mappings are purely local constructs: they only work on a cell in
    isolation, and this is one of the important features of the finite element
    method. Having to ask whether one of the vertices of an edge is a hanging
    node requires querying the neighborhood of a cell; furthermore, such a
    query does not just involve the 6 face neighbors of a cell in 3d, but may
    require traversing a possibly very large number of other cells that
    connect to an edge. Even if it can be done, one still needs to do
    different things depending on how the neighborhood looks like, producing
    code that is likely very complex, hard to maintain, and possibly slow.

  Consequently, at least for the moment, none of these ideas are
  implemented. This leads to the undesirable consequence of discontinuous
  geometries, but, as discussed above, the effects of this do not appear to
  pose problem in actual practice.

</ul>


examples/step-54/doc/intro.dox
<br>

<i>This program was contributed by Andrea Mola and Luca Heltai.</i>

@note This program elaborates on concepts of industrial geometry, using tools
that interface with the OpenCASCADE library (http://www.opencascade.org) that
allow the specification of arbitrary IGES files to describe the boundaries for
your geometries.

@dealiiTutorialDOI{10.5281/zenodo.546220,https://zenodo.org/badge/DOI/10.5281/zenodo.546220.svg}

<a name="Intro"></a>
<h1>Introduction</h1>


In some of the previous tutorial programs (step-1, step-3, step-5, step-6 and
step-49 among others) we have learned how to use the mesh refinement methods
provided in deal.II. These tutorials have shown how to employ such tools to
produce a fine grid for a single simulation, as done in step-3; or to start from
a coarse grid and carry out a series of simulations on adaptively refined grids,
as is the case of step-6. Regardless of which approach is taken, the mesh
refinement requires a suitable geometrical description of the computational
domain boundary in order to place, at each refinement, the new mesh nodes onto
the boundary surface. For instance, step-5 shows how creating a circular grid
automatically attaches a circular manifold object to the computational domain,
so that the faces lying on the boundary are refined onto the circle. step-53
shows how to do this with a Manifold defined by experimentally obtained data.
But, at least as far as elementary boundary shapes are concerned, deal.II really
only provides circles, spheres, boxes and other elementary combinations. In this
tutorial, we will show how to use a set of classes developed to import arbitrary
CAD geometries, assign them to the desired boundary of the computational domain,
and refine a computational grid on such complex shapes.


<h3> CAD surfaces </h3>

In the most common industrial practice, the geometrical models of arbitrarily
shaped objects are realized by means of Computer Aided Design (CAD) tools. The
use of CAD modelers has spread in the last decades, as they allow for the
generation of a full virtual model of each designed object, which through a
computer can be visualized, inspected, and analyzed in its finest details well
before it is physically crafted.  From a mathematical perspective, the engine
lying under the hood of CAD modelers is represented by analytical geometry,
and in particular by parametric curves and surfaces such as B-splines and
NURBS that are rich enough that they can represent most surfaces of practical
interest.  Once a virtual model is ready, all the geometrical features of the
desired object are stored in files which materially contain the coefficients
of the parametric surfaces and curves composing the object. Depending on the
specific CAD tool used to define the geometrical model, there are of course
several different file formats in which the information of a CAD model can be
organized. To provide a common ground to exchange data across CAD tools, the
U.S. National Bureau of Standards published in 1980 the Initial Graphics
Exchange Representation (IGES) neutral file format, which is used in this
example.

<h3> The CAD boundary projector classes </h3>

To import and interrogate CAD models, the deal.II library implements a series of
wrapper functions for the OpenCASCADE open source library for CAD modeling.
These functions allow to import IGES files into OpenCASCADE native objects, and
wrap them inside a series of Manifold classes.

Once imported from an IGES file, the model is stored in a
<code>TopoDS_Shape</code>, which is the generic topological entity defined in
the OpenCASCADE framework. From a <code>TopoDS_Shape</code>, it is then possible
to access all the sub-shapes (such as vertices, edges and faces) composing it,
along with their geometrical description. In the deal.II framework, the
topological entities composing a shape are used to create a corresponding
Manifold representation. In step-6 we saw how to use GridGenerator::hyper_sphere()
to create a hyper sphere, which automatically attaches a SphericalManifold
to all boundary faces. This guarantees that boundary faces stay on a
sphere or circle during mesh refinement. The functions of the CAD modeling interface
have been designed to retain the same structure, allowing the user to build a
projector object using the imported CAD shapes, maintaining the same procedure
we used in other tutorial programs, i.e., assigning such projector object to
cells, faces or edges of a coarse mesh. At each refinement cycle, the new mesh
nodes will be then automatically generated by projecting a midpoint of an
existing object onto the specified geometry.

Differently from a spherical or circular boundary, a boundary with a complex
geometry poses problems as to where it is best to place the new nodes created
upon refinement on the prescribed shape. PolarManifold, for example, transforms
the surrounding points to polar coordinates, calculates the average in that
coordinate system (for each coordinate individually) and finally transforms
the point back to Cartesian coordinates.

In the case of an arbitrary and complex shape though, an appropriate choice for
the placement of a new node cannot be identified that easily. The OpenCASCADE
wrappers in deal.II provide several projector classes that employ different
projection strategies. A first projector, implemented in the
OpenCASCADE::ArclengthProjectionLineManifold class, is to be used only for
edge refinement. It is built assigning it a topological shape of dimension
one, either a <code>TopoDS_Edge</code> or a <code>TopoDS_Wire</code> (which is
a compound shape, made of several connected <code>TopoDS_Edge</code>s) and
refines a mesh edge finding the new vertex as the point splitting in two even
parts the curvilinear length of the CAD curve portion that lies between the
vertices of the original edge.

<img src="https://www.dealii.org/images/steps/developer/step-54.CurveSplit.png" alt="" width="500">


A different projection strategy has been implemented in the
OpenCASCADE::NormalProjectionBoundary class. The <code>TopoDS_Shape</code>
assigned at construction time can be arbitrary (a collection of shapes, faces,
edges or a single face or edge will all work). The new cell nodes are first
computed by averaging the surrounding points in the same way as FlatManifold
does. In a second step, all the new nodes will be projected onto the
<code>TopoDS_Shape</code> along the direction normal to the shape. If no
normal projection is available, the point which is closest to the
shape---typically lying on the shape boundary---is selected.  If the shape is
composed of several sub-shapes, the projection is carried out onto every
single sub-shape, and the closest projection point is selected.

<img src="https://www.dealii.org/images/steps/developer/step-54.NormalProjectionEdge.png" alt="" width="500">
<img src="https://www.dealii.org/images/steps/developer/step-54.NormalProjection.png" alt="" width="500">

As we are about to experience, for some shapes, setting the projection
direction as that normal to the CAD surface will not lead to surface mesh
elements of suitable quality. This is because the direction normal to the CAD
surface has in principle nothing to do with the direction along which the mesh
needs the new nodes to be located. The
OpenCASCADE::DirectionalProjectionBoundary class, in this case, can help. This
class is constructed assigning a <code>TopoDS_Shape</code> (containing at
least a face) and a direction along which all the projections will be carried
out. New points will be computed by first averaging the surrounding points (as
in the FlatManifold case), and then taking the closest intersection between
the topological shape and the line passing through the resulting point, along
the direction used at construction time.  In this way, the user will have a
higher control on the projection direction to be enforced to ensure good mesh
quality.

<img src="https://www.dealii.org/images/steps/developer/step-54.DirectionalProjection.png" alt="" width="500">


Of course the latter approach is effective only when the orientation of the
surface is rather uniform, so that a single projection direction can be
identified. In cases in which the surface direction is approaching the
projection direction, it is even possible that the directional projection is
not found. To overcome these problems, the
OpenCASCADE::NormalToMeshProjectionBoundary class implements a third
projection algorithm. The OpenCASCADE::NormalToMeshProjectionBoundary class is
built assigning a <code>TopoDS_Shape</code> (containing at least one face) to
the constructor, and works exactly like a
OpenCASCADE::DirectionalProjection. But, as the name of the class suggests,
OpenCASCADE::NormalToMeshProjectionBoundary tries to come up with a suitable
estimate of the direction normal to the mesh elements to be refined, and uses
it for the projection of the new nodes onto the CAD surface. If we consider a
mesh edge in a 2D space, the direction of its axis is a direction along which
to split it in order to give rise to two new cells of the same length. We here
extended this concept in 3D, and project all new nodes in a direction that
approximates the cell normal.

In the next figure, which is inspired by the geometry considered in this
tutorial, we make an attempt to compare the behavior of the three projectors
considered. As can be seen on the left, given the original cell (in blue), the
new point found with the normal projection is in a position which does not
allow for the generation of evenly spaced new elements (in red). The situation
will get worse in further refinement steps.  Since the geometry we considered
is somehow perpendicular to the horizontal direction, the directional
projection (central image) defined with horizontal direction as the projection
direction, does a rather good job in getting the new mesh point. Yet, since
the surface is almost horizontal at the bottom of the picture, we can expect
problems in those regions when further refinement steps are carried
out. Finally, the picture on the right shows that a node located on the cell
axis will result in two new cells having the same length. Of course the
situation in 3D gets a little more complicated than that described in this
simple 2D case. Nevertheless, the results of this test confirm that the normal
to the mesh direction is the best approach among the three tested, when
arbitrarily shaped surfaces are considered, and unless you have a geometry for
which a more specific approach is known to be appropriate.


<img src="https://www.dealii.org/images/steps/developer/step-54.ProjectionComparisons.png" alt="" width="700">


<h3> The testcase </h3>

In this program, we will consider creating a surface mesh for a real geometry
describing the bow of a ship (this geometry is frequently used in CAD and mesh
generation comparisons and is freely available). The surface mesh we get from
this could then be used to solve a boundary element equation to simulate the
flow of water around the ship (in a way similar to step-34) but we will not
try to do this here. To already give you an idea of the geometry we consider,
here is a picture:

<img src="https://www.dealii.org/images/steps/developer/step-54.bare.png" alt="" width="500">

In the program, we read both the geometry and a coarse mesh from files, and
then employ several of the options discussed above to place new vertices for a
sequence of mesh refinement steps.


examples/step-54/doc/results.dox
<h1>Results</h1>

The program execution produces a series of mesh files <code>3d_mesh_*.vtk</code>
that we can visualize with any of the usual visualization programs that can read the VTK
file format.

The following table illustrates the results obtained employing the normal projection strategy. The first two
rows of the table show side views of the grids obtained for progressive levels
of refinement, overlain on a very fine rendering of the exact geometry. The
dark and light red areas simply indicate whether the current mesh or the fine
geometry is closer to the observer; the distinction does not carry any
particularly deep meaning. The last row
of pictures depict front views (mirrored to both sides of the geometry) of the
same grids shown in the second row.


<table style="width:90%">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.common_0.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_1.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_2.png" alt="" width="400"></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_3.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_4.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_5.png" alt="" width="400"></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_front_3.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_front_4.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_front_5.png" alt="" width="400"></td>
  </tr>
</table>

As can be seen in the pictures---and as we anticipated---the normal refinement strategy is unable to produce nicely shaped elements
when applied to surfaces with significant curvature changes. This is
particularly apparent at the bulb of the hull where all new points have been
placed in the upper part of the bulb and the lower part remains completely
unresolved.

The following table, which is arranged as the previous one, illustrates
the results obtained adopting the directional projection approach, in which the projection direction selected was the y-axis (which
is indicated with a small yellow arrow at the bottom left of each image).


<table style="width:90%">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.common_0.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.directional_1.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.directional_2.png" alt="" width="400"></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.directional_3.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.directional_4.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.directional_5.png" alt="" width="400"></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.directional_front_3.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.directional_front_4.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.directional_front_5.png" alt="" width="400"></td>
  </tr>
</table>

The images confirm that the quality of the mesh obtained with a directional projection is sensibly higher than that obtained projecting along the
surface normal. Yet, a number of elements elongated in the y-direction are observed around the bottom of the bulb, where the surface is almost parallel to the
direction chosen for the projection.

The final test shows results using instead the projection normal to the faces:

<table style="width:90%">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.common_0.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_to_mesh_1.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_to_mesh_2.png" alt="" width="400"></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_to_mesh_3.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_to_mesh_4.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_to_mesh_5.png" alt="" width="400"></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_to_mesh_front_3.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_to_mesh_front_4.png" alt="" width="400"></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-54.normal_to_mesh_front_5.png" alt="" width="400"></td>
  </tr>
</table>

The pictures confirm that the normal to mesh projection approach leads to grids that remain evenly spaced
throughtout the refinement steps. At the same time, these meshes represent rather well the original geometry even in the bottom region
of the bulb, which is not well recovered employing the directional projector or the normal projector.


examples/step-55/doc/intro.dox
<br>

<i>This program was contributed by Timo Heister. Special thanks to Sander
Rhebergen for the inspiration to finally write this tutorial.

This material is based upon work partially supported by National Science
Foundation grant DMS1522191 and the Computational Infrastructure in
Geodynamics initiative (CIG), through the National Science Foundation under
Award No. EAR-0949446 and The University of California-Davis.

The authors would like to thank the Isaac Newton Institute for
Mathematical Sciences, Cambridge, for support and hospitality during
the programme Melt in the Mantle where work on this tutorial was
undertaken. This work was supported by EPSRC grant no EP/K032208/1.
</i>


@note As a prerequisite of this program, you need to have PETSc or Trilinos
and the p4est library installed. The installation of deal.II together with
these additional libraries is described in the <a href="../../readme.html"
target="body">README</a> file.

<a name="Intro"></a>
<h1>Introduction</h1>

Building on step-40, this tutorial shows how to solve linear PDEs with several
components in parallel using MPI with PETSc or Trilinos for the linear
algebra. For this, we return to the Stokes equations as discussed in
step-22. The motivation for writing this tutorial is to provide an
intermediate step (pun intended) between step-40 (parallel Laplace) and
step-32 (parallel coupled Stokes with Boussinesq for a time dependent
problem).

The learning outcomes for this tutorial are:

- You are able to solve PDEs with several variables in parallel and can
  apply this to different problems.

- You understand the concept of optimal preconditioners and are able to check
  this for a particular problem.

- You are able to construct manufactured solutions using the free computer
  algreba system SymPy (https://sympy.org).

- You can implement various other tasks for parallel programs: error
  computation, writing graphical output, etc.

- You can visualize vector fields, stream lines, and contours of vector
  quantities.

We are solving for a velocity $\textbf{u}$ and pressure $p$ that satisfy the
Stokes equation, which reads
@f{eqnarray*}
  - \triangle \textbf{u} + \nabla p &=& \textbf{f}, \\
  -\textrm{div}\; \textbf{u} &=& 0.
@f}


<h3>Optimal preconditioners</h3>

Make sure that you read (even better: try) what is described in "Block Schur
complement preconditioner" in the "Possible Extensions" section in step-22.
Like described there, we are going to solve the block system using a Krylov
method and a block preconditioner.

Our goal here is to construct a very simple (maybe the simplest?) optimal
preconditioner for the linear system. A preconditioner is called "optimal" or
"of optimal complexity", if the number of iterations of the preconditioned
system is independent of the mesh size $h$. You can extend that definition to
also require indepence of the number of processors used (we will discuss that
in the results section), the computational domain and the mesh quality, the
test case itself, the polynomial degree of the finite element space, and more.

Why is a constant number of iterations considered to be "optimal"? Assume the
discretized PDE gives a linear system with N unknowns. Because the matrix
coming from the FEM discretization is sparse, a matrix-vector product can be
done in O(N) time. A preconditioner application can also only be O(N) at best
(for example doable with multigrid methods). If the number of iterations
required to solve the linear system is independent of $h$ (and therefore N),
the total cost of solving the system will be O(N). It is not possible to beat
this complexity, because even looking at all the entries of the right-hand
side already takes O(N) time. For more information see @cite elman2005,
Chapter 2.5 (Multigrid).

The preconditioner described here is even simpler than the one described in
step-22 and will typically require more iterations and consequently time to
solve. When considering preconditioners, optimality is not the only important
metric. But an optimal and expensive preconditioner is typically more
desirable than a cheaper, non-optimal one. This is because, eventually, as the
mesh size becomes smaller and smaller and linear problems become bigger and
bigger, the former will eventually beat the latter.

<h3>The solver and preconditioner</h3>

We precondition the linear system
@f{eqnarray*}
  \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
  \left(\begin{array}{c}
    U \\ P
  \end{array}\right)
  =
  \left(\begin{array}{c}
    F \\ 0
  \end{array}\right),
@f}

with the block diagonal preconditioner
@f{eqnarray*}
  P^{-1}
  =
  \left(\begin{array}{cc}
    A & 0 \\ 0 & S
  \end{array}\right) ^{-1},
  =
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ 0 & S^{-1}
  \end{array}\right),
@f}
where $S=-BA^{-1} B^T$ is the Schur complement.

With this choice of $P$, assuming that we handle $A^{-1}$ and $S^{-1}$ exactly
(which is an "idealized" situation), the preconditioned linear system has
three distinct eigenvalues independent of $h$ and is therefore "optimal".  See
section 6.2.1 (especially p. 292) in @cite elman2005. For comparison,
using the ideal version of the upper block-triangular preconditioner in
step-22 (also used in step-56) would have all eigenvalues be equal to one.

We will use approximations of the inverse operations in $P^{-1}$ that are
(nearly) independent of $h$. In this situation, one can again show, that the
eigenvalues are independent of $h$. For the Krylov method we choose MINRES,
which is attractive for the analysis (iteration count is proven to be
independent of $h$, see the remainder of the chapter 6.2.1 in the book
mentioned above), great from the computational standpoint (simpler and cheaper
than GMRES for example), and applicable (matrix and preconditioner are
symmetric).

For the approximations we will use a CG solve with the mass matrix in the
pressure space for approximating the action of $S^{-1}$. Note that the mass
matrix is spectrally equivalent to $S$. We can expect the number of CG
iterations to be independent of $h$, even with a simple preconditioner like
ILU.

For the approximation of the velocity block $A$ we will perform a single AMG
V-cycle. In practice this choice is not exactly independent of $h$, which can
explain the slight increase in iteration numbers. A possible explanation is
that the coarsest level will be solved exactly and the number of levels and
size of the coarsest matrix is not predictable.


<h3>The testcase</h3>

We will construct a manufactured solution based on the classical Kovasznay problem,
see @cite kovasznay1948laminar. Here
is an image of the solution colored by the x velocity including
streamlines of the velocity:

 <img src="https://www.dealii.org/images/steps/developer/step-55.solution.png" alt="">

We have to cheat here, though, because we are not solving the non-linear
Navier-Stokes equations, but the linear Stokes system without convective
term. Therefore, to recreate the exact same solution, we use the method of
manufactured solutions with the solution of the Kovasznay problem. This will
effectively move the convective term into the right-hand side $f$.

The right-hand side is computed using the script "reference.py" and we use
the exact solution for boundary conditions and error computation.


examples/step-55/doc/results.dox
<h1>Results</h1>

As expected from the discussion above, the number of iterations is independent
of the number of processors and only very slightly dependent on $h$:

<table>
<tr>
  <th colspan="2">PETSc</th>
  <th colspan="8">number of processors</th>
</tr>
<tr>
  <th>cycle</th>
  <th>dofs</th>
  <th>1</th>
  <th>2</th>
  <th>4</th>
  <th>8</th>
  <th>16</th>
  <th>32</th>
  <th>64</th>
  <th>128</th>
</tr>
<tr>
  <td>0</td>
  <td>659</td>
  <td>49</td>
  <td>49</td>
  <td>49</td>
  <td>51</td>
  <td>51</td>
  <td>51</td>
  <td>49</td>
  <td>49</td>
</tr>
<tr>
  <td>1</td>
  <td>2467</td>
  <td>52</td>
  <td>52</td>
  <td>52</td>
  <td>52</td>
  <td>52</td>
  <td>54</td>
  <td>54</td>
  <td>53</td>
</tr>
<tr>
  <td>2</td>
  <td>9539</td>
  <td>56</td>
  <td>56</td>
  <td>56</td>
  <td>54</td>
  <td>56</td>
  <td>56</td>
  <td>54</td>
  <td>56</td>
</tr>
<tr>
  <td>3</td>
  <td>37507</td>
  <td>57</td>
  <td>57</td>
  <td>57</td>
  <td>57</td>
  <td>57</td>
  <td>56</td>
  <td>57</td>
  <td>56</td>
</tr>
<tr>
  <td>4</td>
  <td>148739</td>
  <td>58</td>
  <td>59</td>
  <td>57</td>
  <td>59</td>
  <td>57</td>
  <td>57</td>
  <td>57</td>
  <td>57</td>
</tr>
<tr>
  <td>5</td>
  <td>592387</td>
  <td>60</td>
  <td>60</td>
  <td>59</td>
  <td>59</td>
  <td>59</td>
  <td>59</td>
  <td>59</td>
  <td>59</td>
</tr>
<tr>
  <td>6</td>
  <td>2364419</td>
  <td>62</td>
  <td>62</td>
  <td>61</td>
  <td>61</td>
  <td>61</td>
  <td>61</td>
  <td>61</td>
  <td>61</td>
</tr>
</table>

<table>
<tr>
  <th colspan="2">Trilinos</th>
  <th colspan="8">number of processors</th>
</tr>
<tr>
  <th>cycle</th>
  <th>dofs</th>
  <th>1</th>
  <th>2</th>
  <th>4</th>
  <th>8</th>
  <th>16</th>
  <th>32</th>
  <th>64</th>
  <th>128</th>
</tr>
<tr>
  <td>0</td>
  <td>659</td>
  <td>37</td>
  <td>37</td>
  <td>37</td>
  <td>37</td>
  <td>37</td>
  <td>37</td>
  <td>37</td>
  <td>37</td>
</tr>
<tr>
  <td>1</td>
  <td>2467</td>
  <td>92</td>
  <td>89</td>
  <td>89</td>
  <td>82</td>
  <td>86</td>
  <td>81</td>
  <td>78</td>
  <td>78</td>
</tr>
<tr>
  <td>2</td>
  <td>9539</td>
  <td>102</td>
  <td>99</td>
  <td>96</td>
  <td>95</td>
  <td>95</td>
  <td>88</td>
  <td>83</td>
  <td>95</td>
</tr>
<tr>
  <td>3</td>
  <td>37507</td>
  <td>107</td>
  <td>105</td>
  <td>104</td>
  <td>99</td>
  <td>100</td>
  <td>96</td>
  <td>96</td>
  <td>90</td>
</tr>
<tr>
  <td>4</td>
  <td>148739</td>
  <td>112</td>
  <td>112</td>
  <td>111</td>
  <td>111</td>
  <td>127</td>
  <td>126</td>
  <td>115</td>
  <td>117</td>
</tr>
<tr>
  <td>5</td>
  <td>592387</td>
  <td>116</td>
  <td>115</td>
  <td>114</td>
  <td>112</td>
  <td>118</td>
  <td>120</td>
  <td>131</td>
  <td>130</td>
</tr>
<tr>
  <td>6</td>
  <td>2364419</td>
  <td>130</td>
  <td>126</td>
  <td>120</td>
  <td>120</td>
  <td>121</td>
  <td>122</td>
  <td>121</td>
  <td>123</td>
</tr>
</table>

While the PETSc results show a constant number of iterations, the iterations
increase when using Trilinos. This is likely because of the different settings
used for the AMG preconditioner. For performance reasons we do not allow
coarsening below a couple thousand unknowns. As the coarse solver is an exact
solve (we are using LU by default), a change in number of levels will
influence the quality of a V-cycle. Therefore, a V-cycle is closer to an exact
solver for smaller problem sizes.

<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

<h4>Investigate Trilinos iterations</h4>

Play with the smoothers, smoothing steps, and other properties for the
Trilinos AMG to achieve an optimal preconditioner.

<h4>Solve the Oseen problem instead of the Stokes system</h4>

This change requires changing the outer solver to GMRES or BiCGStab, because
the system is no longer symmetric.

You can prescribe the exact flow solution as $b$ in the convective term $b
\cdot \nabla u$. This should give the same solution as the original problem,
if you set the right hand side to zero.

<h4>Adaptive refinement</h4>

So far, this tutorial program refines the mesh globally in each step.
Replacing the code in StokesProblem::refine_grid() by something like
@code
Vector<float> estimated_error_per_cell(triangulation.n_active_cells());

FEValuesExtractors::Vector velocities(0);
KellyErrorEstimator<dim>::estimate(
  dof_handler,
  QGauss<dim - 1>(fe.degree + 1),
  std::map<types::boundary_id, const Function<dim> *>(),
  locally_relevant_solution,
  estimated_error_per_cell,
  fe.component_mask(velocities));
parallel::distributed::GridRefinement::refine_and_coarsen_fixed_number(
  triangulation, estimated_error_per_cell, 0.3, 0.0);
triangulation.execute_coarsening_and_refinement();
@endcode
makes it simple to explore adaptive mesh refinement.


examples/step-56/doc/intro.dox
<i>This program was contributed by Ryan Grove and Timo Heister.

This material is based upon work partially supported by National Science
Foundation grant DMS1522191 and the Computational Infrastructure in
Geodynamics initiative (CIG), through the National Science Foundation under
Award No. EAR-0949446 and The University of California-Davis.

The authors would like to thank the Isaac Newton Institute for
Mathematical Sciences, Cambridge, for support and hospitality during
the programme Melt in the Mantle where work on this tutorial was
undertaken. This work was supported by EPSRC grant no EP/K032208/1.
</i>

@dealiiTutorialDOI{10.5281/zenodo.400995,https://zenodo.org/badge/DOI/10.5281/zenodo.400995.svg}

<a name="Intro"></a>
<h1>Introduction</h1>

<h3> Stokes Problem </h3>

The purpose of this tutorial is to create an efficient linear solver for the
Stokes equation and compare it to alternative approaches.  Here, we will use
FGMRES with geometric multigrid as a preconditioner velocity block, and we
will show in the results section that this is a fundamentally better approach
than the linear solvers used in step-22 (including the scheme described in
"Possible Extensions").  Fundamentally, this is because only with multigrid it
is possible to get $O(n)$ solve time, where $n$ is the number of unknowns of
the linear system. Using the Timer class, we collect some statistics to
compare setup times, solve times, and number of iterations. We also compute
errors to make sure that what we have implemented is correct.

Let $u \in H_0^1 = \{ u \in H^1(\Omega), u|_{\partial \Omega} = 0 \}$
and $p \in L_*^2 = \{ p \in L^2(\Omega), \int_\Omega p = 0
\}$. The Stokes equations read as follows in non-dimensionalized form:

@f{eqnarray*}
 - 2 \text{div} \frac {1}{2} \left[ (\nabla \textbf{u})
 + (\nabla \textbf{u})^T\right] + \nabla p & =& f \\
 - \nabla \cdot u &=& 0
@f}

Note that we are using the deformation tensor instead of $\Delta u$ (a
detailed description of the difference between the two can be found in
step-22, but in summary, the deformation tensor is more physical as
well as more expensive).

<h3> Linear %Solver and Preconditioning Issues </h3>

The weak form of
the discrete equations naturally leads to the following linear system
for the nodal values of the velocity and pressure fields:
@f{eqnarray*}
\left(\begin{array}{cc} A & B^T \\ B & 0
\end{array}\right) \left(\begin{array}{c} U \\ P \end{array}\right) =
\left(\begin{array}{c} F \\ 0 \end{array}\right).
@f}

Our goal is to compare several solution approaches.  While step-22
solves the linear system using a "Schur complement approach" in two
separate steps, we instead attack the
block system at once using FMGRES with an efficient
preconditioner, in the spirit of the approach outlined in the "Results"
section of step-22. The idea is as follows: if we find a block
preconditioner $P$ such that the matrix

@f{eqnarray*}
\left(\begin{array}{cc} A & B^T \\ B & 0 \end{array}\right) P^{-1}
@f}

is simple, then an iterative solver with that preconditioner will
converge in a few iterations. Notice that we are doing right
preconditioning here.  Using the Schur complement $S=BA^{-1}B^T$,
we find that

@f{eqnarray*}
P^{-1} = \left(\begin{array}{cc} A & B^T \\ 0 &
 S \end{array}\right)^{-1}
@f}

is a good choice. Let $\widetilde{A^{-1}}$ be an approximation of $A^{-1}$
and $\widetilde{S^{-1}}$ of $S^{-1}$, we see
@f{eqnarray*}
P^{-1} =
\left(\begin{array}{cc} A^{-1} & 0 \\ 0 & I \end{array}\right)
\left(\begin{array}{cc} I & B^T \\ 0 & -I \end{array}\right)
\left(\begin{array}{cc} I & 0 \\ 0 & S^{-1} \end{array}\right)
\approx
\left(\begin{array}{cc} \widetilde{A^{-1}} & 0 \\ 0 & I \end{array}\right)
\left(\begin{array}{cc} I & B^T \\ 0 & -I \end{array}\right)
\left(\begin{array}{cc} I & 0 \\ 0 & \widetilde{S^{-1}} \end{array}\right).
  @f}

Since $P$ is aimed to be a preconditioner only, we shall use
the approximations on the right in the equation above.

As discussed in step-22, $-M_p^{-1}=:\widetilde{S^{-1}} \approx
S^{-1}$, where $M_p$ is the pressure mass matrix and is solved approximately by using CG
with ILU as a preconditioner, and $\widetilde{A^{-1}}$ is obtained by one of
multiple methods: solving a linear system with CG and ILU as
preconditioner, just using one application of an ILU, solving a linear
system with CG and GMG (Geometric
Multigrid as described in step-16) as a preconditioner, or just performing a single V-cycle
of GMG.

As a comparison, instead of FGMRES, we also use the direct solver
UMFPACK on the whole system to compare our results with.  If you want to use
a direct solver (like UMFPACK), the system needs to be invertible. To avoid
the one dimensional null space given by the constant pressures, we fix the first pressure unknown
 to zero. This is not necessary for the iterative solvers.


<h3> Reference Solution </h3>

The test problem is a "Manufactured Solution" (see step-7 for
details), and we choose $u=(u_1,u_2,u_3)=(2\sin (\pi x), - \pi y \cos
(\pi x),- \pi z \cos (\pi x))$ and $p = \sin (\pi x)\cos (\pi y)\sin
(\pi z)$.
We apply Dirichlet boundary conditions for the velocity on the whole
boundary of the domain $\Omega=[0,1]\times[0,1]\times[0,1]$.
To enforce the boundary conditions we can just use our reference solution.

If you look up in the deal.II manual what is needed to create a class
derived from <code>Function@<dim@></code>, you will find that this
class has numerous @p virtual functions, including
Function::value(), Function::vector_value(), Function::value_list(),
etc., all of which can be overloaded.  Different parts of deal.II
will require different ones of these particular
functions. This can be confusing at first, but luckily the only thing
you actually have to implement is @p value().  The other virtual
functions in the Function class have default
implementations inside that will call your implementation of @p value
by default.

Notice that our reference solution fulfills $\nabla \cdot u = 0$. In
addition, the pressure is chosen to have a mean value of zero.  For
the "Method of Manufactured Solutions" of step-7, we need to find $\bf
f$ such that:

@f{align*}
{\bf f} =   - 2 \text{div} \frac {1}{2} \left[ (\nabla \textbf{u}) + (\nabla \textbf{u})^T\right] + \nabla p.
@f}

Using the reference solution above, we obtain:

@f{eqnarray*}
{\bf f} &=& (2 \pi^2 \sin (\pi x),- \pi^3 y \cos(\pi
x),- \pi^3 z \cos(\pi x))\\ & & + (\pi \cos(\pi x) \cos(\pi y)
\sin(\pi z) ,- \pi \sin(\pi y) \sin(\pi x) \sin(\pi z), \pi \cos(\pi
z) \sin(\pi x) \cos(\pi y)) @f}

<h3> Computing Errors </h3>

Because we do not enforce the mean
pressure to be zero for our numerical solution in the linear system,
we need to post process the solution after solving. To do this we use
the VectorTools::compute_mean_value() function to compute the mean value
of the pressure to subtract it from the pressure.


<h3> DoF Handlers </h3>

The way we implement geometric multigrid here only executes it on the
velocity variables (i.e., the $A$ matrix described above) but not the
pressure. One could implement this in different ways, including one in
which one considers all coarse grid operations as acting on $2\times
2$ block systems where we only consider the top left
block. Alternatively, we can implement things by really only
considering a linear system on the velocity part of the overall finite
element discretization. The latter is the way we want to use here.

To implement this, one would need to be able to ask questions such as
"May I have just part of a DoFHandler?". This is not possible at the
time when this program was written, so in order to answer this request
for our needs, we simply create a separate, second DoFHandler for just the
velocities. We then build linear systems for the multigrid
preconditioner based on only this second DoFHandler, and simply
transfer the first block of (overall) vectors into corresponding
vectors for the entire second DoFHandler. To make this work, we have
to assure that the <i>order</i> in which the (velocity) degrees of freedom are
ordered in the two DoFHandler objects is the same. This is in fact the
case by first distributing degrees of freedom on both, and then using
the same sequence of DoFRenumbering operations on both.


<h3> Differences from the Step 22 tutorial </h3>

The main difference between step-56 and step-22 is that we use block
solvers instead of the Schur Complement approach used in
step-22. Details of this approach can be found under the "Block Schur
complement preconditioner" subsection of the "Possible Extensions"
section of step-22. For the preconditioner of the velocity block, we
borrow a class from <a href="https://aspect.geodynamics.org">ASPECT</a>
called @p BlockSchurPreconditioner that has the option to solve for
the inverse of $A$ or just apply one preconditioner sweep for it
instead, which provides us with an expensive and cheap approach,
respectively.


examples/step-56/doc/results.dox
<h1>Results</h1>

<h3> Errors </h3>

We first run the code and confirm that the finite element solution converges
with the correct rates as predicted by the error analysis of mixed finite
element problems. Given sufficiently smooth exact solutions $u$ and $p$,
the errors of the Taylor-Hood element $Q_k \times Q_{k-1}$ should be

@f[
\| u -u_h \|_0 + h ( \| u- u_h\|_1 + \|p - p_h \|_0)
\leq C h^{k+1} ( \|u \|_{k+1} + \| p \|_k )
@f]

see for example Ern/Guermond "Theory and Practice of Finite Elements", Section
4.2.5 p195. This is indeed what we observe, using the $Q_2 \times Q_1$
element as an example (this is what is done in the code, but is easily
changed in <code>main()</code>):

<table align="center" class="doxtable">
  <tr>
    <th>&nbsp;</th>
    <th>L2 Velocity</th>
    <th>Reduction</th>
    <th>L2 Pressure</th>
    <th>Reduction</th>
    <th>H1 Velocity</th>
    <th>Reduction</th>
  </tr>
  <tr>
    <td>3D, 3 global refinements</td>
    <td>0.000670888</td>
    <td align="center">-</td>
    <td>0.0036533</td>
    <td align="center">-</td>
    <td>0.0414704</td>
    <td align="center">-</td>
  </tr>
  <tr>
    <td>3D, 4 global refinements</td>
    <td>8.38E-005</td>
    <td>8.0</td>
    <td>0.00088494</td>
    <td>4.1</td>
    <td>0.0103781</td>
    <td>4.0</td>
  </tr>
  <tr>
    <td>3D, 5 global refinements</td>
    <td>1.05E-005</td>
    <td>8.0</td>
    <td>0.000220253</td>
    <td>4.0</td>
    <td>0.00259519</td>
    <td>4.0</td>
</th>
  </tr>
</table>

<h3> Timing Results </h3>

Let us compare the direct solver approach using UMFPACK to the two
methods in which we choose $\widetilde {A^{-1}}=A^{-1}$ and
$\widetilde{S^{-1}}=S^{-1}$ by solving linear systems with $A,S$ using
CG. The preconditioner for CG is then either ILU or GMG.
The following table summarizes solver iterations, timings, and virtual
memory (VM) peak usage:

<table align="center" class="doxtable">
<tr>
  <th></th>
  <th colspan="3">General</th>
  <th colspan="6">GMG</th>
  <th colspan="6">ILU</th>
  <th colspan="3">UMFPACK</th>
</tr>
<tr>
  <th></th>
  <th></th>
  <th colspan="2">Timings</th>
  <th colspan="2">Timings</th>
  <th colspan="3">Iterations</th>
  <th></th>
  <th colspan="2">Timings</th>
  <th colspan="3">Iterations</th>
  <th></th>
  <th colspan="2">Timings</th>
  <th></th>
</tr>
<tr>
  <th>Cycle</th>
  <th>DoFs</th>
  <th>Setup</th>
  <th>Assembly</th>
  <th>Setup</th>
  <th>Solve</th>
  <th>Outer</th>
  <th>Inner (A)</th>
  <th>Inner (S)</th>
  <th>VM Peak</th>
  <th>Setup</th>
  <th>Solve</th>
  <th>Outer</th>
  <th>Inner (A)</th>
  <th>Inner (S)</th>
  <th>VM Peak</th>
  <th>Setup</th>
  <th>Solve</th>
  <th>VM Peak</th>
</tr>
<tr>
  <td>0</td>
  <td>15468</td>
  <td>0.1s</td>
  <td>0.3s</td>
  <td>0.3s</td>
  <td>1.3s</td>
  <td>21</td>
  <td>67</td>
  <td>22</td>
  <td>4805</td>
  <td>0.3s</td>
  <td>0.6s</td>
  <td>21</td>
  <td>180</td>
  <td>22</td>
  <td>4783</td>
  <td>2.65s</td>
  <td>2.8s</td>
  <td>5054</td>
</tr>
<tr>
  <td>1</td>
  <td>112724</td>
  <td>1.0s</td>
  <td>2.4s</td>
  <td>2.6s</td>
  <td>14s</td>
  <td>21</td>
  <td>67</td>
  <td>22</td>
  <td>5441</td>
  <td>2.8s</td>
  <td>15.8s</td>
  <td>21</td>
  <td>320</td>
  <td>22</td>
  <td>5125</td>
  <td>236s</td>
  <td>237s</td>
  <td>11288</td>
</tr>
<tr>
  <td>2</td>
  <td>859812</td>
  <td>9.0s</td>
  <td>20s</td>
  <td>20s</td>
  <td>101s</td>
  <td>20</td>
  <td>65</td>
  <td>21</td>
  <td>10641</td>
  <td>27s</td>
  <td>268s</td>
  <td>21</td>
  <td>592</td>
  <td>22</td>
  <td>8307</td>
  <td>-</td>
  <td>-</td>
  <td>-</td>
</tr>
</table>

As can be seen from the table:

1. UMFPACK uses large amounts of memory, especially in 3d. Also, UMFPACK
timings do not scale favorably with problem size.

2. Because we are using inner solvers for $A$ and $S$, ILU and GMG require the
same number of outer iterations.

3. The number of (inner) iterations for $A$ increases for ILU with refinement, leading
to worse than linear scaling in solve time. In contrast, the number of inner
iterations for $A$ stays constant with GMG leading to nearly perfect scaling in
solve time.

4. GMG needs slightly more memory than ILU to store the level and interface
matrices.

<h3> Possibilities for extensions </h3>

<h4> Check higher order discretizations </h4>

Experiment with higher order stable FE pairs and check that you observe the
correct convergence rates.

<h4> Compare with cheap preconditioner </h4>

The introduction also outlined another option to precondition the
overall system, namely one in which we do not choose $\widetilde
{A^{-1}}=A^{-1}$ as in the table above, but in which
$\widetilde{A^{-1}}$ is only a single preconditioner application with
GMG or ILU, respectively.

This is in fact implemented in the code: Currently, the boolean
<code>use_expensive</code> in <code>solve()</code> is set to @p true. The
option mentioned above is obtained by setting it to @p false.

What you will find is that the number of FGMRES iterations stays
constant under refinement if you use GMG this way. This means that the
Multigrid is optimal and independent of $h$.


examples/step-57/doc/intro.dox
<br>

<i>This program was contributed by Liang Zhao and Timo Heister.

This material is based upon work partially supported by National Science
Foundation grant DMS1522191 and the Computational Infrastructure in
Geodynamics initiative (CIG), through the National Science Foundation under
Award No. EAR-0949446 and The University of California-Davis.
</i>

@dealiiTutorialDOI{10.5281/zenodo.484156,https://zenodo.org/badge/DOI/10.5281/zenodo.484156.svg}

<a name="Intro"></a>
<h1>Introduction</h1>

<h3> Navier Stokes Equations </h3>

In this tutorial we show how to solve the incompressible Navier
Stokes equations (NSE) with Newton's method. The flow we consider here
is assumed to be steady. In a domain $\Omega \subset
\mathbb{R}^{d}$, $d=2,3$, with a piecewise smooth boundary
$\partial \Omega$, and a given force field $\textbf{f}$, we seek
a velocity field $\textbf{u}$ and a pressure field $\textbf{p}$
satisfying
@f{eqnarray*}
- \nu \Delta\textbf{u} + (\textbf{u} \cdot \nabla)\textbf{u} + \nabla p &=& \textbf{f}\\
- \nabla \cdot \textbf{u} &=& 0.
@f}

Unlike the Stokes equations as discussed in step-22, the NSE are a
nonlinear system of equations because of the convective term $(\textbf{u} \cdot
\nabla)\textbf{u}$. The first step of computing a numerical solution
is to linearize the system and this will be done using Newton's method. A
time-dependent problem is discussed in step-35, where the system is linearized
using the solution from the last time step and no nonlinear
solve is necessary.

<h3> Linearization of Navier-Stokes Equations </h3>

We define a nonlinear function whose root is a solution to the NSE by
@f{eqnarray*}
F(\mathbf{u}, p) =
  \begin{pmatrix}
    - \nu \Delta\mathbf{u} + (\mathbf{u} \cdot \nabla)\mathbf{u} + \nabla p - \mathbf{f} \\
    - \nabla \cdot \mathbf{u}
  \end{pmatrix}.
@f}

Assuming the initial guess is good enough to
guarantee the convergence of Newton's iteration and denoting
$\textbf{x} = (\textbf{u}, p)$, Newton's iteration on a vector function
can be defined as
@f{eqnarray*}
  \textbf{x}^{k+1} = \textbf{x}^{k} - (\nabla F(\textbf{x}^{k}))^{-1} F(\textbf{x}^{k}),
@f}

where $\textbf{x}^{k+1}$ is the approximate solution in step $k+1$,
$\textbf{x}^{k}$ represents the solution from the previous step, and $\nabla
F(\textbf{x}^{k})$ is the Jacobian matrix evaluated at
$\textbf{x}^{k}$.
A similar iteration can be found in step-15.

The Newton iteration formula implies the new
solution is obtained by adding an update term to the old solution. Instead
of evaluating the Jacobian matrix and taking its inverse, we consider
the update term as a whole, that is
@f{eqnarray*}
  \delta \textbf{x}^{k} = - (\nabla F(\textbf{x}^{k}))^{-1} F(\textbf{x}^{k}),
@f}

where $\textbf{x}^{k+1}=\textbf{x}^{k}+\delta \textbf{x}^{k}$.

We can find the update term by solving the system
@f{eqnarray*}
  \nabla F(\textbf{x}^{k}) \delta \textbf{x}^{k} = -F(\textbf{x}^{k}).
@f}

Here, the left of the previous equation represents the
directional gradient of $F(\textbf{x})$ along $\delta
\textbf{x}^{k}$ at $\textbf{x}^{k}$. By definition, the directional gradient is given by
@f{eqnarray*}
  & &\nabla F(\mathbf{u}^{k}, p^{k}) (\delta \mathbf{u}^{k}, \delta p^{k}) \\
  \\
  &=& \lim_{\epsilon \to 0} \frac{1}{\epsilon}
      \left(
        F(\mathbf{u}^{k} + \epsilon \delta \mathbf{u}^{k},
          p^{k} + \epsilon \nabla \delta p^{k})
      - F(\mathbf{u}^{k}, p^{k})
      \right)\\
  \\
  &=& \lim_{\epsilon \to 0} \frac{1}{\epsilon}
      \begin{pmatrix}
        - \epsilon \nu \Delta \delta \mathbf{u}^{k}
        + \epsilon \mathbf{u}^{k} \cdot \nabla \delta \mathbf{u}^{k}
        + \epsilon \delta \mathbf{u}^{k} \cdot \nabla \mathbf{u}^{k}
        + \epsilon^{2} \delta \mathbf{u}^{k} \cdot \nabla \delta \mathbf{u}^{k}
        + \epsilon \nabla \delta p^{k}\\
        - \epsilon \nabla \cdot \delta \mathbf{u}^{k}
      \end{pmatrix} \\
  \\
  &=& \begin{pmatrix}
        - \nu \Delta \delta \mathbf{u}^{k}
        + \mathbf{u}^{k} \cdot \nabla \delta \mathbf{u}^{k}
        + \delta \mathbf{u}^{k} \cdot \nabla \mathbf{u}^{k}
        + \nabla \delta p^{k}\\
        - \nabla \cdot \delta \mathbf{u}^{k}
      \end{pmatrix}.
@f}

Therefore, we arrive at the linearized system:
@f{eqnarray*}
   -\nu \Delta \delta \mathbf{u}^{k}
  + \mathbf{u}^{k} \cdot \nabla \delta \mathbf{u}^{k}
  + \delta \mathbf{u}^{k} \cdot \nabla \mathbf{u}^{k}
  + \nabla \delta p^{k}
  = -F(\mathbf{x}^k), \\
   -\nabla \cdot\delta \mathbf{u}^{k}
  = \nabla \cdot \mathbf{u}^{k},
@f}

where $\textbf{u}^k$ and $p^k$ are the solutions from the
previous iteration. Additionally, the
right hand side of the second equation is not zero since the discrete
solution is not exactly divergence free (divergence free for the continuous
solution). The right hand side here acts as a correction which leads the
discrete solution of the velocity to be divergence free along Newton's
iteration. In this linear system, the only unknowns are the
update terms $\delta \textbf{u}^{k}$ and $\delta p^{k}$, and we can use a
similar strategy to the one used in step-22 (and derive the weak form in the
same way).

Now, Newton's iteration can be used to solve for the update terms:

<ol>
  <li>Initialization: Initial guess $u_0$ and $p_0$, tolerance $\tau$;</li>
  <li>Linear solve to compute update term $\delta\textbf{u}^{k}$ and
      $\delta p^k$;</li>
  <li>Update the approximation:
      $\textbf{u}^{k+1} = \textbf{u}^{k} + \delta\textbf{u}^{k}$ and
      $p^{k+1} = p^{k} + \delta p^{k}$;</li>
  <li>Check residual norm: $E^{k+1} = \|F(\mathbf{u}^{k+1}, p^{k+1})\|$:
      <ul>
        <li>If $E^{k+1} \leq \tau$, STOP.</li>
        <li>If $E^{k+1} > \tau$, back to step 2.</li>
      </ul></li>
</ol>

<h3> Finding an Initial Guess </h3>

The initial guess needs to be close enough to the solution for Newton's method
to converge; hence, finding a good starting value is crucial to the nonlinear
solver.

When the viscosity $\nu$ is large, a good initial guess can be obtained
by solving the Stokes equation with viscosity $\nu$. While problem dependent,
this works for $\nu \geq 1/400$ for the test problem considered here.

However, the convective term $(\mathbf{u}\cdot\nabla)\mathbf{u}$ will be
dominant if the viscosity is small, like $1/7500$ in test case 2.  In this
situation, we use a continuation method to set up a series of auxiliary NSEs with
viscosity approaching the one in the target NSE. Correspondingly, we create a
sequence $\{\nu_{i}\}$ with $\nu_{n}= \nu$, and accept that the solutions to
two NSE with viscosity $\nu_{i}$ and $\nu_{i+1}$ are close if $|\nu_{i} -
\nu_{i+1}|$ is small.  Then we use the solution to the NSE with viscosity
$\nu_{i}$ as the initial guess of the NSE with $\nu_{i+1}$. This can be thought of
as a staircase from the Stokes equations to the NSE we want to solve.

That is, we first solve a Stokes problem
@f{eqnarray*}
  -\nu_{1} \Delta \textbf{u} + \nabla p &=& \textbf{f}\\
  -\nabla \cdot \textbf{u} &=& 0
@f}

to get the initial guess for
@f{eqnarray*}
  -\nu_{1} \Delta \textbf{u} + (\textbf{u} \cdot \nabla)\textbf{u} + \nabla p &=& \textbf{f},\\
  -\nabla \cdot \textbf{u} &=& 0,
@f}

which also acts as the initial guess of the continuation method.
Here $\nu_{1}$ is relatively large so that the solution to the Stokes problem with viscosity $\nu_{1}$
can be used as an initial guess for the NSE in Newton's iteration.

Then the solution to
@f{eqnarray*}
  -\nu_{i} \Delta \textbf{u} + (\textbf{u} \cdot \nabla)\textbf{u} + \nabla p &=& \textbf{f},\\
  -\nabla \cdot \textbf{u} &=& 0.
@f}

acts as the initial guess for
@f{eqnarray*}
  -\nu_{i+1} \Delta \textbf{u} + (\textbf{u} \cdot \nabla)\textbf{u} + \nabla p &=& \textbf{f},\\
  -\nabla \cdot \textbf{u} &=& 0.
@f}

This process is repeated with a sequence of viscosities $\{\nu_i\}$ that is
determined experimentally so that the final solution can used as a starting
guess for the Newton iteration.

<h3>The %Solver and Preconditioner </h3>

At each step of Newton's iteration, the problem results in solving a
saddle point systems of the form
@f{eqnarray*}
    \begin{pmatrix}
      A & B^{T} \\
      B & 0
    \end{pmatrix}
    \begin{pmatrix}
      U \\
      P
    \end{pmatrix}
    =
    \begin{pmatrix}
      F \\
      0
    \end{pmatrix}.
@f}

This system matrix has the same block structure as the one in step-22. However,
the matrix $A$ at the top left corner is not symmetric because of the nonlinear term.
Instead of solving the above system, we can solve the equivalent system
@f{eqnarray*}
    \begin{pmatrix}
      A + \gamma B^TW^{-1}B & B^{T} \\
      B & 0
    \end{pmatrix}
    \begin{pmatrix}
      U \\
      P
    \end{pmatrix}
    =
    \begin{pmatrix}
      F \\
      0
    \end{pmatrix}
@f}

with a parameter $\gamma$ and an invertible matrix $W$. Here
$\gamma B^TW^{-1}B$ is the Augmented Lagrangian term; see [1] for details.

Denoting the system matrix of the new system by $G$ and the right-hand
side by $b$, we solve it iteratively with right preconditioning
$P^{-1}$ as $GP^{-1}y = b$, where
@f{eqnarray*}
P^{-1} =
  \begin{pmatrix}
    \tilde{A} & B^T \\
    0         & \tilde{S}
  \end{pmatrix}^{-1}
@f}

with $\tilde{A} = A + \gamma B^TW^{-1}B$ and $\tilde{S}$ is the
corresponding Schur complement $\tilde{S} = B^T \tilde{A}^{-1} B$. We
let $W = M_p$ where $M_p$ is the pressure mass matrix, then
$\tilde{S}^{-1}$ can be approximated by
@f{eqnarray*}
\tilde{S}^{-1} \approx -(\nu+\gamma)M_p^{-1}.
@f}

See [1] for details.

We decompose $P^{-1}$ as
@f{eqnarray*}
P^{-1} =
  \begin{pmatrix}
    \tilde{A}^{-1} & 0 \\
    0              & I
  \end{pmatrix}
  \begin{pmatrix}
    I & -B^T \\
    0 & I
  \end{pmatrix}
  \begin{pmatrix}
    I & 0 \\
    0 & \tilde{S}^{-1}
  \end{pmatrix}.
@f}

Here two inexact solvers will be needed for $\tilde{A}^{-1}$ and
$\tilde{S}^{-1}$, respectively (see [1]). Since the pressure mass
matrix is symmetric and positive definite,
CG with ILU as a preconditioner is appropriate to use for $\tilde{S}^{-1}$. For simplicity, we use
the direct solver UMFPACK for $\tilde{A}^{-1}$. The last ingredient is a sparse
matrix-vector product with $B^T$. Instead of computing the matrix product
in the augmented Lagrangian term in $\tilde{A}$, we assemble Grad-Div stabilization
$(\nabla \cdot \phi _{i}, \nabla \cdot \phi _{j}) \approx (B^T
M_p^{-1}B)_{ij}$, as explained in [2].

<h3> Test Case </h3>

We use the lid driven cavity flow as our test case; see [3] for details.
The computational domain is the unit square and the right-hand side is
$f=0$. The boundary condition is
@f{eqnarray*}
  (u(x, y), v(x,y)) &=& (1,0) \qquad\qquad \textrm{if}\ y = 1 \\
  (u(x, y), v(x,y)) &=& (0,0) \qquad\qquad \textrm{otherwise}.
@f}

When solving this problem, the error consists of the nonlinear error (from
Newton's iteration) and the discretization error (dependent on mesh size). The
nonlinear part decreases with each Newton iteration and the discretization error
reduces with mesh refinement. In this example, the solution from the coarse
mesh is transferred to successively finer meshes and used as an initial
guess. Therefore, the nonlinear error is always brought below the tolerance of
Newton's iteration and the discretization error is reduced with each mesh
refinement.

Inside the loop, we involve three solvers: one for $\tilde{A}^{-1}$,
one for $M_p^{-1}$ and one for $Gx=b$. The first two
solvers are invoked in the preconditioner and the outer solver gives us
the update term. Overall convergence is controlled by the nonlinear residual;
as Newton's method does not require an exact Jacobian, we employ FGMRES with a
relative tolerance of only 1e-4 for the outer linear solver. In fact,
we use the truncated Newton solve for this system.
As described in step-22, the inner linear solves are also not required
to be done very accurately. Here we use CG with a relative
tolerance of 1e-6 for the pressure mass matrix. As expected, we still see convergence
of the nonlinear residual down to 1e-14. Also, we use a simple line
search algorithm for globalization of the Newton method.

The cavity reference values for $\mathrm{Re}=400$ and $\mathrm{Re}=7500$ are
from [4] and [5], respectively, where $\mathrm{Re}$ is the Reynolds number and
can be located at [8]. Here the viscosity is defined by $1/\mathrm{Re}$.
Even though we can still find a solution for $\mathrm{Re}=10000$ and the
references contain results for comparison, we limit our discussion here to
$\mathrm{Re}=7500$. This is because the solution is no longer stationary
starting around $\mathrm{Re}=8000$ but instead becomes periodic, see [7] for
details.

<h3> References </h3>
<ol>

  <li>  An Augmented Lagrangian-Based Approach to the Oseen Problem, M. Benzi and M. Olshanskii, SIAM J. SCI. COMPUT. 2006
  <li>  Efficient augmented Lagrangian-type preconditioning for the Oseen problem using Grad-Div stabilization, Timo Heister and Gerd Rapin
  <li>  http://www.cfd-online.com/Wiki/Lid-driven_cavity_problem
  <li>  High-Re solution for incompressible flow using the Navier-Stokes Equations and a Multigrid Method, U. Ghia, K. N. Ghia, and C. T. Shin
  <li>  Numerical solutions of 2-D steady incompressible driven cavity flow at high Reynolds numbers, E. Erturk, T.C. Corke and C. Gokcol
  <li> Implicit Weighted ENO Schemes for the Three-Dimensional Incompressible Navier-Stokes Equations, Yang et al, 1998
  <li> The 2D lid-driven cavity problem revisited, C. Bruneau and M. Saad, 2006
  <li> https://en.wikipedia.org/wiki/Reynolds_number
</ol>


examples/step-57/doc/results.dox
<h1>Results</h1>

Now we use the method we discussed above to solve Navier Stokes equations with
viscosity $1/400$ and $1/7500$.

<h3> Test case 1: Low Reynolds Number </h3>

In the first test case the viscosity is set to be $1/400$. As we discussed in the
introduction, the initial guess is the solution to the corresponding Stokes
problem. In the following table, the residuals at each Newton's iteration on
every mesh is shown. The data in the table shows that Newton's iteration
converges quadratically.

<table align="center" class="doxtable">
<tr>
    <th>$\mathrm{Re}=400$</th>
    <th colspan="2">Mesh0</th>
    <th colspan="2">Mesh1</th>
    <th colspan="2">Mesh2</th>
    <th colspan="2">Mesh3</th>
    <th colspan="2">Mesh4</th>
</tr>
<tr>
    <th>Newton iter   </th>
    <th>Residual      </th>
    <th>FGMRES        </th>
    <th>Residual      </th>
    <th>FGMRES        </th>
    <th>Residual      </th>
    <th>FGMRES        </th>
    <th>Residual      </th>
    <th>FGMRES        </th>
    <th>Residual      </th>
    <th>FGMRES        </th>
</tr>
<tr>
  <td>1</td>
  <td>3.7112e-03</td>
  <td>5</td>
  <td>6.4189e-03</td>
  <td>3</td>
  <td>2.4338e-03</td>
  <td>3</td>
  <td>1.0570e-03</td>
  <td>3</td>
  <td>4.9499e-04</td>
  <td>3</td>
</tr>
<tr>
  <td>2</td>
  <td>7.0849e-04</td>
  <td>5</td>
  <td>9.9458e-04</td>
  <td>5</td>
  <td>1.1409e-04</td>
  <td>6</td>
  <td>1.3544e-05</td>
  <td>6</td>
  <td>1.4171e-06</td>
  <td>6</td>
</tr>
<tr>
  <td>3</td>
  <td>1.9980e-05</td>
  <td>5</td>
  <td>4.5007e-05</td>
  <td>5</td>
  <td>2.9020e-08</td>
  <td>5</td>
  <td>4.4021e-10</td>
  <td>6</td>
  <td>6.3435e-11</td>
  <td>6</td>
</tr>
<tr>
  <td>4</td>
  <td>2.3165e-09</td>
  <td>6</td>
  <td>1.6891e-07</td>
  <td>5</td>
  <td>1.2338e-14</td>
  <td>7</td>
  <td>1.8506e-14</td>
  <td>8</td>
  <td>8.8563e-15</td>
  <td>8</td>
</tr>
<tr>
  <td>5</td>
  <td>1.2585e-13</td>
  <td>7</td>
  <td>1.4520e-11</td>
  <td>6</td>
  <td>1.9044e-13</td>
  <td>8</td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
</tr>
<tr>
  <td>6</td>
  <td></td>
  <td></td>
  <td>1.3998e-15</td>
  <td>8</td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
</tr>
</table>






The following figures show the sequence of generated grids. For the case
of $\mathrm{Re}=400$, the initial guess is obtained by solving Stokes on an
$8 \times 8$ mesh, and the mesh is refined adaptively. Between meshes, the
solution from the coarse mesh is interpolated to the fine mesh to be used as an
initial guess.

<table align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-57.Re400_Mesh0.png" width="232px" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-57.Re400_Mesh1.png" width="232px" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-57.Re400_Mesh2.png" width="232px" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-57.Re400_Mesh3.png" width="232px" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-57.Re400_Mesh4.png" width="232px" alt="">
    </td>
  </tr>
</table>

This picture is the graphical streamline result of lid-driven cavity with
$\mathrm{Re}=400$.
<img src="https://www.dealii.org/images/steps/developer/step-57.Re400_Streamline.png" alt="">

Then the solution is compared with a reference solution
from [4] and the reference solution data can be found in the file "ref_2d_ghia_u.txt".

<img src="https://www.dealii.org/images/steps/developer/step-57.compare-Re400.svg" style="width:50%" alt="">

<h3> Test case 2: High Reynolds Number </h3>

Newton's iteration requires a good initial guess. However, the nonlinear term
dominates when the Reynolds number is large, so that the solution to the Stokes
equations may be far away from the exact solution. If the Stokes solution acts
as the initial guess, the convergence will be lost. The following picture
shows that the nonlinear iteration gets stuck and the residual no longer decreases
in further iterations.

<img src="https://www.dealii.org/images/steps/developer/step-57.Re7500_loss_convergence.svg" style="width:50%" alt="">

The initial guess, therefore, has to be obtained via a continuation method
which has been discussed in the introduction. Here the step size in the continuation method, that is $|\nu_{i}-\nu_{i+1}|$, is 2000 and the initial
mesh is of size $32 \times 32$. After obtaining an initial guess, the mesh is
refined as in the previous test case. The following picture shows that at each
refinement Newton's iteration has quadratic convergence. 52 steps of Newton's
iterations are executed for solving this test case.

<img src="https://www.dealii.org/images/steps/developer/step-57.Re7500_get_convergence.svg" style="width:50%" alt="">

We also show the residual from each step of Newton's iteration on every
mesh. The quadratic convergence is clearly visible in the table.

<table align="center" class="doxtable">
  <tr>
    <th>$\mathrm{Re}=7500$</th>
    <th colspan="2">Mesh0</th>
    <th colspan="2">Mesh1</th>
    <th colspan="2">Mesh2</th>
    <th colspan="2">Mesh3</th>
    <th colspan="2">Mesh4</th>
  </tr>
  <tr>
    <th>Newton iter   </th>
    <th>Residual      </th>
    <th>FGMRES        </th>
    <th>Residual      </th>
    <th>FGMRES        </th>
    <th>Residual      </th>
    <th>FGMRES        </th>
    <th>Residual      </th>
    <th>FGMRES        </th>
    <th>Residual      </th>
    <th>FGMRES        </th>
  </tr>
<tr>
  <td>1</td>
  <td>1.8922e-06</td>
  <td>6</td>
  <td>4.2506e-03</td>
  <td>3</td>
  <td>1.4299e-03</td>
  <td>3</td>
  <td>4.8793e-04</td>
  <td>2</td>
  <td>1.8998e-04</td>
  <td>2</td>
</tr>
<tr>
  <td>2</td>
  <td>3.1644e-09</td>
  <td>8</td>
  <td>1.3732e-03</td>
  <td>7</td>
  <td>4.1506e-04</td>
  <td>7</td>
  <td>9.1119e-05</td>
  <td>8</td>
  <td>1.3555e-05</td>
  <td>8</td>
</tr>
<tr>
  <td>3</td>
  <td>1.7611e-14</td>
  <td>9</td>
  <td>2.1946e-04</td>
  <td>6</td>
  <td>1.7881e-05</td>
  <td>6</td>
  <td>5.2678e-07</td>
  <td>7</td>
  <td>9.3739e-09</td>
  <td>7</td>
</tr>
<tr>
  <td>4</td>
  <td></td>
  <td></td>
  <td>8.8269e-06</td>
  <td>6</td>
  <td>6.8210e-09</td>
  <td>7</td>
  <td>2.2770e-11</td>
  <td>8</td>
  <td>1.2588e-13</td>
  <td>9</td>
</tr>
<tr>
  <td>5</td>
  <td></td>
  <td></td>
  <td>1.2974e-07</td>
  <td>7</td>
  <td>1.2515e-13</td>
  <td>9</td>
  <td>1.7801e-14</td>
  <td>1</td>
  <td></td>
  <td></td>
</tr>
<tr>
  <td>6</td>
  <td></td>
  <td></td>
  <td>4.4352e-11</td>
  <td>7</td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
</tr>
<tr>
  <td>7</td>
  <td></td>
  <td></td>
  <td>6.2863e-15</td>
  <td>9</td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
</tr>
</table>






The sequence of generated grids looks like this:
<table align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-57.Re7500_Mesh0.png" width="232px" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-57.Re7500_Mesh1.png" width="232px" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-57.Re7500_Mesh2.png" width="232px" alt="">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-57.Re7500_Mesh3.png" width="232px" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-57.Re7500_Mesh4.png" width="232px" alt="">
    </td>
  </tr>
</table>
We compare our solution with reference solution from [5].
<img src="https://www.dealii.org/images/steps/developer/step-57.compare-Re7500.svg" style="width:50%" alt="">
The following picture presents the graphical result.
<img src="https://www.dealii.org/images/steps/developer/step-57.Re7500_Streamline.png" alt="">

Furthermore, the error consists of the nonlinear error,
which decreases as we perform Newton iterations, and the discretization error,
which depends on the mesh size. That is why we have to refine the
mesh and repeat Newton's iteration on the next finer mesh. From the table above, we can
see that the residual (nonlinear error) is below 1e-12 on each mesh, but the
following picture shows us the difference between solutions on subsequently finer
meshes.

<img src="https://www.dealii.org/images/steps/developer/step-57.converge-Re7500.svg" style="width:50%" alt="">

<a name="extensions"></a>

<h3>Possibilities for extensions</h3>

<h4>Compare to other solvers</h4>

It is easy to compare the currently implemented linear solver to just using
UMFPACK for the whole linear system. You need to remove the nullspace
containing the constant pressures and it is done in step-56. More interesting
is the comparison to other state of the art preconditioners like PCD. It turns
out that the preconditioner here is very competitive, as can be seen in the
paper [2].

The following table shows the timing results between our iterative approach
(FGMRES) compared to a direct solver (UMFPACK) for the whole system
with viscosity set to 1/400. Even though we use the same direct solver for
the velocity block in the iterative solver, it is considerably faster and
consumes less memory. This will be even more pronounced in 3d.

<table align="center" class="doxtable">
<tr>
  <th>Refinement Cycle</th>
  <th>DoFs</th>
  <th>Iterative: Total/s (Setup/s)</th>
  <th>Direct: Total/s (Setup/s)</th>
</tr>
<tr>
  <td>5</td>
  <td>9539</td>
  <td>0.10 (0.06)</td>
  <td>0.13 (0.12)</td>
</tr>
<tr>
  <td>6</td>
  <td>37507</td>
  <td>0.58 (0.37)</td>
  <td>1.03 (0.97)</td>
</tr>
<tr>
  <td>7</td>
  <td>148739</td>
  <td>3.59 (2.73)</td>
  <td>7.78 (7.53)</td>
</tr>
<tr>
  <td>8</td>
  <td>592387</td>
  <td>29.17 (24.94)</td>
  <td>(>4GB RAM)</td>
</tr>
</table>


<h4>3d computations</h4>

The code is set up to also run in 3d. Of course the reference values are
different, see [6] for example. High resolution computations are not doable
with this example as is, because a direct solver for the velocity block does
not work well in 3d. Rather, a parallel solver based on algebraic or geometric
multigrid is needed. See below.

<h4>Parallelization</h4>

For larger computations, especially in 3d, it is necessary to implement MPI
parallel solvers and preconditioners. A good starting point would be step-55,
which uses algebraic multigrid for the velocity block for the Stokes
equations. Another option would be to take a look at the list of codes
in the <a href="https://www.dealii.org/code-gallery.html">deal.II code
gallery</a>, which already contains parallel Navier-Stokes solvers.


examples/step-58/doc/intro.dox
<br>

<i>This program was contributed by Wolfgang Bangerth (Colorado State
University) and Yong-Yong Cai (<a href="http://www.csrc.ac.cn/en/">Beijing
Computational Science Research Center</a>, CSRC) and is the result of the
first author's time as a visitor at CSRC.

This material is based upon work partially supported by National Science
Foundation grants OCI-1148116, OAC-1835673, DMS-1821210, and EAR-1925595;
and by the Computational Infrastructure in
Geodynamics initiative (CIG), through the National Science Foundation under
Award No. EAR-1550901 and The University of California-Davis.
</i>

<a name="Intro"></a>
<h1>Introduction</h1>

The <a
href="https://en.wikipedia.org/wiki/Nonlinear_Schr%C3%B6dinger_equation">Nonlinear
Schr&ouml;dinger Equation (NLSE)</a> for a function $\psi=\psi(\mathbf
x,t)$ and a potential $V=V(\mathbf x)$ is a model often used in
quantum mechanics and nonlinear optics. If one measures in appropriate
quantities (so that $\hbar=1$), then it reads as follows:
@f{align*}{
  - i \frac{\partial \psi}{\partial t}
  - \frac 12 \Delta \psi
  + V \psi
  + \kappa |\psi|^2 \psi
  &= 0
  \qquad\qquad
  &
  \text{in}\; \Omega\times (0,T),
  \\
  \psi(\mathbf x,0) &= \psi_0(\mathbf x)
  &
  \text{in}\; \Omega,
  \\
  \psi(\mathbf x,t) &= 0
  &
  \text{on}\; \partial\Omega\times (0,T).
@f}
If there is no potential, i.e. $V(\mathbf x)=0$, then it can be used
to describe the propagation of light in optical fibers. If $V(\mathbf
x)\neq 0$, the equation is also sometimes called the <a
href="https://en.wikipedia.org/wiki/Gross%E2%80%93Pitaevskii_equation">Gross-Pitaevskii
equation</a> and can be used to model the time dependent behavior of
<a
href="https://en.wikipedia.org/wiki/Bose%E2%80%93Einstein_condensate">Bose-Einstein
condensates</a>.

For this particular tutorial program, the physical interpretation of
the equation is not of much concern to us. Rather, we want to use it
as a model that allows us to explain two aspects:
- It is a <b>complex-valued equation</b> for $\psi \in H^1(\Omega,{\mathbb
  C})$. We have previously seen complex-valued equations in step-29,
  but there have opted to split the equations into real and imaginary
  parts and consequently ended up solving a system of two real-valued
  equations. In contrast, the goal here is to show how to solve
  problems in which we keep everything as complex numbers.
- The equation is a nice model problem to explain how <b>operator
  splitting methods</b> work. This is because it has terms with
  fundamentally different character: on the one hand, $- \frac 12
  \Delta \psi$ is a regular spatial operator in the way we have seen
  many times before; on the other hand, $\kappa |\psi(\mathbf x,t)|^2
  \psi$ has no spatial or temporal derivatives, i.e., it is a purely
  local operator. It turns out that we have efficient methods for each
  of these terms (in particular, we have analytic solutions for the
  latter), and that we may be better off treating these terms
  differently and separately. We will explain this in more detail
  below.



<h3>A note about the character of the equations</h3>

At first glance, the equations appear to be parabolic and similar to
the heat equation (see step-26) as there is only a single time
derivative and two spatial derivatives. But this is misleading.
Indeed, that this is not the correct interpretation is
more easily seen if we assume for a moment that the potential $V=0$
and $\kappa=0$. Then we have the equation
@f{align*}{
  - i \frac{\partial \psi}{\partial t}
  - \frac 12 \Delta \psi
  &= 0.
@f}
If we separate the solution into real and imaginary parts, $\psi=v+iw$,
with $v=\textrm{Re}\;\psi,\; w=\textrm{Im}\;\psi$,
then we can split the one equation into its real and imaginary parts
in the same way as we did in step-29:
@f{align*}{
  \frac{\partial w}{\partial t}
  - \frac 12 \Delta v
  &= 0,
  \\
  -\frac{\partial v}{\partial t}
  - \frac 12 \Delta w
  &= 0.
@f}
Not surprisingly, the factor $i$ in front of the time derivative
couples the real and imaginary parts of the equation. If we want to
understand this equation further, take the time derivative of one of
the equations, say
@f{align*}{
  \frac{\partial^2 w}{\partial t^2}
  - \frac 12 \Delta \frac{\partial v}{\partial t}
  &= 0,
@f}
(where we have assumed that, at least in some formal sense, we can
commute the spatial and temporal derivatives), and then insert the
other equation into it:
@f{align*}{
  \frac{\partial^2 w}{\partial t^2}
  + \frac 14 \Delta^2 w
  &= 0.
@f}
This equation is hyperbolic and similar in character to the wave
equation. (This will also be obvious if you look at the video
in the "Results" section of this program.) Furthermore, we could
have arrived at the same equation for $v$ as well.
Consequently, a better assumption for the NLSE is to think of
it as a hyperbolic, wave-propagation equation than as a diffusion
equation such as the heat equation. (You may wonder whether it is
correct that the operator $\Delta^2$ appears with a positive sign
whereas in the wave equation, $\Delta$ has a negative sign. This is
indeed correct: After multiplying by a test function and integrating
by parts, we want to come out with a positive (semi-)definite
form. So, from $-\Delta u$ we obtain $+(\nabla v,\nabla u)$. Likewise,
after integrating by parts twice, we obtain from $+\Delta^2 u$ the
form $+(\Delta v,\Delta u)$. In both cases do we get the desired positive
sign.)

The real NLSE, of course, also has the terms $V\psi$ and
$\kappa|\psi|^2\psi$. However, these are of lower order in the spatial
derivatives, and while they are obviously important, they do not
change the character of the equation.

In any case, the purpose of this discussion is to figure out
what time stepping scheme might be appropriate for the equation. The
conclusions is that, as a hyperbolic-kind of equation, we need to
choose a time step that satisfies a CFL-type condition. If we were to
use an explicit method (which we will not), we would have to investigate
the eigenvalues of the matrix that corresponds to the spatial
operator. If you followed the discussions of the video lectures
(@dealiiVideoLectureSeeAlso{26,27,28})
then you will remember that the pattern is that one needs to make sure
that $k^s \propto h^t$ where $k$ is the time step, $h$ the mesh width,
and $s,t$ are the orders of temporal and spatial derivatives.
Whether you take the original equation ($s=1,t=2$) or the reformulation
for only the real or imaginary part, the outcome is that we would need to
choose $k \propto h^2$ if we were to use an explicit time stepping
method. This is not feasible for the same reasons as in step-26 for
the heat equation: It would yield impractically small time steps
for even only modestly refined meshes. Rather, we have to use an
implicit time stepping method and can then choose a more balanced
$k \propto h$. Indeed, we will use the implicit Crank-Nicolson
method as we have already done in step-23 before for the regular
wave equation.


<h3>The general idea of operator splitting</h3>

@dealiiVideoLecture{30.25}

If one thought of the NLSE as an ordinary differential equation in
which the right hand side happens to have spatial derivatives, i.e.,
write it as
@f{align*}{
  \frac{d\psi}{dt}
  &=
  i\frac 12 \Delta \psi
  -i V \psi
  -i\kappa |\psi|^2 \psi,
  \qquad\qquad
  &
  \text{for}\; t \in (0,T),
  \\
  \psi(0) &= \psi_0,
@f}
one may be tempted to "formally solve" it by integrating both sides
over a time interval $[t_{n},t_{n+1}]$ and obtain
@f{align*}{
  \psi(t_{n+1})
  &=
  \psi(t_n)
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  i\frac 12 \Delta \psi(t)
  -i V \psi(t)
  -i\kappa |\psi(t)|^2 \psi(t)
  \right)
  \;
  dt.
@f}
Of course, it's not that simple: the $\psi(t)$ in the integrand is
still changing over time in accordance with the differential equation,
so we cannot just evaluate the integral (or approximate it easily via
quadrature) because we don't know $\psi(t)$.
But we can write this with separate contributions as
follows, and this will allow us to deal with different terms separately:
@f{align*}{
  \psi(t_{n+1})
  &=
  \psi(t_n)
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  i\frac 12 \Delta \psi(t)
  \right)
  \;
  dt
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  -i V \psi(t)
  \right)
  \;
  dt
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  -i\kappa |\psi(t)|^2 \,\psi(t)
  \right)
  \;
  dt.
@f}
The way this equation can now be read is as follows: For each time interval
$[t_{n},t_{n+1}]$, the change $\psi(t_{n+1})-\psi(t_{n})$ in the
solution consists of three contributions:
- The contribution of the Laplace operator.
- The contribution of the potential $V$.
- The contribution of the "phase" term $-i\kappa |\psi(t)|^2\,\psi(t)$.

<i>Operator splitting</i> is now an approximation technique that
allows us to treat each of these contributions separately. (If we
want: In practice, we will treat the first two together, and the last
one separate. But that is a detail, conceptually we could treat all of
them differently.) To this end, let us introduce three separate "solutions":
@f{align*}{
  \psi^{(1)}(t_{n+1})
  &=
  \psi(t_n)
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  i\frac 12 \Delta \psi^{(1)}(t)
  \right)
  \;
  dt,
\\
  \psi^{(2)}(t_{n+1})
  &=
  \psi(t_n)
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  -i V \psi^{(2)}(t)
  \right)
  \;
  dt,
\\
  \psi^{(3)}(t_{n+1})
  &=
  \psi(t_n)
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  -i\kappa |\psi^{(3)}(t)|^2 \,\psi^{(3)}(t)
  \right)
  \;
  dt.
@f}

These three "solutions" can be thought of as satisfying the following
differential equations:
@f{align*}{
  \frac{d\psi^{(1)}}{dt}
  &=
  i\frac 12 \Delta \psi^{(1)},
  \qquad
  &
  \text{for}\; t \in (t_n,t_{n+1}),
  \qquad\qquad\text{with initial condition}\;
  \psi^{(1)}(t_n) &= \psi(t_n),
\\
  \frac{d\psi^{(2)}}{dt}
  &=
  -i V \psi^{(2)},
  &
  \text{for}\; t \in (t_n,t_{n+1}),
  \qquad\qquad\text{with initial condition}\;
  \psi^{(2)}(t_n) &= \psi(t_n),
\\
  \frac{d\psi^{(3)}}{dt}
  &=
  -i\kappa |\psi^{(3)}|^2 \,\psi^{(3)},
  &
  \text{for}\; t \in (t_n,t_{n+1}),
  \qquad\qquad\text{with initial condition}\;
  \psi^{(3)}(t_n) &= \psi(t_n).
@f}
In other words, they are all trajectories $\psi^{(k)}$ that start at
$\psi(t_n)$ and integrate up the effects of exactly one of the three
terms. The increments resulting from each of these terms over our time
interval are then $I^{(1)}=\psi^{(1)}(t_{n+1})-\psi(t_n)$,
$I^{(2)}=\psi^{(2)}(t_{n+1})-\psi(t_n)$, and
$I^{(3)}=\psi^{(3)}(t_{n+1})-\psi(t_n)$.

It is now reasonable to assume (this is an approximation!) that the
change due to all three of the effects in question is well approximated
by the sum of the three separate increments:
@f{align*}{
 \psi(t_{n+1})-\psi(t_n)
 \approx
 I^{(1)} + I^{(2)} + I^{(3)}.
@f}
This intuition is indeed correct, though the approximation is not
exact: the difference between the exact left hand side and the term
$I^{(1)}+I^{(2)}+I^{(3)}$ (i.e., the difference between the <i>exact</i> increment
for the exact solution $\psi(t)$ when moving from $t_n$ to $t_{n+1}$,
and the increment composed of the three parts on the right hand side),
is proportional to $\Delta t=t_{n+1}-t_{n}$. In other words, this
approach introduces an error of size ${\cal O}(\Delta t)$. Nothing we
have done so far has discretized anything in time or space, so the
<i>overall</i> error is going to be ${\cal O}(\Delta t)$ plus whatever
error we commit when approximating the integrals (the temporal
discretization error) plus whatever error we commit when approximating
the spatial dependencies of $\psi$ (the spatial error).

Before we continue with discussions about operator splitting, let us
talk about why one would even want to go this way? The answer is
simple: For some of the separate equations for the $\psi^{(k)}$, we
may have ways to solve them more efficiently than if we throw
everything together and try to solve it at once. For example, and
particularly pertinent in the current case: The equation for
$\psi^{(3)}$, i.e.,
@f{align*}{
  \frac{d\psi^{(3)}}{dt}
  &=
  -i\kappa |\psi^{(3)}|^2 \,\psi^{(3)},
  \qquad\qquad
  &
  \text{for}\; t \in (t_n,t_{n+1}),
  \qquad\qquad\text{with initial condition}\;
  \psi^{(3)}(t_n) &= \psi(t_n),
@f}
or equivalently,
@f{align*}{
  \psi^{(3)}(t_{n+1})
  &=
  \psi(t_n)
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  -i\kappa |\psi^{(3)}(t)|^2 \,\psi^{(3)}(t)
  \right)
  \;
  dt,
@f}
can be solved exactly: the equation is solved by
@f{align*}{
  \psi^{(3)}(t) = e^{-i\kappa|\psi(t_n)|^2 (t-t_{n})} \psi(t_n).
@f}
This is easy to see if (i) you plug this solution into the
differential equation, and (ii) realize that the magnitude
$|\psi^{(3)}|$ is constant, i.e., the term $|\psi(t_n)|^2$ in the
exponent is in fact equal to $|\psi^{(3)}(t)|^2$. In other words, the
solution of the ODE for $\psi^{(3)}(t)$ only changes its <i>phase</i>,
but the <i>magnitude</i> of the complex-valued function $\psi^{(3)}(t)$
remains constant. This makes computing $I^{(3)}$ particularly convenient:
we don't actually need to solve any ODE, we can write the solution
down by hand. Using the operator splitting approach, none of the
methods to compute $I^{(1)},I^{(2)}$ therefore have to deal with the nonlinear
term and all of the associated unpleasantries: we can get away with
solving only <i>linear</i> problems, as long as we allow ourselves the
luxury of using an operator splitting approach.

Secondly, one often uses operator splitting if the different physical
effects described by the different terms have different time
scales. Imagine, for example, a case where we really did have some
sort of diffusion equation. Diffusion acts slowly, but if $\kappa$ is
large, then the "phase rotation" by the term $-i\kappa
|\psi^{(3)}(t)|^2 \,\psi^{(3)}(t)$ acts quickly. If we treated
everything together, this would imply having to take rather small time
steps. But with operator splitting, we can take large time steps
$\Delta t=t_{n+1}-t_{n}$ for the diffusion, and (assuming we didn't
have an analytic solution) use an ODE solver with many small time
steps to integrate the "phase rotation" equation for $\psi^{(3)}$ from
$t_n$ to $t_{n+1}$. In other words, operator splitting allows us to
decouple slow and fast time scales and treat them differently, with
methods adjusted to each case.


<h3>Operator splitting: the "Lie splitting" approach</h3>

While the method above allows to compute the three contributions
$I^{(k)}$ in parallel, if we want, the method can be made slightly
more accurate and easy to implement if we don't let the trajectories
for the $\psi^{(k)}$ start all at $\psi(t_n)$, but instead let the
trajectory for $\psi^{(2)}$ start at the <i>end point</i> of the
trajectory for $\psi^{(1)}$, namely $\psi^{(1)}(t_{n+1})$; similarly,
we will start the trajectory for $\psi^{(3)}$ start at the end point
of the trajectory for $\psi^{(2)}$, namely $\psi^{(2)}(t_{n+1})$. This
method is then called "Lie splitting" and has the same order of error
as the method above, i.e., the splitting error is ${\cal O}(\Delta
t)$.

This variation of operator splitting can be written as
follows (carefully compare the initial conditions to the ones above):
@f{align*}{
  \frac{d\psi^{(1)}}{dt}
  &=
  i\frac 12 \Delta \psi^{(1)},
  \qquad
  &
  \text{for}\; t \in (t_n,t_{n+1}),
  \qquad\qquad\text{with initial condition}\;
  \psi^{(1)}(t_n) &= \psi(t_n),
\\
  \frac{d\psi^{(2)}}{dt}
  &=
  -i V \psi^{(2)},
  &
  \text{for}\; t \in (t_n,t_{n+1}),
  \qquad\qquad\text{with initial condition}\;
  \psi^{(2)}(t_n) &= \psi^{(1)}(t_{n+1}),
\\
  \frac{d\psi^{(3)}}{dt}
  &=
  -i\kappa |\psi^{(3)}|^2 \,\psi^{(3)},
  &
  \text{for}\; t \in (t_n,t_{n+1}),
  \qquad\qquad\text{with initial condition}\;
  \psi^{(3)}(t_n) &= \psi^{(2)}(t_{n+1}).
@f}
(Obviously, while the formulas above imply that we should solve these
problems in this particular order, it is equally valid to first solve
for trajectory 3, then 2, then 1, or any other permutation.)

The integrated forms of these equations are then
@f{align*}{
  \psi^{(1)}(t_{n+1})
  &=
  \psi(t_n)
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  i\frac 12 \Delta \psi^{(1)}(t)
  \right)
  \;
  dt,
\\
  \psi^{(2)}(t_{n+1})
  &=
  \psi^{(1)}(t_{n+1})
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  -i V \psi^{(2)}(t)
  \right)
  \;
  dt,
\\
  \psi^{(3)}(t_{n+1})
  &=
  \psi^{(2)}(t_{n+1})
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  -i\kappa |\psi^{(3)}(t)|^2 \,\psi^{(3)}(t)
  \right)
  \;
  dt.
@f}
From a practical perspective, this has the advantage that we need
to keep around fewer solution vectors: Once $\psi^{(1)}(t_n)$ has been
computed, we don't need $\psi(t_n)$ any more; once $\psi^{(2)}(t_n)$
has been computed, we don't need $\psi^{(1)}(t_n)$ any more. And once
$\psi^{(3)}(t_n)$ has been computed, we can just call it
$\psi(t_{n+1})$ because, if you insert the first into the second, and
then into the third equation, you see that the right hand side of
$\psi^{(3)}(t_n)$ now contains the contributions of all three physical
effects:
@f{align*}{
  \psi^{(3)}(t_{n+1})
  &=
  \psi(t_n)
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  i\frac 12 \Delta \psi^{(1)}(t)
  \right)
  \;
  dt
  +
  \int_{t_n}^{t_{n+1}}
  \left(
  -i V \psi^{(2)}(t)
  \right)
  \;
  dt+
  \int_{t_n}^{t_{n+1}}
  \left(
  -i\kappa |\psi^{(3)}(t)|^2 \,\psi^{(3)}(t)
  \right)
  \;
  dt.
@f}
(Compare this again with the "exact" computation of $\psi(t_{n+1})$:
It only differs in how we approximate $\psi(t)$ in each of the three integrals.)
In other words, Lie splitting is a lot simpler to implement that the
original method outlined above because data handling is so much
simpler.


<h3>Operator splitting: the "Strang splitting" approach</h3>

As mentioned above, Lie splitting is only ${\cal O}(\Delta t)$
accurate. This is acceptable if we were to use a first order time
discretization, for example using the explicit or implicit Euler
methods to solve the differential equations for $\psi^{(k)}$. This is
because these time integration methods introduce an error proportional
to $\Delta t$ themselves, and so the splitting error is proportional
to an error that we would introduce anyway, and does not diminish the
overall convergence order.

But we typically want to use something higher order -- say, a
<a href="https://en.wikipedia.org/wiki/Crank%E2%80%93Nicolson_method">Crank-Nicolson</a>
or
<a href="https://en.wikipedia.org/wiki/Backward_differentiation_formula">BDF2</a>
method -- since these are often not more expensive than a
simple Euler method. It would be a shame if we were to use a time
stepping method that is ${\cal O}(\Delta t^2)$, but then lose the
accuracy again through the operator splitting.

This is where the <a
href="https://en.wikipedia.org/wiki/Strang_splitting">Strang
splitting</a> method comes in. It is easier to explain if we had only
two parts, and so let us combine the effects of the Laplace operator
and of the potential into one, and the phase rotation into a second
effect. (Indeed, this is what we will do in the code since solving the
equation with the Laplace equation with or without the potential costs
the same -- so we merge these two steps.) The Lie splitting method
from above would then do the following: It computes solutions of the
following two ODEs,
@f{align*}{
  \frac{d\psi^{(1)}}{dt}
  &=
  i\frac 12 \Delta \psi^{(1)} -i V \psi^{(1)},
  \qquad
  &
  \text{for}\; t \in (t_n,t_{n+1}),
  \qquad\qquad\text{with initial condition}\;
  \psi^{(1)}(t_n) &= \psi(t_n),
\\
  \frac{d\psi^{(2)}}{dt}
  &=
  -i\kappa |\psi^{(2)}|^2 \,\psi^{(2)},
  &
  \text{for}\; t \in (t_n,t_{n+1}),
  \qquad\qquad\text{with initial condition}\;
  \psi^{(2)}(t_n) &= \psi^{(1)}(t_{n+1}),
@f}
and then uses the approximation $\psi(t_{n+1}) \approx
\psi^{(2)}(t_{n+1})$. In other words, we first make one full time step
for physical effect one, then one full time step for physical effect
two. The solution at the end of the time step is simply the sum of the
increments due to each of these physical effects separately.

In contrast,
<a href="https://en.wikipedia.org/wiki/Gilbert_Strang">Gil Strang</a>
(one of the titans of numerical analysis starting in the mid-20th
century) figured out that it is more accurate to first do
one half-step for one physical effect, then a full time step for the
other physical effect, and then another half step for the first. Which
one is which does not matter, but because it is so simple to do the
phase rotation, we will use this effect for the half steps and then
only need to do one spatial solve with the Laplace operator plus
potential. This operator splitting method is now ${\cal O}(\Delta
t^2)$ accurate. Written in formulas, this yields the following
sequence of steps:
@f{align*}{
  \frac{d\psi^{(1)}}{dt}
  &=
  -i\kappa |\psi^{(1)}|^2 \,\psi^{(1)},
  &&
  \text{for}\; t \in (t_n,t_n+\tfrac 12\Delta t),
  \qquad\qquad&\text{with initial condition}\;
  \psi^{(1)}(t_n) &= \psi(t_n),
\\
  \frac{d\psi^{(2)}}{dt}
  &=
  i\frac 12 \Delta \psi^{(2)} -i V \psi^{(2)},
  \qquad
  &&
  \text{for}\; t \in (t_n,t_{n+1}),
  \qquad\qquad&\text{with initial condition}\;
  \psi^{(2)}(t_n) &= \psi^{(1)}(t_n+\tfrac 12\Delta t),
\\
  \frac{d\psi^{(3)}}{dt}
  &=
  -i\kappa |\psi^{(3)}|^2 \,\psi^{(3)},
  &&
  \text{for}\; t \in (t_n+\tfrac 12\Delta t,t_{n+1}),
  \qquad\qquad&\text{with initial condition}\;
  \psi^{(3)}(t_n) &= \psi^{(2)}(t_{n+1}).
@f}
As before, the first and third step can be computed exactly for this
particular equation, yielding
@f{align*}{
  \psi^{(1)}(t_n+\tfrac 12\Delta t) &= e^{-i\kappa|\psi(t_n)|^2 \tfrac
  12\Delta t} \; \psi(t_n),
  \\
  \psi^{(3)}(t_{n+1}) &= e^{-i\kappa|\psi^{(2)}(t_{n+1})|^2 \tfrac
  12\Delta t} \; \psi^{(2)}(t_{n+1}).
@f}

This is then how we are going to implement things in this program:
In each time step, we execute three steps, namely
- Update the solution value at each node by analytically integrating
  the phase rotation equation by one half time step;
- Solving the space-time equation that corresponds to the full step
  for $\psi^{(2)}$, namely
  $-i\frac{\partial\psi^{(2)}}{\partial t}
  -
  \frac 12 \Delta \psi^{(2)} + V \psi^{(2)} = 0$,
  with initial conditions equal to the solution of the first half step
  above.
- Update the solution value at each node by analytically integrating
  the phase rotation equation by another half time step.

This structure will be reflected in an obvious way in the main time
loop of the program.



<h3>Time discretization</h3>

From the discussion above, it should have become clear that the only
partial differential equation we have to solve in each time step is
@f{align*}{
  -i\frac{\partial\psi^{(2)}}{\partial t}
  -
  \frac 12 \Delta \psi^{(2)} + V \psi^{(2)} = 0.
@f}
This equation is linear. Furthermore, we only have to solve it from
$t_n$ to $t_{n+1}$, i.e., for exactly one time step.

To do this, we will apply the second order accurate Crank-Nicolson
scheme that we have already used in some of the other time dependent
codes (specifically: step-23 and step-26). It reads as follows:
@f{align*}{
  -i\frac{\psi^{(n,2)}-\psi^{(n,1)}}{k_{n+1}}
  -
  \frac 12 \Delta \left[\frac 12
  \left(\psi^{(n,2)}+\psi^{(n,1)}\right)\right]
  +
  V \left[\frac 12 \left(\psi^{(n,2)}+\psi^{(n,1)}\right)\right] = 0.
@f}
Here, the "previous" solution $\psi^{(n,1)}$ (or the "initial
condition" for this part of the time step) is the output of the
first phase rotation half-step; the output of the current step will
be denoted by $\psi^{(n,2)}$. $k_{n+1}=t_{n+1}-t_n$ is
the length of the time step. (One could argue whether $\psi^{(n,1)}$
and $\psi^{(n,1)}$ live at time step $n$ or $n+1$ and what their upper
indices should be. This is a philosophical discussion without practical
impact, and one might think of $\psi^{(n,1)}$ as something like
$\psi^{(n+\tfrac 13)}$, and $\psi^{(n,2)}$ as
$\psi^{(n+\tfrac 23)}$ if that helps clarify things -- though, again
$n+\frac 13$ is not to be understood as "one third time step after
$t_n$" but more like "we've already done one third of the work necessary
for time step $n+1$".)

If we multiply the whole equation with $k_{n+1}$ and sort terms with
the unknown $\psi^{(n+1,2)}$ to the left and those with the known
$\psi^{(n,2)}$ to the right, then we obtain the following (spatial)
partial differential equation that needs to be solved in each time
step:
@f{align*}{
  -i\psi^{(n,2)}
  -
  \frac 14 k_{n+1} \Delta \psi^{(n,2)}
  +
  \frac 12 k_{n+1} V \psi^{(n,2)}
  =
  -i\psi^{(n,1)}
  +
  \frac 14 k_{n+1} \Delta \psi^{(n,1)}
  -
  \frac 12 k_{n+1} V \psi^{(n,1)}.
@f}



<h3>Spatial discretization and dealing with complex variables</h3>

As mentioned above, the previous tutorial program dealing with
complex-valued solutions (namely, step-29) separated real and imaginary
parts of the solution. It thus reduced everything to real
arithmetic. In contrast, we here want to keep things
complex-valued.

The first part of this is that we need to define the discretized
solution as $\psi_h^n(\mathbf x)=\sum_j \Psi^n_j \varphi_j(\mathbf
x) \approx \psi(\mathbf x,t_n)$ where the $\varphi_j$ are the usual shape functions (which are
real valued) but the expansion coefficients $\Psi^n_j$ at time step
$n$ are now complex-valued. This is easily done in deal.II: We just
have to use Vector<std::complex<double>> instead of Vector<double> to
store these coefficients.

Of more interest is how to build and solve the linear
system. Obviously, this will only be necessary for the second step of
the Strang splitting discussed above, with the time discretization of
the previous subsection. We obtain the fully discrete version through
straightforward substitution of $\psi^n$ by $\psi^n_h$ and
multiplication by a test function:
@f{align*}{
  -iM\Psi^{(n,2)}
  +
  \frac 14 k_{n+1} A \Psi^{(n,2)}
  +
  \frac 12 k_{n+1} W \Psi^{(n,2)}
  =
  -iM\Psi^{(n+1,1)}
  -
  \frac 14 k_{n+1} A \Psi^{(n,1)}
  -
  \frac 12 k_{n+1} W \Psi^{(n,1)},
@f}
or written in a more compact way:
@f{align*}{
  \left[
    -iM
    +
    \frac 14 k_{n+1} A
    +
    \frac 12 k_{n+1} W
  \right] \Psi^{(n,2)}
  =
  \left[
    -iM
    -
    \frac 14 k_{n+1} A
    -
   \frac 12 k_{n+1} W
  \right] \Psi^{(n,1)}.
@f}
Here, the matrices are defined in their obvious ways:
@f{align*}{
  M_{ij} &= (\varphi_i,\varphi_j), \\
  A_{ij} &= (\nabla\varphi_i,\nabla\varphi_j), \\
  W_{ij} &= (\varphi_i,V \varphi_j).
@f}
Note that all matrices individually are in fact symmetric,
real-valued, and at least positive semidefinite, though the same is
obviously not true for
the system matrix $C = -iM + \frac 14 k_{n+1} A + \frac 12 k_{n+1} W$
and the corresponding matrix
$R = -iM - \frac 14 k_{n+1} A - \frac 12 k_{n+1} W$
on the right hand side.


<h3>Linear solvers</h3>

@dealiiVideoLecture{34}

The only remaining important question about the solution procedure is
how to solve the complex-valued linear system
@f{align*}{
  C \Psi^{(n+1,2)}
  =
  R \Psi^{(n+1,1)},
@f}
with the matrix $C = -iM + \frac 14 k_{n+1} A + \frac 12 k_{n+1}
W$ and a right hand side that is easily computed as the product of
a known matrix and the previous part-step's solution.
As usual, this comes down to the question of what properties the
matrix $C$ has. If it is symmetric and positive definite, then we can
for example use the Conjugate Gradient method.

Unfortunately, the matrix's only useful property is that it is complex
symmetric, i.e., $C_{ij}=C_{ji}$, as is easy to see by recalling that
$M,A,W$ are all symmetric. It is not, however,
<a href="https://en.wikipedia.org/wiki/Hermitian_matrix">Hermitian</a>,
which would require that $C_{ij}=\bar C_{ji}$ where the bar indicates complex
conjugation.

Complex symmetry can be exploited for iterative solvers as a quick
literature search indicates. We will here not try to become too
sophisticated (and indeed leave this to the <a
href="#extensions">Possibilities for extensions</a> section below) and
instead simply go with the good old standby for problems without
properties: A direct solver. That's not optimal, especially for large
problems, but it shall suffice for the purposes of a tutorial program.
Fortunately, the SparseDirectUMFPACK class allows solving complex-valued
problems.


<h3>Definition of the test case</h3>

Initial conditions for the NLSE are typically chosen to represent
particular physical situations. This is beyond the scope of this
program, but suffice it to say that these initial conditions are
(i) often superpositions of the wave functions of particles located
at different points, and that (ii) because $|\psi(\mathbf x,t)|^2$
corresponds to a particle density function, the integral
@f[
  N(t) = \int_\Omega |\psi(\mathbf x,t)|^2
@f]
corresponds to the number of particles in the system. (Clearly, if
one were to be physically correct, $N(t)$ better be a constant if
the system is closed, or $\frac{dN}{dt}<0$ if one has absorbing
boundary conditions.) The important point is that one should choose
initial conditions so that
@f[
  N(0) = \int_\Omega |\psi_0(\mathbf x)|^2
@f]
makes sense.

What we will use here, primarily because it makes for good graphics,
is the following:
@f[
  \psi_0(\mathbf x) = \sqrt{\sum_{k=1}^4 \alpha_k e^{-\frac{r_k^2}{R^2}}},
@f]
where $r_k = |\mathbf x-\mathbf x_k|$ is the distance from the (fixed)
locations $\mathbf x_k$, and
$\alpha_k$ are chosen so that each of the Gaussians that we are
adding up adds an integer number of particles to $N(0)$. We achieve
this by making sure that
@f[
  \int_\Omega \alpha_k e^{-\frac{r_k^2}{R^2}}
@f]
is a positive integer. In other words, we need to choose $\alpha$
as an integer multiple of
@f[
  \left(\int_\Omega e^{-\frac{r_k^2}{R^2}}\right)^{-1}
  =
  \left(R^d\sqrt{\pi^d}\right)^{-1},
@f]
assuming for the moment that $\Omega={\mathbb R}^d$ -- which is
of course not the case, but we'll ignore the small difference in
integral.

Thus, we choose $\alpha_k=\left(R^d\sqrt{\pi^d}\right)^{-1}$ for all, and
$R=0.1$. This $R$ is small enough that the difference between the
exact (infinite) integral and the integral over $\Omega$ should not be
too concerning.
We choose the four points $\mathbf x_k$ as $(\pm 0.3, 0), (0, \pm
0.3)$ -- also far enough away from the boundary of $\Omega$ to keep
ourselves on the safe side.

For simplicity, we pose the problem on the square $[-1,1]^2$. For
boundary conditions, we will use time-independent Neumann conditions of the
form
@f[
  \nabla\psi(\mathbf x,t)\cdot \mathbf n=0 \qquad\qquad \forall \mathbf x\in\partial\Omega.
@f]
This is not a realistic choice of boundary conditions but sufficient
for what we want to demonstrate here. We will comment further on this
in the <a href="#extensions">Possibilities for extensions</a> section below.

Finally, we choose $\kappa=1$, and the potential as
@f[
  V(\mathbf x)
  =
  \begin{cases} 0 & \text{if}\; |\mathbf x|<0.7
                \\
                1000 & \text{otherwise}.
  \end{cases}
@f]
Using a large potential makes sure that the wave function $\psi$ remains
small outside the circle of radius 0.7. All of the Gaussians that make
up the initial conditions are within this circle, and the solution will
mostly oscillate within it, with a small amount of energy radiating into
the outside. The use of a large potential also makes sure that the nonphysical
boundary condition does not have too large an effect.


examples/step-58/doc/results.dox
<h1>Results</h1>

Running the code results in screen output like the following:
```
Number of active cells: 4096
Number of degrees of freedom: 16641

Time step 1 at t=0
Time step 2 at t=0.00390625
Time step 3 at t=0.0078125
Time step 4 at t=0.0117188
[...]
```
Running the program also yields a good number of output files that we will
visualize in the following.


<h3>Visualizing the solution</h3>

The `output_results()` function of this program generates output files that
consist of a number of variables: The solution (split into its real and imaginary
parts), the amplitude, and the phase. If we visualize these four fields, we get
images like the following after a few time steps (at time $t=0.242$, to be
precise:

<div class="twocolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-58.re.png"
         alt="Real part of the solution at t=0.242"
         width="400">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-58.im.png"
         alt="Imaginary part of the solution at t=0.242"
         width="400">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-58.magnitude.png"
         alt="Amplitude of the solution at t=0.242"
         width="400">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-58.phase.png"
         alt="Phase of the solution at t=0.242"
         width="400">
  </div>
</div>

While the real and imaginary parts of the solution shown above are not
particularly interesting (because, from a physical perspective, the
global offset of the phase and therefore the balance between real and
imaginary components, is meaningless), it is much more interesting to
visualize the amplitude $|\psi(\mathbf x,t)|^2$ and phase
$\text{arg}(\psi(\mathbf x,t))$ of the solution and, in particular,
their evolution. This leads to pictures like the following:

The phase picture shown here clearly has some flaws:
- First, phase is a "cyclic quantity", but the color scale uses a
  fundamentally different color for values close to $-\pi$ than
  for values close to $+\pi$. This is a nuisance -- what we need
  is a "cyclic color map" that uses the same colors for the two
  extremes of the range of the phase. Such color maps exist,
  see <a href="https://nicoguaro.github.io/posts/cyclic_colormaps/">this
  blog post of Nicolás Guarín-Zapata</a> or
  <a href="https://stackoverflow.com/questions/23712207/cyclic-colormap-without-visual-distortions-for-use-in-phase-angle-plots">this
  StackExchange post</a>, for example. The problem is that the
  author's favorite
  one of the two big visualization packages, VisIt, does not have any
  of these color maps built in. In an act of desperation, I therefore
  had to resort to using Paraview given that it has several of the
  color maps mentioned in the post above implemented. The picture
  below uses the `nic_Edge` map in which both of the extreme values are shown
  as black.
- There is a problem on cells in which the phase wraps around. If
  at some evaluation point of the cell the phase value is close to
  $-\pi$ and at another evaluation point it is close to $+\pi$, then
  what we would really like to happen is for the entire cell to have a
  color close to the extremes. But, instead, visualization programs
  produce a linear interpolation in which the values within the cell,
  i.e., between the evaluation points, is linearly interpolated between
  these two values, covering essentially the entire range of possible
  phase values and, consequently, cycling through the entire
  rainbow of colors from dark red to dark green over the course of
  one cell. The solution to this problem is to just output
  the phase value on each cell as a piecewise constant. Because
  averaging values close to the $-\pi$ and $+\pi$ is going to
  result in an average that has nothing to do with the actual phase
  angle, the `ComplexPhase` class just uses the *maximal* phase
  angle encountered on each cell.

With these modifications, the phase plot now looks as follows:

<p align="center">
  <img src="https://www.dealii.org/images/steps/developer/step-58.phase-cyclic.png"
         alt="Phase of the solution at t=0.242, with a cyclic color map"
         width="400">
</p>

Finally, we can generate a movie out of this. (To be precise, the video
uses two more global refinement cycles and a time step half the size
of what is used in the program above.) The author of these lines
made the movie with VisIt,
because that's what he's more familiar with, and using a hacked color map
that is also cyclic -- though this color map lacks all of the skill employed by
the people who wrote the posts mentioned in the links above. It
does, however, show the character of the solution as a wave equation
if you look at the shaded part of the domain outside the circle of
radius 0.7 in which the potential is zero -- you can see how every time
one of the bumps (showing the amplitude $|\psi_h(\mathbf x,t)|^2$)
bumps into the area where the potential is large: a wave travels
outbound from there. Take a look at the video:

@htmlonly
<p align="center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/nraszP3GZHk"
   frameborder="0"
   allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
   allowfullscreen></iframe>
 </p>
@endhtmlonly

So why did I end up shading the area where the potential $V(\mathbf x)$ is
large? In that outside region, the solution is relatively small. It is also
relatively smooth. As a consequence, to some approximate degree, the
equation in that region simplifies to
@f[
  - i \frac{\partial \psi}{\partial t}
  + V \psi
  \approx 0,
@f]
or maybe easier to read:
@f[
  \frac{\partial \psi}{\partial t}
  \approx - i V \psi.
@f]
To the degree to which this approximation is valid (which, among other things,
eliminates the traveling waves you can see in the video), this equation has
a solution
@f[
  \psi(\mathbf x, t) = \psi(\mathbf x, 0) e^{-i V t}.
@f]
Because $V$ is large, this means that the phase *rotates quite rapidly*.
If you focus on the semi-transparent outer part of the domain, you can
see that. If one colors this region in the same way as the inner part of
the domain, this rapidly flashing outer part may be psychedelic, but is also
distracting of what's happening on the inside; it's also quite hard to
actually see the radiating waves that are easy to see at the beginning
of the video.


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

<h4> Better linear solvers </h4>

The solver chosen here is just too simple. It is also not efficient.
What we do here is give the matrix to a sparse direct solver in every
time step and let it find the solution of the linear system. But we
know that we could do far better:

- First, we should make use of the fact that the matrix doesn't
  actually change from time step to time step. This is an artifact
  of the fact that we here have constant boundary values and that
  we don't change the time step size -- two assumptions that might
  not be true in actual applications. But at least in cases where this
  does happen to be the case, it would make sense to only factorize
  the matrix once (i.e., compute $L$ and $U$ factors once) and then
  use these factors for all following time steps until the matrix
  $C$ changes and requires a new factorization. The interface of the
  SparseDirectUMFPACK class allows for this.

- Ultimately, however, sparse direct solvers are only efficient for
  relatively small problems, say up to a few 100,000 unknowns. Beyond
  this, one needs iterative solvers such as the Conjugate Gradient method (for
  symmetric and positive definite problems) or GMRES. We have used many
  of these in other tutorial programs. In all cases, they need to be
  accompanied by good preconditioners. For the current case, one
  could in principle use GMRES -- a method that does not require
  any specific properties of the matrix -- but would be better
  advised to implement an iterative scheme that exploits the one
  structural feature we know is true for this problem: That the matrix
  is complex-symmetric (albeit not Hermitian).


<h4> Boundary conditions </h4>

In order to be usable for actual, realistic problems, solvers for the
nonlinear Schr&ouml;dinger equation need to utilize boundary conditions
that make sense for the problem at hand. We have here restricted ourselves
to simple Neumann boundary conditions -- but these do not actually make
sense for the problem. Indeed, the equations are generally posed on an
infinite domain. But, since we can't compute on infinite domains, we need
to truncate it somewhere and instead pose boundary conditions that make
sense for this artificially small domain. The approach widely used is to
use the <a
href="https://en.wikipedia.org/wiki/Perfectly_matched_layer">Perfectly
Matched Layer</a> method that corresponds to a particular
kind of attenuation. It is, in a different context, also used in
step-62.


<h4> Adaptive meshes </h4>

Finally, we know from experience and many other tutorial programs that
it is worthwhile to use adaptively refined meshes, rather than the uniform
meshes used here. It would, in fact, not be very difficult to add this
here: It just requires periodic remeshing and transfer of the solution
from one mesh to the next. step-26 will be a good guide for how this
could be implemented.


examples/step-59/doc/intro.dox
<br>

<i>
This program was contributed by Katharina Kormann and Martin
Kronbichler.

This work was partly supported by the German Research Foundation (DFG) through
the project "High-order discontinuous Galerkin for the exa-scale" (ExaDG)
within the priority program "Software for Exascale Computing" (SPPEXA). </i>

<a name="Intro"></a>
<h1>Introduction</h1>

Matrix-free operator evaluation enables very efficient implementations of
discretization with high-order polynomial bases due to a method called sum
factorization. This concept has been introduced in the step-37 and step-48
tutorial programs. In this tutorial program, we extend those concepts to
discontinuous Galerkin (DG) schemes that include face integrals, a class of
methods where high orders are particularly widespread.

The underlying idea of the matrix-free evaluation is the same as for
continuous elements: The matrix-vector product that appears in an iterative
solver or multigrid smoother is not implemented by a classical sparse matrix
kernel, but instead applied implicitly by the evaluation of the underlying
integrals on the fly. For tensor product shape functions that are integrated
with a tensor product quadrature rule, this evaluation is particularly
efficient by using the sum-factorization technique, which decomposes the
initially $(k+1)^{2d}$ operations for interpolation involving $(k+1)^d$ vector
entries with associated shape functions at degree $k$ in $d$ dimensions to
$(k+1)^d$ quadrature points into $d$ one-dimensional operations of cost
$(k+1)^{d+1}$ each. In 3D, this reduces the order of complexity by two powers
in $k$. When measured as the complexity per degree of freedom, the complexity
is $\mathcal O(k)$ in the polynomial degree. Due to the presence of face
integrals in DG, and due to the fact that operations on quadrature points
involve more memory transfer, which both scale as $\mathcal O(1)$, the
observed complexity is often constant for moderate $k\leq 10$. This means that
a high order method can be evaluated with the same throughput in terms of
degrees of freedom per second as a low-order method.

More information on the algorithms are available in the preprint
<br>
<a href="https://arxiv.org/abs/1711.03590">Fast matrix-free evaluation of
discontinuous Galerkin finite element operators</a> by Martin Kronbichler and
Katharina Kormann, arXiv:1711.03590.

<h3>The symmetric interior penalty formulation for the Laplacian</h3>

For this tutorial program, we exemplify the matrix-free DG framework for the
interior penalty discretization of the Laplacian, i.e., the same scheme as the
one used for the step-39 tutorial program. The discretization of the Laplacian
is given by the following weak form
@f{align*}
&\sum_{K\in\text{cells}} \left(\nabla v_h, \nabla u_h\right)_{K}+\\
&\sum_{F\in\text{faces}}\Big(-\left<\jump{v_h}, \average{\nabla u_h}\right>_{F} - \left<\average{\nabla v_h}, \jump{u_h}\right>_{F} + \left<\jump{v_h}, \sigma \jump{u_h}\right>_{F}\Big) \\
&= \sum_{K\in\text{cells}}\left(v_h, f\right)_{K},
@f}
where $\jump{v} = v^- \mathbf{n}^- + v^+ \mathbf{n}^+ = \mathbf n^{-}
\left(v^- - v^+\right)$ denotes the directed jump of the quantity $v$ from the
two associated cells $K^-$ and $K^+$, and $\average{v}=\frac{v^- + v^+}{2}$
is the average from both sides.

The terms in the equation represent the cell integral after integration by
parts, the primal consistency term that arises at the element interfaces due
to integration by parts and insertion of an average flux, the adjoint
consistency term that is added for restoring symmetry of the underlying
matrix, and a penalty term with factor $\sigma$, whose magnitude is equal the
length of the cells in direction normal to face multiplied by $k(k+1)$, see
step-39. The penalty term is chosen such that an inverse estimate holds and
the final weak form is coercive, i.e., positive definite in the discrete
setting. The adjoint consistency term and the penalty term involve the jump
$\jump{u_h}$ at the element interfaces, which disappears for the analytic
solution $u$. Thus, these terms are consistent with the original PDE, ensuring
that the method can retain optimal orders of convergence.

In the implementation below, we implement the weak form above by moving the
normal vector $\mathbf{n}^-$ from the jump terms to the derivatives to form a
<i>normal</i> derivative of the form $\mathbf{n}^-\cdot \nabla u_h$. This
makes the implementation on quadrature points slightly more efficient because
we only need to work with scalar terms rather than tensors, and is
mathematically equivalent.

For boundary conditions, we use the so-called mirror principle that defines
<i>artificial</i> exterior values $u^+$ by extrapolation from the interior
solution $u^-$ combined with the given boundary data, setting $u^+ = -u^- + 2
g_\text{D}$ and $\mathbf{n}^-\cdot \nabla u^+ = \mathbf{n}^-\cdot \nabla u^-$
on Dirichlet boundaries and $u^+=u^-$ and $\mathbf{n}^-\cdot \nabla u^+ =
-\mathbf{n}^-\cdot \nabla u^- + 2 g_\text{N}$ on Neumann boundaries, for given
Dirichlet values $g_\text{D}$ and Neumann values $g_\text{N}$. These
expressions are then inserted in the above weak form. Contributions involving
the known quantities $g_\text{D}$ and $g_\text{N}$ are eventually moved to the
right hand side, whereas the unknown value $u^-$ is retained on the left hand
side and contributes to the matrix terms similarly as interior faces. Upon
these manipulations, the same weak form as in step-39 is obtained.

<h3>Face integration support in MatrixFree and FEFaceEvaluation</h3>

The matrix-free framework of deal.II provides the necessary infrastructure to
implement the action of the discretized equation above. As opposed to the
MatrixFree::cell_loop() that we used in step-37 and step-48, we now build a
code in terms of MatrixFree::loop() that takes three function pointers, one
for the cell integrals, one for the inner face integrals, and one for the
boundary face integrals (in analogy to the design of MeshWorker used in the
step-39 tutorial program). In each of these three functions, we then implement
the respective terms on the quadrature points. For interpolation between the
vector entries and the values and gradients on quadrature points, we use the
class FEEvaluation for cell contributions and FEFaceEvaluation for face
contributions. The basic usage of these functions has been discussed
extensively in the step-37 tutorial program.

In MatrixFree::loop(), all interior faces are visited exactly once, so one
must make sure to compute the contributions from both the test functions
$v_h^-$ and $v_h^+$. Given the fact that the test functions on both sides are
indeed independent, the weak form above effectively means that we submit the
same contribution to both an FEFaceEvaluation object called `phi_inner` and
`phi_outer` for testing with the normal derivative of the test function, and
values with opposite sign for testing with the values of the test function,
because the latter involves opposite signs due to the jump term. For faces
between cells of different refinement level, the integration is done from the
refined side, and FEFaceEvaluation automatically performs interpolation to a
subface on the coarse side. Thus, a hanging node never appears explicitly in a
user implementation of a weak form.

The fact that each face is visited exactly once also applies to those faces at
subdomain boundaries between different processors when parallelized with MPI,
where one cell belongs to one processor and one to the other. The setup in
MatrixFree::reinit() splits the faces between the two sides, and eventually
only reports the faces actually handled locally in
MatrixFree::n_inner_face_batches() and MatrixFree::n_boundary_face_batches(),
respectively. Note that, in analogy to the cell integrals discussed in
step-37, deal.II applies vectorization over several faces to use SIMD, working
on something we call a <i>batch of faces</i> with a single instruction. The
face batches are independent from the cell batches, even though the time at
which face integrals are processed is kept close to the time when the cell
integrals of the respective cells are processed, in order to increase the data
locality.

Another thing that is new in this program is the fact that we no longer split
the vector access like FEEvaluation::read_dof_values() or
FEEvaluation::distribute_local_to_global() from the evaluation and integration
steps, but call combined functions FEEvaluation::gather_evaluate() and
FEEvaluation::integrate_scatter(), respectively. This is useful for face
integrals because, depending on what gets evaluated on the faces, not all
vector entries of a cell must be touched in the first place. Think for example
of the case of the nodal element FE_DGQ with node points on the element
surface: If we are interested in the shape function values on a face, only
$(k+ 1)^{d-1}$ degrees of freedom contribute to them in a non-trivial way (in
a more technical way of speaking, only $(k+1)^{d-1}$ shape functions have a
nonzero support on the face and return true for
FiniteElement::has_support_on_face()). When compared to the $(k+1)^d$ degrees
of freedom of a cell, this is one power less.

Now of course we are not interested in only the function values, but also the
derivatives on the cell. Fortunately, there is an element in deal.II that
extends this property of reduced access also for derivatives on faces, the
FE_DGQHermite element.

<h3>The FE_DGQHermite element</h3>

The element FE_DGQHermite belongs to the family of FE_DGQ elements, i.e., its
shape functions are a tensor product of 1D polynomials and the element is
fully discontinuous. As opposed to the nodal character in the usual FE_DGQ
element, the FE_DGQHermite element is a mixture of nodal contributions and
derivative contributions based on a Hermite-like concept. The underlying
polynomial class is Polynomials::HermiteLikeInterpolation and can be
summarized as follows: For cubic polynomials, we use two polynomials to
represent the function value and first derivative at the left end of the unit
interval, $x=0$, and two polynomials to represent the function value and first
derivative and the right end of the unit interval, $x=1$. At the opposite
ends, both the value and first derivative of the shape functions are zero,
ensuring that only two out of the four basis functions contribute to values
and derivative on the respective end. However, we deviate from the classical
Hermite interpolation in not strictly assigning one degree of freedom for the
value and one for the first derivative, but rather allow the first derivative
to be a linear combination of the first and the second shape function. This is
done to improve the conditioning of the interpolation. Also, when going to
degrees beyond three, we add node points in the element interior in a
Lagrange-like fashion, combined with double zeros in the points $x=0$ and
$x=1$. The position of these extra nodes is determined by the zeros of some
Jacobi polynomials as explained in the description of the class
Polynomials::HermiteLikeInterpolation.

Using this element, we only need to access $2(k+1)^{d-1}$ degrees of freedom
for computing both values and derivatives on a face. The check whether the
Hermite property is fulfilled is done transparently inside
FEFaceEvaluation::gather_evaluate() and FEFaceEvaluation::integrate_scatter()
that check the type of the basis and reduce the access to data if
possible. Obviously, this would not be possible if we had separated
FEFaceEvaluation::read_dof_values() from FEFaceEvaluation::evaluate(), because
the amount of entries we need to read depends on the type of the derivative
(only values, first derivative, etc.) and thus must be given to
`read_dof_values()`.

This optimization is not only useful for computing the face integrals, but
also for the MPI ghost layer exchange: In a naive exchange, we would need to
send all degrees of freedom of a cell to another processor if the other
processor is responsible for computing the face's contribution. Since we know
that only some of the degrees of freedom in the evaluation with
FEFaceEvaluation are touched, it is natural to only exchange the relevant
ones. The MatrixFree::loop() function has support for a selected data exchange
when combined with LinearAlgebra::distributed::Vector. To make this happen, we
need to tell the loop what kind of evaluation on faces we are going to do,
using an argument of type MatrixFree::DataAccessOnFaces, as can be seen in the
implementation of `LaplaceOperator::vmult()` below. The way data is exchanged
in that case is as follows: The ghost layer data in the vector still pretends
to represent all degrees of freedom, such that FEFaceEvaluation can continue
to read the values as if the cell were a locally owned one. The data exchange
routines take care of the task for packing and unpacking the data into this
format. While this sounds pretty complicated, we will show in the results
section below that this really pays off by comparing the performance to a
baseline code that does not specify the data access on faces.

<h3>An approximate block-Jacobi smoother using the fast diagonalization method</h3>

In the tradition of the step-37 program, we again solve a Poisson problem with
a geometric multigrid preconditioner inside a conjugate gradient
solver. Instead of computing the diagonal and use the basic
PreconditionChebyshev as a smoother, we choose a different strategy in this
tutorial program. We implement a block-Jacobi preconditioner, where a block
refers to all degrees of freedom on a cell. Rather than building the full cell
matrix and applying its LU factorization (or inverse) in the preconditioner
&mdash; an operation that would be heavily memory bandwidth bound and thus
pretty slow &mdash; we approximate the inverse of the block by a special
technique called fast diagonalization method.

The idea of the method is to take use of the structure of the cell matrix. In
case of the Laplacian with constant coefficients discretized on a Cartesian
mesh, the cell matrix $L$ can be written as
@f{align*}{
L &= A_1 \otimes M_0 + M_1 \otimes A_0
@f}
in 2D and
@f{align*}{
L &= A_2 \otimes M_1 \otimes M_0 + M_2 \otimes A_1 \otimes M_0 + M_2 \otimes M_1 \otimes A_0
@f}
in 3D. The matrices $A_0$ and $A_1$ denote the 1D Laplace matrix (including
the cell and face term associated to the current cell values $u^-_h$ and
$v^-_h$) and $M_0$ and $M_1$ are the mass matrices. Note that this simple
tensor product structure is lost once there are non-constant coefficients on
the cell or the geometry is not constant any more. We mention that a similar
setup could also be used to replace the computed integrals with this final
tensor product form of the matrices, which would cut the operations for the
operator evaluation into less than half. However, given the fact that this
only holds for Cartesian cells and constant coefficients, which is a pretty
narrow case, we refrain from pursuing this idea.

Interestingly, the exact inverse of the matrix $L$ can be found through tensor
products due to a method introduced by <a
href="http://dl.acm.org/citation.cfm?id=2716130">R. E. Lynch, J. R. Rice,
D. H. Thomas, Direct solution of partial difference equations by tensor
product methods, Numerische Mathematik 6, 185-199</a> from 1964,
@f{align*}{
L^{-1} &= S_1 \otimes S_0 (\Lambda_1 \otimes I + I \otimes \Lambda_0)^{-1}
S_1^\mathrm T \otimes S_0^\mathrm T,
@f}
where $S_d$ is the matrix of eigenvectors to the generalized eigenvalue problem
in the given tensor direction $d$:
@f{align*}{
A_d s  &= \lambda M_d s, \quad d = 0, \ldots,\mathrm{dim-1},
@f}
and $\Lambda_d$ is the diagonal matrix representing the generalized
eigenvalues $\lambda$. Note that the vectors $s$ are such that they
simultaneously diagonalize $A_d$ and $M_d$, i.e. $S_d^{\mathrm T} A_d S_d =
\Lambda_d$ and $S_d^{\mathrm T} M_d S_d = I$.

The deal.II library implements a class using this concept, called
TensorProductMatrixSymmetricSum.

For the sake of this program, we stick with constant coefficients and
Cartesian meshes, even though an approximate version based on tensor products
would still be possible for a more general mesh, and the operator evaluation
itself is of course generic. Also, we do not bother with adaptive meshes where
the multigrid algorithm would need to get access to flux matrices over the
edges of different refinement, as explained in step-39. One thing we do,
however, is to still wrap our block-Jacobi preconditioner inside
PreconditionChebyshev. That class relieves us from finding an appropriate
relaxation parameter (which would be around 0.7 in 2D and 0.5 in 3D for the
block-Jacobi smoother), and often increases smoothing efficiency a bit over
plain Jacobi smoothing in that it enables lower the time to solution when
setting the degree of the Chebyshev polynomial to one or two.

Note that the block-Jacobi smoother has an additional benefit: The fast
diagonalization method can also be interpreted as a change from the
Hermite-like polynomials underlying FE_DGQHermite to a basis where the cell
Laplacian is diagonal. Thus, it cancels the effect of the basis, and we get
the same iteration counts irrespective of whether we use FE_DGQHermite or
FE_DGQ. This is in contrast to using the PreconditionChebyshev class with only
the diagonal (a point-Jacobi scheme), where FE_DGQ and FE_DGQHermite do indeed
behave differently and FE_DGQ needs 2-5 less iterations than FE_DGQHermite,
despite the modification made to the Hermite-like shape functions to ensure a
good conditioning.


examples/step-59/doc/results.dox
<h1>Results</h1>

<h3>Program output</h3>

Like in step-37, we evaluate the multigrid solver in terms of run time.  In
two space dimensions with elements of degree 8, a possible output could look
as follows:
@code
Running with 12 MPI processes, element FE_DGQHermite<2>(8)

Cycle 0
Number of degrees of freedom: 5184
Total setup time              0.0282445 s
Time solve (14 iterations)    0.0110712 s
Verification via L2 error:    1.66232e-07

Cycle 1
Number of degrees of freedom: 20736
Total setup time              0.0126282 s
Time solve (14 iterations)    0.0157021 s
Verification via L2 error:    2.91505e-10

Cycle 2
Number of degrees of freedom: 82944
Total setup time              0.0227573 s
Time solve (14 iterations)    0.026568 s
Verification via L2 error:    6.64514e-13

Cycle 3
Number of degrees of freedom: 331776
Total setup time              0.0604685 s
Time solve (14 iterations)    0.0628356 s
Verification via L2 error:    5.57513e-13

Cycle 4
Number of degrees of freedom: 1327104
Total setup time              0.154359 s
Time solve (13 iterations)    0.219555 s
Verification via L2 error:    3.08139e-12

Cycle 5
Number of degrees of freedom: 5308416
Total setup time              0.467764 s
Time solve (13 iterations)    1.1821 s
Verification via L2 error:    3.90334e-12

Cycle 6
Number of degrees of freedom: 21233664
Total setup time              1.73263 s
Time solve (13 iterations)    5.21054 s
Verification via L2 error:    4.94543e-12
@endcode

Like in step-37, the number of CG iterations remains constant with increasing
problem size. The iteration counts are a bit higher, which is because we use a
lower degree of the Chebyshev polynomial (2 vs 5 in step-37) and because the
interior penalty discretization has a somewhat larger spread in
eigenvalues. Nonetheless, 13 iterations to reduce the residual by 12 orders of
magnitude, or almost a factor of 9 per iteration, indicates an overall very
efficient method. In particular, we can solve a system with 21 million degrees
of freedom in 5 seconds when using 12 cores, which is a very good
efficiency. Of course, in 2D we are well inside the regime of roundoff for a
polynomial degree of 8 &ndash; as a matter of fact, around 83k DoFs or 0.025s
would have been enough to fully converge this (simple) analytic solution
here.

Not much changes if we run the program in three spatial dimensions, except for
the fact that we now use do something more useful with the higher polynomial
degree and increasing mesh sizes, as the roundoff errors are only obtained at
the finest mesh. Still, it is remarkable that we can solve a 3D Laplace
problem with a wave of three periods to roundoff accuracy on a twelve-core
machine pretty easily - using about 3.5 GB of memory in total for the second
to largest case with 24m DoFs, taking not more than eight seconds. The largest
case uses 30GB of memory with 191m DoFs.

@code
Running with 12 MPI processes, element FE_DGQHermite<3>(8)

Cycle 0
Number of degrees of freedom: 5832
Total setup time              0.0210681 s
Time solve (15 iterations)    0.0956945 s
Verification via L2 error:    0.0297194

Cycle 1
Number of degrees of freedom: 46656
Total setup time              0.0452428 s
Time solve (15 iterations)    0.113827 s
Verification via L2 error:    9.55733e-05

Cycle 2
Number of degrees of freedom: 373248
Total setup time              0.190423 s
Time solve (15 iterations)    0.218309 s
Verification via L2 error:    2.6868e-07

Cycle 3
Number of degrees of freedom: 2985984
Total setup time              0.627914 s
Time solve (15 iterations)    1.0595 s
Verification via L2 error:    4.6918e-10

Cycle 4
Number of degrees of freedom: 23887872
Total setup time              2.85215 s
Time solve (15 iterations)    8.30576 s
Verification via L2 error:    9.38583e-13

Cycle 5
Number of degrees of freedom: 191102976
Total setup time              16.1324 s
Time solve (15 iterations)    65.57 s
Verification via L2 error:    3.17875e-13
@endcode

<h3>Comparison of efficiency at different polynomial degrees</h3>

In the introduction and in-code comments, it was mentioned several times that
high orders are treated very efficiently with the FEEvaluation and
FEFaceEvaluation evaluators. Now, we want to substantiate these claims by
looking at the throughput of the 3D multigrid solver for various polynomial
degrees. We collect the times as follows: We first run a solver at problem
size close to ten million, indicated in the first four table rows, and record
the timings. Then, we normalize the throughput by recording the number of
million degrees of freedom solved per second (MDoFs/s) to be able to compare
the efficiency of the different degrees, which is computed by dividing the
number of degrees of freedom by the solver time.

<table align="center" class="doxtable">
  <tr>
   <th>degree</th>
   <th>1</th>
   <th>2</th>
   <th>3</th>
   <th>4</th>
   <th>5</th>
   <th>6</th>
   <th>7</th>
   <th>8</th>
   <th>9</th>
   <th>10</th>
   <th>11</th>
   <th>12</th>
  </tr>
  <tr>
   <th>Number of DoFs</th>
   <td>2097152</td>
   <td>7077888</td>
   <td>16777216</td>
   <td>32768000</td>
   <td>7077888</td>
   <td>11239424</td>
   <td>16777216</td>
   <td>23887872</td>
   <td>32768000</td>
   <td>43614208</td>
   <td>7077888</td>
   <td>8998912</td>
  </tr>
  <tr>
   <th>Number of iterations</th>
   <td>13</td>
   <td>12</td>
   <td>12</td>
   <td>12</td>
   <td>13</td>
   <td>13</td>
   <td>15</td>
   <td>15</td>
   <td>17</td>
   <td>19</td>
   <td>18</td>
   <td>18</td>
  </tr>
  <tr>
   <th>Solver time [s]</th>
   <td>0.713</td>
   <td>2.150</td>
   <td>4.638</td>
   <td>8.803</td>
   <td>2.041</td>
   <td>3.295</td>
   <td>5.723</td>
   <td>8.306</td>
   <td>12.75</td>
   <td>19.25</td>
   <td>3.530</td>
   <td>4.814</td>
  </tr>
  <tr>
   <th>MDoFs/s</th>
   <td>2.94</td>
   <td>3.29</td>
   <td>3.62</td>
   <td>3.72</td>
   <td>3.47</td>
   <td>3.41</td>
   <td>2.93</td>
   <td>2.88</td>
   <td>2.57</td>
   <td>2.27</td>
   <td>2.01</td>
   <td>1.87</td>
  </tr>
</table>

We clearly see how the efficiency per DoF initially improves until it reaches
a maximum for the polynomial degree $k=4$. This effect is surprising, not only
because higher polynomial degrees often yield a vastly better solution, but
especially also when having matrix-based schemes in mind where the denser
coupling at higher degree leads to a monotonously decreasing throughput (and a
drastic one in 3D, with $k=4$ being more than ten times slower than
$k=1$!). For higher degrees, the throughput decreases a bit, which is both due
to an increase in the number of iterations (going from 12 at $k=2,3,4$ to 19
at $k=10$) and due to the $\mathcal O(k)$ complexity of operator
evaluation. Nonetheless, efficiency as the time to solution would be still
better for higher polynomial degrees because they have better convergence rates (at least
for problems as simple as this one): For $k=12$, we reach roundoff accuracy
already with 1 million DoFs (solver time less than a second), whereas for $k=8$
we need 24 million DoFs and 8 seconds. For $k=5$, the error is around
$10^{-9}$ with 57m DoFs and thus still far away from roundoff, despite taking 16
seconds.

Note that the above numbers are a bit pessimistic because they include the
time it takes the Chebyshev smoother to compute an eigenvalue estimate, which
is around 10 percent of the solver time. If the system is solved several times
(as e.g. common in fluid dynamics), this eigenvalue cost is only paid once and
faster times become available.

<h3>Evaluation of efficiency of ingredients</h3>

Finally, we take a look at some of the special ingredients presented in this
tutorial program, namely the FE_DGQHermite basis in particular and the
specification of MatrixFree::DataAccessOnFaces. In the following table, the
third row shows the optimized solver above, the fourth row shows the timings
with only the MatrixFree::DataAccessOnFaces set to `unspecified` rather than
the optimal `gradients`, and the last one with replacing FE_DGQHermite by the
basic FE_DGQ elements where both the MPI exchange are more expensive and the
operations done by FEFaceEvaluation::gather_evaluate() and
FEFaceEvaluation::integrate_scatter().

<table align="center" class="doxtable">
  <tr>
   <th>degree</th>
   <th>1</th>
   <th>2</th>
   <th>3</th>
   <th>4</th>
   <th>5</th>
   <th>6</th>
   <th>7</th>
   <th>8</th>
   <th>9</th>
   <th>10</th>
   <th>11</th>
   <th>12</th>
  </tr>
  <tr>
   <th>Number of DoFs</th>
   <td>2097152</td>
   <td>7077888</td>
   <td>16777216</td>
   <td>32768000</td>
   <td>7077888</td>
   <td>11239424</td>
   <td>16777216</td>
   <td>23887872</td>
   <td>32768000</td>
   <td>43614208</td>
   <td>7077888</td>
   <td>8998912</td>
  </tr>
  <tr>
   <th>Solver time optimized as in tutorial [s]</th>
   <td>0.713</td>
   <td>2.150</td>
   <td>4.638</td>
   <td>8.803</td>
   <td>2.041</td>
   <td>3.295</td>
   <td>5.723</td>
   <td>8.306</td>
   <td>12.75</td>
   <td>19.25</td>
   <td>3.530</td>
   <td>4.814</td>
  </tr>
  <tr>
   <th>Solver time MatrixFree::DataAccessOnFaces::unspecified [s]</th>
   <td>0.711</td>
   <td>2.151</td>
   <td>4.675</td>
   <td>8.968</td>
   <td>2.243</td>
   <td>3.655</td>
   <td>6.277</td>
   <td>9.082</td>
   <td>13.50</td>
   <td>20.05</td>
   <td>3.817</td>
   <td>5.178</td>
  </tr>
  <tr>
   <th>Solver time FE_DGQ [s]</th>
   <td>0.712</td>
   <td>2.041</td>
   <td>5.066</td>
   <td>9.335</td>
   <td>2.379</td>
   <td>3.802</td>
   <td>6.564</td>
   <td>9.714</td>
   <td>14.54</td>
   <td>22.76</td>
   <td>4.148</td>
   <td>5.857</td>
  </tr>
</table>

The data in the table shows that not using MatrixFree::DataAccessOnFaces
increases costs by around 10% for higher polynomial degrees. For lower
degrees, the difference is obviously less pronounced because the
volume-to-surface ratio is more beneficial and less data needs to be
exchanged. The difference is larger when looking at the matrix-vector product
only, rather than the full multigrid solver shown here, with around 20% worse
timings just because of the MPI communication.

For $k=1$ and $k=2$, the Hermite-like basis functions do obviously not really
pay off (indeed, for $k=1$ the polynomials are exactly the same as for FE_DGQ)
and the results are similar as with the FE_DGQ basis. However, for degrees
starting at three, we see an increasing advantage for FE_DGQHermite, showing
the effectiveness of these basis functions.

<h3>Possibilities for extension</h3>

As mentioned in the introduction, the fast diagonalization method is tied to a
Cartesian mesh with constant coefficients. If we wanted to solve
variable-coefficient problems, we would need to invest a bit more time in the
design of the smoother parameters by selecting proper generalizations (e.g.,
approximating the inverse on the nearest box-shaped element).

Another way of extending the program would be to include support for adaptive
meshes, for which interface operations at edges of different refinement
level become necessary, as discussed in step-39.


examples/step-6/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

@dealiiVideoLecture{15,16,17,17.25,17.5,17.75}

This program is finally about one of the main features of deal.II:
the use of adaptively (locally) refined meshes. The program is still
based on step-4 and step-5, and, as you will see, it does not actually
take very much code to enable adaptivity. Indeed, while we do a great
deal of explaining, adaptive meshes can be added to an existing program
with barely a dozen lines of additional code. The program shows what
these lines are, as well as another important ingredient of adaptive
mesh refinement (AMR): a criterion that can be used to determine whether
it is necessary to refine a cell because the error is large on it,
whether the cell can be coarsened because the error is particularly
small on it, or whether we should just leave the cell as it is. We
will discuss all of these issues in the following.


<h3> What adaptively refined meshes look like </h3>

There are a number of ways how one can adaptively refine meshes. The
basic structure of the overall algorithm is always the same and consists
of a loop over the following steps:
- Solve the PDE on the current mesh;
- Estimate the error on each cell using some criterion that is indicative
  of the error;
- Mark those cells that have large errors for refinement, mark those that have
  particularly small errors for coarsening, and leave the rest alone;
- Refine and coarsen the cells so marked to obtain a new mesh;
- Repeat the steps above on the new mesh until the overall error is
  sufficiently small.

For reasons that are probably lost to history (maybe that these functions
used to be implemented in FORTRAN, a language that does not care about
whether something is spelled in lower or UPPER case letters, with programmers
often choosing upper case letters habitually), the loop above is often
referenced in publications about mesh adaptivity as the
SOLVE-ESTIMATE-MARK-REFINE loop (with this spelling).

Beyond this structure, however, there are a variety of ways to achieve
this. Fundamentally, they differ in how exactly one generates one mesh
from the previous one.

If one were to use triangles (which deal.II does not do), then there are
two essential possibilities:
- Longest-edge refinement: In this strategy, a triangle marked for refinement
  is cut into two by introducing one new edge from the midpoint of the longest
  edge to the opposite vertex. Of course, the midpoint from the longest edge
  has to somehow be balanced by *also* refining the cell on the other side of
  that edge (if there is one). If the edge in question is also the longest
  edge of the neighboring cell, then we can just run a new edge through the
  neighbor to the opposite vertex; otherwise a slightly more involved
  construction is necessary that adds more new vertices on at least one
  other edge of the neighboring cell, and then may propagate to the neighbors
  of the neighbor until the algorithm terminates. This is hard to describe
  in words, and because deal.II does not use triangles not worth the time here.
  But if you're curious, you can always watch video lecture 15 at the link
  shown at the top of this introduction.
- Red-green refinement: An alternative is what is called "red-green refinement".
  This strategy is even more difficult to describe (but also discussed in the
  video lecture) and has the advantage that the refinement does not propagate
  beyond the immediate neighbors of the cell that we want to refine. It is,
  however, substantially more difficult to implement.

There are other variations of these approaches, but the important point is
that they always generate a mesh where the lines where two cells touch
are entire edges of both adjacent cells. With a bit of work, this strategy
is readily adapted to three-dimensional meshes made from tetrahedra.

Neither of these methods works for quadrilaterals in 2d and hexahedra in 3d,
or at least not easily. The reason is that the transition elements created
out of the quadrilateral neighbors of a quadrilateral cell that is to be refined
would be triangles, and we don't want this. Consequently,
the approach to adaptivity chosen in deal.II is to use grids in which
neighboring cells may differ in refinement level by one. This then
results in nodes on the interfaces of cells which belong to one
side, but are unbalanced on the other. The common term for these is
&ldquo;hanging nodes&rdquo;, and these meshes then look like this in a very
simple situation:

@image html hanging_nodes.png "A simple mesh with hanging nodes"

A more complicated two-dimensional mesh would look like this (and is
discussed in the "Results" section below):

<img src="https://www.dealii.org/images/steps/developer/step_6_grid_5_ladutenko.svg"
     alt="Fifth adaptively refined Ladutenko grid: the cells are clustered
          along the inner circle."
     width="300" height="300">

Finally, a three-dimensional mesh (from step-43) with such hanging nodes is shown here:

<img src="https://www.dealii.org/images/steps/developer/step-43.3d.mesh.png" alt=""
     width="300" height="300">

The first and third mesh are of course based on a square and a cube, but as the
second mesh shows, this is not necessary. The important point is simply that we
can refine a mesh independently of its neighbors (subject to the constraint
that a cell can be only refined once more than its neighbors), but that we end
up with these &ldquo;hanging nodes&rdquo; if we do this.


<h3> Why adapatively refined meshes? </h3>

Now that you have seen what these adaptively refined meshes look like,
you should ask <i>why</i> we would want to do this. After all, we know from
theory that if we refine the mesh globally, the error will go down to zero
as
@f{align*}{
  \|\nabla(u-u_h)\|_{\Omega} \le C h_\text{max}^p \| \nabla^{p+1} u \|_{\Omega},
@f}
where $C$ is some constant independent of $h$ and $u$,
$p$ is the polynomial degree of the finite element in use, and
$h_\text{max}$ is the diameter of the largest cell. So if the
<i>largest</i> cell is important, then why would we want to make
the mesh fine in some parts of the domain but not all?

The answer lies in the observation that the formula above is not
optimal. In fact, some more work shows that the following
is a better estimate (which you should compare to the square of
the estimate above):
@f{align*}{
  \|\nabla(u-u_h)\|_{\Omega}^2 \le C \sum_K h_K^{2p} \| \nabla^{p+1} u \|^2_K.
@f}
(Because $h_K\le h_\text{max}$, this formula immediately implies the
previous one if you just pull the mesh size out of the sum.)
What this formula suggests is that it is not necessary to make
the <i>largest</i> cell small, but that the cells really only
need to be small <i>where $\| \nabla^{p+1} u \|_K$ is large</i>!
In other words: The mesh really only has to be fine where the
solution has large variations, as indicated by the $p+1$st derivative.
This makes intuitive sense: if, for example, we use a linear element
$p=1$, then places where the solution is nearly linear (as indicated
by $\nabla^2 u$ being small) will be well resolved even if the mesh
is coarse. Only those places where the second derivative is large
will be poorly resolved by large elements, and consequently
that's where we should make the mesh small.

Of course, this <i>a priori estimate</i> is not very useful
in practice since we don't know the exact solution $u$ of the
problem, and consequently, we cannot compute $\nabla^{p+1}u$.
But, and that is the approach commonly taken, we can compute
numerical approximations of $\nabla^{p+1}u$ based only on
the discrete solution $u_h$ that we have computed before. We
will discuss this in slightly more detail below. This will then
help us determine which cells have a large $p+1$st derivative,
and these are then candidates for refining the mesh.


<h3> How to deal with hanging nodes in theory </h3>

The methods using triangular meshes mentioned above go to great
lengths to make sure that each vertex is a vertex of all adjacent
cells -- i.e., that there are no hanging nodes. This then
automatically makes sure that we can define shape functions in such a
way that they are globally continuous (if we use the common $Q_p$
Lagrange finite element methods we have been using so far in the
tutorial programs, as represented by the FE_Q class).

On the other hand, if we define shape functions on meshes with hanging
nodes, we may end up with shape functions that are not continuous. To
see this, think about the situation above where the top right cell is
not refined, and consider for a moment the use of a bilinear finite
element. In that case, the shape functions associated with the hanging
nodes are defined in the obvious way on the two small cells adjacent
to each of the hanging nodes. But how do we extend them to the big
adjacent cells? Clearly, the function's extension to the big cell
cannot be bilinear because then it needs to be linear along each edge
of the large cell, and that means that it needs to be zero on the
entire edge because it needs to be zero on the two vertices of the
large cell on that edge. But it is not zero at the hanging node itself
when seen from the small cells' side -- so it is not continuous. The
following three figures show three of the shape functions along the
edges in question that turn out to not be continuous when defined in
the usual way simply based on the cells they are adjacent to:

<div class="threecolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      @image html hanging_nodes_shape_functions_1.png "A discontinuous shape function adjacent to a hanging node"
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      @image html hanging_nodes_shape_functions_2.png "A discontinuous shape function at a hanging node"
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      @image html hanging_nodes_shape_functions_3.png "A discontinuous shape function adjacent to a hanging node"
    </div>
  </div>
</div>


But we do want the finite element solution to be continuous so that we
have a &ldquo;conforming finite element method&rdquo; where the
discrete finite element space is a proper subset of the $H^1$ function
space in which we seek the solution of the Laplace equation.
To guarantee that the global solution is continuous at these nodes as well, we
have to state some additional constraints on the values of the solution at
these nodes. The trick is to realize that while the shape functions shown
above are discontinuous (and consequently an <i>arbitrary</i> linear combination
of them is also discontinuous), that linear combinations in which the shape
functions are added up as $u_h(\mathbf x)=\sum_j U_j \varphi_j(\mathbf x)$
can be continuous <i>if the coefficients $U_j$ satisfy certain relationships</i>.
In other words, the coefficients $U_j$ can not be chosen arbitrarily
but have to satisfy certain constraints so that the function $u_h$ is in fact
continuous.
What these constraints have to look is relatively easy to
understand conceptually, but the implementation in software is
complicated and takes several thousand lines of code. On the other
hand, in user code, it is only about half a dozen lines you have to
add when dealing with hanging nodes.

In the program below, we will show how we can get these
constraints from deal.II, and how to use them in the solution of the
linear system of equations. Before going over the details of the program
below, you may want to take a look at the @ref constraints documentation
module that explains how these constraints can be computed and what classes in
deal.II work on them.


<h3> How to deal with hanging nodes in practice </h3>

The practice of hanging node constraints is rather simpler than the
theory we have outlined above. In reality, you will really only have to
add about half a dozen lines of additional code to a program like step-4
to make it work with adaptive meshes that have hanging nodes. The
interesting part about this is that it is entirely independent of the
equation you are solving: The algebraic nature of these constraints has nothing
to do with the equation and only depends on the choice of finite element.
As a consequence, the code to deal with these constraints is entirely
contained in the deal.II library itself, and you do not need to worry
about the details.

The steps you need to make this work are essentially like this:
- You have to create an AffineConstraints object, which (as the name
  suggests) will store all constraints on the finite element space. In
  the current context, these are the constraints due to our desire to
  keep the solution space continuous even in the presence of hanging
  nodes. (Below we will also briefly mention that we will also put
  boundary values into this same object, but that is a separate matter.)
- You have to fill this object using the function
  DoFTools::make_hanging_node_constraints() to ensure continuity of
  the elements of the finite element space.
- You have to use this object when you copy the local contributions to
  the matrix and right hand side into the global objects, by using
  AffineConstraints::distribute_local_to_global(). Up until
  now, we have done this ourselves, but now with constraints, this
  is where the magic happens and we apply the constraints to the
  linear system. What this function does is make sure that the
  degrees of freedom located at hanging nodes are not, in fact,
  really free. Rather, they are factually eliminated from the
  linear system by setting their rows and columns to zero and putting
  something on the diagonal to ensure the matrix remains invertible.
  The matrix resulting from this process remains symmetric and
  positive definite for the Laplace equation we solve here, so we can
  continue to use the Conjugate Gradient method for it.
- You then solve the linear system as usual, but at the end of this
  step, you need to make sure that the degrees of "freedom" located
  on hanging nodes get their correct (constrained) value so that the
  solution you then visualize or evaluate in other ways is in
  fact continuous. This is done by calling
  AffineConstraints::distribute() immediately after solving.

These four steps are really all that is necessary -- it's that simple
from a user perspective. The fact that, in the function calls mentioned
above, you will run through several thousand lines of not-so-trivial
code is entirely immaterial to this: In user code, there are really
only four additional steps.


<h3> How we obtain locally refined meshes </h3>

The next question, now that we know how to <i>deal</i> with meshes that
have these hanging nodes is how we <i>obtain</i> them.

A simple way has already been shown in step-1: If you <i>know</i> where
it is necessary to refine the mesh, then you can create one by hand. But
in reality, we don't know this: We don't know the solution of the PDE
up front (because, if we did, we wouldn't have to use the finite element
method), and consequently we do not know where it is necessary to
add local mesh refinement to better resolve areas where the solution
has strong variations. But the discussion above shows that maybe we
can get away with using the discrete solution $u_h$ on one mesh to
estimate the derivatives $\nabla^{p+1} u$, and then use this to determine
which cells are too large and which already small enough. We can then
generate a new mesh from the current one using local mesh refinement.
If necessary, this step is then repeated until we are happy with our
numerical solution -- or, more commonly, until we run out of computational
resources or patience.

So that's exactly what we will do.
The locally refined grids are produced using an <i>error estimator</i>
which estimates the energy error for numerical solutions of the Laplace
operator. Since it was developed by Kelly and
co-workers, we often refer to it as the &ldquo;Kelly refinement
indicator&rdquo; in the library, documentation, and mailing list. The
class that implements it is called
KellyErrorEstimator, and there is a great deal of information to
be found in the documentation of that class that need not be repeated
here. The summary, however, is that the class computes a vector with
as many entries as there are @ref GlossActive "active cells", and
where each entry contains an estimate of the error on that cell.
This estimate is then used to refine the cells of the mesh: those
cells that have a large error will be marked for refinement, those
that have a particularly small estimate will be marked for
coarsening. We don't have to do this by hand: The functions in
namespace GridRefinement will do all of this for us once we have
obtained the vector of error estimates.

It is worth noting that while the Kelly error estimator was developed
for Laplace's equation, it has proven to be a suitable tool to generate
locally refined meshes for a wide range of equations, not even restricted
to elliptic only problems. Although it will create non-optimal meshes for other
equations, it is often a good way to quickly produce meshes that are
well adapted to the features of solutions, such as regions of great
variation or discontinuities.



<h3> Boundary conditions </h3>

It turns out that one can see Dirichlet boundary conditions as just another
constraint on the degrees of freedom. It's a particularly simple one,
indeed: If $j$ is a degree of freedom on the boundary, with position
$\mathbf x_j$, then imposing the boundary condition $u=g$ on $\partial\Omega$
simply yields the constraint $U_j=g({\mathbf x}_j)$.

The AffineConstraints class can handle such constraints as well, which makes it
convenient to let the same object we use for hanging node constraints
also deal with these Dirichlet boundary conditions.
This way, we don't need to apply the boundary conditions after assembly
(like we did in the earlier steps).
All that is necessary is that we call the variant of
VectorTools::interpolate_boundary_values() that returns its information
in an AffineConstraints object, rather than the `std::map` we have used
in previous tutorial programs.


 <h3> Other things this program shows </h3>


Since the concepts used for locally refined grids are so important,
we do not show much other material in this example. The most
important exception is that we show how to use biquadratic elements
instead of the bilinear ones which we have used in all previous
examples. In fact, the use of higher order elements is accomplished by
only replacing three lines of the program, namely the initialization of
the <code>fe</code> member variable in the constructor of the main
class of this program, and the use of an appropriate quadrature formula
in two places. The rest of the program is unchanged.

The only other new thing is a method to catch exceptions in the
<code>main</code> function in order to output some information in case the
program crashes for some reason. This is discussed below in more detail.


examples/step-6/doc/results.dox
<h1>Results</h1>


The output of the program looks as follows:
@code
Cycle 0:
   Number of active cells:       20
   Number of degrees of freedom: 89
Cycle 1:
   Number of active cells:       44
   Number of degrees of freedom: 209
Cycle 2:
   Number of active cells:       92
   Number of degrees of freedom: 449
Cycle 3:
   Number of active cells:       200
   Number of degrees of freedom: 921
Cycle 4:
   Number of active cells:       440
   Number of degrees of freedom: 2017
Cycle 5:
   Number of active cells:       956
   Number of degrees of freedom: 4425
Cycle 6:
   Number of active cells:       1916
   Number of degrees of freedom: 8993
Cycle 7:
   Number of active cells:       3860
   Number of degrees of freedom: 18353
@endcode



As intended, the number of cells roughly doubles in each cycle. The
number of degrees is slightly more than four times the number of
cells; one would expect a factor of exactly four in two spatial
dimensions on an infinite grid (since the spacing between the degrees
of freedom is half the cell width: one additional degree of freedom on
each edge and one in the middle of each cell), but it is larger than
that factor due to the finite size of the mesh and due to additional
degrees of freedom which are introduced by hanging nodes and local
refinement.



The program outputs the solution and mesh in each cycle of the
refinement loop. The solution looks as follows:

<img src="https://www.dealii.org/images/steps/developer/step-6.solution.9.2.png" alt="">

It is interesting to follow how the program arrives at the final mesh:

<div class="twocolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_0.svg"
         alt="Initial grid: the five-cell circle grid with one global refinement."
         width="300" height="300">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_1.svg"
         alt="First grid: the five-cell circle grid with two global refinements."
         width="300" height="300">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_2.svg"
         alt="Second grid: the five-cell circle grid with one adaptive refinement."
         width="300" height="300">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_3.svg"
         alt="Third grid: the five-cell circle grid with two adaptive
         refinements, showing clustering around the inner circle."
         width="300" height="300">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_4.svg"
         alt="Fourth grid: the five-cell circle grid with three adaptive
         refinements, showing clustering around the inner circle."
         width="300" height="300">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_5.svg"
         alt="Fifth grid: the five-cell circle grid with four adaptive
         refinements, showing clustering around the inner circle."
         width="300" height="300">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_6.svg"
         alt="Sixth grid: the five-cell circle grid with five adaptive
         refinements, showing clustering around the inner circle."
         width="300" height="300">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_7.svg"
         alt="Last grid: the five-cell circle grid with six adaptive
         refinements, showing that most cells are clustered around the inner circle."
         width="300" height="300">
  </div>
</div>


It is clearly visible that the region where the solution has a kink,
i.e. the circle at radial distance 0.5 from the center, is
refined most. Furthermore, the central region where the solution is
very smooth and almost flat, is almost not refined at all, but this
results from the fact that we did not take into account that the
coefficient is large there. The region outside is refined rather
arbitrarily, since the second derivative is constant there and refinement
is therefore mostly based on the size of the cells and their deviation
from the optimal square.



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

<h4>Solvers and preconditioners</h4>


One thing that is always worth playing around with if one solves
problems of appreciable size (much bigger than the one we have here)
is to try different solvers or preconditioners. In the current case,
the linear system is symmetric and positive definite, which makes the
CG algorithm pretty much the canonical choice for solving. However,
the SSOR preconditioner we use in the <code>solve()</code> function is
up for grabs.

In deal.II, it is relatively simple to change the preconditioner. For
example, by changing the existing lines of code
@code
  PreconditionSSOR<SparseMatrix<double>> preconditioner;
  preconditioner.initialize(system_matrix, 1.2);
@endcode
into
@code
  PreconditionSSOR<SparseMatrix<double>> preconditioner;
  preconditioner.initialize(system_matrix, 1.0);
@endcode
we can try out different relaxation parameters for SSOR. By using
@code
  PreconditionJacobi<SparseMatrix<double>> preconditioner;
  preconditioner.initialize(system_matrix);
@endcode
we can use Jacobi as a preconditioner. And by using
@code
  SparseILU<double> preconditioner;
  preconditioner.initialize(system_matrix);
@endcode
we can use a simple incomplete LU decomposition without any thresholding or
strengthening of the diagonal (to use this preconditioner, you have to also
add the header file <code>deal.II/lac/sparse_ilu.h</code> to the include list
at the top of the file).

Using these various different preconditioners, we can compare the
number of CG iterations needed (available through the
<code>solver_control.last_step()</code> call, see
step-4) as well as CPU time needed (using the Timer class,
discussed, for example, in step-28) and get the
following results (left: iterations; right: CPU time):

<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-6.q2.dofs_vs_iterations.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-6.q2.dofs_vs_time.png" alt="">
    </td>
  </tr>
</table>

As we can see, all preconditioners behave pretty much the same on this
simple problem, with the number of iterations growing like ${\cal
O}(N^{1/2})$ and because each iteration requires around ${\cal
O}(N)$ operations the total CPU time grows like ${\cal
O}(N^{3/2})$ (for the few smallest meshes, the CPU time is so small
that it doesn't record). Note that even though it is the simplest
method, Jacobi is the fastest for this problem.

The situation changes slightly when the finite element is not a
bi-quadratic one as set in the constructor of this program, but a
bi-linear one. If one makes this change, the results are as follows:

<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-6.q1.dofs_vs_iterations.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-6.q1.dofs_vs_time.png" alt="">
    </td>
  </tr>
</table>

In other words, while the increase in iterations and CPU time is as
before, Jacobi is now the method that requires the most iterations; it
is still the fastest one, however, owing to the simplicity of the
operations it has to perform. This is not to say that Jacobi
is actually a good preconditioner -- for problems of appreciable size, it is
definitely not, and other methods will be substantially better -- but really
only that it is fast because its implementation is so simple that it can
compensate for a larger number of iterations.

The message to take away from this is not that simplicity in
preconditioners is always best. While this may be true for the current
problem, it definitely is not once we move to more complicated
problems (elasticity or Stokes, for examples step-8 or
step-22). Secondly, all of these preconditioners still
lead to an increase in the number of iterations as the number $N$ of
degrees of freedom grows, for example ${\cal O}(N^\alpha)$; this, in
turn, leads to a total growth in effort as ${\cal O}(N^{1+\alpha})$
since each iteration takes ${\cal O}(N)$ work. This behavior is
undesirable: we would really like to solve linear systems with $N$
unknowns in a total of ${\cal O}(N)$ work; there is a class
of preconditioners that can achieve this, namely geometric (step-16,
step-37, step-39)
or algebraic multigrid (step-31, step-40, and several others)
preconditioners. They are, however, significantly more complex than
the preconditioners outlined above.

Finally, the last message to take
home is that when the data shown above was generated (in 2018), linear
systems with 100,000 unknowns are
easily solved on a desktop machine in about a second, making
the solution of relatively simple 2d problems even to very high
accuracy not that big a task as it used to be even in the
past. At the time, the situation for 3d problems was entirely different,
but even that has changed substantially in the intervening time -- though
solving problems in 3d to high accuracy remains a challenge.


<h4>A better mesh</h4>

If you look at the meshes above, you will see even though the domain is the
unit disk, and the jump in the coefficient lies along a circle, the cells
that make up the mesh do not track this geometry well. The reason, already hinted
at in step-1, is that in the absence of other information,
the Triangulation class only sees a bunch of
coarse grid cells but has, of course, no real idea what kind of geometry they
might represent when looked at together. For this reason, we need to tell
the Triangulation what to do when a cell is refined: where should the new
vertices at the edge midpoints and the cell midpoint be located so that the
child cells better represent the desired geometry than the parent cell.

To visualize what the triangulation actually knows about the geometry,
it is not enough to just output the location of vertices and draw a
straight line for each edge; instead, we have to output both interior
and boundary lines as multiple segments so that they look
curved. We can do this by making one change to the gnuplot part of
<code>output_results</code>:
@code
{
  GridOut       grid_out;
  std::ofstream output("grid-" + std::to_string(cycle) + ".gnuplot");
  GridOutFlags::Gnuplot gnuplot_flags(false, 5, /*curved_interior_cells*/true);
  grid_out.set_flags(gnuplot_flags);
  MappingQGeneric<dim> mapping(3);
  grid_out.write_gnuplot(triangulation, output, &mapping);
}
@endcode

In the code above, we already do this for faces that sit at the boundary: this
happens automatically since we use GridGenerator::hyper_ball, which attaches a
SphericalManifold to the boundary of the domain. To make the mesh
<i>interior</i> also track a circular domain, we need to work a bit harder,
though. First, recall that our coarse mesh consists of a central square
cell and four cells around it. Now first consider what would happen if we
also attached the SphericalManifold object not only to the four exterior faces
but also the four cells at the perimeter as well as all of their faces. We can
do this by adding the following snippet (testing that the center of a cell is
larger than a small multiple, say one tenth, of the cell diameter away from
center of the mesh only fails for the central square of the mesh):
@code
GridGenerator::hyper_ball(triangulation);
// after GridGenerator::hyper_ball is called the Triangulation has
// a SphericalManifold with id 0. We can use it again on the interior.
const Point<dim> mesh_center;
for (const auto &cell : triangulation.active_cell_iterators())
  if (mesh_center.distance (cell->center()) > cell->diameter()/10)
    cell->set_all_manifold_ids(0);

triangulation.refine_global(1);
@endcode

After a few global refinement steps, this would lead to a mesh of the following
kind:


  <div class="onecolumn" style="width: 80%">
    <div>
      <img src="https://www.dealii.org/images/steps/developer/step_6_bad_grid_4.svg"
           alt="Grid where some central cells are nearly triangular."
           width="300" height="300">
    </div>
  </div>

This is not a good mesh: the central cell has been refined in such a way that
the children located in the four corners of the original central cell
<i>degenerate</i>: they all tend towards triangles as mesh refinement
continues. This means that the Jacobian matrix of the transformation from
reference cell to actual cell degenerates for these cells, and because
all error estimates for finite element solutions contain the norm of the
inverse of the Jacobian matrix, you will get very large errors on these
cells and, in the limit as mesh refinement, a loss of convergence order because
the cells in these corners become worse and worse under mesh refinement.

So we need something smarter. To this end, consider the following solution
originally developed by Konstantin Ladutenko. We will use the following code:
@code
GridGenerator::hyper_ball(triangulation);

const Point<dim> mesh_center;
const double core_radius  = 1.0/5.0,
             inner_radius = 1.0/3.0;

// Step 1: Shrink the inner cell
//
// We cannot get a circle out of the inner cell because of
// the degeneration problem mentioned above. Rather, shrink
// the inner cell to a core radius of 1/5 that stays
// sufficiently far away from the place where the
// coefficient will have a discontinuity and where we want
// to have cell interfaces that actually lie on a circle.
// We do this shrinking by just scaling the location of each
// of the vertices, given that the center of the circle is
// simply the origin of the coordinate system.
for (const auto &cell : triangulation.active_cell_iterators())
  if (mesh_center.distance(cell->center()) < 1e-5)
    {
      for (const auto v : cell->vertex_indices())
        cell->vertex(v) *= core_radius/mesh_center.distance(cell->vertex(v));
    }

// Step 2: Refine all cells except the central one
for (const auto &cell : triangulation.active_cell_iterators())
  if (mesh_center.distance(cell->center()) >= 1e-5)
    cell->set_refine_flag();
triangulation.execute_coarsening_and_refinement();

// Step 3: Resize the inner children of the outer cells
//
// The previous step replaced each of the four outer cells
// by its four children, but the radial distance at which we
// have intersected is not what we want to later refinement
// steps. Consequently, move the vertices that were just
// created in radial direction to a place where we need
// them.
for (const auto &cell : triangulation.active_cell_iterators())
  for (const auto v : cell->vertex_indices())
    {
      const double dist = mesh_center.distance(cell->vertex(v));
      if (dist > core_radius*1.0001 && dist < 0.9999)
        cell->vertex(v) *= inner_radius/dist;
    }

// Step 4: Apply curved manifold description
//
// As discussed above, we can not expect to subdivide the
// inner four cells (or their faces) onto concentric rings,
// but we can do so for all other cells that are located
// outside the inner radius. To this end, we loop over all
// cells and determine whether it is in this zone. If it
// isn't, then we set the manifold description of the cell
// and all of its bounding faces to the one that describes
// the spherical manifold already introduced above and that
// will be used for all further mesh refinement.
for (const auto &cell : triangulation.active_cell_iterators())
  {
    bool is_in_inner_circle = false;
    for (const auto v : cell->vertex_indices())
      if (mesh_center.distance(cell->vertex(v)) < inner_radius)
        {
          is_in_inner_circle = true;
          break;
        }

    if (is_in_inner_circle == false)
    // The Triangulation already has a SphericalManifold with
    // manifold id 0 (see the documentation of
    // GridGenerator::hyper_ball) so we just attach it to the outer
    // ring here:
      cell->set_all_manifold_ids(0);
  }
@endcode

This code then generates the following, much better sequence of meshes:

<div class="twocolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_0_ladutenko.svg"
         alt="Initial grid: the Ladutenko grid with one global refinement."
         width="300" height="300">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_1_ladutenko.svg"
         alt="First adaptively refined Ladutenko grid."
         width="300" height="300">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_2_ladutenko.svg"
         alt="Second adaptively refined Ladutenko grid."
         width="300" height="300">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_3_ladutenko.svg"
         alt="Third adaptively refined Ladutenko grid."
         width="300" height="300">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_4_ladutenko.svg"
         alt="Fourth adaptively refined Ladutenko grid. The cells are clustered
         along the inner circle."
         width="300" height="300">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step_6_grid_5_ladutenko.svg"
         alt="Fifth adaptively refined Ladutenko grid: the cells are clustered
         along the inner circle."
         width="300" height="300">
  </div>
</div>

Creating good meshes, and in particular making them fit the geometry you
want, is a complex topic in itself. You can find much more on this in
step-49, step-53, and step-54, among other tutorial programs that cover
the issue. step-65 shows another, less manual way to achieve a mesh
well fit to the problem here.
Information on curved domains can also be found in the
documentation module on @ref manifold "Manifold descriptions".

Why does it make sense to choose a mesh that tracks the internal
interface? There are a number of reasons, but the most essential one
comes down to what we actually integrate in our bilinear
form. Conceptually, we want to integrate the term $A_{ij}^K=\int_K
a(\mathbf x) \nabla \varphi_i(\mathbf x) \nabla \varphi_j(\mathbf x) ; dx$ as the
contribution of cell $K$ to the matrix entry $A_{ij}$. We can not
compute it exactly and have to resort to quadrature. We know that
quadrature is accurate if the integrand is smooth. That is because
quadrature in essence computes a polynomial approximation to the
integrand that coincides with the integrand in the quadrature points,
and then computes the volume under this polynomial as an approximation
to the volume under the original integrand. This polynomial
interpolant is accurate if the integrand is smooth on a cell, but it
is usually rather inaccurate if the integrand is discontinuous on a
cell.

Consequently, it is worthwhile to align cells in such a way that the
interfaces across which the coefficient is discontinuous are aligned
with cell interfaces. This way, the coefficient is constant on each
cell, following which the integrand will be smooth, and its polynomial
approximation and the quadrature approximation of the integral will
both be accurate. Note that such an alignment is common in many
practical cases, so deal.II provides a number of functions (such as
@ref GlossMaterialId "material_id") to help manage such a scenario.
Refer to step-28 and step-46 for examples of how material ids can be
applied.

Finally, let us consider the case of a coefficient that has a smooth
and non-uniform distribution in space. We can repeat once again all of
the above discussion on the representation of such a function with the
quadrature. So, to simulate it accurately there are a few readily
available options: you could reduce the cell size, increase the order
of the polynomial used in the quadrature formula, select a more
appropriate quadrature formula, or perform a combination of these
steps. The key is that providing the best fit of the coefficient's
spatial dependence with the quadrature polynomial will lead to a more
accurate finite element solution of the PDE.

As a final note: The discussion in the previous paragraphs shows, we here
have a very concrete way of stating what we think of a good mesh -- it should
be aligned with the jump in the coefficient. But one could also have asked
this kind of question in a more general setting: Given some equation with
a smooth solution and smooth coefficients, can we say what a good mesh
would look like? This is a question for which the answer is easier to state
in intuitive terms than mathematically: A good mesh has cells that all,
by and large, look like squares (or cubes, in 3d). A bad mesh would contain
cells that are very elongated in some directions or, more generally, for which
there are cells that have both short and long edges. There are many ways
in which one could assign a numerical quality index to each cell that measures
whether the cell is "good" or "bad"; some of these are often chosen because
they are cheap and easy to compute, whereas others are based on what enters
into proofs of convergence. An example of the former would be the ratio of
the longest to the shortest edge of a cell: In the ideal case, that ratio
would be one; bad cells have values much larger than one. An example of the
latter kind would consider the gradient (the "Jacobian") of the mapping
from the reference cell $\hat K=[0,1]^d$ to the real cell $K$; this
gradient is a matrix, and a quantity that enters into error estimates
is the maximum over all points on the reference cell of the ratio of the
largest to the smallest eigenvalue of this matrix. It is again not difficult
to see that this ratio is constant if the cell $K$ is an affine image of
$\hat K$, and that it is one for squares and cubes.

In practice, it might be interesting to visualize such quality measures.
The function GridTools::compute_aspect_ratio_of_cells() provides one
way to get this kind of information. Even better, visualization tools
such as VisIt often allow you to visualize this sort of information
for a variety of measures from within the visualization software
itself; in the case of VisIt, just add a "pseudo-color" plot and select
one of the mesh quality measures instead of the solution field.


<h4>Playing with the regularity of the solution</h4>

From a mathematical perspective, solutions of the Laplace equation
@f[
  -\Delta u = f
@f]
on smoothly bounded, convex domains are known to be smooth themselves. The exact degree
of smoothness, i.e., the function space in which the solution lives, depends
on how smooth exactly the boundary of the domain is, and how smooth the right
hand side is. Some regularity of the solution may be lost at the boundary, but
one generally has that the solution is twice more differentiable in
compact subsets of the domain than the right hand side.
If, in particular, the right hand side satisfies $f\in C^\infty(\Omega)$, then
$u \in C^\infty(\Omega_i)$ where $\Omega_i$ is any compact subset of $\Omega$
($\Omega$ is an open domain, so a compact subset needs to keep a positive distance
from $\partial\Omega$).

The situation we chose for the current example is different, however: we look
at an equation with a non-constant coefficient $a(\mathbf x)$:
@f[
  -\nabla \cdot (a \nabla u) = f.
@f]
Here, if $a$ is not smooth, then the solution will not be smooth either,
regardless of $f$. In particular, we expect that wherever $a$ is discontinuous
along a line (or along a plane in 3d),
the solution will have a kink. This is easy to see: if for example $f$
is continuous, then $f=-\nabla \cdot (a \nabla u)$ needs to be
continuous. This means that $a \nabla u$ must be continuously differentiable
(not have a kink). Consequently, if $a$ has a discontinuity, then $\nabla u$
must have an opposite discontinuity so that the two exactly cancel and their
product yields a function without a discontinuity. But for $\nabla u$ to have
a discontinuity, $u$ must have a kink. This is of course exactly what is
happening in the current example, and easy to observe in the pictures of the
solution.

In general, if the coefficient $a(\mathbf x)$ is discontinuous along a line in 2d,
or a plane in 3d, then the solution may have a kink, but the gradient of the
solution will not go to infinity. That means, that the solution is at least
still in the <a href="https://en.wikipedia.org/wiki/Sobolev_space">Sobolev space</a>
$W^{1,\infty}$ (i.e., roughly speaking, in the
space of functions whose derivatives are bounded). On the other hand,
we know that in the most
extreme cases -- i.e., where the domain has reentrant corners, the
right hand side only satisfies $f\in H^{-1}$, or the coefficient $a$ is only in
$L^\infty$ -- all we can expect is that $u\in H^1$ (i.e., the
<a
href="https://en.wikipedia.org/wiki/Sobolev_space#Sobolev_spaces_with_integer_k">Sobolev
space</a> of functions whose derivative is square integrable), a much larger space than
$W^{1,\infty}$. It is not very difficult to create cases where
the solution is in a space $H^{1+s}$ where we can get $s$ to become as small
as we want. Such cases are often used to test adaptive finite element
methods because the mesh will have to resolve the singularity that causes
the solution to not be in $W^{1,\infty}$ any more.

The typical example one uses for this is called the <i>Kellogg problem</i>
(referring to @cite Kel74), which in the commonly used form has a coefficient
$a(\mathbf x)$ that has different values in the four quadrants of the plane
(or eight different values in the octants of ${\mathbb R}^3$). The exact degree
of regularity (the $s$ in the index of the Sobolev space above) depends on the
values of $a(\mathbf x)$ coming together at the origin, and by choosing the
jumps large enough, the regularity of the solution can be made as close as
desired to $H^1$.

To implement something like this, one could replace the coefficient
function by the following (shown here only for the 2d case):
@code
template <int dim>
double coefficient (const Point<dim> &p)
{
  if ((p[0] < 0) && (p[1] < 0))           // lower left quadrant
    return 1;
  else if ((p[0] >= 0) && (p[1] < 0))     // lower right quadrant
    return 10;
  else if ((p[0] < 0) && (p[1] >= 0))     // upper left quadrant
    return 100;
  else if ((p[0] >= 0) && (p[1] >= 0))    // upper right quadrant
    return 1000;
  else
    {
      Assert(false, ExcInternalError());
      return 0;
    }
}
@endcode
(Adding the <code>Assert</code> at the end ensures that either an exception
is thrown or that the program aborts if we ever get to that point
-- which of course we shouldn't,
but this is a good way to insure yourself: we all make mistakes by
sometimes not thinking of all cases, for example by checking
for <code>p[0]</code> to be less than and greater than zero,
rather than greater-or-equal to zero, and thereby forgetting
some cases that would otherwise lead to bugs that are awkward
to find. The <code>return 0;</code> at the end is only there to
avoid compiler warnings that the function does not end in a
<code>return</code> statement -- the compiler cannot see that the
function would never actually get to that point because of the
preceding <code>Assert</code> statement.)

By playing with such cases where four or more sectors come
together and on which the coefficient has different values, one can
construct cases where the solution has singularities at the
origin. One can also see how the meshes are refined in such cases.


examples/step-60/doc/intro.dox
<br>

<i>This program was contributed by Luca Heltai and Giovanni Alzetta, SISSA, Trieste.
</i>

@dealiiTutorialDOI{10.5281/zenodo.1243280,https://zenodo.org/badge/DOI/10.5281/zenodo.1243280.svg}


<h1>Introduction</h1>

<h3>Non-matching grid constraints through distributed Lagrange multipliers</h3>


In this tutorial we consider the case of two domains, $\Omega$ in
$R^{\text{spacedim}}$ and $\Gamma$ in $R^{\text{dim}}$, where $\Gamma$ is
embedded in $\Omega$ ($\Gamma \subseteq \Omega$). We want to solve a partial
differential equation on $\Omega$, enforcing some conditions on the solution of
the problem *on the embedded domain* $\Gamma$.

There are two interesting scenarios:

- the geometrical dimension `dim` of the embedded domain $\Gamma$ is the same of
the domain $\Omega$ (`spacedim`), that is, the spacedim-dimensional measure of
$\Gamma$ is not zero, or

- the embedded domain $\Gamma$ has an intrinsic dimension `dim` which is smaller
than that of $\Omega$ (`spacedim`), thus its spacedim-dimensional measure is
zero; for example it is a curve embedded in a two dimensional domain, or a
surface embedded in a three-dimensional domain.

In both cases define the restriction operator $\gamma$ as the operator that,
given a continuous function on $\Omega$, returns its (continuous) restriction on
$\Gamma$, i.e.,

\f[
\gamma : C^0(\Omega) \mapsto C^0(\Gamma), \quad \text{ s.t. } \gamma u = u|_{\Gamma} \in C^0(\Gamma),
\quad \forall u \in C^0(\Omega).
\f]

It is well known that the operator $\gamma$ can be extended to a continuous
operator on $H^1(\Omega)$, mapping functions in $H^1(\Omega)$ to functions in
$H^1(\Gamma)$ when the intrinsic dimension of $\Gamma$ is the same of $\Omega$.

The same is true, with a less regular range space (namely $H^{1/2}(\Gamma)$),
when the dimension of $\Gamma$ is one less with respect to $\Omega$, and
$\Gamma$ does not have a boundary. In this second case, the operator $\gamma$ is
also known as the *trace* operator, and it is well defined for Lipschitz
co-dimension one curves and surfaces $\Gamma$ embedded in $\Omega$ (read  <a
href="https://en.wikipedia.org/wiki/Trace_operator">this wikipedia article</a>
for further details on the trace operator).

The co-dimension two case is a little more complicated, and in general it is not
possible to construct a continuous trace operator, not even from $H^1(\Omega)$ to
$L^2(\Gamma)$, when the dimension of $\Gamma$ is zero or one respectively in two
and three dimensions.

In this tutorial program we're not interested in further details on $\gamma$: we
take the extension $\gamma$ for granted, assuming that the dimension of the
embedded domain (`dim`) is always smaller by one or equal with respect to the
dimension of the embedding domain $\Omega$ (`spacedim`).

We are going to solve the following differential problem: given a sufficiently
regular function $g$ on $\Gamma$, find the solution $u$ to

@f{eqnarray*}{
- \Delta u + \gamma^T \lambda &=& 0  \text{ in } \Omega\\
\gamma u &=& g  \text{ in } \Gamma \\
u & = & 0 \text{ on } \partial\Omega.
@f}

This is a constrained problem, where we are looking for a harmonic function $u$
that satisfies homogeneous boundary conditions on $\partial\Omega$, subject to
the constraint $\gamma u = g$ using a Lagrange multiplier.

This problem has a physical interpretation: harmonic functions, i.e., functions
that satisfy the Laplace equation, can be thought of as the displacements of a
membrane whose boundary values are prescribed. The current situation then
corresponds to finding the shape of a membrane for which not only the
displacement at the boundary, but also on $\Gamma$ is prescribed. For example,
if $\Gamma$ is a closed curve in 2d space, then that would model a soap film
that is held in place by a wire loop along $\partial \Omega$ as well as a second
loop along $\Gamma$. In cases where $\Gamma$ is a whole area, you can think of
this as a membrane that is stretched over an obstacle where $\Gamma$ is the
contact area. (If the contact area is not known we have a different problem --
called the "obstacle problem" -- which is modeled in step-41.)

As a first example we study the zero Dirichlet boundary condition on
$\partial\Omega$. The same equations apply if we apply zero Neumann boundary
conditions on $\partial\Omega$ or a mix of the two.

The variational formulation can be derived by introducing two infinite
dimensional spaces $V(\Omega)$ and $Q^*(\Gamma)$, respectively for the solution
$u$ and for the Lagrange multiplier $\lambda$.

Multiplying the first equation by $v \in V(\Omega)$ and the second by $q \in
Q(\Gamma)$, integrating by parts when possible, and exploiting the boundary
conditions on $\partial\Omega$, we obtain the following variational problem:

Given a sufficiently regular function $g$ on $\Gamma$, find the solution $u$ to
@f{eqnarray*}{
(\nabla u, \nabla v)_{\Omega} + (\lambda, \gamma v)_{\Gamma} &=& 0 \qquad \forall v \in V(\Omega) \\
(\gamma u, q)_{\Gamma} &=& (g,q)_{\Gamma} \qquad \forall q \in Q(\Gamma),
@f}

where $(\cdot, \cdot)_{\Omega}$ and $(\cdot, \cdot)_{\Gamma}$ represent,
respectively, $L^2$ scalar products in $\Omega$ and in $\Gamma$.

Inspection of the variational formulation tells us that the space $V(\Omega)$
can be taken to be $H^1_0(\Omega)$. The space $Q(\Gamma)$, in the co-dimension
zero case, should be taken as $H^1(\Gamma)$, while in the co-dimension one case
should be taken as $H^{1/2}(\Gamma)$.

The function $g$ should therefore be either in $H^1(\Gamma)$ (for the
co-dimension zero case) or $H^{1/2}(\Gamma)$ (for the co-dimension one case).
This leaves us with a Lagrange multiplier $\lambda$ in $Q^*(\Gamma)$, which is
either $H^{-1}(\Gamma)$ or $H^{-1/2}(\Gamma)$.

There are two options for the discretization of the problem above. One could choose
matching discretizations, where the Triangulation for $\Gamma$ is aligned with the
Triangulation for $\Omega$, or one could choose to discretize the two domains in
a completely independent way.

The first option is clearly more indicated for the simple problem we
proposed above: it is sufficient to use a single Triangulation for $\Omega$ and
then impose certain constraints depending $\Gamma$. An example of this approach
is studied in step-40, where the solution has to stay above an obstacle and this
is achieved imposing constraints on $\Omega$.

To solve more complex problems, for example one where the domain $\Gamma$ is time
dependent, the second option could be a more viable solution. Handling
non aligned meshes is complex by itself: to illustrate how is done we study a
simple problem.

The technique we describe here is presented in the literature using one of many names:
the <b>immersed finite element method</b>, the <b>fictitious boundary method</b>, the
<b>distributed Lagrange multiplier method</b>, and others. The main principle is
that the discretization of the two grids and of the two finite element spaces
are kept completely independent. This technique is particularly efficient for
the simulation of fluid-structure interaction problems, where the configuration
of the embedded structure is part of the problem itself, and one solves a
(possibly non-linear) elastic problem to determine the (time dependent)
configuration of $\Gamma$, and a (possibly non-linear) flow problem in $\Omega
\setminus \Gamma$, plus coupling conditions on the interface between the fluid
and the solid.

In this tutorial program we keep things a little simpler, and we assume that the
configuration of the embedded domain is given in one of two possible ways:

- as a deformation mapping $\psi: \Gamma_0 \mapsto \Gamma \subseteq \Omega$,
defined on a continuous finite dimensional space on $\Gamma_0$ and representing,
for any point $x \in \Gamma_0$, its coordinate $\psi(x)$ in $\Omega$;

- as a displacement mapping $\delta \psi(x) = \psi(x)-x$ for $x\in \Gamma_0$,
representing for any point $x$ the displacement vector applied in order to
deform $x$ to its actual configuration $\psi(x) = x +\delta\psi(x)$.

We define the embedded reference domain $\Gamma_0$ `embedded_grid`: on
this triangulation we construct a finite dimensional space (`embedded_configuration_dh`)
to describe either the deformation or the displacement through a FiniteElement
system of FE_Q objects (`embedded_configuration_fe`). This finite dimensional
space is used only to interpolate a user supplied function
(`embedded_configuration_function`) representing either $\psi$ (if the
parameter `use_displacement` is set to @p false) or $\delta\psi$ (if the
parameter `use_displacement` is set to @p true).

The Lagrange multiplier $\lambda$ and the user supplied function $g$ are
defined through another finite dimensional space `embedded_dh`, and through
another FiniteElement `embedded_fe`, using the same reference domain. In
order to take into account the deformation of the domain, either a MappingFEField
or a MappingQEulerian object are initialized with the `embedded_configuration`
vector.

In the embedding space, a standard finite dimensional space `space_dh` is
constructed on the embedding grid `space_grid`, using the
FiniteElement `space_fe`, following almost verbatim the approach taken in step-6.

We represent the discretizations of the spaces $V$ and $Q$ with
\f[
V_h(\Omega) = \text{span} \{v_i\}_{i=1}^n
\f]
and
\f[
Q_h(\Gamma) = \text{span} \{q_i\}_{i=1}^m
\f]
respectively, where $n$ is the dimension of `space_dh`, and $m$
the dimension of `embedded_dh`.

Once all the finite dimensional spaces are defined, the variational formulation
of the problem above leaves us with the following finite dimensional system
of equations:

\f[
\begin{pmatrix}
K & C^T \\
C & 0
\end{pmatrix}
\begin{pmatrix}
u \\
\lambda
\end{pmatrix}
=
\begin{pmatrix}
0 \\
G
\end{pmatrix}
\f]

where

@f{eqnarray*}{
K_{ij} &\dealcoloneq& (\nabla v_j, \nabla v_i)_\Omega   \qquad i,j=1,\dots,n \\
C_{\alpha j} &\dealcoloneq& (v_j, q_\alpha)_\Gamma  \qquad j=1,\dots,n, \alpha = 1,\dots, m \\\\
G_{\alpha} &\dealcoloneq& (g, q_\alpha)_\Gamma \qquad \alpha = 1,\dots, m.
@f}

While the matrix $K$ is the standard stiffness matrix for the Poisson problem on
$\Omega$, and the vector $G$ is a standard right-hand-side vector for a finite
element problem with forcing term $g$ on $\Gamma$, (see, for example, step-3),
the matrix $C$ or its transpose $C^T$ are non-standard since they couple
information on two non-matching grids.

In particular, the integral that appears in the computation of a single entry of
$C$, is computed on $\Gamma$. As usual in finite elements we split this
integral into contributions from all cells of the triangulation used to
discretize $\Gamma$, we transform the integral on $K$ to an integral on the
reference element $\hat K$, where $F_{K}$ is the mapping from $\hat K$ to $K$,
and compute the integral on $\hat K$ using a quadrature formula:

\f[
C_{\alpha j} \dealcoloneq (v_j, q_\alpha)_\Gamma  = \sum_{K\in \Gamma} \int_{\hat K}
\hat q_\alpha(\hat x) (v_j \circ F_{K}) (\hat x) J_K (\hat x) \mathrm{d} \hat x =
\sum_{K\in \Gamma} \sum_{i=1}^{n_q}  \big(\hat q_\alpha(\hat x_i)  (v_j \circ F_{K}) (\hat x_i) J_K (\hat x_i) w_i \big)
\f]

Computing this sum is non-trivial because we have to evaluate $(v_j \circ F_{K})
(\hat x_i)$. In general, if $\Gamma$ and $\Omega$ are not aligned, the point
$F_{K}(\hat x_i)$ is completely arbitrary with respect to $\Omega$, and unless
we figure out a way to interpolate all basis functions of $V_h(\Omega)$ on an
arbitrary point on $\Omega$, we cannot compute the integral needed for an entry
of the matrix $C$.

To evaluate $(v_j \circ F_{K}) (\hat x_i)$ the following steps needs to be
taken (as shown in the picture below):

- For a given cell $K$ in $\Gamma$ compute the real point $y_i \dealcoloneq F_{K} (\hat
x_i)$, where $x_i$ is one of the quadrature points used for the integral on $K
\subseteq \Gamma$.

- Find the cell of $\Omega$ in which $y_i$ lies. We shall call this element $T$.

- To evaluate the basis function use the inverse of the mapping $G_T$ that
transforms the reference element $\hat T$ into the element $T$: $v_j(y_i) = \hat
v_j \circ G^{-1}_{T} (y_i)$.

<p align="center"> <img
  src="https://www.dealii.org/images/steps/developer/step-60.C_interpolation.png"
  alt=""> </p>

The three steps above can be computed by calling, in turn,

- GridTools::find_active_cell_around_point(), followed by

- Mapping::transform_real_to_unit_cell(). We then

- construct a custom Quadrature formula, containing the point in the reference
 cell and then

- construct an FEValues object, with the given quadrature formula, and
 initialized with the cell obtained in the first step.

This is what the deal.II function VectorTools::point_value() does when
evaluating a finite element field (not just a single shape function) at an
arbitrary point; but this would be inefficient in this case.

A better solution is to use a convenient wrapper to perform the first three
steps on a collection of points: GridTools::compute_point_locations(). If one is
actually interested in computing the full coupling matrix, then it is possible
to call the method NonMatching::create_coupling_mass_matrix(), that performs the
above steps in an efficient way, reusing all possible data structures, and
gathering expensive steps together. This is the function we'll be using later in
this tutorial.

We solve the final saddle point problem by an iterative solver, applied to the
Schur complement $S$ (whose construction is described, for example, in step-20),
and we construct $S$ using LinearOperator classes.


<h3>The testcase</h3>

The problem we solve here is identical to step-4, with the difference that we
impose some constraints on an embedded domain $\Gamma$. The tutorial is written
in a dimension independent way, and in the results section we show how to vary
both `dim` and `spacedim`.

The tutorial is compiled for `dim` equal to one and `spacedim` equal to two. If
you want to run the program in embedding dimension `spacedim` equal to three,
you will most likely want to change the reference domain for $\Gamma$ to be, for
example, something you read from file, or a closed sphere that you later deform
to something more interesting.

In the default scenario, $\Gamma$ has co-dimension one, and this tutorial
program implements the Fictitious Boundary Method. As it turns out, the same
techniques are used in the Variational Immersed Finite Element Method, and
the coupling operator $C$ defined above is the same in almost all of these
non-matching methods.

The embedded domain is assumed to be included in $\Omega$, which we take as the
unit square $[0,1]^2$. The definition of the fictitious domain $\Gamma$ can be
modified through the parameter file, and can be given as a mapping from the
reference interval $[0,1]$ to a curve in $\Omega$.

If the curve is closed, then the results will be similar to running the same
problem on a grid whose boundary is $\Gamma$. The program will happily run also
with a non-closed $\Gamma$, although in those cases the mathematical
formulation of the problem is more difficult, since $\Gamma$ will have a
boundary by itself that has co-dimension two with respect to the domain
$\Omega$.


<h3>References</h3>

<ul>
<li> Glowinski, R., T.-W. Pan, T.I. Hesla, and D.D. Joseph. 1999. “A Distributed
  Lagrange Multiplier/fictitious Domain Method for Particulate Flows.”
  International Journal of Multiphase Flow 25 (5). Pergamon: 755–94.

<li> Boffi, D., L. Gastaldi, L. Heltai, and C.S. Peskin. 2008. “On the
  Hyper-Elastic Formulation of the Immersed Boundary Method.” Computer Methods
  in Applied Mechanics and Engineering 197 (25–28).

<li> Heltai, L., and F. Costanzo. 2012. “Variational Implementation of Immersed
  Finite Element Methods.” Computer Methods in Applied Mechanics and Engineering
  229–232.
</ul>


examples/step-60/doc/results.dox
<h1>Results</h1>

The directory in which this program is run does not contain a parameter file by
default. On the other hand, this program wants to read its parameters from a
file called parameters.prm -- and so, when you execute it the first time, you
will get an exception that no such file can be found:

@code
----------------------------------------------------
Exception on processing:

--------------------------------------------------------
An error occurred in line <74> of file <../source/base/parameter_acceptor.cc> in function
    static void dealii::ParameterAcceptor::initialize(const std::string &, const std::string &, const ParameterHandler::OutputStyle, dealii::ParameterHandler &)
The violated condition was:
    false
Additional information:
    You specified <parameters.prm> as input parameter file, but it does not exist. We created it for you.
--------------------------------------------------------

Aborting!
----------------------------------------------------
@endcode

However, as the error message already states, the code that triggers the
exception will also generate a parameters.prm file that simply contains the
default values for all parameters this program cares about. By inspection of the
parameter file, we see the following:

@code
# Listing of Parameters
# ---------------------
subsection Distributed Lagrange<1,2>
  set Coupling quadrature order                    = 3
  set Embedded configuration finite element degree = 1
  set Embedded space finite element degree         = 1
  set Embedding space finite element degree        = 1
  set Homogeneous Dirichlet boundary ids           = 0, 1, 2, 3
  set Initial embedded space refinement            = 7
  set Initial embedding space refinement           = 4
  set Local refinements steps near embedded domain = 3
  set Use displacement in embedded interface       = false
  set Verbosity level                              = 10


  subsection Embedded configuration
    # Sometimes it is convenient to use symbolic constants in the expression
    # that describes the function, rather than having to use its numeric value
    # everywhere the constant appears. These values can be defined using this
    # parameter, in the form `var1=value1, var2=value2, ...'.
    #
    # A typical example would be to set this runtime parameter to
    # `pi=3.1415926536' and then use `pi' in the expression of the actual
    # formula. (That said, for convenience this class actually defines both
    # `pi' and `Pi' by default, but you get the idea.)
    set Function constants  = R=.3, Cx=.4, Cy=.4                 # default:

    # The formula that denotes the function you want to evaluate for
    # particular values of the independent variables. This expression may
    # contain any of the usual operations such as addition or multiplication,
    # as well as all of the common functions such as `sin' or `cos'. In
    # addition, it may contain expressions like `if(x>0, 1, -1)' where the
    # expression evaluates to the second argument if the first argument is
    # true, and to the third argument otherwise. For a full overview of
    # possible expressions accepted see the documentation of the muparser
    # library at http://muparser.beltoforion.de/.
    #
    # If the function you are describing represents a vector-valued function
    # with multiple components, then separate the expressions for individual
    # components by a semicolon.
    set Function expression = R*cos(2*pi*x)+Cx; R*sin(2*pi*x)+Cy # default: 0

    # The names of the variables as they will be used in the function,
    # separated by commas. By default, the names of variables at which the
    # function will be evaluated are `x' (in 1d), `x,y' (in 2d) or `x,y,z' (in
    # 3d) for spatial coordinates and `t' for time. You can then use these
    # variable names in your function expression and they will be replaced by
    # the values of these variables at which the function is currently
    # evaluated. However, you can also choose a different set of names for the
    # independent variables at which to evaluate your function expression. For
    # example, if you work in spherical coordinates, you may wish to set this
    # input parameter to `r,phi,theta,t' and then use these variable names in
    # your function expression.
    set Variable names      = x,y,t
  end

  subsection Embedded value
    # Sometimes it is convenient to use symbolic constants in the expression
    # that describes the function, rather than having to use its numeric value
    # everywhere the constant appears. These values can be defined using this
    # parameter, in the form `var1=value1, var2=value2, ...'.
    #
    # A typical example would be to set this runtime parameter to
    # `pi=3.1415926536' and then use `pi' in the expression of the actual
    # formula. (That said, for convenience this class actually defines both
    # `pi' and `Pi' by default, but you get the idea.)
    set Function constants  =

    # The formula that denotes the function you want to evaluate for
    # particular values of the independent variables. This expression may
    # contain any of the usual operations such as addition or multiplication,
    # as well as all of the common functions such as `sin' or `cos'. In
    # addition, it may contain expressions like `if(x>0, 1, -1)' where the
    # expression evaluates to the second argument if the first argument is
    # true, and to the third argument otherwise. For a full overview of
    # possible expressions accepted see the documentation of the muparser
    # library at http://muparser.beltoforion.de/.
    #
    # If the function you are describing represents a vector-valued function
    # with multiple components, then separate the expressions for individual
    # components by a semicolon.
    set Function expression = 1     # default: 0

    # The names of the variables as they will be used in the function,
    # separated by commas. By default, the names of variables at which the
    # function will be evaluated are `x' (in 1d), `x,y' (in 2d) or `x,y,z' (in
    # 3d) for spatial coordinates and `t' for time. You can then use these
    # variable names in your function expression and they will be replaced by
    # the values of these variables at which the function is currently
    # evaluated. However, you can also choose a different set of names for the
    # independent variables at which to evaluate your function expression. For
    # example, if you work in spherical coordinates, you may wish to set this
    # input parameter to `r,phi,theta,t' and then use these variable names in
    # your function expression.
    set Variable names      = x,y,t
  end

  subsection Schur solver control
    set Log frequency = 1
    set Log history   = false
    set Log result    = true
    set Max steps     = 1000   # default: 100
    set Reduction     = 1.e-12 # default: 1.e-2
    set Tolerance     = 1.e-12 # default: 1.e-10
  end

end
@endcode

If you now run the program, you will get a file called `used_parameters.prm`,
containing a shorter version of the above parameters (without comments and
documentation), documenting all parameters that were used to run your program:
@code
# Parameter file generated with
# DEAL_II_PACKAGE_VERSION = 9.0.0
subsection Distributed Lagrange<1,2>
  set Coupling quadrature order                    = 3
  set Embedded configuration finite element degree = 1
  set Embedded space finite element degree         = 1
  set Embedding space finite element degree        = 1
  set Homogeneous Dirichlet boundary ids           = 0, 1, 2, 3
  set Initial embedded space refinement            = 7
  set Initial embedding space refinement           = 4
  set Local refinements steps near embedded domain = 3
  set Use displacement in embedded interface       = false
  set Verbosity level                              = 10
  subsection Embedded configuration
    set Function constants  = R=.3, Cx=.4, Cy=.4
    set Function expression = R*cos(2*pi*x)+Cx; R*sin(2*pi*x)+Cy
    set Variable names      = x,y,t
  end
  subsection Embedded value
    set Function constants  =
    set Function expression = 1
    set Variable names      = x,y,t
  end
  subsection Schur solver control
    set Log frequency = 1
    set Log history   = false
    set Log result    = true
    set Max steps     = 1000
    set Reduction     = 1.e-12
    set Tolerance     = 1.e-12
  end
end
@endcode

The rationale behind creating first `parameters.prm` file (the first time the
program is run) and then a `used_parameters.prm` (every other times you run the
program), is because you may want to leave most parameters to their default
values, and only modify a handful of them.

For example, you could use the following (perfectly valid) parameter file with
this tutorial program:
@code
subsection Distributed Lagrange<1,2>
  set Initial embedded space refinement            = 7
  set Initial embedding space refinement           = 4
  set Local refinements steps near embedded domain = 3
  subsection Embedded configuration
    set Function constants  = R=.3, Cx=.4, Cy=.4
    set Function expression = R*cos(2*pi*x)+Cx; R*sin(2*pi*x)+Cy
    set Variable names      = x,y,t
  end
  subsection Embedded value
    set Function constants  =
    set Function expression = 1
    set Variable names      = x,y,t
  end
end
@endcode

and you would obtain exactly the same results as in test case 1 below.

<h3> Test case 1: </h3>

For the default problem the value of $u$ on $\Gamma$ is set to the constant $1$:
this is like imposing a constant Dirichlet boundary condition on $\Gamma$, seen
as boundary of the portion of $\Omega$ inside $\Gamma$. Similarly on $\partial
\Omega$ we have zero Dirichlet boundary conditions.


<div class="twocolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-60.1_no_grid.png"
           alt = ""
           width="500">
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-60.1_grid.png"
           alt = ""
           width="500">
    </div>
  </div>
</div>

The output of the program will look like the following:

@code
DEAL::Embedded dofs: 129
DEAL::Embedding minimal diameter: 0.0110485, embedded maximal diameter: 0.00781250, ratio: 0.707107
DEAL::Embedding dofs: 2429
DEAL:cg::Starting value 0.166266
DEAL:cg::Convergence step 108 value 7.65958e-13


+---------------------------------------------+------------+------------+
| Total CPU time elapsed since start          |     0.586s |            |
|                                             |            |            |
| Section                         | no. calls |  CPU time  | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble coupling system        |         1 |     0.132s |        23% |
| Assemble system                 |         1 |    0.0733s |        12% |
| Output results                  |         1 |     0.087s |        15% |
| Setup coupling                  |         1 |    0.0244s |       4.2% |
| Setup grids and dofs            |         1 |    0.0907s |        15% |
| Solve system                    |         1 |     0.178s |        30% |
+---------------------------------+-----------+------------+------------+



+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |     0.301s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble coupling system        |         1 |    0.0385s |        13% |
| Assemble system                 |         1 |    0.0131s |       4.3% |
| Output results                  |         1 |    0.0736s |        24% |
| Setup coupling                  |         1 |    0.0234s |       7.7% |
| Setup grids and dofs            |         1 |    0.0679s |        23% |
| Solve system                    |         1 |    0.0832s |        28% |
+---------------------------------+-----------+------------+------------+

@endcode

You may notice that, in terms of CPU time, assembling the coupling system is
twice as expensive as assembling the standard Poisson system, even though the
matrix is smaller. This is due to the non-matching nature of the discretization.
Whether this is acceptable or not, depends on the applications.

If the problem was set in a three-dimensional setting, and the immersed mesh was
time dependent, it would be much more expensive to recreate the mesh at each
step rather than use the technique we present here. Moreover, you may be able to
create a very fast and optimized solver on a uniformly refined square or cubic
grid, and embed the domain where you want to perform your computation using the
technique presented here. This would require you to only have a surface
representatio of your domain (a much cheaper and easier mesh to produce).

To play around a little bit, we are going to complicate a little the fictitious
domain as well as the boundary conditions we impose on it.

<h3> Test case 2 and 3: </h3>

If we use the following parameter file:
@code
subsection Distributed Lagrange<1,2>
  set Coupling quadrature order                    = 3
  set Embedded configuration finite element degree = 1
  set Embedded space finite element degree         = 1
  set Embedding space finite element degree        = 1
  set Homogeneous Dirichlet boundary ids           = 0,1,2,3
  set Initial embedded space refinement            = 8
  set Initial embedding space refinement           = 4
  set Local refinements steps near embedded domain = 4
  set Use displacement in embedded interface       = false
  set Verbosity level                              = 10
  subsection Embedded configuration
    set Function constants  = R=.3, Cx=.5, Cy=.5, r=.1, w=12
    set Function expression = (R+r*cos(w*pi*x))*cos(2*pi*x)+Cx; (R+r*cos(w*pi*x))*sin(2*pi*x)+Cy
    set Variable names      = x,y,t
  end
  subsection Embedded value
    set Function constants  =
    set Function expression = x-.5
    set Variable names      = x,y,t
  end
  subsection Schur solver control
    set Log frequency = 1
    set Log history   = false
    set Log result    = true
    set Max steps     = 100000
    set Reduction     = 1.e-12
    set Tolerance     = 1.e-12
  end
end
@endcode

We get a "flowery" looking domain, where we impose a linear boundary condition
$g=x-.5$. This test shows that the method is actually quite accurate in
recovering an exactly linear function from its boundary conditions, and even
though the meshes are not aligned, we obtain a pretty good result.

Replacing $x-.5$ with $2(x-.5)^2-2(y-.5)^2$, i.e., modifying the parameter file
such that we have
@code
  ...
  subsection Embedded value
    set Function constants  =
    set Function expression = 2*(x-.5)^2-2*(y-.5)^2
    set Variable names      = x,y,t
  end
@endcode
produces the saddle on the right.

<div class="twocolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-60.3_grid.png"
           alt = ""
           width="500">
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-60.4_grid.png"
           alt = ""
           width="500">
    </div>
  </div>
</div>

<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

<h4> Running with `spacedim` equal to three</h4>

While the current tutorial program is written for `spacedim` equal to two, there
are only minor changes you have to do in order for the program to run in
different combinations of dimensions.

If you want to run with `spacedim` equal to three and `dim` equal to two, then
you will almost certainly want to perform the following changes:

- use a different reference domain for the embedded grid, maybe reading it from
  a file. It is not possible to construct a smooth closed surface with one
  single parametrization of a square domain, therefore you'll most likely want
  to use a reference domain that is topologically equivalent to a the boundary
  of a sphere.

- use a displacement instead of the deformation to map $\Gamma_0$ into $\Gamma$

<h4> More general domains </h4>

We have seen in other tutorials (for example in step-5 and step-54) how to read
grids from input files. A nice generalization for this tutorial program would be
to allow the user to select a grid to read from the parameter file itself,
instead of hardcoding the mesh type in the tutorial program itself.

<h4> Preconditioner</h4>

At the moment, we have no preconditioner on the Schur complement. This is ok for
two dimensional problems, where a few hundred iterations bring the residual down
to the machine precision, but it's not going to work in three dimensions.

It is not obvious what a good preconditioner would be here. The physical problem
we are solving with the Schur complement, is to associate to the Dirichlet data
$g$, the value of the Lagrange multiplier $\lambda$. $\lambda$ can be
interpreted as the *jump* in the normal gradient that needs to be imposed on $u$
across $\Gamma$, in order to obtain the Dirichlet data $g$.

So $S$ is some sort of Neumann to Dirichlet map, and we would like to have a
good approximation for the Dirichlet to Neumann map. A possibility would be to
use a Boundary Element approximation of the problem on $\Gamma$, and construct a
rough approximation of the hyper-singular operator for the Poisson problem
associated to $\Gamma$, which is precisely a Dirichlet to Neumann map.

<h4> Parallel Code </h4>

The simple code proposed here can serve as a starting point for more
complex problems which, to be solved, need to be run on parallel
code, possibly using distributed meshes (see step-17, step-40, and the
documentation for parallel::shared::Triangulation and
parallel::distributed::Triangulation).

When using non-matching grids in parallel a problem arises: to compute the
matrix $C$ a process needs information about both meshes on the same portion of
real space but, when working with distributed meshes, this information may not
be available, because the locally owned part of the $\Omega$ triangulation
stored on a given processor may not be physically co-located with the locally
owned part of the $\Gamma$ triangulation stored on the same processor.

Various strategies can be implemented to tackle this problem:

- distribute the two meshes so that this constraint is satisfied;

- use communication for the parts of real space where the constraint is not
  satisfied;

- use a distributed triangulation for the embedding space, and a shared
  triangulation for the emdedded configuration.

The latter strategy is clearly the easiest to implement, as most of the
functions used in this tutorial program will work unchanged also in the parallel
case. Of course one could use the reversal strategy (that is, have a distributed
embedded Triangulation and a shared embedding Triangulation).

However, this strategy is most likely going to be more expensive, since by
definition the embedding grid is larger than the embedded grid, and it makes
more sense to distribute the largest of the two grids, maintaining the smallest
one shared among all processors.


examples/step-61/doc/intro.dox
<br>

<i>
This program was contributed by Zhuoran Wang.
Some more information about this program, as well as more numerical
results, are presented in @cite Wang2019 .
</i>

<a name="Intro"></a>
<h1>Introduction</h1>

This tutorial program presents an implementation of the "weak Galerkin"
finite element method for the Poisson equation. In some sense, the motivation for
considering this method starts from the same point as in step-51: We would like to
consider discontinuous shape functions, but then need to address the fact that
the resulting problem has a much larger number of degrees of freedom compared to
the usual continuous Galerkin method (because, for
example, each vertex carries as many degrees of freedom as there are adjacent cells).
We also have to address the fact that, unlike in the continuous
Galerkin method, <i>every</i> degree of freedom
on one cell couples with all of the degrees of freedom on each of its face neighbor
cells. Consequently, the matrix one gets from the "traditional" discontinuous
Galerkin methods are both large and relatively dense.

Both the hybridized discontinuous Galerkin method (HDG) in step-51 and the weak
Galerkin (WG) method in this tutorial address the issue of coupling by introducing
additional degrees of freedom whose shape functions only live on a face between
cells (i.e., on the "skeleton" of the mesh), and which therefore "insulate" the
degrees of freedom on the adjacent cells from each other: cell degrees of freedom
only couple with other cell degrees of freedom on the same cell, as well as face
degrees of freedom, but not with cell degrees of freedom on neighboring cells.
Consequently, the coupling of shape functions for these cell degrees of freedom
indeed couple on exactly one cell and the degrees of freedom defined on its
faces.

For a given equation, say the second order Poisson equation,
the difference between the HDG and the WG method is how precisely one formulates
the problem that connects all of these different shape functions. (Indeed,
for some WG and HDG formulation, it is possible to show that they are equivalent.)
The HDG does things by reformulating second order problems in terms of a system of first
order equations and then conceptually considers the face degrees of freedom
to be "fluxes" of this first order system. In contrast, the WG method keeps things
in second order form and considers the face degrees of freedom as of the same
type as the primary solution variable, just restricted to the lower-dimensional
faces. For the purposes of the equation, one then needs to somehow "extend"
these shape functions into the interior of the cell when defining what it means
to apply a differential operator to them. Compared to the HDG, the method
has the advantage that it does not lead to a proliferation of unknowns due
to rewriting the equation as a first-order system, but it is also not quite
as easy to implement. However, as we will see in the following, this
additional effort is not prohibitive.


<h3> Weak Galerkin finite element methods </h3>

Weak Galerkin Finite Element Methods (WGFEMs) use discrete weak functions
to approximate scalar unknowns, and discrete weak gradients to
approximate classical gradients.
The method was originally introduced by Junping Wang and Xiu Ye
in the paper
<a href="https://doi.org/10.1016/j.cam.2012.10.003">
<i>A weak Galerkin finite element method for second order elliptic problems</i>,
J. Comput. Appl. Math., 103-115, 2013</a>.
Compared to the continuous Galerkin method,
the weak Galerkin method satisfies important physical properties, namely
local mass conservation and bulk normal flux continuity.
It results in a SPD linear system, and optimal convergence rates can
be obtained with mesh refinement.


<h3> The equation to solve </h3>
This program solves the Poisson equation
using the weak Galerkin finite element method:
@f{align*}{
  \nabla \cdot \left( -\mathbf{K} \nabla p \right)
    &= f,
    \qquad \mathbf{x} \in \Omega, \\
  p &=  p_D,\qquad \mathbf{x} \in \Gamma^D, \\
  \mathbf{u} \cdot \mathbf{n} &= u_N,
  \qquad \mathbf{x} \in \Gamma^N,
@f}
where $\Omega \subset \mathbb{R}^n (n=2,3)$ is a bounded domain.
In the context of the flow of a fluid through a porous medium,
$p$ is the pressure, $\mathbf{K}$ is a permeability tensor,
$f$ is the source term, and
$p_D, u_N$ represent Dirichlet and Neumann boundary conditions.
We can introduce a flux, $\mathbf{u} = -\mathbf{K} \nabla p$, that corresponds
to the Darcy velocity (in the way we did in step-20) and this variable will
be important in the considerations below.

In this program, we will consider a test case where the exact pressure
is $p = \sin \left( \pi x\right)\sin\left(\pi y \right)$ on the unit square domain,
with homogeneous Dirichelet boundary conditions and $\mathbf{K}$ the identity matrix.
Then we will calculate $L_2$ errors of pressure, velocity, and flux.


<h3> Weak Galerkin scheme </h3>

The Poisson equation above has a solution $p$ that needs to satisfy the weak
formulation of the problem,
@f{equation*}
\mathcal{A}\left(p,q \right) = \mathcal{F} \left(q \right),
@f}
for all test functions $q$, where
@f{equation*}
\mathcal{A}\left(p,q\right)
  \dealcoloneq \int_\Omega \left(\mathbf{K} \nabla p\right) \cdot \nabla q \;\mathrm{d}x,
@f}
and
@f{equation*}
\mathcal{F}\left(q\right)
  \dealcoloneq \int_\Omega f \, q \;\mathrm{d}x
  - \int_{\Gamma^N} u_N q \; \mathrm{d}x.
@f}
Here, we have integrated by parts in the bilinear form, and we are evaluating
the gradient of $p,p$ in the interior and the values of $q$ on the boundary
of the domain. All of this is well defined because we assume that the solution
is in $H^1$ for which taking the gradient and evaluating boundary values
are valid operations.

The idea of the weak Galerkin method is now to approximate the exact $p$
solution with a <i>discontinuous function</i> $p_h$. This function may only be
discontinuous along interfaces between cells, and because we will want to
evaluate this function also along interfaces, we have to
prescribe not only what values it is supposed to have in the cell interiors
but also its values along interfaces. We do this by saying that $p_h$ is
actually a tuple, $p_h=(p^\circ,p^\partial)$, though it's really just
a single function that is either equal to $p^\circ(x)$ or $p^\partial(x)$,
depending on whether it is evaluated at a point $x$ that lies in the cell
interior or on cell interfaces.

We would then like to simply stick this approximation into the bilinear
form above. This works for the case where we have to evaluate the
test function $q_h$ on the boundary (where we would simply take its interface
part $q_h^\partial$) but we have to be careful with the gradient because
that is only defined in cell interiors. Consequently,
the weak Galerkin scheme for the Poisson equation is defined by
@f{equation*}
\mathcal{A}_h\left(p_h,q \right) = \mathcal{F} \left(q_h \right),
@f}
for all discrete test functions $q_h$, where
@f{equation*}
\mathcal{A}_h\left(p_h,q_h\right)
  \dealcoloneq \sum_{K \in \mathbb{T}}
    \int_K \mathbf{K} \nabla_{w,d} p_h \cdot \nabla_{w,d} q_h \;\mathrm{d}x,
@f}
and
@f{equation*}
\mathcal{F}\left(q_h\right)
  \dealcoloneq \sum_{K \in \mathbb{T}} \int_K f \, q_h^\circ \;\mathrm{d}x
  - \sum_{\gamma \in \Gamma_h^N} \int_\gamma u_N q_h^\partial \;\mathrm{d}x,
@f}
The key point is that here, we have replaced the gradient $\nabla p_h$ by the
<i>discrete weak gradient</i> operator
$\nabla_{w,d} p_h$ that makes sense for our peculiarly defined approximation $p_h$.

The question is then how that operator works. For this, let us first say how we
think of the discrete approximation $p_h$ of the pressure. As mentioned above,
the "function" $p_h$ actually consists of two parts: the values $p_h^\circ$ in
the interior of cells, and $p_h^\partial$ on the interfaces. We have to define
discrete (finite-dimensional) function spaces for both of these; in this
program, we will use FE_DGQ for $p_h^\circ$ as the space in the interior of
cells (defined on each cell, but in general discontinuous along interfaces),
and FE_FaceQ for $p_h^\partial$ as the space on the interfaces.

Then let us consider just a single cell (because the integrals above are all
defined cell-wise, and because the weak discrete gradient is defined cell-by-cell).
The restriction of $p_h$ to cell $K$, $p_h|_K$ then consists
of the pair $(p_h^\circ|_K,p_h^\partial|_{\partial K})$. In essence, we can
think of $\nabla_{w,d} p_h$ of some function defined on $K$ that approximates
the gradient; in particular, if $p_h|_K$ was the restriction of a differentiable
function (to the interior and boundary of $K$ -- which would make it continuous
between the interior and boundary), then
$\nabla_{w,d} p_h$ would simply be the exact gradient $\nabla p_h$. But, since
$p_h|_K$ is not continuous between interior and boundary of $K$, we need a more
general definition; furthermore, we can not deal with arbitrary functions, and
so require that $\nabla_{w,d} p_h$ is also in a finite element space (which, since
the gradient is a vector, has to be vector-valued, and because the weak gradient
is defined on each cell separately, will also be discontinuous between cells).

The way this is done is to define this weak gradient operator $\nabla_{w,d}|_K :
DGQ_k(K) \times DGQ_r(\partial K) \rightarrow RT_s(K)$ (where $RT_s(K)$ is the
vector-valued Raviart-Thomas space of order $s$ on cell $K$) in the following way:
@f{equation*}{
  \int_K \mathbf v_h \cdot (\nabla_{w,d} p_h)
  =
  -\int_K (\nabla \cdot \mathbf v_h) p_h^\circ
  +\int_{\partial K} (\mathbf v_h \cdot \mathbf n) p_h^\partial,
@f}
for all test functions $\mathbf v_h \in RT_s(K)$.
This is, in essence, simply an application of the integration-by-parts
formula. In other words, for a given $p_h=(p^\circ_h,p^\partial_h)$,
we need to think of $\nabla_{w,d} p_h|_K$ as that
Raviart-Thomas function of degree $s$ for which the left hand side and right hand side
are equal for all test functions.

A key point to make is then the following: While the usual gradient $\nabla$ is
a *local* operator that computes derivatives based simply on the value of
a function at a point and its (infinitesimal) neighborhood, the weak discrete gradient
$\nabla_{w,d}$ does not have this property: It depends on the values of the function
it is applied to on the entire cell, including the cell's boundary. Both are,
however, linear operators as is clear from the definition of $\nabla_{w,d}$
above, and that will allow us to represent $\nabla_{w,d}$ via a matrix
in the discussion below.

@note It may be worth pointing out that while the weak discrete
  gradient is an element of the Raviart-Thomas space $RT_s(K)$ on each
  cell $K$, it is discontinuous between cells. On the other hand, the
  Raviart-Thomas space $RT_s=RT_s({\mathbb T})$ defined on the entire
  mesh and implemented by the FE_RaviartThomas class represents
  functions that have continuous normal components at interfaces
  between cells. This means that <i>globally</i>, $\nabla_{w,d} p_h$
  is not in $RT_s$, even though it is on every cell $K$ in $RT_s(K)$.
  Rather, it is in a "broken" Raviart-Thomas space that below we will
  represent by the symbol $DGRT_s$. (The term "broken" here refers to
  the process of "breaking something apart", and not to the synonym to
  the expression "not functional".) One might therefore (rightfully) argue that
  the notation used in the weak Galerkin literature is a bit misleading,
  but as so often it all depends on the context in which a certain
  notation is used -- in the current context, references to the
  Raviart-Thomas space or element are always understood to be to the
  "broken" spaces.

@note deal.II happens to have an implementation of this broken Raviart-Thomas
  space: The FE_DGRT class. As a consequence, in this tutorial we will simply
  always use the FE_DGRT class, even though in all of those places where
  we have to compute cell-local matrices and vectors, it makes no difference.


<h3> Representing the weak gradient </h3>

Since $p_h$ is an element of a finite element space, we can expand it in a basis
as we always do, i.e., we can write
@f{equation*}{
  p_h(\mathbf x) = \sum_j P_j \varphi_j(\mathbf x).
@f}
Here, since $p_h$ has two components (the interior and the interface components),
the same must hold true for the basis functions $\varphi_j(\mathbf x)$, which we
can write as $\varphi_j = (\varphi_j^\circ,\varphi_j^\partial)$. If you've
followed the descriptions in step-8, step-20, and the
@ref vector_valued "documentation module on vector-valued problems",
it will be no surprise that for some values of $j$, $\varphi_j^\circ$ will be
zero, whereas for other values of $j$, $\varphi_j^\partial$ will be zero -- i.e.,
shape functions will be of either one or the other kind. That is not important,
here, however. What is important is that we need to wonder how we can represent
$\nabla_{w,d} \varphi_j$ because that is clearly what will appear in the
problem when we want to implement the bilinear form
@f{equation*}
\mathcal{A}_h\left(p_h,q_h\right)
  = \sum_{K \in \mathbb{T}}
    \int_K \mathbf{K} \nabla_{w,d} p_h \cdot \nabla_{w,d} q_h \;\mathrm{d}x,
@f}

The key point is that $\nabla_{w,d} \varphi_j$ is known to be a member of the
"broken" Raviart-Thomas space $DGRT_s$. What this means is that we can
represent (on each cell $K$ separately)
@f{equation*}
\nabla_{w,d} \varphi_j|_K
  = \sum_k C_{jk}^K \mathbf v_k|_K
@f}
where the functions $\mathbf v_k \in DGRT_s$, and where $C^K$ is a matrix of
dimension
@f{align*}{
 \text{dim}\left(DGQ_k(K) \times DGQ_r(K)\right) &\times \text{dim}\left(RT_s(K)\right)
  \\
 &=
 \left(\text{dim}(DGQ_k(K)) + \text{dim}(DGQ_r(K))\right) \times \text{dim}\left(RT_s(K)\right).
@f}
(That the weak discrete gradient can be represented as a matrix should not come
as a surprise: It is a linear operator from one finite dimensional
space to another finite dimensional space. If one chooses bases
for both of these spaces, then <i>every linear operator</i> can
of course be written as a matrix mapping the vector of expansion coefficients
with regards to the basis of the domain space of the operator, to
the vector of expansion coefficients with regards to the basis in the image
space.)

Using this expansion, we can easily use the definition of the weak
discrete gradient above to define what the matrix is going to be:
@f{equation*}{
  \int_K \mathbf v_i \cdot \left(\sum_k C_{jk}^K \mathbf v_k\right)
  =
  -\int_K (\nabla \cdot \mathbf v_i) \varphi_j^\circ
  +\int_{\partial K} (\mathbf v_i \cdot \mathbf n) \varphi_j^\partial,
@f}
for all test functions $\mathbf v_i \in DGRT_s$.

This clearly leads to a linear system of the form
@f{equation*}{
  \sum_k M_{ik}^K C_{jk}^K
  =
  G_{ij}^K
@f}
with
@f{equation*}{
  M_{ik}^K = \int_K \mathbf v_i \cdot \mathbf v_k,
  \qquad\qquad
  G_{ij}^K = -\int_K (\nabla \cdot \mathbf v_i) \varphi_j^\circ
             +\int_{\partial K} (\mathbf v_i \cdot \mathbf n) \varphi_j^\partial,
@f}
and consequently
@f{equation*}{
  \left(C^K\right)^T = \left(M^K\right)^{-1} G^K.
@f}
(In this last step, we have assumed that the indices $i,j,k$ only range
over those degrees of freedom active on cell $K$,
thereby ensuring that the mass matrix on the space $RT_s(K)$ is invertible.)
Equivalently, using the symmetry of the matrix $M$, we have that
@f{equation*}{
  C^K = \left(G^K\right)^{T} \left(M^K\right)^{-1}.
@f}
Also worth pointing out is that the
matrices $C^K$ and $G^K$ are of course not square but rectangular.


<h3> Assembling the linear system </h3>

Having explained how the weak discrete gradient is defined, we can now
come back to the question of how the linear system for the equation in question
should be assembled. Specifically, using the definition of the bilinear
form ${\cal A}_h$ shown above, we then need to compute the elements of the
local contribution to the global matrix,
@f{equation*}{
  A^K_{ij} = \int_K \left({\mathbf K} \nabla_{w,d} \varphi_i\right) \cdot \nabla_{w,d} \varphi_j.
@f}
As explained above, we can expand $\nabla_{w,d} \varphi_i$ in terms of the
Raviart-Thomas basis on each cell, and similarly for $\nabla_{w,d} \varphi_j$:
@f{equation*}{
  A^K_{ij} = \int_K
    \left(
      {\mathbf K}
      \sum_k C_{ik}^K \mathbf v_k|_K
    \right)
    \cdot
    \sum_l C_{jl}^K \mathbf v_l|_K.
@f}
By re-arranging sums, this yields the following expression:
@f{equation*}{
  A^K_{ij} =
    \sum_k \sum_l C_{ik}^K C_{jl}^K
     \int_K
    \left(
      {\mathbf K}
      \mathbf v_k|_K
    \right)
    \cdot
    \mathbf v_l|_K.
@f}
So, if we have the matrix $C^K$ for each cell $K$, then we can easily compute
the contribution $A^K$ for cell $K$ to the matrix $A$ as follows:
@f{equation*}{
  A^K_{ij} =
    \sum_k \sum_l C_{ik}^K C_{jl}^K
    H^K_{kl}
    =
    \sum_k \sum_l C_{ik}^K H^K_{kl} C_{jl}^K
    =
    \left(C^K H^K (C^K)^T \right)_{ij}.
@f}
Here,
@f{equation*}{
  H^K_{kl} =
  \int_K
    \left(
      {\mathbf K}
      \mathbf v_k|_K
    \right)
    \cdot
    \mathbf v_l|_K,
@f}
which is really just the mass matrix on cell $K$ using the Raviart-Thomas
basis and weighting by the permeability tensor $\mathbf K$. The derivation
here then shows that the weak Galerkin method really just requires us
to compute these $C^K$ and $H^K$ matrices on each cell $K$, and then
$A^K = C^K H^K (C^K)^T$, which is easily computed. The code to be shown
below does exactly this.

Having so computed the contribution $A^K$ of cell $K$ to the global
matrix, all we have to do is to "distribute" these local contributions
into the global matrix. How this is done is first shown in step-3 and
step-4. In the current program, this will be facilitated by calling
AffineConstraints::distribute_local_to_global().

A linear system of course also needs a right hand side. There is no difficulty
associated with computing the right hand side here other than the fact
that we only need to use the cell-interior part $\varphi_i^\circ$ for
each shape function $\varphi_i$.


<h3> Post-processing and <i>L<sub>2</sub></i>-errors </h3>

The discussions in the previous sections have given us a linear
system that we can solve for the numerical pressure $p_h$. We can use
this to compute an approximation to the variable $\mathbf u = -{\mathbf K}\nabla p$
that corresponds to the velocity with which the medium flows in a porous
medium if this is the model we are trying to solve. This kind of
step -- computing a derived quantity from the solution of the discrete
problem -- is typically called "post-processing".

Here, instead of using the exact gradient of $p_h$, let us instead
use the discrete weak gradient of $p_h$ to calculate the velocity on each element.
As discussed above,
on each element the gradient of the numerical pressure $\nabla p$ can be
approximated by discrete weak gradients  $ \nabla_{w,d}\phi_i$:
@f{equation*}
\nabla_{w,d} p_h
= \nabla_{w,d} \left(\sum_{i} P_i \phi_i\right)
= \sum_{i} P_i \nabla_{w,d}\phi_i.
@f}

On cell $K$,
the numerical velocity $ \mathbf{u}_h = -\mathbf{K} \nabla_{w,d}p_h$ can be written as
@f{align*}{
  \mathbf{u}_h
  &= -\mathbf{K} \nabla_{w,d} p_h
   = -\mathbf{K}\sum_{i} \sum_{j} P_i C^K_{ij}\mathbf{v}_j,
@f}
where $C^K$ is the expansion matrix from above, and
$\mathbf{v}_j$ is the basis function of the $RT$ space on a cell.

Unfortunately, $\mathbf{K} \mathbf{v}_j$ may not be in the $RT$ space
(unless, of course, if $\mathbf K$ is constant times the identity matrix).
So, in order to represent it in a finite element program, we need to
project it back into a finite dimensional space we can work with. Here,
we will use the $L_2$-projection to project it back to the (broken) $RT$
space.

We define the projection as
$ \mathbf{Q}_h \left( \mathbf{K}\mathbf{v}_j \right) =
\sum_{k} d_{jk}\mathbf{v}_k$ on each cell $K$.
For any $j$,
$\left( \mathbf{Q}_h \left( \mathbf{Kv}_j \right),\mathbf{v}_k \right)_K =
\left( \mathbf{Kv}_j,\mathbf{v}_k \right)_K.$
So, rather than the formula shown above, the numerical velocity on cell $K$
instead becomes
@f{equation*}
\mathbf{u}_h = \mathbf{Q}_h \left( -\mathbf{K}\nabla_{w,d}p_h \right) =
-\sum_i \sum_j P_i B^K_{ij}\mathbf{Q}_h \left( \mathbf{K}\mathbf{v}_j \right),
@f}
and we have the following system to solve for the coefficients $d_{jk}$:
@f{equation*}
 \sum_j
  \left(\mathbf{v}_i,\mathbf{v}_j\right)
   d_{jk}
   =
    \left( \mathbf{Kv}_j,\mathbf{v}_k \right).
@f}
In the implementation below, the matrix with elements
$
   d_{jk}
$
is called <code>cell_matrix_D</code>,
whereas the matrix with elements
$
      \left( \mathbf{Kv}_j,\mathbf{v}_k \right)
$
is called <code>cell_matrix_E</code>.

Then the elementwise velocity is
@f{equation*}
\mathbf{u}_h = -\sum_{i} \sum_{j}P_ic_{ij}\sum_{k}d_{jk}\mathbf{v}_k =
\sum_{k}- \left(\sum_{j} \sum_{i} P_ic_{ij}d_{jk} \right)\mathbf{v}_k,
@f}
where $-\sum_{j} \sum_{i} P_ic_{ij}d_{jk}$ is called
`cell_velocity` in the code.

Using this velocity obtained by "postprocessing" the solution, we can
define the $L_2$-errors of pressure, velocity, and flux
by the following formulas:
@f{align*}{
\|p-p_h^\circ\|^2
  &= \sum_{K \in \mathbb{T}} \|p-p_h^\circ\|_{L_2(K)}^2, \\
 \|\mathbf{u}-\mathbf{u}_h\|^2
  &= \sum_{K \in \mathbb{T}} \|\mathbf{u}-\mathbf{u}_h\|_{L_2(K)^2}^d,\\
\|(\mathbf{u}-\mathbf{u}_h) \cdot \mathbf{n}\|^2
  &= \sum_{K \in \mathbb{T}} \sum_{\gamma \subset \partial K}
    \frac{|K|}{|\gamma|} \|\mathbf{u} \cdot \mathbf{n} - \mathbf{u}_h \cdot \mathbf{n}\|_{L_2(\gamma)}^2,
@f}
where $| K |$ is the area of the element,
$\gamma$ are faces of the element,
$\mathbf{n}$ are unit normal vectors of each face. The last of these
norms measures the accuracy of the normal component of the velocity
vectors over the interfaces between the cells of the mesh. The scaling
factor $|K|/|\gamma|$ is chosen so as to scale out the difference in
the length (or area) of the collection of interfaces as the mesh size
changes.

The first of these errors above is easily computed using
VectorTools::integrate_difference. The others require a bit more work
and are implemented in the code below.


examples/step-61/doc/results.dox
<h1>Results</h1>

We run the program with a right hand side that will produce the
solution $p = \sin(\pi x) \sin(\pi y)$ and with homogeneous Dirichlet
boundary conditions in the domain $\Omega = (0,1)^2$. In addition, we
choose the coefficient matrix in the differential operator
$\mathbf{K}$ as the identity matrix. We test this setup using
$\mbox{WG}(Q_0,Q_0;RT_{[0]})$, $\mbox{WG}(Q_1,Q_1;RT_{[1]})$ and
$\mbox{WG}(Q_2,Q_2;RT_{[2]})$ element combinations, which one can
select by using the appropriate constructor argument for the
`WGDarcyEquation` object in `main()`. We will then visualize pressure
values in interiors of cells and on faces. We want to see that the
pressure maximum is around 1 and the minimum is around 0. With mesh
refinement, the convergence rates of pressure, velocity and flux
should then be around 1 for $\mbox{WG}(Q_0,Q_0;RT_{[0]})$ , 2 for
$\mbox{WG}(Q_1,Q_1;RT_{[1]})$, and 3 for
$\mbox{WG}(Q_2,Q_2;RT_{[2]})$.


<h3>Test results on <i>WG(Q<sub>0</sub>,Q<sub>0</sub>;RT<sub>[0]</sub>)</i></h3>

The following figures show interior pressures and face pressures using the
$\mbox{WG}(Q_0,Q_0;RT_{[0]})$ element. The mesh is refined 2 times (top)
and 4 times (bottom), respectively. (This number can be adjusted in the
`make_grid()` function.) When the mesh is coarse, one can see
the face pressures $p^\partial$ neatly between the values of the interior
pressures $p^\circ$ on the two adjacent cells.

<table align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-61.wg000_2d_2.png" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-61.wg000_3d_2.png" alt=""></td>
  </tr>
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-61.wg000_2d_4.png" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-61.wg000_3d_4.png" alt=""></td>
  </tr>
</table>

From the figures, we can see that with the mesh refinement, the maximum and
minimum pressure values are approaching the values we expect.
Since the mesh is a rectangular mesh and numbers of cells in each direction is even, we
have symmetric solutions. From the 3d figures on the right,
we can see that on $\mbox{WG}(Q_0,Q_0;RT_{[0]})$, the pressure is a constant
in the interior of the cell, as expected.

<h4>Convergence table for <i>k=0</i></h4>

We run the code with differently refined meshes (chosen in the `make_grid()` function)
and get the following convergence rates of pressure,
velocity, and flux (as defined in the introduction).

<table align="center" class="doxtable">
  <tr>
   <th>number of refinements </th><th>  $\|p-p_h^\circ\|$  </th><th>  $\|\mathbf{u}-\mathbf{u}_h\|$ </th><th> $\|(\mathbf{u}-\mathbf{u}_h) \cdot \mathbf{n}\|$ </th>
  </tr>
  <tr>
   <td>   2                  </td><td>    1.587e-01        </td><td>        5.113e-01               </td><td>   7.062e-01 </td>
  </tr>
  <tr>
   <td>   3                  </td><td>    8.000e-02        </td><td>        2.529e-01               </td><td>   3.554e-01 </td>
  </tr>
  <tr>
   <td>   4                  </td><td>    4.006e-02        </td><td>        1.260e-01               </td><td>   1.780e-01 </td>
  </tr>
  <tr>
   <td>   5                  </td><td>    2.004e-02        </td><td>        6.297e-02               </td><td>   8.902e-02 </td>
  </tr>
  <tr>
   <th>Conv.rate             </th><th>      1.00           </th><th>          1.00                  </th><th>      1.00   </th>
  </tr>
</table>

We can see that the convergence rates of $\mbox{WG}(Q_0,Q_0;RT_{[0]})$ are around 1.
This, of course, matches our theoretical expectations.


<h3>Test results on <i>WG(Q<sub>1</sub>,Q<sub>1</sub>;RT<sub>[1]</sub>)</i></h3>

We can repeat the experiment from above using the next higher polynomial
degree:
The following figures are interior pressures and face pressures implemented using
$\mbox{WG}(Q_1,Q_1;RT_{[1]})$. The mesh is refined 4 times.  Compared to the
previous figures using
$\mbox{WG}(Q_0,Q_0;RT_{[0]})$, on each cell, the solution is no longer constant
on each cell, as we now use bilinear polynomials to do the approximation.
Consequently, there are 4 pressure values in one interior, 2 pressure values on
each face.

<table align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-61.wg111_2d_4.png" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-61.wg111_3d_4.png" alt=""></td>
  </tr>
</table>

Compared to the corresponding image for the $\mbox{WG}(Q_0,Q_0;RT_{[0]})$
combination, the solution is now substantially more accurate and, in
particular so close to being continuous at the interfaces that we can
no longer distinguish the interface pressures $p^\partial$ from the
interior pressures $p^\circ$ on the adjacent cells.

<h4>Convergence table for <i>k=1</i></h4>

The following are the convergence rates of pressure, velocity, and flux
we obtain from using the $\mbox{WG}(Q_1,Q_1;RT_{[1]})$ element combination:

<table align="center" class="doxtable">
  <tr>
   <th>number of refinements </th><th>  $\|p-p_h^\circ\|$  </th><th>  $\|\mathbf{u}-\mathbf{u}_h\|$ </th><th> $\|(\mathbf{u}-\mathbf{u}_h) \cdot \mathbf{n}\|$ </th>
  </tr>
  <tr>
    <td>  2           </td><td>           1.613e-02      </td><td>          5.093e-02     </td><td>             7.167e-02   </td>
  </tr>
  <tr>
    <td>  3           </td><td>           4.056e-03      </td><td>          1.276e-02     </td><td>             1.802e-02    </td>
  </tr>
  <tr>
    <td>  4           </td><td>           1.015e-03      </td><td>          3.191e-03     </td><td>             4.512e-03  </td>
  </tr>
  <tr>
    <td>  5           </td><td>           2.540e-04      </td><td>          7.979e-04     </td><td>             1.128e-03  </td>
  </tr>
  <tr>
    <th>Conv.rate     </th><th>              2.00        </th><th>             2.00       </th><th>                 2.00    </th>
  </tr>
</table>

The convergence rates of $WG(Q_1,Q_1;RT_{[1]})$ are around 2, as expected.



<h3>Test results on <i>WG(Q<sub>2</sub>,Q<sub>2</sub>;RT<sub>[2]</sub>)</i></h3>

Let us go one polynomial degree higher.
The following are interior pressures and face pressures implemented using
$WG(Q_2,Q_2;RT_{[2]})$, with mesh size $h = 1/32$ (i.e., 5 global mesh
refinement steps). In the program, we use
`data_out_face.build_patches(fe.degree)` when generating graphical output
(see the documentation of DataOut::build_patches()), which here implies that
we divide each 2d cell interior into 4 subcells in order to provide a better
visualization of the quadratic polynomials.
<table align="center">
  <tr>
    <td><img src="https://www.dealii.org/images/steps/developer/step-61.wg222_2d_5.png" alt=""></td>
    <td><img src="https://www.dealii.org/images/steps/developer/step-61.wg222_3d_5.png" alt=""></td>
  </tr>
</table>


<h4>Convergence table for <i>k=2</i></h4>

As before, we can generate convergence data for the
$L_2$ errors of pressure, velocity, and flux
using the $\mbox{WG}(Q_2,Q_2;RT_{[2]})$ combination:

<table align="center" class="doxtable">
  <tr>
   <th>number of refinements </th><th>  $\|p-p_h^\circ\|$  </th><th>  $\|\mathbf{u}-\mathbf{u}_h\|$ </th><th> $\|(\mathbf{u}-\mathbf{u}_h) \cdot \mathbf{n}\|$ </th>
  </tr>
  <tr>
     <td>  2               </td><td>       1.072e-03       </td><td>         3.375e-03       </td><td>           4.762e-03   </td>
  </tr>
  <tr>
    <td>   3               </td><td>       1.347e-04       </td><td>         4.233e-04       </td><td>           5.982e-04    </td>
  </tr>
  <tr>
    <td>   4               </td><td>       1.685e-05      </td><td>          5.295e-05       </td><td>           7.487e-05  </td>
  </tr>
  <tr>
    <td>   5               </td><td>       2.107e-06      </td><td>          6.620e-06       </td><td>           9.362e-06  </td>
  </tr>
  <tr>
    <th>Conv.rate          </th><th>         3.00         </th><th>            3.00          </th><th>              3.00    </th>
  </tr>
</table>

Once more, the convergence rates of $\mbox{WG}(Q_2,Q_2;RT_{[2]})$ is
as expected, with values around 3.


examples/step-62/doc/intro.dox
<br>

<i>This program was contributed by Daniel Garcia-Sanchez.</i>
<br>


@note As a prerequisite of this program, you need to have HDF5, complex PETSc,
and the p4est libraries installed. The installation of deal.II
together with these additional libraries is described in the <a
href="../../readme.html" target="body">README</a> file.

<h1>Introduction</h1>
A phononic crystal is a periodic nanostructure that modifies the motion of
mechanical vibrations or [phonons](https://en.wikipedia.org/wiki/Phonon).
Phononic structures can be used to disperse, route and confine mechanical vibrations.
These structures have potential applications in
[quantum information](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.86.1391)
and have been used to study
[macroscopic quantum phenomena](https://science.sciencemag.org/content/358/6360/203).
Phononic crystals are usually fabricated in
[cleanrooms](https://en.wikipedia.org/wiki/Cleanroom).

In this tutorial we show how to a design a
[phononic superlattice cavity](https://doi.org/10.1103/PhysRevA.94.033813)
which is a particular type of phononic crystal that can be used to confine
mechanical vibrations. A phononic superlattice cavity is formed by two
[Distributed Bragg Reflector](https://en.wikipedia.org/wiki/Distributed_Bragg_reflector),
mirrors and a $\lambda/2$ cavity where $\lambda$ is the acoustic
wavelength. Acoustic DBRs are  periodic structures where a set of bilayer
stacks with contrasting physical properties (sound velocity index) is
repeated $N$ times.
Superlattice cavities are usually grown on a
[Gallium Arsenide](https://en.wikipedia.org/wiki/Gallium_arsenide)
wafer by
[Molecular Beam Epitaxy](https://en.wikipedia.org/wiki/Molecular-beam_epitaxy).
The bilayers correspond to GaAs/AlAs mirror pairs.
As shown below, the thickness of the mirror layers (brown and green) is
$\lambda/4$ and the thickness of the cavity (blue) is $\lambda/2$.

<img alt="Phononic superlattice cavity" src="https://www.dealii.org/images/steps/developer/step-62.01.svg" height="200" />

In this tutorial we calculate the
[band gap](https://en.wikipedia.org/wiki/Band_gap) and the
mechanical resonance of a phononic superlattice cavity but the code presented here
can be easily used to design and calculate other types of
[phononic crystals](https://science.sciencemag.org/content/358/6360/203).

The device is a waveguide in which the wave goes from left to right.
The simulations of this tutorial are done in 2D, but the code is dimension
independent and can be easily used with 3D simulations.
The waveguide width is equal to the $y$ dimension of the domain and the
waveguide length is equal to the $x$ dimension of the domain.
There are two regimes that depend on the waveguide width:
- Single mode: In this case the width of the structure is much
  smaller than the wavelength.
  This case can be solved either with FEM (the approach that we take here) or with
  a simple semi-analytical
  [1D transfer matrix formalism](https://en.wikipedia.org/wiki/Transfer_matrix).
- Multimode: In this case the width of the structure is larger than the wavelength.
  This case can be solved using FEM
  or with a [scattering matrix formalism](https://doi.org/10.1103/PhysRevA.94.033813).
  Although we do not study this case in this tutorial, it is very easy to reach the multimode
  regime by increasing the parameter waveguide width (`dimension_y` in the jupyter notebook).

The simulations of this tutorial are performed in the frequency domain.
To calculate the transmission spectrum, we use a
[procedure](https://meep.readthedocs.io/en/latest/Python_Tutorials/Resonant_Modes_and_Transmission_in_a_Waveguide_Cavity/)
that is commonly used in time domain [FDTD](https://en.wikipedia.org/wiki/Finite-difference_time-domain_method)
simulations. A pulse at a certain frequency is generated on the left side of the
structure and the transmitted energy is measured on the right side of the structure.
The simulation is run twice. First, we run the simulation with the phononic
structure and measure the transmitted energy:

<img alt="Phononic superlattice cavity" src="https://www.dealii.org/images/steps/developer/step-62.02.svg" height="200" />

Then, we run the simulation without the phononic structure and measure the transmitted
energy. We use the simulation without the structure for the calibration:

<img alt="Phononic superlattice cavity" src="https://www.dealii.org/images/steps/developer/step-62.03.svg" height="200" />

The transmission coefficient corresponds to the energy of the first simulation
divided by the calibration energy.
We repeat this procedure for each frequency step.


<h3>Elastic equations</h3>
What we want to simulate here is the transmission of elastic
waves. Consequently, the right description of the problem uses the
elastic equations, which in the time domain are given by
@f[
\rho\partial_{tt} u_i - \partial_j (c_{ijkl} \varepsilon_{kl}) = f_i,
\qquad i=0,1,2
@f]
where the stiffness tensor $c_{ijkl}$ depends on the spatial coordinates and
the strain is the symmetrized gradient of the displacement, given by
@f[
\varepsilon_{kl} =\frac{1}{2}(\partial_k u_l + \partial_l u_k)
@f]

[A perfectly matched layer (PML)](https://en.wikipedia.org/wiki/Perfectly_matched_layer)
can be used to truncate the solution at the boundaries.
A PML is a transformation that results in a complex coordinate
stretching.

Instead of a time domain approach, this tutorial program converts the
equations above into the frequency domain by performing a Fourier
transform with regard to the time variable.
The elastic equations in the frequency domain then read as follows
@f{eqnarray*}
\nabla\cdot(\boldsymbol{\bar\sigma} \xi \boldsymbol{\Lambda})&=&-\omega^2\rho\xi\mathbf{\bar u}\\
\boldsymbol{\bar \sigma} &=&\mathbf{C}\boldsymbol{\bar\varepsilon}\\
\boldsymbol{\bar\varepsilon}&=&\frac{1}{2}[(\nabla\mathbf{\bar{u}}\boldsymbol{\Lambda}+\boldsymbol{\Lambda}^\mathrm{T}(\nabla\mathbf{\bar{u}})^\mathrm{T})]\\
\xi &=&\prod_i^\textrm{dim}s_i\\
\boldsymbol{\Lambda} &=& \operatorname{diag}(1/s_0,1/s_1,1/s_2)\qquad\textrm{for 3D}\\
\boldsymbol{\Lambda} &=& \operatorname{diag}(1/s_0,1/s_1)\qquad\textrm{for 2D}
@f}
where the coefficients $s_i = 1+is_i'(x,y,z)$ account for the absorption.
There are 3 $s_i$ coefficients in 3D and 2 in 2D.
The imaginary par of $s_i$ is equal to zero outside the PML.
The PMLs are reflectionless only for the exact wave equations.
When the set of equations is discretized the PML is no longer reflectionless.
The reflections can be made arbitrarily small as long as the
medium is slowly varying, see
[the adiabatic theorem](https://doi.org/10.1103/PhysRevE.66.066608).
In the code a quadratic turn-on of the PML has been used.
A linear and cubic turn-on is also
[known to work](https://doi.org/10.1364/OE.16.011376).
These equations can be expanded into
@f[
-\omega^2\rho \xi  u_m - \partial_n \left(\frac{\xi}{s_n}c_{mnkl}
\varepsilon_{kl}\right) = f_m
@f]
@f[
\varepsilon_{kl} =\frac{1}{2}\left(\frac{1}{s_k}\partial_k u_l
+ \frac{1}{s_l}\partial_l u_k\right)
@f]
where summation over repeated indices (here $n$, as well as $k$ and $l$) is as always implied.
Note that the strain is no longer symmetric after applying the complex coordinate
stretching of the PML.
This set of equations can be written as
@f[
-\omega^2\rho \xi  u_m - \partial_n \left(\frac{\xi c_{mnkl}}{2s_n s_k} \partial_k u_l
+ \frac{\xi c_{mnkl}}{2s_n s_l} \partial_l u_k\right) = f_m
@f]

The same as the strain, the stress tensor is not symmetric inside the PML ($s_j\neq 0$).
Indeed the fields inside the PML are not physical.
It is useful to introduce the tensors $\alpha_{mnkl}$ and $\beta_{mnkl}$.
@f[
-\omega^2\rho \xi  u_m - \partial_n \left(\alpha_{mnkl}\partial_k u_l
+  \beta_{mnkl}\partial_l u_k\right) = f_m
@f]

We can multiply by $\varphi_m$ and integrate over the domain $\Omega$ and integrate by parts.
@f{eqnarray*}
-\omega^2\int_\Omega\rho\xi\varphi_m u_m + \int_\Omega\partial_n\varphi_m \left(\frac{\xi c_{mnkl}}{2s_n s_k} \partial_k u_l
+ \frac{\xi c_{mnkl}}{2s_n s_l} \partial_l u_k\right) = \int_\Omega\varphi_m f_m
@f}
It is this set of equations we want to solve for a set of frequencies $\omega$ in order to compute the
transmission coefficient as function of frequency.
The linear system becomes
@f{eqnarray*}
AU&=&F\\
A_{ij} &=& -\omega^2\int_\Omega\rho \xi\varphi_m^i \varphi_m^j + \int_\Omega\partial_n\varphi_m^i \left(\frac{\xi c_{mnkl}}{2s_n s_k} \partial_k \varphi_l^j
+ \frac{\xi c_{mnkl}}{2s_n s_l} \partial_l \varphi_k^j\right)\\
F_i &=& \int_\Omega\varphi_m^i f_m
@f}

<h3>Simulation parameters</h3>
In this tutorial we use a python
[jupyter notebook](https://github.com/dealii/dealii/blob/master/examples/step-62/step-62.ipynb)
to set up the parameters and run the simulation.
First we create a HDF5 file where we store the parameters and the results of
the simulation.

Each of the simulations (displacement and calibration) is stored in a separate HDF5 group:
@code{.py}
import numpy as np
import h5py
import matplotlib.pyplot as plt
import subprocess
import scipy.constants as constants
import scipy.optimize

# This considerably reduces the size of the svg data
plt.rcParams['svg.fonttype'] = 'none'

h5_file = h5py.File('results.h5', 'w')
data = h5_file.create_group('data')
displacement = data.create_group('displacement')
calibration = data.create_group('calibration')

# Set the parameters
for group in [displacement, calibration]:
    # Dimensions of the domain
    # The waveguide length is equal to dimension_x
    group.attrs['dimension_x'] = 2e-5
    # The waveguide width is equal to dimension_y
    group.attrs['dimension_y'] = 2e-8

    # Position of the probe that we use to measure the flux
    group.attrs['probe_pos_x']   = 8e-6
    group.attrs['probe_pos_y']   = 0
    group.attrs['probe_width_y'] = 2e-08

    # Number of points in the probe
    group.attrs['nb_probe_points'] = 5

    # Global refinement
    group.attrs['grid_level'] = 1

    # Cavity
    group.attrs['cavity_resonance_frequency'] = 20e9
    group.attrs['nb_mirror_pairs']            = 15

    # Material
    group.attrs['poissons_ratio'] = 0.27
    group.attrs['youngs_modulus'] = 270000000000.0
    group.attrs['material_a_rho'] = 3200
    if group == displacement:
        group.attrs['material_b_rho'] = 2000
    else:
        group.attrs['material_b_rho'] = 3200
    group.attrs['lambda'] = (group.attrs['youngs_modulus'] * group.attrs['poissons_ratio'] /
                           ((1 + group.attrs['poissons_ratio']) *
                           (1 - 2 * group.attrs['poissons_ratio'])))
    group.attrs['mu']= (group.attrs['youngs_modulus'] / (2 * (1 + group.attrs['poissons_ratio'])))

    # Force
    group.attrs['max_force_amplitude'] = 1e26
    group.attrs['force_sigma_x']       = 1e-7
    group.attrs['force_sigma_y']       = 1
    group.attrs['max_force_width_x']   = 3e-7
    group.attrs['max_force_width_y']   = 2e-8
    group.attrs['force_x_pos']         = -8e-6
    group.attrs['force_y_pos']         = 0

    # PML
    group.attrs['pml_x']            = True
    group.attrs['pml_y']            = False
    group.attrs['pml_width_x']      = 1.8e-6
    group.attrs['pml_width_y']      = 5e-7
    group.attrs['pml_coeff']        = 1.6
    group.attrs['pml_coeff_degree'] = 2

    # Frequency sweep
    group.attrs['center_frequency']    = 20e9
    group.attrs['frequency_range']     = 0.5e9
    group.attrs['start_frequency']     = group.attrs['center_frequency'] - group.attrs['frequency_range'] / 2
    group.attrs['stop_frequency']      = group.attrs['center_frequency'] + group.attrs['frequency_range'] / 2
    group.attrs['nb_frequency_points'] = 400

    # Other parameters
    if group == displacement:
        group.attrs['simulation_name'] = 'phononic_cavity_displacement'
    else:
        group.attrs['simulation_name'] = 'phononic_cavity_calibration'
    group.attrs['save_vtu_files'] = False

h5_file.close()
@endcode


examples/step-62/doc/results.dox
<h1>Results</h1>

<h3>Resonance frequency and bandgap</h3>

The results are analyzed in the
[jupyter notebook](https://github.com/dealii/dealii/blob/master/examples/step-62/step-62.ipynb)
with the following code
@code{.py}
h5_file = h5py.File('results.h5', 'r')
data = h5_file['data']

# Gaussian function that we use to fit the resonance
def resonance_f(freq, freq_m, quality_factor, max_amplitude):
    omega = 2 * constants.pi * freq
    omega_m = 2 * constants.pi * freq_m
    gamma = omega_m / quality_factor
    return max_amplitude * omega_m**2 * gamma**2 / (((omega_m**2 - omega**2)**2 + gamma**2 * omega**2))

frequency = data['displacement']['frequency'][...]
# Average the probe points
displacement = np.mean(data['displacement']['displacement'], axis=0)
calibration_displacement = np.mean(data['calibration']['displacement'], axis=0)
reflection_coefficient = displacement / calibration_displacement
reflectivity = (np.abs(np.mean(data['displacement']['displacement'][...]**2, axis=0))/
                np.abs(np.mean(data['calibration']['displacement'][...]**2, axis=0)))

try:
    x_data = frequency
    y_data = reflectivity
    quality_factor_guess = 1e3
    freq_guess = x_data[np.argmax(y_data)]
    amplitude_guess = np.max(y_data)
    fit_result, covariance = scipy.optimize.curve_fit(resonance_f, x_data, y_data,
                                                      [freq_guess, quality_factor_guess, amplitude_guess])
    freq_m = fit_result[0]
    quality_factor = np.abs(fit_result[1])
    max_amplitude = fit_result[2]
    y_data_fit = resonance_f(x_data, freq_m, quality_factor, max_amplitude)

    fig = plt.figure()
    plt.plot(frequency / 1e9, reflectivity, frequency / 1e9, y_data_fit)
    plt.xlabel('frequency (GHz)')
    plt.ylabel('amplitude^2 (a.u.)')
    plt.title('Transmission\n' + 'freq = ' + "%.7g" % (freq_guess / 1e9) + 'GHz Q = ' + "%.6g" % quality_factor)
except:
    fig = plt.figure()
    plt.plot(frequency / 1e9, reflectivity)
    plt.xlabel('frequency (GHz)')
    plt.ylabel('amplitude^2 (a.u.)')
    plt.title('Transmission')

fig = plt.figure()
plt.plot(frequency / 1e9, np.angle(reflection_coefficient))
plt.xlabel('frequency (GHz)')
plt.ylabel('phase (rad)')
plt.title('Phase (transmission coefficient)\n')

plt.show()
h5_file.close()
@endcode

A phononic cavity is characterized by the
[resonance frequency](https://en.wikipedia.org/wiki/Resonance) and the
[the quality factor](https://en.wikipedia.org/wiki/Q_factor).
The quality factor is equal to the ratio between the stored energy in the resonator and the energy
dissipated energy per cycle, which is approximately equivalent to the ratio between the
resonance frequency and the
[full width at half maximum (FWHM)](https://en.wikipedia.org/wiki/Full_width_at_half_maximum).
The FWHM is equal to the bandwidth over which the power of vibration is greater than half the
power at the resonant frequency.
@f[
Q = \frac{f_r}{\Delta f} = \frac{\omega_r}{\Delta \omega} =
2 \pi \times \frac{\text{energy stored}}{\text{energy dissipated per cycle}}
@f]

The square of the amplitude of the mechanical resonance $a^2$ as a function of the frequency
has a gaussian shape
@f[
a^2 = a_\textrm{max}^2\frac{\omega^2\Gamma^2}{(\omega_r^2-\omega^2)^2+\Gamma^2\omega^2}
@f]
where $f_r = \frac{\omega_r}{2\pi}$ is the resonance frequency and $\Gamma=\frac{\omega_r}{Q}$ is the dissipation rate.
We used the previous equation in the jupyter notebook to fit the mechanical resonance.

Given the values we have chosen for the parameters, one could estimate the resonance frequency
analytically. Indeed, this is then confirmed by what we get in this program:
the phononic superlattice cavity exhibits a mechanical resonance at 20GHz and a quality factor of 5046.
The following images show the transmission amplitude and phase as a function of frequency in the
vicinity of the resonance frequency:

<img alt="Phononic superlattice cavity" src="https://www.dealii.org/images/steps/developer/step-62.05.png" height="400" />
<img alt="Phononic superlattice cavity" src="https://www.dealii.org/images/steps/developer/step-62.06.png" height="400" />

The images above suggest that the periodic structure has its intended effect: It really only lets waves of a very
specific frequency pass through, whereas all other waves are reflected. This is of course precisely what one builds
these sorts of devices for.
But it is not quite this easy. In practice, there is really only a "band gap", i.e., the device blocks waves other than
the desired one at 20GHz only within a certain frequency range. Indeed, to find out how large this "gap" is within
which waves are blocked, we can extend the frequency range to 16 GHz through the appropriate parameters in the
input file. We then obtain the following image:

<img alt="Phononic superlattice cavity" src="https://www.dealii.org/images/steps/developer/step-62.07.png" height="400" />

What this image suggests is that in the range of around 18 to around 22 GHz, really only the waves with a frequency
of 20 GHz are allowed to pass through, but beyond this range, there are plenty of other frequencies that can pass
through the device.

<h3>Mode profile</h3>

We can inspect the mode profile with Paraview or VisIt.
As we have discussed, at resonance all the mechanical
energy is transmitted and the amplitude of motion is amplified inside the cavity.
It can be observed that the PMLs are quite effective to truncate the solution.
The following image shows the mode profile at resonance:

<img alt="Phononic superlattice cavity" src="https://www.dealii.org/images/steps/developer/step-62.08.png" height="400" />

On the other hand,  out of resonance all the mechanical energy is
reflected. The following image shows the profile at 19.75 GHz.
Note the interference between the force pulse and the reflected wave
at the position $x=-8\mu\textrm{m}$.

<img alt="Phononic superlattice cavity" src="https://www.dealii.org/images/steps/developer/step-62.09.png" height="400" />

<h3>Experimental applications</h3>

Phononic superlattice cavities find application in
[quantum optomechanics](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.86.1391).
Here we have presented the simulation of a 2D superlattice cavity,
but this code can be used as well to simulate "real world" 3D devices such as
[micropillar superlattice cavities](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.060101),
which are promising candidates to study macroscopic quantum phenomena.
The 20GHz mode of a micropillar superlattice cavity is essentially a mechanical harmonic oscillator that is very well isolated
from the environment. If the device is cooled down to 20mK in a dilution fridge, the mode would then become a
macroscopic quantum harmonic oscillator.


<h3>Possibilities for extensions</h3>

Instead of setting the parameters in the C++ file we could set the parameters
using a python script and save them in the HDF5 file that we will use for
the simulations. Then the deal.II program will read the parameters from the
HDF5 file.

@code{.py}
import numpy as np
import h5py
import matplotlib.pyplot as plt
import subprocess
import scipy.constants as constants
import scipy.optimize

# This considerably reduces the size of the svg data
plt.rcParams['svg.fonttype'] = 'none'

h5_file = h5py.File('results.h5', 'w')
data = h5_file.create_group('data')
displacement = data.create_group('displacement')
calibration = data.create_group('calibration')

# Set the parameters
for group in [displacement, calibration]:
    # Dimensions of the domain
    # The waveguide length is equal to dimension_x
    group.attrs['dimension_x'] = 2e-5
    # The waveguide width is equal to dimension_y
    group.attrs['dimension_y'] = 2e-8

    # Position of the probe that we use to measure the flux
    group.attrs['probe_pos_x']   = 8e-6
    group.attrs['probe_pos_y']   = 0
    group.attrs['probe_width_y'] = 2e-08

    # Number of points in the probe
    group.attrs['nb_probe_points'] = 5

    # Global refinement
    group.attrs['grid_level'] = 1

    # Cavity
    group.attrs['cavity_resonance_frequency'] = 20e9
    group.attrs['nb_mirror_pairs']            = 15

    # Material
    group.attrs['poissons_ratio'] = 0.27
    group.attrs['youngs_modulus'] = 270000000000.0
    group.attrs['material_a_rho'] = 3200
    if group == displacement:
        group.attrs['material_b_rho'] = 2000
    else:
        group.attrs['material_b_rho'] = 3200
    group.attrs['lambda'] = (group.attrs['youngs_modulus'] * group.attrs['poissons_ratio'] /
                           ((1 + group.attrs['poissons_ratio']) *
                           (1 - 2 * group.attrs['poissons_ratio'])))
    group.attrs['mu']= (group.attrs['youngs_modulus'] / (2 * (1 + group.attrs['poissons_ratio'])))

    # Force
    group.attrs['max_force_amplitude'] = 1e26
    group.attrs['force_sigma_x']       = 1e-7
    group.attrs['force_sigma_y']       = 1
    group.attrs['max_force_width_x']   = 3e-7
    group.attrs['max_force_width_y']   = 2e-8
    group.attrs['force_x_pos']         = -8e-6
    group.attrs['force_y_pos']         = 0

    # PML
    group.attrs['pml_x']            = True
    group.attrs['pml_y']            = False
    group.attrs['pml_width_x']      = 1.8e-6
    group.attrs['pml_width_y']      = 5e-7
    group.attrs['pml_coeff']        = 1.6
    group.attrs['pml_coeff_degree'] = 2

    # Frequency sweep
    group.attrs['center_frequency']    = 20e9
    group.attrs['frequency_range']     = 0.5e9
    group.attrs['start_frequency']     = group.attrs['center_frequency'] - group.attrs['frequency_range'] / 2
    group.attrs['stop_frequency']      = group.attrs['center_frequency'] + group.attrs['frequency_range'] / 2
    group.attrs['nb_frequency_points'] = 400

    # Other parameters
    if group == displacement:
        group.attrs['simulation_name'] = 'phononic_cavity_displacement'
    else:
        group.attrs['simulation_name'] = 'phononic_cavity_calibration'
    group.attrs['save_vtu_files'] = False

h5_file.close()
@endcode

In order to read the HDF5 parameters we have to use the
HDF5::File::FileAccessMode::open flag.
@code{.py}
      HDF5::File data_file("results.h5",
                           HDF5::File::FileAccessMode::open,
                           MPI_COMM_WORLD);
      auto       data = data_file.open_group("data");
@endcode


examples/step-63/doc/intro.dox
<br>

<i>This program was contributed by Thomas C. Clevenger and Timo Heister.

The creation of this tutorial was partially supported by NSF Award
DMS-1522191, DMS-1901529, OAC-1835452, by the Computational
Infrastructure in Geodynamics initiative (CIG), through the NSF under
Award EAR-0949446 and EAR-1550901 and The University of California -
Davis.
</i>

@dealiiTutorialDOI{10.5281/zenodo.3382899,https://zenodo.org/badge/DOI/10.5281/zenodo.3382899.svg}

<a name="Intro"></a>
<h1>Introduction</h1>

This program solves an advection-diffusion problem using a geometric multigrid
(GMG) preconditioner. The basics of this preconditioner are discussed in step-16;
here we discuss the necessary changes needed for a non-symmetric
PDE. Additionally, we introduce the idea of block smoothing (as compared to
point smoothing in step-16), and examine the effects of DoF renumbering for
additive and multiplicative smoothers.

<h3>Equation</h3>
The advection-diffusion equation is given by
@f{align*}{
-\varepsilon \Delta u + \boldsymbol{\beta}\cdot \nabla u & = f &
\text{ in } \Omega\\
u &= g & \text{ on } \partial\Omega
@f}
where $\varepsilon>0$, $\boldsymbol{\beta}$ is the <i>advection
direction</i>, and $f$ is a source. A few notes:

1. If $\boldsymbol{\beta}=\boldsymbol{0}$, this is the Laplace equation solved in step-16
(and many other places).

2. If $\varepsilon=0$ then this is the stationary advection equation solved in
step-9.

3. One can define a dimensionless number for this problem, called the
<i>Peclet number</i>: $\mathcal{P} \dealcoloneq \frac{\|\boldsymbol{\beta}\|
L}{\varepsilon}$, where $L$ is the length scale of the domain. It
characterizes the kind of equation we are
considering: If $\mathcal{P}>1$, we say the problem is
<i>advection-dominated</i>, else if $\mathcal{P}<1$ we will say the problem is
<i>diffusion-dominated</i>.

For the discussion in this tutorial we will be concerned with
advection-dominated flow. This is the complicated case: We know that
for diffusion-dominated problems, the standard Galerkin method works
just fine, and we also know that simple multigrid methods such as
those defined in step-16 are very efficient. On the other hand, for
advection-dominated problems, the standard Galerkin approach leads to
oscillatory and unstable discretizations, and simple solvers are often
not very efficient. This tutorial program is therefore intended to
address both of these issues.


<h4>Streamline diffusion</h4>

Using the standard Galerkin finite element method, for suitable test
functions $v_h$, a discrete weak form of the PDE would read
@f{align*}{
a(u_h,v_h) = F(v_h)
@f}
where
@f{align*}{
a(u_h,v_h) &= (\varepsilon \nabla v_h,\, \nabla u_h) +
(v_h,\,\boldsymbol{\beta}\cdot \nabla u_h),\\
F(v_h) &= (v_h,\,f).
@f}

Unfortunately, one typically gets oscillatory solutions with this
approach. Indeed, the following error estimate can be shown for this
formulation:
@f{align*}{
\|\nabla (u-u_h)\| \leq (1+\mathcal{P}) \inf_{v_h} \|\nabla (u-v_h)\|.
@f}
The infimum on the right can be estimated as follows if the exact
solution is sufficiently smooth:
@f{align*}{
  \inf_{v_h} \|\nabla (u-v_h)\|.
  \le
  \|\nabla (u-I_h u)\|
  \le
  h^k
  C
  \|\nabla^k u)\|
@f}
where $k$ is the polynomial degree of the finite elements used. As a
consequence, we obtain the estimate
@f{align*}{
\|\nabla (u-u_h)\|
\leq (1+\mathcal{P}) C h^k
  \|\nabla^k u)\|.
@f}
In other words, the numerical solution will converge. On the other hand,
given the definition of $\mathcal{P}$ above, we have to expect poor
numerical solutions with a large error when $\varepsilon \ll
\|\boldsymbol{\beta}\| L$, i.e., if the problem has only a small
amount of diffusion.

To combat this, we will consider the new weak form
@f{align*}{
a(u_h,\,v_h) + \sum_K (-\varepsilon \Delta u_h +
\boldsymbol{\beta}\cdot \nabla u_h-f,\,\delta_K
\boldsymbol{\beta}\cdot \nabla v_h)_K = F(v_h)
@f}
where the sum is done over all cells $K$ with the inner product taken
for each cell, and $\delta_K$ is a cell-wise constant
stabilization parameter defined in
@cite john2006discontinuity.

Essentially, adding in the
discrete strong form residual enhances the coercivity of the bilinear
form $a(\cdot,\cdot)$ which increases the stability of the discrete
solution. This method is commonly referred to as <i>streamline
diffusion</i> or <i>SUPG</i> (streamline upwind/Petrov-Galerkin).


<h3>Smoothers</h3>

One of the goals of this tutorial is to expand from using a simple
(point-wise) Gauss-Seidel (SOR) smoother that is used in step-16
(class PreconditionSOR) on each level of the multigrid hierarchy.
The term "point-wise" is traditionally used in solvers to indicate that one
solves at one "grid point" at a time; for scalar problems, this means
to use a solver that updates one unknown of the linear
system at a time, keeping all of the others fixed; one would then
iterate over all unknowns in the problem and, once done, start over again
from the first unknown until these "sweeps" converge. Jacobi,
Gauss-Seidel, and SOR iterations can all be interpreted in this way.
In the context of multigrid, one does not think of these methods as
"solvers", but as "smoothers". As such, one is not interested in
actually solving the linear system. It is enough to remove the high-frequency
part of the residual for the multigrid method to work, because that allows
restricting the solution to a coarser mesh.  Therefore, one only does a few,
fixed number of "sweeps" over all unknowns. In the code in this
tutorial this is controlled by the "Smoothing steps" parameter.

But these methods are known to converge rather slowly when used as
solvers. While as multigrid smoothers, they are surprisingly good,
they can also be improved upon. In particular, we consider
"cell-based" smoothers here as well. These methods solve for all
unknowns on a cell at once, keeping all other unknowns fixed; they
then move on to the next cell, and so on and so forth. One can think
of them as "block" versions of Jacobi, Gauss-Seidel, or SOR, but
because degrees of freedom are shared among multiple cells, these
blocks overlap and the methods are in fact
best be explained within the framework of additive and multiplicative
Schwarz methods.

In contrast to step-16, our test problem contains an advective
term. Especially with a small diffusion constant $\varepsilon$, information is
transported along streamlines in the given advection direction. This means
that smoothers are likely to be more effective if they allow information to
travel in downstream direction within a single smoother
application. If we want to solve one unknown (or block of unknowns) at
a time in the order in which these unknowns (or blocks) are
enumerated, then this information propagation property
requires reordering degrees of freedom or cells (for the cell-based smoothers)
accordingly so that the ones further upstream are treated earlier
(have lower indices) and those further downstream are treated later
(have larger indices). The influence of the ordering will be visible
in the results section.

Let us now briefly define the smoothers used in this tutorial.
For a more detailed introduction, we refer to
@cite KanschatNotesIterative and the books @cite smith2004domain and @cite toselli2006domain.
A Schwarz
preconditioner requires a decomposition
@f{align*}{
V = \sum_{j=1}^J V_j
@f}
of our finite element space $V$. Each subproblem $V_j$ also has a Ritz
projection $P_j: V \rightarrow V_j$ based on the bilinear form
$a(\cdot,\cdot)$. This projection induces a local operator $A_j$ for each
subproblem $V_j$. If $\Pi_j:V\rightarrow V_j$ is the orthogonal projector onto
$V_j$, one can show $A_jP_j=\Pi_j^TA$.

With this we can define an <i>additive Schwarz preconditioner</i> for the
operator $A$ as
@f{align*}{
 B^{-1} = \sum_{j=1}^J P_j A^{-1} = \sum_{j=1}^J A_j^{-1} \Pi_j^T.
@f}
In other words, we project our solution into each subproblem, apply the
inverse of the subproblem $A_j$, and sum the contributions up over all $j$.

Note that one can interpret the point-wise (one unknown at a time)
Jacobi method as an additive
Schwarz method by defining a subproblem $V_j$ for each degree of
freedom. Then, $A_j^{-1}$ becomes a multiplication with the inverse of a
diagonal entry of $A$.

For the "Block Jacobi" method used in this tutorial, we define a subproblem
$V_j$ for each cell of the mesh on the current level. Note that we use a
continuous finite element, so these blocks are overlapping, as degrees of
freedom on an interface between two cells belong to both subproblems. The
logic for the Schwarz operator operating on the subproblems (in deal.II they
are called "blocks") is implemented in the class RelaxationBlock. The "Block
Jacobi" method is implemented in the class RelaxationBlockJacobi. Many
aspects of the class (for example how the blocks are defined and how to invert
the local subproblems $A_j$) can be configured in the smoother data, see
RelaxationBlock::AdditionalData and DoFTools::make_cell_patches() for details.

So far, we discussed additive smoothers where the updates can be applied
independently and there is no information flowing within a single smoother
application. A <i>multiplicative Schwarz preconditioner</i> addresses this
and is defined by
@f{align*}{
 B^{-1} = \left( I- \prod_{j=1}^J \left(I-P_j\right) \right) A^{-1}.
@f}
In contrast to above, the updates on the subproblems $V_j$ are applied
sequentially. This means that the update obtained when inverting the
subproblem $A_j$ is immediately used in $A_{j+1}$. This becomes
visible when writing out the project:
@f{align*}{
 B^{-1}
 =
 \left(
   I
   -
   \left(I-P_1\right)\left(I-P_2\right)\cdots\left(I-P_J\right)
 \right)
 A^{-1}
 =
   A^{-1}
   -
   \left[ \left(I-P_1\right)
   \left[ \left(I-P_2\right)\cdots
     \left[\left(I-P_J\right) A^{-1}\right] \cdots \right] \right]
@f}

When defining the sub-spaces $V_j$ as whole blocks of degrees of
freedom, this method is implemented in the class RelaxationBlockSOR and used when you
select "Block SOR" in this tutorial. The class RelaxationBlockSOR is also
derived from RelaxationBlock. As such, both additive and multiplicative
Schwarz methods are implemented in a unified framework.

Finally, let us note that the standard Gauss-Seidel (or SOR) method can be
seen as a multiplicative Schwarz method with a subproblem for each DoF.


<h3>Test problem</h3>

We will be considering the following test problem: $\Omega =
[-1,\,1]\times[-1,\,1]\backslash B_{0.3}(0)$, i.e., a square
with a circle of radius 0.3 centered at the
origin removed. In addition, we use $\varepsilon=0.005$, $\boldsymbol{\beta} =
[-\sin(\pi/6),\,\cos(\pi/6)]$, $f=0$, and Dirichlet boundary values
@f{align*}{
g = \left\{\begin{array}{ll} 1 & \text{if } x=-1 \text{ or } y=-1,\,x\geq 0.5 \\
0 & \text{otherwise} \end{array}\right.
@f}

The following figures depict the solutions with (left) and without
(right) streamline diffusion. Without streamline diffusion we see large
oscillations around the boundary layer, demonstrating the instability
of the standard Galerkin finite element method for this problem.

<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-63-solution.png" alt="">
    </td>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-63-solution-no-sd.png" alt="">
    </td>
  </tr>
</table>


examples/step-63/doc/results.dox
<h1>Results</h1>

<h3> GMRES Iteration Numbers </h3>

The major advantage for GMG is that it is an $\mathcal{O}(n)$ method,
that is, the complexity of the problem increases linearly with the
problem size. To show then that the linear solver presented in this
tutorial is in fact $\mathcal{O}(n)$, all one needs to do is show that
the iteration counts for the GMRES solve stay roughly constant as we
refine the mesh.

Each of the following tables gives the GMRES iteration counts to reduce the
initial residual by a factor of $10^8$. We selected a sufficient number of smoothing steps
(based on the method) to get iteration numbers independent of mesh size. As
can be seen from the tables below, the method is indeed $\mathcal{O}(n)$.

<h4> DoF/Cell Renumbering </h4>

The point-wise smoothers ("Jacobi" and "SOR") get applied in the order the
DoFs are numbered on each level. We can influence this using the
DoFRenumbering namespace. The block smoothers are applied based on the
ordering we set in `setup_smoother()`. We can visualize this numbering. The
following pictures show the cell numbering of the active cells in downstream,
random, and upstream numbering (left to right):

<img src="https://www.dealii.org/images/steps/developer/step-63-cell-order.png" alt="">

Let us start with the additive smoothers. The following table shows
the number of iterations necessary to obtain convergence from GMRES:

<table align="center" class="doxtable">
<tr>
  <th></th>
  <th></th>
  <th colspan="1">$Q_1$</th>
  <th colspan="7">Smoother (smoothing steps)</th>
</tr>
<tr>
  <th></th>
  <th></th>
  <th></th>
  <th colspan="3">Jacobi (6)</th>
  <th></th>
  <th colspan="3">Block Jacobi (3)</th>
</tr>
<tr>
  <th></th>
  <th></th>
  <th></th>
  <th colspan="3">Renumbering Strategy</th>
  <th></th>
  <th colspan="3">Renumbering Strategy</th>
</tr>
<tr>
  <th>Cells</th>
  <th></th>
  <th>DoFs</th>
  <th>Downstream</th>
  <th>Random</th>
  <th>Upstream</th>
  <th></th>
  <th>Downstream</th>
  <th>Random</th>
  <th>Upstream</th>
</tr>
<tr>
  <th>32</th>
  <th></th>
  <th>48</th>
  <td>3</th>
  <td>3</th>
  <td>3</th>
  <th></th>
  <td>3</th>
  <td>3</th>
  <td>3</th>
</tr>
<tr>
  <th>128</th>
  <th></th>
  <th>160</th>
  <td>6</th>
  <td>6</th>
  <td>6</th>
  <th></th>
  <td>6</th>
  <td>6</th>
  <td>6</th>
</tr>
<tr>
  <th>512</th>
  <th></th>
  <th>576</th>
  <td>11</th>
  <td>11</th>
  <td>11</th>
  <th></th>
  <td>9</th>
  <td>9</th>
  <td>9</th>
</tr>
<tr>
  <th>2048</th>
  <th></th>
  <th>2176</th>
  <td>15</th>
  <td>15</th>
  <td>15</th>
  <th></th>
  <td>13</th>
  <td>13</th>
  <td>13</th>
</tr>
<tr>
  <th>8192</th>
  <th></th>
  <th>8448</th>
  <td>18</th>
  <td>18</th>
  <td>18</th>
  <th></th>
  <td>15</th>
  <td>15</th>
  <td>15</th>
</tr>
<tr>
  <th>32768</th>
  <th></th>
  <th>33280</th>
  <td>20</th>
  <td>20</th>
  <td>20</th>
  <th></th>
  <td>16</th>
  <td>16</th>
  <td>16</th>
</tr>
<tr>
  <th>131072</th>
  <th></th>
  <th>132096</th>
  <td>20</th>
  <td>20</th>
  <td>20</th>
  <th></th>
  <td>16</th>
  <td>16</th>
  <td>16</th>
</tr>
</table>

We see that renumbering the
DoFs/cells has no effect on convergence speed. This is because these
smoothers compute operations on each DoF (point-smoother) or cell
(block-smoother) independently and add up the results. Since we can
define these smoothers as an application of a sum of matrices, and
matrix addition is commutative, the order at which we sum the
different components will not affect the end result.

On the other hand, the situation is different for multiplicative smoothers:

<table align="center" class="doxtable">
<tr>
  <th></th>
  <th></th>
  <th colspan="1">$Q_1$</th>
  <th colspan="7">Smoother (smoothing steps)</th>
</tr>
<tr>
  <th></th>
  <th></th>
  <th></th>
  <th colspan="3">SOR (3)</th>
  <th></th>
  <th colspan="3">Block SOR (1)</th>
</tr>
<tr>
  <th></th>
  <th></th>
  <th></th>
  <th colspan="3">Renumbering Strategy</th>
  <th></th>
  <th colspan="3">Renumbering Strategy</th>
</tr>
<tr>
  <th>Cells</th>
  <th></th>
  <th>DoFs</th>
  <th>Downstream</th>
  <th>Random</th>
  <th>Upstream</th>
  <th></th>
  <th>Downstream</th>
  <th>Random</th>
  <th>Upstream</th>
</tr>
<tr>
  <th>32</th>
  <th></th>
  <th>48</th>
  <td>2</th>
  <td>2</th>
  <td>3</th>
  <th></th>
  <td>2</th>
  <td>2</th>
  <td>3</th>
</tr>
<tr>
  <th>128</th>
  <th></th>
  <th>160</th>
  <td>5</th>
  <td>5</th>
  <td>7</th>
  <th></th>
  <td>5</th>
  <td>5</th>
  <td>7</th>
</tr>
<tr>
  <th>512</th>
  <th></th>
  <th>576</th>
  <td>7</th>
  <td>9</th>
  <td>11</th>
  <th></th>
  <td>7</th>
  <td>7</th>
  <td>12</th>
</tr>
<tr>
  <th>2048</th>
  <th></th>
  <th>2176</th>
  <td>10</th>
  <td>12</th>
  <td>15</th>
  <th></th>
  <td>8</th>
  <td>10</th>
  <td>17</th>
</tr>
<tr>
  <th>8192</th>
  <th></th>
  <th>8448</th>
  <td>11</th>
  <td>15</th>
  <td>19</th>
  <th></th>
  <td>10</th>
  <td>11</th>
  <td>20</th>
</tr>
<tr>
  <th>32768</th>
  <th></th>
  <th>33280</th>
  <td>12</th>
  <td>16</th>
  <td>20</th>
  <th></th>
  <td>10</th>
  <td>12</th>
  <td>21</th>
</tr>
<tr>
  <th>131072</th>
  <th></th>
  <th>132096</th>
  <td>12</th>
  <td>16</th>
  <td>19</th>
  <th></th>
  <td>11</th>
  <td>12</th>
  <td>21</th>
</tr>
</table>

Here, we can speed up
convergence by renumbering the DoFs/cells in the advection direction,
and similarly, we can slow down convergence if we do the renumbering
in the opposite direction. This is because advection-dominated
problems have a directional flow of information (in the advection
direction) which, given the right renumbering of DoFs/cells,
multiplicative methods are able to capture.

This feature of multiplicative methods is, however, dependent on the
value of $\varepsilon$. As we increase $\varepsilon$ and the problem
becomes more diffusion-dominated, we have a more uniform propagation
of information over the mesh and there is a diminished advantage for
renumbering in the advection direction. On the opposite end, in the
extreme case of $\varepsilon=0$ (advection-only), we have a 1st-order
PDE and multiplicative methods with the right renumbering become
effective solvers: A correct downstream numbering may lead to methods
that require only a single iteration because information can be
propagated from the inflow boundary downstream, with no information
transport in the opposite direction. (Note, however, that in the case
of $\varepsilon=0$, special care must be taken for the boundary
conditions in this case).


<h4> %Point vs. block smoothers </h4>

We will limit the results to runs using the downstream
renumbering. Here is a cross comparison of all four smoothers for both
$Q_1$ and $Q_3$ elements:

<table align="center" class="doxtable">
<tr>
  <th></th>
  <td></th>
  <th colspan="1">$Q_1$</th>
  <th colspan="4">Smoother (smoothing steps)</th>
  <th></th>
  <th colspan="1">$Q_3$</th>
  <th colspan="4">Smoother (smoothing steps)</th>
</tr>
<tr>
  <th colspan="1">Cells</th>
  <td></th>
  <th colspan="1">DoFs</th>
  <th colspan="1">Jacobi (6)</th>
  <th colspan="1">Block Jacobi (3)</th>
  <th colspan="1">SOR (3)</th>
  <th colspan="1">Block SOR (1)</th>
  <th></th>
  <th colspan="1">DoFs</th>
  <th colspan="1">Jacobi (6)</th>
  <th colspan="1">Block Jacobi (3)</th>
  <th colspan="1">SOR (3)</th>
  <th colspan="1">Block SOR (1)</th>
</tr>
<tr>
  <th>32</th>
  <td></th>
  <th>48</th>
  <td>3</th>
  <td>3</th>
  <td>2</th>
  <td>2</th>
  <td></th>
  <th>336</th>
  <td>15</th>
  <td>14</th>
  <td>15</th>
  <td>6</th>
</tr>
<tr>
  <th>128</th>
  <td></th>
  <th>160</th>
  <td>6</th>
  <td>6</th>
  <td>5</th>
  <td>5</th>
  <td></th>
  <th>1248</th>
  <td>23</th>
  <td>18</th>
  <td>21</th>
  <td>9</th>
</tr>
<tr>
  <th>512</th>
  <td></th>
  <th>576</th>
  <td>11</th>
  <td>9</th>
  <td>7</th>
  <td>7</th>
  <td></th>
  <th>4800</th>
  <td>29</th>
  <td>21</th>
  <td>28</th>
  <td>9</th>
</tr>
<tr>
  <th>2048</th>
  <td></th>
  <th>2176</th>
  <td>15</th>
  <td>13</th>
  <td>10</th>
  <td>8</th>
  <td></th>
  <th>18816</th>
  <td>33</th>
  <td>22</th>
  <td>32</th>
  <td>9</th>
</tr>
<tr>
  <th>8192</th>
  <td></th>
  <th>8448</th>
  <td>18</th>
  <td>15</th>
  <td>11</th>
  <td>10</th>
  <td></th>
  <th>74496</th>
  <td>35</th>
  <td>22</th>
  <td>34</th>
  <td>10</th>
</tr>
<tr>
  <th>32768</th>
  <td></th>
  <th>33280</th>
  <td>20</th>
  <td>16</th>
  <td>12</th>
  <td>10</th>
  <td></th>
</tr>
<tr>
  <th>131072</th>
  <td></th>
  <th>132096</th>
  <td>20</th>
  <td>16</th>
  <td>12</th>
  <td>11</th>
  <td></th>
</tr>
</table>

We see that for $Q_1$, both multiplicative smoothers require a smaller
combination of smoothing steps and iteration counts than either
additive smoother. However, when we increase the degree to a $Q_3$
element, there is a clear advantage for the block smoothers in terms
of the number of smoothing steps and iterations required to
solve. Specifically, the block SOR smoother gives constant iteration
counts over the degree, and the block Jacobi smoother only sees about
a 38% increase in iterations compared to 75% and 183% for Jacobi and
SOR respectively.

<h3> Cost </h3>

Iteration counts do not tell the full story in the optimality of a one
smoother over another. Obviously we must examine the cost of an
iteration. Block smoothers here are at a disadvantage as they are
having to construct and invert a cell matrix for each cell. Here is a
comparison of solve times for a $Q_3$ element with 74,496 DoFs:

<table align="center" class="doxtable">
<tr>
  <th colspan="1">$Q_3$</th>
  <th colspan="4">Smoother (smoothing steps)</th>
</tr>
<tr>
  <th colspan="1">DoFs</th>
  <th colspan="1">Jacobi (6)</th>
  <th colspan="1">Block Jacobi (3)</th>
  <th colspan="1">SOR (3)</th>
  <th colspan="1">Block SOR (1)</th>
</tr>
<tr>
  <th>74496</th>
  <td>0.68s</th>
  <td>5.82s</th>
  <td>1.18s</th>
  <td>1.02s</th>
</tr>
</table>

The smoother that requires the most iterations (Jacobi) actually takes
the shortest time (roughly 2/3 the time of the next fastest
method). This is because all that is required to apply a Jacobi
smoothing step is multiplication by a diagonal matrix which is very
cheap. On the other hand, while SOR requires over 3x more iterations
(each with 3x more smoothing steps) than block SOR, the times are
roughly equivalent, implying that a smoothing step of block SOR is
roughly 9x slower than a smoothing step of SOR. Lastly, block Jacobi
is almost 6x more expensive than block SOR, which intuitively makes
sense from the fact that 1 step of each method has the same cost
(inverting the cell matrices and either adding or multiply them
together), and block Jacobi has 3 times the number of smoothing steps per
iteration with 2 times the iterations.


<h3> Additional points </h3>

There are a few more important points to mention:

<ol>
<li> For a mesh distributed in parallel, multiplicative methods cannot
be executed over the entire domain. This is because they operate one
cell at a time, and downstream cells can only be handled once upstream
cells have already been done. This is fine on a single processor: The
processor just goes through the list of cells one after the
other. However, in parallel, it would imply that some processors are
idle because upstream processors have not finished doing the work on
cells upstream from the ones owned by the current processor. Once the
upstream processors are done, the downstream ones can start, but by
that time the upstream processors have no work left. In other words,
most of the time during these smoother steps, most processors are in
fact idle. This is not how one obtains good parallel scalability!

One can use a hybrid method where
a multiplicative smoother is applied on each subdomain, but as you
increase the number of subdomains, the method approaches the behavior
of an additive method. This is a major disadvantage to these methods.
</li>

<li> Current research into block smoothers suggest that soon we will be
able to compute the inverse of the cell matrices much cheaper than
what is currently being done inside deal.II. This research is based on
the fast diagonalization method (dating back to the 1960s) and has
been used in the spectral community for around 20 years (see, e.g., <a
href="https://doi.org/10.1007/s10915-004-4787-3"> Hybrid
Multigrid/Schwarz Algorithms for the Spectral Element Method by Lottes
and Fischer</a>). There are currently efforts to generalize these
methods to DG and make them more robust. Also, it seems that one
should be able to take advantage of matrix-free implementations and
the fact that, in the interior of the domain, cell matrices tend to
look very similar, allowing fewer matrix inverse computations.
</li>
</ol>

Combining 1. and 2. gives a good reason for expecting that a method
like block Jacobi could become very powerful in the future, even
though currently for these examples it is quite slow.


<h3> Possibilities for extensions </h3>

<h4> Constant iterations for Q<sub>5</sub> </h4>

Change the number of smoothing steps and the smoother relaxation
parameter (set in <code>Smoother::AdditionalData()</code> inside
<code>create_smoother()</code>, only necessary for point smoothers) so
that we maintain a constant number of iterations for a $Q_5$ element.

<h4> Effectiveness of renumbering for changing epsilon </h4>

Increase/decrease the parameter "Epsilon" in the `.prm` files of the
multiplicative methods and observe for which values renumbering no
longer influences convergence speed.

<h4> Mesh adaptivity </h4>

The code is set up to work correctly with an adaptively refined mesh (the
interface matrices are created and set). Devise a suitable refinement
criterium or try the KellyErrorEstimator class.


examples/step-64/doc/intro.dox
<br>

<i>
This program was contributed by Bruno Turcksin and Daniel Arndt, Oak Ridge National Laboratory.
</i>


<h1>Introduction</h1>

This example shows how to implement a matrix-free method on the GPU using CUDA
for the Helmholtz equation with variable coefficients on a hypercube. The linear
system will be solved using the conjugate gradient method and is parallelized
 with MPI.

In the last few years, heterogeneous computing in general and GPUs in particular
have gained a lot of popularity. This is because GPUs offer better computing
capabilities and memory bandwidth than CPUs for a given power budget.
Among the architectures available in early 2019, GPUs are about 2x-3x as power
efficient than server CPUs with wide <a
href="https://en.wikipedia.org/wiki/SIMD">SIMD</a> for PDE-related
tasks. GPUs are also
the most popular architecture for machine learning. On the other hand,
GPUs are not easy to program. This program explores the deal.II
capabilities to see how efficiently such a program can be implemented.

While we have tried for the interface of the matrix-free classes for the CPU and
the GPU to be as close as possible, there are a few differences. When using
the matrix-free framework on a GPU, one must write some CUDA code. However, the
amount is fairly small and the use of CUDA is limited to a few keywords.


<h3>The test case</h3>

In this example, we consider the Helmholtz problem @f{eqnarray*} - \nabla \cdot
\nabla u + a(\mathbf x) u &=&1,\\ u &=& 0 \quad \text{on } \partial \Omega @f}
where $a(\mathbf x)$ is a variable coefficient.

We choose as domain $\Omega=[0,1]^3$ and $a(\mathbf x)=\frac{10}{0.05 +
2\|\mathbf x\|^2}$. Since the coefficient is symmetric around the origin but
the domain is not, we will end up with a non-symmetric solution.

If you've made it this far into the tutorial, you will know how the
weak formulation of this problem looks like and how, in principle, one
assembles linear systems for it. Of course, in this program we will in
fact not actually form the matrix, but rather only represent its
action when one multiplies with it.


<h3>Moving data to and from the device</h3>

GPUs (we will use the term "device" from now on to refer to the GPU) have their own memory
that is separate from the memory accessible to the CPU (we will use the term
"host" from now on). A normal calculation on the device can be divided in three
separate steps:
 -# the data is moved from the host to the device,
 -# the computation is done on the device,
 -# the result is moved back from the device to the host

The data movements can either be done explicitly by the user code or done
automatically using UVM (Unified Virtual Memory). In deal.II, only the first
method is supported. While it means an extra burden for the user, this
allows for
better control of data movement and more importantly it avoids to mistakenly run
important kernels on the host instead of the device.

The data movement in deal.II is done using LinearAlgebra::ReadWriteVector. These
vectors can be seen as buffers on the host that are used to either store data
received from the device or to send data to the device. There are two types of vectors
that can be used on the device:
- LinearAlgebra::CUDAWrappers::Vector, which is similar to the more common
Vector<Number>, and
- LinearAlgebra::distributed::Vector<Number,
MemorySpace::CUDA>, which is a regular
LinearAlgebra::distributed::Vector where we have specified which memory
space to use.

If no memory space is specified, the default is MemorySpace::Host.

Next, we show how to move data to/from the device using
LinearAlgebra::CUDAWrappers::Vector:
@code
  unsigned int size = 10;
  LinearAlgebra::ReadWriteVector<double> rw_vector(size);

  ...do something with the rw_vector...

  // Move the data to the device:
  LinearAlgebra::CUDAWrappers::Vector<double> vector_dev(size);
  vector_dev.import(rw_vector, VectorOperations::insert);

  ...do some computations on the device...

  // Move the data back to the host:
  rw_vector.import(vector_dev, VectorOperations::insert);
@endcode
Both of the vector classes used here only work on a single machine,
i.e., one memory space on a host and one on a device.

But there are cases where one wants to run a parallel computation
between multiple MPI processes on a number of machines, each of which
is equipped with GPUs. In that case, one wants to use
`LinearAlgebra::distributed::Vector<Number,MemorySpace::CUDA>`,
which is similar but the `import()` stage may involve MPI communication:
@code
  IndexSet locally_owned_dofs, locally_relevant_dofs;
  ...fill the two IndexSet objects...

  // Create the ReadWriteVector using an IndexSet instead of the size
  LinearAlgebra::ReadWriteVector<double> owned_rw_vector(locally_owned_dofs);

  ...do something with the rw_vector...

  // Move the data to the device:
  LinearAlgebra::distributed::Vector<double, MemorySpace::CUDA>
    distributed_vector_dev(locally_owned_dofs, MPI_COMM_WORLD);
  distributed_vector_dev.import(owned_rw_vector, VectorOperations::insert);

  ...do something with the dev_vector...

  // Create a ReadWriteVector with a different IndexSet:
  LinearAlgebra::ReadWriteVector<double>
    relevant_rw_vector(locally_relevant_dofs);

  // Move the data to the host, possibly using MPI communication:
  relevant_rw_vector.import(distributed_vector_dev, VectorOperations::insert);
@endcode
The `relevant_rw_vector` is an object that stores a subset of all
elements of the vector. Typically, these are the
@ref GlossLocallyRelevantDof "locally relevant DoFs",
which implies that they overlap between different MPI
processes. Consequently, the elements stored in that vector on one
machine may not coincide with the ones stored by the GPU on that
machine, requiring MPI communication to import them.

In all of these cases, while importing a vector, values can either be
inserted (using VectorOperation::insert) or added to prior content of
the vector (using VectorOperation::add).


<h3>Matrix-vector product implementation</h3>

The code necessary to evaluate the matrix-free operator on the device is very
similar to the one on the host. However, there are a few differences, the main
ones being that the `local_apply()` function in Step-37 and the loop over
quadrature points both need to be encapsulated in their own functors.


examples/step-64/doc/results.dox
<h1>Results</h1>

Since the main purpose of this tutorial is to demonstrate how to use the
CUDAWrappers::MatrixFree interface, not to compute anything useful in
itself, we just show the expected output here:
@code
Cycle 0
   Number of active cells:       8
   Number of degrees of freedom: 343
  Solved in 27 iterations.
  solution norm: 0.0205439

Cycle 1
   Number of active cells:       64
   Number of degrees of freedom: 2197
  Solved in 60 iterations.
  solution norm: 0.0205269

Cycle 2
   Number of active cells:       512
   Number of degrees of freedom: 15625
  Solved in 114 iterations.
  solution norm: 0.0205261

Cycle 3
   Number of active cells:       4096
   Number of degrees of freedom: 117649
  Solved in 227 iterations.
  solution norm: 0.0205261
@endcode

One can make two observations here: First, the norm of the numerical solution
converges, presumably to the norm of the exact (but unknown)
solution. And second, the number of iterations roughly doubles with
each refinement of the mesh. (This is in keeping with the expectation
that the number of CG iterations grows with the square root of the
condition number of the matrix; and that we know that the condition
number of the matrix of a second-order differential operation grows
like ${\cal O}(h^{-2})$.) This is of course rather inefficient, as an
optimal solver would have a number of iterations that is independent
of the size of the problem. But having such a solver would require
using a better preconditioner than the identity matrix we have used here.


<a name="extensions"></a>
<h3> Possibilities for extensions </h3>

Currently, this program uses no preconditioner at all. This is mainly
since constructing an efficient matrix-free preconditioner is
non-trivial.  However, simple choices just requiring the diagonal of
the corresponding matrix are good candidates and these can be computed
in a matrix-free way as well. Alternatively, and maybe even better,
one could extend the tutorial to use multigrid with Chebyshev
smoothers similar to step-37.


examples/step-65/doc/intro.dox

<br>

<i>
This program was contributed by Martin Kronbichler.
</i>

<a name="Intro"></a>
<h1>Introduction</h1>

This tutorial program presents an advanced manifold class,
TransfiniteInterpolationManifold, and how to work around its main
disadvantage, the relatively high cost.

<h3>Working with manifolds</h3>

<h4>What we want</h4>

In many applications, the finite element mesh must be able to represent a
relatively complex geometry. In the step-1, step-49, and step-53 tutorial
programs, some techniques to generate grids available within the deal.II
library have been introduced. Given a base mesh, deal.II is then able to
create a finer mesh by subdividing the cells into children, either uniformly
or only in selected parts of the computational domain. Besides the basic
meshing capabilities collected in the GridGenerator namespace, deal.II also
comes with interfaces to read in meshes generated by (quad- and hex-only) mesh
generators using the functions of namespace GridIn, as for example
demonstrated in step-5. A fundamental limitation of
externally generated meshes is that the information provided by the generated
cells in the mesh only consists of the position of the vertices and their
connectivity, without the context of the underlying geometry that used to be
available in the mesh generator that originally created this mesh. This
becomes problematic once the mesh is refined within deal.II and additional
points need to be placed. The step-54 tutorial program shows how to
overcome this limitation by using CAD surfaces in terms of the OpenCASCADE
library, and step-53 by providing the same kind of information
programmatically from within the source code.

Within deal.II, the placement of new points during mesh refinement or for the
definition of higher order mappings is controlled by manifold objects, see the
@ref manifold "manifold module"
for details.
To give an example, consider the following situation of a two-dimensional
annulus (with pictures taken from the manifold module). If we start with an
initial mesh of 10 cells and refine the mesh three times globally without
attaching any manifolds, we would obtain the following mesh:

@image html hypershell-nothing.png ""

The picture looks like this because, by default, deal.II only knows
where to put the vertices of child cells by averaging the locations of
the vertices of the parent cell. This yields a polygonal domain whose
faces are the edges of the original (coarse mesh) cells.
Obviously, we must attach a curved description to the boundary faces of the
triangulation to reproduce the circular shape upon mesh refinement, like in
the following picture:

@image html hypershell-boundary-only.png ""

This is better: At least the inner and outer boundaries are now
approaching real circles if we continue to refine the mesh.
However, the mesh in this picture is still not optimal for an annulus in the
sense that the <i>interior</i> lines from one cell to the next have kinks at certain vertices,
and one would rather like to use the following mesh:

@image html hypershell-all.png ""

In this last (optimal) case, which is also the default produced by
GridGenerator::hyper_shell(), the curved manifold description (in this case a
polar manifold description) is applied not only to the boundary faces, but to
the whole domain. Whenever the triangulation requests a new point, e.g., the
mid point of the edges or the cells when it refines a cell into four children,
it will place them along the respective mid points in the polar coordinate
system. By contrast, the case above where only the boundary was subject to the
polar manifold, only mid points along the boundary would be placed along the
curved description, whereas mid points in the interior would be computed by
suitable averages of the surrounding points in the Cartesian coordinate system
(see the @ref manifold "manifold module" for more details).

At this point, one might assume that curved volume descriptions are the way to
go. This is generally not wrong, though it is sometimes not so easy to
describe how exactly this should work. Here are a couple of examples:

- Imagine that the mesh above had actually been a disk, not just a ring.
  In that case the polar manifold degenerates at the origin and
  would not produce reasonable new points. In fact, defining a
  manifold description for things that are supposed "to look round"
  but might have points at or close to the origin is surprisingly very
  difficult.
- A similar thing happens at the origin
  of the three-dimensional ball when one tries to attach a spherical manifold to
  the whole volume &ndash; in this case, the computation of new manifold points
  would abort with an exception.
- CAD geometries often only describe the boundary of the domain, in a
  similar way to how we only attached a manifold to the boundary in
  the second picture above. Similarly, step-54 only uses the CAD
  geometry to generate a surface mesh (maybe because that is what is
  needed to solve the problem in question), but if one wanted to solve
  a problem in the water or the air around the ship described there,
  we would need to have a volume mesh. The question is then how
  exactly we should describe what is supposed to happen in the
  interior of the domain.

These simple examples make it clear that
for many interesting cases we must step back from the desire to have an
analytic curved description for the full volume: There will need to be
<i>some</i> kind of information that leads to curvature also in the
interior, but it must be possible to do this without actually writing
down an explicit formula that describes the kind of geometry.

So what happens if we don't do anything at all in the interior and
only describe the surface as a manifold? Sometimes, as in the ring
shown above, the result is not terrible. But sometimes it is. Consider the
case of a torus (e.g. generated with GridGenerator::torus()) with a
TorusManifold object attached to the surface only, no additional manifolds on
the interior cells and faces, and with six cells in toroidal direction before
refinement. If the mesh is refined once, we would obtain the following mesh,
shown with the upper half of the mesh clipped away:

@image html torus_no_inner_manifold.png ""

This is clearly sub-optimal. Indeed, if we had started with fewer than
the six cells shown above in toroidal direction, the mapping actually
inverts in some regions
because the new points placed along interior cells intersect with the boundary
as they are not following the circular shape along the toroidal direction. The
simple case of a torus can still be fixed because we know that the toroidal
direction follows a cylindrical coordinate system, so attaching a
TorusManifold to the surface combined with CylindricalManifold with
appropriate periodicity in toroidal direction applied to all interior entities
would produce a high-quality mesh as follows, now shown with two top cells
hidden:

@image html torus_cylindrical_inner_manifold.png ""

This mesh is pretty good, but obviously it is linked to a good description of
the volume, which we lack in other cases. Actually, there is an imperfection
also in this case, as we can see some unnatural kinks of two adjacent cells in
the interior of the domain which are hidden by the top two boundary cells, as
opposed to the following setup (the default manifolds applied by
GridGenerator::torus() and using the TransfiniteInterpolationManifold):

@image html torus_transfinite_manifold.png ""

<h3>The class TransfiniteInterpolationManifold</h3>

In order to find a better strategy, let us look at the two-dimensional disk
again (that is also the base entity rotated along the toroidal direction in
the torus). As we learned above, we can only apply the curved polar
description to the boundary (or a rim of cells sufficiently far away from the
origin) but must eventually transition to a straight description towards the
disk's center. If we use a flat manifold in the interior of the cells
(i.e., one in which new vertices are created by averaging of the
adjacent existing ones) and a
polar manifold only for the boundary of the disk, we get the following mesh
upon four global refinements:

@image html circular_mesh_only_boundary_manifold.png ""

That's not a terrible mesh. At the same time,
if you know that the original coarse mesh consisted of a single square
in the middle, with four caps around it, then it's not hard to see
every refinement step that happened to this mesh to get the picture
above.

While the triangulation class of deal.II tries to propagate information from
the boundary into the interior when creating new points, the reach of this
algorithm is limited:

@image html circular_mesh_boundary_cells.png ""

The picture above highlights those cells on the disk that are touching the
boundary and where boundary information could in principle be taken into
account when only looking at a single cell at the time. Clearly, the area
where some curvature can be taken into account gets more limited as the mesh
is refined, thus creating the seemingly irregular spots in the mesh: When
computing the center of any one of the boundary cells in the leftmost picture,
the ideal position is the mid point between the outer circle and the cell in
the middle. This is exactly what is used for the first refinement step in the
Triangulation class. However, for the second refinement all interior edges as
well as the interior cell layers can only add points according to a flat
manifold description.

At this point, we realize what would be needed to create a better mesh: For
<i>all</i> new points in <i>any</i> child cell that is created within the red shaded
layer on the leftmost picture, we want to compute the interpolation with
respect to the curvature in the area covered by the respective coarse
cell. This is achieved by adding the class TransfiniteInterpolationManifold to
the highlighted cells of the coarse grid in the leftmost panel of the figure
above. This class adheres to the general manifold interfaces, i.e., given any
set of points within its domain of definition, it can compute weighted
averages conforming to the manifold (using a formula that will be given in a
minute). These weighted averages are used whenever the mesh is refined, or
when a higher order mapping (such as MappingQGeneric or MappingC1)
is evaluated on a given cell
subject to this manifold. Using this manifold on the shaded cells of the
coarse grid of the disk (i.e., not only in the outer-most layer of
cells) produces the following mesh upon four global
steps of refinement:

@image html circular_mesh_transfinite_interpolation.png ""

There are still some kinks in the lines of this mesh, but they are
restricted to the faces between coarse mesh cells, whereas the rest of
the mesh is about as smooth as one would like. Indeed,
given a straight-sided central cell, this representation is the best possible
one as all mesh cells follow a smooth transition from the straight sides in
the square block in the interior to the circular shape on the boundary. (One
could possibly do a bit better by allowing some curvature also in the central
square block, that eventually vanishes as the center is approached.)


<h4>How it works</h4>

In the simple case of a disk with one curved and three straight edges, we can
explicitly write down how to achieve the blending of the shapes. For this, it
is useful to map the physical cell, like the top one, back to the reference
coordinate system $(\xi,\eta)\in (0,1)^2$ where we compute averages between
certain points. If we were to use a simple bilinear map spanned by four
vertices $(x_0,y_0), (x_1,y_1), (x_2,y_2), (x_3, y_3)$, the image of a point
$(\xi, \eta)\in (0,1)^2$ would be
@f{align*}{
(x,y) = (1-\xi)(1-\eta) (x_0,y_0) + \xi(1-\eta) (x_1,y_1) +
       (1-\xi)\eta  (x_2,y_2) + \xi\eta  (x_3,y_3).
@f}

For the case of the curved surface, we want to modify this formula. For the
top cell of the coarse mesh of the disk, we can assume that the points
$(x_0,y_0)$ and $(x_1,y_1)$ sit along the straight line at the lower end and
the points $(x_2,y_2)$ and $(x_3,y_3)$ are connected by a quarter circle along
the top. We would then map a point $(\xi, \eta)$ as
@f{align*}{
(x,y) = (1-\eta) \big[(1-\xi) (x_0,y_0) + \xi (x_1,y_1)\big] +
      \eta \mathbf{c}_3(\xi),
@f}
where $\mathbf{c}_3(\xi)$ is a curve that describes the $(x,y)$ coordinates of
the quarter circle in terms of an arclength parameter $\xi\in (0,1)$. This
represents a linear interpolation between the straight lower edge and the
curved upper edge of the cell, and is the basis for the picture shown above.

This formula is easily generalized to the case where all four edges are
described by a curve rather than a straight line. We call the four functions,
parameterized by a single coordinate $\xi$ or $\eta$ in the horizontal and
vertical directions, $\mathbf{c}_0, \mathbf{c}_1, \mathbf{c}_2,
\mathbf{c}_3$ for the left, right, lower, and upper edge of a
quadrilateral, respectively. The interpolation then reads
@f{align*}{
(x,y) =& (1-\xi)\mathbf{c}_0(\eta) + \xi \mathbf{c}_1(\eta)
        +(1-\eta)\mathbf{c}_2(\xi) + \eta \mathbf{c}_3(\xi)\\
       &-\big[(1-\xi)(1-\eta) (x_0,y_0) + \xi(1-\eta) (x_1,y_1) +
        (1-\xi)\eta  (x_2,y_2) + \xi\eta  (x_3,y_3)\big].
@f}

This formula assumes that the boundary curves match and coincide with the
vertices $(x_0,y_0), (x_1,y_1), (x_2,y_2), (x_3, y_3)$, e.g. $\mathbf{c}_0(0)
= (x_0,y_0)$ or $\mathbf{c}_0(1) = (x_2,y_2)$. The subtraction of the bilinear
interpolation in the second line of the formula makes sure that the prescribed
curves are followed exactly on the boundary: Along each of the four edges, we
need to subtract the contribution of the two adjacent edges evaluated in the
corners, which is then simply a vertex position. It is easy to check
that the formula for the circle above is reproduced if three of the four
curves $\mathbf{c}_i$ are straight and thus coincide with the bilinear
interpolation.

This formula, called transfinite interpolation, was introduced in 1973 by <a
href="https://doi.org/10.1002%2Fnme.1620070405">Gordon and Hall</a>. Even
though transfinite interpolation essentially only represents a linear blending
of the bounding curves, the interpolation exactly follows the boundary curves
for each real number $\xi\in (0,1)$ or $\eta\in (0,1)$, i.e., it interpolates
in an infinite number of points, which was the original motivation to label
this variant of interpolation a transfinite one by Gordon and Hall. Another
interpretation is that the transfinite interpolation interpolates from the
left and right and the top and bottom linearly, from which we need to subtract
the bilinear interpolation to ensure a unit weight in the interior of the
domain.

The transfinite interpolation is easily generalized to three spatial
dimensions. In that case, the interpolation allows to blend 6 different
surface descriptions for any of the quads of a three-dimensional cell and 12
edge descriptions for the lines of a cell. Again, to ensure a consistent map,
it is necessary to subtract the contribution of edges and add the contribution
of vertices again to make the curves follow the prescribed surface or edge
description. In the three-dimensional case, it is also possible to use a
transfinite interpolation from a curved edge both into the adjacent faces and
the adjacent cells.

The interpolation of the transfinite interpolation in deal.II is general in
the sense that it can deal with arbitrary curves. It will evaluate the curves
in terms of their original coordinates of the $d$-dimensional space but with
one (or two, in the case of edges in 3D) coordinate held fixed at $0$ or $1$ to ensure
that any other manifold class, including CAD files if desired, can be applied
out of the box. Transfinite interpolation is a standard ingredient in mesh
generators, so the main strength of the integration of this feature within the
deal.II library is to enable it during adaptive refinement and coarsening of
the mesh, and for creating higher-degree mappings that use manifolds to insert
additional points beyond the mesh vertices.

As a final remark on transfinite interpolation, we mention that the mesh
refinement strategies in deal.II in absence of a volume manifold description
are also based on the weights of the transfinite interpolation and optimal in
that sense. The difference is that the default algorithm sees only one
cell at a time, and so will apply the optimal algorithm only on those
cells touching the curved manifolds. In contrast, using the
transfinite mapping on entire <i>patches</i> of cells (originating
from one coarser cell) allows to use the transfinite interpolation
method in a way that propagates information from the boundary to cells
far away.


<h3>Transfinite interpolation is expensive and how to deal with it</h3>

A mesh with a transfinite manifold description is typically set up in two
steps. The first step is to create a coarse mesh (or read it in from a file) and to
attach a curved manifold to some of the mesh entities. For the above example
of the disk, we attach a polar manifold to the faces along the outer circle
(this is done automatically by GridGenerator::hyper_ball()). Before we start
refining the mesh, we then assign a TransfiniteInterpolationManifold to all
interior cells and edges of the mesh, which of course needs to be based on
some manifold id that we have assigned to those entities (everything except
the circle on the boundary). It does not matter whether we also assign a
TransfiniteInterpolationManifold to the inner square of the disk or not
because the transfinite interpolation on a coarse cell with straight
edges (or flat faces in 3d) simply yields subdivided children with
straight edges (flat faces).

Later, when the mesh is refined or when a higher-order mapping is set up based
on this mesh, the cells will query the underlying manifold object for new
points. This process takes a set of surrounding points, for example the four
vertices of a two-dimensional cell, and a set of weights to each of these
points, for definition a new point. For the mid point of a cell, each of the
four vertices would get weight 0.25. For the transfinite interpolation
manifold, the process of building weighted sums requires some serious work. By
construction, we want to combine the points in terms of the reference
coordinates $\xi$ and $\eta$ (or $\xi, \eta, \zeta$ in 3D) of the surrounding
points. However, the interface of the manifold classes in deal.II does not get
the reference coordinates of the surrounding points (as they are not stored
globally) but rather the physical coordinates only. Thus, the first step the
transfinite interpolation manifold has to do is to invert the mapping and find
the reference coordinates within one of the coarse cells of the transfinite
interpolation (e.g. one of the four shaded coarse-grid cells of the disk mesh
above). This inversion is done by a Newton iteration (or rather,
finite-difference based Newton scheme combined with Broyden's method) and
queries the transfinite interpolation according to the formula above several
times. Each of these queries in turn might call an expensive manifold, e.g. a
spherical description of a ball, and be expensive on its own. Since the
Manifold interface class of deal.II only provides a set of points, the
transfinite interpolation initially does not even know to which coarse grid
cell the set of surrounding points belong to and needs to search among several
cells based on some heuristics. In terms of <a
href="https://en.wikipedia.org/wiki/Atlas_(topology)#Charts">charts</a>,
one could describe the
implementation of the transfinite interpolation as an <a
href="https://en.wikipedia.org/wiki/Atlas_(topology)">atlas</a>-based
implementation: Each cell of the initial coarse grid of the triangulation
represents a chart with its own reference space, and the surrounding manifolds
provide a way to transform from the chart space (i.e., the reference cell) to
the physical space. The collection of the charts of the coarse grid cells is
an atlas, and as usual, the first thing one does when looking up something in
an atlas is to find the right chart.

Once the reference coordinates of the surrounding points have been found, a
new point in the reference coordinate system is computed by a simple weighted
sum. Finally, the reference point is inserted into the formula for the
transfinite interpolation, which gives the desired new point.

In a number of cases, the curved manifold is not only used during mesh
refinement, but also to ensure a curved representation of boundaries within
the cells of the computational domain. This is a necessity to guarantee
high-order convergence for high-order polynomials on complex geometries
anyway, but sometimes an accurate geometry is also desired with linear shape
functions. This is often done by polynomial descriptions of the cells and
called the isoparametric concept if the polynomial degree to represent the
curved mesh elements is the same as the degree of the polynomials for the
numerical solution. If the degree of the geometry is higher or lower than the
solution, one calls that a super- or sub-parametric geometry representation,
respectively. In deal.II, the standard class for polynomial representation is
MappingQGeneric. If, for example, this class is used with polynomial degree $4$ in 3D, a
total of 125 (i.e., $(4+1)^3$) points are needed for the
interpolation. Among these points, 8 are the cell's vertices and already
available from the mesh, but the other 117 need to be provided by the
manifold. In case the transfinite interpolation manifold is used, we can
imagine that going through the pull-back into reference coordinates of some
yet to be determined coarse cell, followed by subsequent push-forward on each
of the 117 points, is a lot of work and can be very time consuming.

What makes things worse is that the structure of many programs is such
that the
mapping is queried several times independently for the same cell. Its primary
use is in the assembly of the linear system, i.e., the computation of the
system matrix and the right hand side, via the `mapping` argument of the
FEValues object. However, also the interpolation of boundary values, the
computation of numerical errors, writing the output, and evaluation of error
estimators must involve the same mapping to ensure a consistent interpretation
of the solution vectors. Thus, even a linear stationary problem that is solved
once will evaluate the points of the mapping several times. For the cubic case
in 3D mentioned above, this means computing 117 points per cell by an
expensive algorithm many times. The situation is more pressing for nonlinear
or time-dependent problems where those operations are done over and over
again.

As the manifold description via a transfinite interpolation can easily be
hundreds of times more expensive than a similar query on a flat manifold, it
makes sense to compute the additional points only once and use them in all
subsequent calls. The deal.II library provides the class MappingQCache for
exactly this purpose. The cache is typically not overly big compared to the
memory consumed by a system matrix, as will become clear when looking at the
results of this tutorial program. The usage of MappingQCache is simple: Once
the mesh has been set up (or changed during refinement), we call
MappingQCache::initialize() with the desired triangulation as well as a
desired mapping as arguments. The initialization then goes through all cells
of the mesh and queries the given mapping for its additional points. Those get
stored for an identifier of the cell so that they can later be returned
whenever the mapping computes some quantities related to the cell (like the
Jacobians of the map between the reference and physical coordinates).

As a final note, we mention that the TransfiniteInterpolationManifold also
makes the refinement of the mesh more expensive. In this case, the
MappingQCache does not help because it would compute points that can
subsequently not be re-used; there currently does not exist a more
efficient mechanism in deal.II. However, the mesh refinement contains many
other expensive steps as well, so it is not as big as an issue compared to the
rest of the computation. It also only happens at most once per time
step or nonlinear iteration.

<h3>The test case</h3>

In this tutorial program, the usage of TransfiniteInterpolationManifold is
exemplified in combination with MappingQCache. The test case is relatively
simple and takes up the solution stages involved in many typical programs,
e.g., the step-6 tutorial program. As a geometry, we select one prototype use
of TransfiniteInterpolationManifold, namely a setup involving a spherical ball
that is in turn surrounded by a cube. Such a setup would be used, for example,
for a spherical inclusion embedded in a background medium, and if that
inclusion has different material properties that require that the
interface between the two materials needs to be tracked by element interfaces. A
visualization of the grid is given here:

<img src="https://www.dealii.org/images/steps/developer/step-65-mesh.png" alt="">

For this case, we want to attach a spherical description to the surface inside
the domain and use the transfinite interpolation to smoothly switch to the
straight lines of the outer cube and the cube at the center of the ball.

Within the program, we will follow a typical flow in finite element programs,
starting from the setup of DoFHandler and sparsity patterns, the assembly of a
linear system for solving the Poisson equation with a jumping coefficient, its
solution with a simple iterative method, computation of some numerical error
with VectorTools::integrate_difference() as well as an error estimator. We
record timings for each section and run the code twice. In the first run, we
hand a MappingQGeneric object to each stage of the program separately, where
points get re-computed over and over again. In the second run, we use
MappingQCache instead.


examples/step-65/doc/results.dox

<h1>Results</h1>

<h3>Program output</h3>

If we run the three-dimensional version of this program with polynomials of
degree three, we get the following program output:

@code
> make run
Scanning dependencies of target \step-65
[ 33%] Building CXX object CMakeFiles/\step-65.dir/\step-65.cc.o
[ 66%] Linking CXX executable \step-65
[ 66%] Built target \step-65
[100%] Run \step-65 with Release configuration

====== Running with the basic MappingQGeneric class ======

   Number of active cells:       6656
   Number of degrees of freedom: 181609
   Number of solver iterations:  285
   L2 error vs exact solution:   8.99339e-08
   H1 error vs exact solution:   6.45341e-06
   Max cell-wise error estimate: 0.00743406


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |      49.4s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble linear system          |         1 |       5.8s |        12% |
| Compute constraints             |         1 |     0.109s |      0.22% |
| Compute error estimator         |         1 |      16.5s |        33% |
| Compute error norms             |         1 |      9.11s |        18% |
| Solve linear system             |         1 |      9.92s |        20% |
| Write output                    |         1 |      4.85s |       9.8% |
+---------------------------------+-----------+------------+------------+

====== Running with the optimized MappingQCache class ======

   Memory consumption cache:     22.9981 MB
   Number of active cells:       6656
   Number of degrees of freedom: 181609
   Number of solver iterations:  285
   L2 error vs exact solution:   8.99339e-08
   H1 error vs exact solution:   6.45341e-06
   Max cell-wise error estimate: 0.00743406


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |      18.4s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble linear system          |         1 |      1.44s |       7.8% |
| Compute constraints             |         1 |   0.00336s |         0% |
| Compute error estimator         |         1 |     0.476s |       2.6% |
| Compute error norms             |         1 |     0.505s |       2.7% |
| Initialize mapping cache        |         1 |      4.96s |        27% |
| Solve linear system             |         1 |      9.95s |        54% |
| Write output                    |         1 |     0.875s |       4.8% |
+---------------------------------+-----------+------------+------------+

[100%] Built target run
@endcode

Before discussing the timings, we look at the memory consumption for the
MappingQCache object: Our program prints that it utilizes 23 MB of
memory. If we relate this number to the memory consumption of a single
(solution or right hand side) vector,
which is 1.5 MB (namely, 181,609 elements times 8 bytes per entry in
double precision), or to the memory consumed by the
system matrix and the sparsity pattern (which is 274 MB), we realize that it is
not an overly heavy data structure, given its benefits.

With respect to the timers, we see a clear improvement in the overall run time
of the program by a factor of 2.7. If we disregard the iterative solver, which
is the same in both cases (and not optimal, given the simple preconditioner we
use, and the fact that sparse matrix-vector products waste operations for
cubic polynomials), the advantage is a factor of almost 5. This is pretty
impressive for a linear stationary problem, and cost savings would indeed be
much more prominent for time-dependent and nonlinear problems where assembly
is called several times. If we look into the individual components, we get a
clearer picture of what is going on and why the cache is so efficient: In the
MappingQGeneric case, essentially every operation that involves a mapping take
at least 5 seconds to run. The norm computation runs two
VectorTools::integrate_difference() functions, which each take almost 5
seconds. (The computation of constraints is cheaper because it only evaluates
the mapping in cells at the boundary for the interpolation of boundary
conditions.) If we compare these 5 seconds to the time it takes to fill the
MappingQCache, which is 5.2 seconds (for all cells, not just the active ones),
it becomes obvious that the computation of the mapping support points
dominates over everything else in the MappingQGeneric case. Perhaps the most
striking result is the time for the error estimator, labeled "Compute error
estimator", where the MappingQGeneric implementation takes 17.3 seconds and
the MappingQCache variant less than 0.5 seconds. The reason why the former is
so expensive (three times more expensive than the assembly, for instance) is
that the error estimation involves evaluation of quantities over faces, where
each face in the mesh requests additional points of the mapping that in turn
go through the very expensive TransfiniteInterpolationManifold class. As there
are six faces per cell, this happens much more often than in assembly. Again,
MappingQCache nicely eliminates the repeated evaluation, aggregating all the
expensive steps involving the manifold in a single initialization call that
gets repeatedly used.


examples/step-66/doc/intro.dox
<br>

<i>
This program was contributed by Fabian Castelli.

A version of this code was presented and discussed in
@cite castelli2021numerical
G.F. Castelli: Numerical Investigation of Cahn-Hilliard-Type Phase-Field
Models for Battery Active Particles, PhD thesis, Karlsruhe Institute of
Technology (KIT), 2021. (To be published)

Fabian Castelli acknowledges financial support by the German Research
Foundation (DFG) through the Research Training Group 2218 SiMET -- Simulation
of mechano-electro-thermal processes in lithium-ion batteries, project number
281041241.

Finally Fabian Castelli would like to thank Timo Heister for the encouragement
and advice in writing this tutorial.
</i>


<a name="Intro"></a>
<h1>Introduction</h1>

The aim of this tutorial program is to demonstrate how to solve a nonlinear
problem using Newton's method within the matrix-free framework. This tutorial
combines several techniques already introduced in step-15, step-16, step-37,
step-48 and others.


<h3>Problem formulation</h3>
On the unit circle $\Omega = \bigl\{ x \in \mathbb{R}^2 : \|x\| \leq 1 \bigr\}$
we consider the following nonlinear elliptic boundary value problem subject to a
homogeneous Dirichlet boundary condition: Find a function
$u\colon\Omega\to\mathbb{R}$ such that it holds:
@f{align*}
    - \Delta u &= \exp(u) & \quad & \text{in } \Omega,\\
             u &= 0       & \quad & \text{on } \partial\Omega.
@f}
This problem is also called the <i>Gelfand problem</i> and is a typical example
for problems from combustion theory, see for example
@cite bebernes1989mathematical.


<h3>Discretization with finite elements</h3>
As usual, we first derive the weak formulation for this problem by multiplying
with a smooth test function $v\colon\Omega\to\mathbb{R}$ respecting the
boundary condition and integrating over the domain $\Omega$. Integration by
parts and putting the term from the right hand side to the left yields the weak
formulation: Find a function $u\colon\Omega\to\mathbb{R}$ such that for all
test functions $v$ it holds:
@f{align*}{
 \int_\Omega \nabla v \cdot \nabla u \,\mathrm{d}x
 -
 \int_\Omega v \exp(u) \,\mathrm{d}x
 =
 0.
@f}

Choosing the Lagrangian finite element space $V_h \dealcoloneq
\bigl\{ v \in C(\overline{\Omega}) : v|_Q \in \mathbb{Q}_p \text{ for all }
Q \in \mathcal{T}_h \bigr\} \cap H_0^1(\Omega)$, which directly incorporates
the homogeneous Dirichlet boundary condition, we can define a basis
$\{\varphi_i\}_{i=1,\dots,N}$ and thus it suffices to test only with those
basis functions. So the discrete problem reads as follows: Find $u_h\in V_h$
such that for all $i=1,\dots,N$ it holds:
@f{align*}{
 F(u_h)
 \dealcoloneq
 \int_\Omega \nabla \varphi_i \cdot \nabla u_h \,\mathrm{d}x
 -
 \int_\Omega \varphi_i \exp(u_h) \,\mathrm{d}x \stackrel{!}{=} 0.
@f}
As each finite element function is a linear combination of the basis functions
$\{\varphi_i\}_{i=1,\dots,N}$, we can identify the finite element solution by
a vector from $\mathbb{R}^N$ consisting of the unknown values in each degree of
freedom (DOF). Thus, we define the nonlinear function
$F\colon\mathbb{R}^N\to\mathbb{R}^N$ representing the discrete nonlinear
problem.

To solve this nonlinear problem we use Newton's method. So given an
initial guess $u_h^0\in V_h$, which already fulfills the Dirichlet boundary
condition, we determine a sequence of Newton steps $\bigl( u_h^n \bigr)_n$ by
successively applying the following scheme:
@f{align*}{
 &\text{Solve for } s_h^n\in V_h: \quad & F'(u_h^n)[s_h^n] &= -F(u_h^n),\\
 &\text{Update: }                       & u_h^{n+1} &= u_h^n + s_h^n.
@f}
So in each Newton step we have to solve a linear problem $A\,x = b$, where the
system matrix $A$ is represented by the Jacobian
$F'(u_h^n)[\,\cdot\,]\colon\mathbb{R}^N\to\mathbb{R}^N$ and the right hand side
$b$ by the negative residual $-F(u_h^n)$. The solution vector $x$ is in that
case the Newton update of the $n$-th Newton step. Note, that we assume an
initial guess $u_h^0$, which already fulfills the Dirichlet boundary conditions
of the problem formulation (in fact this could also be an inhomogeneous
Dirichlet boundary condition) and thus the Newton updates $s_h$ satisfy a
homogeneous Dirichlet condition.

Until now we only tested with the basis functions, however, we can also
represent any function of $V_h$ as linear combination of basis functions. More
mathematically this means, that every element of $V_h$ can be
identified with a vector $U\in\mathbb{R}^N$ via the representation formula:
$u_h = \sum_{i=1}^N U_i \varphi_i$. So using this we can give an expression for
the discrete Jacobian and the residual:
@f{align*}{
 A_{i,j} = \bigl( F'(u_h^n) \bigr)_{i,j}
 &=
 \int_\Omega \nabla\varphi_i \cdot \nabla \varphi_j \,\mathrm{d} x
 -
 \int_\Omega \varphi_i \, \exp( u_h ) \varphi_j \,\mathrm{d} x,\\
 b_{i} = \bigl( F(u_h^n) \bigr)_{i}
 &=
 \int_\Omega \nabla\varphi_i \cdot \nabla u_h^n \,\mathrm{d} x
 -
 \int_\Omega \varphi_i \, \exp( u_h^n ) \,\mathrm{d} x.
@f}
Compared to step-15 we could also have formed the Frech{\'e}t derivative of the
nonlinear function corresponding to the strong formulation of the problem and
discretized it afterwards. However, in the end we would get the same set of
discrete equations.


<h3>Numerical linear algebra</h3>
Note, how the system matrix, actually the Jacobian, depends on the previous
Newton step $A = F'(u^n)$. Hence we need to tell the function that computes
the system matrix about the solution at the last Newton step. In an
implementation with a classical <code>assemble_system()</code> function we
would gather this information from the last Newton step during assembly by the
use of the member functions FEValuesBase::get_function_values() and
FEValuesBase::get_function_gradients(). The <code>assemble_system()</code>
function would then looks like:
@code
template <int dim>
void GelfandProblem<dim>::assemble_system()
{
  system_matrix = 0;
  system_rhs    = 0;

  const QGauss<dim> quadrature_formula(fe.degree + 1);
  FEValues<dim>     fe_values(fe,
                          quadrature_formula,
                          update_values | update_gradients | update_JxW_values);

  const unsigned int n_q_points    = fe_values.n_quadrature_points;
  const unsigned int dofs_per_cell = fe_values.dofs_per_cell;

  FullMatrix<double>                   cell_matrix(dofs_per_cell);
  Vector<double>                       cell_rhs(dofs_per_cell);
  std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);

  std::vector<Tensor<1, dim>> newton_step_gradients(n_q_points);
  std::vector<double>         newton_step_values(n_q_points);


  for (const auto &cell : dof_handler.active_cell_iterators())
    {
      cell_matrix = 0.0;
      cell_rhs    = 0.0;

      fe_values.reinit(cell);

      fe_values.get_function_values(solution, newton_step_values);
      fe_values.get_function_gradients(solution, newton_step_gradients);

      for (unsigned int q = 0; q < n_q_points; ++q)
        {
          const double nonlinearity = std::exp(newton_step_values[q]);
          const double dx           = fe_values.JxW(q);

          for (unsigned int i = 0; i < dofs_per_cell; ++i)
            {
              const double         phi_i      = fe_values.shape_value(i, q);
              const Tensor<1, dim> grad_phi_i = fe_values.shape_grad(i, q);

              for (unsigned int j = 0; j < dofs_per_cell; ++j)
                {
                  const double         phi_j      = fe_values.shape_value(j, q);
                  const Tensor<1, dim> grad_phi_j = fe_values.shape_grad(j, q);

                  cell_matrix(i, j) +=
                    (grad_phi_i * grad_phi_j - phi_i * nonlinearity * phi_j) *
                    dx;
                }

              cell_rhs(i) += (-grad_phi_i * newton_step_gradients[q] +
                              phi_i * newton_step_values[q]) *
                             dx;
            }
        }

      cell->get_dof_indices(local_dof_indices);

      constraints.distribute_local_to_global(
        cell_matrix, cell_rhs, local_dof_indices, system_matrix, system_rhs);
    }
}
@endcode

Since we want to solve this problem without storing a matrix, we need to tell
the matrix-free operator this information before we use it. Therefore in the
derived class <code>JacobianOperator</code> we will implement a function
called <code>evaluate_newton_step</code>, which will process the information of
the last Newton step prior to the usage of the matrix-vector implementation.
Furthermore we want to use a geometric multigrid (GMG) preconditioner for the
linear solver, so in order to apply the multilevel operators we need to pass the
last Newton step also to these operators. This is kind of a tricky task, since
the vector containing the last Newton step has to be interpolated to all levels
of the triangulation. In the code this task will be done by the function
MGTransferMatrixFree::interpolate_to_mg(). Note, a fundamental difference to
the previous cases, where we set up and used a geometric multigrid
preconditioner, is the fact, that we can reuse the MGTransferMatrixFree object
for the computation of all Newton steps. So we can save some work here by
defining a class variable and using an already set up MGTransferMatrixFree
object <code>mg_transfer</code> that was initialized in the
<code>setup_system()</code> function.
@code
template <int dim, int fe_degree>
void GelfandProblem<dim, fe_degree>::compute_update()
{
  TimerOutput::Scope t(computing_timer, "compute update");

  solution.update_ghost_values();

  system_matrix.evaluate_newton_step(solution);

  mg_transfer.interpolate_to_mg(dof_handler, mg_solution, solution);


  // Set up options for the multilevel preconditioner
  // ...

  for (unsigned int level = 0; level < triangulation.n_global_levels(); ++level)
    {
      mg_matrices[level].evaluate_newton_step(mg_solution[level]);
    }

  // Define the actual preconditioner
  // ...

  // Solve the linear system
  // ...
}
@endcode

The function evaluating the nonlinearity works basically in the same way as the
function <code>evaluate_coefficient</code> from step-37 evaluating a coefficient
function. The idea is to use an FEEvaluation object to evaluate the Newton step
and store the expression in a table for all cells and all quadrature points:
@code
template <int dim, int fe_degree, typename number>
void JacobianOperator<dim, fe_degree, number>::evaluate_newton_step(
  const LinearAlgebra::distributed::Vector<number> &newton_step)
{
  const unsigned int n_cells = this->data->n_cell_batches();

  FEEvaluation<dim, fe_degree, fe_degree + 1, 1, number> phi(*this->data);

  nonlinear_values.reinit(n_cells, phi.n_q_points);

  for (unsigned int cell = 0; cell < n_cells; ++cell)
    {
      phi.reinit(cell);
      phi.read_dof_values_plain(newton_step);
      phi.evaluate(EvaluationFlags::values);

      for (unsigned int q = 0; q < phi.n_q_points; ++q)
        {
          nonlinear_values(cell, q) = std::exp(phi.get_value(q));
        }
    }
}
@endcode


<h3>Triangulation</h3>
As said in step-37 the matrix-free method gets more efficient if we choose a
higher order finite element space. Since we want to solve the problem on the
$d$-dimensional unit ball, it would be good to have an appropriate boundary
approximation to overcome convergence issues. For this reason we use an
isoparametric approach with the MappingQGeneric class to recover the smooth
boundary as well as the mapping for inner cells. In addition, to get a good
triangulation in total we make use of the TransfiniteInterpolationManifold.


examples/step-66/doc/results.dox
<h1>Results</h1>

The aim of this tutorial step was to demonstrate the solution of a nonlinear
PDE with the matrix-free framework.



<h3>Program output</h3>
Running the program on two processes in release mode via
@code
cmake . && make release && make && mpirun -n 2 ./step-66
@endcode
gives the following output on the console
@code
================================================================================
START DATE: 2021/5/18, TIME: 16:25:48
--------------------------------------------------------------------------------
Running with 2 MPI processes
Vectorization over 4 doubles = 256 bits (AVX), VECTORIZATION_LEVEL=2
Finite element space: FE_Q<2>(4)
================================================================================
--------------------------------------------------------------------------------
Cycle 0
--------------------------------------------------------------------------------
Set up system...
   Triangulation: 20 cells
   DoFHandler:    337 DoFs

Solve using Newton's method...
   Nstep 1, errf = 0.00380835, errx = 3.61904, it = 7
   Nstep 2, errf = 3.80167e-06, errx = 0.104353, it = 6
   Nstep 3, errf = 3.97939e-12, errx = 0.00010511, it = 4
   Nstep 4, errf = 2.28859e-13, errx = 1.07726e-10, it = 1
Convergence step 4 value 2.28859e-13 (used wall time: 0.0096409 s)

Time for setup+solve (CPU/Wall) 0.015617/0.0156447 s

Output results...
  H1 seminorm: 0.773426



+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |    0.0286s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| assemble right hand side        |         4 |  9.71e-05s |      0.34% |
| compute residual                |         4 |  0.000137s |      0.48% |
| compute update                  |         4 |   0.00901s |        32% |
| make grid                       |         1 |   0.00954s |        33% |
| setup system                    |         1 |   0.00585s |        20% |
| solve                           |         1 |   0.00966s |        34% |
+---------------------------------+-----------+------------+------------+

.
.
.

--------------------------------------------------------------------------------
Cycle 6
--------------------------------------------------------------------------------
Set up system...
   Triangulation: 81920 cells
   DoFHandler:    1311745 DoFs

Solve using Newton's method...
   Nstep 1, errf = 5.90478e-05, errx = 231.427, it = 9
   Nstep 2, errf = 5.89991e-08, errx = 6.67102, it = 6
   Nstep 3, errf = 4.28813e-13, errx = 0.0067188, it = 4
Convergence step 3 value 4.28813e-13 (used wall time: 4.82953 s)

Time for setup+solve (CPU/Wall) 6.25094/6.37174 s

Output results...
  H1 seminorm: 0.773426



+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |      9.04s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| assemble right hand side        |         3 |    0.0827s |      0.91% |
| compute residual                |         3 |    0.0909s |         1% |
| compute update                  |         3 |      4.65s |        51% |
| setup system                    |         1 |      1.54s |        17% |
| solve                           |         1 |      4.83s |        53% |
+---------------------------------+-----------+------------+------------+

================================================================================
START DATE: 2021/5/18, TIME: 16:26:00
--------------------------------------------------------------------------------
Running with 2 MPI processes
Vectorization over 4 doubles = 256 bits (AVX), VECTORIZATION_LEVEL=2
Finite element space: FE_Q<3>(4)
================================================================================

.
.
.

--------------------------------------------------------------------------------
Cycle 5
--------------------------------------------------------------------------------
Set up system...
   Triangulation: 229376 cells
   DoFHandler:    14729857 DoFs

Solve using Newton's method...
   Nstep 1, errf = 6.30096e-06, errx = 481.74, it = 8
   Nstep 2, errf = 4.25607e-10, errx = 4.14315, it = 6
   Nstep 3, errf = 7.29563e-13, errx = 0.000321775, it = 2
Convergence step 3 value 7.29563e-13 (used wall time: 133.793 s)

Time for setup+solve (CPU/Wall) 226.809/232.615 s

Output results...
  H1 seminorm: 0.588667



+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |       390s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| assemble right hand side        |         3 |      2.06s |      0.53% |
| compute residual                |         3 |      2.46s |      0.63% |
| compute update                  |         3 |       129s |        33% |
| setup system                    |         1 |      98.8s |        25% |
| solve                           |         1 |       134s |        34% |
+---------------------------------+-----------+------------+------------+
@endcode

We show the solution for the two- and three-dimensional problem in the
following figure.

<div class="twocolumn" style="width: 80%; text-align: center;">
  <div>
    <img src = "https://www.dealii.org/images/steps/developer/step-66.solution-2d.png"
     alt     = "Solution of the two-dimensional Gelfand problem."
     width   = "100%">
  </div>
  <div>
    <img src = "https://www.dealii.org/images/steps/developer/step-66.solution-3d.png"
     alt     = "Solution of the three-dimensional Gelfand problem."
     width   = "100%">
  </div>
</div>



<h3>Newton solver</h3>
In the program output above we find some interesting information about the
Newton iterations. The terminal output in each refinement cycle presents
detailed diagnostics of the Newton method, which show first of all the number
of Newton steps and for each step the norm of the residual $\|F(u_h^{n+1})\|$,
the norm of the Newton update $\|s_h^n\|$, and the number of CG iterations
<code>it</code>.

We observe that for all cases the Newton method converges in approximately
three to four steps, which shows the quadratic convergence of the Newton method
with a full step length $\alpha = 1$. However, be aware that for a badly chosen
initial guess $u_h^0$, the Newton method will also diverge quadratically.
Usually if you do not have an appropriate initial guess, you try a few damped
Newton steps with a reduced step length $\alpha < 1$ until the Newton step is
again in the quadratic convergence domain. This damping and relaxation of the
Newton step length truly requires a more sophisticated implementation of the
Newton method, which we designate to you as a possible extension of the
tutorial.

Furthermore, we see that the number of CG iterations is approximately constant
with successive mesh refinements and an increasing number of DoFs. This is of
course due to the geometric multigrid preconditioner and similar to the
observations made in other tutorials that use this method, e.g., step-16 and
step-37. Just to give an example, in the three-dimensional case after five
refinements, we have approximately 14.7 million distributed DoFs with
fourth-order Lagrangian finite elements, but the number of CG iterations is
still less than ten.

In addition, there is one more very useful optimization that we applied and
that should be mentioned here. In the <code>compute_update()</code> function we
explicitly reset the vector holding the Newton update before passing it as the
output vector to the solver. In that case we use a starting value of zero for
the CG method, which is more suitable than the previous Newton update, the
actual content of the <code>newton_update</code> before resetting, and thus
reduces the number of CG iterations by a few steps.



<h3>Possibilities for extensions</h3>
A couple of possible extensions are available concerning minor updates fo the
present code as well as a deeper numerical investigation of the Gelfand problem.

<h4>More sophisticated Newton iteration</h4>
Beside a step size controlled version of the Newton iteration as mentioned
already in step-15, one could also implement a more flexible stopping criterion
for the Newton iteration. For example one could replace the fixed tolerances
for the residual <code>TOLf</code> and for the Newton updated <code>TOLx</code>
and implement a mixed error control with a given absolute and relative
tolerance, such that the Newton iteration exists with success as, e.g.,
@f{align*}{
  \|F(u_h^{n+1})\| \leq \texttt{RelTol} \|u_h^{n+1}\| + \texttt{AbsTol}.
@f}
For more advanced applications with many nonlinear systems to solve, for
example at each time step for a time-dependent problem, it turns out that it is
not necessary to set up and assemble the Jacobian anew at every single Newton
step or even for each time step. Instead, the existing Jacobian from a previous
step can be used for the Newton iteration. The Jacobian is then only rebuilt
if, for example, the Newton iteration converges too slowly. Such an idea yields
a <a href="https://en.wikipedia.org/wiki/Quasi-Newton_method">quasi-Newton
method</a>. Admittedly, when using the matrix-free framework, the assembly of
the Jacobian is omitted anyway, but with in this way one can try to optimize
the reassembly of the geometric multigrid preconditioner. Remember that each
time the solution from the old Newton step must be distributed to all levels
and the mutligrid preconditioner must be reinitialized.

<h4>Parallel scalability and thread parallelism</h4>
In the results section of step-37 and others, the parallel scalability of the
matrix-free framework on a large number of processors has already been
demonstrated very impressively. In the nonlinear case we consider here, we note
that one of the bottlenecks could become the transfer and evaluation of the
matrix-free Jacobi operator and its multistage operators in the previous Newton
step, since we need to transfer the old solution at all stages in each step. A
first parallel scalability analysis in @cite castelli2021numerical shows quite
good strong scalability when the problem size is large enough. However, a more
detailed analysis needs to be performed for reliable results. Moreover, the
problem has been solved only with MPI so far, without using the possibilities
of shared memory parallelization with threads. Therefore, for this example, you
could try hybrid parallelization with MPI and threads, such as described in
step-48.

<h4>Comparison to matrix-based methods</h4>
Analogously to step-50 and the mentioned possible extension of step-75, you can
convince yourself which method is faster.

<h4>Eigenvalue problem</h4>
One can consider the corresponding eigenvalue problem, which is called Bratu
problem. For example, if we define a fixed eigenvalue $\lambda\in[0,6]$, we can
compute the corresponding discrete eigenfunction. You will notice that the
number of Newton steps will increase with increasing $\lambda$. To reduce the
number of Newton steps you can use the following trick: start from a certain
$\lambda$, compute the eigenfunction, increase $\lambda=\lambda +
\delta_\lambda$, and then use the previous solution as an initial guess for the
Newton iteration. In the end you can plot the $H^1(\Omega)$-norm over the
eigenvalue $\lambda \mapsto \|u_h\|_{H^1(\Omega)}$. What do you observe for
further increasing $\lambda>7$?


examples/step-67/doc/intro.dox

<br>

<i>
This program was contributed by Martin Kronbichler. Many ideas presented here
are the result of common code development with Niklas Fehn, Katharina Kormann,
Peter Munch, and Svenja Schoeder.

This work was partly supported by the German Research Foundation (DFG) through
the project "High-order discontinuous Galerkin for the exa-scale" (ExaDG)
within the priority program "Software for Exascale Computing" (SPPEXA).
</i>

<a name="Intro"></a>
<h1>Introduction</h1>

This tutorial program solves the Euler equations of fluid dynamics using an
explicit time integrator with the matrix-free framework applied to a
high-order discontinuous Galerkin discretization in space. For details about
the Euler system and an alternative implicit approach, we also refer to the
step-33 tutorial program. You might also want to look at step-69 for
an alternative approach to solving these equations.


<h3>The Euler equations</h3>

The Euler equations are a conservation law, describing the motion of a
compressible inviscid gas,
@f[
\frac{\partial \mathbf{w}}{\partial t} + \nabla \cdot \mathbf{F}(\mathbf{w}) =
\mathbf{G}(\mathbf w),
@f]
where the $d+2$ components of the solution vector are $\mathbf{w}=(\rho, \rho
u_1,\ldots,\rho u_d,E)^{\mathrm T}$. Here, $\rho$ denotes the fluid density,
${\mathbf u}=(u_1,\ldots, u_d)^\mathrm T$ the fluid velocity, and $E$ the
energy density of the gas. The velocity is not directly solved for, but rather
the variable $\rho \mathbf{u}$, the linear momentum (since this is the
conserved quantity).

The Euler flux function, a $(d+2)\times d$ matrix, is defined as
@f[
  \mathbf F(\mathbf w)
  =
  \begin{pmatrix}
  \rho \mathbf{u}\\
  \rho \mathbf{u} \otimes \mathbf{u} + \mathbb{I}p\\
  (E+p)\mathbf{u}
  \end{pmatrix}
@f]
with $\mathbb{I}$ the $d\times d$ identity matrix and $\otimes$ the outer
product; its components denote the mass, momentum, and energy fluxes, respectively.
The right hand side forcing is given by
@f[
  \mathbf G(\mathbf w)
  =
  \begin{pmatrix}
  0\\
  \rho\mathbf{g}\\
  \rho \mathbf{u} \cdot \mathbf{g}
  \end{pmatrix},
@f]
where the vector $\mathbf g$ denotes the direction and magnitude of
gravity. It could, however, also denote any other external force per unit mass
that is acting on the fluid. (Think, for example, of the electrostatic
forces exerted by an external electric field on charged particles.)

The three blocks of equations, the second involving $d$ components, describe
the conservation of mass, momentum, and energy. The pressure is not a
solution variable but needs to be expressed through a "closure relationship"
by the other variables; we here choose the relationship appropriate
for a gas with molecules composed of two atoms, which at moderate
temperatures is given by $p=(\gamma - 1) \left(E-\frac 12 \rho
\mathbf{u}\cdot \mathbf{u}\right)$ with the constant $\gamma = 1.4$.


<h3>High-order discontinuous Galerkin discretization</h3>

For spatial discretization, we use a high-order discontinuous Galerkin (DG)
discretization, using a solution expansion of the form
@f[
\mathbf{w}_h(\mathbf{x}, t) =
\sum_{j=1}^{n_\mathbf{dofs}} \boldsymbol{\varphi}_j(\mathbf{x}) {w}_j(t).
@f]
Here, $\boldsymbol{\varphi}_j$ denotes the $j$th basis function, written
in vector form with separate shape functions for the different components and
letting $w_j(t)$ go through the density, momentum, and energy variables,
respectively. In this form, the space dependence is contained in the shape
functions and the time dependence in the unknown coefficients $w_j$. As
opposed to the continuous finite element method where some shape functions
span across element boundaries, the shape functions are local to a single
element in DG methods, with a discontinuity from one element to the next. The
connection of the solution from one cell to its neighbors is instead
imposed by the numerical fluxes
specified below. This allows for some additional flexibility, for example to
introduce directionality in the numerical method by, e.g., upwinding.

DG methods are popular methods for solving problems of transport character
because they combine low dispersion errors with controllable dissipation on
barely resolved scales. This makes them particularly attractive for simulation
in the field of fluid dynamics where a wide range of active scales needs to be
represented and inadequately resolved features are prone to disturb the
important well-resolved features. Furthermore, high-order DG methods are
well-suited for modern hardware with the right implementation. At the same
time, DG methods are no silver bullet. In particular when the solution
develops discontinuities (shocks), as is typical for the Euler equations in
some flow regimes, high-order DG methods tend to oscillatory solutions, like
all high-order methods when not using flux- or slope-limiters. This is a consequence of <a
href="https://en.wikipedia.org/wiki/Godunov%27s_theorem">Godunov's theorem</a>
that states that any total variation limited (TVD) scheme that is linear (like
a basic DG discretization) can at most be first-order accurate. Put
differently, since DG methods aim for higher order accuracy, they cannot be
TVD on solutions that develop shocks. Even though some communities claim that
the numerical flux in DG methods can control dissipation, this is of limited
value unless <b>all</b> shocks in a problem align with cell boundaries. Any
shock that passes through the interior of cells will again produce oscillatory
components due to the high-order polynomials. In the finite element and DG
communities, there exist a number of different approaches to deal with shocks,
for example the introduction of artificial diffusion on troubled cells (using
a troubled-cell indicator based e.g. on a modal decomposition of the
solution), a switch to dissipative low-order finite volume methods on a
subgrid, or the addition of some limiting procedures. Given the ample
possibilities in this context, combined with the considerable implementation
effort, we here refrain from the regime of the Euler equations with pronounced
shocks, and rather concentrate on the regime of subsonic flows with wave-like
phenomena. For a method that works well with shocks (but is more expensive per
unknown), we refer to the step-69 tutorial program.

For the derivation of the DG formulation, we multiply the Euler equations with
test functions $\mathbf{v}$ and integrate over an individual cell $K$, which
gives
@f[
\left(\mathbf{v}, \frac{\partial \mathbf{w}}{\partial t}\right)_{K}
+ \left(\mathbf{v}, \nabla \cdot \mathbf{F}(\mathbf{w})\right)_{K} =
\left(\mathbf{v},\mathbf{G}(\mathbf w)\right)_{K}.
@f]

We then integrate the second term by parts, moving the divergence
from the solution slot to the test function slot, and producing an integral
over the element boundary:
@f[
\left(\mathbf{v}, \frac{\partial \mathbf{w}}{\partial t}\right)_{K}
- \left(\nabla \mathbf{v}, \mathbf{F}(\mathbf{w})\right)_{K}
+ \left<\mathbf{v}, \mathbf{n} \cdot \widehat{\mathbf{F}}(\mathbf{w})
\right>_{\partial K} =
\left(\mathbf{v},\mathbf{G}(\mathbf w)\right)_{K}.
@f]
In the surface integral, we have replaced the term $\mathbf{F}(\mathbf w)$ by
the term $\widehat{\mathbf{F}}(\mathbf w)$, the numerical flux. The role of
the numerical flux is to connect the solution on neighboring elements and
weakly impose continuity of the solution. This ensures that the global
coupling of the PDE is reflected in the discretization, despite independent
basis functions on the cells. The connectivity to the neighbor is included by
defining the numerical flux as a function $\widehat{\mathbf{F}}(\mathbf w^-,
\mathbf w^+)$ of the solution from both sides of an interior face, $\mathbf
w^-$ and $\mathbf w^+$. A basic property we require is that the numerical flux
needs to be <b>conservative</b>. That is, we want all information (i.e.,
mass, momentum, and energy) that leaves a cell over
a face to enter the neighboring cell in its entirety and vice versa. This can
be expressed as $\widehat{\mathbf{F}}(\mathbf w^-, \mathbf w^+) =
\widehat{\mathbf{F}}(\mathbf w^+, \mathbf w^-)$, meaning that the numerical
flux evaluates to the same result from either side. Combined with the fact
that the numerical flux is multiplied by the unit outer normal vector on the
face under consideration, which points in opposite direction from the two
sides, we see that the conservation is fulfilled. An alternative point of view
of the numerical flux is as a single-valued intermediate state that links the
solution weakly from both sides.

There is a large number of numerical flux functions available, also called
Riemann solvers. For the Euler equations, there exist so-called exact Riemann
solvers -- meaning that the states from both sides are combined in a way that
is consistent with the Euler equations along a discontinuity -- and
approximate Riemann solvers, which violate some physical properties and rely
on other mechanisms to render the scheme accurate overall. Approximate Riemann
solvers have the advantage of beging cheaper to compute. Most flux functions
have their origin in the finite volume community, which are similar to DG
methods with polynomial degree 0 within the cells (called volumes). As the
volume integral of the Euler operator $\mathbf{F}$ would disappear for
constant solution and test functions, the numerical flux must fully represent
the physical operator, explaining why there has been a large body of research
in that community. For DG methods, consistency is guaranteed by higher order
polynomials within the cells, making the numerical flux less of an issue and
usually affecting only the convergence rate, e.g., whether the solution
converges as $\mathcal O(h^p)$, $\mathcal O(h^{p+1/2})$ or $\mathcal
O(h^{p+1})$ in the $L_2$ norm for polynomials of degree $p$. The numerical
flux can thus be seen as a mechanism to select more advantageous
dissipation/dispersion properties or regarding the extremal eigenvalue of the
discretized and linearized operator, which affect the maximal admissible time
step size in explicit time integrators.

In this tutorial program, we implement two variants of fluxes that can be
controlled via a switch in the program (of course, it would be easy to make
them a run time parameter controlled via an input file). The first flux is
the local Lax--Friedrichs flux
@f[
\hat{\mathbf{F}}(\mathbf{w}^-,\mathbf{w}^+) =
\frac{\mathbf{F}(\mathbf{w}^-)+\mathbf{F}(\mathbf{w}^+)}{2} +
   \frac{\lambda}{2}\left[\mathbf{w}^--\mathbf{w}^+\right]\otimes
   \mathbf{n^-}.
@f]

In the original definition of the Lax--Friedrichs flux, a factor $\lambda =
\max\left(\|\mathbf{u}^-\|+c^-, \|\mathbf{u}^+\|+c^+\right)$ is used
(corresponding to the maximal speed at which information is moving on
the two sides of the interface), stating
that the difference between the two states, $[\![\mathbf{w}]\!]$ is penalized
by the largest eigenvalue in the Euler flux, which is $\|\mathbf{u}\|+c$,
where $c=\sqrt{\gamma p / \rho}$ is the speed of sound. In the implementation
below, we modify the penalty term somewhat, given that the penalty is of
approximate nature anyway. We use
@f{align*}{
\lambda
&=
\frac{1}{2}\max\left(\sqrt{\|\mathbf{u^-}\|^2+(c^-)^2},
                     \sqrt{\|\mathbf{u}^+\|^2+(c^+)^2}\right)
\\
&=
\frac{1}{2}\sqrt{\max\left(\|\mathbf{u^-}\|^2+(c^-)^2,
                           \|\mathbf{u}^+\|^2+(c^+)^2\right)}.
@f}
The additional factor $\frac 12$ reduces the penalty strength (which results
in a reduced negative real part of the eigenvalues, and thus increases the
admissible time step size). Using the squares within the sums allows us to
reduce the number of expensive square root operations, which is 4 for the
original Lax--Friedrichs definition, to a single one.
This simplification leads to at most a factor of
2 in the reduction of the parameter $\lambda$, since $\|\mathbf{u}\|^2+c^2 \leq
\|\mathbf{u}\|^2+2 c |\mathbf{u}\| + c^2 = \left(\|\mathbf{u}\|+c\right)^2
\leq 2 \left(\|\mathbf{u}\|^2+c^2\right)$, with the last inequality following
from Young's inequality.

The second numerical flux is one proposed by Harten, Lax and van Leer, called
the HLL flux. It takes the different directions of propagation of the Euler
equations into account, depending on the speed of sound. It utilizes some
intermediate states $\bar{\mathbf{u}}$ and $\bar{c}$ to define the two
branches $s^\mathrm{p} = \max\left(0, \bar{\mathbf{u}}\cdot \mathbf{n} +
\bar{c}\right)$ and $s^\mathrm{n} = \min\left(0, \bar{\mathbf{u}}\cdot
\mathbf{n} - \bar{c}\right)$. From these branches, one then defines the flux
@f[
\hat{\mathbf{F}}(\mathbf{w}^-,\mathbf{w}^+) =
\frac{s^\mathrm{p} \mathbf{F}(\mathbf{w}^-)-s^\mathrm{n} \mathbf{F}(\mathbf{w}^+)}
                   {s^\mathrm p - s^\mathrm{n} } +
\frac{s^\mathrm{p} s^\mathrm{n}}{s^\mathrm{p}-s^\mathrm{n}}
\left[\mathbf{w}^--\mathbf{w}^+\right]\otimes \mathbf{n^-}.
@f]
Regarding the definition of the intermediate state $\bar{\mathbf{u}}$ and
$\bar{c}$, several variants have been proposed. The variant originally
proposed uses a density-averaged definition of the velocity, $\bar{\mathbf{u}}
= \frac{\sqrt{\rho^-} \mathbf{u}^- + \sqrt{\rho^+}\mathbf{u}^+}{\sqrt{\rho^-}
+ \sqrt{\rho^+}}$. Since we consider the Euler equations without shocks, we
simply use arithmetic means, $\bar{\mathbf{u}} = \frac{\mathbf{u}^- +
\mathbf{u}^+}{2}$ and $\bar{c} = \frac{c^- + c^+}{2}$, with $c^{\pm} =
\sqrt{\gamma p^{\pm} / \rho^{\pm}}$, in this tutorial program, and leave other
variants to a possible extension. We also note that the HLL flux has been
extended in the literature to the so-called HLLC flux, where C stands for the
ability to represent contact discontinuities.

At the boundaries with no neighboring state $\mathbf{w}^+$ available, it is
common practice to deduce suitable exterior values from the boundary
conditions (see the general literature on DG methods for details). In this
tutorial program, we consider three types of boundary conditions, namely
<b>inflow boundary conditions</b> where all components are prescribed,
@f[
\mathbf{w}^+ = \begin{pmatrix} \rho_\mathrm{D}(t)\\
(\rho \mathbf u)_{\mathrm D}(t) \\ E_\mathrm{D}(t)\end{pmatrix} \quad
 \text{(Dirichlet)},
@f]
<b>subsonic outflow boundaries</b>, where we do not prescribe exterior
solutions as the flow field is leaving the domain and use the interior values
instead; we still need to prescribe the energy as there is one incoming
characteristic left in the Euler flux,
@f[
\mathbf{w}^+ = \begin{pmatrix} \rho^-\\
(\rho \mathbf u)^- \\ E_\mathrm{D}(t)\end{pmatrix} \quad
 \text{(mixed Neumann/Dirichlet)},
@f]
and <b>wall boundary condition</b> which describe a no-penetration
configuration:
@f[
\mathbf{w}^+ = \begin{pmatrix} \rho^-\\
(\rho \mathbf u)^- - 2 [(\rho \mathbf u)^-\cdot \mathbf n] \mathbf{n}
 \\ E^-\end{pmatrix}.
@f]

The polynomial expansion of the solution is finally inserted to the weak form
and test functions are replaced by the basis functions. This gives a discrete
in space, continuous in time nonlinear system with a finite number of unknown
coefficient values $w_j$, $j=1,\ldots,n_\text{dofs}$. Regarding the choice of
the polynomial degree in the DG method, there is no consensus in literature as
of 2019 as to what polynomial degrees are most efficient and the decision is
problem-dependent. Higher order polynomials ensure better convergence rates
and are thus superior for moderate to high accuracy requirements for
<b>smooth</b> solutions. At the same time, the volume-to-surface ratio
of where degrees of freedom are located,
increases with higher degrees, and this makes the effect of the numerical flux
weaker, typically reducing dissipation. However, in most of the cases the
solution is not smooth, at least not compared to the resolution that can be
afforded. This is true for example in incompressible fluid dynamics,
compressible fluid dynamics, and the related topic of wave propagation. In this
pre-asymptotic regime, the error is approximately proportional to the
numerical resolution, and other factors such as dispersion errors or the
dissipative behavior become more important. Very high order methods are often
ruled out because they come with more restrictive CFL conditions measured
against the number of unknowns, and they are also not as flexible when it
comes to representing complex geometries. Therefore, polynomial degrees
between two and six are most popular in practice, see e.g. the efficiency
evaluation in @cite FehnWallKronbichler2019 and references cited therein.

<h3>Explicit time integration</h3>

To discretize in time, we slightly rearrange the weak form and sum over all
cells:
@f[
\sum_{K \in \mathcal T_h} \left(\boldsymbol{\varphi}_i,
\frac{\partial \mathbf{w}}{\partial t}\right)_{K}
=
\sum_{K\in \mathcal T_h}
\left[
\left(\nabla \boldsymbol{\varphi}_i, \mathbf{F}(\mathbf{w})\right)_{K}
-\left<\boldsymbol{\varphi}_i,
\mathbf{n} \cdot \widehat{\mathbf{F}}(\mathbf{w})\right>_{\partial K} +
\left(\boldsymbol{\varphi}_i,\mathbf{G}(\mathbf w)\right)_{K}
\right],
@f]
where $\boldsymbol{\varphi}_i$ runs through all basis functions with from 1 to
$n_\text{dofs}$.

We now denote by $\mathcal M$ the mass matrix with entries $\mathcal M_{ij} =
\sum_{K} \left(\boldsymbol{\varphi}_i,
\boldsymbol{\varphi}_j\right)_K$, and by
@f[
\mathcal L_h(t,\mathbf{w}_h) = \left[\sum_{K\in \mathcal T_h}
\left[
\left(\nabla \boldsymbol{\varphi}_i, \mathbf{F}(\mathbf{w}_h)\right)_{K}
- \left<\boldsymbol{\varphi}_i,
\mathbf{n} \cdot \widehat{\mathbf{F}}(\mathbf{w}_h)\right>_{\partial K}
+ \left(\boldsymbol{\varphi}_i,\mathbf{G}(\mathbf w_h)\right)_{K}
\right]\right]_{i=1,\ldots,n_\text{dofs}}.
@f]
the operator evaluating the right-hand side of the Euler operator, given a
function $\mathbf{w}_h$ associated with a global vector of unknowns
and the finite element in use. This function $\mathcal L_h$ is explicitly time-dependent as the
numerical flux evaluated at the boundary will involve time-dependent data
$\rho_\mathrm{D}$, $(\rho \mathbf{u})_\mathrm{D}$, and $E_\mathbf{D}$ on some
parts of the boundary, depending on the assignment of boundary
conditions. With this notation, we can write the discrete in space, continuous
in time system compactly as
@f[
\mathcal M \frac{\partial \mathbf{w}_h}{\partial t} =
\mathcal L_h(t, \mathbf{w}_h),
@f]
where we have taken the liberty to also denote the global solution
vector by $\mathbf{w}_h$ (in addition to the the corresponding finite
element function). Equivalently, the system above has the form
@f[
\frac{\partial \mathbf{w}_h}{\partial t} =
\mathcal M^{-1} \mathcal L_h(t, \mathbf{w}_h).
@f]

For hyperbolic systems discretized by high-order discontinuous Galerkin
methods, explicit time integration of this system is very popular. This is due
to the fact that the mass matrix $\mathcal M$ is block-diagonal (with each
block corresponding to only variables of the same kind defined on the same
cell) and thus easily inverted. In each time step -- or stage of a
Runge--Kutta scheme -- one only needs to evaluate the differential operator
once using the given data and subsequently apply the inverse of the mass
matrix. For implicit time stepping, on the other hand, one would first have to
linearize the equations and then iteratively solve the linear system, which
involves several residual evaluations and at least a dozen applications of
the linearized operator, as has been demonstrated in the step-33 tutorial
program.

Of course, the simplicity of explicit time stepping comes with a price, namely
conditional stability due to the so-called Courant--Friedrichs--Lewy (CFL)
condition. It states that the time step cannot be larger than the fastest
propagation of information by the discretized differential operator. In more
modern terms, the speed of propagation corresponds to the largest eigenvalue
in the discretized operator, and in turn depends on the mesh size, the
polynomial degree $p$ and the physics of the Euler operator, i.e., the
eigenvalues of the linearization of $\mathbf F(\mathbf w)$ with respect to
$\mathbf{w}$. In this program, we set the time step as follows:
@f[
\Delta t = \frac{\mathrm{Cr}}{p^{1.5}}\left(\frac{1}
           {\max\left[\frac{\|\mathbf{u}\|}{h_u} + \frac{c}{h_c}\right]}\right),
@f]

with the maximum taken over all quadrature points and all cells. The
dimensionless number $\mathrm{Cr}$ denotes the Courant number and can be
chosen up to a maximally stable number $\mathrm{Cr}_\text{max}$, whose value
depends on the selected time stepping method and its stability properties. The
power $p^{1.5}$ used for the polynomial scaling is heuristic and represents
the closest fit for polynomial degrees between 1 and 8, see e.g.
@cite SchoederKormann2018. In the limit of higher degrees, $p>10$, a scaling of
$p^2$ is more accurate, related to the inverse estimates typically used for
interior penalty methods. Regarding the <i>effective</i> mesh sizes $h_u$ and
$h_c$ used in the formula, we note that the convective transport is
directional. Thus an appropriate scaling is to use the element length in the
direction of the velocity $\mathbf u$. The code below derives this scaling
from the inverse of the Jacobian from the reference to real cell, i.e., we
approximate $\frac{\|\mathbf{u}\|}{h_u} \approx \|J^{-1} \mathbf
u\|_{\infty}$. The acoustic waves, instead, are isotropic in character, which
is why we use the smallest feature size, represented by the smallest singular
value of $J$, for the acoustic scaling $h_c$. Finally, we need to add the
convective and acoustic limits, as the Euler equations can transport
information with speed $\|\mathbf{u}\|+c$.

In this tutorial program, we use a specific variant of <a
href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods">explicit
Runge--Kutta methods</a>, which in general use the following update procedure
from the state $\mathbf{w}_h^{n}$ at time $t^n$ to the new time $t^{n+1}$ with
$\Delta t = t^{n+1}-t^n$:
@f[
\begin{aligned}
\mathbf{k}_1 &= \mathcal M^{-1} \mathcal L_h\left(t^n, \mathbf{w}_h^n\right),
\\
\mathbf{k}_2 &= \mathcal M^{-1} \mathcal L_h\left(t^n+c_2\Delta t,
                       \mathbf{w}_h^n + a_{21} \Delta t \mathbf{k}_1\right),
\\
&\vdots \\
\mathbf{k}_s &= \mathcal M^{-1} \mathcal L_h\left(t^n+c_s\Delta t,
  \mathbf{w}_h^n + \sum_{j=1}^{s-1} a_{sj} \Delta t \mathbf{k}_j\right),
\\
\mathbf{w}_h^{n+1} &= \mathbf{w}_h^n + \Delta t\left(b_1 \mathbf{k}_1 +
b_2 \mathbf{k}_2 + \ldots + b_s \mathbf{k}_s\right).
\end{aligned}
@f]
The vectors $\mathbf{k}_i$, $i=1,\ldots,s$, in an $s$-stage scheme are
evaluations of the operator at some intermediate state and used to define the
end-of-step value $\mathbf{w}_h^{n+1}$ via some linear combination. The scalar
coefficients in this scheme, $c_i$, $a_{ij}$, and $b_j$, are defined such that
certain conditions are satisfied for higher order schemes, the most basic one
being $c_i = \sum_{j=1}^{i-1}a_{ij}$. The parameters are typically collected in
the form of a so-called <a
href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#Explicit_Runge%E2%80%93Kutta_methods">Butcher
tableau</a> that collects all of the coefficients that define the
scheme. For a five-stage scheme, it would look like this:
@f[
\begin{array}{c|ccccc}
0 \\
c_2 & a_{21} \\
c_3 & a_{31} & a_{32} \\
c_4 & a_{41} & a_{42} & a_{43} \\
c_5 & a_{51} & a_{52} & a_{53} & a_{54} \\
\hline
& b_1 & b_2 & b_3 & b_4 & b_5
\end{array}
@f]

In this tutorial program, we use a subset of explicit Runge--Kutta methods,
so-called low-storage Runge--Kutta methods (LSRK), which assume additional
structure in the coefficients. In the variant used by reference
@cite KennedyCarpenterLewis2000, the assumption is to use Butcher tableaus of
the form
@f[
\begin{array}{c|ccccc}
0 \\
c_2 & a_1 \\
c_3 & b_1 & a_2 \\
c_4 & b_1 & b_2 & a_3 \\
c_5 & b_1 & b_2 & b_3 & a_4 \\
\hline
& b_1 & b_2 & b_3 & b_4 & b_5
\end{array}
@f]
With such a definition, the update to $\mathbf{w}_h^n$ shares the storage with
the information for the intermediate values $\mathbf{k}_i$. Starting with
$\mathbf{w}^{n+1}=\mathbf{w}^n$ and $\mathbf{r}_1 = \mathbf{w}^n$, the update
in each of the $s$ stages simplifies to
@f[
\begin{aligned}
\mathbf{k}_i &=
\mathcal M^{-1} \mathcal L_h\left(t^n+c_i\Delta t, \mathbf{r}_{i} \right),\\
\mathbf{r}_{i+1} &= \mathbf{w}_h^{n+1} + \Delta t \, a_i \mathbf{k}_i,\\
\mathbf{w}_h^{n+1} &= \mathbf{w}_h^{n+1} + \Delta t \, b_i \mathbf{k}_i.
\end{aligned}
@f]
Besides the vector $\mathbf w_h^{n+1}$ that is successively updated, this scheme
only needs two auxiliary vectors, namely the vector $\mathbf{k}_i$ to hold the
evaluation of the differential operator, and the vector $\mathbf{r}_i$ that
holds the right-hand side for the differential operator application. In
subsequent stages $i$, the values $\mathbf{k}_i$ and $\mathbf{r}_i$ can use
the same storage.

The main advantages of low-storage variants are the reduced memory consumption
on the one hand (if a very large number of unknowns must be fit in memory,
holding all $\mathbf{k}_i$ to compute subsequent updates can be a limit
already for $s$ in between five and eight -- recall that we are using
an explicit scheme, so we do not need to store any matrices that are
typically much larger than a few vectors), and the reduced memory access on
the other. In this program, we are particularly interested in the latter
aspect. Since cost of operator evaluation is only a small multiple of the cost
of simply streaming the input and output vector from memory with the optimized
matrix-free methods of deal.II, we must consider the cost of vector updates,
and low-storage variants can deliver up to twice the throughput of
conventional explicit Runge--Kutta methods for this reason, see e.g. the
analysis in @cite SchoederKormann2018.

Besides three variants for third, fourth and fifth order accuracy from the
reference @cite KennedyCarpenterLewis2000, we also use a fourth-order accurate
variant with seven stages that was optimized for acoustics setups from
@cite TseliosSimos2007. Acoustic problems are one of the interesting aspects of
the subsonic regime of the Euler equations where compressibility leads to the
transmission of sound waves; often, one uses further simplifications of the
linearized Euler equations around a background state or the acoustic wave
equation around a fixed frame.


<h3>Fast evaluation of integrals by matrix-free techniques</h3>

The major ingredients used in this program are the fast matrix-free techniques
we use to evaluate the operator $\mathcal L_h$ and the inverse mass matrix
$\mathcal M$. Actually, the term <i>matrix-free</i> is a slight misnomer,
because we are working with a nonlinear operator and do not linearize the
operator that in turn could be represented by a matrix. However, fast
evaluation of integrals has become popular as a replacement of sparse
matrix-vector products, as shown in step-37 and step-59, and we have coined
this infrastructure <i>matrix-free functionality</i> in deal.II for this
reason. Furthermore, the inverse mass matrix is indeed applied in a
matrix-free way, as detailed below.

The matrix-free infrastructure allows us to quickly evaluate the integrals in
the weak forms. The ingredients are the fast interpolation from solution
coefficients into values and derivatives at quadrature points, point-wise
operations at quadrature points (where we implement the differential operator
as derived above), as well as multiplication by all test functions and
summation over quadrature points. The first and third component make use of
sum factorization and have been extensively discussed in the step-37 tutorial
program for the cell integrals and step-59 for the face integrals. The only
difference is that we now deal with a system of $d+2$ components, rather than
the scalar systems in previous tutorial programs. In the code, all that
changes is a template argument of the FEEvaluation and FEFaceEvaluation
classes, the one to set the number of components. The access to the vector is
the same as before, all handled transparently by the evaluator. We also note
that the variant with a single evaluator chosen in the code below is not the
only choice -- we could also have used separate evalators for the separate
components $\rho$, $\rho \mathbf u$, and $E$; given that we treat all
components similarly (also reflected in the way we state the equation as a
vector system), this would be more complicated here. As before, the
FEEvaluation class provides explicit vectorization by combining the operations
on several cells (and faces), involving data types called
VectorizedArray. Since the arithmetic operations are overloaded for this type,
we do not have to bother with it all that much, except for the evaluation of
functions through the Function interface, where we need to provide particular
<i>vectorized</i> evaluations for several quadrature point locations at once.

A more substantial change in this program is the operation at quadrature
points: Here, the multi-component evaluators provide us with return types not
discussed before. Whereas FEEvaluation::get_value() would return a scalar
(more precisely, a VectorizedArray type due to vectorization across cells) for
the Laplacian of step-37, it now returns a type that is
`Tensor<1,dim+2,VectorizedArray<Number>>`. Likewise, the gradient type is now
`Tensor<1,dim+2,Tensor<1,dim,VectorizedArray<Number>>>`, where the outer
tensor collects the `dim+2` components of the Euler system, and the inner
tensor the partial derivatives in the various directions. For example, the
flux $\mathbf{F}(\mathbf{w})$ of the Euler system is of this type. In order to reduce the amount of
code we have to write for spelling out these types, we use the C++ `auto`
keyword where possible.

From an implementation point of view, the nonlinearity is not a big
difficulty: It is introduced naturally as we express the terms of the Euler
weak form, for example in the form of the momentum term $\rho \mathbf{u}
\otimes \mathbf{u}$. To obtain this expression, we first deduce the velocity
$\mathbf{u}$ from the momentum variable $\rho \mathbf{u}$. Given that $\rho
\mathbf{u}$ is represented as a $p$-degree polynomial, as is $\rho$, the
velocity $\mathbf{u}$ is a rational expression in terms of the reference
coordinates $\hat{\mathbf{x}}$. As we perform the multiplication $(\rho
\mathbf{u})\otimes \mathbf{u}$, we obtain an expression that is the
ratio of two polynomials, with polynomial degree $2p$ in the
numerator and polynomial degree $p$ in the denominator. Combined with the
gradient of the test function, the integrand is of degree $3p$ in the
numerator and $p$ in the denominator already for affine cells, i.e.,
for parallelograms/ parallelepipeds.
For curved cells, additional polynomial and rational expressions
appear when multiplying the integrand by the determinant of the Jacobian of
the mapping. At this point, one usually needs to give up on insisting on exact
integration, and take whatever accuracy the Gaussian (more precisely,
Gauss--Legrende) quadrature provides. The situation is then similar to the one
for the Laplace equation, where the integrand contains rational expressions on
non-affince cells and is also only integrated approximately. As these formulas
only integrate polynomials exactly, we have to live with the <a
href="https://mathoverflow.net/questions/26018/what-are-variational-crimes-and-who-coined-the-term">variational
crime</a> in the form of an integration error.

While inaccurate integration is usually tolerable for elliptic problems, for
hyperbolic problems inexact integration causes some headache in the form of an
effect called <b>aliasing</b>. The term comes from signal processing and
expresses the situation of inappropriate, too coarse sampling. In terms of
quadrature, the inappropriate sampling means that we use too few quadrature
points compared to what would be required to accurately sample the
variable-coefficient integrand. It has been shown in the DG literature that
aliasing errors can introduce unphysical oscillations in the numerical
solution for <i>barely</i> resolved simulations. The fact that aliasing mostly
affects coarse resolutions -- whereas finer meshes with the same scheme
work fine -- is not surprising because well-resolved simulations
tend to be smooth on length-scales of a cell (i.e., they have
small coefficients in the higher polynomial degrees that are missed by
too few quadrature points, whereas the main solution contribution in the lower
polynomial degrees is still well-captured -- this is simply a consequence of Taylor's
theorem). To address this topic, various approaches have been proposed in the
DG literature. One technique is filtering which damps the solution components
pertaining to higher polynomial degrees. As the chosen nodal basis is not
hierarchical, this would mean to transform from the nodal basis into a
hierarchical one (e.g., a modal one based on Legendre polynomials) where the
contributions within a cell are split by polynomial degrees. In that basis,
one could then multiply the solution coefficients associated with higher
degrees by a small number, keep the lower ones intact (to not destroy consistency), and
then transform back to the nodal basis. However, filters reduce the accuracy of the
method. Another, in some sense simpler, strategy is to use more quadrature
points to capture non-linear terms more accurately. Using more than $p+1$
quadrature points per coordinate directions is sometimes called
over-integration or consistent integration. The latter name is most common in
the context of the incompressible Navier-Stokes equations, where the
$\mathbf{u}\otimes \mathbf{u}$ nonlinearity results in polynomial integrands
of degree $3p$ (when also considering the test function), which can be
integrated exactly with $\textrm{floor}\left(\frac{3p}{2}\right)+1$ quadrature
points per direction as long as the element geometry is affine. In the context
of the Euler equations with non-polynomial integrands, the choice is less
clear. Depending on the variation in the various variables both
$\textrm{floor}\left(\frac{3p}{2}\right)+1$ or $2p+1$ points (integrating
exactly polynomials of degree $3p$ or $4p$, respectively) are common.

To reflect this variability in the choice of quadrature in the program, we
keep the number of quadrature points a variable to be specified just as the
polynomial degree, and note that one would make different choices depending
also on the flow configuration. The default choice is $p+2$ points -- a bit
more than the minimum possible of $p+1$ points. The FEEvaluation and
FEFaceEvaluation classes allow to seamlessly change the number of points by a
template parameter, such that the program does not get more complicated
because of that.


<h3>Evaluation of the inverse mass matrix with matrix-free techniques</h3>

The last ingredient is the evaluation of the inverse mass matrix $\mathcal
M^{-1}$. In DG methods with explicit time integration, mass matrices are
block-diagonal and thus easily inverted -- one only needs to invert the
diagonal blocks. However, given the fact that matrix-free evaluation of
integrals is closer in cost to the access of the vectors only, even the
application of a block-diagonal matrix (e.g. via an array of LU factors) would
be several times more expensive than evaluation of $\mathcal L_h$
simply because just storing and loading matrices of size
`dofs_per_cell` times `dofs_per_cell` for higher order finite elements
repeatedly is expensive. As this is
clearly undesirable, part of the community has moved to bases where the mass
matrix is diagonal, for example the <i>L<sub>2</sub></i>-orthogonal Legendre basis using
hierarchical polynomials or Lagrange polynomials on the points of the Gaussian
quadrature (which is just another way of utilizing Legendre
information). While the diagonal property breaks down for deformed elements,
the error made by taking a diagonal mass matrix and ignoring the rest (a
variant of mass lumping, though not the one with an additional integration
error as utilized in step-48) has been shown to not alter discretization
accuracy. The Lagrange basis in the points of Gaussian quadrature is sometimes
also referred to as a collocation setup, as the nodal points of the
polynomials coincide (= are "co-located") with the points of quadrature, obviating some
interpolation operations. Given the fact that we want to use more quadrature
points for nonlinear terms in $\mathcal L_h$, however, the collocation
property is lost. (More precisely, it is still used in FEEvaluation and
FEFaceEvaluation after a change of basis, see the matrix-free paper
@cite KronbichlerKormann2019.)

In this tutorial program, we use the collocation idea for the application of
the inverse mass matrix, but with a slight twist. Rather than using the
collocation via Lagrange polynomials in the points of Gaussian quadrature, we
prefer a conventional Lagrange basis in Gauss-Lobatto points as those make the
evaluation of face integrals cheap. This is because for Gauss-Lobatto
points, some of the node points are located on the faces of the cell
and it is not difficult to show that on any given face, the only shape
functions with non-zero values are exactly the ones whose node points
are in fact located on that face. One could of course also use the
Gauss-Lobatto quadrature (with some additional integration error) as was done
in step-48, but we do not want to sacrifice accuracy as these
quadrature formulas are generally of lower order than the general
Gauss quadrature formulas. Instead, we use an idea described in the reference
@cite KronbichlerSchoeder2016 where it was proposed to change the basis for the
sake of applying the inverse mass matrix. Let us denote by $S$ the matrix of
shape functions evaluated at quadrature points, with shape functions in the row
of the matrix and quadrature points in columns. Then, the mass matrix on a cell
$K$ is given by
@f[
\mathcal M^K = S J^K S^\mathrm T.
@f]
Here, $J^K$ is the diagonal matrix with the determinant of the Jacobian times
the quadrature weight (JxW) as entries. The matrix $S$ is constructed as the
Kronecker product (tensor product) of one-dimensional matrices, e.g. in 3D as
@f[
S = S_{\text{1D}}\otimes S_{\text{1D}}\otimes S_{\text{1D}},
@f]
which is the result of the basis functions being a tensor product of
one-dimensional shape functions and the quadrature formula being the tensor
product of 1D quadrature formulas. For the case that the number of polynomials
equals the number of quadrature points, all matrices in $S J^K S^\mathrm T$
are square, and also the ingredients to $S$ in the Kronecker product are
square. Thus, one can invert each matrix to form the overall inverse,
@f[
\left(\mathcal M^K\right)^{-1} = S_{\text{1D}}^{-\mathrm T}\otimes
S_{\text{1D}}^{-\mathrm T}\otimes S_{\text{1D}}^{-\mathrm T}
\left(J^K\right)^{-1}
S_{\text{1D}}^{-1}\otimes S_{\text{1D}}^{-1}\otimes S_{\text{1D}}^{-1}.
@f]
This formula is of exactly the same structure as the steps in the forward
evaluation of integrals with sum factorization techniques (i.e., the
FEEvaluation and MatrixFree framework of deal.II). Hence, we can utilize the
same code paths with a different interpolation matrix,
$S_{\mathrm{1D}}^{-\mathrm{T}}$ rather than $S_{\mathrm{1D}}$.

The class MatrixFreeOperators::CellwiseInverseMassMatrix implements this
operation: It changes from the basis contained in the finite element (in this
case, FE_DGQ) to the Lagrange basis in Gaussian quadrature points. Here, the
inverse of a diagonal mass matrix can be evaluated, which is simply the inverse
of the `JxW` factors (i.e., the quadrature weight times the determinant of the
Jacobian from reference to real coordinates). Once this is done, we can change
back to the standard nodal Gauss-Lobatto basis.

The advantage of this particular way of applying the inverse mass matrix is
a cost similar to the forward application of a mass matrix, which is cheaper
than the evaluation of the spatial operator $\mathcal L_h$
with over-integration and face integrals. (We
will demonstrate this with detailed timing information in the
<a href="#Results">results section</a>.) In fact, it
is so cheap that it is limited by the bandwidth of reading the source vector,
reading the diagonal, and writing into the destination vector on most modern
architectures. The hardware used for the result section allows to do the
computations at least twice as fast as the streaming of the vectors from
memory.


<h3>The test case</h3>

In this tutorial program, we implement two test cases. The first case is a
convergence test limited to two space dimensions. It runs a so-called
isentropic vortex which is transported via a background flow field. The second
case uses a more exciting setup: We start with a cylinder immersed in a
channel, using the GridGenerator::channel_with_cylinder() function. Here, we
impose a subsonic initial field at Mach number of $\mathrm{Ma}=0.307$ with a
constant velocity in $x$ direction. At the top and bottom walls as well as at
the cylinder, we impose a no-penetration (i.e., tangential flow)
condition. This setup forces the flow to re-orient as compared to the initial
condition, which results in a big sound wave propagating away from the
cylinder. In upstream direction, the wave travels more slowly (as it
has to move against the oncoming gas), including a
discontinuity in density and pressure. In downstream direction, the transport
is faster as sound propagation and fluid flow go in the same direction, which smears
out the discontinuity somewhat. Once the sound wave hits the upper and lower
walls, the sound is reflected back, creating some nice shapes as illustrated
in the <a href="#Results">results section</a> below.


examples/step-67/doc/results.dox
<h1>Results</h1>

<h3>Program output</h3>

Running the program with the default settings on a machine with 40 processes
produces the following output:
@code
Running with 40 MPI processes
Vectorization over 8 doubles = 512 bits (AVX512)
Number of degrees of freedom: 147,456 ( = 4 [vars] x 1,024 [cells] x 36 [dofs/cell/var] )
Time step size: 0.00689325, minimal h: 0.3125, initial transport scaling: 0.102759

Time:       0, dt:   0.0069, error rho:   2.76e-07, rho * u:  1.259e-06, energy: 2.987e-06
Time:    1.01, dt:   0.0069, error rho:   1.37e-06, rho * u:  2.252e-06, energy: 4.153e-06
Time:    2.01, dt:   0.0069, error rho:  1.561e-06, rho * u:   2.43e-06, energy: 4.493e-06
Time:    3.01, dt:   0.0069, error rho:  1.714e-06, rho * u:  2.591e-06, energy: 4.762e-06
Time:    4.01, dt:   0.0069, error rho:  1.843e-06, rho * u:  2.625e-06, energy: 4.985e-06
Time:    5.01, dt:   0.0069, error rho:  1.496e-06, rho * u:  1.961e-06, energy: 4.142e-06
Time:       6, dt:   0.0083, error rho:  1.007e-06, rho * u:  7.119e-07, energy: 2.972e-06
Time:       7, dt:   0.0095, error rho:  9.096e-07, rho * u:  3.786e-07, energy: 2.626e-06
Time:       8, dt:   0.0096, error rho:  8.439e-07, rho * u:  3.338e-07, energy:  2.43e-06
Time:       9, dt:   0.0096, error rho:  7.822e-07, rho * u:  2.984e-07, energy: 2.248e-06
Time:      10, dt:   0.0096, error rho:  7.231e-07, rho * u:  2.666e-07, energy: 2.074e-06

+-------------------------------------------+------------------+------------+------------------+
| Total wallclock time elapsed              |     2.249s    30 |     2.249s |     2.249s     8 |
|                                           |                  |                               |
| Section                       | no. calls |   min time  rank |   avg time |   max time  rank |
+-------------------------------------------+------------------+------------+------------------+
| compute errors                |        11 |  0.008066s    13 |   0.00952s |   0.01041s    20 |
| compute transport speed       |       258 |   0.01012s    13 |   0.05392s |   0.08574s    25 |
| output                        |        11 |    0.9597s    13 |    0.9613s |    0.9623s     6 |
| rk time stepping total        |      1283 |    0.9827s    25 |     1.015s |      1.06s    13 |
| rk_stage - integrals L_h      |      6415 |    0.8803s    26 |    0.9198s |    0.9619s    14 |
| rk_stage - inv mass + vec upd |      6415 |   0.05677s    15 |   0.06487s |   0.07597s    13 |
+-------------------------------------------+------------------+------------+------------------+
@endcode

The program output shows that all errors are small. This is due to the fact
that we use a relatively fine mesh of $32^2$ cells with polynomials of degree
5 for a solution that is smooth. An interesting pattern shows for the time
step size: whereas it is 0.0069 up to time 5, it increases to 0.0096 for later
times. The step size increases once the vortex with some motion on top of the
speed of sound (and thus faster propagation) leaves the computational domain
between times 5 and 6.5. After that point, the flow is simply uniform
in the same direction, and the maximum velocity of the gas is reduced
compared to the previous state where the uniform velocity was overlaid
by the vortex. Our time step formula recognizes this effect.

The final block of output shows detailed information about the timing
of individual parts of the programs; it breaks this down by showing
the time taken by the fastest and the slowest processor, and the
average time -- this is often useful in very large computations to
find whether there are processors that are consistently overheated
(and consequently are throttling their clock speed) or consistently
slow for other reasons.
The summary shows that 1283 time steps have been performed
in 1.02 seconds (looking at the average time among all MPI processes), while
the output of 11 files has taken additional 0.96 seconds. Broken down per time
step and into the five Runge--Kutta stages, the compute time per evaluation is
0.16 milliseconds. This high performance is typical of matrix-free evaluators
and a reason why explicit time integration is very competitive against
implicit solvers, especially for large-scale simulations. The breakdown of
computational times at the end of the program run shows that the evaluation of
integrals in $\mathcal L_h$ contributes with around 0.92 seconds and the
application of the inverse mass matrix with 0.06 seconds. Furthermore, the
estimation of the transport speed for the time step size computation
contributes with another 0.05 seconds of compute time.

If we use three more levels of global refinement and 9.4 million DoFs in total,
the final statistics are as follows (for the modified Lax--Friedrichs flux,
$p=5$, and the same system of 40 cores of dual-socket Intel Xeon Gold 6230):
@code
+-------------------------------------------+------------------+------------+------------------+
| Total wallclock time elapsed              |     244.9s    12 |     244.9s |     244.9s    34 |
|                                           |                  |                               |
| Section                       | no. calls |   min time  rank |   avg time |   max time  rank |
+-------------------------------------------+------------------+------------+------------------+
| compute errors                |        11 |    0.4239s    12 |    0.4318s |    0.4408s     9 |
| compute transport speed       |      2053 |     3.962s    12 |     6.727s |     10.12s     7 |
| output                        |        11 |     30.35s    12 |     30.36s |     30.37s     9 |
| rk time stepping total        |     10258 |     201.7s     7 |     205.1s |     207.8s    12 |
| rk_stage - integrals L_h      |     51290 |     121.3s     6 |     126.6s |     136.3s    16 |
| rk_stage - inv mass + vec upd |     51290 |     66.19s    16 |     77.52s |     81.84s    10 |
+-------------------------------------------+------------------+------------+------------------+
@endcode

Per time step, the solver now takes 0.02 seconds, about 25 times as long as
for the small problem with 147k unknowns. Given that the problem involves 64
times as many unknowns, the increase in computing time is not
surprising. Since we also do 8 times as many time steps, the compute time
should in theory increase by a factor of 512. The actual increase is 205 s /
1.02 s = 202. This is because the small problem size cannot fully utilize the
40 cores due to communication overhead. This becomes clear if we look into the
details of the operations done per time step. The evaluation of the
differential operator $\mathcal L_h$ with nearest neighbor communication goes
from 0.92 seconds to 127 seconds, i.e., it increases with a factor of 138. On
the other hand, the cost for application of the inverse mass matrix and the
vector updates, which do not need to communicate between the MPI processes at
all, has increased by a factor of 1195. The increase is more than the
theoretical factor of 512 because the operation is limited by the bandwidth
from RAM memory for the larger size while for the smaller size, all vectors
fit into the caches of the CPU. The numbers show that the mass matrix
evaluation and vector update part consume almost 40% of the time spent by the
Runge--Kutta stages -- despite using a low-storage Runge--Kutta integrator and
merging of vector operations! And despite using over-integration for the
$\mathcal L_h$ operator. For simpler differential operators and more expensive
time integrators, the proportion spent in the mass matrix and vector update
part can also reach 70%. If we compute a throughput number in terms of DoFs
processed per second and Runge--Kutta stage, we obtain @f[ \text{throughput} =
\frac{n_\mathrm{time steps} n_\mathrm{stages}
n_\mathrm{dofs}}{t_\mathrm{compute}} = \frac{10258 \cdot 5 \cdot
9.4\,\text{MDoFs}}{205s} = 2360\, \text{MDoFs/s} @f] This throughput number is
very high, given that simply copying one vector to another one runs at
only around 10,000 MDoFs/s.

If we go to the next-larger size with 37.7 million DoFs, the overall
simulation time is 2196 seconds, with 1978 seconds spent in the time
stepping. The increase in run time is a factor of 9.3 for the L_h operator
(1179 versus 127 seconds) and a factor of 10.3 for the inverse mass matrix and
vector updates (797 vs 77.5 seconds). The reason for this non-optimal increase
in run time can be traced back to cache effects on the given hardware (with 40
MB of L2 cache and 55 MB of L3 cache): While not all of the relevant data fits
into caches for 9.4 million DoFs (one vector takes 75 MB and we have three
vectors plus some additional data in MatrixFree), there is capacity for one and
a half vector nonetheless. Given that modern caches are more sophisticated than
the naive least-recently-used strategy (where we would have little re-use as
the data is used in a streaming-like fashion), we can assume that a sizeable
fraction of data can indeed be delivered from caches for the 9.4 million DoFs
case. For the larger case, even with optimal caching less than 10 percent of
data would fit into caches, with an associated loss in performance.


<h3>Convergence rates for the analytical test case</h3>

For the modified Lax--Friedrichs flux and measuring the error in the momentum
variable, we obtain the following convergence table (the rates are very
similar for the density and energy variables):

<table align="center" class="doxtable">
  <tr>
    <th>&nbsp;</th>
    <th colspan="3"><i>p</i>=2</th>
    <th colspan="3"><i>p</i>=3</th>
    <th colspan="3"><i>p</i>=5</th>
  </tr>
  <tr>
    <th>n_cells</th>
    <th>n_dofs</th>
    <th>error mom</th>
    <th>rate</th>
    <th>n_dofs</th>
    <th>error mom</th>
    <th>rate</th>
    <th>n_dofs</th>
    <th>error mom</th>
    <th>rate</th>
  </tr>
  <tr>
    <td align="right">16</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td align="right">2,304</td>
    <td align="center">1.373e-01</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td align="right">64</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td align="right">4,096</td>
    <td align="center">9.130e-02</td>
    <td>&nbsp;</td>
    <td align="right">9,216</td>
    <td align="center">8.899e-03</td>
    <td>3.94</td>
  </tr>
  <tr>
    <td align="right">256</td>
    <td align="right">9,216</td>
    <td align="center">5.577e-02</td>
    <td>&nbsp;</td>
    <td align="right">16,384</td>
    <td align="center">7.381e-03</td>
    <td>3.64</td>
    <td align="right">36,864</td>
    <td align="center">2.082e-04</td>
    <td>5.42</td>
  </tr>
  <tr>
    <td align="right">1024</td>
    <td align="right">36,864</td>
    <td align="center">4.724e-03</td>
    <td>3.56</td>
    <td align="right">65,536</td>
    <td align="center">3.072e-04</td>
    <td>4.59</td>
    <td align="right">147,456</td>
    <td align="center">2.625e-06</td>
    <td>6.31</td>
  </tr>
  <tr>
    <td align="right">4096</td>
    <td align="right">147,456</td>
    <td align="center">6.205e-04</td>
    <td>2.92</td>
    <td align="right">262,144</td>
    <td align="center">1.880e-05</td>
    <td>4.03</td>
    <td align="right">589,824</td>
    <td align="center">3.268e-08</td>
    <td>6.33</td>
  </tr>
  <tr>
    <td align="right">16,384</td>
    <td align="right">589,824</td>
    <td align="center">8.279e-05</td>
    <td>2.91</td>
    <td align="right">1,048,576</td>
    <td align="center">1.224e-06</td>
    <td>3.94</td>
    <td align="right">2,359,296</td>
    <td align="center">9.252e-10</td>
    <td>5.14</td>
  </tr>
  <tr>
    <td align="right">65,536</td>
    <td align="right">2,359,296</td>
    <td align="center">1.105e-05</td>
    <td>2.91</td>
    <td align="right">4,194,304</td>
    <td align="center">7.871e-08</td>
    <td>3.96</td>
    <td align="right">9,437,184</td>
    <td align="center">1.369e-10</td>
    <td>2.77</td>
  </tr>
  <tr>
    <td align="right">262,144</td>
    <td align="right">9,437,184</td>
    <td align="center">1.615e-06</td>
    <td>2.77</td>
    <td align="right">16,777,216</td>
    <td align="center">4.961e-09</td>
    <td>3.99</td>
    <td align="right">37,748,736</td>
    <td align="center">7.091e-11</td>
    <td>0.95</td>
  </tr>
</table>

If we switch to the Harten-Lax-van Leer flux, the results are as follows:
<table align="center" class="doxtable">
  <tr>
    <th>&nbsp;</th>
    <th colspan="3"><i>p</i>=2</th>
    <th colspan="3"><i>p</i>=3</th>
    <th colspan="3"><i>p</i>=5</th>
  </tr>
  <tr>
    <th>n_cells</th>
    <th>n_dofs</th>
    <th>error mom</th>
    <th>rate</th>
    <th>n_dofs</th>
    <th>error mom</th>
    <th>rate</th>
    <th>n_dofs</th>
    <th>error mom</th>
    <th>rate</th>
  </tr>
  <tr>
    <td align="right">16</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td align="right">2,304</td>
    <td align="center">1.339e-01</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td align="right">64</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td align="right">4,096</td>
    <td align="center">9.037e-02</td>
    <td>&nbsp;</td>
    <td align="right">9,216</td>
    <td align="center">8.849e-03</td>
    <td>3.92</td>
  </tr>
  <tr>
    <td align="right">256</td>
    <td align="right">9,216</td>
    <td align="center">4.204e-02</td>
    <td>&nbsp;</td>
    <td align="right">16,384</td>
    <td align="center">9.143e-03</td>
    <td>3.31</td>
    <td align="right">36,864</td>
    <td align="center">2.501e-04</td>
    <td>5.14</td>
  </tr>
  <tr>
    <td align="right">1024</td>
    <td align="right">36,864</td>
    <td align="center">4.913e-03</td>
    <td>3.09</td>
    <td align="right">65,536</td>
    <td align="center">3.257e-04</td>
    <td>4.81</td>
    <td align="right">147,456</td>
    <td align="center">3.260e-06</td>
    <td>6.26</td>
  </tr>
  <tr>
    <td align="right">4096</td>
    <td align="right">147,456</td>
    <td align="center">7.862e-04</td>
    <td>2.64</td>
    <td align="right">262,144</td>
    <td align="center">1.588e-05</td>
    <td>4.36</td>
    <td align="right">589,824</td>
    <td align="center">2.953e-08</td>
    <td>6.79</td>
  </tr>
  <tr>
    <td align="right">16,384</td>
    <td align="right">589,824</td>
    <td align="center">1.137e-04</td>
    <td>2.79</td>
    <td align="right">1,048,576</td>
    <td align="center">9.400e-07</td>
    <td>4.08</td>
    <td align="right">2,359,296</td>
    <td align="center">4.286e-10</td>
    <td>6.11</td>
  </tr>
  <tr>
    <td align="right">65,536</td>
    <td align="right">2,359,296</td>
    <td align="center">1.476e-05</td>
    <td>2.95</td>
    <td align="right">4,194,304</td>
    <td align="center">5.799e-08</td>
    <td>4.02</td>
    <td align="right">9,437,184</td>
    <td align="center">2.789e-11</td>
    <td>3.94</td>
  </tr>
  <tr>
    <td align="right">262,144</td>
    <td align="right">9,437,184</td>
    <td align="center">2.038e-06</td>
    <td>2.86</td>
    <td align="right">16,777,216</td>
    <td align="center">3.609e-09</td>
    <td>4.01</td>
    <td align="right">37,748,736</td>
    <td align="center">5.730e-11</td>
    <td>-1.04</td>
  </tr>
</table>

The tables show that we get optimal $\mathcal O\left(h^{p+1}\right)$
convergence rates for both numerical fluxes. The errors are slightly smaller
for the Lax--Friedrichs flux for $p=2$, but the picture is reversed for
$p=3$; in any case, the differences on this testcase are relatively
small.

For $p=5$, we reach the roundoff accuracy of $10^{-11}$ with both
fluxes on the finest grids. Also note that the errors are absolute with a
domain length of $10^2$, so relative errors are below $10^{-12}$. The HLL flux
is somewhat better for the highest degree, which is due to a slight inaccuracy
of the Lax--Friedrichs flux: The Lax--Friedrichs flux sets a Dirichlet
condition on the solution that leaves the domain, which results in a small
artificial reflection, which is accentuated for the Lax--Friedrichs
flux. Apart from that, we see that the influence of the numerical flux is
minor, as the polynomial part inside elements is the main driver of the
accucary. The limited influence of the flux also has consequences when trying
to approach more challenging setups with the higher-order DG setup: Taking for
example the parameters and grid of step-33, we get oscillations (which in turn
make density negative and make the solution explode) with both fluxes once the
high-mass part comes near the boundary, as opposed to the low-order finite
volume case ($p=0$). Thus, any case that leads to shocks in the solution
necessitates some form of limiting or artificial dissipation. For another
alternative, see the step-69 tutorial program.


<h3>Results for flow in channel around cylinder in 2D</h3>

For the test case of the flow around a cylinder in a channel, we need to
change the first code line to
@code
  constexpr unsigned int testcase = 1;
@endcode
This test case starts with a background field of a constant velocity
of Mach number 0.31 and a constant initial density; the flow will have
to go around an obstacle in the form of a cylinder. Since we impose a
no-penetration condition on the cylinder walls, the flow that
initially impinges head-on onto to cylinder has to rearrange,
which creates a big sound wave. The following pictures show the pressure at
times 0.1, 0.25, 0.5, and 1.0 (top left to bottom right) for the 2D case with
5 levels of global refinement, using 102,400 cells with polynomial degree of
5 and 14.7 million degrees of freedom over all 4 solution variables.
We clearly see the discontinuity that
propagates slowly in the upstream direction and more quickly in downstream
direction in the first snapshot at time 0.1. At time 0.25, the sound wave has
reached the top and bottom walls and reflected back to the interior. From the
different distances of the reflected waves from lower and upper walls we can
see the slight asymmetry of the Sch&auml;fer-Turek test case represented by
GridGenerator::channel_with_cylinder() with somewhat more space above the
cylinder compared to below. At later times, the picture is more chaotic with
many sound waves all over the place.

<table align="center" class="doxtable" style="width:85%">
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-67.pressure_010.png" alt="" width="100%">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-67.pressure_025.png" alt="" width="100%">
    </td>
  </tr>
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-67.pressure_050.png" alt="" width="100%">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-67.pressure_100.png" alt="" width="100%">
    </td>
  </tr>
</table>

The next picture shows an elevation plot of the pressure at time 1.0 looking
from the channel inlet towards the outlet at the same resolution -- here,
we can see the large number
of reflections. In the figure, two types of waves are visible. The
larger-amplitude waves correspond to various reflections that happened as the
initial discontinuity hit the walls, whereas the small-amplitude waves of
size similar to the elements correspond to numerical artifacts. They have their
origin in the finite resolution of the scheme and appear as the discontinuity
travels through elements with high-order polynomials. This effect can be cured
by increasing resolution. Apart from this effect, the rich wave structure is
the result of the transport accuracy of the high-order DG method.

<img src="https://www.dealii.org/images/steps/developer/step-67.pressure_elevated.jpg" alt="" width="40%">

With 2 levels of global refinement with 1,600 cells, the mesh and its
partitioning on 40 MPI processes looks as follows:

<img src="https://www.dealii.org/images/steps/developer/step-67.grid-owner.png" alt="" width="70%">

When we run the code with 4 levels of global refinements on 40 cores, we get
the following output:
@code
Running with 40 MPI processes
Vectorization over 8 doubles = 512 bits (AVX512)
Number of degrees of freedom: 3,686,400 ( = 4 [vars] x 25,600 [cells] x 36 [dofs/cell/var] )
Time step size: 7.39876e-05, minimal h: 0.001875, initial transport scaling: 0.00110294

Time:       0, dt:  7.4e-05, norm rho:   4.17e-16, rho * u:  1.629e-16, energy: 1.381e-15
Time:    0.05, dt:  6.3e-05, norm rho:    0.02075, rho * u:    0.03801, energy:   0.08772
Time:     0.1, dt:  5.9e-05, norm rho:    0.02211, rho * u:    0.04515, energy:   0.08953
Time:    0.15, dt:  5.7e-05, norm rho:    0.02261, rho * u:    0.04592, energy:   0.08967
Time:     0.2, dt:  5.8e-05, norm rho:    0.02058, rho * u:    0.04361, energy:   0.08222
Time:    0.25, dt:  5.9e-05, norm rho:    0.01695, rho * u:    0.04203, energy:   0.06873
Time:     0.3, dt:  5.9e-05, norm rho:    0.01653, rho * u:     0.0401, energy:   0.06604
Time:    0.35, dt:  5.7e-05, norm rho:    0.01774, rho * u:    0.04264, energy:    0.0706

...

Time:    1.95, dt:  5.8e-05, norm rho:    0.01488, rho * u:    0.03923, energy:   0.05185
Time:       2, dt:  5.7e-05, norm rho:    0.01432, rho * u:    0.03969, energy:   0.04889

+-------------------------------------------+------------------+------------+------------------+
| Total wallclock time elapsed              |     273.6s    13 |     273.6s |     273.6s     0 |
|                                           |                  |                               |
| Section                       | no. calls |   min time  rank |   avg time |   max time  rank |
+-------------------------------------------+------------------+------------+------------------+
| compute errors                |        41 |   0.01112s    35 |    0.0672s |    0.1337s     0 |
| compute transport speed       |      6914 |     5.422s    35 |     15.96s |     29.99s     1 |
| output                        |        41 |     37.24s    35 |      37.3s |     37.37s     0 |
| rk time stepping total        |     34564 |     205.4s     1 |     219.5s |     230.1s    35 |
| rk_stage - integrals L_h      |    172820 |     153.6s     1 |     164.9s |     175.6s    27 |
| rk_stage - inv mass + vec upd |    172820 |     47.13s    13 |     53.09s |     64.05s    33 |
+-------------------------------------------+------------------+------------+------------------+
@endcode

The norms shown here for the various quantities are the deviations
$\rho'$, $(\rho u)'$, and $E'$ against the background field (namely, the
initial condition). The distribution of run time is overall similar as in the
previous test case. The only slight difference is the larger proportion of
time spent in $\mathcal L_h$ as compared to the inverse mass matrix and vector
updates. This is because the geometry is deformed and the matrix-free
framework needs to load additional arrays for the geometry from memory that
are compressed in the affine mesh case.

Increasing the number of global refinements to 5, the output becomes:
@code
Running with 40 MPI processes
Vectorization over 8 doubles = 512 bits (AVX512)
Number of degrees of freedom: 14,745,600 ( = 4 [vars] x 102,400 [cells] x 36 [dofs/cell/var] )

...

+-------------------------------------------+------------------+------------+------------------+
| Total wallclock time elapsed              |      2693s    32 |      2693s |      2693s    23 |
|                                           |                  |                               |
| Section                       | no. calls |   min time  rank |   avg time |   max time  rank |
+-------------------------------------------+------------------+------------+------------------+
| compute errors                |        41 |   0.04537s    32 |     0.173s |    0.3489s     0 |
| compute transport speed       |     13858 |     40.75s    32 |     85.99s |     149.8s     0 |
| output                        |        41 |     153.8s    32 |     153.9s |     154.1s     0 |
| rk time stepping total        |     69284 |      2386s     0 |      2450s |      2496s    32 |
| rk_stage - integrals L_h      |    346420 |      1365s    32 |      1574s |      1718s    19 |
| rk_stage - inv mass + vec upd |    346420 |     722.5s    10 |     870.7s |      1125s    32 |
+-------------------------------------------+------------------+------------+------------------+
@endcode

The effect on performance is similar to the analytical test case -- in
theory, computation times should increase by a factor of 8, but we actually
see an increase by a factor of 11 for the time steps (219.5 seconds versus
2450 seconds). This can be traced back to caches, with the small case mostly
fitting in caches. An interesting effect, typical of programs with a mix of
local communication (integrals $\mathcal L_h$) and global communication (computation of
transport speed) with some load imbalance, can be observed by looking at the
MPI ranks that encounter the minimal and maximal time of different phases,
respectively. Rank 0 reports the fastest throughput for the "rk time stepping
total" part. At the same time, it appears to be slowest for the "compute
transport speed" part, almost a factor of 2 slower than the
average and almost a factor of 4 compared to the faster rank.
Since the latter involves global communication, we can attribute the
slowness in this part to the fact that the local Runge--Kutta stages have
advanced more quickly on this rank and need to wait until the other processors
catch up. At this point, one can wonder about the reason for this imbalance:
The number of cells is almost the same on all MPI processes.
However, the matrix-free framework is faster on affine and Cartesian
cells located towards the outlet of the channel, to which the lower MPI ranks
are assigned. On the other hand, rank 32, which reports the highest run time
for the Runga--Kutta stages, owns the curved cells near the cylinder, for
which no data compression is possible. To improve throughput, we could assign
different weights to different cell types when partitioning the
parallel::distributed::Triangulation object, or even measure the run time for a
few time steps and try to rebalance then.

The throughput per Runge--Kutta stage can be computed to 2085 MDoFs/s for the
14.7 million DoFs test case over the 346,000 Runge--Kutta stages, slightly slower
than the Cartesian mesh throughput of 2360 MDoFs/s reported above.

Finally, if we add one additional refinement, we record the following output:
@code
Running with 40 MPI processes
Vectorization over 8 doubles = 512 bits (AVX512)
Number of degrees of freedom: 58,982,400 ( = 4 [vars] x 409,600 [cells] x 36 [dofs/cell/var] )

...

Time:    1.95, dt:  1.4e-05, norm rho:    0.01488, rho * u:    0.03923, energy:   0.05183
Time:       2, dt:  1.4e-05, norm rho:    0.01431, rho * u:    0.03969, energy:   0.04887

+-------------------------------------------+------------------+------------+------------------+
| Total wallclock time elapsed              | 2.166e+04s    26 | 2.166e+04s | 2.166e+04s    24 |
|                                           |                  |                               |
| Section                       | no. calls |   min time  rank |   avg time |   max time  rank |
+-------------------------------------------+------------------+------------+------------------+
| compute errors                |        41 |    0.1758s    30 |     0.672s |     1.376s     1 |
| compute transport speed       |     27748 |     321.3s    34 |     678.8s |      1202s     1 |
| output                        |        41 |     616.3s    32 |     616.4s |     616.4s    34 |
| rk time stepping total        |    138733 | 1.983e+04s     1 | 2.036e+04s | 2.072e+04s    34 |
| rk_stage - integrals L_h      |    693665 | 1.052e+04s    32 | 1.248e+04s | 1.387e+04s    19 |
| rk_stage - inv mass + vec upd |    693665 |      6404s    10 |      7868s | 1.018e+04s    32 |
+-------------------------------------------+------------------+------------+------------------+
@endcode

The "rk time stepping total" part corresponds to a throughput of 2010 MDoFs/s. The
overall run time to perform 139k time steps is 20k seconds (5.7 hours) or 7
time steps per second -- not so bad for having nearly 60 million
unknowns. More throughput can be achieved by adding more cores to
the computation.


<h3>Results for flow in channel around cylinder in 3D</h3>

Switching the channel test case to 3D with 3 global refinements, the output is
@code
Running with 40 MPI processes
Vectorization over 8 doubles = 512 bits (AVX512)
Number of degrees of freedom: 221,184,000 ( = 5 [vars] x 204,800 [cells] x 216 [dofs/cell/var] )

...

Time:    1.95, dt:  0.00011, norm rho:    0.01131, rho * u:    0.03056, energy:   0.04091
Time:       2, dt:  0.00011, norm rho:     0.0119, rho * u:    0.03142, energy:   0.04425

+-------------------------------------------+------------------+------------+------------------+
| Total wallclock time elapsed              | 1.734e+04s     4 | 1.734e+04s | 1.734e+04s    38 |
|                                           |                  |                               |
| Section                       | no. calls |   min time  rank |   avg time |   max time  rank |
+-------------------------------------------+------------------+------------+------------------+
| compute errors                |        41 |    0.6551s    34 |     3.216s |     7.281s     0 |
| compute transport speed       |      3546 |       160s    34 |     393.2s |     776.9s     0 |
| output                        |        41 |      1350s    34 |      1353s |      1357s     0 |
| rk time stepping total        |     17723 | 1.519e+04s     0 | 1.558e+04s | 1.582e+04s    34 |
| rk_stage - integrals L_h      |     88615 | 1.005e+04s    32 | 1.126e+04s |  1.23e+04s    11 |
| rk_stage - inv mass + vec upd |     88615 |      3056s    11 |      4322s |      5759s    32 |
+-------------------------------------------+------------------+------------+------------------+
@endcode

The physics are similar to the 2D case, with a slight motion in the z
direction due to the gravitational force. The throughput per Runge--Kutta
stage in this case is
@f[
\text{throughput} = \frac{n_\mathrm{time steps} n_\mathrm{stages}
n_\mathrm{dofs}}{t_\mathrm{compute}} =
\frac{17723 \cdot 5 \cdot 221.2\,\text{M}}{15580s} = 1258\, \text{MDoFs/s}.
@f]

The throughput is lower than in 2D because the computation of the $\mathcal L_h$ term
is more expensive. This is due to over-integration with `degree+2` points and
the larger fraction of face integrals (worse volume-to-surface ratio) with
more expensive flux computations. If we only consider the inverse mass matrix
and vector update part, we record a throughput of 4857 MDoFs/s for the 2D case
of the isentropic vortex with 37.7 million unknowns, whereas the 3D case
runs with 4535 MDoFs/s. The performance is similar because both cases are in
fact limited by the memory bandwidth.

If we go to four levels of global refinement, we need to increase the number
of processes to fit everything in memory -- the computation needs around 350
GB of RAM memory in this case. Also, the time it takes to complete 35k time
steps becomes more tolerable by adding additional resources. We therefore use
6 nodes with 40 cores each, resulting in a computation with 240 MPI processes:
@code
Running with 240 MPI processes
Vectorization over 8 doubles = 512 bits (AVX512)
Number of degrees of freedom: 1,769,472,000 ( = 5 [vars] x 1,638,400 [cells] x 216 [dofs/cell/var] )

...

Time:    1.95, dt:  5.6e-05, norm rho:    0.01129, rho * u:     0.0306, energy:   0.04086
Time:       2, dt:  5.6e-05, norm rho:    0.01189, rho * u:    0.03145, energy:   0.04417

+-------------------------------------------+------------------+------------+------------------+
| Total wallclock time elapsed              | 5.396e+04s   151 | 5.396e+04s | 5.396e+04s     0 |
|                                           |                  |                               |
| Section                       | no. calls |   min time  rank |   avg time |   max time  rank |
+-------------------------------------------+------------------+------------+------------------+
| compute errors                |        41 |     2.632s   178 |     7.221s |     16.56s     0 |
| compute transport speed       |      7072 |       714s   193 |      1553s |      3351s     0 |
| output                        |        41 |      8065s   176 |      8070s |      8079s     0 |
| rk time stepping total        |     35350 |  4.25e+04s     0 |  4.43e+04s | 4.515e+04s   193 |
| rk_stage - integrals L_h      |    176750 | 2.936e+04s   134 | 3.222e+04s |  3.67e+04s    99 |
| rk_stage - inv mass + vec upd |    176750 |      7004s    99 | 1.207e+04s |  1.55e+04s   132 |
+-------------------------------------------+------------------+------------+------------------+
@endcode
This simulation had nearly 2 billion unknowns -- quite a large
computation indeed, and still only needed around 1.5 seconds per time
step.


<h3>Possibilities for extensions</h3>

The code presented here straight-forwardly extends to adaptive meshes, given
appropriate indicators for setting the refinement flags. Large-scale
adaptivity of a similar solver in the context of the acoustic wave equation
has been achieved by the <a href="https://github.com/kronbichler/exwave">exwave
project</a>. However, in the present context, the benefits of adaptivity are often
limited to early times and effects close to the origin of sound waves, as the
waves eventually reflect and diffract. This leads to steep gradients all over
the place, similar to turbulent flow, and a more or less globally
refined mesh.

Another topic that we did not discuss in the results section is a comparison
of different time integration schemes. The program provides four variants of
low-storage Runga--Kutta integrators that each have slightly different
accuracy and stability behavior. Among the schemes implemented here, the
higher-order ones provide additional accuracy but come with slightly lower
efficiency in terms of step size per stage before they violate the CFL
condition. An interesting extension would be to compare the low-storage
variants proposed here with standard Runge--Kutta integrators or to use vector
operations that are run separate from the mass matrix operation and compare
performance.


<h4>More advanced numerical flux functions and skew-symmetric formulations</h4>

As mentioned in the introduction, the modified Lax--Friedrichs flux and the
HLL flux employed in this program are only two variants of a large body of
numerical fluxes available in the literature on the Euler equations. One
example is the HLLC flux (Harten-Lax-van Leer-Contact) flux which adds the
effect of rarefaction waves missing in the HLL flux, or the Roe flux. As
mentioned in the introduction, the effect of numerical fluxes on high-order DG
schemes is debatable (unlike for the case of low-order discretizations).

A related improvement to increase the stability of the solver is to also
consider the spatial integral terms. A shortcoming in the rather naive
implementation used above is the fact that the energy conservation of the
original Euler equations (in the absence of shocks) only holds up to a
discretization error. If the solution is under-resolved, the discretization
error can give rise to an increase in the numerical energy and eventually
render the discretization unstable. This is because of the inexact numerical
integration of the terms in the Euler equations, which both contain rational
nonlinearities and higher-degree content from curved cells. A way out of this
dilemma are so-called skew-symmetric formulations, see @cite Gassner2013 for a
simple variant. Skew symmetry means that switching the role of the solution
$\mathbf{w}$ and test functions $\mathbf{v}$ in the weak form produces the
exact negative of the original quantity, apart from some boundary terms. In
the discrete setting, the challenge is to keep this skew symmetry also when
the integrals are only computed approximately (in the continuous case,
skew-symmetry is a consequence of integration by parts). Skew-symmetric
numerical schemes balance spatial derivatives in the conservative form
$(\nabla \mathbf v, \mathbf{F}(\mathbf w))_{K}$ with contributions in the
convective form $(\mathbf v, \tilde{\mathbf{F}}(\mathbf w)\nabla
\mathbf{w})_{K}$ for some $\tilde{\mathbf{F}}$. The precise terms depend on
the equation and the integration formula, and can in some cases by understood
by special skew-symmetric finite difference schemes.

To get started, interested readers could take a look at
https://github.com/kronbichler/advection_miniapp, where a
skew-symmetric DG formulation is implemented with deal.II for a simple advection
equation.

<h4>Equipping the code for supersonic calculations</h4>

As mentioned in the introduction, the solution to the Euler equations develops
shocks as the Mach number increases, which require additional mechanisms to
stabilize the scheme, e.g. in the form of limiters. The main challenge besides
actually implementing the limiter or artificial viscosity approach would be to
load-balance the computations, as the additional computations involved for
limiting the oscillations in troubled cells would make them more expensive than the
plain DG cells without limiting. Furthermore, additional numerical fluxes that
better cope with the discontinuities would also be an option.

One ingredient also necessary for supersonic flows are appropriate boundary
conditions. As opposed to the subsonic outflow boundaries discussed in the
introduction and implemented in the program, all characteristics are outgoing
for supersonic outflow boundaries, so we do not want to prescribe any external
data,
@f[
\mathbf{w}^+ = \mathbf{w}^- = \begin{pmatrix} \rho^-\\
(\rho \mathbf u)^- \\ E^-\end{pmatrix} \quad
 \text{(Neumann)}.
@f]

In the code, we would simply add the additional statement
@code
            else if (supersonic_outflow_boundaries.find(boundary_id) !=
                     supersonic_outflow_boundaries.end())
              {
                w_p        = w_m;
                at_outflow = true;
              }
@endcode
in the `local_apply_boundary_face()` function.

<h4>Extension to the linearized Euler equations</h4>

When the interest with an Euler solution is mostly in the propagation of sound
waves, it often makes sense to linearize the Euler equations around a
background state, i.e., a given density, velocity and energy (or pressure)
field, and only compute the change against these fields. This is the setting
of the wide field of aeroacoustics. Even though the resolution requirements
are sometimes considerably reduced, implementation gets somewhat more
complicated as the linearization gives rise to additional terms. From a code
perspective, in the operator evaluation we also need to equip the code with
the state to linearize against. This information can be provided either by
analytical functions (that are evaluated in terms of the position of the
quadrature points) or by a vector similar to the solution. Based on that
vector, we would create an additional FEEvaluation object to read from it and
provide the values of the field at quadrature points. If the background
velocity is zero and the density is constant, the linearized Euler equations
further simplify and can equivalently be written in the form of the
acoustic wave equation.

A challenge in the context of sound propagation is often the definition of
boundary conditions, as the computational domain needs to be of finite size,
whereas the actual simulation often spans an infinite (or at least much
larger) physical domain. Conventional Dirichlet or Neumann boundary conditions
give rise to reflections of the sound waves that eventually propagate back to
the region of interest and spoil the solution. Therefore, various variants of
non-reflecting boundary conditions or sponge layers, often in the form of
<a
href="https://en.wikipedia.org/wiki/Perfectly_matched_layer">perfectly
matched layers</a> -- where the solution is damped without reflection
-- are common.


<h4>Extension to the compressible Navier-Stokes equations</h4>

The solver presented in this tutorial program can also be extended to the
compressible Navier--Stokes equations by adding viscous terms, as described in
@cite FehnWallKronbichler2019. To keep as much of the performance obtained
here despite the additional cost of elliptic terms, e.g. via an interior
penalty method, one can switch the basis from FE_DGQ to FE_DGQHermite like in
the step-59 tutorial program.


<h4>Using cell-centric loops and shared memory</h4>

In this tutorial, we used face-centric loops. Here, cell and face integrals
are treated in separate loops, resulting in multiple writing accesses into the
result vector, which is relatively expensive on modern hardware since writing
operations generally result also in an implicit read operation. Element-centric
loops, on the other hand, are processing a cell and in direct succession
processing all its 2d faces. Although this kind of loop implies that fluxes have
to be computed twice (for each side of an interior face), the fact that the
result vector has to accessed only once might - and the fact that the resulting
algorithm is free of race-conditions and as such perfectly suitable for
shared memory - already give a performance boost. If you are interested in these
advanced topics, you can take a look at step-76 where we take the present
tutorial and modify it so that we can use these features.


examples/step-68/doc/intro.dox
<br>

<i>
This program was contributed by
Bruno Blais (Polytechnique Montréal),
Toni El Geitani Nehme (Polytechnique Montréal),
Rene Gassmöller (University of California Davis),
and Peter Munch (Technical University of Munich and Helmholtz-Zentrum Geesthacht).
Bruno Blais was supported by NSERC Discovery grant
RGPIN-2020-04510, by Compute Canada and Calcul Québec.
</i>

<h1>Introduction</h1>

<h3>Simulation of the motion of massless tracer particles in a vortical flow</h3>

Particles play an important part in numerical models for a large
 number of applications. Particles are routinely used
 as massless tracers to visualize the dynamic of a transient flow. They
 can also play an intrinsic role as part of a more complex finite element
 model, as is the case for the Particle-In-Cell (PIC) method @cite GLHPW2018
 or they can even be used to simulate the motion of granular matter, as in
 the Discrete Element Method (DEM) @cite Blais2019. In the case
 of DEM, the resulting model is not related to the finite element method anymore,
 but just leads to a system of ordinary differential equation which describes
 the motion of the particles and the dynamic of their collisions. All of
 these models can be built using deal.II's particle handling capabilities.

In the present step, we use particles as massless tracers to illustrate
the dynamic of a vortical flow. Since the particles are massless tracers,
the position of each particle $i$ is described by the
following ordinary differential equation (ODE):
@f[
\frac{d \textbf{x}_i}{dt} =\textbf{u}(\textbf{x}_i)
@f]

where $\textbf{x}_i$ is the position of particle $i$ and $\textbf{u}(\textbf{x}_i)$ the flow velocity at its position.
In the present step, this ODE is solved using the explicit Euler method. The resulting scheme is:
@f[
\textbf{x}_{i}^{n+1} = \textbf{x}_{i}^{n} + \Delta t \; \textbf{u}(\textbf{x}_{i}^{n})
@f]

where $\textbf{x}_{i}^{n+1}$ and $\textbf{x}_{i}^{n}$ are the position
of particle $i$ at time $t+\Delta t$ and $t$, respectively and where $\Delta t$
is the time step. In the present step, the velocity at the location of particles
is obtained in two different fashions:
- By evaluating the velocity function at the location of the particles;
- By evaluating the velocity function on a background triangulation and, using
a  finite element support, interpolating at the position of the particle.

The first approach is not practical, since the velocity profile
is generally not known analytically. The second approach, based on interpolating a solution
at the position of the particles, mimics exactly what would be done in a
realistic computational fluid dynamic simulation, and this follows the way we have also evaluated
the finite element solution at particle locations in step-19. In this step, we illustrate both strategies.

We note that much greater accuracy could be obtained by using a fourth
order Runge-Kutta method or another appropriate scheme for the time integration
of the motion of the particles.  Implementing a more advanced time-integration scheme
would be a straightforward extension of this step.

<h3>Particles in deal.II</h3>

In deal.II, Particles::Particle are very simple and flexible entities that can be used
to build PIC, DEM or any type of particle-based models. Particles have a location
in real space, a location in the reference space of the element in which they
are located and a unique ID. In the majority of cases, simulations that include
particles require a significant number of them. Thus, it becomes interesting
to handle all particles through an entity which agglomerates all particles.
In deal.II, this is achieved through the use of the Particles::ParticleHandler class.

By default, particles do not have a diameter,
a mass or any other physical properties which we would generally expect of physical particles. However, through
a ParticleHandler, particles have access to a Particles::PropertyPool. This PropertyPool is
an array which can be used to store an arbitrary number of properties
associated with the particles. Consequently, users can build their own
particle solver and attribute the desired properties to the particles (e.g., mass, charge,
diameter, temperature, etc.). In the present tutorial, this is used to
store the value of the fluid velocity and the process id to which the particles
belong.

<h3>Challenges related to distributed particle simulations</h3>

Although the present step is not computationally intensive, simulations that
include many particles can be computationally demanding and require parallelization.
The present step showcases the distributed parallel capabilities of deal.II for particles.
In general, there are three main challenges
that specifically arise in parallel distributed simulations that include particles:
- Generating the particles on the distributed triangulation;
- Exchanging the particles that leave local domains between the processors;
- Load balancing the simulation so that every processor has a similar computational load.
These challenges and their solution in deal.II have been discussed in more detail in
@cite GLHPW2018, but we will summarize them below.

There are of course also questions on simply setting up a code that uses particles. These have largely already been
addressed in step-19. Some more advanced techniques will also be discussed in step-70.

<h4>Parallel particle generation</h4>

Generating distributed particles in a scalable way is not straightforward since
the processor to which they belong must first be identified before the cell in
which they are located is found.  deal.II provides numerous capabilities to
generate particles through the Particles::Generator namespace.  Some of these
particle generators create particles only on the locally owned subdomain. For
example, Particles::Generators::regular_reference_locations() creates particles
at the same reference locations within each cell of the local subdomain and
Particles::Generators::probabilistic_locations() uses a globally defined probability
density function to determine how many and where to generate particles locally.

In other situations, such as the present step, particles must be generated at
specific locations on cells that may be owned only by a subset of the processors.
In  most of these situations, the insertion of the particles is done for a very
limited number of time-steps and, consequently, does not constitute a large
portion of the computational cost. For these occasions, deal.II provides
convenient Particles::Generators that can globally insert the particles even if
the particle is not located in a cell owned by the parallel process on which the call to create the particle is initiated. The
generators first locate on which subdomain the particles are situated, identify
in which cell they are located and exchange the necessary information among
the processors to ensure that the particle is generated with the right
properties. Consequently, this type of particle generation can be communication
intensive. The Particles::Generators::dof_support_points and the
Particles::Generators::quadrature_points generate particles using a
triangulation and the points of an associated DoFHandler or quadrature
respectively. The triangulation that is used to generate the particles can be
the same triangulation that is used for the background mesh, in which case these
functions are very similar to the
Particles::Generators::regular_reference_locations() function described in the
previous paragraph. However, the triangulation used to generate particles can
also be different (non-matching) from the triangulation of the background grid,
which is useful to generate particles in particular shapes (as in this
example), or to transfer information between two different computational grids
(as in step-70).  Furthermore, the Particles::ParticleHandler class provides the
Particles::ParticleHandler::insert_global_particles() function which enables the
global insertion of particles from a vector of arbitrary points and a global
vector of bounding boxes. In the present step, we use the
Particles::Generators::quadrature_points() function on a non-matching triangulation to
insert particles located at positions in the shape of a disk.

<h4>Particle exchange</h4>

As particles move around in parallel distributed computations they may leave
the locally owned subdomain and need to be transferred to their new owner
processes. This situation can arise in two very different ways: First, if the
previous owning process knows the new owner of the particles that were lost
(for example because the particles moved from the locally owned cell of one processor
into an adjacent ghost cells of a distributed
triangulation) then the transfer can be handled efficiently as a point-to-point
communication between each process and the new owners. This transfer happens
automatically whenever particles are sorted into their new cells. Secondly,
the previous owner may not know to which process the particle has moved. In
this case the particle is discarded by default, as a global search for the
owner can be expensive. step-19 shows how such a discarded particle can still
be collected, interpreted, and potentially reinserted by the user. In the
present example we prevent the second case by imposing a CFL criterion on the
timestep to ensure particles will at most move into the ghost layer of the
local process and can therefore be send to neighboring processes automatically.

<h4>Balancing mesh and particle load</h4>

The last challenge that arises in parallel distributed computations using
particles is to balance the computational load between work that is done on the
grid, for example solving the finite-element problem, and the work that is done
on the particles, for example advecting the particles or computing the forces
between particles or between particles and grid. By default, for example in
step-40, deal.II distributes the background mesh as evenly as possible between
the available processes, that is it balances the number of cells on each
process. However, if some cells own many more particles than other cells, or if
the particles of one cell are much more computationally expensive than the
particles in other cells, then this problem no longer scales efficiently (for a
discussion of what we consider "scalable" programs, see
@ref GlossParallelScaling "this glossary entry"). Thus, we have to apply a form of
"load balancing", which means we estimate the computational load that is
associated with each cell and its particles. Repartitioning the mesh then
accounts for this combined computational load instead of the simplified
assumption of the number of cells @cite GLHPW2018.

In this section we only discussed the particle-specific challenges in distributed
computation. Parallel challenges that particles share with
finite-element solutions (parallel output, data transfer during mesh
refinement) can be addressed with the solutions found for
finite-element problems already discussed in other examples.

<h3>The testcase</h3>

In the present step, we use particles as massless tracers to illustrate
the dynamics of a particular vortical flow: the Rayleigh--Kothe vortex. This flow pattern
is generally used as a complex test case for interface tracking methods
(e.g., volume-of-fluid and level set approaches) since
it leads to strong rotation and elongation of the fluid @cite Blais2013.

The stream function $\Psi$ of this Rayleigh-Kothe vortex is defined as:

@f[
\Psi = \frac{1}{\pi} \sin^2 (\pi x) \sin^2 (\pi y) \cos \left( \pi \frac{t}{T} \right)
@f]
where $T$ is half the period of the flow. The velocity profile in 2D ($\textbf{u}=[u,v]^T$) is :
@f{eqnarray*}
   u &=&  - \frac{\partial\Psi}{\partial y} = -2 \sin^2 (\pi x) \sin (\pi y) \cos (\pi y)  \cos \left( \pi \frac{t}{T} \right)\\
   v &=&  \frac{\partial\Psi}{\partial x} = 2 \cos(\pi x) \sin(\pi x) \sin^2 (\pi y) \cos \left( \pi \frac{t}{T} \right)
@f}

The velocity profile is illustrated in the following animation:

@htmlonly
<p align="center">
  <iframe width="560" height="500" src="https://www.youtube.com/embed/m6hQm7etji8"
   frameborder="0"
   allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
   allowfullscreen></iframe>
 </p>
@endhtmlonly

It can be seen that this velocity reverses periodically due to the term
$\cos \left( \pi \frac{t}{T} \right)$ and that material will end up at its
starting position after every period of length $t=2T$. We will run this tutorial
program for exactly one period and compare the final particle location to the
initial location to illustrate this flow property. This example uses the testcase
to produce two models that handle the particles
slightly differently. The first model prescribes the exact analytical velocity
solution as the velocity for each particle. Therefore in this model there is no
error in the assigned velocity to the particles, and any deviation of particle
positions from the analytical position at a given time results from the error
in solving the equation of motion for the particle inexactly, using a time stepping method. In the second model the
analytical velocity field is first interpolated to a finite-element vector
space (to simulate the case that the velocity was obtained from solving a
finite-element problem, in the same way as the ODE for each particle in step-19 depends on a finite element
solution). This finite-element "solution" is then evaluated at
the locations of the particles to solve their equation of motion. The
difference between the two cases allows to assess whether the chosen
finite-element space is sufficiently accurate to advect the particles with the
optimal convergence rate of the chosen particle advection scheme, a question
that is important in practice to determine the accuracy of the combined
algorithm (see e.g. @cite Gassmoller2019).


examples/step-68/doc/results.dox
<h1>Results</h1>

The directory in which this program is run contains an example parameter file by default.
If you do not specify a parameter file as an argument on the command
line, the program will try to read the file "parameters.prm" by default, and
will execute the code.

On any number of cores, the simulation output will look like:

@code
bash$ mpirun -np 4 ./step-68 parameters.prm
Number of particles inserted: 606
Repartitioning triangulation after particle generation
Writing particle output file: analytical-particles-0
Writing particle output file: analytical-particles-10
Writing particle output file: analytical-particles-20
Writing particle output file: analytical-particles-30
...
Number of particles inserted: 606
Repartitioning triangulation after particle generation
Writing particle output file: interpolated-particles-0
Writing background field file: background-0
Writing particle output file: interpolated-particles-10
Writing background field file: background-10
Writing particle output file: interpolated-particles-20
Writing background field file: background-20
Writing particle output file: interpolated-particles-30
Writing background field file: background-30
...
Writing particle output file: interpolated-particles-1980
Writing background field file: background-1980
Writing particle output file: interpolated-particles-1990
Writing background field file: background-1990
Writing particle output file: interpolated-particles-2000
Writing background field file: background-2000
@endcode

We note that, by default, the simulation runs the particle tracking with
an analytical velocity for 2000 iterations, then restarts from the beginning and runs the particle tracking with
velocity interpolation for the same duration. The results are written every
10th iteration.

<h3> Motion of the particles </h3>

The following animation displays the trajectory of the particles as they
are advected by the flow field. We see that after the complete duration of the
flow, the particle go back to their initial configuration as is expected.

@htmlonly
<p align="center">
  <iframe width="560" height="500" src="https://www.youtube.com/embed/EbgS5Ch35Xs"
   frameborder="0"
   allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
   allowfullscreen></iframe>
 </p>
@endhtmlonly

<h3> Dynamic load balancing </h3>

The following animation shows the impact of dynamic load balancing. We clearly
see that the subdomains adapt themselves to balance the number of particles per
subdomain. However, a perfect load balancing is not reached, in part due to
the coarseness of the background mesh.

@htmlonly
<p align="center">
  <iframe width="560" height="500" src="https://www.youtube.com/embed/ubUcsR4ECj4"
   frameborder="0"
   allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
   allowfullscreen></iframe>
 </p>
@endhtmlonly


<h3>Possibilities for extensions</h3>

This program highlights some of the main capabilities for handling particles in deal.II, notably their
capacity to be used in distributed parallel simulations. However, this step could
be extended in numerous manners:
- High-order time integration (for example using a Runge-Kutta 4 method) could be
used to increase the accuracy or allow for larger time-step sizes with the same accuracy.
- The full equation of motion (with inertia) could be solved for the particles. In
this case the particles would need to have additional properties such as their mass,
as in step-19, and if one wanted to also consider interactions with the fluid, their diameter.
- Coupling to a flow solver. This step could be straightforwardly coupled to any parallel
program in which the Stokes (step-32, step-70) or the Navier-Stokes equations are solved (e.g., step-57).
- Computing the difference in final particle positions between the two models
would allow to quantify the influence of the interpolation error on particle motion.


examples/step-69/doc/intro.dox
<i>
  This program was contributed by Matthias Maier (Texas A&M University),
  and Ignacio Tomas (Sandia National Laboratories$^{\!\dagger}$).
</i>

$^\dagger$<em>Sandia National Laboratories is a multimission laboratory
managed and operated by National Technology & Engineering Solutions of Sandia,
LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S.
Department of Energy's National Nuclear Security Administration under contract
DE-NA0003525. This document describes objective technical results and analysis.
Any subjective views or opinions that might be expressed in the paper do not
necessarily represent the views of the U.S. Department of Energy or the United
States Government.</em>

@note This tutorial step implements a first-order accurate <i>guaranteed
maximum wavespeed method</i> based on a first-order <i>graph viscosity</i>
for solving Euler's equations of gas dynamics @cite GuermondPopov2016. As
such it is presented primarily for educational purposes. For actual
research computations you might want to consider exploring a corresponding
<a href="https://github.com/conservation-laws/ryujin">high-performance
implementation of a second-order accurate scheme</a> that uses <i>convex
limiting</i> techniques, and strong stability-preserving (SSP) time
integration, see @cite GuermondEtAl2018
(<a href="https://conservation-laws.43-1.org/">website</a>).

@dealiiTutorialDOI{10.5281/zenodo.3698223,https://zenodo.org/badge/DOI/10.5281/zenodo.3698223.svg}

<a name="Intro"></a>
<h1>Introduction</h1>

This tutorial presents a first-order scheme for solving compressible
Euler's equations that is based on three ingredients: a
<i>collocation</i>-type discretization of Euler's equations in the context
of finite elements; a graph-viscosity stabilization based on a
<i>guaranteed</i> upper bound of the local wave speed; and explicit
time-stepping. As such, the ideas and techniques presented in this tutorial
step are drastically different from those used in step-33, which focuses on
the use of automatic differentiation. From a programming perspective this
tutorial will focus on a number of techniques found in large-scale
computations: hybrid thread-MPI parallelization; efficient local numbering
of degrees of freedom; concurrent post-processing and write-out of results
using worker threads; as well as checkpointing and restart.

It should be noted that first-order schemes in the context of hyperbolic
conservation laws require prohibitively many degrees of freedom to resolve
certain key features of the simulated fluid, and thus, typically only serve
as elementary building blocks in higher-order schemes
@cite GuermondEtAl2018. However, we hope that the reader still finds the
tutorial step to be a good starting point (in particular with respect to
the programming techniques) before jumping into full research codes such as
the second-order scheme discussed in @cite GuermondEtAl2018.


<a name="eulerequations"></a>
<h3>Euler's equations of gas dynamics</h3>

The compressible Euler's equations of gas dynamics are written in
conservative form as follows:
@f{align}
\mathbf{u}_t + \text{div} \, \mathbb{f}(\mathbf{u}) = \boldsymbol{0} ,
@f}
where $\mathbf{u}(\textbf{x},t):\mathbb{R}^{d} \times \mathbb{R}
\rightarrow \mathbb{R}^{d+2}$, and $\mathbb{f}(\mathbf{u}):\mathbb{R}^{d+2}
\rightarrow \mathbb{R}^{(d+2) \times d}$, and $d \geq 1$ is the space
dimension. We say that $\mathbf{u} \in \mathbb{R}^{d+2}$ is the state and
$\mathbb{f}(\mathbf{u}) \in  \mathbb{R}^{(d+2) \times d}$ is the flux of
the system. In the case of Euler's equations the state is given by
$\textbf{u} = [\rho, \textbf{m},E]^{\top}$: where $\rho \in \mathbb{R}^+$
denotes the density, $\textbf{m} \in \mathbb{R}^d$ is the momentum, and $E
\in \mathbb{R}^+$ is the total energy of the system. The flux of the system
$\mathbb{f}(\mathbf{u})$ is defined as
@f{align*}
\mathbb{f}(\textbf{u})
=
\begin{bmatrix}
  \textbf{m}^\top \\
  \rho^{-1} \textbf{m} \otimes \textbf{m} + \mathbb{I} p\\
  \tfrac{\textbf{m}^\top}{\rho} (E + p)
\end{bmatrix},
@f}
where $\mathbb{I} \in \mathbb{R}^{d \times d}$ is the identity matrix and
$\otimes$ denotes the tensor product. Here, we have introduced the pressure
$p$ that, in general, is defined by a closed-form equation of state.
In this tutorial we limit the discussion to the class of polytropic
ideal gases for which the pressure is given by
@f{align*}
p = p(\textbf{u}) := (\gamma -1) \Big(E -
\tfrac{|\textbf{m}|^2}{2\,\rho}
\Big),
@f}
where the factor $\gamma \in (1,5/3]$ denotes the <a
href="https://en.wikipedia.org/wiki/Heat_capacity_ratio">ratio of specific
heats</a>.


<h4>Solution theory</h4>

Hyperbolic conservation laws, such as
@f{align*}
\mathbf{u}_t + \text{div} \, \mathbb{f}(\mathbf{u}) = \boldsymbol{0},
@f}
pose a significant challenge with respect to solution theory. An evident
observation is that rewriting the equation in variational form and testing with
the solution itself does not lead to an energy estimate because the pairing
$\langle \text{div} \, \mathbb{f}(\mathbf{u}), \mathbf{u}\rangle$ (understood as
the $L^2(\Omega)$ inner product or duality pairing) is not guaranteed to be
non-negative. Notions such as energy-stability or $L^2(\Omega)$-stability are
(in general) meaningless in this context.

Historically, the most fruitful step taken in order to deepen the
understanding of hyperbolic conservation laws was to assume that the
solution is formally defined as $\mathbf{u} := \lim_{\epsilon \rightarrow
0^+} \mathbf{u}^{\epsilon}$ where $\mathbf{u}^{\epsilon}$ is the solution
of the parabolic regularization
@f{align}
\mathbf{u}_t^{\epsilon} + \text{div} \, \mathbb{f}(\mathbf{u}^{\epsilon})
- {\epsilon} \Delta \mathbf{u}^{\epsilon} = 0.
@f}
Such solutions, which are understood as the solution recovered in the
zero-viscosity limit, are often referred to as <i>viscosity solutions</i>.
(This is, because physically $\epsilon$ can be understood as related to the viscosity of the
fluid, i.e., a quantity that indicates the amount of friction neighboring gas particles moving at
different speeds exert on each other. The Euler equations themselves are derived under
the assumption of no friction, but can physically be expected to describe the limiting
case of vanishing friction or viscosity.)
Global existence and uniqueness of such solutions is an open issue.
However, we know at least that if such viscosity solutions exists they have
to satisfy the constraint $\textbf{u}(\mathbf{x},t) \in \mathcal{B}$ for
all $\mathbf{x} \in \Omega$ and $t \geq 0$ where
@f{align}
  \mathcal{B} = \big\{ \textbf{u} =
  [\rho, \textbf{m},E]^{\top} \in \mathbb{R}^{d+2} \, \big |
  \
  \rho > 0 \, ,
  \
  \ E - \tfrac{|\textbf{m}|^2}{2 \rho} > 0 \, ,
  \
  s(\mathbf{u}) \geq \min_{x \in \Omega} s(\mathbf{u}_0(\mathbf{x}))
  \big\}.
@f}
Here, $s(\mathbf{u})$ denotes the specific entropy
@f{align}
  s(\mathbf{u}) = \ln \Big(\frac{p(\mathbf{u})}{\rho^{\gamma}}\Big).
@f}
We will refer to $\mathcal{B}$ as the invariant set of Euler's equations.
In other words, a state $\mathbf{u}(\mathbf{x},t)\in\mathcal{B}$ obeys
positivity of the density, positivity of the internal energy, and a local
minimum principle on the specific entropy. This condition is a simplified
version of a class of pointwise stability constraints satisfied by the
exact (viscosity) solution. By pointwise we mean that the constraint has to
be satisfied at every point of the domain, not just in an averaged
(integral, or high order moments) sense.

In context of a numerical approximation, a violation of such a constraint
has dire consequences: it almost surely leads to catastrophic failure of
the numerical scheme, loss of hyperbolicity, and overall, loss of
well-posedness of the (discrete) problem. It would also mean that we have computed
something that can not be interpreted physically. (For example, what are we to make
of a computed solution with a negative density?) In the following we will
formulate a scheme that ensures that the discrete approximation of
$\mathbf{u}(\mathbf{x},t)$ remains in $\mathcal{B}$.


<h4>Variational versus collocation-type discretizations</h4>

Following Step-9, Step-12, Step-33, and Step-67, at this point it might look
tempting to base a discretization of Euler's equations on a (semi-discrete)
variational formulation:
@f{align*}
  (\partial_t\mathbf{u}_{h},\textbf{v}_h)_{L^2(\Omega)}
  - ( \mathbb{f}(\mathbf{u}_{h}) ,\text{grad} \, \textbf{v}_{h})_{L^2(\Omega)}
  + s_h(\mathbf{u}_{h},\textbf{v}_h)_{L^2(\Omega)} = \boldsymbol{0}
  \quad\forall \textbf{v}_h \in \mathbb{V}_h.
@f}
Here, $\mathbb{V}_h$ is an appropriate finite element space, and
$s_h(\cdot,\cdot)_{L^2(\Omega)}$ is some linear stabilization method
(possibly complemented with some ad-hoc shock-capturing technique, see for
instance Chapter 5 of @cite GuermondErn2004 and references therein). Most
time-dependent discretization approaches described in the deal.II tutorials
are based on such a (semi-discrete) variational approach. Fundamentally,
from an analysis perspective, variational discretizations are conceived
to provide some notion of global (integral) stability, meaning an
estimate of the form
@f{align*}
  |\!|\!| \mathbf{u}_{h}(t) |\!|\!| \leq |\!|\!| \mathbf{u}_{h}(0) |\!|\!|
@f}
holds true, where $|\!|\!| \cdot |\!|\!| $ could represent the
$L^2(\Omega)$-norm or, more generally, some discrete (possibly mesh
dependent) energy-norm. Variational discretizations of hyperbolic
conservation laws have been very popular since the mid eighties, in
particular combined with SUPG-type stabilization and/or upwinding
techniques (see the early work of @cite Brooks1982 and @cite Johnson1986). They
have proven to be some of the best approaches for simulations in the subsonic
shockless regime and similarly benign situations.

<!-- In particular, tutorial Step-67 focuses on Euler's equation of gas
dynamics in the subsonic regime using dG techniques. -->

However, in the transonic and supersonic regimes, and shock-hydrodynamics
applications the use of variational schemes might be questionable. In fact,
at the time of this writing, most shock-hydrodynamics codes are still
firmly grounded on finite volume methods. The main reason for failure of
variational schemes in such extreme regimes is the lack of pointwise
stability. This stems from the fact that <i>a priori</i> bounds on
integrated quantities (e.g. integrals of moments) have in general no
implications on pointwise properties of the solution. While some of these
problems might be alleviated by the (perpetual) chase of the right shock
capturing scheme, finite difference-like and finite volume schemes still
have an edge in many regards.

In this tutorial step we therefore depart from variational schemes. We will
present a completely algebraic formulation (with the flavor of a
collocation-type scheme) that preserves constraints pointwise, i.e.,
@f{align*}
  \textbf{u}_h(\mathbf{x}_i,t) \in \mathcal{B}
  \;\text{at every node}\;\mathbf{x}_i\;\text{of the mesh}.
@f}
Contrary to finite difference/volume schemes, the scheme implemented in
this step maximizes the use of finite element software infrastructure,
works on any mesh, in any space dimension, and is theoretically guaranteed
to always work, all the time, no exception. This illustrates that deal.II
can be used far beyond the context of variational schemes in Hilbert spaces
and that a large number of classes, modules and namespaces from deal.II can
be adapted for such a purpose.


<h3>Description of the scheme </h3>

Let $\mathbb{V}_h$ be scalar-valued finite dimensional space spanned by a
basis $\{\phi_i\}_{i \in \mathcal{V}}$ where: $\phi_i:\Omega \rightarrow
\mathbb{R}$ and $\mathcal{V}$ is the set of all indices (nonnegative
integers) identifying each scalar Degree of Freedom (DOF) in the mesh.
Therefore a scalar finite element functional $u_h \in \mathbb{V}_h$ can
be written as $u_h = \sum_{i \in \mathcal{V}} U_i \phi_i$ with $U_i \in
\mathbb{R}$. We introduce the notation for vector-valued approximation
spaces $\pmb{\mathbb{V}}_h := \{\mathbb{V}_h\}^{d+2}$. Let $\mathbf{u}_h
\in \pmb{\mathbb{V}}_h$, then it can be written as $\mathbf{u}_h = \sum_{i
\in \mathcal{V}} \mathbf{U}_i \phi_i$ where $\mathbf{U}_i \in
\mathbb{R}^{d+2}$ and $\phi_i$ is a scalar-valued shape function.

@note We purposely refrain from using vector-valued finite element
spaces in our notation. Vector-valued finite element spaces
are natural for variational formulations of PDE systems (e.g. Navier-Stokes).
In such context, the interactions that have to be computed describe
<i>interactions between DOFs</i>: with proper renumbering of the
vector-valued DoFHandler (i.e. initialized with an FESystem) it is possible
to compute the block-matrices (required in order to advance the solution)
with relative ease. However, the interactions that have to be computed in
the context of time-explicit collocation-type schemes (such as finite
differences and/or the scheme presented in this tutorial) can be
better described as <i>interactions between nodes</i> (not between DOFs).
In addition, in our case we do not solve a linear equation in order to
advance the solution. This leaves very little reason to use vector-valued
finite element spaces both in theory and/or practice.

We will use the usual Lagrange finite elements: let $\{\mathbf{x}_i\}_{i \in
\mathcal{V}}$ denote the set of all support points (see @ref GlossSupport "this
glossary entry"), where $\mathbf{x}_i \in \mathbb{R}^d$. Then each index $i \in
\mathcal{V}$ uniquely identifies a support point $\mathbf{x}_i$, as well as a
scalar-valued shape function $\phi_i$. With this notation at hand we can define
the (explicit time stepping) scheme as:
@f{align*}{
  m_i \frac{\mathbf{U}_i^{n+1} - \mathbf{U}_i^{n}}{\tau}
  + \sum_{j \in \mathcal{I}(i)} \mathbb{f}(\mathbf{U}_j^{n})\cdot
  \mathbf{c}_{ij} - \sum_{j \in \mathcal{I}(i)}
  d_{ij} \mathbf{U}_j^{n} = \boldsymbol{0} \, ,
@f}
where
  - $m_i \dealcoloneq \int_{\Omega} \phi_i \, \mathrm{d}\mathbf{x}$
    is the lumped mass matrix
  - $\tau$ is the time step size
  - $\mathbf{c}_{ij} \dealcoloneq \int_{\Omega} \nabla\phi_j\phi_i \,
    \mathrm{d}\mathbf{x}$ (note that $\mathbf{c}_{ij}\in \mathbb{R}^d$)
    is a vector-valued matrix that was used to approximate the divergence
    of the flux in a weak sense.
  - $\mathcal{I}(i) \dealcoloneq \{j \in \mathcal{V} \ | \ \mathbf{c}_{ij}
    \not \equiv \boldsymbol{0}\} \cup \{i\}$ is the adjacency list
    containing all degrees of freedom coupling to the index $i$. In other
    words $\mathcal{I}(i)$ contains all nonzero column indices for row
    index i. $\mathcal{I}(i)$ will also be called a "stencil".
  - $\mathbb{f}(\mathbf{U}_j^{n})$ is the flux $\mathbb{f}$ of the
    hyperbolic system evaluated for the state $\mathbf{U}_j^{n}$ associated
    with support point $\mathbf{x}_j$.
  - $d_{ij} \dealcoloneq \max \{ \lambda_{\text{max}}
    (\mathbf{U}_i^{n},\mathbf{U}_j^{n}, \textbf{n}_{ij}),
    \lambda_{\text{max}} (\mathbf{U}_j^{n}, \mathbf{U}_i^{n},
    \textbf{n}_{ji}) \} \|\mathbf{c}_{ij}\|$ if $i \not = j$ is the so
    called <i>graph viscosity</i>. The graph viscosity serves as a
    stabilization term, it is somewhat the discrete counterpart of
    $\epsilon \Delta \mathbf{u}$ that appears in the notion of viscosity
    solution described above. We will base our construction of $d_{ij}$ on
    an estimate of the maximal local wavespeed $\lambda_{\text{max}}$ that
    will be explained in detail in a moment.
  - the diagonal entries of the viscosity matrix are defined as
    $d_{ii} = - \sum_{j \in \mathcal{I}(i)\backslash \{i\}} d_{ij}$.
  - $\textbf{n}_{ij} = \frac{\mathbf{c}_{ij}}{ \|\mathbf{c}_{ij}\| }$ is a
    normalization of the $\textbf{c}_{ij}$ matrix that enters the
    approximate Riemann solver with which we compute an the approximations
    $\lambda_{\text{max}}$ on the local wavespeed. (This will be explained
    further down below).

The definition of $\lambda_{\text{max}} (\mathbf{U},\mathbf{V},
\textbf{n})$ is far from trivial and we will postpone the precise
definition in order to focus first on some algorithmic and implementation
questions. We note that
  - $m_i$ and $\mathbf{c}_{ij}$ do not evolve in time (provided we keep the
    discretization fixed). It thus makes sense to assemble these
    matrices/vectors once in a so called <i>offline computation</i> and reuse
    them in every time step. They are part of what we are going to call
    off-line data.
  - At every time step we have to evaluate $\mathbb{f}(\mathbf{U}_j^{n})$ and
    $d_{ij} \dealcoloneq \max \{ \lambda_{\text{max}}
    (\mathbf{U}_i^{n},\mathbf{U}_j^{n}, \textbf{n}_{ij}),
    \lambda_{\text{max}} (\mathbf{U}_j^{n}, \mathbf{U}_i^{n},
    \textbf{n}_{ji}) \} \|\mathbf{c}_{ij}\| $, which will
    constitute the bulk of the computational cost.

Consider the following pseudo-code, illustrating a possible straight
forward strategy for computing the solution $\textbf{U}^{n+1}$ at a new
time $t_{n+1} = t_n + \tau_n$ given a known state $\textbf{U}^{n}$ at time
$t_n$:
@f{align*}
&\textbf{for } i \in \mathcal{V} \\
&\ \ \ \  \{\mathbf{c}_{ij}\}_{j \in \mathcal{I}(i)} \leftarrow
\mathtt{gather\_cij\_vectors} (\textbf{c}, \mathcal{I}(i)) \\
&\ \ \ \ \{\textbf{U}_j^n\}_{j \in \mathcal{I}(i)} \leftarrow
\mathtt{gather\_state\_vectors} (\textbf{U}^n, \mathcal{I}(i)) \\
&\ \ \ \ \ \textbf{U}_i^{n+1} \leftarrow \mathbf{U}_i^{n} \\
&\ \ \ \ \textbf{for } j \in \mathcal{I}(i)\backslash\{i\} \\
&\ \ \ \ \ \ \ \  \texttt{compute } d_{ij} \\
&\ \ \ \ \ \ \ \  \texttt{compute } \mathbb{f}(\mathbf{U}_j^{n}) \\
&\ \ \ \ \ \ \ \  \textbf{U}_i^{n+1} \leftarrow \textbf{U}_i^{n+1} - \frac{\tau_n}{m_i}
 \mathbb{f}(\mathbf{U}_j^{n})\cdot \mathbf{c}_{ij} + d_{ij} \mathbf{U}_j^{n} \\
&\ \ \ \ \textbf{end} \\
&\ \ \ \ \mathtt{scatter\_updated\_state} (\textbf{U}_i^{n+1}) \\
&\textbf{end}
@f}

We note here that:
- This "assembly" does not require any form of quadrature or cell-loops.
- Here $\textbf{c}$ and $\textbf{U}^n$ are a global matrix and a global vector
containing all the vectors $\mathbf{c}_{ij}$ and all the states
$\mathbf{U}_j^n$ respectively.
- $\mathtt{gather\_cij\_vectors}$, $\mathtt{gather\_state\_vectors}$, and
$\mathtt{scatter\_updated\_state}$ are hypothetical implementations that
either collect (from) or write (into) global matrices and vectors.
- If we assume a Cartesian mesh in two space
dimensions, first-order polynomial space $\mathbb{Q}^1$, and that
$\mathbf{x}_i$ is an interior node (i.e. $\mathbf{x}_i$ is not on the boundary
of the domain) then: $\{\textbf{U}_j^n\}_{j \in \mathcal{I}(i)}$ should contain
nine state vector elements (i.e. all the states in the patch/macro element
associated to the shape function $\phi_i$). This is one of the major
differences with the usual cell-based loop where the gather functionality
(encoded in FEValuesBase<dim, spacedim>.get_function_values() in the case
of deal.II) only collects values for the local cell (just a subset of the
patch).

The actual implementation will deviate from above code in one key aspect:
the time-step size $\tau$ has to be chosen subject to a CFL condition
@f{align*}
  \tau_n = c_{\text{cfl}}\,\min_{
  i\in\mathcal{V}}\left(\frac{m_i}{-2\,d_{ii}^{n}}\right),
@f}
where $0<c_{\text{cfl}}\le1$ is a chosen constant. This will require to
compute all $d_{ij}$ in a separate step prior to actually performing above
update. The core principle remains unchanged, though: we do not loop over
cells but rather over all edges of the sparsity graph.

@note It is not uncommon to encounter such fully-algebraic schemes (i.e.
no bilinear forms, no cell loops, and no quadrature) outside of the finite
element community in the wider CFD community. There is a rich history of
application of this kind of schemes, also called <i>edge-based</i> or
<i>graph-based</i> finite element schemes (see for instance
@cite Rainald2008 for a historical overview). However, it is important to
highlight that the algebraic structure of the scheme (presented in this
tutorial) and the node-loops are not just a performance gimmick. Actually, the
structure of this scheme was born out of theoretical necessity: the proof of
pointwise stability of the scheme hinges on the specific algebraic structure of
the scheme. In addition, it is not possible to compute the algebraic
viscosities $d_{ij}$ using cell-loops since they depend nonlinearly on
information that spans more than one cell (superposition does not hold: adding
contributions from separate cells does not lead to the right result).

<h3>Stable boundary conditions and conservation properties.</h3>

In the example considered in this tutorial step we use three different types of
boundary conditions: essential-like boundary conditions (we prescribe a
state at the left boundary of our domain), outflow boundary conditions
(also called "do-nothing" boundary conditions) at the right boundary of the
domain, and "reflecting" boundary conditions $\mathbf{m} \cdot
\boldsymbol{\nu} = 0$ (also called "slip" boundary conditions) at the top,
bottom, and surface of the obstacle. We will not discuss much about
essential and "do-nothing" boundary conditions since their implementation
is relatively easy and the reader will be able to pick-up the
implementation directly from the (documented) source code. In this portion
of the introduction we will focus only on the "reflecting" boundary
conditions which are somewhat more tricky.

@note At the time of this writing (early 2020) it is not unreasonable to say
that both analysis and implementation of stable boundary conditions for
hyperbolic systems of conservation laws is an open issue. For the case of
variational formulations, stable boundary conditions are those leading to a
well-posed (coercive) bilinear form. But for general hyperbolic
systems of conservation laws (and for the algebraic formulation used in this
tutorial) coercivity has no applicability and/or meaning as a notion of
stability. In this tutorial step we will use preservation of the invariant set
as our main notion of stability which (at the very least) guarantees
well-posedness of the discrete problem.

For the case of the reflecting boundary conditions we will proceed as follows:
- For every time step advance in time satisfying no boundary condition at all.
- Let $\partial\Omega^r$ be the portion of the boundary where we want to
  enforce reflecting boundary conditions. At the end of the time step we enforce
  reflecting boundary conditions strongly in a post-processing step where we
  execute the projection
    @f{align*}
    \mathbf{m}_i \dealcoloneq \mathbf{m}_i - (\widehat{\boldsymbol{\nu}}_i
    \cdot \mathbf{m}_i)  \widehat{\boldsymbol{\nu}}_i \ \
    \text{where} \ \
    \widehat{\boldsymbol{\nu}}_i \dealcoloneq
    \frac{\int_{\partial\Omega} \phi_i \widehat{\boldsymbol{\nu}} \,
    \, \mathrm{d}\mathbf{s}}{\big|\int_{\partial\Omega} \phi_i
    \widehat{\boldsymbol{\nu}} \, \mathrm{d}\mathbf{s}\big|}
    \ \ \text{for all }\mathbf{x}_i \in \partial\Omega^r
    \ \ \ \ \boldsymbol{(1)}
    @f}
  that removes the normal component of $\mathbf{m}$. This is a somewhat
  naive idea that preserves a few fundamental properties of the PDE as we
  explain below.

This is approach is usually called "explicit treatment of boundary conditions".
The well seasoned finite element person might find this approach questionable.
No doubt, when solving parabolic, or elliptic equations, we typically enforce
essential (Dirichlet-like) boundary conditions by making them part of the
approximation space $\mathbb{V}$, and treat natural (e.g. Neumann) boundary
conditions as part of the variational formulation. We also know that explicit
treatment of boundary conditions (in the context of parabolic PDEs) almost
surely leads to catastrophic consequences. However, in the context of nonlinear
hyperbolic equations we have that:
- It is relatively easy to prove that (for the case of reflecting boundary
conditions) explicit treatment of boundary conditions is not only conservative
but also guarantees preservation of the property $\mathbf{U}_i \in \mathcal{B}$
for all $i \in \mathcal{V}$ (well-posedness). This is perhaps the most
important reason to use explicit enforcement of boundary conditions.
- To the best of our knowledge: we are not aware of any mathematical result
proving that it is possible to guarantee the property $\mathbf{U}_i \in
\mathcal{B}$ for all $i \in \mathcal{V}$ when using either direct enforcement of
boundary conditions into the approximation space, or weak enforcement using the
Nitsche penalty method (which is for example widely used in discontinuous
Galerkin schemes). In addition, some of these traditional ideas lead to quite
restrictive time step constraints.
- There is enough numerical evidence suggesting that explicit treatment of
Dirichlet-like boundary conditions is stable under CFL conditions and does not
introduce any loss in accuracy.

If $\mathbf{u}_t + \text{div} \, \mathbb{f}(\mathbf{u}) = \boldsymbol{0}$
represents Euler's equation with reflecting boundary conditions on the entirety
of the boundary (i.e. $\partial\Omega^r \equiv \partial\Omega$) and we
integrate in space and time $\int_{\Omega}\int_{t_1}^{t_2}$ we would obtain
@f{align*}
\int_{\Omega} \rho(\mathbf{x},t_2) \, \mathrm{d}\mathbf{x} =
\int_{\Omega} \rho(\mathbf{x},t_1) \, \mathrm{d}\mathbf{x} \ , \ \
\int_{\Omega} \mathbf{m}(\mathbf{x},t_2) \, \mathrm{d}\mathbf{x}
+ \int_{t_1}^{t_2} \! \int_{\partial\Omega} p \boldsymbol{\nu} \,
\mathrm{d}\mathbf{s} \mathrm{d}t =
\int_{\Omega} \mathbf{m}(\mathbf{x},t_1) \,
\mathrm{d}\mathbf{x} \ , \ \
\int_{\Omega} E(\mathbf{x},t_2) \, \mathrm{d}\mathbf{x} =
\int_{\Omega} E(\mathbf{x},t_1) \, \mathrm{d}\mathbf{x} \ \ \ \
\boldsymbol{(2)}
@f}
Note that momentum is NOT a conserved quantity (interaction with walls leads to
momentum gain/loss): however $\mathbf{m}$ has to satisfy a momentum balance.
Even though we will not use reflecting boundary conditions in the entirety of
the domain, we would like to know that our implementation of reflecting
boundary conditions is consistent with the conservation properties mentioned
above. In particular, if we use the projection $\boldsymbol{(1)}$ in the
entirety of the domain the following discrete mass-balance can be guaranteed:
@f{align*}
\sum_{i \in \mathcal{V}} m_i \rho_i^{n+1} =
\sum_{i \in \mathcal{V}} m_i \rho_i^{n} \ , \ \
\sum_{i \in \mathcal{V}} m_i \mathbf{m}_i^{n+1}
+ \tau_n \int_{\partial\Omega} \Big(\sum_{i \in \mathcal{V}} p_i^{n} \phi_i\Big)
\widehat{\boldsymbol{\nu}} \mathrm{d}\mathbf{s} =
\sum_{i \in \mathcal{V}} m_i \mathbf{m}_i^{n} \ , \ \
\sum_{i \in \mathcal{V}} m_i E_i^{n+1} = \sum_{i \in \mathcal{V}} m_i
E_i^{n} \ \ \ \
\boldsymbol{(3)}
@f}
where $p_i$ is the pressure at the nodes that lie at the boundary. Clearly
$\boldsymbol{(3)}$ is the discrete counterpart of $\boldsymbol{(2)}$. The
proof of identity $\boldsymbol{(3)}$ is omitted, but we briefly mention that
it hinges on the definition of the <i>nodal normal</i>
$\widehat{\boldsymbol{\nu}}_i$ provided in $\boldsymbol{(1)}$. We also note that
this enforcement of reflecting boundary conditions is different from the one
originally advanced in @cite GuermondEtAl2018.


examples/step-69/doc/results.dox
<a name="Results"></a>
<h1>Results</h1>

Running the program with default parameters in release mode takes about 1
minute on a 4 core machine (with hyperthreading):
@verbatim
# mpirun -np 4 ./step-69 | tee output
Reading parameters and allocating objects... done

    ####################################################
    #########                                  #########
    #########       create triangulation       #########
    #########                                  #########
    ####################################################

Number of active cells:       36864

    ####################################################
    #########                                  #########
    #########       compute offline data       #########
    #########                                  #########
    ####################################################

Number of degrees of freedom: 37376

    ####################################################
    #########                                  #########
    #########         set up time step         #########
    #########                                  #########
    ####################################################

    ####################################################
    #########                                  #########
    #########    interpolate initial values    #########
    #########                                  #########
    #########                                  #########
    ####################################################

TimeLoop<dim>::interpolate_initial_values(t = 0)
TimeLoop<dim>::output(t = 0, checkpoint = 0)

    ####################################################
    #########                                  #########
    #########         enter main loop          #########
    #########                                  #########
    #########                                  #########
    ####################################################

    ####################################################
    #########                                  #########
    #########      Cycle  000001  (0.0%)       #########
    #########      at time t = 0.00000000      #########
    #########                                  #########
    ####################################################

[...]

    ####################################################
    #########                                  #########
    #########     Cycle  007553  (100.0%)      #########
    #########      at time t = 3.99984036      #########
    #########                                  #########
    ####################################################

TimeLoop<dim>::output(t = 4.00038, checkpoint = 1)

+------------------------------------------------------------------------+------------+------------+
| Total CPU time elapsed since start                                     |       357s |            |
|                                                                        |            |            |
| Section                                                    | no. calls |  CPU time  | % of total |
+------------------------------------------------------------+-----------+------------+------------+
| discretization - setup                                     |         1 |     0.113s |         0% |
| offline_data - assemble lumped mass matrix, and c_ij       |         1 |     0.167s |         0% |
| offline_data - compute |c_ij|, and n_ij                    |         1 |   0.00255s |         0% |
| offline_data - create sparsity pattern and set up matrices |         1 |    0.0224s |         0% |
| offline_data - distribute dofs                             |         1 |    0.0617s |         0% |
| offline_data - fix slip boundary c_ij                      |         1 |    0.0329s |         0% |
| schlieren_postprocessor - compute schlieren plot           |       201 |     0.811s |      0.23% |
| schlieren_postprocessor - prepare scratch space            |         1 |   7.6e-05s |         0% |
| time_loop - setup scratch space                            |         1 |     0.127s |         0% |
| time_loop - stalled output                                 |       200 |  0.000685s |         0% |
| time_step - 1 compute d_ij                                 |      7553 |       240s |        67% |
| time_step - 2 compute d_ii, and tau_max                    |      7553 |      11.5s |       3.2% |
| time_step - 3 perform update                               |      7553 |       101s |        28% |
| time_step - 4 fix boundary states                          |      7553 |     0.724s |       0.2% |
| time_step - prepare scratch space                          |         1 |   0.00245s |         0% |
+------------------------------------------------------------+-----------+------------+------------+
@endverbatim

One thing that becomes evident is the fact that the program spends two
thirds of the execution time computing the graph viscosity d_ij and about a
third of the execution time in performing the update, where computing the
flux $f(U)$ is the expensive operation. The preset default resolution is
about 37k gridpoints, which amounts to about 148k spatial degrees of
freedom in 2D. An animated schlieren plot of the solution looks as follows:

<img src="https://www.dealii.org/images/steps/developer/step-69.coarse.gif" alt="" height="300">

It is evident that 37k gridpoints for the first-order method is nowhere
near the resolution needed to resolve any flow features. For comparison,
here is a "reference" computation with a second-order method and about 9.5M
gridpoints (<a href="https://github.com/conservation-laws/ryujin">github
project page</a>):

<img src="https://www.dealii.org/images/steps/developer/step-69.2nd-order.t400.jpg" alt="" height="300">

So, we give the first-order method a second chance and run it with about
2.4M gridpoints on a small compute server:

@verbatim
# mpirun -np 16 ./step-69 | tee output

[...]

    ####################################################
    #########                                  #########
    #########     Cycle  070216  (100.0%)      #########
    #########      at time t = 3.99999231      #########
    #########                                  #########
    ####################################################

TimeLoop<dim>::output(t = 4.00006, checkpoint = 1)

[...]

+------------------------------------------------------------------------+------------+------------+
| Total wallclock time elapsed since start                               |  6.75e+03s |            |
|                                                                        |            |            |
| Section                                                    | no. calls |  wall time | % of total |
+------------------------------------------------------------+-----------+------------+------------+
| discretization - setup                                     |         1 |      1.97s |         0% |
| offline_data - assemble lumped mass matrix, and c_ij       |         1 |      1.19s |         0% |
| offline_data - compute |c_ij|, and n_ij                    |         1 |    0.0172s |         0% |
| offline_data - create sparsity pattern and set up matrices |         1 |     0.413s |         0% |
| offline_data - distribute dofs                             |         1 |      1.05s |         0% |
| offline_data - fix slip boundary c_ij                      |         1 |     0.252s |         0% |
| schlieren_postprocessor - compute schlieren plot           |       201 |      1.82s |         0% |
| schlieren_postprocessor - prepare scratch space            |         1 |  0.000497s |         0% |
| time_loop - setup scratch space                            |         1 |      1.45s |         0% |
| time_loop - stalled output                                 |       200 |   0.00342s |         0% |
| time_step - 1 compute d_ij                                 |     70216 |  4.38e+03s |        65% |
| time_step - 2 compute d_ii, and tau_max                    |     70216 |       419s |       6.2% |
| time_step - 3 perform update                               |     70216 |  1.87e+03s |        28% |
| time_step - 4 fix boundary states                          |     70216 |        24s |      0.36% |
| time_step - prepare scratch space                          |         1 |    0.0227s |         0% |
+------------------------------------------------------------+-----------+------------+------------+
@endverbatim

And with the following result:

<img src="https://www.dealii.org/images/steps/developer/step-69.fine.gif" alt="" height="300">

That's substantially better, although of course at the price of having run
the code for roughly 2 hours on 16 cores.



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

The program showcased here is really only first-order accurate, as
discussed above. The pictures above illustrate how much diffusion that
introduces and how far the solution is from one that actually resolves
the features we care about.

This can be fixed, but it would exceed what a *tutorial* is about.
Nevertheless, it is worth showing what one can achieve by adding a
second-order scheme. For example, here is a video computed with <a
href=https://conservation-laws.43-1.org/>the following research code</a>
that shows (with a different color scheme) a 2d simulation that corresponds
to the cases shown above:

@htmlonly
<p align="center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/xIwJZlsXpZ4"
   frameborder="0"
   allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
   allowfullscreen></iframe>
 </p>
@endhtmlonly

This simulation was done with 38 million degrees of freedom
(continuous $Q_1$ finite elements) per component of the solution
vector. The exquisite detail of the solution is remarkable for these
kinds of simulations, including in the sub-sonic region behind the
obstacle.

One can also with relative ease further extend this to the 3d case:

@htmlonly
<p align="center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/vBCRAF_c8m8"
   frameborder="0"
   allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
   allowfullscreen></iframe>
 </p>
@endhtmlonly

Solving this becomes expensive, however: The simulation was done with
1,817 million degrees of freedom (continuous $Q_1$ finite elements)
per component (for a total of 9.09 billion spatial degrees of freedom)
and ran on 30,720 MPI ranks. The code achieved an average througput of
969M grid points per second (0.04M gridpoints per second per CPU). The
front and back wall show a "Schlieren plot": the magnitude of the
gradient of the density on an exponential scale from white (low) to
black (high). All other cutplanes and the surface of the obstacle show
the magnitude of the vorticity on a white (low) - yellow (medium) -
red (high) scale. (The scales of the individual cutplanes have been
adjusted for a nicer visualization.)


examples/step-7/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

In this program, we will mainly consider two aspects:
<ol>
  <li> Verification of correctness of the program and generation of convergence
  tables;
  <li> Non-homogeneous Neumann boundary conditions for the Helmholtz equation.
</ol>
Besides these topics, again a variety of improvements and tricks will be
shown.


<h3>Verification of correctness</h3>

There has probably never been a
non-trivial finite element program that worked right from the start. It is
therefore necessary to find ways to verify whether a computed solution is
correct or not. Usually, this is done by choosing the set-up of a simulation
in such a way that we know the exact continuous solution and evaluate the difference
between continuous and computed discrete solution. If this difference
converges to zero with the right order of convergence, this is already a good
indication of correctness, although there may be other sources of error
persisting which have only a small contribution to the total error or are of
higher order. In the context of finite element simulations, this technique
of picking the solution by choosing appropriate right hand sides and
boundary conditions
is often called the <i>Method of Manufactured Solution</i>.

In this example, we will not go into the theories of systematic software
verification which is a very complicated problem. Rather we will demonstrate
the tools which deal.II can offer in this respect. This is basically centered
around the functionality of a single function, VectorTools::integrate_difference().
This function computes the difference between a given continuous function and
a finite element field in various norms on each cell.
Of course, like with any other integral, we can only evaluate these norms using quadrature formulas;
the choice of the right quadrature formula is therefore crucial to the
accurate evaluation of the error. This holds in particular for the $L_\infty$
norm, where we evaluate the maximal deviation of numerical and exact solution
only at the quadrature points; one should then not try to use a quadrature
rule whose evaluation occurs only at points where
[super-convergence](https://en.wikipedia.org/wiki/Superconvergence) might occur, such as
the Gauss points of the lowest-order Gauss quadrature formula for which the
integrals in the assembly of the matrix is correct (e.g., for linear elements,
do not use the QGauss(2) quadrature formula). In fact, this is generally good
advice also for the other norms: if your quadrature points are fortuitously
chosen at locations where the error happens to be particularly small due to
superconvergence, the computed error will look like it is much smaller than
it really is and may even suggest a higher convergence order. Consequently,
we will choose a different quadrature formula for the integration of these
error norms than for the assembly of the linear system.

The function VectorTools::integrate_difference() evaluates the desired norm on each
cell $K$ of the triangulation and returns a vector which holds these
values for each cell. From the local values, we can then obtain the global error. For
example, if the vector $\mathbf e$ with element $e_K$ for all cells
$K$ contains the local $L_2$ norms $\|u-u_h\|_K$, then
@f[
  E = \| {\mathbf e} \| = \left( \sum_K e_K^2 \right)^{1/2}
@f]
is the global $L_2$ error $E=\|u-u_h\|_\Omega$.

In the program, we will show how to evaluate and use these quantities, and we
will monitor their values under mesh refinement. Of course, we have to choose
the problem at hand such that we can explicitly state the solution and its
derivatives, but since we want to evaluate the correctness of the program,
this is only reasonable. If we know that the program produces the correct
solution for one (or, if one wants to be really sure: many) specifically
chosen right hand sides, we can be rather confident that it will also compute
the correct solution for problems where we don't know the exact values.

In addition to simply computing these quantities, we will show how to generate
nicely formatted tables from the data generated by this program that
automatically computes convergence rates etc. In addition, we will compare
different strategies for mesh refinement.


<h3>Non-homogeneous Neumann boundary conditions</h3>

The second, totally
unrelated, subject of this example program is the use of non-homogeneous
boundary conditions. These are included into the variational form using
boundary integrals which we have to evaluate numerically when assembling the
right hand side vector.

Before we go into programming, let's have a brief look at the mathematical
formulation. The equation that we want to solve here is the Helmholtz equation
"with the nice sign":
@f[
  -\Delta u + \alpha u = f,
@f]
on the square $[-1,1]^2$ with $\alpha=1$, augmented by Dirichlet boundary conditions
@f[
  u = g_1
@f]
on some part $\Gamma_1$ of the boundary $\Gamma$, and Neumann conditions
@f[
  {\mathbf n}\cdot \nabla u = g_2
@f]
on the rest $\Gamma_2 = \Gamma \backslash \Gamma_1$.
In our particular testcase, we will use $\Gamma_1=\Gamma \cap\{\{x=1\}
\cup \{y=1\}\}$.
(We say that this equation has the "nice sign" because the operator
$-\Delta + \alpha I$ with the identity $I$ and $\alpha>0$ is a positive definite
operator; the <a
href="https://en.wikipedia.org/wiki/Helmholtz_equation">equation with
the "bad sign"</a> is $-\Delta u - \alpha u$ and results from modeling
time-harmonic processes. The operator is not positive
definite if $\alpha>0$ is large, and this leads to all sorts of issues
we need not discuss here. The operator may also not be invertible --
i.e., the equation does not have a unique solution -- if $\alpha$
happens to be one of the eigenvalues of $-\Delta$.)

Because we want to verify the convergence of our numerical solution $u_h$,
we want a setup so that we know the exact solution $u$. This is where
the Method of Manufactured Solutions comes in. To this end, let us
choose a function
@f[
  \bar u(x) = \sum_{i=1}^3 \exp\left(-\frac{|x-x_i|^2}{\sigma^2}\right)
@f]
where the centers $x_i$ of the exponentials are
  $x_1=(-\frac 12,\frac 12)$,
  $x_2=(-\frac 12,-\frac 12)$, and
  $x_3=(\frac 12,-\frac 12)$,
and the half width is set to $\sigma=\frac {1}{8}$. The method of manufactured
solution then says: choose
@f{align*}
  f &= -\Delta \bar u + \bar u, \\
  g_1 &= \bar u|_{\Gamma_1}, \\
  g_2 &= {\mathbf n}\cdot \nabla\bar u|_{\Gamma_2}.
@f}
With this particular choice, we infer that of course the solution of the
original problem happens to be $u=\bar u$. In other words, by choosing
the right hand sides of the equation and the boundary conditions in a
particular way, we have manufactured ourselves a problem to which we
know the solution. This allows us then to compute the error of our
numerical solution. In the code below, we represent $\bar u$ by the
<code>Solution</code> class, and other classes will be used to
denote $\bar u|_{\Gamma_1}=g_1$ and ${\mathbf n}\cdot \nabla\bar u|_{\Gamma_2}=g_2$.

Using the above definitions, we can state the weak formulation of the
equation, which reads: find $u\in H^1_g=\{v\in H^1: v|_{\Gamma_1}=g_1\}$ such
that
@f[
  {(\nabla v, \nabla u)}_\Omega + {(v,u)}_\Omega
  =
  {(v,f)}_\Omega + {(v,g_2)}_{\Gamma_2}
@f]
for all test functions $v\in H^1_0=\{v\in H^1: v|_{\Gamma_1}=0\}$. The
boundary term ${(v,g_2)}_{\Gamma_2}$ has appeared by integration by parts and
using $\partial_n u=g_2$ on $\Gamma_2$ and $v=0$ on $\Gamma_1$. The cell
matrices and vectors which we use to build the global matrices and right hand
side vectors in the discrete formulation therefore look like this:
@f{eqnarray*}
  A_{ij}^K &=& \left(\nabla \varphi_i, \nabla \varphi_j\right)_K
              +\left(\varphi_i, \varphi_j\right)_K,
  \\
  F_i^K &=& \left(\varphi_i, f\right)_K
           +\left(\varphi_i, g_2\right)_{\partial K\cap \Gamma_2}.
@f}
Since the generation of the domain integrals has been shown in previous
examples several times, only the generation of the contour integral is of
interest here. It basically works along the following lines: for domain
integrals we have the <code>FEValues</code> class that provides values and
gradients of the shape values, as well as Jacobian determinants and other
information and specified quadrature points in the cell; likewise, there is a
class <code>FEFaceValues</code> that performs these tasks for integrations on
faces of cells. One provides it with a quadrature formula for a manifold with
dimension one less than the dimension of the domain is, and the cell and the
number of its face on which we want to perform the integration. The class will
then compute the values, gradients, normal vectors, weights, etc. at the
quadrature points on this face, which we can then use in the same way as for
the domain integrals. The details of how this is done are shown in the
following program.


<h3>A note on good programming practice</h3>

Besides the mathematical topics outlined above, we also want to use this
program to illustrate one aspect of good programming practice, namely the use
of namespaces. In programming the deal.II library, we have take great care not
to use names for classes and global functions that are overly generic, say
<code>f(), sz(), rhs()</code> etc. Furthermore, we have put everything into
namespace <code>dealii</code>. But when one writes application programs that
aren't meant for others to use, one doesn't always pay this much attention. If
you follow the programming style of step-1 through step-6, these functions
then end up in the global namespace where, unfortunately, a lot of other stuff
also lives (basically everything the C language provides, along with
everything you get from the operating system through header files). To make
things a bit worse, the designers of the C language were also not always
careful in avoiding generic names; for example, the symbols <code>j1,
jn</code> are defined in C header files (they denote Bessel functions).

To avoid the problems that result if names of different functions or variables
collide (often with confusing error messages), it is good practice to put
everything you do into a <a
href="http://en.wikipedia.org/wiki/Namespace_(computer_science)">namespace</a>. Following
this style, we will open a namespace <code>Step7</code> at the top of the
program, import the deal.II namespace into it, put everything that's specific
to this program (with the exception of <code>main()</code>, which must be in
the global namespace) into it, and only close it at the bottom of the file. In
other words, the structure of the program is of the kind
@code
  #includes ...

  namespace Step7
  {
    using namespace dealii;

    ...everything to do with the program...
  }

  int main ()
  {
    ...do whatever main() does...
  }
@endcode
We will follow this scheme throughout the remainder of the deal.II tutorial.


examples/step-7/doc/results.dox
<h1>Results</h1>


The program generates two kinds of output. The first are the output
files <code>solution-adaptive-q1.vtk</code>,
<code>solution-global-q1.vtk</code>, and
<code>solution-global-q2.vtk</code>. We show the latter in a 3d view
here:


<img src="https://www.dealii.org/images/steps/developer/step-7.solution.png" alt="">




Secondly, the program writes tables not only to disk, but also to the
screen while running. The output looks like the following (recall that
columns labeled as "<code>H1</code>" actually show the $H^1$ <i>semi-</i>norm
of the error, not the full $H^1$ norm):


@code
examples/\step-7> make run
Solving with Q1 elements, adaptive refinement
=============================================

Cycle 0:
   Number of active cells:       64
   Number of degrees of freedom: 81
Cycle 1:
   Number of active cells:       124
   Number of degrees of freedom: 157
Cycle 2:
   Number of active cells:       280
   Number of degrees of freedom: 341
Cycle 3:
   Number of active cells:       577
   Number of degrees of freedom: 690
Cycle 4:
   Number of active cells:       1099
   Number of degrees of freedom: 1264
Cycle 5:
   Number of active cells:       2191
   Number of degrees of freedom: 2452
Cycle 6:
   Number of active cells:       4165
   Number of degrees of freedom: 4510
Cycle 7:
   Number of active cells:       7915
   Number of degrees of freedom: 8440
Cycle 8:
   Number of active cells:       15196
   Number of degrees of freedom: 15912

cycle cells dofs     L2        H1      Linfty
    0    64    81 1.840e+00 2.858e+00 1.835e+00
    1   124   157 5.190e-02 1.200e+00 1.344e-01
    2   280   341 1.439e-02 7.892e-01 7.554e-02
    3   577   690 8.627e-03 5.061e-01 2.805e-02
    4  1099  1264 3.217e-03 3.030e-01 1.073e-02
    5  2191  2452 1.445e-03 2.097e-01 5.073e-03
    6  4165  4510 8.387e-04 1.460e-01 2.013e-03
    7  7915  8440 7.051e-04 1.053e-01 1.804e-03
    8 15196 15912 2.774e-04 7.463e-02 6.911e-04

Solving with Q1 elements, global refinement
===========================================

Cycle 0:
   Number of active cells:       64
   Number of degrees of freedom: 81
Cycle 1:
   Number of active cells:       256
   Number of degrees of freedom: 289
Cycle 2:
   Number of active cells:       1024
   Number of degrees of freedom: 1089
Cycle 3:
   Number of active cells:       4096
   Number of degrees of freedom: 4225
Cycle 4:
   Number of active cells:       16384
   Number of degrees of freedom: 16641

cycle cells dofs     L2        H1      Linfty
    0    64    81 1.840e+00 2.858e+00 1.835e+00
    1   256   289 3.570e-02 1.199e+00 1.307e-01
    2  1024  1089 1.192e-02 7.565e-01 7.168e-02
    3  4096  4225 3.047e-03 3.823e-01 2.128e-02
    4 16384 16641 7.660e-04 1.917e-01 5.554e-03

n cells         H1                   L2
0    64 2.858e+00    -    - 1.840e+00     -    -
1   256 1.199e+00 2.38 1.25 3.570e-02 51.54 5.69
2  1024 7.565e-01 1.58 0.66 1.192e-02  2.99 1.58
3  4096 3.823e-01 1.98 0.98 3.047e-03  3.91 1.97
4 16384 1.917e-01 1.99 1.00 7.660e-04  3.98 1.99

Solving with Q2 elements, global refinement
===========================================

Cycle 0:
   Number of active cells:       64
   Number of degrees of freedom: 289
Cycle 1:
   Number of active cells:       256
   Number of degrees of freedom: 1089
Cycle 2:
   Number of active cells:       1024
   Number of degrees of freedom: 4225
Cycle 3:
   Number of active cells:       4096
   Number of degrees of freedom: 16641
Cycle 4:
   Number of active cells:       16384
   Number of degrees of freedom: 66049

cycle cells dofs     L2        H1      Linfty
    0    64   289 1.606e-01 1.278e+00 3.029e-01
    1   256  1089 7.638e-03 5.248e-01 4.816e-02
    2  1024  4225 8.601e-04 1.086e-01 4.827e-03
    3  4096 16641 1.107e-04 2.756e-02 7.802e-04
    4 16384 66049 1.393e-05 6.915e-03 9.971e-05

n cells         H1                   L2
0    64 1.278e+00    -    - 1.606e-01     -    -
1   256 5.248e-01 2.43 1.28 7.638e-03 21.03 4.39
2  1024 1.086e-01 4.83 2.27 8.601e-04  8.88 3.15
3  4096 2.756e-02 3.94 1.98 1.107e-04  7.77 2.96
4 16384 6.915e-03 3.99 1.99 1.393e-05  7.94 2.99

Solving with Q2 elements, adaptive refinement
===========================================

Cycle 0:
   Number of active cells:       64
   Number of degrees of freedom: 289
Cycle 1:
   Number of active cells:       124
   Number of degrees of freedom: 577
Cycle 2:
   Number of active cells:       289
   Number of degrees of freedom: 1353
Cycle 3:
   Number of active cells:       547
   Number of degrees of freedom: 2531
Cycle 4:
   Number of active cells:       1057
   Number of degrees of freedom: 4919
Cycle 5:
   Number of active cells:       2059
   Number of degrees of freedom: 9223
Cycle 6:
   Number of active cells:       3913
   Number of degrees of freedom: 17887
Cycle 7:
   Number of active cells:       7441
   Number of degrees of freedom: 33807
Cycle 8:
   Number of active cells:       14212
   Number of degrees of freedom: 64731

cycle cells dofs     L2        H1      Linfty
    0    64   289 1.606e-01 1.278e+00 3.029e-01
    1   124   577 7.891e-03 5.256e-01 4.852e-02
    2   289  1353 1.070e-03 1.155e-01 4.868e-03
    3   547  2531 5.962e-04 5.101e-02 1.876e-03
    4  1057  4919 1.977e-04 3.094e-02 7.923e-04
    5  2059  9223 7.738e-05 1.974e-02 7.270e-04
    6  3913 17887 2.925e-05 8.772e-03 1.463e-04
    7  7441 33807 1.024e-05 4.121e-03 8.567e-05
    8 14212 64731 3.761e-06 2.108e-03 2.167e-05
@endcode


One can see the error reduction upon grid refinement, and for the
cases where global refinement was performed, also the convergence
rates can be seen. The linear and quadratic convergence rates of Q1
and Q2 elements in the $H^1$ semi-norm can clearly be seen, as
are the quadratic and cubic rates in the $L_2$ norm.




Finally, the program also generated LaTeX versions of the tables (not shown
here) that is written into a file in a way so that it could be
copy-pasted into a LaTeX document.


<h4> When is the error "small"? </h4>

What we showed above is how to determine the size of the error
$\|u-u_h\|$ in a number of different norms. We did this primarily
because we were interested in testing that our solutions *converge*.
But from an engineering perspective, the question is often more
practical: How fine do I have to make my mesh so that the error is
"small enough"? In other words, if in the table above the $H^1$
semi-norm has been reduced to `4.121e-03`, is this good enough for me
to sign the blueprint and declare that our numerical simulation showed
that the bridge is strong enough?

In practice, we are rarely in this situation because I can not
typically compare the numerical solution $u_h$ against the exact
solution $u$ in situations that matter -- if I knew $u$, I would not
have to compute $u_h$. But even if I could, the question to ask in
general is then: `4.121e-03` *what*? The solution will have physical
units, say kg-times-meter-squared, and I'm integrating a function with
units square of the above over the domain, and then take the square
root. So if the domain is two-dimensional, the units of
$\|u-u_h\|_{L_2}$ are kg-times-meter-cubed. The question is then: Is
$4.121\times 10^{-3}$ kg-times-meter-cubed small? That depends on what
you're trying to simulate: If you're an astronomer used to masses
measured in solar masses and distances in light years, then yes, this
is a fantastically small number. But if you're doing atomic physics,
then no: That's not small, and your error is most certainly not
sufficiently small; you need a finer mesh.

In other words, when we look at these sorts of numbers, we generally
need to compare against a "scale". One way to do that is to not look
at the *absolute* error $\|u-u_h\|$ in whatever norm, but at the
*relative* error $\|u-u_h\|/\|u\|$. If this ratio is $10^{-5}$, then
you know that *on average*, the difference between $u$ and $u_h$ is
0.001 per cent -- probably small enough for engineering purposes.

How do we compute $\|u\|$? We just need to do an integration loop over
all cells, quadrature points on these cells, and then sum things up
and take the square root at the end. But there is a simpler way often
used: You can call
@code
    Vector<double> zero_vector (dof_handler.n_dofs());
    Vector<float> norm_per_cell(triangulation.n_active_cells());
    VectorTools::integrate_difference(dof_handler,
                                      zero_vector,
                                      Solution<dim>(),
                                      norm_per_cell,
                                      QGauss<dim>(fe->degree + 1),
                                      VectorTools::L2_norm);
@endcode
which computes $\|u-0\|_{L_2}$. Alternatively, if you're particularly
lazy and don't feel like creating the `zero_vector`, you could use
that if the mesh is not too coarse, then $\|u\| \approx \|u_h\|$, and
we can compute $\|u\| \approx \|u_h\|=\|0-u_h\|$ by calling
@code
    Vector<float> norm_per_cell(triangulation.n_active_cells());
    VectorTools::integrate_difference(dof_handler,
                                      solution,
                                      ZeroFunction<dim>(),
                                      norm_per_cell,
                                      QGauss<dim>(fe->degree + 1),
                                      VectorTools::L2_norm);
@endcode
In both cases, one then only has to combine the vector of cellwise
norms into one global norm as we already do in the program, by calling
@code
    const double L2_norm =
      VectorTools::compute_global_error(triangulation,
                                        norm_per_cell,
                                        VectorTools::L2_norm);
@endcode



<h3> Possibilities for extensions </h3>

<h4> Higher Order Elements </h4>

Go ahead and run the program with higher order elements ($Q_3$, $Q_4$, ...). You
will notice that assertions in several parts of the code will trigger (for
example in the generation of the filename for the data output). You might have to address these,
but it should not be very hard to get the program to work!

<h4> Convergence Comparison </h4>

Is Q1 or Q2 better? What about adaptive versus global refinement? A (somewhat
unfair but typical) metric to compare them, is to look at the error as a
function of the number of unknowns.

To see this, create a plot in log-log style with the number of unknowns on the
$x$ axis and the $L_2$ error on the $y$ axis. You can add reference lines for
$h^2=N^{-1}$ and $h^3=N^{-3/2}$ and check that global and adaptive refinement
follow those. If one makes the (not completely unreasonable)
assumption that with a good linear solver, the computational effort is
proportional to the number of unknowns $N$, then it is clear that an
error reduction of ${\cal O}(N^{-3/2})$ is substantially better than a
reduction of the form ${\cal O}(N^{-1})$: That is, that adaptive
refinement gives us the desired error level with less computational
work than if we used global refinement. This is not a particularly
surprising conclusion, but it's worth checking these sorts of
assumptions in practice.

Of course, a fairer comparison would be to plot runtime (switch to release
mode first!) instead of number of unknowns on the $x$ axis. If you
plotted run time against the number of unknowns by timing each
refinement step (e.g., using the Timer class), you will notice that
the linear solver is not perfect -- its run time grows faster than
proportional to the linear system size -- and picking a better
linear solver might be appropriate for this kind of comparison.


examples/step-70/doc/intro.dox
<br>

<i>This program was contributed by Luca Heltai (International School for
Advanced Studies, Trieste), Bruno Blais (Polytechnique Montréal),
and Rene Gassmöller (University of California Davis)
</i>

@dealiiTutorialDOI{10.5281/zenodo.3829064,https://zenodo.org/badge/DOI/10.5281/zenodo.3829064.svg}


<h1>Introduction</h1>

<h3>Massively parallel non-matching grid simulations of fluid structure interaction problems</h3>

In this tutorial we consider a mixing problem in the laminar flow regime.
Such problems occur in a wide range of applications ranging from chemical engineering to power
generation (e.g. turbomachinery). Mixing problems are particularly hard to solve numerically,
because they often involve a container (with fixed boundaries, and possibly
complex geometries such as baffles), represented by the domain $\Omega$,
and one (or more) immersed and rotating impellers (represented by the domain $\Omega^{\text{imp}}$).
The domain in which we would like to solve the flow equations is the (time
dependent) difference between the two domains, namely:
$\Omega\setminus\Omega^{\text{imp}}$.

For rotating impellers, the use of Arbitrary Lagrangian Eulerian formulations
(in which the fluid domain -- along with the mesh! -- is smoothly deformed to follow the deformations
of the immersed solid) is not possible, unless only small times (i.e.,
small fluid domain deformations) are considered. If one wants to track the
evolution of the flow across multiple rotations of the impellers, the resulting
deformed grid would simply be too distorted to be useful.

In this case, a viable alternative strategy would be to use non-matching
methods (similarly to what we have done in step-60), where a background fixed
grid (that may or may not be locally refined in time to better capture the solid
motion) is coupled with a rotating, independent, grid.

In order to maintain the same notations used in step-60, we use $\Omega$ to
denote the domain in ${\mathbb R}^{\text{spacedim}}$ representing the container of both
the fluid and the impeller, and we use $\Gamma$ in ${\mathbb R}^{\text{dim}}$ to denote
either the full impeller (when its `spacedim` measure is non-negligible, i.e.,
when we can represent it as a grid of dimension `dim` equal to `spacedim`),
a co-dimension one representation of a thin impeller, or just the boundary of
the full impeller.

The domain $\Gamma$ is embedded in $\Omega$ ($\Gamma \subseteq \Omega$) and it
is non-matching: It does not, in general, align with any of the
features of the volume mesh. We solve a partial differential equation on $\Omega$,
enforcing some conditions on the solution of the problem on the embedded
domain $\Gamma$ by some penalization techniques. In the current case,
the condition is that the velocity of the fluid at points on $\Gamma$
equal the velocity of the solid impeller at that point.

The technique we describe here is presented in the literature using one of many
names: the <b>immersed finite element method</b> and the <b>fictitious boundary
method</b> among others.  The main principle is that the discretization of the
two grids are kept completely independent. In the present tutorial, this
approach is used to solve for the motion of a viscous fluid, described by the
Stokes equation, that is agitated by a rigid non-deformable impeller.

Thus, the equations solved in $\Omega$ are the Stokes equations for a creeping
flow (i.e. a flow where $\text{Re}\rightarrow 0$) and a no-slip boundary
condition is applied on the moving *embedded domain* $\Gamma$ associated with
the impeller. However, this tutorial could be readily extended
to other equations (e.g. the Navier-Stokes equations, linear elasticity
equation, etc.). It can be seen as a natural extension of step-60 that
enables the solution of large problems using a distributed parallel computing
architecture via MPI.

However, contrary to step-60, the Dirichlet boundary conditions on $\Gamma$
are imposed weakly instead of through the use of Lagrange multipliers, and we
concentrate on dealing with the coupling of two fully distributed
triangulations (a combination that was not possible in the implementation of
step-60).

There are two interesting scenarios that occur when one wants to enforce
conditions on the embedded domain $\Gamma$:

- The geometrical dimension `dim` of the embedded domain $\Gamma$ is the same of
the domain $\Omega$ (`spacedim`), that is, the spacedim-dimensional measure of
$\Gamma$ is not zero. In this case, the imposition of the Dirichlet boundary
boundary condition on $\Gamma$ is done through a volumetric penalization. If the
applied penalization only depends on the velocity, this is often referred
to as $\mathcal{L}^2$ penalization whereas if the penalization depends
on both the velocity and its gradient, it is an $\mathcal{H}^1$ penalization.
The case of the $\mathcal{L}^2$ penalization is very similar to a Darcy-type
approach. Both $\mathcal{L}^2$ and $\mathcal{H}^1$ penalizations have been
analyzed extensively (see, for example, @cite Angot1999).

- The embedded domain $\Gamma$ has an intrinsic dimension `dim` which is smaller
than that of $\Omega$ (`spacedim`), thus its spacedim-dimensional measure is
zero; for example it is a curve embedded in a two dimensional domain, or a
surface embedded in a three-dimensional domain. This is of course
physically impossible, but one may consider very thin sheets of metal
moving in a fluid as essentially lower-dimensional if the thickness of
the sheet is negligible. In this case, the boundary
condition is imposed weakly on $\Gamma$ by applying the
<a href="https://en.wikipedia.org/wiki/Joachim_Nitsche">Nitsche</a> method (see
@cite Freund1995).

Both approaches have very similar requirements and result in highly
similar formulations. Thus, we treat them almost in the same way.

In this tutorial program we are not interested in further details on $\Gamma$:
we assume that the dimension of the embedded domain (`dim`) is always smaller by
one or equal with respect to the dimension of the embedding domain $\Omega$
(`spacedim`).

We are going to solve the following differential problem: given a sufficiently
regular function $g$ on $\Gamma$, find the solution $(\textbf{u},p)$ to

@f{eqnarray*}
  -\Delta \mathbf{u} + \nabla p &=& 0,\\
  -\nabla \cdot \textbf{u} &=& 0,\\
  \textbf{u} &=& \textbf{g}  \text{ in } \Gamma,\\
  \textbf{u} &=& 0 \text{ on } \partial\Omega.
@f}

This equation, which we have normalized by scaling the time units in
such a way that the viscosity has a numerical value of 1, describes
slow, viscous flow such as honey or lava.
The main goal of this tutorial is to show how to impose the velocity field
condition $\mathbf{u} = \mathbf{g}$ on a non-matching $\Gamma$ in a weak way,
using a penalization method. A more extensive discussion of the Stokes
problem including body forces, different boundary conditions, and solution
strategies can be found in step-22.

Let us start by considering the Stokes problem alone, in the entire domain
$\Omega$. We look for a velocity field $\mathbf{u}$ and a pressure field $p$
that satisfy the Stokes equations with homogeneous boundary conditions
on $\partial\Omega$.

The weak form of the Stokes equations is obtained by first writing it in vector
form as
@f{eqnarray*}
  \begin{pmatrix}
    {-\Delta \textbf{u} + \nabla p}
    \\
    {-\textrm{div}\;\textbf{u}}
  \end{pmatrix}
  =
  \begin{pmatrix}
  0
  \\
  0
  \end{pmatrix},
@f}
forming the dot product from the left with a vector-valued test
function $\phi = \begin{pmatrix}\textbf{v} \\ q\end{pmatrix}$, and integrating
over the domain $\Omega$, yielding the following set of equations:
@f{eqnarray*}
  (\mathrm v,
   -\Delta \textbf{u} + \nabla p)_{\Omega}
  -
  (q,\textrm{div}\; \textbf{u})_{\Omega}
  =
  0
@f}
which has to hold for all test functions $\phi = \begin{pmatrix}\textbf{v}
\\ q\end{pmatrix}$.


Integrating by parts and exploiting the boundary conditions on $\partial\Omega$,
we obtain the following variational problem:
@f{eqnarray*}{
(\nabla \textbf{v}, \nabla \textbf{u})_{\Omega} - (\textrm{div}\; \textbf{v}, p)_{\Omega}
 - (q, \textrm{div}\; \textbf{u})_{\Omega}&=& 0
@f}

where $(\cdot, \cdot)_{\Omega}$ represents the $L^2$ scalar
product. This is the same variational form used in step-22.

This variational formulation does not take into account the embedded domain.
Contrary to step-60, we do not enforce strongly the constraints of
$\textbf{u}$ on $\Gamma$, but enforce them weakly via a penalization term.

The analysis of this weak imposition of the boundary condition depends on the
spacedim-dimensional measure of $\Gamma$ as either positive (if `dim` is equal
to `spacedim`) or zero (if `dim` is smaller than `spacedim`). We discuss both
scenarios.


<h4>Co-dimension one case</h4>

In this case, we assume that $\Gamma$ is the boundary of the actual impeller,
that is, a closed curve embedded in a two-dimensional domain or a closed
surface in a three-dimensional domain. The idea of this method starts by
considering a weak imposition of the Dirichlet boundary condition on $\Gamma$,
following the Nitsche method. This is achieved by using the following modified formulation
on the fluid domain, where no strong conditions on the test functions on $\Gamma$ are imposed:

@f{multline*}{
(\nabla \textbf{v}, \nabla \textbf{u})_{\Omega\setminus\Omega^{\text{imp}}} - (\textrm{div}\;  \textbf{v}, p)_{\Omega\setminus\Omega^{\text{imp}}}
  - (q, \textrm{div}\; \textbf{u})_{\Omega\setminus\Omega^{\text{imp}}} \\
  - (\textbf{v},\nabla \textbf{u} \cdot \textbf{n})_{\Gamma}
  + (\textbf{v}\cdot \textbf{n},p)_{\Gamma} \\
 -  (\nabla\textbf{v}\cdot \textbf{n},\textbf{u})_{\Gamma}
 + (q, \textbf{u} \cdot \textbf{n})_{\Gamma}
 + \beta (\textbf{v},\textbf{u})_{\Gamma} \\
=  - (\nabla\textbf{v}\cdot \textbf{n},\textbf{g})_{\Gamma} + (q, \textbf{g} \cdot \textbf{n})_{\Gamma}
 + \beta (\textbf{v},\textbf{g})_{\Gamma}.
@f}

The integrals over $\Gamma$ are lower-dimensional integrals. It can be shown (see
@cite Freund1995) that there exists a positive constant
$C_1$ so that if $\beta > C_1$, the weak imposition of the boundary will
be consistent and stable. The first two additional integrals on $\Gamma$ (the
second line in the equation above) appear naturally after integrating by parts,
when one does not assume that $\mathbf{v}$ is zero on
$\Gamma$.

The third line in the equation above contains two terms that are added to ensure
consistency of the weak form, and a stabilization term, that is there to enforce
the boundary condition with an error which is consistent with the approximation
error. The consistency terms and the stabilization term are added to the
right hand side with the actual boundary data $\mathbf{g}$.

When $\mathbf{u}$ satisfies the condition $\mathbf{u}=\mathbf{g}$ on $\Gamma$,
all the consistency and stability integrals on $\Gamma$ cancel out, and one is
left with the usual weak form of Stokes flow, that is, the above formulation is
consistent.

We note that an alternative (non-symmetric) formulation can be used :

@f{multline*}{
(\nabla \textbf{v}, \nabla \textbf{u})_{\Omega\setminus\Omega^{\text{imp}}} -  (\textrm{div}\;  \textbf{v}, p)_{\Omega\setminus\Omega^{\text{imp}}}
  - (q, \textrm{div}\; \textbf{u})_{\Omega\setminus\Omega^{\text{imp}}} \\
  -(\textbf{v},\nabla \textbf{u} \cdot \textbf{n})_{\Gamma}
  + (\textbf{v}\cdot \textbf{n},p)_{\Gamma} \\
   +(\nabla\textbf{v}\cdot \textbf{n},\textbf{u})_{\Gamma}
 - (q, \textbf{u} \cdot \textbf{n})_{\Gamma}
 + \beta (\textbf{v},\textbf{u})_{\Gamma} \\
=   (\nabla\textbf{v}\cdot \textbf{n},\textbf{g})_{\Gamma} - (q, \textbf{g} \cdot \textbf{n})_{\Gamma}
 + \beta (\textbf{v},\textbf{g})_{\Gamma}.
@f}
Note the different sign of the first terms on the third and fourth lines.
In this case, the stability and consistency conditions become $\beta > 0$. In
the symmetric case, the value of $\beta$ is dependent on $h$, and it is in
general chosen such that $\beta = C h^{-1} $ with $h$
a measure of size of the face being integrated and $C$ a constant such that
$1 \leq C \leq 10$. This is as one usually does with the Nitsche
penalty method to enforcing Dirichlet boundary conditions.

The non-symmetric approach, on the other hand, is related to how one
enforced continuity for the non-symmetric interior penalty method for
discontinuous Galerkin methods (the "NIPG" method @cite Riviere1999).
Even if the non-symmetric case seems advantageous w.r.t.
possible choices of stabilization parameters, we opt for the symmetric
discretization, since in this case it can be shown that the dual problem is
also consistent, leading to a solution where not only the energy norm of the
solution converges with the correct order, but also its $L^2$
norm. Furthermore, the resulting matrix remains symmetric.

The above formulation works under the assumption that the domain is discretized
exactly. However, if the deformation of the impeller is a rigid body
motion, it is possible to artificially extend the solution of the Stokes
problem inside the propeller itself, since a rigid body motion is also a
solution to the Stokes problem. The idea is then to solve the same problem,
inside $\Omega^{\text{imp}}$, imposing the same boundary conditions on
$\Gamma$, using the same penalization technique, and testing with test
functions $\mathbf{v}$ which are globally continuous over $\Omega$.

This results in the following (intermediate) formulation:
@f{multline*}{
(\nabla \textbf{v}, \nabla \textbf{u})_{\Omega} - (\textrm{div}\;  \textbf{v}, p)_{\Omega}
  - (q, \textrm{div}\; \textbf{u})_{\Omega} \\
  - (\textbf{v},  \lbrack \nabla \textbf{u} \rbrack \cdot \textbf{n})_{\Gamma}
  + (\textbf{v}\cdot \textbf{n},\lbrack p \rbrack )_{\Gamma} \\
 -  (\lbrack \nabla\textbf{v} \rbrack \cdot \textbf{n},\textbf{u})_{\Gamma}
 + (\lbrack q \rbrack, \textbf{u} \cdot n)_{\Gamma}
 + 2\beta (\textbf{v},\textbf{u})_{\Gamma} \\
=  - (\lbrack \nabla\textbf{v}\rbrack\cdot \textbf{n},\textbf{g})_{\Gamma} + (\lbrack q\rbrack, \textbf{g} \cdot n)_{\Gamma}
 + 2\beta (\textbf{v},\textbf{g})_{\Gamma},
@f}
where the jump terms, denoted with $\lbrack \cdot \rbrack$, are computed with
respect to a fixed orientation of the normal vector $\textbf{n}$. The
factor of 2 appears in front of $\beta$ since we see every part of
$\Gamma$ twice, once from within the fluid and once from within the
obstacle moving around in it. (For all of the other integrals over
$\Gamma$, we visit each part of $\Gamma$ twice, but with opposite
signs, and consequently get the jump terms.)

Here we notice that, unlike in discontinuous Galerkin methods, the test
and trial functions are continuous across $\Gamma$. Moreover, if $\Gamma$ is
not aligned with cell boundaries, all the jump terms are also zero, since, in
general, finite element function spaces are smooth inside each cell, and if
$\Gamma$ cuts through an element intersecting its boundary only at a finite
number of points, all the contributions on $\Gamma$, with the exception of
the stabilization ones, can be neglected from the formulation, resulting in
the following final form of the variational formulation:

@f{multline*}{
(\nabla \textbf{v}, \nabla \textbf{u})_{\Omega} - (\textrm{div}\;  \textbf{v}, p)_{\Omega}
  - (q, \textrm{div}\; \textbf{u})_{\Omega}  + 2\beta (\textbf{v},\textbf{u})_{\Gamma} \\
=  2\beta (\textbf{v},\textbf{g})_{\Gamma}.
@f}

In step-60, the imposition of the constraint
required the addition of new variables in the form of Lagrange multipliers.
This is not the case for this tutorial program. The imposition of the
boundary condition using Nitsche's method only modifies the system matrix
and the right-hand side without adding additional unknowns.
However, the velocity vector $\textbf{u}$ on the embedded domain will not match
exactly the prescribed velocity $\textbf{g}$, but only up to a numerical error
which is in the same order as the interpolation error of the finite element
method. Furthermore, as in step-60, we still need to integrate over the
non-matching embedded grid in order to construct the boundary term necessary
to impose the boundary condition over $\Gamma$.


<h4>Co-dimension zero case</h4>

In this case, $\Gamma$ has the same dimension, but is embedded into
$\Omega$. We can think of this as a thick object moving around in the fluid.
In the case of $\mathcal{L}^2$ penalization, the additional penalization
term can be interpreted as a Darcy term within $\Gamma$, resulting in:

@f{eqnarray*}
(\nabla \textbf{v}, \nabla \textbf{u})_{\Omega} - & (\textrm{div}\;  \textbf{v}, p)_{\Omega}
  - (q, \textrm{div}\; \textbf{u})_{\Omega}  + \beta (\textbf{v},\textbf{u})_{\Gamma}
=  \beta (\textbf{v},\textbf{g})_{\Gamma}.
@f}

Here, integrals over $\Gamma$ are simply integrals over a part of the volume.
The $\mathcal{L}^2$ penalization thus consists in adding a volumetric term that
constrains the velocity of the fluid to adhere to the velocity of the rigid body
within $\Gamma$. Also in this case, $\beta$ must be chosen sufficiently large
in order to ensure that the Dirichlet boundary condition in $\Gamma$ is
sufficiently respected, but not too high in order to maintain the proper
conditioning of the system matrix.

A $\mathcal{H}^1$ penalization may be constructed in a similar manner, with the
addition of a viscous component to the penalization that dampens the velocity
gradient within $\Gamma$:

@f{eqnarray*}{
(\nabla \textbf{v}, \nabla \textbf{u})_{\Omega} - & (\textrm{div}\;  \textbf{v}, p)_{\Omega}
  - (q, \textrm{div}\; \textbf{u})_{\Omega}
  + \beta_1 (\textbf{v},\textbf{u})_{\Gamma}
  + \beta_2 (\nabla \textbf{v}, \nabla \textbf{u})_{\Gamma}
=  \beta_1 (\textbf{v},\textbf{g})_{\Gamma}
+ \beta_2 (\nabla \textbf{v}, \nabla \textbf{g})_{\Gamma}.
@f}

Notice that the $L^2$ penalization (`dim` equal to `spacedim`) and the Nitsche
penalization (`dim` equal to `spacedim-1`) result in the exact same numerical
implementation, thanks to the dimension independent capabilities of deal.II.


<h4>Representation of Ω and Γ</h4>

In this tutorial, both the embedded grid $\Gamma$ and the embedding
grid are described using a parallel::distributed::Triangulation. These two
triangulations can be built from functions in the GridGenerator namespace or by reading
a mesh file produced with another application (e.g. GMSH, see the
discussion in step-49). This is slightly
more general than what was previously done in step-60.

The addition of the immersed boundary method, whether
it is in the `dim=spacedim` or `dim<spacedim` case, only introduces
additional terms in the system matrix and the right-hand side of the
system which result from the integration over $\Gamma$. This does not
modify the number of variables for which the problem
must be solved. The challenge is thus related to the integrals
that must be carried over $\Gamma$.

As usual in finite elements we split this integral into contributions from all
cells of the triangulation used to
discretize $\Gamma$, we transform the integral on $K$ to an integral on the
reference element $\hat K$, where $F_{K}$ is the mapping from $\hat K$ to $K$,
and compute the integral on $\hat K$ using a quadrature formula. For example:

\f[
\beta (\textbf{v},\textbf{u})_{\Gamma} =  \sum_{K\in \Gamma} \int_{\hat K}
\hat{\textbf{u}}(\hat x) (\textbf{v} \circ F_{K}) (\hat x) J_K (\hat x) \mathrm{d} \hat x =
\sum_{K\in \Gamma} \sum_{i=1}^{n_q}  \big(\hat{\textbf{u}}(\hat x_i)  (\textbf{v} \circ F_{K}) (\hat x_i) J_K (\hat x_i) w_i \big)
\f]

Computing this sum is non-trivial because we have to evaluate $(v_j \circ F_{K})
(\hat x_i)$. In general, if $\Gamma$ and $\Omega$ are not aligned, the point
$y_i = F_{K}(\hat x_i)$ is completely arbitrary with respect to $\Omega$, and unless
we figure out a way to interpolate all basis functions of $V_h(\Omega)$ on an
arbitrary point on $\Omega$, we cannot compute the integral needed.


To evaluate $(v_j \circ F_{K}) (\hat x_i)$ the following steps needs to be
taken (as shown in the picture below):

- For a given cell $K$ in $\Gamma$ compute the real point $y_i \dealcoloneq F_{K} (\hat
x_i)$, where $x_i$ is one of the quadrature points used for the integral on $K
\subseteq \Gamma$. This is the easy part:
FEValues::quadrature_point() gives us the real-space locations of all
quadrature points.

- Find the cell of $\Omega$ in which $y_i$ lies. We shall call this element $T$.

- Find the reference coordinates within $T$ of $y_i$. For this, we
need the inverse of the mapping $G_T$ that
transforms the reference element $\hat T$ into the element $T$: $\hat y_i = G^{-1}_{T} (y_i)$.

- Evaluate the basis function $v_j$ of the $\Omega$ mesh at this
  point $\hat y_i$. This is, again, relatively simple using FEValues.


<p align="center"> <img
  src="https://www.dealii.org/images/steps/developer/step-60.C_interpolation.png"
  alt=""> </p>

In step-60, the second through fourth steps above were computed by calling, in turn,

- GridTools::find_active_cell_around_point(), followed by

- Mapping::transform_real_to_unit_cell(). We then

- construct a custom Quadrature formula, containing the point in the reference
 cell and then

- construct an FEValues object, with the given quadrature formula, and
 initialized with the cell obtained in the first step.

Although this approach could work for the present case, it does not lends itself
readily to parallel simulations using distributed triangulations. Indeed,
since the position of the quadrature points on the cells of the
embedded domain $\Gamma$ do not match that of the embedding triangulation
and since $\Gamma$ is constantly moving, this would require that the triangulation representing
$\Gamma$ be stored in it's entirety for all of the processors. As the number
of processor and the number of cells in $\Gamma$ increases, this leads
to a severe bottleneck in terms of memory. Consequently, an alternative strategy is sought
in this step.


<h4>Using particles to track Γ</h4>

Remember that for both the penalization approach ($\mathcal{L}^2$ or $\mathcal{H}^1$)
and the Nitsche method, we want to compute integrals that are approximated by
the quadrature. That is, we need to compute
\f[
\beta (\textbf{v},\textbf{u})_{\Gamma} =
\sum_{K\in \Gamma} \sum_{i=1}^{n_q}  \big(\hat{\textbf{u}}(\hat x_i)  (\textbf{v} \circ F_{K}) (\hat x_i) J_K (\hat x_i) w_i \big)
\f]
If you followed the discussion above, then you will recall that $\textbf{u}$
and $\textbf{v}$ are shape functions defined on the fluid mesh.
The only things defined on the solid mesh are:
$F_K(\hat x_i)$, which is the location of a quadrature point on a solid cell that
is part of $\Gamma$, $J_K$ is the determinant of its Jacobian, and $w_i$ the corresponding
quadrature weight.

The important part to realize is now this: $w_i$ is a property of
the quadrature formula and does not change with time. Furthermore,
the Jacobian matrix of $F_K$ itself changes as the solid obstacle
moves around in the fluid, but because the solid is considered
non-deforming (it only translates and rotates, but doesn't dilate),
the determinant of the Jacobian remains constant. As a consequence,
the product $J_K(\hat x_i) w_i$ (which we typically denote by `JxW`)
remains constant for each quadrature point. So the only thing we need
keep track of are the positions $x_i=F_K(\hat x_i)$ -- but these
move with the velocity of the solid domain.

In other words, we don't actually need to keep the solid mesh at all.
All we need is the positions $x_i(t)$ and corresponding `JxW` values.
Since both of these properties are point-properties (or point-vectors) that are
attached to the solid material, they can be idealized as a set of disconnected
infinitesimally small "particles", which carry the required `JxW` information with the
movement of the solid. deal.II has the ability to distribute and
store such a set of particles in large-scale parallel computations in the form of
the ParticleHandler class (for details on the implementation see @cite GLHPW2018),
and we will make use of this functionality in this tutorial.

Thus, the approach taken in this step is as follows:
- Create a parallel::distributed::Triangulation for the domain $\Gamma$;
- Create Particles::Particle at the positions of the quadrature points on $\Gamma$;
- Call the Particles::ParticleHandler::insert_global_particles() function,
  to distribute the particles across processors, *following the solid
  triangulation*;
- Attach the `JxW` values as a "property" to each Particles::Particle object.

This structure is relatively expensive to generate, but must only be generated
once per simulation. Once the Particles::ParticleHandler is generated and the
required information is attached to the particle, the integrals over $\Gamma$
can be carried out by exploiting the fact that particles are grouped cellwise
inside ParticleHandler, allowing us to:
- Looping over all cells of $\Omega$ that contain at least one particle
- Looping over all particles in the given cell
- Compute the integrals and fill the global matrix.

Since the Particles::ParticleHandler can manage the exchange of particles from
one processor to the other, the embedded
triangulation can be moved or deformed by displacing the particles.
The only constraint associated with this displacement is that particles should
be displaced by a distance that is no larger than the size of one
cell. That's because that is the limit to which
Particles::ParticleHandler can track which cell a particle that leaves
its current cell now resides in.

Once the entire problem (the Stokes problem and the immersed boundary
imposition) is assembled,
the final saddle point problem is solved by an iterative solver, applied to the
Schur complement $S$ (whose construction is described, for example, in step-22),
and we construct $S$ using LinearOperator classes.


<h3>The testcase</h3>

The problem we solve here is a demonstration of the time-reversibility of Stokes
flow. This is often illustrated in science education experiments with a
Taylor-Couette flow and dye droplets that revert back to their original shape
after the fluid has been displaced in a periodic manner.

@htmlonly

<iframe width="560" height="315" src="https://www.youtube.com/embed/p08_KlTKP50" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

@endhtmlonly

In the present problem, a very viscous fluid is agitated by the rotation of
an impeller, which, in 2D, is modeled by a rectangular grid. The impeller
rotates for a given number of revolutions, after which the flow is reversed such
that the same number of revolutions is carried out in the opposite direction. We
recall that since the Stokes equations are self-adjoint, creeping flows are
reversible. Consequently, if the impeller motion is reversed in the opposite
direction, the fluid should return to its original position. In the present
case, this is illustrated by inserting a circle of passive tracer particles that
are advected by the fluid and which return to their original position, thus
demonstrating the time-reversibility of the flow.


<h3> More references</h3>

This tutorial program uses a number of techniques on imposing velocity
conditions on non-matching interfaces in the interior of the fluid.
For more background material, you may want to look up the following references:
@cite Freund1995,
@cite Angot1999,
@cite Glowinski1999,
@cite Boffi2008,
@cite Heltai2012.


examples/step-70/doc/results.dox
<h1>Results</h1>

The directory in which this program is run contains a number of sample
parameter files that you can use to reproduce the results presented in this
section. If you do not specify a parameter file as an argument on the command
line, the program will try to read the file "`parameters.prm`" by default, and
will execute the two dimensional version of the code. As explained in
the discussion of the source code, if
your file name contains the string "23", then the program will run a three
dimensional problem, with immersed solid of co-dimension one. If it contains
the string "3", it will run a three dimensional problem, with immersed solid of
co-dimension zero, otherwise it will run a two dimensional problem with
immersed solid of co-dimension zero.

Regardless of the specific parameter file name, if the specified file does not
exist, when you execute the program you will get an exception that no such file
can be found:

@code
----------------------------------------------------
Exception on processing:

--------------------------------------------------------
An error occurred in line <74> of file <../source/base/parameter_acceptor.cc> in function
    static void dealii::ParameterAcceptor::initialize(const std::string &, const std::string &, const ParameterHandler::OutputStyle, dealii::ParameterHandler &)
The violated condition was:
    false
Additional information:
    You specified <parameters.prm> as input parameter file, but it does not exist. We created it for you.
--------------------------------------------------------

Aborting!
----------------------------------------------------
@endcode

However, as the error message already states, the code that triggers the
exception will also generate the specified file ("`parameters.prm`" in this case)
that simply contains the default values for all parameters this program cares
about (for the correct dimension and co-dimension, according to the whether a
string "23" or "3" is contained in the file name). By inspection of the default
parameter file, we see the following:

@code
# Listing of Parameters
# ---------------------
subsection Stokes Immersed Problem
  set Final time                            = 1
  # Extraction level of the rtree used to construct global bounding boxes
  set Fluid bounding boxes extraction level = 1

  # Boundary Ids over which homogeneous Dirichlet boundary conditions are
  # applied
  set Homogeneous Dirichlet boundary ids    = 0

  # Initial mesh refinement used for the fluid domain Omega
  set Initial fluid refinement              = 5

  # Initial mesh refinement used for the solid domain Gamma
  set Initial solid refinement              = 5
  set Nitsche penalty term                  = 100
  set Number of time steps                  = 501
  set Output directory                      = .
  set Output frequency                      = 1

  # Refinement of the volumetric mesh used to insert the particles
  set Particle insertion refinement         = 3
  set Velocity degree                       = 2
  set Viscosity                             = 1


  subsection Angular velocity
    # Sometimes it is convenient to use symbolic constants in the expression
    # that describes the function, rather than having to use its numeric value
    # everywhere the constant appears. These values can be defined using this
    # parameter, in the form `var1=value1, var2=value2, ...'.
    #
    # A typical example would be to set this runtime parameter to
    # `pi=3.1415926536' and then use `pi' in the expression of the actual
    # formula. (That said, for convenience this class actually defines both
    # `pi' and `Pi' by default, but you get the idea.)
    set Function constants  =

    # The formula that denotes the function you want to evaluate for
    # particular values of the independent variables. This expression may
    # contain any of the usual operations such as addition or multiplication,
    # as well as all of the common functions such as `sin' or `cos'. In
    # addition, it may contain expressions like `if(x>0, 1, -1)' where the
    # expression evaluates to the second argument if the first argument is
    # true, and to the third argument otherwise. For a full overview of
    # possible expressions accepted see the documentation of the muparser
    # library at http://muparser.beltoforion.de/.
    #
    # If the function you are describing represents a vector-valued function
    # with multiple components, then separate the expressions for individual
    # components by a semicolon.
    set Function expression = t < .500001 ? 6.283185 : -6.283185 # default: 0

    # The names of the variables as they will be used in the function,
    # separated by commas. By default, the names of variables at which the
    # function will be evaluated are `x' (in 1d), `x,y' (in 2d) or `x,y,z' (in
    # 3d) for spatial coordinates and `t' for time. You can then use these
    # variable names in your function expression and they will be replaced by
    # the values of these variables at which the function is currently
    # evaluated. However, you can also choose a different set of names for the
    # independent variables at which to evaluate your function expression. For
    # example, if you work in spherical coordinates, you may wish to set this
    # input parameter to `r,phi,theta,t' and then use these variable names in
    # your function expression.
    set Variable names      = x,y,t
  end

  subsection Grid generation
    set Fluid grid generator              = hyper_cube
    set Fluid grid generator arguments    = -1: 1: false
    set Particle grid generator           = hyper_ball
    set Particle grid generator arguments = 0.3, 0.3: 0.1: false
    set Solid grid generator              = hyper_rectangle
    set Solid grid generator arguments    = -.5, -.1: .5, .1: false
  end

  subsection Refinement and remeshing
    set Maximum number of cells        = 20000
    set Refinement coarsening fraction = 0.3
    set Refinement fraction            = 0.3
    set Refinement maximal level       = 8
    set Refinement minimal level       = 5
    set Refinement step frequency      = 5
    set Refinement strategy            = fixed_fraction
  end

  subsection Right hand side
    # Sometimes it is convenient to use symbolic constants in the expression
    # that describes the function, rather than having to use its numeric value
    # everywhere the constant appears. These values can be defined using this
    # parameter, in the form `var1=value1, var2=value2, ...'.
    #
    # A typical example would be to set this runtime parameter to
    # `pi=3.1415926536' and then use `pi' in the expression of the actual
    # formula. (That said, for convenience this class actually defines both
    # `pi' and `Pi' by default, but you get the idea.)
    set Function constants  =

    # The formula that denotes the function you want to evaluate for
    # particular values of the independent variables. This expression may
    # contain any of the usual operations such as addition or multiplication,
    # as well as all of the common functions such as `sin' or `cos'. In
    # addition, it may contain expressions like `if(x>0, 1, -1)' where the
    # expression evaluates to the second argument if the first argument is
    # true, and to the third argument otherwise. For a full overview of
    # possible expressions accepted see the documentation of the muparser
    # library at http://muparser.beltoforion.de/.
    #
    # If the function you are describing represents a vector-valued function
    # with multiple components, then separate the expressions for individual
    # components by a semicolon.
    set Function expression = 0; 0; 0

    # The names of the variables as they will be used in the function,
    # separated by commas. By default, the names of variables at which the
    # function will be evaluated are `x' (in 1d), `x,y' (in 2d) or `x,y,z' (in
    # 3d) for spatial coordinates and `t' for time. You can then use these
    # variable names in your function expression and they will be replaced by
    # the values of these variables at which the function is currently
    # evaluated. However, you can also choose a different set of names for the
    # independent variables at which to evaluate your function expression. For
    # example, if you work in spherical coordinates, you may wish to set this
    # input parameter to `r,phi,theta,t' and then use these variable names in
    # your function expression.
    set Variable names      = x,y,t
  end

end
@endcode

If you now run the program, you will get a file called `parameters_22.prm` in
the directory specified by the parameter `Output directory` (which defaults to
the current directory) containing a shorter version of the above parameters
(without comments and documentation), documenting all parameters that were used
to run your program:

@code
subsection Stokes Immersed Problem
  set Final time                            = 1
  set Fluid bounding boxes extraction level = 1
  set Homogeneous Dirichlet boundary ids    = 0
  set Initial fluid refinement              = 5
  set Initial solid refinement              = 5
  set Nitsche penalty term                  = 100
  set Number of time steps                  = 501
  set Output directory                      = .
  set Output frequency                      = 1
  set Particle insertion refinement         = 3
  set Velocity degree                       = 2
  set Viscosity                             = 1
  subsection Angular velocity
    set Function constants  =
    set Function expression = t < .500001 ? 6.283185 : -6.283185
    set Variable names      = x,y,t
  end
  subsection Grid generation
    set Fluid grid generator              = hyper_cube
    set Fluid grid generator arguments    = -1: 1: false
    set Particle grid generator           = hyper_ball
    set Particle grid generator arguments = 0.3, 0.3: 0.1: false
    set Solid grid generator              = hyper_rectangle
    set Solid grid generator arguments    = -.5, -.1: .5, .1: false
  end
  subsection Refinement and remeshing
    set Maximum number of cells        = 20000
    set Refinement coarsening fraction = 0.3
    set Refinement fraction            = 0.3
    set Refinement maximal level       = 8
    set Refinement minimal level       = 5
    set Refinement step frequency      = 5
    set Refinement strategy            = fixed_fraction
  end
  subsection Right hand side
    set Function constants  =
    set Function expression = 0; 0; 0
    set Variable names      = x,y,t
  end
end
@endcode

The rationale behind creating first `parameters.prm` file (the first time the
program is run) and then a `output/parameters_22.prm` (every time you
run the program with an existing input file), is because you may want
to leave most parameters to their
default values, and only modify a handful of them, while still beeing able to
reproduce the results and inspect what parameters were used for a specific
simulation. It is generally good scientific practice to store the
parameter file you used for a simulation along with the simulation
output so that you can repeat the exact same run at a later time if necessary.

Another reason is because the input file may only contain those
parameters that differ from their defaults.
For example, you could use the following (perfectly valid) parameter file with
this tutorial program:
@code
subsection Stokes Immersed Problem
  set Final time                         = 1
  set Nitsche penalty term               = 10
  set Number of time steps               = 101
  set Velocity degree                    = 3
end
@endcode
and you would run the program with Q3/Q2 Taylor-Hood finite elements, for 101
steps, using a Nitsche penalty of `10`, and leaving all the other parameters to
their default value. The output directory then contains a record of
not just these parameters, but indeed all parameters used in the
simulation. You can inspect all the other parameters in the
produced file `parameters_22.prm`.


<h3> Two dimensional test case </h3>

The default problem generates a co-dimension zero impeller, consisting of a
rotating rectangular grid, where the rotation is for half a time unit in one
direction, and half a time unit in the opposite direction, with constant angular
velocity equal to $\approx 2\pi \frac{\text{rad}}{\text{time unit}}$. Consequently, the impeller does half a
rotation and returns to its original position. The following animation
displays the velocity magnitude, the motion of the solid impeller and of the
tracer particles.


<p align="center">
   <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-70.2d_tracing.gif"
           alt = ""
           width="500">
    </div>
</p>

On one core, the output of the program will look like the following:

@code
bash$ mpirun -np 1 ./step-70 test.prm
Running StokesImmersedProblem<2> using Trilinos.
Cycle 0:
Time : 0, time step: 0.002
   Number of degrees of freedom: 9539 (8450+1089 -- 0+0)
Tracer particles: 337
Solid particles: 9216
   Solved in 158 iterations.
   Number of degrees of freedom: 9845 (8722+1123 -- 9216+337)
Cycle 1:
Time : 0.002, time step: 0.002
   Solved in 142 iterations.
Cycle 2:
Time : 0.004, time step: 0.002
   Solved in 121 iterations.
Cycle 3:
Time : 0.006, time step: 0.002
   Solved in 121 iterations.

...

Cycle 499:
Time : 0.998, time step: 0.002
   Solved in 199 iterations.
Cycle 500:
Time : 1, time step: 0.002
   Solved in 196 iterations.

+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |       302s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble Nitsche terms          |       501 |      43.3s |        14% |
| Assemble Stokes terms           |       501 |      21.5s |       7.1% |
| Initial setup                   |         1 |  0.000792s |         0% |
| Output fluid                    |       502 |      31.8s |        11% |
| Output solid particles          |       502 |      32.2s |        11% |
| Output tracer particles         |       502 |      0.61s |       0.2% |
| Refine                          |       100 |      4.68s |       1.5% |
| Set solid particle position     |       500 |      3.34s |       1.1% |
| Set tracer particle motion      |       501 |     0.729s |      0.24% |
| Setup dofs                      |       101 |       2.2s |      0.73% |
| Solve                           |       501 |       164s |        54% |
+---------------------------------+-----------+------------+------------+
@endcode

You may notice that assembling the coupling system is more expensive than
assembling the Stokes part. This depends highly on the number of Gauss points
(solid particles) that are used to apply the Nitsche restriction.
In the present case, a relatively low number of tracer particles are used.
Consequently, tracking their motion is relatively cheap.

The following movie shows the evolution of the solution over time:

@htmlonly
<p align="center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/y4Gypj2jpXw"
   frameborder="0"
   allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
   allowfullscreen></iframe>
 </p>
@endhtmlonly

The movie shows the rotating obstacle in gray (actually a
superposition of the solid particles plotted with large enough dots
that they overlap), <a
href="https://en.wikipedia.org/wiki/Streamlines,_streaklines,_and_pathlines">streamlines
of the fluid flow</a> in light colors (including the corner vertices
that form at specific times during the simulation), and the tracer particles in
bluish tones.

The simulation shows that at the end time,
the tracer particles have somewhat returned to their
original position, although they have been distorted by the flow field. The
following image compares the initial and the final position of the particles
after one time unit of flow.

<p align="center">
   <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-70.tracer_comparison.png"
           alt = ""
           width="500">
    </div>
</p>

In this case, we see that the tracer particles that were outside of the swept
volume of the impeller have returned very close to their initial position,
whereas those in the swept volume were slightly more deformed. This deformation
is non-physical. It is caused by the numerical error induced by the explicit
Euler scheme used to advect the particles, by the loss of accuracy due to the
fictitious domain and, finally, by the discretization error on the Stokes
equations. The first two errors are the leading cause of this deformation and
they could be alleviated by the use of a finer mesh and a lower time step.


<h3> Three dimensional test case </h3>

To play around a little bit, we complicate the fictitious domain (taken from
https://grabcad.com/library/lungstors-blower-1), and run a co-dimension one
simulation in three space dimensions, using the following
"`parameters_23.prm`" file:

@code
subsection Stokes Immersed Problem
  set Final time                            = 1
  set Homogeneous Dirichlet boundary ids    = 0
  set Fluid bounding boxes extraction level = 1
  set Initial fluid refinement              = 3
  set Initial solid refinement              = 0
  set Nitsche penalty term                  = 10
  set Number of time steps                  = 101
  set Output frequency                      = 1
  set Particle insertion refinement         = 3
  set Velocity degree                       = 2
  set Viscosity                             = 1
  subsection Angular velocity
    set Function constants  =
    set Function expression = t < .500001 ? 5 : -5
    set Variable names      = x,y,z,t
  end
  subsection Grid generation
    set Fluid grid generator              = hyper_rectangle
    set Fluid grid generator arguments    = -50,-50, -10: 50, 50, 40: false
    set Solid grid generator              = impeller.vtk
    set Solid grid generator arguments    = 1:impeller.step
    set Particle grid generator           = hyper_ball
    set Particle grid generator arguments = 30, 30, 20: 10: false
  end
  subsection Refinement and remeshing
    set Maximum number of cells        = 100000
    set Refinement coarsening fraction = 0.3
    set Refinement fraction            = 0.3
    set Refinement maximal level       = 6
    set Refinement step frequency      = 5
    set Refinement strategy            = fixed_fraction
  end
  subsection Right hand side
    set Function constants  =
    set Function expression = 0; 0; 0; 0
    set Variable names      = x,y,z,t
  end
end
@endcode

In this case, the timing outputs are a bit different:

@code
+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |  5.54e+03s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble Nitsche terms          |       101 |       111s |         2% |
| Assemble Stokes terms           |       101 |       208s |       3.8% |
| Initial setup                   |         1 |   0.00187s |         0% |
| Output fluid                    |       102 |      15.5s |      0.28% |
| Output solid particles          |       102 |      2.63s |         0% |
| Output tracer particles         |       102 |      2.49s |         0% |
| Refine                          |        20 |      18.4s |      0.33% |
| Set solid particle position     |       100 |       6.1s |      0.11% |
| Set tracer particle motion      |       101 |      10.8s |       0.2% |
| Setup dofs                      |        21 |      13.9s |      0.25% |
| Solve                           |       101 |  5.16e+03s |        93% |
+---------------------------------+-----------+------------+------------+
@endcode

Now, the solver is taking most of the solution time in three dimensions,
and the particle motion and Nitsche assembly remain relatively
unimportant as far as run time is concerned.


@htmlonly
<p align="center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/Srwq7zyR9mg"
   frameborder="0"
   allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
   allowfullscreen></iframe>
 </p>
@endhtmlonly


<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

The current tutorial program shows a one-way coupling between the fluid and the
solid, where the solid motion is imposed (and not solved for), and read in the
solid domain by exploiting the location and the weights of the solid quadrature
points.

The structure of the code already allows one to implement a two-way coupling,
by exploiting the possibility to read values of the fluid velocity on the
quadrature points of the solid grid. For this to be more efficient in terms of
MPI communication patterns, one should maintain ownership of the quadrature
points on the solid processor that owns the cells where they have been created.
In the current code, it is sufficient to define the IndexSet of the vectors
used to exchange information of the quadrature points by using the solid
partition instead of the initial fluid partition.

This allows the combination of the technique used in this tutorial program with
those presented in the tutorial step-60 to solve a fluid structure interaction
problem with distributed Lagrange multipliers, on
parallel::distributed::Triangulation objects.

The timings above show that the current preconditioning strategy does not work
well for Nitsche penalization, and we should come up with a better
preconditioner if we want to aim at larger problems. Moreover, a checkpoint
restart strategy should be implemented to allow for longer simulations to be
interrupted and restored, as it is done for example in the step-69 tutorial.


examples/step-71/doc/intro.dox
<br>

<i>This program was contributed by Jean-Paul Pelteret.
</i>


<h1>Introduction</h1>

The aim of this tutorial is, quite simply, to introduce the fundamentals of both
[automatic](https://en.wikipedia.org/wiki/Automatic_differentiation)
and [symbolic differentiation](https://en.wikipedia.org/wiki/Computer_algebra)
(respectively abbreviated as AD
and SD): Ways in which one can, in source code, describe a function
$\mathbf f(\mathbf x)$ and automatically also obtain a representation of derivatives
$\nabla \mathbf f(\mathbf x)$ (the "Jacobian"),
$\nabla^2 \mathbf f(\mathbf x)$ (the "Hessian"), etc., without having
to write additional lines of code. Doing this is quite helpful in
solving nonlinear or optimization problems where one would like to
only describe the nonlinear equation or the objective function in the
code, without having to also provide their derivatives (which are
necessary for a Newton method for solving a nonlinear problem, or for
finding a minimizer).

Since AD and SD tools are somewhat independent of finite elements and boundary value
problems, this tutorial is going to be different to the others that you may have
read beforehand. It will focus specifically on how these frameworks work and
the principles and thinking behind them, and will forgo looking at them in the
direct context of a finite element simulation.

We will, in fact, look at two different sets of problems that have greatly
different levels of complexity, but when framed properly hold sufficient
similarity that the same AD and SD frameworks can be leveraged. With these
examples the aim is to build up an understanding of the steps that are required
to use the AD and SD tools, the differences between them, and hopefully identify
where they could be immediately be used in order to improve or simplify existing
code.

It's plausible that you're wondering what AD and SD are, in the first place. Well,
that question is easy to answer but without context is not very insightful. So
we're not going to cover that in this introduction, but will rather defer this
until the first introductory example where we lay out the key points as this
example unfolds. To complement this, we should mention that the core theory for
both frameworks is extensively discussed in the @ref auto_symb_diff module, so
it bears little repeating here.

Since we have to pick *some* sufficiently interesting topic to investigate
and identify where AD and SD can be used effectively, the main problem that's
implemented in the second half of the tutorial is one of modeling a coupled
constitutive law, specifically a magneto-active material (with hysteretic effects).
As a means of an introduction to that, later in the introduction some grounding
theory for that class of materials will be presented.
Naturally, this is not a field (or even a class of materials) that is of
interest to a wide audience. Therefore, the author wishes to express up front
that this theory and any subsequent derivations mustn't be considered the focus
of this tutorial. Instead, keep in mind the complexity of the problem that arises
from the relatively innocuous description of the constitutive law, and what we
might (in the context of a boundary value problem) need to derive from that.
We will perform some computations with these constitutive laws at the level of a
representative continuum point (so, remaining in the  realm of continuum
mechanics), and will produce some benchmark results around which we can frame
a final discussion on the topic of computational performance.

Once we have the foundation upon which we can build further concepts, we
will see how AD in particular can be exploited at a finite element (rather than
continuum) level: this is a topic that is covered in step-72, as well as step-33.
But before then, let's take a moment to think about why we might want to consider
using these sorts of tools, and what benefits they can potentially offer you.


<h3>A motivation: Why would I use these tools?</h3>

The primary driver for using AD or SD is typically that there is some situation
that requires differentiation to be performed, and that doing so is sufficiently
challenging to make the prospect of using an external tool to perform that specific
task appealing. A broad categorization for the circumstances under which AD or
SD can be rendered most useful include (but are probably not limited to) the
following:
- <b>Rapid prototyping:</b> For a new class of problems where you're trying to
  implement a solution quickly, and want to remove some of the intricate details
  (in terms of both the mathematics as well as the organizational structure of
  the code itself). You might be willing to justify any additional computational
  cost, which would be offset by an increased agility in restructuring your code
  or modifying the part of the problem that is introducing some complex nonlinearity
  with minimal effort.
- <b>Complex problems:</b> It could very well be that some problems just happen to have
  a nonlinearity that is incredibly challenging to linearize or formulate by hand.
  Having this challenge taken care of for you by a tool that is, for the most part,
  robust, reliable, and accurate may alleviate some of the pains in implementing
  certain problems. Examples of this include step-15, where the
  derivative of the nonlinear PDE we solve is not incredibly difficult
  to derive, but sufficiently cumbersome that one has to pay attention
  in doing so by hand, and where implementing the corresponding finite
  element formulation of the Newton step takes more than just the few
  lines that it generally takes to implement the bilinear form;
  step-33 (where we actually use AD) is an even more extreme example.
- <b>Verification:</b> For materials and simulations that exhibit nonlinear response,
  an accurate rather than only approximate material tangent (the term mechanical engineers use for
  the derivative of a material law) can be the difference between convergent and
  divergent behavior, especially at high external (or coupling) loads.
  As the complexity of the problem increases, so do the opportunities to introduce
  subtle (or, perhaps, not-so-subtle) errors that produce predictably negative
  results.
  Additionally, there is a lot to be gained by verifying that the implementation is
  completely correct. For example, certain categories of problems are known to exhibit
  instabilities, and therefore when you start to lose quadratic convergence in a
  nonlinear solver (e.g., Newton's method) then this may not be a huge surprise to
  the investigator. However, it is hard (if not impossible) to distinguish between
  convergence behavior that is produced as you near an unstable solution and when
  you simply have an error in the material or finite element linearization, and
  start to drift off the optimal convergence path due to that. Having a
  method of verifying the correctness of the implementation of a constitutive law
  linearization, for example, is perhaps the only meaningful way that you can
  use to catch such errors, assuming that you've got nobody else to scrutinize your code.
  Thankfully, with some tactical programming it is quite straight-forward to structure
  a code for reuse, such that you can use the same classes in production code and
  directly verify them in, for instance, a unit-test framework.

This tutorial program will have two parts: One where we just introduce
the basic ideas of automatic and symbolic differentiation support in
deal.II using a simple set of examples; and one where we apply this to
a realistic but much more complicated case. For that second half, the
next section will provide some background on magneto-mechanical
materials -- you can skip this section if all you want to learn
about is what AD and SD actually are, but you probably want to read
over this section if you are interested in how to apply AD and SD for
concrete situations.


<h3>Theory for magneto-mechanical materials</h3>

<h4>Thermodynamic principles</h4>

As a prelude to introducing the coupled magneto-mechanical material law that we'll use
to model a magneto-active polymer, we'll start with a very concise summary of
the salient thermodynamics to which these constitutive laws must subscribe.
The basis for the theory, as summarized here, is described in copious detail by
Truesdell and Toupin @cite Truesdell1960a and Coleman and Noll @cite Coleman1963a,
and follows the logic laid out by Holzapfel @cite Holzapfel2007a.

Starting from the first law of thermodynamics, and following a few technical
assumptions, it can be shown the the balance between the kinetic plus internal
energy rates and the power supplied to the system from external
sources is given by the following relationship that equates the rate
of change of the energy in an (arbitrary) volume $V$ on the left, and
the sum of forces acting on that volume on the right:
@f[
  D_{t} \int\limits_{V} \left[
    \frac{1}{2} \rho_{0} \mathbf{v} \cdot \mathbf{v}
    + U^{*}_{0} \right] dV
= \int\limits_{V} \left[
  \rho_{0} \mathbf{v} \cdot \mathbf{a}
  + \mathbf{P}^{\text{tot}} : \dot{\mathbf{F}}
  + \boldsymbol{\mathbb{H}} \cdot \dot{\boldsymbol{\mathbb{B}}}
  + \mathbb{E} \cdot \dot{\mathbb{D}}
  - D_{t} M^{*}_{0}
  - \nabla_{0} \cdot \mathbf{Q}
  + R_{0} \right] dV .
@f]
Here $D_{t}$ represents the total time derivative,
$\rho_{0}$ is the material density as measured in the Lagrangian reference frame,
$\mathbf{v}$ is the material velocity and $\mathbf{a}$ its acceleration,
$U^{*}_{0}$ is the internal energy per unit reference volume,
$\mathbf{P}^{\text{tot}}$ is the total Piola stress tensor and $\dot{\mathbf{F}}$ is
the time rate of the deformation gradient tensor,
$\boldsymbol{\mathbb{H}}$ and $\boldsymbol{\mathbb{B}}$ are, respectively, the magnetic field vector and the
magnetic induction (or magnetic flux density) vector,
$\mathbb{E}$ and $\mathbb{D}$ are the electric field vector and electric
displacement vector, and
$\mathbf{Q}$ and $R_{0}$ represent the referential thermal flux vector and thermal
source.
The material differential operator
$\nabla_{0} (\bullet) \dealcoloneq \frac{d(\bullet)}{d\mathbf{X}}$
where $\mathbf{X}$ is the material position vector.
With some rearrangement of terms, invoking the arbitrariness of the integration
volume $V$, the total internal energy density rate $\dot{E}_{0}$ can be identified as
@f[
  \dot{E}_{0}
= \mathbf{P}^{\text{tot}} : \dot{\mathbf{F}}
  + \boldsymbol{\mathbb{H}} \cdot \dot{\boldsymbol{\mathbb{B}}}
  + \mathbb{E} \cdot \dot{\mathbb{D}}
  - \nabla_{0} \cdot \mathbf{Q}
  + R_{0} .
@f]
The total internal energy includes contributions that arise not only due to
mechanical deformation (the first term), and thermal fluxes and sources (the
fourth and fifth terms), but also due to the intrinsic energy stored in the
magnetic and electric fields themselves (the second and third terms,
respectively).

The second law of thermodynamics, known also as the entropy inequality principle,
informs us that certain thermodynamic processes are irreversible. After accounting
for the total entropy and rate of entropy input, the Clausius-Duhem inequality
can be derived. In local form (and in the material configuration), this reads
@f[
  \theta \dot{\eta}_{0}
  - R_{0}
  + \nabla_{0} \cdot \mathbf{Q}
  - \frac{1}{\theta} \nabla_{0} \theta \cdot \mathbf{Q}
  \geq 0 .
@f]
The quantity $\theta$ is the absolute temperature, and
$\eta_{0}$ represents the entropy per unit reference volume.

Using this to replace $R_{0} - \nabla_{0} \cdot \mathbf{Q}$ in the result
stemming from the first law of thermodynamics, we now have the relation
@f[
  \mathbf{P}^{\text{tot}} : \dot{\mathbf{F}}
  + \boldsymbol{\mathbb{H}} \cdot \dot{\boldsymbol{\mathbb{B}}}
  + \mathbb{E} \cdot \dot{\mathbb{D}}
  + \theta \dot{\eta}_{0}
  - \dot{E}_{0}
  - \frac{1}{\theta} \nabla_{0} \theta \cdot \mathbf{Q}
  \geq 0 .
@f]
On the basis of Fourier's law, which informs us that heat flows from regions
of high temperature to low temperature, the last term is always positive and
can be ignored.
This renders the local dissipation inequality
@f[
  \mathbf{P}^{\text{tot}} : \dot{\mathbf{F}}
  + \boldsymbol{\mathbb{H}} \cdot \dot{\boldsymbol{\mathbb{B}}}
  + \mathbb{E} \cdot \dot{\mathbb{D}}
  - \left[ \dot{E}_{0} - \theta \dot{\eta}_{0}  \right]
  \geq 0 .
@f]
It is postulated @cite Holzapfel2007a that the Legendre transformation
@f[
  \psi^{*}_{0}
= \psi^{*}_{0} \left( \mathbf{F}, \boldsymbol{\mathbb{B}}, \mathbb{D}, \theta \right)
= E_{0} - \theta \eta_{0} ,
@f]
from which we may define the free energy density function $\psi^{*}_{0}$ with the stated
parameterization, exists and is valid.
Taking the material rate of this equation and substituting it into the local
dissipation inequality results in the generic expression
@f[
  \mathcal{D}_{\text{int}}
  = \mathbf{P}^{\text{tot}} : \dot{\mathbf{F}}
  + \boldsymbol{\mathbb{H}} \cdot \dot{\boldsymbol{\mathbb{B}}}
  + \mathbb{E} \cdot \dot{\mathbb{D}}
  - \dot{\theta} \eta_{0}
  - \dot{\psi}^{*}_{0} \left( \mathbf{F}, \boldsymbol{\mathbb{B}}, \mathbb{D}, \theta \right)
  \geq 0 .
@f]
Under the assumption of isothermal conditions, and that the electric field does
not excite the material in a manner that is considered non-negligible, then this
dissipation inequality reduces to
@f[
  \mathcal{D}_{\text{int}}
  = \mathbf{P}^{\text{tot}} : \dot{\mathbf{F}}
  + \boldsymbol{\mathbb{H}} \cdot \dot{\boldsymbol{\mathbb{B}}}
  - \dot{\psi}^{*}_{0} \left( \mathbf{F}, \boldsymbol{\mathbb{B}} \right)
  \geq 0 .
@f]

<h4>Constitutive laws</h4>

When considering materials that exhibit mechanically dissipative behavior,
it can be shown that this can be captured within the dissipation inequality
through the augmentation of the material free energy density function with additional
parameters that represent internal variables @cite Holzapfel1996a. Consequently,
we write it as
@f[
  \mathcal{D}_{\text{int}}
  = \mathbf{P}^{\text{tot}} : \dot{\mathbf{F}}
  + \boldsymbol{\mathbb{H}} \cdot \dot{\boldsymbol{\mathbb{B}}}
  - \dot{\psi}^{*}_{0} \left( \mathbf{F}, \mathbf{F}_{v}^{i}, \boldsymbol{\mathbb{B}} \right)
  \geq 0 .
@f]
where $\mathbf{F}_{v}^{i} = \mathbf{F}_{v}^{i} \left( t \right)$ represents the
internal variable (which acts like a measure of the deformation gradient)
associated with the `i`th mechanical dissipative (viscous) mechanism.
As can be inferred from its parameterization, each of these internal parameters
is considered to evolve in time.
Currently the free energy density function $\psi^{*}_{0}$ is parameterized in terms of
the magnetic induction $\boldsymbol{\mathbb{B}}$. This is the natural parameterization that
comes as a consequence of the considered balance laws. Should such a class of
materials to be incorporated within a finite-element model, it would be ascertained
that a certain formulation of the magnetic problem, known as the magnetic vector
potential formulation, would need to be adopted. This has its own set of challenges,
so where possible the more simple magnetic scalar potential formulation may be
preferred. In that case, the magnetic problem needs to be parameterized in terms
of the magnetic field $\boldsymbol{\mathbb{H}}$. To make this re-parameterization, we execute
a final Legendre transformation
@f[
  \tilde{\psi}_{0} \left( \mathbf{F}, \mathbf{F}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)
  = \psi^{*}_{0} \left( \mathbf{F}, \mathbf{F}_{v}^{i}, \boldsymbol{\mathbb{B}} \right)
  - \boldsymbol{\mathbb{H}} \cdot \boldsymbol{\mathbb{B}} .
@f]
At the same time, we may take advantage of the principle of material frame
indifference in order to express the energy density function in terms of symmetric
deformation measures:
@f[
  \psi_{0} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)
  = \tilde{\psi}_{0} \left( \mathbf{F}, \mathbf{F}_{v}^{i}, \boldsymbol{\mathbb{H}} \right) .
@f]
The upshot of these two transformations (leaving out considerable explicit and
hidden details) renders the final expression for the reduced dissipation
inequality as
@f[
  \mathcal{D}_{\text{int}}
  = \mathbf{S}^{\text{tot}} : \frac{1}{2} \dot{\mathbf{C}}
  - \boldsymbol{\mathbb{B}} \cdot \dot{\boldsymbol{\mathbb{H}}}
  - \dot{\psi}_{0} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)
  \geq 0 .
@f]
(Notice the sign change on the second term on the right hand side, and the
transfer of the time derivative to the magnetic induction vector.)
The stress quantity $\mathbf{S}^{\text{tot}}$ is known as the total Piola-Kirchhoff
stress tensor and its energy conjugate $\mathbf{C} = \mathbf{F}^{T} \cdot \mathbf{F}$
is the right Cauchy-Green deformation tensor, and
$\mathbf{C}_{v}^{i} = \mathbf{C}_{v}^{i} \left( t \right)$ is the re-parameterized
internal variable associated with the `i`th mechanical dissipative (viscous)
mechanism.

Expansion of the material rate of the energy density function, and rearrangement of the
various terms, results in the expression
@f[
  \mathcal{D}_{\text{int}}
  = \left[ \mathbf{S}^{\text{tot}} - 2 \frac{\partial \psi_{0}}{\partial \mathbf{C}} \right] : \frac{1}{2} \dot{\mathbf{C}}
  - \sum\limits_{i}\left[ 2 \frac{\partial \psi_{0}}{\partial \mathbf{C}_{v}^{i}} \right] : \frac{1}{2} \dot{\mathbf{C}}_{v}^{i}
  + \left[ - \boldsymbol{\mathbb{B}} - \frac{\partial \psi_{0}}{\partial \boldsymbol{\mathbb{H}}} \right] \cdot \dot{\boldsymbol{\mathbb{H}}}
  \geq 0 .
@f]
At this point, its worth noting the use of the
[partial derivatives](https://en.wikipedia.org/wiki/Partial_derivative)
$\partial \left( \bullet \right)$. This is an important detail that will be
fundamental to a certain design choice made within the tutorial.
As brief reminder of what this signifies, the partial derivative of a
multi-variate function returns the derivative of that function with respect
to one of those variables while holding the others constant:
@f[
  \frac{\partial f\left(x, y\right)}{\partial x}
  = \frac{d f\left(x, y\right)}{d x} \Big\vert_{y} .
@f]
More specific to what's encoded in the dissipation inequality (with the very general
free energy density function $\psi_{0}$ with its parameterization yet to be formalized),
if one of the input variables is a function of another, it is also held constant
and the chain rule does not propagate any further, while the computing total
derivative would imply judicious use of the chain rule. This can be better
understood by comparing the following two statements:
@f{align*}
  \frac{\partial f\left(x, y\left(x\right)\right)}{\partial x}
  &= \frac{d f\left(x, y\left(x\right)\right)}{d x} \Big\vert_{y} \\
  \frac{d f\left(x, y\left(x\right)\right)}{d x}
  &= \frac{d f\left(x, y\left(x\right)\right)}{d x} \Big\vert_{y}
   + \frac{d f\left(x, y\left(x\right)\right)}{d y} \Big\vert_{x} \frac{d y\left(x\right)}{x} .
@f}

Returning to the thermodynamics of the problem, we next exploit the arbitrariness
of the quantities $\dot{\mathbf{C}}$ and $\dot{\boldsymbol{\mathbb{H}}}$,
by application of the Coleman-Noll procedure @cite Coleman1963a, @cite Coleman1967a.
This leads to the identification of the kinetic conjugate quantities
@f[
  \mathbf{S}^{\text{tot}}
  = \mathbf{S}^{\text{tot}} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)
  \dealcoloneq 2 \frac{\partial \psi_{0} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)}{\partial \mathbf{C}} , \\
  \boldsymbol{\mathbb{B}}
  = \boldsymbol{\mathbb{B}} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)
  \dealcoloneq - \frac{\partial \psi_{0} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)}{\partial \boldsymbol{\mathbb{H}}} .
@f]
(Again, note the use of the partial derivatives to define the stress and magnetic
induction in this generalized setting.)
From what terms remain in the dissipative power (namely those related to the
mechanical dissipative mechanisms), if they are assumed to be independent of
one another then, for each mechanism `i`,
@f[
  \frac{\partial \psi_{0}}{\partial \mathbf{C}_{v}^{i}} : \dot{\mathbf{C}}_{v}^{i}
  \leq 0 .
@f]
This constraint must be satisfied through the appropriate choice of free energy
function, as well as a carefully considered evolution law for the internal
variables.

In the case that there are no dissipative mechanisms to be captured within the
constitutive model (e.g., if the material to be modelled is magneto-hyperelastic)
then the free energy density function
$\psi_{0} = \psi_{0} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)$ reduces to a stored
energy density function, and the total stress and magnetic induction can be simplified
@f{align*}{
  \mathbf{S}^{\text{tot}}
  = \mathbf{S}^{\text{tot}} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)
  &\dealcoloneq 2 \frac{d \psi_{0} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)}{d \mathbf{C}} , \\
  \boldsymbol{\mathbb{B}}
  = \boldsymbol{\mathbb{B}} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)
  &\dealcoloneq - \frac{d \psi_{0} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)}{d \boldsymbol{\mathbb{H}}} ,
@f}
where the operator $d$ denotes the total derivative operation.

For completeness, the linearization of the stress tensor and magnetic induction
are captured within the fourth-order total referential elastic tangent tensor
$\mathcal{H}^{\text{tot}} $, the second-order magnetostatic tangent tensor $\mathbb{D}$ and the
third-order total referential magnetoelastic coupling tensor $\mathfrak{P}^{\text{tot}}$.
Irrespective of the parameterization of $\mathbf{S}^{\text{tot}}$ and $\boldsymbol{\mathbb{B}}$,
these quantities may be computed by
@f{align*}{
  \mathcal{H}^{\text{tot}}
  &= 2 \frac{d \mathbf{S}^{\text{tot}}}{d \mathbf{C}} , \\
  \mathbb{D}
  &= \frac{d \boldsymbol{\mathbb{B}}}{d \boldsymbol{\mathbb{H}}} , \\
  \mathfrak{P}^{\text{tot}}
  &= - \frac{d \mathbf{S}^{\text{tot}}}{d \boldsymbol{\mathbb{H}}} , \\
  \left[ \mathfrak{P}^{\text{tot}} \right]^{T}
  &= 2 \frac{d \boldsymbol{\mathbb{B}}}{d \mathbf{C}} .
@f}
For the case of rate-dependent materials, this expands to
@f{align*}{
  \mathcal{H}^{\text{tot}} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)
  &= 4 \frac{d^{2} \psi_{0} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)}{\partial \mathbf{C} \otimes d \mathbf{C}} , \\
  \mathbb{D} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)
  &= -\frac{d^{2} \psi_{0} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)}{\partial \boldsymbol{\mathbb{H}} \otimes d \boldsymbol{\mathbb{H}}} , \\
  \mathfrak{P}^{\text{tot}} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)
  &= - 2 \frac{d^{2} \psi_{0} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)}{\partial \boldsymbol{\mathbb{H}} \otimes d \mathbf{C}} , \\
  \left[ \mathfrak{P}^{\text{tot}} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)  \right]^{T}
  &= - 2 \frac{d^{2} \psi_{0} \left( \mathbf{C}, \mathbf{C}_{v}^{i}, \boldsymbol{\mathbb{H}} \right)}{\partial \mathbf{C} \otimes d \boldsymbol{\mathbb{H}}} ,
@f}
while for rate-independent materials the linearizations are
@f{align*}{
  \mathcal{H}^{\text{tot}} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)
  &= 4 \frac{d^{2} \psi_{0} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)}{d \mathbf{C} \otimes d \mathbf{C}} , \\
  \mathbb{D} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)
  &= -\frac{d^{2} \psi_{0} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)}{d \boldsymbol{\mathbb{H}} \otimes d \boldsymbol{\mathbb{H}}} , \\
  \mathfrak{P}^{\text{tot}} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)
  &= - 2 \frac{d^{2} \psi_{0} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)}{d \boldsymbol{\mathbb{H}} \otimes d \mathbf{C}} , \\
  \left[ \mathfrak{P}^{\text{tot}} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)  \right]^{T}
  &= - 2 \frac{d^{2} \psi_{0} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)}{d \mathbf{C} \otimes d \boldsymbol{\mathbb{H}}} .
@f}
The subtle difference between them is the application of a partial derivative during
the calculation of the first derivatives. We'll see later how this affects the choice
of AD versus SD for this specific application. For now, we'll simply introduce
the two specific materials that are implemented within this tutorial.

<h5>Magnetoelastic constitutive law</h5>

The first material that we'll consider is one that is governed by a
magneto-hyperelastic constitutive law. This material responds to both
deformation as well as immersion in a magnetic field, but exhibits no
time- or history-dependent behavior (such as dissipation through viscous
damping or magnetic hysteresis, etc.). The *stored energy density
function* for such a material is only parameterized in terms of the
(current) field variables, but not their time derivatives or past values.

We'll choose the energy density function, which captures both the energy
stored in the material due to deformation and magnetization, as well as
the energy stored in the magnetic field itself, to be
@f[
  \psi_{0} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)
= \frac{1}{2} \mu_{e} f_{\mu_{e}} \left( \boldsymbol{\mathbb{H}} \right)
    \left[ \text{tr}(\mathbf{C}) - d - 2 \ln (\text{det}(\mathbf{F}))
    \right]
+ \lambda_{e} \ln^{2} \left(\text{det}(\mathbf{F}) \right)
- \frac{1}{2} \mu_{0} \mu_{r} \text{det}(\mathbf{F})
    \left[ \boldsymbol{\mathbb{H}} \cdot \mathbf{C}^{-1} \cdot
    \boldsymbol{\mathbb{H}} \right]
@f]
with
@f[
  f_{\mu_{e}} \left( \boldsymbol{\mathbb{H}} \right)
= 1 + \left[ \frac{\mu_{e}^{\infty}}{\mu_{e}} - 1 \right]
    \tanh \left( 2 \frac{\boldsymbol{\mathbb{H}} \cdot
    \boldsymbol{\mathbb{H}}}
      {\left(h_{e}^{\text{sat}}\right)^{2}} \right)
@f]
and for which the variable $d = \text{tr}(\mathbf{I})$ ($\mathbf{I}$
being the rank-2 identity tensor) represents the spatial dimension and
$\mathbf{F}$ is the deformation gradient tensor. To give some brief
background to the various components of $\psi_{0}$, the first two terms
bear a great resemblance to the stored energy density function for a
(hyperelastic) Neohookean material. The only difference between what's
used here and the Neohookean material is the scaling of the elastic shear
modulus by the magnetic field-sensitive saturation function $f_{\mu_{e}}
\left( \boldsymbol{\mathbb{H}} \right)$ (see @cite Pelteret2018a, equation
29). This function will, in effect, cause the material to stiffen in the
presence of a strong magnetic field. As it is governed by a sigmoid-type
function, the shear modulus will asymptotically converge on the specified
saturation shear modulus. It can also be shown that the last term in
$\psi_{0}$ is the stored energy density function for magnetic field (as
derived from first principles), scaled by the relative permeability
constant. This definition collectively implies that the material is
linearly magnetized, i.e., the magnetization vector and magnetic field
vector are aligned. (This is certainly not obvious with the magnetic energy
stated in its current form, but when the magnetic induction and magnetization
are derived from $\psi_{0}$ and all magnetic fields are expressed in the
<em>current configuration</em> then this correlation becomes clear.)
As for the specifics of what the magnetic induction, stress tensor, and the
various material tangents look like, we'll defer presenting these to the
tutorial body where the full, unassisted implementation of the constitutive
law is defined.

<h5>Magneto-viscoelastic constitutive law</h5>

The second material that we'll formulate is one for a
magneto-viscoelastic material with a single dissipative mechanism `i`.
The *free energy density function* that we'll be considering is defined as
@f{align*}{
  \psi_{0} \left( \mathbf{C}, \mathbf{C}_{v}, \boldsymbol{\mathbb{H}}
  \right)
&= \psi_{0}^{ME} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)
+ \psi_{0}^{MVE} \left( \mathbf{C}, \mathbf{C}_{v},
\boldsymbol{\mathbb{H}} \right)
\\ \psi_{0}^{ME} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)
&= \frac{1}{2} \mu_{e} f_{\mu_{e}^{ME}} \left( \boldsymbol{\mathbb{H}}
\right)
    \left[ \text{tr}(\mathbf{C}) - d - 2 \ln (\text{det}(\mathbf{F}))
    \right]
+ \lambda_{e} \ln^{2} \left(\text{det}(\mathbf{F}) \right)
- \frac{1}{2} \mu_{0} \mu_{r} \text{det}(\mathbf{F})
    \left[ \boldsymbol{\mathbb{H}} \cdot \mathbf{C}^{-1} \cdot
    \boldsymbol{\mathbb{H}} \right]
\\ \psi_{0}^{MVE} \left( \mathbf{C}, \mathbf{C}_{v},
\boldsymbol{\mathbb{H}} \right)
&= \frac{1}{2} \mu_{v} f_{\mu_{v}^{MVE}} \left( \boldsymbol{\mathbb{H}}
\right)
    \left[ \mathbf{C}_{v} : \left[
      \left[\text{det}\left(\mathbf{F}\right)\right]^{-\frac{2}{d}}
      \mathbf{C} \right] - d - \ln\left(
      \text{det}\left(\mathbf{C}_{v}\right) \right)  \right]
@f}
with
@f[
  f_{\mu_{e}}^{ME} \left( \boldsymbol{\mathbb{H}} \right)
= 1 + \left[ \frac{\mu_{e}^{\infty}}{\mu_{e}} - 1 \right]
    \tanh \left( 2 \frac{\boldsymbol{\mathbb{H}} \cdot
    \boldsymbol{\mathbb{H}}}
      {\left(h_{e}^{\text{sat}}\right)^{2}} \right)
@f]
@f[
  f_{\mu_{v}}^{MVE} \left( \boldsymbol{\mathbb{H}} \right)
= 1 + \left[ \frac{\mu_{v}^{\infty}}{\mu_{v}} - 1 \right]
    \tanh \left( 2 \frac{\boldsymbol{\mathbb{H}} \cdot
    \boldsymbol{\mathbb{H}}}
      {\left(h_{v}^{\text{sat}}\right)^{2}} \right)
@f]
and the evolution law
@f[
  \dot{\mathbf{C}}_{v} \left( \mathbf{C} \right)
= \frac{1}{\tau} \left[
      \left[\left[\text{det}\left(\mathbf{F}\right)\right]^{-\frac{2}{d}}
        \mathbf{C}\right]^{-1}
    - \mathbf{C}_{v} \right]
@f]
for the internal viscous variable.
We've chosen the magnetoelastic part of the energy
$\psi_{0}^{ME} \left( \mathbf{C}, \boldsymbol{\mathbb{H}} \right)$
to match that of the first material model that we explored, so this part
needs no further explanation. As for the viscous part $\psi_{0}^{MVE}$,
this component of the free energy (in conjunction with the evolution law for
the viscous deformation tensor) is taken from @cite Linder2011a (with the
additional scaling by the viscous saturation function described in
@cite Pelteret2018a). It is derived in a thermodynamically consistent
framework that, at its core, models the movement of polymer chains on a
micro-scale level.

To proceed beyond this point, we'll also need to consider the time
discretization of the evolution law.
Choosing the implicit first-order backwards difference scheme, then
@f[
  \dot{\mathbf{C}}_{v}
\approx \frac{\mathbf{C}_{v}^{(t)} - \mathbf{C}_{v}^{(t-1)}}{\Delta t}
= \frac{1}{\tau} \left[
      \left[\left[\text{det}\left(\mathbf{F}\right)\right]^{-\frac{2}{d}}
        \mathbf{C}\right]^{-1}
    - \mathbf{C}_{v}^{(t)} \right]
@f]
where the superscript $(t)$ denotes that the quantity is taken at the
current timestep, and $(t-1)$ denotes quantities taken at the previous
timestep (i.e., a history variable). The timestep size $\Delta t$ is the
difference between the current time and that of the previous timestep.
Rearranging the terms so that all internal variable quantities at the
current time are on the left hand side of the equation, we get
@f[
\mathbf{C}_{v}^{(t)}
= \frac{1}{1 + \frac{\Delta t}{\tau_{v}}} \left[
    \mathbf{C}_{v}^{(t-1)}
  + \frac{\Delta t}{\tau_{v}}
    \left[\left[\text{det}\left(\mathbf{F}\right)\right]^{-\frac{2}{d}}
    \mathbf{C} \right]^{-1}
  \right]
@f]
that matches @cite Linder2011a equation 54.

<h3>Rheological experiment</h3>

Now that we have shown all of these formulas for the thermodynamics and theory
governing magneto-mechanics and constitutive models, let us outline what the
program will do with all of this.
We wish to do something *meaningful* with the materials laws that we've formulated,
and so it makes sense to subject them to some mechanical and magnetic loading
conditions that are, in some way, representative of some conditions that might
be found either in an application or in a laboratory setting. One way to achieve
that aim would be to embed these constitutive laws in a finite element model to
simulate a device. In this instance, though, we'll keep things simple (we are
focusing on the automatic and symbolic differentiation concepts, after all)
and will find a concise way to faithfully replicate an industry-standard
rheological experiment using an analytical expression for the loading conditions.

The rheological experiment that we'll reproduce,
which idealizes a laboratory experiment that was used to characterize
magneto-active polymers, is detailed in @cite Pelteret2018a
(as well as @cite Pelteret2019a, in which it is documented along with the
real-world experiments). The images below provide a visual description of
the problem set up.

<table align="center" class="tutorial" cellspacing="3" cellpadding="3">
  <tr>
    <td align="center">
        <img
        src="https://www.dealii.org/images/steps/developer/step-71.parallel_plate-geometry.png"
        alt="" height="300">
  <p align="center">
        The basic functional geometry of the parallel-plate rotational
        rheometer. The smooth rotor (blue) applies a torque to an
        experimental sample (red) of radius $r$ and height $H$ while an
        axially aligned magnetic field generated by a a
        magneto-rheological device. Although the time-dependent
        deformation profile of the may be varied, one common experiment
        would be to subject the material to a harmonic torsional
        deformation of constant amplitude and frequency $\omega$.
  </p>
    </td>
    <td align="center">
        <img
        src="https://www.dealii.org/images/steps/developer/step-71.parallel_plate-kinematics.png"
        alt="" height="300">
  <p align="center">
        Schematic of the kinematics of the problem, assuming no
        preloading or compression of the sample. A point $\mathbf{P}$
        located at azimuth $\Theta$ is displaced to location $\mathbf{p}$
        at azimuth $\theta = \Theta + \alpha$.
  </p>
    </td>
  </tr>
</table>

Under the assumptions that an incompressible medium is being tested,
and that the deformation profile through the sample thickness is linear,
then the displacement at some measurement point $\mathbf{X}$ within
the sample, expressed in radial coordinates, is
@f{align*}
  r(\mathbf{X})
  &= \frac{R(X_{1}, X_{2})}{\sqrt{\lambda_{3}}} , \\
  \theta(\mathbf{X})
  & = \Theta(X_{1}, X_{2}) + \underbrace{\tau(t)
       \lambda_{3} X_{3}}_{\alpha(X_{3}, t)} , \\
  z(\mathbf{X})
  &= \lambda_{3} X_{3}
@f}
where
$R(X_{1}, X_{2})$ and $\Theta(X_{1}, X_{2})$ are the radius at
-- and angle of -- the sampling point,
$\lambda_{3}$ is the (constant) axial deformation,
$\tau(t) = \frac{A}{RH} \sin\left(\omega t\right)$ is the time-dependent
torsion angle per unit length that will be prescribed using a
sinusoidally repeating oscillation of fixed amplitude $A$.
The magnetic field is aligned axially, i.e., in the $X_{3}$ direction.

This summarizes everything that we need to fully characterize the idealized
loading at any point within the rheological sample. We'll set up the problem
in such a way that we "pick" a representative point with this sample, and
subject it to a harmonic shear deformation at a constant axial deformation
(by default, a compressive load) and a constant, axially applied magnetic
field. We will record the stress and magnetic induction at this point, and
will output that data to file for post-processing. Although its not necessary
for this particular problem, we will also be computing the tangents as well.
Even though they are not directly used in this particular piece of work, these
second derivatives are needed to embed the constitutive law within a
finite element model (one possible extension to this work). We'll therefore
take the opportunity to check our hand calculations for correctness using
the assisted differentiation frameworks.

<h3>Suggested literature</h3>

In addition to the already mentioned @ref auto_symb_diff module, the following are a few
references that discuss in more detail
- magneto-mechanics, and some aspects of automated differentiation frameworks: @cite Pao1978a, @cite Pelteret2019a, and
- the automation of finite element frameworks using AD and/or SD: @cite Logg2012a, @cite Korelc2016a.

<br>


examples/step-71/doc/results.dox
<h1>Results</h1>

<h3>Introductory example</h3>

The first exploratory example produces the following output. It is verified that
all three implementations produce identical results.
@code
> ./step-71
Simple example using automatic differentiation...
... all calculations are correct!
Simple example using symbolic differentiation.
... all calculations are correct!
@endcode

<h3>Constitutive modelling</h3>

To help summarize the results from the virtual experiment itself, below are some
graphs showing the shear stress, plotted against the shear strain, at a select
location within the material sample. The plots show the stress-strain curves under
three different magnetic loads, and for the last cycle of the (mechanical)
loading profile, when the rate-dependent material reaches a repeatable
("steady-state") response. These types of graphs are often referred to as
[Lissajous plots](https://en.wikipedia.org/wiki/Lissajous_curve). The area
of the ellipse that the curve takes for viscoelastic materials provides some
measure of how much energy is dissipated by the material, and its ellipticity
indicates the phase shift of the viscous response with respect to the elastic
response.

<table align="center" class="tutorial" cellspacing="3" cellpadding="3">
  <tr>
     <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-71.lissajous_plot-me.png" alt="" width="400">
	<p align="center">
        Lissajous plot for the magneto-elastic material.
	</p>
    </td>
    <td align="center">
        <img src="https://www.dealii.org/images/steps/developer/step-71.lissajous_plot-mve.png" alt="" width="400">
	<p align="center">
        Lissajous plot for the magneto-viscoelastic material.
	</p>
    </td>
  </tr>
</table>

It is not surprising to see that the magneto-elastic material response has an unloading
curve that matches the loading curve -- the material is non-dissipative after all.
But here it's clearly noticeable how the gradient of the curve increases as the
applied magnetic field increases. The tangent at any point along this curve is
related to the instantaneous shear modulus and, due to the way that the energy
density function was defined, we expect that the shear modulus increases as the
magnetic field strength increases.
We observe much the same behavior for the magneto-viscoelastic material. The major
axis of the ellipse traced by the loading-unloading curve has a slope that increases
as a greater magnetic load is applied. At the same time, the more energy is
dissipated by the material.

As for the code output, this is what is printed to the console for the part
pertaining to the rheological experiment conducted with the magnetoelastic
material:
@code
Coupled magnetoelastic constitutive law using automatic differentiation.
Timestep = 0 @ time = 0s.
Timestep = 125 @ time = 0.314159s.
Timestep = 250 @ time = 0.628318s.
Timestep = 375 @ time = 0.942477s.
...
Timestep = 12250 @ time = 30.7876s.
Timestep = 12375 @ time = 31.1018s.
Timestep = 12500 @ time = 31.4159s.
... all calculations are correct!
@endcode

And this portion of the output pertains to the experiment performed with the
magneto-viscoelastic material:
@code
Coupled magneto-viscoelastic constitutive law using symbolic differentiation.
Using LLVM optimizer.
Timestep = 0 @ time = 0s.
Timestep = 125 @ time = 0.314159s.
Timestep = 250 @ time = 0.628318s.
Timestep = 375 @ time = 0.942477s.
...
Timestep = 12250 @ time = 30.7876s.
Timestep = 12375 @ time = 31.1018s.
Timestep = 12500 @ time = 31.4159s.
... all calculations are correct!
@endcode

The timer output is also emitted to the console, so we can compare time taken
to perform the hand- and assisted- calculations and get some idea of the overhead
of using the AD and SD frameworks.
Here are the timings taken from the magnetoelastic experiment using
the AD framework, based on the Sacado component of the Trilinos library:
@code
+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |       3.2s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assisted computation            |     12501 |      3.02s |        95% |
| Hand calculated                 |     12501 |    0.0464s |       1.5% |
+---------------------------------+-----------+------------+------------+
@endcode
With respect to the computations performed using automatic differentiation
(as a reminder, this is with two levels of differentiation using the Sacado
library in conjunction with dynamic forward auto-differentiable types), we
observe that the assisted computations takes about $65 \times$ longer to
compute the desired quantities. This does seem like quite a lot of overhead
but, as mentioned in the introduction, it's entirely subjective and
circumstance-dependent as to whether or not this is acceptable or not:
Do you value computer time more than human time for doing the
necessary hand-computations of derivatives, verify their correctness,
implement them, and verify the correctness of the implementation? If
you develop a research code that will only be run for a relatively
small number of experiments, you might value your own time more. If
you develop a production code that will be run over and over on
10,000-core clusters for hours, your considerations might be different.
In any case, the one nice feature
of the AD approach is the "drop in" capability when functions and classes are
templated on the scalar type. This means that minimal effort is required to
start working with it.

In contrast, the timings for magneto-viscoelastic material as implemented using
just-in-time (JIT) compiled symbolic algebra indicate that, at some non-negligible cost during
initialization, the calculations themselves are a lot more efficiently executed:
@code
+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |      1.34s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assisted computation            |     12501 |     0.376s |        28% |
| Hand calculated                 |     12501 |     0.368s |        27% |
| Initialize symbolic CL          |         1 |     0.466s |        35% |
+---------------------------------+-----------+------------+------------+
@endcode
Since the initialization phase need, most likely, only be executed once per
thread, this initial expensive phase can be offset by the repeated use of a
single Differentiation::SD::BatchOptimizer instance. Even though the
magneto-viscoelastic constitutive law has more terms to calculate when compared
to its magnetoelastic counterpart, it still is a whole order of magnitude faster
to execute the computations of the kinetic variables and tangents. And when compared
to the hand computed variant that uses the caching scheme, the calculation time
is nearly equal. So although using the symbolic framework requires a paradigm
shift in terms of how one implements and manipulates the symbolic expressions,
it can offer good performance and flexibility that the AD frameworks lack.

On the point of data caching, the added cost of value caching for the
magneto-viscoelastic material implementation is, in fact, about a $6\times$
increase in the time spent in `update_internal_data()` when compared to the
implementation using intermediate values for the numerical experiments conducted
with this material. Here's a sample output of the timing comparison extracted for
the "hand calculated" variant when the caching data structure is removed:
@code
+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |      1.01s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assisted computation            |     12501 |     0.361s |        36% |
| Hand calculated                 |     12501 |    0.0562s |       5.6% |
| Initialize symbolic CL          |         1 |     0.469s |        47% |
+---------------------------------+-----------+------------+------------+
@endcode

With some minor adjustment we can quite easily test the different optimization
schemes for the batch optimizer. So let's compare the computational expense
associated with the `LLVM` batch optimizer setting versus the alternatives.
Below are the timings reported for the `lambda` optimization method (retaining
the use of CSE):
@code
+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |      3.87s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assisted computation            |     12501 |      3.12s |        81% |
| Hand calculated                 |     12501 |     0.394s |        10% |
| Initialize symbolic CL          |         1 |     0.209s |       5.4% |
+---------------------------------+-----------+------------+------------+
@endcode
The primary observation here is that an order of magnitude greater time is spent
in the "Assisted computation" section when compared to the `LLVM` approach.

Last of all we'll test how `dictionary` substitution, in conjunction with CSE,
performs. Dictionary substitution simply does all of the evaluation within the
native CAS framework itself, with no transformation of the underlying data
structures taking place. Only the use of CSE, which caches intermediate results,
will provide any "acceleration" in this instance. With that in mind, here are
the results from this selection:
@code
+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |  1.54e+03s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assisted computation            |     12501 |  1.54e+03s |     1e+02% |
| Hand calculated                 |     12501 |     0.563s |         0% |
| Initialize symbolic CL          |         1 |     0.184s |         0% |
+---------------------------------+-----------+------------+------------+
@endcode
Needless to say, compared to the other two methods, these results took quite
some time to produce... The `dictionary` substitution
method is perhaps only really viable for simple expressions or when the number
of calls is sufficiently small.

<h1>So, which framework should I use?</h1>

Perhaps you've been convinced that these tools have some merit, and can be
of immediate help or use to you. The obvious question now is which one to
use. Focusing specifically at a continuum point level, where you would be
using these frameworks to compute derivatives of a constitutive law in
particular, we can say the following:
- Automatic differentiation probably provides the simplest entry point into
  the world of assisted differentiation.
- Given a sufficiently generic implementation of a constitutive framework,
  AD can often be used as a drop-in replacement for the intrinsic scalar types
  and the helper classes can then be leveraged to compute first (and possibly
  higher order) derivatives with minimal effort.
- As a qualification to the above point, being a "drop-in replacement" does not
  mean that you must not be contentious of what the algorithms that these numbers
  are being passed through are doing. It is possible to inadvertently perform
  an operation that would, upon differentiating, return an incorrect result.
  So this is definitely something that one should be aware of.
  A concrete example: When computing the eigenvalues of a tensor, if the tensor
  is diagonal then a short-cut to the result is simply to return the diagonal
  entries directly (as extracted from the input tensor). This is completely
  correct in terms of computing the eigenvalues themselves, but not going
  through the algorithm that would otherwise compute the eigenvalues for a
  non-diagonal tensor has had an unintended side-effect, namely that the
  eigenvalues appear (to the AD framework) to be completely decoupled from
  one another and their cross-sensitivities are not encoded in the returned
  result. Upon differentiating, many entries of the derivative tensor will
  be missing. To fix this issue, one has to ensure that the standard eigenvalue
  solving algorithm is used so that the sensitivities of the returned eigenvalues
  with respect to one another are encoded in the result.
- Computations involving AD number types may be expensive. The expense increases
  (sometimes quite considerably) as the order of the differential operations
  increases. This may be mitigated by computational complexity of surrounding
  operations (such as a linear solve, for example), but is ultimately problem
  specific.
- AD is restricted to the case where only total derivatives are required. If a
  differential operation requires a partial derivative with respect to an
  independent variable then it is not appropriate to use it.
- Each AD library has its own quirks (sad to say but, in the author's experience,
  true), so it may take some trial and error to find the appropriate library and
  choice of AD number to suit your purposes. The reason for these "quirks"
  often boils down to the overall philosophy behind the library (data structures,
  the use of template meta-programming, etc.) as well as the mathematical
  implementation of the derivative computations (for example, manipulations of
  results using logarithmic functions to change basis might restrict the domain
  for the input values -- details all hidden from the user, of course).
  Furthermore, one library might be able to compute the desired results quicker
  than another, so some initial exploration might be beneficial in that regard.
- Symbolic differentiation (well, the use of a CAS in general) provides the most
  flexible framework with which to perform assisted computations.
- The SD framework can do everything that the AD frameworks can, with the
  additional benefit of having low-level control over when certain manipulations
  and operations are performed.
- Acceleration of expression evaluation is possible, potentially leading to
  near-native performance of the SD framework compared to some hand implementations
  (this comparison being dependent on the overall program design, of course) at
  the expense of the initial optimization call.
- Clever use of the Differentiation::SD::BatchOptimizer could minimize the
  expense of the costly call that optimizes the dependent expressions.
  The possibility to serialize the Differentiation::SD::BatchOptimizer
  that often (but not always) this expensive call can be done once and then
  reused in a later simulation.
- If two or more material laws differ by only their material parameters, for
  instance, then a single batch optimizer can be shared between them as long
  as those material parameters are considered to be symbolic. The implication
  of this is that you can "differentiate once, evaluate in many contexts".
- The SD framework may partially be used as a "drop-in replacement" for scalar
  types, but one (at the very least) has to add some more framework around it
  to perform the value substitution step, converting symbolic types to their
  numerical counterparts.
- It may not be possible to use SD numbers within some specialized algorithms.
  For example, if an algorithm has an exit point or code branch based off of
  some concrete, numerical value that the (symbolic) input argument should take,
  then obviously this isn't going to work. One either has to reimplement the
  algorithm specifically for SD number types (somewhat inconvenient, but
  frequently possible as conditionals are supported by the
  Differentiation::SD::Expression class), or one must use a creative means
  around this specific issue (e.g., introduce a symbolic expression that
  represents the result returned by this algorithm, perhaps declaring it
  to be a
  [symbolic function](https://dealii.org/developer/doxygen/deal.II/namespaceDifferentiation_1_1SD.html#a876041f6048705c7a8ad0855cdb1bd7a)
  if that makes sense within the context in which it is to be used. This can
  later be substituted by its numerical values, and if declared a symbolic
  function then its deferred derivatives may also be incorporated into the
  calculations as substituted results.).
- The biggest drawback to using SD is that using it requires a paradigm shift,
  and that one has to frame most problems differently in order to take the
  most advantage of it. (Careful consideration of how the data structures
  are used and reused is also essential to get it to work effectively.) This may
  mean that one needs to play around with it a bit and build up an understanding
  of what the sequence of typical operations is and what specifically each step
  does in terms of manipulating the underlying data. If one has the time and
  inclination to do so, then the benefits of using this tool may be substantial.

<h1>Possibilities for extension</h1>

There are a few logical ways in which this program could be extended:
- Perhaps the most obvious extension would be to implement and test other constitutive models.
  This could still be within the realm of coupled magneto-mechanical problems, perhaps considering
  alternatives to the "Neo-Hookean"-type elastic part of the energy functions, changing the
  constitutive law for the dissipative energy (and its associated evolution law), or including
  magnetic hysteretic effects or damage models for the composite polymer that these material
  seek to model.
- Of course, the implemented models could be modified or completely replaced with models that are
  focused on other aspects of physics, such as electro-active polymers, biomechanical materials,
  elastoplastic media, etc.
- Implement a different time-discretization scheme for the viscoelastic evolution law.
- Instead of deriving everything directly from an energy density function, use the
  Differentiation::AD::VectorFunction to directly linearize the kinetic quantities.
  This would mean that only a once-differentiable auto-differentiable number type
  would be required, and would certainly improve the performance greatly.
  Such an approach would also offer the opportunity for dissipative materials,
  such as the magneto-viscoelastic one consider here, to be implemented in
  conjunction with AD. This is because the linearization invokes the total
  derivative of the dependent variables with respect to the field variables, which
  is exactly what the AD frameworks can provide.
- Investigate using other auto-differentiable number types and frameworks (such as
  ADOL-C). Since each AD library has its own implementation, the choice of which
  to use could result in performance increases and, in the most unfortunate cases,
  more stable computations. It can at least be said that for the AD libraries that
  deal.II supports, the accuracy of results should be largely unaffected by this decision.
- Embed one of these constitutive laws within a finite element simulation.

With less effort, one could think about re-writing nonlinear problem
solvers such as the one implemented in step-15 using AD or SD
approaches to compute the Newton matrix. Indeed, this is done in
step-72.


examples/step-72/doc/intro.dox
<br>

<i>This program was contributed by Jean-Paul Pelteret and Wolfgang Bangerth.

Wolfgang Bangerth's work is partially supported by National Science
Foundation grants OCI-1148116, OAC-1835673, DMS-1821210, and EAR-1925595;
and by the Computational Infrastructure in
Geodynamics initiative (CIG), through the National Science Foundation under
Award No. EAR-1550901 and The University of California-Davis.
</i>


<h1>Introduction</h1>

<h3>Motivation</h3>

This program solves the same problem as step-15, that is, it solves
for the
[minimal surface equation](https://en.wikipedia.org/wiki/Minimal_surface)
  @f{align*}
    F(u) \dealcoloneq -\nabla \cdot \left( \frac{1}{\sqrt{1+|\nabla u|^{2}}}\nabla u \right) &= 0 \qquad
    \qquad &&\textrm{in} ~ \Omega
    \\
    u&=g \qquad\qquad &&\textrm{on} ~ \partial \Omega.
  @f}

Among the issues we had identified there (see the
<a href="step_15#extensions">Possibilities for extensions</a> section)
was that when wanting to use
a Newton iteration, we needed to compute the derivative of the
residual of the equation with regard to the solution $u$ (here,
because the right hand side is zero, the residual is simply the left
hand side). For the equation we have here, this is cumbersome but not
impossible -- but one can easily imagine much more complicated
equations where just implementing the residual itself correctly is a
challenge, let alone doing so for the derivative necessary to compute
the Jacobian matrix. We will address this issue in this program: Using
the automatic differentiation techniques discussed in great detail in
step-71, we will come up with a way how we only have to implement the
residual and get the Jacobian for free.

In fact, we can even go one step further. While in step-15 we have
just taken the equation as a given, the minimal surface equation is
actually the product of minimizing an energy. Specifically,
the minimal surface equations are the Euler-Lagrange equations that
correspond to minimizing the energy
  @f[
    E(u) = \int_\Omega \Psi \left( u \right)
  @f]
where the *energy density* is given by
  @f[
    \Psi \left( u \right) = \sqrt{1+|\nabla u|^{2}}.
  @f]
This is the same as saying that we seek to find the stationary point of
the variation of the energy functional
  @f[
    \min\limits_{u} E \left( u \right)
      \quad \rightarrow \quad
      \delta E \left( u, \varphi \right) \dealcoloneq
      \left(\varphi, F(u)\right) = 0
      \qquad
      \forall \varphi,
  @f]
as this is where the equilibrium solution to the boundary value problem lies.

The key point then is that, maybe, we don't even need to implement the
residual, but that implementing the simpler energy density $\Psi(u)$
might actually be enough.

Our goal then is this: When
using a Newton iteration, we need to repeatedly solve the
linear partial differential equation
  @f{align*}
    F'(u^{n},\delta u^{n}) &=- F(u^{n})
  @f}
so that we can compute the update
  @f{align*}
    u^{n+1}&=u^{n}+\alpha^n \delta u^{n}
  @f}
with the solution $\delta u^{n}$ of the Newton step. As discussed in step-15,
we can compute the derivative $F'(u,\delta u)$ by hand and
obtain
  @f[
  F'(u,\delta u)
  =
  - \nabla \cdot \left( \frac{1}{\left(1+|\nabla u|^{2}\right)^{\frac{1}{2}}}\nabla
  \delta u \right) +
  \nabla \cdot \left( \frac{\nabla u \cdot
  \nabla \delta u}{\left(1+|\nabla u|^{2}\right)^{\frac{3}{2}}} \nabla u
  \right).
  @f]

So here then is what this program is about: It is about techniques
that can help us with computing $F'(u,\delta u)$ without having to
implement it explicitly, either by providing an implementation of
$F(u)$ or an implementation of $E(u)$. More precisely, we will
implement three different approaches and compare them in terms of
run-time but also -- maybe more importantly -- how much human effort
it takes to implement them:
- The method used in step-15 to form the Jacobian matrix.
- Computing the Jacobian matrix from an implementation of the
  residual $F(u)$, using automatic differentiation.
- Computing both the residual and Jacobian matrix from an
  implementation of the energy functional $E(u)$, also using automatic
  differentiation.

For the first of these methods, there are no conceptual changes
compared to step-15.


<h3> Computing the Jacobian from the residual </h3>

For the second method, let us outline how we will approach the issue
using automatic differentiation
to compute the linearization of the residual vector. To this end, let
us change notation for a moment and denote by $F(U)$ not the residual
of the differential equation, but in fact the *residual vector* --
i.e., the *discrete residual*. We do so because that is what we
*actually* do when we discretize the problem on a given mesh: We solve
the problem $F(U)=0$ where $U$ is the vector of unknowns.

More precisely, the $i$th component of the residual is given by
@f[
  F(U)_i \dealcoloneq
  \int\limits_{\Omega}\nabla \varphi_i \cdot \left[ \frac{1}{\sqrt{1+|\nabla
  u|^{2}}} \nabla u \right] \, dV ,
@f]
where $u(\mathbf x)=\sum_j U_j \varphi_j(\mathbf x)$. Given this, the
contribution for cell $K$ is
@f[
  F(U)_i^K \dealcoloneq
  \int\limits_K\nabla \varphi_i \cdot \left[ \frac{1}{\sqrt{1+|\nabla
  u|^{2}}} \nabla u \right] \, dV ,
@f]
Its first order Taylor expansion is given as
@f[
  F(U + \delta U)_i^K
  \approx F(U)_i^K
  + \sum_{j}^{n_{\textrm{dofs}}} \left[ \frac{\partial F(U)_i^K}{\partial
  U_j} \delta U_j \right],
@f]
and consequently we can compute the contribution of cell $K$ to the
Jacobian matrix $J$ as $J(U)_{ij}^K = \frac{\partial F(U)_i^K}{\partial U_j}$. The
important point here is that on cell $K$, we can express
@f[
  F(U)_i^K \dealcoloneq
  \int\limits_K\nabla \varphi_i \cdot \left[ \frac{1}{\sqrt{1+\left|
  \sum_{j'}^{n_\textrm{dofs}} U_{j'} \nabla \varphi_{j'}\right|^{2}}}
  \left(\sum_{j''}^{n_\textrm{dofs}} U_{j''} \nabla \varphi_{j''}\right)\right] \, dV.
@f]
For clarity, we have used $j'$ and $j''$ as counting indices to make
clear that they are distinct from each other and from $j$ above.
Because in this formula, $F(U)$ only depends on the coefficients
$U_j$, we can compute the derivative $J(U)_{ij}^K$ as a matrix via
automatic differentiation of $F(U)_i^K$. By the same argument as we
always use, it is clear that $F(U)^K$ does not actually depend on
*all* unknowns $U_j$, but only on those unknowns for which $j$ is a
shape function that lives on cell $K$, and so in practice, we restrict
$F(U)^K$ and $J(U)^K$ to that part of the vector and matrix that
corresponds to the *local* DoF indices, and then distribute from the
local cell $K$ to the global objects.

Using all of these realizations, the approach will then be to
implement $F(U)^K$ in the program and let the automatic differentiation
machinery compute the derivatives $J(U)^K$ from that.


<h3> Computing the Jacobian and the residual from the energy functional </h3>

For the final implementation of the assembly process, we will move a level
higher than the residual: our entire linear system will be determined
directly from the energy functional that governs the physics of this
boundary value problem. We can take advantage of the fact that we can
calculate the total energy in the domain directly from the local
contributions, i.e.,
@f[
  E \left( U \right) \dealcoloneq \int\limits_{\Omega} \Psi \left( u
  \right) \, dV .
@f]
In the discrete setting, this means that on each finite element we have
@f[
   E \left( U \right)^K
    \dealcoloneq \int\limits_{K} \Psi \left( u \right) \, dV
    \approx \sum\limits_{q}^{n_{\textrm{q-points}}} \Psi \left( u \left(
    \mathbf{x}_{q} \right) \right) \underbrace{\vert J_{q} \vert \times W_{q}}_{\text{JxW(q)}} .
@f]
If we implement the cell energy, which depends on the field solution,
we can compute its first (discrete) variation
@f[
  F(U)^K_i
    = \frac{\partial E(U)^K}{\partial U_i}
@f]
and, thereafter, its second (discrete) variation
@f[
  J(U)^K_{ij}
    = \frac{\partial^{2}  E(U)^K}{\partial U_i \partial U_j}.
@f]
So, from the cell contribution to the total energy function, we may expect
to have the approximate residual and tangent contributions generated
for us as long as we can provide an implementation of the local energy
$E(U)^K$. Again, due to the design of the
automatic differentiation variables used in this tutorial, in practice
these approximations for the contributions to the residual vector and
tangent matrix are actually accurate to machine precision.


examples/step-72/doc/results.dox
<h1>Results</h1>

Since there was no change to the physics of the problem that has first been analyzed
in step-15, there is nothing to report about that. The only outwardly noticeable
difference between them is that, by default, this program will only run 9 mesh
refinement steps (as opposed to step-15, which executes 11 refinements).
This will be observable in the simulation status that appears between the
header text that prints which assembly method is being used, and the final
timings. (All timings reported below were obtained in release mode.)

@code
Mesh refinement step 0
  Initial residual: 1.53143
  Residual: 1.08746
  Residual: 0.966748
  Residual: 0.859602
  Residual: 0.766462
  Residual: 0.685475

...

Mesh refinement step 9
  Initial residual: 0.00924594
  Residual: 0.00831928
  Residual: 0.0074859
  Residual: 0.0067363
  Residual: 0.00606197
  Residual: 0.00545529
@endcode

So what is interesting for us to compare is how long the assembly process takes
for the three different implementations, and to put that into some greater context.
Below is the output for the hand linearization (as computed on a circa 2012
four core, eight thread laptop -- but we're only really interested in the
relative time between the different implementations):
@code
******** Assembly approach ********
Unassisted implementation (full hand linearization).

...

+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |      35.1s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble                        |        50 |      1.56s |       4.5% |
| Solve                           |        50 |      30.8s |        88% |
+---------------------------------+-----------+------------+------------+
@endcode
And for the implementation that linearizes the residual in an automated
manner using the Sacado dynamic forward AD number type:
@code
******** Assembly approach ********
Automated linearization of the finite element residual.

...

+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |      40.1s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble                        |        50 |       8.8s |        22% |
| Solve                           |        50 |      28.6s |        71% |
+---------------------------------+-----------+------------+------------+
@endcode
And, lastly, for the implementation that computes both the residual and
its linearization directly from an energy functional (using nested Sacado
dynamic forward AD numbers):
@code
******** Assembly approach ********
Automated computation of finite element residual and linearization using a variational formulation.

...

+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |      48.8s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| Assemble                        |        50 |      16.7s |        34% |
| Solve                           |        50 |      29.3s |        60% |
+---------------------------------+-----------+------------+------------+
@endcode

It's evident that the more work that is passed off to the automatic differentiation
framework to perform, the more time is spent during the assembly process. Accumulated
over all refinement steps, using one level of automatic differentiation resulted
in $5.65 \times$ more computational time being spent in the assembly stage when
compared to unassisted assembly, while assembling the discrete linear system took
$10.7 \times$ longer when deriving directly from the energy functional.
Unsurprisingly, the overall time spent solving the linear system remained unchanged.
This means that the proportion of time spent in the solve phase to the assembly phase
shifted significantly as the number of times automated differentiation was performed
at the finite element level. For many, this might mean that leveraging higher-order
differentiation (at the finite element level) in production code leads to an
unacceptable overhead, but it may still be useful during the prototyping phase.
A good compromise between the two may, therefore, be the automated linearization
of the finite element residual, which offers a lot of convenience at a measurable,
but perhaps not unacceptable, cost. Alternatively, one could consider
not re-building the Newton matrix in every step -- a topic that is
explored in substantial depth in step-77.

Of course, in practice the actual overhead is very much dependent on the problem being evaluated
(e.g., how many components there are in the solution, what the nature of the function
being differentiated is, etc.). So the exact results presented here should be
interpreted within the context of this scalar problem alone, and when it comes to
other problems, some preliminary investigation by the user is certainly warranted.


<h3> Possibilities for extensions </h3>

Like step-71, there are a few items related to automatic differentiation that could
be evaluated further:
- The use of other AD frameworks should be investigated, with the outlook that
  alternative implementations may provide performance benefits.
- It is also worth evaluating AD number types other than those that have been
  hard-coded into this tutorial. With regard to twice differentiable types
  employed at the finite-element level, mixed differentiation modes ("RAD-FAD")
  should in principle be more computationally efficient than the single
  mode ("FAD-FAD") types employed here. The reason that the RAD-FAD type was not
  selected by default is that, at the time of writing, there remain some
  bugs in its implementation within the Sacado library that lead to memory leaks.
  This is documented in the @ref auto_symb_diff module.
- It might be the case that using reduced precision types (i.e., `float`) as the
  scalar types for the AD numbers could render a reduction in computational
  expense during assembly. Using `float` as the data type for the
  matrix and the residual is not unreasonable, given that the Newton
  update is only meant to get us closer to the solution, but not
  actually *to* the solution; as a consequence, it makes sense to
  consider using reduced-precision data types for computing these
  updates, and then accumulating these updates in a solution vector
  that uses the full `double` precision accuracy.
- One further method of possibly reducing resources during assembly is to frame
  the AD implementations as a constitutive model. This would be similar to the
  approach adopted in step-71, and pushes the starting point for the automatic
  differentiation one level higher up the chain of computations. This, in turn,
  means that less operations are tracked by the AD library, thereby reducing the
  cost of differentiating (even though one would perform the differentiation at
  each cell quadrature point).
- step-77 is yet another variation of step-15 that addresses a very
  different part of the problem: Line search and whether it is
  necessary to re-build the Newton matrix in every nonlinear
  iteration. Given that the results above show that using automatic
  differentiation comes at a cost, the techniques in step-77 have the
  potential to offset these costs to some degree. It would therefore
  be quite interesting to combine the current program with the
  techniques in step-77. For production codes, this would certainly be
  the way to go.


examples/step-74/doc/intro.dox
<br>

<i>
This program was contributed by Timo Heister and Jiaqi Zhang.
<br>
This material is based upon work partly supported by the National
Science Foundation Award DMS-2028346, OAC-2015848, EAR-1925575, by the Computational
Infrastructure in Geodynamics initiative (CIG), through the NSF under Award
EAR-0949446 and EAR-1550901 and The University of California -- Davis.
</i>


<a name="Intro"></a>
<h1><em>Symmetric interior penalty Galerkin</em> (SIPG) method for Poisson's equation</h1>

<h3>Overview</h3>
In this tutorial, we display the usage of the FEInterfaceValues class,
which is designed for assembling face terms arising from discontinuous Galerkin (DG) methods.
The FEInterfaceValues class provides an easy way to obtain the jump
and the average of shape functions and of the solution across cell faces.
This tutorial includes the following topics.
<ol>
  <li> The SIPG method for Poisson's equation, which has already been used in step-39 and step-59.
  <li> Assembling of face terms using FEInterfaceValues and the system matrix using MeshWorker::mesh_loop(), which is similar to step-12.
  <li> Adaptive mesh refinement using an error estimator.
  <li> Two test cases: convergence test for a smooth function and adaptive mesh refinement test for a singular solution.
</ol>

<h3>The equation</h3>
In this example, we consider Poisson's equation
@f[
- \nabla \cdot \left( \nu  \nabla u\right) = f  \qquad   \mbox{in } \Omega,
@f]
subject to the boundary condition
@f[
u = g_D \qquad \mbox{on } \partial \Omega.
@f]
For simplicity, we assume that the diffusion coefficient $\nu$ is constant here.
Note that if $\nu$ is discontinuous, we need to take this into account when computing jump terms
on cell faces.

We denote the mesh by ${\mathbb T}_h$, and $K\in{\mathbb T}_h$ is a mesh cell.
The sets of interior and boundary faces are denoted by ${\mathbb F}^i_h$ and ${\mathbb F}^b_h$
respectively. Let $K^0$ and $K^1$ be the two cells sharing a face $f\in F_h^i$,
and $\mathbf n$ be the outer normal vector of $K^0$. Then the jump
operator is given by the "here minus there" formula,
@f[
\jump{v} = v^0 - v^1
@f]
and the averaging operator as
@f[
\average{v} = \frac{v^0 + v^1}{2}
@f]
respectively. Note that when $f\subset \partial \Omega$, we define $\jump{v} = v$ and
$\average{v}=v$.
The discretization using the SIPG is given by the following weak formula
(more details can be found in @cite di2011mathematical and the references therein)
@f{align*}
&\sum_{K\in {\mathbb T}_h} (\nabla v_h, \nu \nabla u_h)_K\\
&-\sum_{F \in F_h^i} \left\{
    \left< \jump{v_h}, \nu\average{ \nabla u_h} \cdot  \mathbf n \right>_F
   +\left<\average{ \nabla v_h }\cdot \mathbf n,\nu\jump{u_h}\right>_F
   -\left<\jump{v_h},\nu \sigma \jump{u_h} \right>_F
  \right\}\\
&-\sum_{F \in F_h^b} \left\{
    \left<v_h, \nu  \nabla u_h\cdot \mathbf n \right>_F
  + \left< \nabla v_h \cdot \mathbf n , \nu u_h\right>_F
  - \left< v_h,\nu \sigma u_h\right>_F
  \right\}\\
&=(v_h, f)_\Omega
  - \sum_{F \in F_h^b} \left\{
    \left< \nabla v_h \cdot \mathbf n, \nu g_D\right>_F - \left<v_h,\nu \sigma g_D\right>_F
  \right\}.
@f}


<h3>The penalty parameter</h3>
The penalty parameter is defined as $\sigma = \gamma/h_f$, where $h_f$ a local length scale associated
with the cell face; here we choose an approximation of the length of the cell in the direction normal to the face:
$\frac 1{h_f} = \frac 12 \left(\frac 1{h_K} + \frac 1{h_{K'}}\right)$,
where $K,K'$ are the two cells adjacent to the face $f$ and we we
compute $h_K = \frac{|K|}{|f|}$.

In the formula above, $\gamma$ is the penalization constant.
To ensure the discrete coercivity, the penalization constant has to be large enough @cite ainsworth2007posteriori.
People do not really have consensus on which of the formulas proposed
in the literature should be used. (This is similar to the situation
discussed in the "Results" section of step-47.)
One can just pick a large constant, while other options could be the multiples of $(p+1)^2$ or $p(p+1)$. In this code,
we follow step-39 and use $\gamma = p(p+1)$.


<h3>A posteriori error estimator</h3>
In this example, with a slight modification, we use the error estimator by Karakashian and Pascal @cite karakashian2003posteriori
@f[
\eta^2 = \sum_{K \in {\mathbb T}_h} \eta^2_{K} +  \sum_{f_i \in {\mathbb F}^i_h}  \eta^2_{f_i} + \sum_{f_b \in F^i_b}\eta^2_{f_b}
@f]
where
@f{align*}{
\eta^2_{K} &= h_K^2 \left\| f + \nu \Delta u_h \right\|_K^2,
\\
\eta^2_{f_i} &= \sigma \left\| \jump{u_h}  \right\|_f^2   +  h_f \left\|  \jump{\nu \nabla u_h} \cdot \mathbf n   \right\|_f^2,
\\
\eta_{f_b}^2 &=  \sigma \left\| u_h-g_D \right\|_f^2.
@f}
Here we use $\sigma = \gamma/h_f$ instead of $\gamma^2/h_f$ for the jump terms of $u_h$ (the first term in $\eta^2_{f_i}$ and $\eta_{f_b}^2$).

In order to compute this estimator, in each cell $K$ we compute
@f{align*}{
\eta_{c}^2 &= h_K^2 \left\| f + \nu \Delta u_h \right\|_K^2,
\\
\eta_{f}^2 &= \sum_{f\in \partial K}\lbrace \sigma \left\| \jump{u_h}  \right\|_f^2   +  h_f \left\|  \jump{\nu \nabla u_h} \cdot \mathbf n  \right\|_f^2 \rbrace,
\\
\eta_{b}^2 &= \sum_{f\in \partial K \cap \partial \Omega}  \sigma \left\| (u_h -g_D)  \right\|_f^2.
@f}
Then the square of the error estimate per cell is
@f[
\eta_\text{local}^2 =\eta_{c}^2+0.5\eta_{f}^2+\eta_{b}^2.
@f]
The factor of $0.5$ results from the fact that the overall error
estimator includes each interior face only once, and so the estimators per cell
count it with a factor of one half for each of the two adjacent cells.
Note that we compute $\eta_\text{local}^2$ instead of $\eta_\text{local}$ to simplify the implementation.
The error estimate square per cell is then stored in a global vector, whose $l_1$ norm is equal to $\eta^2$.

<h3>The test case</h3>
In the first test problem, we run a convergence test using a smooth manufactured solution with $\nu =1$ in 2D
@f{align*}{
u&=\sin(2\pi x)\sin(2\pi y), &\qquad\qquad &(x,y)\in\Omega=(0,1)\times (0,1),
\\
u&=0,                        &\qquad\qquad &\text{on } \partial \Omega,
@f}
and $f= 8\pi^2 u$. We compute errors against the manufactured solution and evaluate the convergence rate.

In the second test, we choose Functions::LSingularityFunction on a L-shaped domain (GridGenerator::hyper_L) in 2D.
The solution is given in the polar coordinates by $u(r,\phi) = r^{\frac{2}{3}}\sin \left(\frac{2}{3}\phi \right)$,
which has a singularity at the origin. An error estimator is constructed to detect the region with large errors,
according to which the mesh is refined adaptively.


examples/step-74/doc/results.dox
<h1>Results</h1>

The output of this program consist of the console output and
solutions in vtu format.

In the first test case, when you run the program, the screen output should look like the following:
@code
Cycle 0
  Number of active cells       : 16
  Number of degrees of freedom : 256
  Error in the L2 norm         : 0.00193285
  Error in the H1 seminorm     : 0.106087
  Error in the energy norm     : 0.150625

Cycle 1
  Number of active cells       : 64
  Number of degrees of freedom : 1024
  Error in the L2 norm         : 9.60497e-05
  Error in the H1 seminorm     : 0.0089954
  Error in the energy norm     : 0.0113265

Cycle 2
.
.
.
@endcode

When using the smooth case with polynomial degree 3, the convergence
table will look like this:
<table align="center" class="doxtable">
  <tr>
    <th>cycle</th>
    <th>n_cellss</th>
    <th>n_dofs</th>
    <th>L2 </th>
    <th>rate</th>
    <th>H1</th>
    <th>rate</th>
    <th>Energy</th>
  </tr>
  <tr>
    <td align="center">0</td>
    <td align="right">16</td>
    <td align="right">256</td>
    <td align="center">1.933e-03</td>
    <td>&nbsp;</td>
    <td align="center">1.061e-01</td>
    <td>&nbsp;</td>
    <td align="center">1.506e-01</td>
  </tr>
  <tr>
    <td align="center">1</td>
    <td align="right">64</td>
    <td align="right">1024</td>
    <td align="center">9.605e-05</td>
    <td align="center">4.33</td>
    <td align="center">8.995e-03</td>
    <td align="center">3.56</td>
    <td align="center">1.133e-02</td>
  </tr>
  <tr>
    <td align="center">2</td>
    <td align="right">256</td>
    <td align="right">4096</td>
    <td align="center">5.606e-06</td>
    <td align="center">4.10</td>
    <td align="center">9.018e-04</td>
    <td align="center">3.32</td>
    <td align="center">9.736e-04</td>
  </tr>
  <tr>
    <td align="center">3</td>
    <td align="right">1024</td>
    <td align="right">16384</td>
    <td align="center">3.484e-07</td>
    <td align="center">4.01</td>
    <td align="center">1.071e-04</td>
    <td align="center">3.07</td>
    <td align="center">1.088e-04</td>
  </tr>
  <tr>
    <td align="center">4</td>
    <td align="right">4096</td>
    <td align="right">65536</td>
    <td align="center">2.179e-08</td>
    <td align="center">4.00</td>
    <td align="center">1.327e-05</td>
    <td align="center">3.01</td>
    <td align="center">1.331e-05</td>
  </tr>
  <tr>
    <td align="center">5</td>
    <td align="right">16384</td>
    <td align="right">262144</td>
    <td align="center">1.363e-09</td>
    <td align="center">4.00</td>
    <td align="center">1.656e-06</td>
    <td align="center">3.00</td>
    <td align="center">1.657e-06</td>
  </tr>
</table>

Theoretically, for polynomial degree $p$, the order of convergence in $L_2$
norm and $H^1$ seminorm should be $p+1$ and $p$, respectively. Our numerical
results are in good agreement with theory.

In the second test case, when you run the program, the screen output should look like the following:
@code
Cycle 0
  Number of active cells       : 192
  Number of degrees of freedom : 3072
  Error in the L2 norm         : 0.000323585
  Error in the H1 seminorm     : 0.0296202
  Error in the energy norm     : 0.0420478
  Estimated error              : 0.136067

Cycle 1
  Number of active cells       : 249
  Number of degrees of freedom : 3984
  Error in the L2 norm         : 0.000114739
  Error in the H1 seminorm     : 0.0186571
  Error in the energy norm     : 0.0264879
  Estimated error              : 0.0857186

Cycle 2
.
.
.
@endcode

The following figure provides a log-log plot of the errors versus
the number of degrees of freedom for this test case on the L-shaped
domain. In order to interpret it, let $n$ be the number of degrees of
freedom, then on uniformly refined meshes, $h$ is of order
$1/\sqrt{n}$ in 2D. Combining the theoretical results in the previous case,
we see that if the solution is sufficiently smooth,
we can expect the error in the $L_2$ norm to be of order $O(n^{-\frac{p+1}{2}})$
and in $H^1$ seminorm to be $O(n^{-\frac{p}{2}})$. It is not a priori
clear that one would get the same kind of behavior as a function of
$n$ on adaptively refined meshes like the ones we use for this second
test case, but one can certainly hope. Indeed, from the figure, we see
that the SIPG with adaptive mesh refinement produces asymptotically
the kinds of hoped-for results:

<img width="600px" src="https://www.dealii.org/images/steps/developer/step-74.log-log-plot.png" alt="">

In addition, we observe that the error estimator decreases
at almost the same rate as the errors in the energy norm and $H^1$ seminorm,
and one order lower than the $L_2$ error. This suggests
its ability to predict regions with large errors.

While this tutorial is focused on the implementation, the step-59 tutorial program achieves an efficient
large-scale solver in terms of computing time with matrix-free solution techniques.
Note that the step-59 tutorial does not work with meshes containing hanging nodes at this moment,
because the multigrid interface matrices are not as easily determined,
but that is merely the lack of some interfaces in deal.II, nothing fundamental.


examples/step-75/doc/intro.dox
<br>

<i>This program was contributed by Marc Fehling, Peter Munch and
Wolfgang Bangerth.
<br>
This material is based upon work partly supported by the National
Science Foundation under Award No. DMS-1821210, EAR-1550901, and
OAC-1835673. Any opinions, findings, and conclusions or recommendations
expressed in this publication are those of the authors and do not
necessarily reflect the views of the National Science Foundation.
<br>
Peter Munch would like to thank Timo Heister, Martin Kronbichler, and
Laura Prieto Saavedra for many very interesting discussions.
</i>


@note As a prerequisite of this program, you need to have the p4est
library and the Trilinos library installed. The installation of deal.II
together with these additional libraries is described in the <a
href="../../readme.html" target="body">README</a> file.



<a name="Intro"></a>
<h1>Introduction</h1>

In the finite element context, more degrees of freedom usually yield a
more accurate solution but also require more computational effort.

Throughout previous tutorials, we found ways to effectively distribute
degrees of freedom by aligning the grid resolution locally with the
complexity of the solution (adaptive mesh refinement, step-6). This
approach is particularly effective if we do not only adapt the grid
alone, but also locally adjust the polynomial degree of the associated
finite element on each cell (hp-adaptation, step-27).

In addition, assigning more processes to run your program simultaneously
helps to tackle the computational workload in lesser time. Depending on
the hardware architecture of your machine, your program must either be
prepared for the case that all processes have access to the same memory
(shared memory, step-18), or that processes are hosted on several
independent nodes (distributed memory, step-40).

In the high-performance computing segment, memory access turns out to be
the current bottleneck on supercomputers. We can avoid storing matrices
altogether by computing the effect of matrix-vector products on the fly
with MatrixFree methods (step-37). They can be used for geometric
multigrid methods (step-50) and also for polynomial multigrid methods to
speed solving the system of equation tremendously.

This tutorial combines all of these particularities and presents a
state-of-the-art way how to solve a simple Laplace problem: utilizing
both hp-adaptation and matrix-free hybrid multigrid methods on machines
with distributed memory.


<h3>Load balancing</h3>

For parallel applications in FEM, we partition the grid into
subdomains (aka domain decomposition), which are assigned to processes.
This partitioning happens on active cells in deal.II as demonstrated in
step-40. There, each cell has the same finite element and the same
number of degrees of freedom assigned, and approximately the same
workload. To balance the workload among all processes, we have to
balance the number of cells on all participating processes.

With hp-adaptive methods, this is no longer the case: the finite element
type may vary from cell to cell and consequently also the number of
degrees of freedom. Matching the number of cells does not yield a
balanced workload. In the matrix-free context, the workload can be
assumed to be proportional the number of degrees of freedom of each
process, since in the best case only the source and the destination
vector have to be loaded.

One could balance the workload by assigning weights to every cell which
are proportional to the number of degrees of freedom, and balance the
sum of all weights between all processes. Assigning individual weights
to each cell can be realized with the class parallel::CellWeights which
we will use later.


<h3>hp-decision indicators</h3>

With hp-adaptive methods, we not only have to decide which cells we want
to refine or coarsen, but we also have the choice how we want to do
that: either by adjusting the grid resolution or the polynomial degree
of the finite element.

We will again base the decision on which cells to adapt on (a
posteriori) computed error estimates of the current solution, e.g.,
using the KellyErrorEstimator. We will similarly decide how to adapt
with (a posteriori) computed smoothness estimates: large polynomial
degrees work best on smooth parts of the solution while fine grid
resolutions are favorable on irregular parts. In step-27, we presented a
way to calculate smoothness estimates based on the decay of Fourier
coefficients. Let us take here the opportunity and present an
alternative that follows the same idea, but with Legendre coefficients.

We will briefly present the idea of this new technique, but limit its
description to 1D for simplicity. Suppose $u_\text{hp}(x)$ is a finite
element function defined on a cell $K$ as
@f[
u_\text{hp}(x) = \sum c_i \varphi_i(x)
@f]
where each $\varphi_i(x)$ is a shape function.
We can equivalently represent $u_\text{hp}(x)$ in the basis of Legendre
polynomials $P_k$ as
@f[
u_\text{hp}(x) = \sum l_k P_k(x).
@f]
Our goal is to obtain a mapping between the finite element coefficients
$c_i$ and the Legendre coefficients $l_k$. We will accomplish this by
writing the problem as a $L^2$-projection of $u_\text{hp}(x)$ onto the
Legendre basis. Each coefficient $l_k$ can be calculated via
@f[
l_k = \int_K u_\text{hp}(x) P_k(x) dx.
@f]
By construction, the Legendre polynomials are orthogonal under the
$L^2$-inner product on $K$. Additionally, we assume that they have been
normalized, so their inner products can be written as
@f[
\int_K P_i(x) P_j(x) dx = \det(J_K) \, \delta_{ij}
@f]
where $\delta_{ij}$ is the Kronecker delta, and $J_K$ is the Jacobian of
the mapping from $\hat{K}$ to $K$, which (in this tutorial) is assumed
to be constant (i.e., the mapping must be affine).

Hence, combining all these assumptions, the projection matrix for
expressing $u_\text{hp}(x)$ in the Legendre basis is just $\det(J_K) \,
\mathbb{I}$ -- that is, $\det(J_K)$ times the identity matrix. Let $F_K$
be the Mapping from $K$ to its reference cell $\hat{K}$. The entries in
the right-hand side in the projection system are, therefore,
@f[
\int_K u_\text{hp}(x) P_k(x) dx
= \det(J_K) \int_\hat{K} u_\text{hp}(F_K(\hat{x})) P_k(F_K(\hat{x})) d\hat{x}.
@f]
Recalling the shape function representation of $u_\text{hp}(x)$, we can
write this as $\det(J_K) \, \mathbf{C} \, \mathbf{c}$, where
$\mathbf{C}$ is the change-of-basis matrix with entries
@f[
\int_K P_i(x) \varphi_j(x) dx
= \det(J_K) \int_{\hat{K}} P_i(F_K(\hat{x})) \varphi_j(F_K(\hat{x})) d\hat{x}
= \det(J_K) \int_{\hat{K}} \hat{P}_i(\hat{x}) \hat{\varphi}_j(\hat{x}) d\hat{x}
\dealcoloneq \det(J_K) \, C_{ij}
@f]
so the values of $\mathbf{C}$ can be written <em>independently</em> of
$K$ by factoring $\det(J_K)$ out front after transforming to reference
coordinates. Hence, putting it all together, the projection problem can
be written as
@f[
\det(J_K) \, \mathbb{I} \, \mathbf{l} = \det(J_K) \, \mathbf{C} \, \mathbf{c}
@f]
which can be rewritten as simply
@f[
\mathbf{l} = \mathbf{C} \, \mathbf{c}.
@f]

At this point, we need to emphasize that most finite element
applications use unstructured meshes for which mapping is almost always
non-affine. Put another way: the assumption that $J_K$ is constant
across the cell is not true for general meshes. Hence, a correct
calculation of $l_k$ requires not only that we calculate the
corresponding transformation matrix $\mathbf{C}$ for every single cell,
but that we also define a set of Legendre-like orthogonal functions on a
cell $K$ which may have an arbitrary and very complex geometry. The
second part, in particular, is very computationally expensive. The
current implementation of the FESeries transformation classes relies on
the simplification resulting from having a constant Jacobian to increase
performance and thus only yields correct results for affine mappings.
The transformation is only used for the purpose of smoothness estimation
to decide on the type of adaptation, which is not a critical component
of a finite element program. Apart from that, this circumstance does not
pose a problem for this tutorial as we only use square-shaped cells.

Eibner and Melenk @cite eibner2007hp argued that a function is analytic,
i.e., representable by a power series, if and only if the absolute
values of the Legendre coefficients decay exponentially with increasing
index $k$:
@f[
\exists C,\sigma > 0 : \quad \forall k \in \mathbb{N}_0 : \quad |l_k|
\leq C \exp\left( - \sigma k \right) .
@f]
The rate of decay $\sigma$ can be interpreted as a measure for the
smoothness of that function. We can get it as the slope of a linear
regression fit of the transformation coefficients:
@f[
\ln(|l_k|) \sim \ln(C) - \sigma k .
@f]

We will perform this fit on each cell $K$ to get a local estimate for
the smoothness of the finite element approximation. The decay rate
$\sigma_K$ then acts as the decision indicator for hp-adaptation. For a
finite element on a cell $K$ with a polynomial degree $p$, calculating
the coefficients for $k \leq (p+1)$ proved to be a reasonable choice to
estimate smoothness. You can find a more detailed and dimension
independent description in @cite fehling2020.

All of the above is already implemented in the FESeries::Legendre class
and the SmoothnessEstimator::Legendre namespace. With the error
estimates and smoothness indicators, we are then left to flag the cells
for actual refinement and coarsening. Some functions from the
parallel::distributed::GridRefinement and hp::Refinement namespaces will
help us with that later.


<h3>Hybrid geometric multigrid</h3>

Finite element matrices are typically very sparse. Additionally,
hp-adaptive methods correspond to matrices with highly variable numbers
of nonzero entries per row. Some state-of-the-art preconditioners, like
the algebraic multigrid (AMG) ones as used in step-40, behave poorly in
these circumstances.

We will thus rely on a matrix-free hybrid multigrid preconditioner.
step-50 has already demonstrated the superiority of geometric multigrid
methods method when combined with the MatrixFree framework. The
application on hp-adaptive FEM requires some additional work though
since the children of a cell might have different polynomial degrees. As
a remedy, we perform a p-relaxation to linear elements first (similar to
Mitchell @cite mitchell2010hpmg) and then perform h-relaxation in the
usual manner. On the coarsest level, we apply an algebraic multigrid
solver. The combination of p-multigrid, h-multigrid, and AMG makes the
solver to a hybrid multigrid solver.

We will create a custom hybrid multigrid preconditioner with the special
level requirements as described above with the help of the existing
global-coarsening infrastructure via the use of
MGTransferGlobalCoarsening.


<h3>The test case</h3>

For elliptic equations, each reentrant corner typically invokes a
singularity @cite brenner2008. We can use this circumstance to put our
hp-decision algorithms to a test: on all cells to be adapted, we would
prefer a fine grid near the singularity, and a high polynomial degree
otherwise.

As the simplest elliptic problem to solve under these conditions, we
chose the Laplace equation in a L-shaped domain with the reentrant
corner in the origin of the coordinate system.

To be able to determine the actual error, we manufacture a boundary
value problem with a known solution. On the above mentioned domain, one
solution to the Laplace equation is, in polar coordinates,
$(r, \varphi)$:
@f[
u_\text{sol} = r^{2/3} \sin(2/3 \varphi).
@f]

See also @cite brenner2008 or @cite mitchell2014hp. The solution looks as follows:

<div style="text-align:center;">
  <img src="https://www.dealii.org/images/steps/developer/step-75.solution.svg"
       alt="Analytic solution.">
</div>

The singularity becomes obvious by investigating the solution's gradient
in the vicinity of the reentrant corner, i.e., the origin
@f[
\left\| \nabla u_\text{sol} \right\|_{2} = 2/3 r^{-1/3} , \quad
\lim\limits_{r \rightarrow 0} \left\| \nabla u_\text{sol} \right\|_{2} =
\infty .
@f]

As we know where the singularity will be located, we expect that our
hp-decision algorithm decides for a fine grid resolution in this
particular region, and high polynomial degree anywhere else.

So let's see if that is actually the case, and how hp-adaptation
performs compared to pure h-adaptation. But first let us have a detailed
look at the actual code.


examples/step-75/doc/results.dox
<h1>Results</h1>

When you run the program with the given parameters on four processes in
release mode, your terminal output should look like this:
@code
Running with Trilinos on 4 MPI rank(s)...
Calculating transformation matrices...
Cycle 0:
   Number of active cells:       3072
     by partition:               768 768 768 768
   Number of degrees of freedom: 12545
     by partition:               3201 3104 3136 3104
   Number of constraints:        542
     by partition:               165 74 138 165
   Frequencies of poly. degrees: 2:3072
   Solved in 7 iterations.


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |     0.598s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| calculate transformation        |         1 |    0.0533s |       8.9% |
| compute indicators              |         1 |    0.0177s |         3% |
| initialize grid                 |         1 |    0.0397s |       6.6% |
| output results                  |         1 |    0.0844s |        14% |
| setup system                    |         1 |    0.0351s |       5.9% |
| solve system                    |         1 |     0.362s |        61% |
+---------------------------------+-----------+------------+------------+


Cycle 1:
   Number of active cells:       3351
     by partition:               875 761 843 872
   Number of degrees of freedom: 18223
     by partition:               4535 4735 4543 4410
   Number of constraints:        1202
     by partition:               303 290 326 283
   Frequencies of poly. degrees: 2:2523 3:828
   Solved in 7 iterations.


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |     0.442s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| adapt resolution                |         1 |    0.0189s |       4.3% |
| compute indicators              |         1 |    0.0135s |         3% |
| output results                  |         1 |     0.064s |        14% |
| setup system                    |         1 |    0.0232s |       5.2% |
| solve system                    |         1 |     0.322s |        73% |
+---------------------------------+-----------+------------+------------+


...


Cycle 7:
   Number of active cells:       5610
     by partition:               1324 1483 1482 1321
   Number of degrees of freedom: 82062
     by partition:               21116 19951 20113 20882
   Number of constraints:        14383
     by partition:               3825 3225 3557 3776
   Frequencies of poly. degrees: 2:1130 3:1283 4:2727 5:465 6:5
   Solved in 7 iterations.


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |     0.932s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| adapt resolution                |         1 |    0.0182s |       1.9% |
| compute indicators              |         1 |    0.0173s |       1.9% |
| output results                  |         1 |    0.0572s |       6.1% |
| setup system                    |         1 |    0.0252s |       2.7% |
| solve system                    |         1 |     0.813s |        87% |
+---------------------------------+-----------+------------+------------+
@endcode

When running the code with more processes, you will notice slight
differences in the number of active cells and degrees of freedom. This
is due to the fact that solver and preconditioner depend on the
partitioning of the problem, which might yield to slight differences of
the solution in the last digits and ultimately yields to different
adaptation behavior.

Furthermore, the number of iterations for the solver stays about the
same in all cycles despite hp-adaptation, indicating the robustness of
the proposed algorithms and promising good scalability for even larger
problem sizes and on more processes.

Let us have a look at the graphical output of the program. After all
refinement cycles in the given parameter configuration, the actual
discretized function space looks like the following with its
partitioning on twelve processes on the left and the polynomial degrees
of finite elements on the right. In the left picture, each color
represents a unique subdomain. In the right picture, the lightest color
corresponds to the polynomial degree two and the darkest one corresponds
to degree six:

<div class="twocolumn" style="width: 80%; text-align: center;">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-75.subdomains-07.svg"
         alt="Partitioning after seven refinements.">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-75.fedegrees-07.svg"
         alt="Local approximation degrees after seven refinements.">
  </div>
</div>



<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

<h4>Different hp-decision strategies</h4>

The deal.II library offers multiple strategies to decide which type of
adaptation to impose on cells: either adjust the grid resolution or
change the polynomial degree. We only presented the <i>Legendre
coefficient decay</i> strategy in this tutorial, while step-27
demonstrated the <i>Fourier</i> equivalent of the same idea.

See the "possibilities for extensions" section of step-27 for an
overview over these strategies, or the corresponding documentation
for a detailed description.

There, another strategy is mentioned that has not been shown in any
tutorial so far: the strategy based on <i>refinement history</i>. The
usage of this method for parallel distributed applications is more
tricky than the others, so we will highlight the challenges that come
along with it. We need information about the final state of refinement
flags, and we need to transfer the solution across refined meshes. For
the former, we need to attach the hp::Refinement::predict_error()
function to the Triangulation::Signals::post_p4est_refinement signal in
a way that it will be called <i>after</i> the
hp::Refinement::limit_p_level_difference() function. At this stage, all
refinement flags and future FE indices are terminally set and a reliable
prediction of the error is possible. The predicted error then needs to
be transferred across refined meshes with the aid of
parallel::distributed::CellDataTransfer.

Try implementing one of these strategies into this tutorial and observe
the subtle changes to the results. You will notice that all strategies
are capable of identifying the singularities near the reentrant corners
and will perform $h$-refinement in these regions, while preferring
$p$-refinement in the bulk domain. A detailed comparison of these
strategies is presented in @cite fehling2020 .


<h4>Solve with matrix-based methods</h4>

This tutorial focuses solely on matrix-free strategies. All hp-adaptive
algorithms however also work with matrix-based approaches in the
parallel distributed context.

To create a system matrix, you can either use the
LaplaceOperator::get_system_matrix() function, or use an
<code>assemble_system()</code> function similar to the one of step-27.
You can then pass the system matrix to the solver as usual.

You can time the results of both matrix-based and matrix-free
implementations, quantify the speed-up, and convince yourself which
variant is faster.


<h4>Multigrid variants</h4>

For sake of simplicity, we have restricted ourselves to a single type of
coarse-grid solver (CG with AMG), smoother (Chebyshev smoother with
point Jacobi preconditioner), and geometric-coarsening scheme (global
coarsening) within the multigrid algorithm. Feel free to try out
alternatives and investigate their performance and robustness.


examples/step-76/doc/intro.dox

<br>

<i>
This program was contributed by Martin Kronbichler, Peter Munch, and David
Schneider. Many of the features shown here have been added to deal.II during
and for the development of the deal.II-based, efficient, matrix-free
finite-element library for high-dimensional partial differential equations
hyper.deal (see https://github.com/hyperdeal/hyperdeal). For more details and
for applications of the presented features in slightly different contexts
(high-dimensional advection equation and Vlasov-Poisson equations) see the release
paper @cite munch2020hyperdeal.

This work was partly supported by the German Research Foundation (DFG) through
the project "High-order discontinuous Galerkin for the exa-scale" (ExaDG)
within the priority program "Software for Exascale Computing" (SPPEXA) and
by the Bavarian government through the project "High-order matrix-free finite
element implementations with hybrid parallelization and improved data locality"
within the KONWIHR program.
</i>

<a name="Intro"></a>
<h1>Introduction</h1>

This tutorial program solves the Euler equations of fluid dynamics, using an
explicit time integrator with the matrix-free framework applied to a
high-order discontinuous Galerkin discretization in space. The numerical
approach used here is identical to that used in step-67, however, we utilize
different advanced MatrixFree techniques to reach even a higher throughput.

The two main features of this tutorial are:
- the usage of shared-memory features from MPI-3.0 and
- the usage of cell-centric loops, which allow to write to the global vector only
  once and, therefore, are ideal for the usage of shared memory.

Further topics we discuss in this tutorial are the usage and benefits of the
template argument VectorizedArrayType (instead of simply using
VectorizedArray<Number>) as well as the possibility to pass lambdas to
MatrixFree loops.

For details on the numerics, we refer to the documentation of step-67. We
concentrate here only on the key differences.

<h3>Shared-memory and hybrid parallelization with MPI-3.0</h3>

<h4>Motivation</h4>

There exist many shared-memory libraries that are based on threads like TBB,
OpenMP, or TaskFlow. Integrating such libraries into existing MPI programs
allows one to use shared memory. However, these libraries come with an overhead
for the programmer, since all parallelizable code sections have to be found and
transformed according to the library used, including the difficulty when some
third-party numerical library, like an iterative solver package, only relies on
MPI.

Considering a purely MPI-parallelized FEM application, one can identify that
the major time and memory benefit of using shared memory would come from
accessing the part of the solution vector owned by the processes on the same
compute node without the need to make explicit copies and buffering them.
Fur this propose, MPI-3.0 provides shared-memory features based on so-called
windows, where processes can directly access the data of the neighbors on the same
shared-memory domain.

<h4>Basic MPI-3.0 commands</h4>

A few relevant MPI-3.0 commands are worth discussing in detail.
A new MPI communicator <code>comm_sm</code>, which consists of processes from
the communicator <code>comm</code> that have access to the same shared memory,
can be created via:

@code
MPI_Comm_split_type(comm, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &comm_sm);
@endcode

The following code snippet shows the simplified allocation routines of
shared memory for the value type <code>T</code> and the size
<code>local_size</code>, as well as, how to query pointers to the data belonging
to processes in the same shared-memory domain:

@code
MPI_Win          win;         // window
T *              data_this;   // pointer to locally-owned data
std::vector<T *> data_others; // pointers to shared data

// configure shared memory
MPI_Info info;
MPI_Info_create(&info);
MPI_Info_set(info, "alloc_shared_noncontig", "true");

// allocate shared memory
MPI_Win_allocate_shared(local_size * sizeof(T), sizeof(T), info, comm_sm, &data_this, &win);

// get pointers to the shared data owned by the processes in the same sm domain
data_others.resize(size_sm);
int disp_unit = 0; // displacement size - an output parameter we don't need right now
MPI_Aint ssize = 0; // window size - an output parameter we don't need right now
for (int i = 0; i < size_sm; ++i)
  MPI_Win_shared_query(win, i, &ssize, &disp_unit, &data_others[i]);

Assert(data_this == data_others[rank_sm], ExcMessage("Something went wrong!"));
@endcode

Once the data is not needed anymore, the window has to be freed, which also
frees the locally-owned data:

@code
MPI_Win_free(&win);
@endcode

<h4>MPI-3.0 and LinearAlgebra::distributed::Vector</h4>

The commands mentioned in the last section are integrated into
LinearAlgebra::distributed::Vector and are used to allocate shared memory if
an optional (second) communicator is provided to the reinit()-functions.

For example, a vector can be set up with a partitioner (containing the global
communicator) and a sub-communicator (containing the processes on the same
compute node):
@code
vec.reinit(partitioner, comm_sm);
@endcode

Locally owned values and ghost values can be processed as usual. However, now
users also have read access to the values of the shared-memory neighbors via
the function:
@code
const std::vector<ArrayView<const Number>> &
LinearAlgebra::distributed::Vector::shared_vector_data() const;
@endcode

<h4>MPI-3.0 and MatrixFree</h4>

While LinearAlgebra::distributed::Vector provides the option to allocate
shared memory and to access the values of shared memory of neighboring processes
in a coordinated way, it does not actually exploit the benefits of the
usage of shared memory itself.

The MatrixFree infrastructure, however, does:
- On the one hand, within the matrix-free loops MatrixFree::loop(),
  MatrixFree::cell_loop(), and MatrixFree::loop_cell_centric(), only ghost
  values that need to be updated <em>are</em> updated. Ghost values from
  shared-memory neighbors can be accessed directly, making buffering, i.e.,
  copying of the values into the ghost region of a vector possibly redundant.
  To deal with possible race conditions, necessary synchronizations are
  performed within MatrixFree. In the case that values have to be buffered,
  values are copied directly from the neighboring shared-memory process,
  bypassing more expensive MPI operations based on <code>MPI_ISend</code> and
  <code>MPI_IRecv</code>.
- On the other hand, classes like FEEvaluation and FEFaceEvaluation can read
  directly from the shared memory, so buffering the values is indeed
  not necessary in certain cases.

To be able to use the shared-memory capabilities of MatrixFree, MatrixFree
has to be appropriately configured by providing the user-created sub-communicator:

@code
typename MatrixFree<dim, Number>::AdditionalData additional_data;

// set flags as usual (not shown)

additional_data.communicator_sm = comm_sm;

data.reinit(mapping, dof_handler, constraint, quadrature, additional_data);
@endcode


<h3>Cell-centric loops</h3>

<h4>Motivation: FCL vs. CCL</h4>

"Face-centric loops" (short FCL) visit cells and faces (inner and boundary ones) in
separate loops. As a consequence, each entity is visited only once and fluxes
between cells are evaluated only once. How to perform face-centric loops
with the help of MatrixFree::loop() by providing three functions (one for
the cell integrals, one for the inner, and one for the boundary faces) has
been presented in step-59 and step-67.

"Cell-centric loops" (short CCL or ECL (for element-centric loops)
in the hyper.deal release paper), in
contrast, process a cell and in direct succession process all its
faces (i.e., visit all faces twice). Their benefit has become clear for
modern CPU processor architecture in the literature @cite KronbichlerKormann2019,
although this kind of loop implies that fluxes have to be computed twice (for
each side of an interior face). CCL has two primary advantages:
- On the one hand, entries in the solution vector are written exactly once
  back to main memory in the case of CCL, while in the case of FCL at least once
  despite of cache-efficient scheduling of cell and face loops-due to cache
  capacity misses.
- On the other hand, since each entry of the solution vector is accessed exactly
  once, no synchronization between threads is needed while accessing the solution
  vector in the case of CCL. This absence of race conditions during writing into
  the destination vector makes CCL particularly suitable for shared-memory
  parallelization.

One should also note that although fluxes are computed twice in the case of CCL,
this does not automatically translate into doubling of the computation, since
values already interpolated to the cell quadrature points can be interpolated
to a face with a simple 1D interpolation.

<h4>Cell-centric loops and MatrixFree</h4>

For cell-centric loop implementations, the function MatrixFree::loop_cell_centric()
can be used, to which the user can pass a function that should be performed on
each cell.

To derive an appropriate function, which can be passed in MatrixFree::loop_cell_centric(),
one might, in principle, transform/merge the following three functions, which can
be passed to a MatrixFree::loop():

@code
matrix_free.template loop<VectorType, VectorType>(
  [&](const auto &data, auto &dst, const auto &src, const auto range) {
    // operation performed on cells

    FEEvaluation<dim, degree, degree + 1, 1, Number> phi(data);
    for (unsigned int cell = range.first; cell < range.second; ++cell)
      {
        phi.reinit(cell);
        phi.gather_evaluate(src, cell_evaluation_flags);

        // some operations on the cell quadrature points

        phi.integrate_scatter(cell_evaluation_flags, dst);
      }
  },
  [&](const auto &data, auto &dst, const auto &src, const auto range) {
    // operation performed inner faces

    FEFaceEvaluation<dim, degree, degree + 1, 1, Number> phi_m(data, /*is_interior_face=*/true);
    FEFaceEvaluation<dim, degree, degree + 1, 1, Number> phi_p(data, /*is_interior_face=*/false);

    for (unsigned int face = range.first; face < range.second; ++face)
      {
        phi_m.reinit(face);
        phi_m.gather_evaluate(src, face_evaluation_flags);
        phi_p.reinit(face);
        phi_p.gather_evaluate(src, face_evaluation_flags);

        // some operations on the face quadrature points

        phi_m.integrate_scatter(face_evaluation_flags, dst);
        phi_p.integrate_scatter(face_evaluation_flags, dst);
      }
  },
  [&](const auto &data, auto &dst, const auto &src, const auto range) {
    // operation performed boundary faces

    FEFaceEvaluation<dim, degree, degree + 1, 1, Number> phi_m(data, /*is_interior_face=*/true);

    for (unsigned int face = range.first; face < range.second; ++face)
      {
        phi_m.reinit(face);
        phi_m.gather_evaluate(src, face_evaluation_flags);

        // some operations on the face quadrature points

        phi_m.integrate_scatter(face_evaluation_flags, dst);
      }
  },
  dst,
  src);
@endcode

in the following way:

@code
matrix_free.template loop_cell_centric<VectorType, VectorType>(
  [&](const auto &data, auto &dst, const auto &src, const auto range) {
    FEEvaluation<dim, degree, degree + 1, 1, Number>     phi(data);
    FEFaceEvaluation<dim, degree, degree + 1, 1, Number> phi_m(data, /*is_interior_face=*/true);
    FEFaceEvaluation<dim, degree, degree + 1, 1, Number> phi_p(data, /*is_interior_face=*/false);

    for (unsigned int cell = range.first; cell < range.second; ++cell)
      {
        phi.reinit(cell);
        phi.gather_evaluate(src, cell_evaluation_flags);

        // some operations on the cell quadrature points

        phi.integrate_scatter(cell_evaluation_flags, dst);

        // loop over all faces of cell
        for (unsigned int face = 0; face < GeometryInfo<dim>::faces_per_cell; ++face)
          {
            if (data.get_faces_by_cells_boundary_id(cell, face)[0] ==
                numbers::internal_face_boundary_id)
              {
                // internal face
                phi_m.reinit(cell, face);
                phi_m.gather_evaluate(src, face_evaluation_flags);
                phi_p.reinit(cell, face);
                phi_p.gather_evaluate(src, face_evaluation_flags);

                // some operations on the face quadrature points

                phi_m.integrate_scatter(face_evaluation_flags, dst);
              }
            else
              {
                // boundary face
                phi_m.reinit(cell, face);
                phi_m.gather_evaluate(src, face_evaluation_flags);

                // some operations on the face quadrature points

                phi_m.integrate_scatter(face_evaluation_flags, dst);
              }
          }
      }
  },
  dst,
  src);
@endcode

It should be noted that FEFaceEvaluation is initialized now with two numbers,
the cell number and the local face number. The given example only
highlights how to transform face-centric loops into cell-centric loops and
is by no means efficient, since data is read and written multiple times
from and to the global vector as well as computations are performed
redundantly. Below, we will discuss advanced techniques that target these issues.

To be able to use MatrixFree::loop_cell_centric(), following flags of MatrixFree::AdditionalData
have to be enabled:

@code
typename MatrixFree<dim, Number>::AdditionalData additional_data;

// set flags as usual (not shown)

additional_data.hold_all_faces_to_owned_cells       = true;
additional_data.mapping_update_flags_faces_by_cells =
  additional_data.mapping_update_flags_inner_faces |
  additional_data.mapping_update_flags_boundary_faces;

data.reinit(mapping, dof_handler, constraint, quadrature, additional_data);
@endcode

In particular, these flags enable that the internal data structures are set up
for all faces of the cells.

Currently, cell-centric loops in deal.II only work for uniformly refined meshes
and if no constraints are applied (which is the standard case DG is normally
used).


<h3>Providing lambdas to MatrixFree loops</h3>

The examples given above have already used lambdas, which have been provided to
matrix-free loops. The following short examples present how to transform functions between
a version where a class and a pointer to one of its methods are used and a
variant where lambdas are utilized.

In the following code, a class and a pointer to one of its methods, which should
be interpreted as cell integral, are passed to MatrixFree::loop():

@code
void
local_apply_cell(const MatrixFree<dim, Number> &              data,
                 VectorType &                                 dst,
                 const VectorType &                           src,
                 const std::pair<unsigned int, unsigned int> &range) const
{
  FEEvaluation<dim, degree, degree + 1, 1, Number> phi(data);
  for (unsigned int cell = range.first; cell < range.second; ++cell)
    {
      phi.reinit(cell);
      phi.gather_evaluate(src, cell_evaluation_flags);

      // some operations on the quadrature points

      phi.integrate_scatter(cell_evaluation_flags, dst);
    }
}
@endcode

@code
matrix_free.cell_loop(&Operator::local_apply_cell, this, dst, src);
@endcode

However, it is also possible to pass an anonymous function via a lambda function
with the same result:

@code
matrix_free.template cell_loop<VectorType, VectorType>(
  [&](const auto &data, auto &dst, const auto &src, const auto range) {
    FEEvaluation<dim, degree, degree + 1, 1, Number> phi(data);
    for (unsigned int cell = range.first; cell < range.second; ++cell)
      {
        phi.reinit(cell);
        phi.gather_evaluate(src, cell_evaluation_flags);

        // some operations on the quadrature points

        phi.integrate_scatter(cell_evaluation_flags, dst);
      }
  },
  dst,
  src);
@endcode

<h3>VectorizedArrayType</h3>

The class VectorizedArray<Number> is a key component to achieve the high
node-level performance of the matrix-free algorithms in deal.II.
It is a wrapper class around a short vector of $n$ entries of type Number and
maps arithmetic operations to appropriate single-instruction/multiple-data
(SIMD) concepts by intrinsic functions. The length of the vector can be
queried by VectorizedArray::size() and its underlying number type by
VectorizedArray::value_type.

In the default case (<code>VectorizedArray<Number></code>), the vector length is
set at compile time of the library to
match the highest value supported by the given processor architecture.
However, also a second optional template argument can be
specified as <code>VectorizedArray<Number, size></code>, where <code>size</code> explicitly
controls the  vector length within the capabilities of a particular instruction
set. A full list of supported vector lengths is presented in the following table:

<table align="center" class="doxtable">
  <tr>
   <th>double</th>
   <th>float</th>
   <th>ISA</th>
  </tr>
  <tr>
   <td><code>VectorizedArray<double, 1></code></td>
   <td><code>VectorizedArray<float, 1></code></td>
   <td>(auto-vectorization)</td>
  </tr>
  <tr>
   <td><code>VectorizedArray<double, 2></code></td>
   <td><code>VectorizedArray<float, 4></code></td>
   <td>SSE2/AltiVec</td>
  </tr>
  <tr>
   <td><code>VectorizedArray<double, 4></code></td>
   <td><code>VectorizedArray<float, 8></code></td>
   <td>AVX/AVX2</td>
  </tr>
  <tr>
   <td><code>VectorizedArray<double, 8></code></td>
   <td><code>VectorizedArray<float, 16></code></td>
   <td>AVX-512</td>
  </tr>
</table>

This allows users to select the vector length/ISA and, as a consequence, the
number of cells to be processed at once in matrix-free operator evaluations,
possibly reducing the pressure on the caches, an severe issue for very high
degrees (and dimensions).

A possible further reason to reduce the number of filled lanes
is to simplify debugging: instead of having to look at, e.g., 8
cells, one can concentrate on a single cell.

The interface of VectorizedArray also enables the replacement by any type with
a matching interface. Specifically, this prepares deal.II for the <code>std::simd</code>
class that is planned to become part of the C++23 standard. The following table
compares the deal.II-specific SIMD classes and the equivalent C++23 classes:


<table align="center" class="doxtable">
  <tr>
   <th>VectorizedArray (deal.II)</th>
   <th>std::simd (C++23)</th>
  </tr>
  <tr>
   <td><code>VectorizedArray<Number></code></td>
   <td><code>std::experimental::native_simd<Number></code></td>
  </tr>
  <tr>
   <td><code>VectorizedArray<Number, size></code></td>
   <td><code>std::experimental::fixed_size_simd<Number, size></code></td>
  </tr>
</table>


examples/step-76/doc/results.dox
<h1>Results</h1>

Running the program with the default settings on a machine with 40 processes
produces the following output:

@code
Running with 40 MPI processes
Vectorization over 8 doubles = 512 bits (AVX512)
Number of degrees of freedom: 27.648.000 ( = 5 [vars] x 25.600 [cells] x 216 [dofs/cell/var] )
Time step size: 0.000295952, minimal h: 0.0075, initial transport scaling: 0.00441179
Time:       0, dt:   0.0003, norm rho:  5.385e-16, rho * u:  1.916e-16, energy: 1.547e-15
+--------------------------------------+------------------+------------+------------------+
| Total wallclock time elapsed         |     17.52s    10 |     17.52s |     17.52s    11 |
|                                      |                  |                               |
| Section                  | no. calls |   min time  rank |   avg time |   max time  rank |
+--------------------------------------+------------------+------------+------------------+
| compute errors           |         1 |  0.009594s    16 |  0.009705s |  0.009819s     8 |
| compute transport speed  |        22 |    0.1366s     0 |    0.1367s |    0.1368s    18 |
| output                   |         1 |     1.233s     0 |     1.233s |     1.233s    32 |
| rk time stepping total   |       100 |     8.746s    35 |     8.746s |     8.746s     0 |
| rk_stage - integrals L_h |       500 |     8.742s    36 |     8.742s |     8.743s     2 |
+--------------------------------------+------------------+------------+------------------+
@endcode

and the following visual output:

<table align="center" class="doxtable" style="width:85%">
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-67.pressure_010.png" alt="" width="100%">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-67.pressure_025.png" alt="" width="100%">
    </td>
  </tr>
  <tr>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-67.pressure_050.png" alt="" width="100%">
    </td>
    <td>
        <img src="https://www.dealii.org/images/steps/developer/step-67.pressure_100.png" alt="" width="100%">
    </td>
  </tr>
</table>

As a reference, the results of step-67 using FCL are:

@code
Running with 40 MPI processes
Vectorization over 8 doubles = 512 bits (AVX512)
Number of degrees of freedom: 27.648.000 ( = 5 [vars] x 25.600 [cells] x 216 [dofs/cell/var] )
Time step size: 0.000295952, minimal h: 0.0075, initial transport scaling: 0.00441179
Time:       0, dt:   0.0003, norm rho:  5.385e-16, rho * u:  1.916e-16, energy: 1.547e-15
+-------------------------------------------+------------------+------------+------------------+
| Total wallclock time elapsed              |     13.33s     0 |     13.34s |     13.35s    34 |
|                                           |                  |                               |
| Section                       | no. calls |   min time  rank |   avg time |   max time  rank |
+-------------------------------------------+------------------+------------+------------------+
| compute errors                |         1 |  0.007977s    10 |  0.008053s |  0.008161s    30 |
| compute transport speed       |        22 |    0.1228s    34 |    0.2227s |    0.3845s     0 |
| output                        |         1 |     1.255s     3 |     1.257s |     1.259s    27 |
| rk time stepping total        |       100 |     11.15s     0 |     11.32s |     11.42s    34 |
| rk_stage - integrals L_h      |       500 |     8.719s    10 |     8.932s |     9.196s     0 |
| rk_stage - inv mass + vec upd |       500 |     1.944s     0 |     2.377s |      2.55s    10 |
+-------------------------------------------+------------------+------------+------------------+
@endcode

By the modifications shown in this tutorial, we were able to achieve a speedup of
27% for the Runge-Kutta stages.

<h3>Possibilities for extensions</h3>

The algorithms are easily extendable to higher dimensions: a high-dimensional
<a href="https://github.com/hyperdeal/hyperdeal/blob/a9e67b4e625ff1dde2fed93ad91cdfacfaa3acdf/include/hyper.deal/operators/advection/advection_operation.h#L219-L569">advection operator based on cell-centric loops</a>
is part of the hyper.deal library. An extension of cell-centric loops
to locally-refined meshes is more involved.

<h4>Extension to the compressible Navier-Stokes equations</h4>

The solver presented in this tutorial program can also be extended to the
compressible Navier–Stokes equations by adding viscous terms, as also
suggested in step-67. To keep as much of the performance obtained here despite
the additional cost of elliptic terms, e.g. via an interior penalty method, that
tutorial has proposed to switch the basis from FE_DGQ to FE_DGQHermite like in
the step-59 tutorial program. The reasoning behind this switch is that in the
case of FE_DGQ all values of neighboring cells (i.e., $k+1$ layers) are needed,
whilst in the case of FE_DGQHermite only 2 layers, making the latter
significantly more suitable for higher degrees. The additional layers have to be,
on the one hand, loaded from main memory during flux computation and, one the
other hand, have to be communicated. Using the shared-memory capabilities
introduced in this tutorial, the second point can be eliminated on a single
compute node or its influence can be reduced in a hybrid context.

<h4>Block Gauss-Seidel-like preconditioners</h4>

Cell-centric loops could be used to create block Gauss-Seidel preconditioners
that are multiplicative within one process and additive over processes. These
type of preconditioners use during flux computation, in contrast to Jacobi-type
preconditioners, already updated values from neighboring cells. The following
pseudo-code visualizes how this could in principal be achieved:

@code
// vector monitor if cells have been updated or not
Vector<Number> visit_flags(data.n_cell_batches () + data.n_ghost_cell_batches ());

// element centric loop with a modified kernel
data.template loop_cell_centric<VectorType, VectorType>(
  [&](const auto &data, auto &dst, const auto &src, const auto cell_range) {

    for (unsigned int cell = cell_range.first; cell < cell_range.second; ++cell)
      {
        // cell integral as usual (not shown)

        for (unsigned int face = 0; face < GeometryInfo<dim>::faces_per_cell; ++face)
          {
            const auto boundary_id = data.get_faces_by_cells_boundary_id(cell, face)[0];

            if (boundary_id == numbers::internal_face_boundary_id)
              {
                phi_p.reinit(cell, face);

                const auto flags = phi_p.read_cell_data(visit_flags);
                const auto all_neighbors_have_been_updated =
                  std::min(flags.begin(),
                           flags().begin() + data.n_active_entries_per_cell_batch(cell) == 1;

                if(all_neighbors_have_been_updated)
                  phi_p.gather_evaluate(dst, EvaluationFlags::values);
                else
                  phi_p.gather_evaluate(src, EvaluationFlags::values);

                // continue as usual (not shown)
              }
            else
              {
                // boundary integral as usual (not shown)
              }
          }

        // continue as above and apply your favorite algorithm to invert
        // the cell-local operator (not shown)

        // make cells as updated
        phi.set_cell_data(visit_flags, VectorizedArrayType(1.0));
      }
  },
  dst,
  src,
  true,
  MatrixFree<dim, Number, VectorizedArrayType>::DataAccessOnFaces::values);
@endcode

For this purpose, one can exploit the cell-data vector capabilities of
MatrixFree and the range-based iteration capabilities of VectorizedArray.

Please note that in the given example we process <code>VectorizedArrayType::size()</code>
number of blocks, since each lane corresponds to one block. We consider blocks
as updated if all blocks processed by a vector register have been updated. In
the case of Cartesian meshes this is a reasonable approach, however, for
general unstructured meshes this conservative approach might lead to a decrease in the
efficiency of the preconditioner. A reduction of cells processed in parallel
by explicitly reducing the number of lanes used by <code>VectorizedArrayType</code>
might increase the quality of the preconditioner, but with the cost that each
iteration might be more expensive. This dilemma leads us to a further
"possibility for extension": vectorization within an element.


examples/step-77/doc/intro.dox
<br>

<i>
This program was contributed by Wolfgang Bangerth, Colorado State University.

This material is based upon work partially supported by National Science
Foundation grants OAC-1835673, DMS-1821210, and EAR-1925595;
and by the Computational Infrastructure in
Geodynamics initiative (CIG), through the National Science Foundation under
Award No. EAR-1550901 and The University of California-Davis.
</i>
<br>

<a name="Intro"></a>
<h1>Introduction</h1>

The step-15 program solved the following, nonlinear equation
describing the minimal surface problem:
@f{align*}{
    -\nabla \cdot \left( \frac{1}{\sqrt{1+|\nabla u|^{2}}}\nabla u \right) &= 0 \qquad
    \qquad &&\textrm{in} ~ \Omega
    \\
    u&=g \qquad\qquad &&\textrm{on} ~ \partial \Omega.
@f}
step-15 uses a Newton method, and
Newton's method works by repeatedly solving a *linearized* problem for
an update $\delta u_k$ -- called the "search direction" --, computing a
"step length"
$\alpha_k$, and then combining them to compute the new
guess for the solution via
@f{align*}{
    u_{k+1} = u_k + \alpha_k \, \delta u_k.
@f}

In the course of the discussions in step-15, we found that it is
awkward to compute the step length, and so just settled for simple
choice: Always choose $\alpha_k=0.1$. This is of course not efficient:
We know that we can only realize Newton's quadratic convergence rate
if we eventually are able to choose $\alpha_k=1$, though we may have
to choose it smaller for the first few iterations where we are still
too far away to use this long a step length.

Among the goals of this program is therefore to address this
shortcoming. Since line search algorithms are not entirely trivial to
implement, one does as one should do anyway: Import complicated
functionality from an external library. To this end, we will make use
of the interfaces deal.II has to one of the big nonlinear solver
packages, namely the
[KINSOL](https://computing.llnl.gov/projects/sundials/kinsol)
sub-package of the
[SUNDIALS](https://computing.llnl.gov/projects/sundials)
suite. %SUNDIALS is, at its heart, a package meant to solve complex
ordinary differential equations (ODEs) and differential-algebraic
equations (DAEs), and the deal.II interfaces allow for this via the
classes in the SUNDIALS namespace: Notably the SUNDIALS::ARKode and
SUNDIALS::IDA classes. But, because that is an important step in the
solution of ODEs and DAEs with implicit methods, %SUNDIALS also has a
solver for nonlinear problems called KINSOL, and deal.II has an
interface to it in the form of the SUNDIALS::KINSOL class. This is
what we will use for the solution of our problem.

But %SUNDIALS isn't just a convenient way for us to avoid writing a
line search algorithm. In general, the solution of nonlinear problems
is quite expensive, and one typically wants to save as much compute
time as possible. One way one can achieve this is as follows: The
algorithm in step-15 discretizes the problem and then in every
iteration solves a linear system of the form
@f{align*}{
  J_k \, \delta U_k = -F_k
@f}
where $F_k$ is the residual vector computed using the current vector
of nodal values $U_k$, $J_k$ is its derivative (called the
"Jacobian"), and $\delta U_k$ is the update vector that corresponds to
the function $\delta u_k$ mentioned above. The construction of
$J_k,F_k$ has been thoroughly discussed in step-15, as has the way to
solve the linear system in each Newton iteration. So let us focus on
another aspect of the nonlinear solution procedure: Computing $F_k$ is
expensive, and assembling the matrix $J_k$ even more so. Do we
actually need to do that in every iteration? It turns out that in many
applications, this is not actually necessary: These methods often converge
even if we replace $J_k$ by an approximation $\tilde J_k$ and solve
@f{align*}{
  \tilde J_k \, \widetilde{\delta U}_k = -F_k
@f}
instead, then update
@f{align*}{
    U_{k+1} = U_k + \alpha_k \, \widetilde{\delta U}_k.
@f}
This may require an iteration or two more because our update
$\widetilde{\delta U}_k$ is not quite as good as $\delta U_k$, but it
may still be a win because we don't have to assemble $J_k$ quite as
often.

What kind of approximation $\tilde J_k$ would we like for $J_k$? Theory
says that as $U_k$ converges to the exact solution $U^\ast$, we need to
ensure that $\tilde J_k$ needs to converge to $J^\ast = \nabla F(U^\ast)$.
In particular, since $J_k\rightarrow J^\ast$, a valid choice is
$\tilde J_k = J_k$. But so is choosing $\tilde J_k = J_k$ every, say,
fifth iteration $k=0,5,10,\ldots$ and for the other iterations, we choose
$\tilde J_k$ equal to the last computed $J_{k'}$. This is what we will do
here: we will just re-use $\tilde J_{k-1}$ from the
previous iteration, which may again be what we had used in the
iteration before that, $\tilde J_{k-2}$.

This scheme becomes even more interesting if, for the solution of the
linear system with $J_k$, we don't just have to assemble a matrix, but
also compute a good preconditioner. For example, if we were to use a
sparse LU decomposition via the SparseDirectUMFPACK class, or used a
geometric or algebraic multigrid. In those cases, we would also not
have to update the preconditioner, whose computation may have taken
about as long or longer than the assembly of the matrix in the first
place. Indeed, with this mindset, we should probably think about using
the *best* preconditioner we can think of, even though their
construction is typically quite expensive: We will hope to amortize
the cost of computing this preconditioner by applying it to more than
one just one linear solve.

The big question is, of course: By what criterion do we decide whether
we can get away with the approximation $\tilde J_k$ based on a
previously computed Jacobian matrix $J_{k-s}$ that goes back $s$
steps, or whether we need to -- at least in this iteration -- actually
re-compute the Jacobian $J_k$ and the corresponding preconditioner?
This is, like the issue with line search, one that requires a
non-trivial amount of code that monitors the convergence of the
overall algorithm. We *could* implement these sorts of things
ourselves, but we probably *shouldn't*: KINSOL already does that for
us. It will tell our code when to "update" the Jacobian matrix.

One last consideration if we were to use an iterative solver instead of
the sparse direct one mentioned above: Not only is it possible to get
away with replacing $J_k$ by some approximation $\tilde J_k$ when
solving for the update $\delta U_k$, but one can also ask whether it
is necessary to solve the linear system
@f{align*}{
  \tilde J_k \widetilde{\delta U}_k = -F_k
@f}
to high accuracy. The thinking goes like this: While our current solution
$U_k$ is still far away from $U^\ast$, why would we solve this linear
system particularly accurately? The update
$U_{k+1}=U_k + \widetilde{\delta U}_k$ is likely still going to be far away
from the exact solution, so why spend much time on solving the linear system
to great accuracy? This is the kind of thinking that underlies algorithms
such as the "Eisenstat-Walker trick" @cite eiwa96 in which one is given
a tolerance to which the linear system above in iteration $k$ has to be
solved, with this tolerance dependent on the progress in the overall
nonlinear solver. As before, one could try to implement this oneself,
but KINSOL already provides this kind of information for us -- though we
will not use it in this program since we use a direct solver that requires
no solver tolerance and just solves the linear system exactly up to
round-off.

As a summary of all of these considerations, we could say the
following: There is no need to reinvent the wheel. Just like deal.II
provides a vast amount of finite-element functionality, %SUNDIALS'
KINSOL package provides a vast amount of nonlinear solver
functionality, and we better use it.


<h3> How deal.II interfaces with KINSOL </h3>

KINSOL, like many similar packages, works in a pretty abstract way. At
its core, it sees a nonlinear problem of the form
@f{align*}{
    F(U) = 0
@f}
and constructs a sequence of iterates $U_k$ which, in general, are
vectors of the same length as the vector returned by the function
$F$. To do this, there are a few things it needs from the user:
- A way to resize a given vector to the correct size.
- A way to evaluate, for a given vector $U$, the function $F(U)$. This
  function is generally called the "residual" operation because the
  goal is of course to find a point $U^\ast$ for which $F(U^\ast)=0$;
  if $F(U)$ returns a nonzero vector, then this is the
  <a href="https://en.wikipedia.org/wiki/Residual_(numerical_analysis)">"residual"</a>
  (i.e., the "rest", or whatever is "left over"). The function
  that will do this is in essence the same as the computation of
  the right hand side vector in step-15, but with an important difference:
  There, the right hand side denoted the *negative* of the residual,
  so we have to switch a sign.
- A way to compute the matrix $J_k$ if that is necessary in the
  current iteration, along with possibly a preconditioner or other
  data structures (e.g., a sparse decomposition via
  SparseDirectUMFPACK if that's what we choose to use to solve a
  linear system). This operation will generally be called the
  "setup" operation.
- A way to solve a linear system $\tilde J_k x = b$ with whatever
  matrix $\tilde J_k$ was last computed. This operation will generally
  be called the "solve" operation.

All of these operations need to be provided to KINSOL by
[std::function](https://en.cppreference.com/w/cpp/utility/functional/function)
objects that take the appropriate set of arguments and that generally
return an integer that indicates success (a zero return value) or
failure (a nonzero return value). Specifically, the objects we will
access are the
SUNDIALS::KINSOL::reinit_vector,
SUNDIALS::KINSOL::residual,
SUNDIALS::KINSOL::setup_jacobian, and
SUNDIALS::KINSOL::solve_jacobian_system
member variables. (See the documentation of these variables for their
details.) In our implementation, we will use
[lambda functions](https://en.cppreference.com/w/cpp/language/lambda)
to implement these "callbacks" that in turn can call member functions;
KINSOL will then call these callbacks whenever its internal algorithms
think it is useful.


<h3> Details of the implementation </h3>

The majority of the code of this tutorial program is as in step-15,
and we will not comment on it in much detail. There is really just one
aspect one has to pay some attention to, namely how to compute $F(U)$
given a vector $U$ on the one hand, and $J(U)$ given a vector $U$
separately. At first, this seems trivial: We just take the
`assemble_system()` function and in the one case throw out all code
that deals with the matrix and in the other case with the right hand
side vector. There: Problem solved.

But it isn't quite as simple. That's because the two are not
independent if we have nonzero Dirichlet boundary values, as we do
here. The linear system we want to solve contains both interior and
boundary degrees of freedom, and when eliminating those degrees of
freedom from those that are truly "free", using for example
AffineConstraints::distribute_local_to_global(), we need to know the
matrix when assembling the right hand side vector.

Of course, this completely contravenes the original intent: To *not*
assemble the matrix if we can get away without it. We solve this
problem as follows:
- We set the starting guess for the solution vector, $U_0$, to one
  where boundary degrees of freedom already have their correct values.
- This implies that all updates can have zero updates for these
  degrees of freedom, and we can build both residual vectors $F(U_k)$
  and Jacobian matrices $J_k$ that corresponds to linear systems whose
  solutions are zero in these vector components. For this special
  case, the assembly of matrix and right hand side vectors is
  independent, and can be broken into separate functions.

There is an assumption here that whenever KINSOL asks for a linear
solver with the (approximation of the) Jacobian, that this will be for
for an update $\delta U$ (which has zero boundary values), a multiple
of which will be added to the solution (which already has the right
boundary values).  This may not be true and if so, we might have to
rethink our approach. That said, it turns out that in practice this is
exactly what KINSOL does when using a Newton method, and so our
approach is successful.


examples/step-77/doc/results.dox
<h1>Results</h1>

When running the program, you get output that looks like this:
@code
Mesh refinement step 0
  Target_tolerance: 0.001

  Computing residual vector... norm=0.231202
  Computing Jacobian matrix
  Factorizing Jacobian matrix
  Solving linear system
  Computing residual vector... norm=0.231202
  Computing residual vector... norm=0.171585
  Solving linear system
  Computing residual vector... norm=0.171585
  Computing residual vector... norm=0.127245
  Computing residual vector... norm=0.0796471
  Solving linear system
  Computing residual vector... norm=0.0796471
  Computing residual vector... norm=0.0625301
  Solving linear system
  Computing residual vector... norm=0.0625301
  Computing residual vector... norm=0.0498864
  Solving linear system
  Computing residual vector... norm=0.0498864
  Computing residual vector... norm=0.0407765
  Solving linear system
  Computing residual vector... norm=0.0407765
  Computing residual vector... norm=0.0341589
  Solving linear system
  Computing residual vector... norm=0.0341589
  Computing residual vector... norm=0.0292867
  Solving linear system
  Computing residual vector... norm=0.0292867
  Computing residual vector... norm=0.0256309
  Computing residual vector... norm=0.0223448
  Solving linear system
  Computing residual vector... norm=0.0223448
  Computing residual vector... norm=0.0202797
  Computing residual vector... norm=0.0183817
  Solving linear system
  Computing residual vector... norm=0.0183817
  Computing residual vector... norm=0.0170464
  Computing residual vector... norm=0.0157967
  Computing Jacobian matrix
  Factorizing Jacobian matrix
  Solving linear system
  Computing residual vector... norm=0.0157967
  Computing residual vector... norm=0.0141572
  Computing residual vector... norm=0.012657
 Solving linear system
  Computing residual vector... norm=0.012657
  Computing residual vector... norm=0.0116863
  Computing residual vector... norm=0.0107696
  Solving linear system
  Computing residual vector... norm=0.0107696
  Computing residual vector... norm=0.0100986
  Computing residual vector... norm=0.00944829
  Computing residual vector... norm=0.00822576
  Solving linear system
  Computing residual vector... norm=0.00822576
  Computing residual vector... norm=0.00781983
  Computing residual vector... norm=0.00741619
  Computing residual vector... norm=0.00661792
  Solving linear system
  Computing residual vector... norm=0.00661792
  Computing residual vector... norm=0.00630571
  Computing residual vector... norm=0.00599457
  Computing residual vector... norm=0.00537663
  Solving linear system
  Computing residual vector... norm=0.00537663
  Computing residual vector... norm=0.00512813
  Computing residual vector... norm=0.00488033
  Computing residual vector... norm=0.00438751
  Computing residual vector... norm=0.00342052
  Solving linear system
  Computing residual vector... norm=0.00342052
  Computing residual vector... norm=0.00326581
  Computing residual vector... norm=0.00311176
  Computing residual vector... norm=0.00280617
  Computing residual vector... norm=0.00220992
  Solving linear system
  Computing residual vector... norm=0.00220992
  Computing residual vector... norm=0.00209976
  Computing residual vector... norm=0.00199943
  Solving linear system
  Computing residual vector... norm=0.00199942
  Computing residual vector... norm=0.00190953
  Computing residual vector... norm=0.00182005
  Computing residual vector... norm=0.00164259
  Computing residual vector... norm=0.00129652


+---------------------------------------------+------------+------------+
| Total wallclock time elapsed since start    |     0.192s |            |
|                                             |            |            |
| Section                         | no. calls |  wall time | % of total |
+---------------------------------+-----------+------------+------------+
| assembling the Jacobian         |         2 |    0.0141s |       7.4% |
| assembling the residual         |        61 |     0.168s |        88% |
| factorizing the Jacobian        |         2 |    0.0016s |      0.83% |
| graphical output                |         1 |   0.00385s |         2% |
| linear system solve             |        19 |    0.0013s |      0.68% |
+---------------------------------+-----------+------------+------------+


Mesh refinement step 1
  Target_tolerance: 0.0001

  Computing residual vector... norm=0.0883422
  Computing Jacobian matrix
  Factorizing Jacobian matrix
  Solving linear system
  Computing residual vector... norm=0.0883422
  Computing residual vector... norm=0.0607066
  Solving linear system
  Computing residual vector... norm=0.0607066
  Computing residual vector... norm=0.0437266
  Solving linear system
  Computing residual vector... norm=0.0437266
  Computing residual vector... norm=0.0327999
  Solving linear system
  Computing residual vector... norm=0.0327999
  Computing residual vector... norm=0.0255418
  Solving linear system
  Computing residual vector... norm=0.0255417
  Computing residual vector... norm=0.0206042
  Solving linear system
  Computing residual vector... norm=0.0206042
  Computing residual vector... norm=0.0171602
  Solving linear system
  Computing residual vector... norm=0.0171602
  Computing residual vector... norm=0.014689
  Solving linear system

[...]
@endcode

The way this should be interpreted is most easily explained by looking at
the first few lines of the output on the first mesh:
@code
Mesh refinement step 0
Mesh refinement step 0
  Target_tolerance: 0.001

  Computing residual vector... norm=0.231202
  Computing Jacobian matrix
  Factorizing Jacobian matrix
  Solving linear system
  Computing residual vector... norm=0.231202
  Computing residual vector... norm=0.171585
  Solving linear system
  Computing residual vector... norm=0.171585
  Computing residual vector... norm=0.127245
  Computing residual vector... norm=0.0796471
  Solving linear system
  Computing residual vector... norm=0.0796471
  ...
@endcode
What is happening is this:
- In the first residual computation, KINSOL computes the residual to see whether
  the desired tolerance has been reached. The answer is no, so it requests the
  user program to compute the Jacobian matrix (and the function then also
  factorizes the matrix via SparseDirectUMFPACK).
- KINSOL then instructs us to solve a linear system of the form
  $J_k \, \delta U_k = -F_k$ with this matrix and the previously computed
  residual vector.
- It is then time to determine how far we want to go in this direction,
  i.e., do line search. To this end, KINSOL requires us to compute the
  residual vector $F(U_k + \alpha_k \delta U_k)$ for different step lengths
  $\alpha_k$. For the first step above, it finds an acceptable $\alpha_k$
  after two tries, the second time around it takes three tries.
- Having found a suitable updated solution $U_{k+1}$, the process is
  repeated except now KINSOL is happy with the current Jacobian matrix
  and does not instruct us to re-build the matrix and its factorization,
  and instead asks us to solve a linear system with that same matrix.

The program also writes the solution to a VTU file at the end
of each mesh refinement cycle, and it looks as follows:
<table width="60%" align="center">
  <tr>
    <td align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-77.solution.png" alt="">
    </td>
  </tr>
</table>


The key takeaway messages of this program are the following:

- The solution is the same as the one we computed in step-15, i.e., the
  interfaces to %SUNDIALS' KINSOL package really did what they were supposed
  to do. This should not come as a surprise, but the important point is that
  we don't have to spend the time implementing the complex algorithms that
  underlie advanced nonlinear solvers ourselves.

- KINSOL is able to avoid all sorts of operations such as rebuilding the
  Jacobian matrix when that is not actually necessary. Comparing the
  number of linear solves in the output above with the number of times
  we rebuild the Jacobian and compute its factorization should make it
  clear that this leads to very substantial savings in terms of compute
  times, without us having to implement the intricacies of algorithms
  that determine when we need to rebuild this information.

<a name="extensions"></a>
<h3> Possibilities for extensions </h3>

For all but the small problems we consider here, a sparse direct solver
requires too much time and memory -- we need an iterative solver like
we use in many other programs. The trade-off between constructing an
expensive preconditioner (say, a geometric or algebraic multigrid method)
is different in the current case, however: Since we can re-use the same
matrix for numerous linear solves, we can do the same for the preconditioner
and putting more work into building a good preconditioner can more easily
be justified than if we used it only for a single linear solve as one
does for many other situations.

But iterative solvers also afford other opportunities. For example (and as
discussed briefly in the introduction), we may not need to solve to
very high accuracy (small tolerances) in early nonlinear iterations as long
as we are still far away from the actual solution. This was the basis of the
Eisenstat-Walker trick mentioned there.

KINSOL provides the function that does the linear solution with a target
tolerance that needs to be reached. We ignore it in the program above
because the direct solver we use does not need a tolerance and instead
solves the linear system exactly (up to round-off, of course), but iterative
solvers could make use of this kind of information -- and, in fact, should.


examples/step-78/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

The Black-Scholes equation is a partial differential equation that falls a bit
out of the ordinary scheme. It describes what the fair price of a "European
call" stock option is. Without going into too much detail, a stock "option" is
a contract one can buy from a bank that allows me, but not requires me, to buy
a specific stock at a fixed price $K$ at a fixed future time $T$ in the
future. The question one would then want to answer as a buyer of such an
option is "How much do I think such a contract is worth?", or as the seller
"How much do I need to charge for this contract?", both as a function of the
time $t<T$ before the contract is up at time $T$ and as a function of the stock
price $S$. Fischer Black and Myron Scholes derived a partial differential
equation for the fair price $V(S,t)$ for such options under the assumption that
stock prices exhibit random price fluctuations with a given level of
"volatility" plus a background exponential price increase (which one can think
of as the inflation rate that simply devalues all money over time). For their
work, Black and Scholes received the Nobel Prize in Economic Sciences in 1997,
making this the first tutorial program dealing with a problem for which someone
has gotten a Nobel Prize @cite black1973pricing.

The equation reads as follows:
@f{align*}{
    &\frac{\partial V}{\partial t} + \frac{\sigma^2S^2}{2} \
    \frac{\partial^2 V}{\partial S^2} + \
    rS\frac{\partial V}{\partial S} - rV = 0, \
    \quad\quad &&\forall S\in \Omega, t \in (0,T)
    \\
    &V(0,t) = 0, \
    &&\forall t \in (0,T)
    \\
    &V(S,t) \rightarrow S, \
    && \text{as } S \rightarrow \infty, \forall t \in (0,T)
    \\
    &V(S,T) = \max(S-K,0) \
    &&\forall S \in \Omega
@f}
where
@f{align*}{
    V(S,t): && \text{Value of call option at time t and asset price S} \\
    \sigma: && \text{Volatility of the underlying asset} \\
    r: && \text{Risk free interest rate} \\
    K : && \text{Strike price for purchasing asset}
@f}

The way we should interpret this equation is that it is a time-dependent partial
differential equation of one "space" variable
$S$ as the price of the stock, and $V(S,t)$ is the price of the option at time
$t$ if the stock price at that time were $S$.

<h3>Particularities of the equation system</h3>

There are a number of oddities in this equation that are worth discussing before
moving on to its numerical solution. First, the "spatial" domain
$\Omega\subset\mathbb{R}$ is unbounded, and thus $S$ can be unbounded in value.
This is because there may be a practical upper bound for stock prices, but not a
conceptual one. The boundary conditions $V(S,t)\rightarrow S$ as
$S\rightarrow \infty$ can then be interpreted as follows: What is the value of
an option that allows me to buy a stock at price $K$ if the stock price (today
or at time $t=T$) is $S\gg K$? One would expect that it is $V\approx S-K$ plus
some adjustment for inflation, or, if we really truly consider huge values of
$S$, we can neglect $K$ and arrive at the statement that the boundary values at
the infinite boundary should be of the form $V\rightarrow S$ as stated above.

In practice, for us to use a finite element method to solve this, we are going
to need to bound $\Omega$. Since this equation describes prices, and it doesn't
make sense to talk about prices being negative, we will set the lower bound of
$\Omega$ to be 0. Then, for an upper bound, we will choose a very large number,
one that $S$ is not very likely to ever get to. We will call this $S_\text{max}$.
So, $\Omega=[0,S_\text{max}]$.

Second, after truncating the domain, we need to ask what boundary values we
should pose at this now finite boundary. To take care of this, we use "put-call"
parity @cite stoll1969relationship. A "pull option" is one in which we are
allowed, but not required, to *sell* a stock at price $K$ to someone at a future
time $T$. This says
@f{align*}{
    V(S,t)+Ke^{-r(T-t)}=P(S,t)+S
@f}
where $V(S,t)$ is the value of the call option, and $P(S,t)$ is the value of the
put option. Since we expect $P(S,t) \rightarrow 0$ as $S \rightarrow \infty$,
this says
@f{align*}{
    V(S,t) \rightarrow S-Ke^{-r(T-t)},
@f}
and we can use this as a reasonable boundary condition at our finite point
$S_\text{max}$.

The second complication of the Block-Scholes equation is that we are given a
final condition, and not an initial condition. This is because we know what the
option is worth at time $t=T$: If the stock price at $T$ is $S<K$, then we have
no incentive to use our option of buying a price $K$ because we can buy that stock
for cheaper on the open market. So $V(S,T)=0$ for $S<K$. On the other hand, if
at time $T$ we have $S>K$, then we can buy the stock at price $K$ via the option
and immediately sell it again on the market for price $S$, giving me a profit of
$S-K$. In other words, $V(S,T)=S-K$ for $S>K$. So, we only know
values for $V$ at the *end time* but not the initial time -- in fact, finding
out what a fair price at the current time (conventionally taken to be $t=0$) is
what solving these equations is all about.

This means that this is not an equation that is posed going forward in
time, but in fact going *backward* in time. Thus it makes sense to solve this
problem in reverse by making the change of variables $\tau=T-t$ where now $\tau$
denotes the time before the strike time $T$.

With all of this, we finally end up with the following problem:
@f{align*}{
    &-\frac{\partial V}{\partial \tau} + \frac{\sigma^2S^2}{2} \
    \frac{\partial^2 V}{\partial S^2} + rS\frac{\partial V}{\partial S} - rV=0\
    , \quad\quad &&\forall S\in [0,S_\text{max}], \tau \in [0,T]
    \\
    &V(0,\tau) = 0, \
    &&\forall \tau \in [0,T]
    \\
    &V(S_\text{max},\tau)=S_\text{max}-Ke^{-r\tau}, \
    &&\forall \tau \in [0,T]
    \\
    &V(S,0) = \max(S-K,0) \
    &&\forall S \in [0,S_\text{max}]
@f}

Conceptually, this is an advection-diffusion-reaction problem for the variable
$V$: There is both a second-order derivative diffusion term, a first-order
derivative advection term, and a zeroth-order reaction term.
We can expect this problem to be a little bit forgiving in practice because for
realistic values of the coefficients, it is diffusive dominated. But, because of
the advective terms in the problem, we will have to be careful with mesh
refinement and time step choice. There is also the issue that the diffusion term
 is written in a non-conservative form and so integration by parts is not
 immediately obvious. This will be discussed in the next section.

<h3>Scheme for the numerical solution</h3>

We will solve this problem using an IMEX method. In particular, we first discretize
in time with the theta method and will later pick different values of theta for
the advective and diffusive terms.
Let $V^n(S)$ approximate $V(S,\tau_n)$:
@f{align*}{
    0=&-\frac{V^n(S)-V^{n-1}(S)}{k_n} \\
    &+\frac{\sigma^2S^2}{2}\left[(1-\theta)\frac{d^2V^{n-1}(S)}{dS^2} + \
    \theta \frac{d^2V^{n}(S)}{dS^2}\right] \\
    &+rS\left[(1-\theta)\frac{dV^{n-1}(S)}{dS} + \
    \theta\frac{dV^{n}(S)}{dS}\right]  \\
    &-r\left[(1-\theta)V^{n-1}(S) + \theta V^n(S)\right]
@f}
Here, $k_n=\tau_n-\tau_{n-1}$ is the time step size. Given this time
discretization, we can proceed to discretize space by multiplying with test
functions and then integrating by parts. Because there are some interesting
details in this due to the advective and non-advective terms in this equation,
this process will be explained in detail.

So, we begin by multiplying by test functions, $\{\phi_i(S)\}_{i\in\mathbb{N}}$:
@f{align*}{
    0=&-\int_0^{S_\text{max}}\phi_i(S)\left[V^n(S)-V^{n-1}(S)\right]dS \\
    &+k_n\int_0^{S_\text{max}}\phi_i(S)\left[\frac{\sigma^2S^2}{2} \
    \left[(1-\theta)\frac{d^2V^{n-1}(S)}{dS^2} + \
     \theta \frac{d^2V^{n}(S)}{dS^2}\right]\right]dS \\
    &+k_n\int_0^{S_\text{max}}\phi_i(S)\left[rS\left[(1-\theta)
     \frac{dV^{n-1}(S)}{dS}\
     + \theta\frac{dV^{n}(S)}{dS}\right]\right]dS  \\
    &-k_n\int_0^{S_\text{max}}\phi_i(S)\left[r\left[(1-\theta)V^{n-1}(S)\
     + \theta V^n(S)\right]\right]dS
@f}


As usual, (1) becomes $-\textbf{M}V^n+\textbf{M}V^{n-1}$ and (4) becomes
$k_n\left[-r(1-\theta)\textbf{M}V^{n-1} - \theta r\textbf{M}V^n\right]$, where
$\textbf{M}_{i,j}=\left(\phi_i(S),\phi_j(S)\right)$, and where we have taken the
liberty of denoting by $V$ not only the function $V(S)$ but also the vector of
nodal values after discretization.

The interesting parts come from (2) and (3).


For (2), we have:
@f{align*}{
    &k_n\int_0^{S_\text{max}}\phi_i(S)\left[\frac{\sigma^2S^2}{2} \
     \left[(1-\theta)\frac{d^2V^{n-1}(S)}{dS^2} + \
     \theta \frac{d^2V^{n}(S)}{dS^2}\right]\right]dS \\
    &=k_n(1-\theta)\int_0^{S_\text{max}}\phi_i(S)\frac{\sigma^2S^2}{2} \
     \frac{d^2V^{n-1}(S)}{dS^2} \
    +k_n\theta\int_0^{S_\text{max}}\phi_i(S)\frac{\sigma^2S^2}{2} \
     \frac{d^2V^{n}(S)}{dS^2}
@f}

There are two integrals here, that are more or less the same, with the
differences being a slightly different coefficient in front of the integral,
and a different time step for V. Therefore, we will outline this integral in the
general case, and account for the differences at the end. So, consider the
general integral, which we will solve using integration by parts:
@f{align*}{
    &\int_{0}^{S_\text{max}} \phi_i(S)\frac{\sigma^2S^2}{2}
        \frac{d^2V^n(S)}{dS^2}dS \\
    &= \phi_i(S)\frac{1}{2}\sigma^2S^2\frac{dV^n(S)}{dS}\Bigg|_0^{S_{max}} - \
    \int_0^{S_\text{max}} \phi_i(S)\sigma^2S\frac{dV^n(S)}{dS}dS - \
    \int_0^{S_\text{max}} \frac{d\phi_i(S)}{dS}\frac{1}{2}\sigma^2S^2 \
    \frac{dV^n(S)}{dS}dS \\
    &= -\int_0^{S_\text{max}} \phi_i(S)\sigma^2S\frac{dV^n(S)}{dS}dS - \
    \int_0^{S_\text{max}} \frac{d\phi_i(S)}{dS}\frac{1}{2}\sigma^2S^2 \
    \frac{dV^n(S)}{dS}dS \\
    &= -\int_0^{S_\text{max}} \phi_i(S)\sigma^2S \sum_j V_j^n
        \frac{d\phi_j(S)}{dS}dS \
    -\int_0^{S_\text{max}} \frac{d\phi_i(S)}{dS}\frac{1}{2} \
    \sigma^2S^2  \sum_k V_k^n \frac{d\phi_k(S)}{dS}dS \\
    &= -\sum_j \sigma^2 \int_0^{S_\text{max}} \phi_i(S)S
        \frac{d\phi_j(S)}{dS}dS V_j^n\
    - \sum_k \frac{1}{2}\sigma^2 \int_0^{S_\text{max}} \frac{d\phi_i(S)}{dS}S^2\
    \frac{d\phi_k}{dS}dS V_k^n \\
    &= -\sum_j \sigma^2 \left(\phi_i(S)S, \frac{d\phi_j(S)}{dS}\right) V_j^n \
    - \sum_k \frac{1}{2}\sigma^2 \left(\frac{d\phi_i(S)}{dS}S^2,\
    \frac{d\phi_k(S)}{dS}\right) V_k^n \\
    &= -\sigma^2\textbf{B}V^n - \frac{1}{2}\sigma^2\textbf{D}V^n, \quad\quad \
    \textbf{B}_{i,j} = \left(\phi_i(S)S, \frac{d\phi_j(S)}{dS}\right),\
    \textbf{D}_{i,j} = \left(\frac{d\phi_i(S)}{dS}S^2,\frac{d\phi_j(S)}{dS}\right)
@f}

So, after adding in the constants and exchanging $V^n$ for $V^{n-1}$ where
applicable, we arrive at the following for (2):
@f{align*}{
    &k_n\int_0^{S_\text{max}}\phi_i(S)\left[\frac{\sigma^2S^2}{2}
        \left[(1-\theta)\
    \frac{d^2V^{n-1}(S)}{dS^2} + \
    \theta \frac{d^2V^{n}(S)}{dS^2}\right]\right]dS \\
    &= k_n\left[-(1-\theta)\sigma^2\textbf{B}V^{n-1}\
     -(1-\theta)\frac{1}{2}\sigma^2\textbf{D}V^{n-1} \
    -\theta\sigma^2\textbf{B}V^{n}
     -\theta\frac{1}{2}\sigma^2\textbf{D}V^{n}\right]
@f}
But, because the matrix $\textbf{B}$ involves an advective term, we will choose
$\theta=0$ there -- in other words, we use an explicit Euler method to treat
advection. Conversely, since the matrix $\textbf{D}$ involves the diffusive term,
we will choose $\theta=1/2$ there -- i.e., we treat diffusion using the second
order Crank-Nicolson method.

So, we arrive at the following:
@f{align*}{
    k_n\left[-\frac{1}{4}\sigma^2\textbf{D}V^{n-1} \
    -\frac{1}{4}\sigma^2\textbf{D}V^n \
    - \sigma^2\textbf{B}V^{n-1}\right]
@f}

Now, to handle (3). For this, we will again proceed by considering the general
case like above.

@f{align*}{
    &\int_{0}^{S_\text{max}} \phi_i(S)rS\frac{dV^n}{dS}dS \\
    &= \phi_i(S)rSV^n\Bigg|_0^{S_\text{max}} - \int_0^{S_\text{max}}
        \left[r\phi_i(S) \
    + r\frac{d\phi_i(S)}{dS}S \right]V^ndS \\
    &= -\int_0^{S_\text{max}} r\phi_i(S)V^ndS - \
    \int_0^{S_\text{max}} r\frac{d\phi_i(S)}{dS}SV^ndS \\
    &= -\int_0^{S_\text{max}} r\phi_i(S) \sum_j V_j^n\phi_j(S)dS \
    -\int_0^{S_\text{max}} rS\frac{d\phi_i(S)}{dS} \sum_k V_k\phi_k(S)dS \\
    &= -\sum_j r\left(\phi_i(S), \phi_j(S)\right) V_j^n -\
     \sum_k r\left(S\frac{d\phi_i(S)}{dS}, \phi_k(S)\right)V_k^n \\
    &= -r\textbf{M}V^n -r\textbf{A}V^n, \quad\quad\
    \textbf{M}_{i,j} = \left(\phi_i(S), \phi_j(S)\right),\
    \textbf{A}_{i,j} = \left(S\frac{d\phi_i(S)}{dS}, \phi_j(S)\right)
@f}

So, again after adding in the constants and exchanging $V^n$ for $V^{n-1}$ where
applicable, we arrive at the following for (3):
@f{align*}{
    &k_n\int_0^{S_\text{max}}\phi_i(S)\left[rS\left[(1-\theta)
        \frac{dV^{n-1}(S)}{dS} +\
     \theta\frac{dV^{n}(S)}{dS}\right]\right]dS \\
    &= k_n\left[-(1-\theta)r\textbf{M}V^{n-1} -(1-\theta)r\textbf{A}V^{n-1}\
    -\theta r\textbf{M}V^n -\theta r\textbf{A}V^n\right]
@f}
Just as before, we will use $\theta=0$ for the matrix $\textbf{A}$ and
$\theta=\frac{1}{2}$ for the matrix $\textbf{M}$. So, we arrive at the
following for (3):
@f{align*}{
    k_n\left[-\frac{1}{2}r\textbf{M}V^{n-1} - \frac{1}{2}r\textbf{M}V^n \
    -r\textbf{A}V^{n-1}\right]
@f}

Now, putting everything together, we obtain the following discrete form for the
Black-Scholes Equation:
@f{align*}{
    0&= \\
    &-\textbf{M}V^n+\textbf{M}V^{n-1} \\
    & +k_n\left[-\frac{1}{4}\sigma^2\textbf{D}V^{n-1} \
    -\frac{1}{4}\sigma^2\textbf{D}V^n \
    - \sigma^2\textbf{B}V^n \
     -\frac{1}{2}r\textbf{M}V^{n-1} - \frac{1}{2}r\textbf{M}V^n \
    -r\textbf{A}V^n \
     -r\frac{1}{2}\textbf{M}V^{n-1} - \frac{1}{2} r\textbf{M}V^n\right] \\
    &= -\textbf{M}V^n + \textbf{M}V^{n-1} +\
    k_n\left[- \frac{1}{4}\sigma^2\textbf{D}V^{n-1} -\
    \frac{1}{4}\sigma^2\textbf{D}V^n - r\textbf{M}V^{n-1} -\
    r\textbf{M}V^n  - \sigma^2\textbf{B}V^{n-1} - r\textbf{A}V^{n-1}\right]
@f}
So, altogether we have:

@f{equation}{
    0 = \textbf{M}V^n - \textbf{M}V^{n-1} +\
    k_n\left[ \frac{1}{4}\sigma^2\textbf{D}V^{n-1} +\
    \frac{1}{4}\sigma^2\textbf{D}V^n + r\textbf{M}V^{n-1} + r\textbf{M}V^n  +\
    \sigma^2\textbf{B}V^{n-1} + r\textbf{A}V^{n-1}\right]\tag{*}
@f}

As usual, we can write this with the unknown quantities on the left and the
known ones on the right. This leads to the following linear system that would
have to be solved in each time step:

@f{align*}{
    \left[\textbf{M}+\frac{1}{4}k_n\sigma^2\textbf{D}+k_nr\textbf{M}\right]V^n\
     =\
    \left[-\frac{1}{4}k_n\sigma^2\textbf{D}-\
    k_nr\textbf{M}+k_n\sigma^2\textbf{B}-\
    k_nr\textbf{A}+\textbf{M}\right]V^{n-1}
@f}




<h3>Test Case</h3>
For this program, we will use the Method of Manufactured Solutions (MMS) to test
 that it is working correctly. This means that we will choose our solution to be
  a certain function similar to step-7. For our case, we will use:
@f{align*}{
    V(S,\tau) = -\tau^2 - S^2 + 6\tag{**}
@f}
This means that, using our PDE, we arrive at the following problem:
@f{align*}{
    &-\frac{\partial V}{\partial \tau} +\
    \frac{\sigma^2S^2}{2}\frac{\partial^2 V}{\partial S^2} +\
    rS\frac{\partial V}{\partial S} - rV = f(S,\tau) \\
    &V(0,\tau) = -\tau^2 + 6 \\
    &V(S_\text{max}, \tau) = -S_\text{max}^2 - \tau^2 + 6 \\
    &V(S, 0) = -S^2 + 6
@f}
Where, $f(S,\tau) = 2\tau - \sigma^2S^2 - 2rS^2 - r(-\tau^2 - S^2 + 6)$.
This set-up now has right hand sides for the equation itself and for the
boundary conditions at $S=0$ that we did not have before, along with "final"
conditions (or, with $\tau$-time "initial conditions") that do not match the
real situation. We will implement this in such a way in the code that it is easy
to exchange -- the introduction of the changes above is just meant to enable the
 use of a manufactured solution.

If the program is working correctly, then it should produce (**) as the
solution. This does mean that we need to modify our variational form somewhat to
account for the non-zero right hand side.

First, we define the following:
@f{align*}{
    F^n_i = \left(\phi_i(S), f^n(S)\right), && \text{where } f^n(S) =\
     f(S,\tau_n)
@f}
So, we arrive at the new equation:

@f{align*}{
    \left[\textbf{M}+\frac{1}{4}k_n\sigma^2\textbf{D}+k_nr\textbf{M}\right]V^n\
     =\
     \left[-\frac{1}{4}k_n\sigma^2\textbf{D}-\
     k_nr\textbf{M}+k_n\sigma^2\textbf{B}-\
     k_nr\textbf{A}+\textbf{M}\right]V^{n-1} -\
      k_n\left[\frac{1}{2}F^{n-1}+\frac{1}{2}F^n\right]
@f}

We then solve this equation as outlined above.


examples/step-78/doc/results.dox
<h1>Results</h1>


Below is the output of the program:
@code
===========================================
Number of active cells: 1
Number of degrees of freedom: 2

Time step 0 at t=0.0002
[...]

Cycle 7:
Number of active cells: 128
Number of degrees of freedom: 129

Time step 0 at t=0.0002
Time step 1000 at t=0.2002
Time step 2000 at t=0.4002
Time step 3000 at t=0.6002
Time step 4000 at t=0.8002

cells dofs    L2        H1      Linfty
    1    2 1.667e-01 5.774e-01 2.222e-01
    2    3 3.906e-02 2.889e-01 5.380e-02
    4    5 9.679e-03 1.444e-01 1.357e-02
    8    9 2.405e-03 7.218e-02 3.419e-03
   16   17 5.967e-04 3.609e-02 8.597e-04
   32   33 1.457e-04 1.804e-02 2.155e-04
   64   65 3.307e-05 9.022e-03 5.388e-05
  128  129 5.016e-06 4.511e-03 1.342e-05

n cells         H1                  L2
      1 5.774e-01    -    - 1.667e-01    -    -
      2 2.889e-01 2.00 1.00 3.906e-02 4.27 2.09
      4 1.444e-01 2.00 1.00 9.679e-03 4.04 2.01
      8 7.218e-02 2.00 1.00 2.405e-03 4.02 2.01
     16 3.609e-02 2.00 1.00 5.967e-04 4.03 2.01
     32 1.804e-02 2.00 1.00 1.457e-04 4.10 2.03
     64 9.022e-03 2.00 1.00 3.307e-05 4.41 2.14
    128 4.511e-03 2.00 1.00 5.016e-06 6.59 2.72
@endcode

What is more interesting is the output of the convergence tables. They are
outputted into the console, as well into a LaTeX file. The convergence tables
are shown above. Here, you can see that the the solution has a convergence rate
of $\mathcal{O}(h)$ with respect to the $H^1$-norm, and the solution has a convergence rate
of $\mathcal{O}(h^2)$ with respect to the $L^2$-norm.


Below is the visualization of the solution.

<div style="text-align:center;">
  <img src="https://www.dealii.org/images/steps/developer/step-78.mms-solution.png"
       alt="Solution of the MMS problem.">
</div>


examples/step-79/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>

Topology Optimization of Elastic Media is a technique used to optimize a
structure that is bearing some load. Ideally, we would like to minimize the
maximum stress placed on a structure by selecting a region $E$ where material is
placed. In other words,
@f[
  \text{minimize}\| \boldsymbol{\sigma} (\mathbf{u}) \|_\infty
@f]
@f[
  \text{subject to } |E|\leq V_{\max},
@f]
@f[
  \text{and } \nabla \cdot \boldsymbol{\sigma} + \mathbf{F} = \mathbf{0}.
@f]

Here, $\boldsymbol{\sigma} = \mathbf{C} : \boldsymbol{\varepsilon}(\mathbf{u})$ is the stress
within the body that is caused by the external forces $\mathbf F$, where we have for simplicity assumed
that the material is linear-elastic and so $\mathbf{C}$ is the stress-strain tensor and
$\boldsymbol{\varepsilon}(\mathbf{u})=\frac{1}{2} (\nabla \mathbf{u} + (\nabla\mathbf{u})^T)$ is the
small-deformation strain as a function of the displacement $\mathbf{u}$ -- see
step-8 and step-17 for more on linear elasticity. In the formulation above,
$V_\text{max}$ is the maximal amount of material we are willing to provide to
build the object. The last of the constraints is the partial differential
equation that relates stress $\boldsymbol{\sigma}$ and forces $\mathbf F$ and is simply the
steady-state force balance.

That said, the infinity norm above creates a problem: As a function of location
of material, this objective function is necessarily not differentiable, making
prospects of optimization rather bleak. So instead, a common approach in
topology optimization is to find an approximate solution by optimizing a related
problem: We would like to minimize the strain energy. This is a
measure of the potential energy stored in an object due to its deformation, but
also works as a measure of total deformation over the structure.

@f[
  \text{minimize  } \int_E \frac{1}{2}\boldsymbol{\sigma} : \boldsymbol{\varepsilon} dV
@f]
@f[
  \text{subject to } \|E\| \leq V_{\max}
@f]
@f[
  \text{and } \nabla \cdot \boldsymbol{\sigma} + \mathbf{F} = \mathbf{0}
@f]

The value of the objective function is calculated using a finite element method,
where the solution is the displacements. This is placed inside of a nonlinear
solver loop that solves for a vector denoting placement of material.

<h3>Solid Isotropic Material with Penalization</h3>

In actual practice, we can only build objects in which the material is either
present, or not present, at any given point -- i.e., we would have an indicator
function $\rho_E(\mathbf{x})\in \{0,1\}$ that describes the material-filled
region and that we want to find through the optimization problem. In this case,
the optimization problem becomes combinatorial, and very expensive to solve.
Instead, we use an approach called Solid Isotropic Material with Penalization,
or SIMP. @cite Bendse2004

The SIMP method is based on an idea of allowing the material to exist in a
location with a density $\rho$ between 0 and 1. A density of 0 suggests the
material is not there, and it is not a part of the structure, while a density of
1 suggests the material is present. Values between 0 and 1 do not reflect a
design we can create in the real-world, but allow us to turn the combinatorial
problem into a continuous one. One then looks at density values $\rho$,
with the constraint that $0 < \rho_{\min} \leq \rho \leq 1$. The minimum value
$\rho_{\min}$, typically chosen to be around $10^{-3}$, avoids the possibility
of having an infinite strain energy, but is small enough to provide accurate
results.

The straightforward application of the effect of this "density" on the
elasticity of the media would be to simply multiply the stiffness tensor $\mathbf{C}_0$
of the medium by the given density, that is, $\mathbf{C} = \rho \mathbf{C}_0$. However, this
approach often gives optimal solutions where density values are far from both 0
and 1. As one wants to find a real-world solution, meaning the material either
is present or it is not, a penalty is applied to these in-between values. A
simple and effective way to do this is to multiply the stiffness tensor by the
density raised to some integer power penalty parameter $p$, so that
$\mathbf{C} = \rho^p \mathbf{C}_0$. This makes density values farther away from 0 or 1 less
effective. It has been shown that using $p=3$ is sufficiently high to create
'black-and-white' solutions: that is, one gets optimal solutions in which
material is either present or not present at all points.

More material should always provide a structure with a lower strain energy, and so the
inequality constraint can be viewed as an equality where the total volume used
is the maximum volume.

Using this density idea also allows us to reframe the volume constraint on the
optimization problem. Use of SIMP then turns the optimization problem into the
following:

@f[
  \text{minimize  } \int_\Omega \frac{1}{2}\boldsymbol{\sigma}(\rho) : \boldsymbol{\varepsilon}(\rho) d\Omega
@f]
@f[
  \text{subject to } \int_\Omega \rho(x) d\Omega= V_{\max},
@f]
@f[
  0<\rho_{\min}\leq \rho(x) \leq 1,
@f]
@f[

  \nabla \cdot \boldsymbol{\sigma}(\rho) + \mathbf{F} = 0 \quad \text{on } \Omega
@f]
The final constraint, the balance of linear momentum (which we will refer to as the elasticity equation),
 gives a method for finding $\boldsymbol{\sigma}$ and $\boldsymbol{\varepsilon}$ given the density $\rho$.

<h3>Elasticity Equation</h3>
The elasticity equation in the time independent limit reads
@f[
  \nabla \cdot \boldsymbol{\sigma} + \mathbf{F} = \mathbf{0} .
@f]
In the situations we will care about, we will assume that the medium has a linear material response
and in that case, we have that
@f[
  \boldsymbol{\sigma} = \mathbf{C} : \boldsymbol{\varepsilon} = \rho^p \mathbf{C}_0 : \boldsymbol{\varepsilon}(\mathbf{u})
   = \rho^p \mathbf{C}_0 : \left[\frac{1}{2} (\nabla \mathbf{u} + (\nabla \mathbf{u})^T) \right] .
@f]
In everything we will do below, we will always consider the displacement
field $\mathbf{u}$ as the only solution variable, rather than considering
$\mathbf{u}$ and $\boldsymbol{\sigma}$ as solution variables (as is done in mixed
formulations).

Furthermore, we will make the assumption that the material is linear isotropic,
in which case the stress-strain tensor can be expressed in terms of the Lam&eacute;
parameters $\lambda,\mu$ such that
@f{align}
  \boldsymbol{\sigma} &= \rho^p (\lambda \text{tr}(\boldsymbol{\varepsilon}) \mathbf{I} + 2 \mu \boldsymbol{\varepsilon}) , \\
  \sigma_{i,j} &= \rho^p (\lambda \varepsilon_{k,k} \delta_{i,j} + 2 \mu \varepsilon_{i,j}) .
@f}
See step-8 for how this transformation works.

Integrating the objective function by parts gives
@f[
  \int_\Omega \boldsymbol{\sigma}(\rho) : (\nabla \mathbf{u} + (\nabla \mathbf{u}))^T  d\Omega+
  \int_\Omega (\nabla \cdot \boldsymbol{\sigma}(\rho)) \cdot \mathbf{u}  d\Omega=
  \int_{\partial \Omega} \mathbf{t} \cdot \mathbf{u} d\partial\Omega ,
@f]
into which the linear elasticity equation can then be substituted, giving
@f[
  \int_\Omega \boldsymbol{\sigma}(\rho) : (\nabla \mathbf{u} + (\nabla \mathbf{u})^T) d\Omega =
  \int_\Omega \mathbf{F}\cdot \mathbf{u} d\Omega+
  \int_{\partial \Omega} \mathbf{t} \cdot \mathbf{u} d\partial\Omega .
@f]
Because we are assuming no body forces, this simplifies further to
@f[
  \int_\Omega \boldsymbol{\sigma}(\rho) : (\nabla \mathbf{u} + (\nabla \mathbf{u})^T) d\Omega
  = \int_{\partial \Omega} \mathbf{t} \cdot \mathbf{u} d\partial\Omega,
@f]
which is the final form of the governing equation that we'll be considering
from this point forward.

<h3>Making the solution mesh-independent</h3>

Typically, the solutions to topology optimization problems are
mesh-dependent, and as such the problem is ill-posed. This is because
fractal structures are often formed as the mesh is refined further. As the mesh gains
resolution, the optimal solution typically gains smaller and smaller structures.
There are a few competing workarounds to this issue, but the most popular for
first order optimization is the sensitivity filter, while second order
optimization methods tend to prefer use of a density filter.

As the filters affect the gradient and Hessian of the strain energy (i.e., the
objective function), the choice of filter has an effect on the solution of the
problem. The density filter as part of a second order method works by
introducing an unfiltered density, which we refer to as $\varrho$, and then
requiring that the density be a convolution of the unfiltered density:
@f[
  \rho = H(\varrho).
@f]
Here, $H$ is an operator so that $\rho(\mathbf{x})$ is some kind of average of
the values of $\varrho$ in the area around $\mathbf{x}$ -- i.e., it is a smoothed
version of $\varrho$.

This prevents checkerboarding; the radius of the filter allows the user to
define an effective minimal beam width for the optimal structures we seek to
find.

<div style="text-align:center;">
  <img src="https://www.dealii.org/images/steps/developer/step-79.checkerboard.png"
       alt="Checkerboarding occurring in an MBB Beam">
</div>

<h3>Complete Problem Formulation</h3>

The minimization problem is now
@f[
  \min_{\rho,\varrho,\mathbf{u}} \int_{\partial\Omega} \mathbf{u} \cdot \mathbf{t} d\partial\Omega
@f]
@f[
  \text{subject to   } \rho = H(\varrho)
@f]
@f[
  \int_\Omega \rho^p \left(\frac{\mu}{2}\left(\boldsymbol{\varepsilon}(\mathbf{v}):
  \boldsymbol{\varepsilon}(\mathbf{u})) \right) + \lambda \left( \nabla \cdot \mathbf{u} \nabla
  \cdot \mathbf{v} \right)  \right) d\Omega = \int_{\partial \Omega} \mathbf{v} \cdot
  \mathbf{t} d\partial\Omega
@f]
@f[
  \int_\Omega \rho d\Omega= V
@f]
@f[
  0\leq \varrho \leq 1
@f]

The inequality constraints are dealt with by first introducing slack variables,
and second using log barriers to ensure that we obtain an interior-point
method. The penalty parameter is going to be $\alpha$, and the following slack
variables are
<ol>
    <li> $s_1$ - a slack variable corresponding to the lower bound</li>
    <li> $s_2$ - a slack variable corresponding to the upper bound.</li>
</ol>
This now gives the following problem:
@f[
  \min_{\rho,\varrho,\mathbf{u}, s_1, s_2} \int_{\partial\Omega} \mathbf{u} \cdot
  \mathbf{t} d\partial\Omega- \alpha \int_\Omega \left(\log(s_1) + \log(s_2)\right) d\Omega
@f]
@f[
  \text{subject to   } \rho = H(\varrho)
@f]
@f[
  \int_\Omega \rho^p \left(\frac{\mu}{2}\left(\boldsymbol{\varepsilon}(\mathbf{v}):
  \boldsymbol{\varepsilon}(\mathbf{u})) \right) + \lambda \left( \nabla \cdot \mathbf{u} \nabla
  \cdot \mathbf{v} \right)  \right) d\Omega = \int_{\partial \Omega} \mathbf{v} \cdot
  \mathbf{t} d\partial\Omega
@f]
@f[
  \int_\Omega \rho d\Omega = V
@f]
@f[
  \varrho = s_1
@f]
@f[
  1-\varrho = s_2
@f]

With these variables in place, we can then follow the usual approach to solving
constrained optimization problems: We introduce a Lagrangian in which we combine
the objective function and the constraints by multiplying the constraints by
Lagrange multipliers. Specifically, we will use the following symbols for the
Lagrange multipliers for the various constraints:
<ol>
    <li> $\mathbf{y}_1 $: a Lagrange multiplier corresponding to the
    elasticity constraint, </li>
    <li> $y_2$: a Lagrange multiplier corresponding to the convolution
    filter constraint, </li>
    <li> $z_1$: a Lagrange multiplier corresponding to the lower slack variable, and </li>
    <li> $z_2$: a Lagrange multiplier corresponding to the upper slack variable. </li>
</ol>
With these variables, the Lagrangian function reads as follows:

@f{align}{
  \mathcal{L} =& \int_{\partial\Omega} \mathbf{u} \cdot \mathbf{t} d\partial\Omega
   - \alpha \int_\Omega \left(\log(s_1) + \log(s_2)\right) d\Omega-  \int_\Omega
   \rho^p \left(\frac{\mu}{2}\left(\boldsymbol{\varepsilon}(\mathbf{y}_1):\boldsymbol{\varepsilon}(\mathbf{u}))
   \right) + \lambda \left( \nabla \cdot \mathbf{u} \nabla \cdot \mathbf{y}_1
   \right)\right) d\Omega - \int_{\partial \Omega} \mathbf{y}_1 \cdot \mathbf{t} d\partial\Omega  \\
   & -\int_\Omega y_2 (\rho - H(\varrho)) d\Omega - \int_\Omega z_1 (\varrho-s_1) d\Omega
   - \int_\Omega z_2 (1 - s_2 -\varrho) d\Omega
@f}

The solution of the optimization problem then needs to satisfy what are known as
the [Karush-Kuhn-Tucker (KKT) conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions):
The derivatives of the Lagrangian with respect to all of its arguments need to be equal to zero, and because we have
inequality constraints, we also have "complementarity" conditions. Since we
here have an infinite-dimensional problem, these conditions all involve
directional derivatives of the Lagrangian with regard to certain test
functions -- in other words, all of these conditions have to be stated in weak
form as is typically the basis for finite element methods anyway.

The barrier method allows us to initially weaken the "complementary slackness"
as required by the typical KKT conditions. Typically, we would require that
$s_i z_i = 0$, but the barrier formulations give KKT conditions where
$s_i z_i = \alpha$, where $\alpha$ is our barrier parameter. As part of the
barrier method, this parameter must be driven close to 0 to give a good
approximation of the original problem.

In the following, let us state all of these conditions where
$d_{\{\bullet\}}$ is a test function that is naturally paired with variational
derivatives of the Lagrangian with respect to the $\{\bullet\}$ function.
For simplicity, we introduce $\Gamma$ to indicate the portion of the boundary
where forces are applied, and Neumann boundary conditions are used.

<ol>
<li> Stationarity:
@f[
  \int_\Omega  - d_\rho y_2 + p\rho^{p-1}d_\rho \left[\lambda
  (\nabla \cdot \mathbf{y}_1) (\nabla \cdot \mathbf{u}) +
  \mu \boldsymbol{\varepsilon}(\mathbf{u}):\boldsymbol{\varepsilon}(\mathbf{y}_1)\right] d\Omega=0\;\;
  \forall d_\rho
@f]
@f[
  \int_\Gamma \mathbf d_\mathbf{u} \cdot \mathbf{t} d\partial\Omega+ \int_\Omega p\rho^{p}
  \left[\lambda (\nabla \cdot \mathbf d_\mathbf{u})( \nabla \cdot \mathbf{y}_1)
  + \mu \boldsymbol{\varepsilon}(\mathbf d_\mathbf{u}):\boldsymbol{\varepsilon}(\mathbf{y}_1)\right] d\Omega=0\;\;
  \forall \mathbf{d}_\mathbf{u}
@f]
@f[
  \int_\Omega -d_\varrho z_1 + d_\varrho z_2 + H(d_\varrho)y_2 d\Omega= 0\;\;\forall
  d_\varrho

@f]
</li>
<li> Primal Feasibility:
@f[
  \int_\Omega \rho^{p}\lambda (\nabla \cdot \mathbf d_{\mathbf{y}_1})
  (\nabla \cdot \mathbf{u}) +  \rho^{p}\mu  \boldsymbol{\varepsilon}(\mathbf
  d_{\mathbf{y}_1}) : \boldsymbol{\varepsilon}(\mathbf{u}) d\Omega - \int_\Gamma \mathbf
  d_{\mathbf{y}_1} \cdot \mathbf{t} d\partial\Omega =0 \;\;\forall \mathbf{d}_{\mathbf{y}_1}
@f]
@f[
  \int_\Omega d_{z_1}(\varrho - s_1) d\Omega = 0\;\;\forall d_{z_1}
@f]
@f[
  \int_\Omega d_{z_z}(1-\varrho-s_2) d\Omega = 0\;\;\forall d_{z_2}
@f]
@f[
  \int_\Omega d_{y_2}(\rho - H(\varrho)) d\Omega = 0\;\;\forall d_{y_2}
@f]
</li>
<li>Complementary Slackness:
@f[
  \int_\Omega d_{s_1}(s_1z_1 - \alpha) d\Omega = 0 \;\;\forall d_{s_1} ,\;\;\;
  \alpha \to 0
@f]
@f[
  \int_\Omega d_{s_2}(s_2z_2 - \alpha) d\Omega = 0  \;\;\forall d_{s_2} ,\;\;\;
  \alpha \to 0
@f]
</li>
<li> Dual Feasibility:
@f[
  s_{1,i},s_{2,i},z_{1,i},z_{2,i} \geq 0 \;\;\;\; \forall i
@f]
</li>
</ol>

<h3>Solution procedure</h3>

The optimality conditions above are, in addition to being convoluted, of a kind
that is not easy to solve: They are generally nonlinear, and some of the
relationships are also inequalities. We will address the nonlinearity using a
Newton method to compute search directions, and come back to how to deal with
the inequalities below when talking about step length procedures.

Newton's method applied to the equations above results in the system of equations
listed below.
Therein, variational derivatives with respect to the $\{\bullet\}$ variable are
taken in the $c_{\{\bullet\}}$ direction.

<ol>
<li> Stationarity: These equations ensure we are at a critical point of the
objective function when constrained.

Equation 1
@f{align}{
  &\int_\Omega-d_\rho c_{y_2} + p(p-1) \rho^{p-2} d_\rho c_\rho [\lambda \nabla
  \cdot \mathbf{y}_1 \nabla \cdot \mathbf{u} +  \mu  \boldsymbol{\varepsilon}(\mathbf{u})
  \boldsymbol{\varepsilon}(\mathbf{y}_1)]
  + p \rho^{p-1} d_\rho[\lambda \nabla \cdot
  \mathbf{c}_{\mathbf{y}_1} \nabla \cdot \mathbf{u} +   \mu  \boldsymbol{\varepsilon}
  (\mathbf{u}) \boldsymbol{\varepsilon}(\mathbf{c}_{\mathbf{y}_1})]  +  p \rho^{p-1} d_\rho
  [\lambda \nabla \cdot {\mathbf{y}_1} \nabla \cdot \mathbf{c}_\mathbf{u} +
  \mu  \boldsymbol{\varepsilon}(\mathbf{c}_\mathbf{u}) \boldsymbol{\varepsilon}(\mathbf{y}_1)] d\Omega \\
  &= -\int_\Omega -d_\rho z_1 + d_\rho z_2 - d_\rho y_2 + p\rho^{p-1}d_\rho
[\lambda \nabla \cdot \mathbf{y}_1 \nabla \cdot \mathbf{u} + \mu \boldsymbol{\varepsilon}
(\mathbf{u})\boldsymbol{\varepsilon}(\mathbf{y}_1)] d\Omega
@f}

Equation 2
@f{align}{
  &\int_\Omega p \rho^{p-1} c_\rho [\lambda \nabla \cdot {\mathbf{y}_1} \nabla
  \cdot \mathbf{d}_\mathbf{u} +  \mu  \boldsymbol{\varepsilon}(\mathbf{d}_\mathbf{u})
  \boldsymbol{\varepsilon}(\mathbf{y})] + \rho^{p} [\lambda \nabla \cdot
  \mathbf{c}_{\mathbf{y}_1} \nabla \cdot \mathbf{d}_\mathbf{u} +  \mu
  \boldsymbol{\varepsilon}(\mathbf{d}_\mathbf{u})\boldsymbol{\varepsilon}(\mathbf{c}_{\mathbf{y}_1})] d\Omega \\
  &= -\int_\Gamma \mathbf{d}_\mathbf{u} \cdot \mathbf{t} -\int_\Omega \rho^{p}
  [\lambda \nabla \cdot \mathbf{y} \nabla \cdot \mathbf{d}_\mathbf{u} + \mu
  \boldsymbol{\varepsilon}(d_\mathbf{u})\boldsymbol{\varepsilon}(\mathbf{y}_1)] d\Omega
@f}

Equation 3
@f[
  \int_\Omega  - d_\varrho c_{z_1} +d_\varrho c_{z_2}  + H(d_\varrho)c_{y_2}  d\Omega =
  -\int_\Omega -d_\varrho z_1 + d_\varrho z_2 + H(d_\varrho)y_2 d\Omega
@f]
</li>

<li> Primal Feasibility: These equations ensure the equality constraints
are met.

Equation 4
@f{align}{
  &\int_\Omega p \rho^{p-1} c_p[\lambda \nabla \cdot
  \mathbf{d}_{\mathbf{y}_1} \nabla \cdot \mathbf{u} +  \mu
  \boldsymbol{\varepsilon}(\mathbf{u}) \boldsymbol{\varepsilon}(\mathbf{d}_{\mathbf{y}_1})] +
  \rho^{p}[\lambda \nabla \cdot \mathbf{d}_{\mathbf{y}_1} \nabla \cdot
  \mathbf{c}_\mathbf{u} +  \mu  \boldsymbol{\varepsilon}(\mathbf{c}_\mathbf{u})
  \boldsymbol{\varepsilon}(\mathbf{d}_{\mathbf{y}_1})] d\Omega \\
  &= -\int_\Omega \rho^{p}[\lambda \nabla \cdot \mathbf{d}_{\mathbf{y}_1} \nabla
  \cdot \mathbf{u} + \mu  \boldsymbol{\varepsilon}(\mathbf{u}) \boldsymbol{\varepsilon}
  (\mathbf{d}_{\mathbf{y}_1})]  + \int_\Gamma  \mathbf{d}_{\mathbf{y}_1}
  \cdot \mathbf{t} d\partial\Omega
@f}

Equation 5
@f[
  -\int_\Omega d_{z_1}(c_\varrho - c_{s_1}) d\Omega=\int_\Omega d_{z_1} (\varrho - s_1) d\Omega
@f]

Equation 6
@f[
  -\int_\Omega d_{z_2}(-c_\varrho-c_{s_2}) d\Omega= \int_\Omega d_{z_2} (1-\varrho-s_2) d\Omega
@f]

Equation 7
@f[
  -\int_\Omega   d_{y_2}(c_\rho - H(c_\varrho)) d\Omega=\int_\Omega d_{y_2}
  (\rho - H(\varrho)) d\Omega
@f]
</li>

<li>Complementary Slackness: These equations essentially ensure the barrier
is met - in the final solution, we need $s^T z = 0$.

Equation 8
@f[
  \int_\Omega d_{s_1}(c_{s_1}z_1/s_1 +  c_{z_1} ) d\Omega=-\int_\Omega d_{s_1}
  (z_1 - \alpha/s_1) d\Omega ,\;\;\; \alpha \to 0
@f]

Equation 9
@f[
  \int_\Omega d_{s_2} (c_{s_2}z_2/s_2 + c_{z_2} ) d\Omega=-\int_\Omega d_{s_2}
  (z_2 - \alpha/s_2)  d\Omega,\;\;\; \alpha \to 0
@f]
</li>

<li>Dual Feasibility: The Lagrange multiplier on slacks and slack variables must be kept
greater than 0. (This is the only part not implemented in the
`SANDTopOpt::assemble_system()` function.)
@f[
  s,z \geq 0
@f]
</li>
</ol>



<h3>Discretization</h3>
We use a quadrilateral mesh with $Q_1$ elements to discretize the displacement and
displacement Lagrange multiplier. Piecewise constant $DGQ_0$ elements are used
to discretize the density, unfiltered density, density slack variables, and
multipliers for the slack variables and filter constraint.

<h3>Nonlinear Algorithm</h3>

While most of the discussion above follows traditional and well-known approaches
to solving nonlinear optimization problems, it turns out that the problem is
actually quite difficult to solve in practice. In particular, it is quite
nonlinear and an important question is not just to find search directions
$c_{\{\bullet\}}$ as discussed above based on a Newton method, but that one needs to
spend quite a lot of attention to how far one wants to go in this direction.
This is often called "line search" and comes down to the question of how to
choose the step length $\alpha_k \in (0,1]$ so that we move from the current
iterate $\mathbf{x}_k$ to the next iterate $\mathbf{x}_{k+1}=\mathbf{x}_k+\alpha_k \mathbf{x}_k$
in as efficient a way as possible. It is well understood that we need to eventually choose
$\alpha_k=1$ to realize the Newton's method's quadratic convergence; however,
in the early iterations, taking such a long step might actually make things
worse, either by leading to a point that has a worse objective function or at
which the constraints are satisfied less well than they are at $\mathbf{x}_k$.

Very complex algorithms have been proposed to deal with this issue
@cite Nocedal2009 @cite Waechter2005. Here, we implement a watchdog-search
algorithm @cite Nocedal2006. When discussing this algorithm, we will use the
vector $\mathbf{x}$ to represent all primal variables - the filtered and
unfiltered densities, slack variables and displacement - and use the vector
$\mathbf{y}$ to represent all of the dual vectors. The (incremental) solution to the nonlinear
system of equations stated above will now be referred to as $\Delta \mathbf{x}$ and $\Delta
\mathbf{y}$ instead of $c_{\{\bullet\}}$. A merit function (explained in more detail later)
is referred to here as $\phi(\mathbf{x,\mathbf{y}})$.

The watchdog algorithm applied to a subproblem with a given barrier parameter
works in the following way: First, the current iteration is saved as a
"watchdog" state, and the merit of the watchdog state is recorded.
A maximal feasible Newton step is then taken. If the merit sufficiently
decreased from the first step, this new step is accepted. If not, another
maximal feasible Newton step is taken, and the merit is again compared to the
watchdog merit.
If after some number (typically between 5 and 8) of Newton steps, the merit did
not adequately decrease, the algorithm takes a scaled Newton step from either
the watchdog state or the last iteration that guarantees
a sufficient decrease of the merit, and that step is accepted. Once a step is
accepted, the norm of the KKT error is measured, and if it is sufficiently
small, the barrier value is decreased. If it is not sufficiently small, the
last accepted step is taken to be the new watchdog step, and the process is
repeated.


Above, the "maximal feasible step" is a scaling of the Newton step in both the
primal and dual variables given by

@f[
  \beta^\mathbf{y} = \min\{1,\max \beta \text{ such that }\left(\mathbf{z}_{k+i}
   + \beta^\mathbf{z}_{k+i} \Delta \mathbf{z}_{k+i}\right)_j \geq \zeta
   \mathbf{z}_{k+i,j} \forall j\}
@f]
@f[
  \beta^\mathbf{x} = \min\{1,\max \beta \text{ such that }\left(\mathbf{s}_{k+i}
   + \beta^\mathbf{s}_{k+i} \Delta \mathbf{s}_{k+i}\right)_j \geq \zeta
   \mathbf{s}_{k+i,j} \forall j\}
@f]

Above, $\zeta$ is the "fraction to boundary" that is allowed on any step.
Because the derivatives become ill-conditioned near the boundary, this technique
stands in for a [trust region](https://en.wikipedia.org/wiki/Trust_region) and is
necessary to ensure good approximations in
the future. $\zeta$ is taken to be $\max\{0.8, 1-\alpha\}$, which allows
movement closer to the boundary as the barrier becomes smaller. In the future,
when implementing the LOQO algorithm for barrier reduction, this must be kept
to 0.8 as the barrier parameter can vary wildly.

Separately, we need to deal with the log-barrier that we have used to enforce
the positivity constraint on the slack variables $s_1,s_2$: In the statement of
the final optimization problem we solve, we have added the term
@f[
  -\alpha \int_\Omega (\log(s_1) + \log(s_2)) d\Omega.
@f]
The question is how we should choose the penalty factor $\alpha$. As with all
penalty methods, we are in reality only interested in the limit as
$\alpha\to 0$, since this is then the problem we really wanted to solve,
subject to the positivity constraints on the slack variables. On the other hand,
we need to choose $\alpha$ large enough to make the problem solvable in
practice. Actual implementations therefore start with a larger value of
$\alpha$ and gradually decrease it as the outer iterations proceed.

In the monotone method implemented here, the barrier parameter is updated
whenever some level of convergence is achieved at the current barrier parameter.
We use the $l_\infty$ norm of the KKT conditions to check for convergence at
each barrier size. The requirement is that
$\|KKT\|_{l_\infty} < c \cdot \alpha$ where $c$ is a constant over any
barrier size and $\alpha$ is the barrier parameter. This forces better
convergence in later iterations, and is the same requirement as is used in
[IPOPT](https://coin-or.github.io/Ipopt/) (an open source software package for
large-scale nonlinear optimization).

Here, the barrier is reduced linearly at larger values, and superlinearly at
smaller values. At larger values, it is multiplied by a constant (around 0.6),
and at lower values the barrier value is replaced by the barrier value raised
to some exponent (around 1.2). This method has proven to be effective at keeping
 the subproblem solvable at large barrier values, while still allowing
 superlinear convergence at smaller barrier values. In practice, this looks like
 the following:
@f[
  \alpha_{k+1} = \min\{\alpha_k^{1.2},0.6\alpha_k\}
@f]

While taking large steps at reducing the barrier size when convergence is
reached is widely used, more recent research has shown that it is typically faster
to use algorithms that adaptively update barrier each iteration, i.e., methods in which
we use concrete criteria at the end of each iteration to determine what the
penalty parameter should be in the next iteration, rather than using reduction
factors that are independent of the current solution. That said, such methods
are also more complicated and we will not do this here.

<h3>Merit %Function</h3>

The algorithm outlined above makes use of a "merit function". Merit functions
are used to determine whether a step from $x_k$ to a proposed point $x_{k+1}$ is
beneficial. In unconstrained optimization problems, one can simply check this
with the objective function we try to minimize, and typically uses conditions such
as the [Wolfe and Goldstein conditions](https://en.wikipedia.org/wiki/Wolfe_conditions).

In constrained optimization problems, the question is how to balance reduction
in the objective function against a possible increase in the violation of
constraints: A proposed step might make the objective function smaller but be
further away from the set of points that satisfy the constraints -- or the other
way around. This trade-off is typically resolved by using a merit function that
combines the two criteria.

Here, we use an exact $l_1$ merit function to test the steps:
@f{align}{
  \phi(\mathbf{x},\mathbf{y}) =& \int_{\partial \Omega} \mathbf{u}\cdot
  \mathbf{t} d\partial\Omega- \alpha \int_\Omega (\log(s_1) + \log(s_2)) + p \sum_i\left|
  \int_\Omega y_{2,i}(H(\varrho) - \rho) d\Omega \right| \\
  & + p \sum_i\left| \int_{\partial \Omega} \mathbf{y}_{1,i}\cdot \mathbf{t}  d\partial\Omega
  - \int_\Omega \rho^p[\lambda \nabla \cdot \mathbf{u} \nabla \cdot \mathbf{y}_{1,i}
  + \mu \boldsymbol{\varepsilon}{\mathbf{u}}\boldsymbol{\varepsilon}{\mathbf{y}_{1,i}}] d\Omega \right|
  + p \sum_i\left| \int_\Omega z_{1,i}(s_1 - \varrho) d\Omega\right|
  + p \sum_i\left| \int_\Omega z_{2,i}(1-\varrho - s_2) d\Omega\right|
@f}

Here, $p$ is a penalty parameter. This merit function being exact means that
there exists some $p_0$ so that for any $p > p_0$, the merit function has its
minima at the same location as the original problem. This penalty parameter is
updated (by recommendation of Nocedal and Wright @cite Benson2002) as follows:
@f[
  p > \frac{\frac{1}{2} \mathbf{x}^T \cdot \mathbf{H} \cdot \mathbf{x} - \mathbf{x}^T \cdot \nabla f}{\|c_i\|_{l_\infty}}
  \quad , i \in \mathcal{E},
@f]
where $\mathbf{H}$ is the Hessian of the objective function, $\mathbf{x}$ is a vector of our
decision (primal) variables, $f$ is the objective function, and $c_i$ is the error on a
current equality constraint.

Our use of this method is partially due to already having most of the necessary
parts calculated in finding the right hand side, but also the use of an exact
merit function ensures that it is minimized in the same location as the overall
problem. Recent research has shown that one can replace merit functions by what
are called "filter methods", and one should consider using these instead as they
prove to be more efficient.


examples/step-79/doc/results.dox
<h1>Results</h1>
<h3>Test Problem</h3>
The algorithms used above are tested against a traditional topology optimization
 problem called the Messerschmitt-Bolkow-Blohm Beam (MBB Beam).

This problem considers the optimal 2-d structure that can be built on a
rectangle 6 units wide, and 1 unit tall. The bottom corners are fixed in place
in the $y$ direction using a zero Dirichlet boundary condition, and a downward
force is applied in the center of the top of the beam by enforcing a Neumann
boundary condition. The rest of the boundary is allowed to move, and has no
external force applied, which takes the form of a zero Neumann boundary
condition. In essence, we are asking the following question: How should we
design a bridge in a way so that if the bottom left and bottom right point of
the bridge are on rollers that allow these points to move horizontally but not
vertically, and so that the displacement in response to the vertical force in
the center is minimal.

While the total volume of the domain is 6 units, 3 units of material are allowed for
the structure. Because of the symmetry of the problem, it could be posed on a
rectangle of width 3 and height 1 by cutting the original domain in half, and
using zero Dirichlet boundary conditions in the $x$ direction along the cut
edge. That said, symmetry of the solution is a good indicator that the program
is working as expected, so we solved the problem on the whole domain,
as shown below. @cite Bendse2004

<div style="text-align:center;">
  <img src="https://www.dealii.org/images/steps/developer/step-79.mbbgeometry.png"
       alt="The MBB problem domain and boundary conditions">
</div>


Using the program discussed above, we find the minimum volume of the MBB Beam and the
individual components of the solution look as follows:

<div class="onecolumn" style="width: 80%; text-align: center;">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-79.filtereddensity.png"
         alt="Filtered Density Solution">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-79.unfiltereddensity.png"
         alt="Unfiltered Density Solution">
  </div>
</div>


These pictures show that what we find here is in accordance with what one
typically sees in other publications on the topic @cite Bendse2004. Maybe more interestingly, the
result looks like a truss bridge (except that we apply the load at the top of
the trusses, rather than the bottom as in real truss bridges, akin to a "deck
truss" bridge), suggesting that the designs that have been used in bridge-
building for centuries are indeed based on ideas we can now show to be optimal
in some sense.


<h4>Possibilities for extensions</h4>

The results shown above took around 75 iterations to find, which is quite
concerning given the expense in solving the large linear systems in each
iteration. Looking at the evolution, it does look as though the convergence has
moments of happening quickly and moments of happening slowly. We believe this to
be due to both a lack of precision on when and how to decrease the boundary
values, as well as our choice of merit function being sub-optimal. In the future,
a LOQO barrier update replacing the monotone reduction, as well as a Markov
Filter in place of a merit function will decrease the number of necessary
iterations significantly.

The barrier decrease is most sensitive in the middle of the convergence, which
is problematic, as it seems like we need it to decrease quickly, then slowly,
then quickly again.

Secondly, the linear solver used here is just the sparse direct solver based on
the SparseDirectUMFPACK class. This works reasonably well on small problems,
but the formulation of the optimization problem detailed above has quite a large
number of variables and so the linear problem is not only large but also has a
lot of nonzero entries in many rows, even on meshes that overall are still
relatively coarse. As a consequence, the solver time dominates the
computations, and more sophisticated approaches at solving the linear system
are necessary.


examples/step-8/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>


In real life, most partial differential equations are really systems
of equations. Accordingly, the solutions are usually
vector-valued. The deal.II library supports such problems (see the
extensive documentation in the @ref vector_valued module), and we will show
that that is mostly rather simple. The only more complicated problems
are in assembling matrix and right hand side, but these are easily
understood as well.

@dealiiVideoLecture{19}

In this tutorial program we will want to solve the
<a href="https://en.wikipedia.org/wiki/Linear_elasticity">elastic equations</a>.
They are an extension to Laplace's equation with a vector-valued solution that
describes the displacement in each space direction of a rigid body
which is subject to a force. Of course, the force is also
vector-valued, meaning that in each point it has a direction and an
absolute value.

One can write the elasticity equations in a number of ways. The one that shows
the symmetry with the Laplace equation in the most obvious way is to write it
as
@f[
  -
  \text{div}\,
  ({\mathbf C} \nabla \mathbf{u})
  =
  \mathbf f,
@f]
where $\mathbf u$ is the vector-valued displacement at each point,
$\mathbf f$ the force, and ${\mathbf C}$ is a rank-4 tensor (i.e., it has four
indices) that encodes the stress-strain relationship -- in essence,
it represents the
<a href="https://en.wikipedia.org/wiki/Hooke%27s_law">"spring constant"</a> in
Hookes law that relates the displacement to the forces. ${\mathbf C}$ will, in many
cases, depend on $\mathbf x$ if the body whose deformation we want to
simulate is composed of different materials.

While the form of the equations above is correct, it is not the way
they are usually derived. In truth, the gradient of the displacement
$\nabla\mathbf u$ (a matrix) has no physical meaning whereas its
symmetrized version,
@f[
\varepsilon(\mathbf u)_{kl} =\frac{1}{2}(\partial_k u_l + \partial_l u_k),
@f]
does and is typically called the "strain". (Here and in the following,
$\partial_k=\frac{\partial}{\partial x_k}$. We will also use the
<a href="https://en.wikipedia.org/wiki/Einstein_notation">Einstein summation
convention</a> that whenever the same index appears twice in an equation,
summation over this index is implied; we will, however, not distinguish
between upper and lower indices.)
With this definition of the strain, the elasticity equations
then read as
@f[
  -
  \text{div}\,
  ({\mathbf C} \varepsilon(\mathbf u))
  =
  \mathbf f,
@f]
which you can think of as the more natural generalization of the Laplace
equation to vector-valued problems. (The form shown first is equivalent to
this form because the tensor ${\mathbf C}$ has certain symmetries, namely that
$C_{ijkl}=C_{ijlk}$, and consequently ${\mathbf C} \varepsilon(\mathbf u)_{kl}
= {\mathbf C} \nabla\mathbf u$.)

One can of course alternatively write these equations in component form:
@f[
  -
  \partial_j (c_{ijkl} \varepsilon_{kl})
  =
  f_i,
  \qquad
  i=1\ldots d.
@f]

In many cases, one knows that the material under consideration is
isotropic, in which case by introduction of the two coefficients
$\lambda$ and $\mu$ the coefficient tensor reduces to
@f[
  c_{ijkl}
  =
  \lambda \delta_{ij} \delta_{kl} +
  \mu (\delta_{ik} \delta_{jl} + \delta_{il} \delta_{jk}).
@f]

The elastic equations can then be rewritten in much simpler a form:
@f[
   -
   \nabla \lambda (\nabla\cdot {\mathbf u})
   -
   (\nabla \cdot \mu \nabla) {\mathbf u}
   -
   \nabla\cdot \mu (\nabla {\mathbf u})^T
   =
   {\mathbf f},
@f]
and the respective bilinear form is then
@f[
  a({\mathbf u}, {\mathbf v}) =
  \left(
    \lambda \nabla\cdot {\mathbf u}, \nabla\cdot {\mathbf v}
  \right)_\Omega
  +
  \sum_{k,l}
  \left(
    \mu \partial_k u_l, \partial_k v_l
  \right)_\Omega
  +
  \sum_{k,l}
  \left(
    \mu \partial_k u_l, \partial_l v_k
  \right)_\Omega,
@f]
or also writing the first term a sum over components:
@f[
  a({\mathbf u}, {\mathbf v}) =
  \sum_{k,l}
  \left(
    \lambda \partial_l u_l, \partial_k v_k
  \right)_\Omega
  +
  \sum_{k,l}
  \left(
    \mu \partial_k u_l, \partial_k v_l
  \right)_\Omega
  +
  \sum_{k,l}
  \left(
    \mu \partial_k u_l, \partial_l v_k
  \right)_\Omega.
@f]

@note As written, the equations above are generally considered to be the right
description for the displacement of three-dimensional objects if the
displacement is small and we can assume that <a
href="http://en.wikipedia.org/wiki/Hookes_law">Hooke's law</a> is valid. In
that case, the indices $i,j,k,l$ above all run over the set $\{1,2,3\}$ (or,
in the C++ source, over $\{0,1,2\}$). However, as is, the program runs in 2d,
and while the equations above also make mathematical sense in that case, they
would only describe a truly two-dimensional solid. In particular, they are not
the appropriate description of an $x-y$ cross-section of a body infinite in
the $z$ direction; this is in contrast to many other two-dimensional equations
that can be obtained by assuming that the body has infinite extent in
$z$-direction and that the solution function does not depend on the $z$
coordinate. On the other hand, there are equations for two-dimensional models
of elasticity; see for example the Wikipedia article on <a
href="http://en.wikipedia.org/wiki/Infinitesimal_strain_theory#Special_cases">plane
strain</a>, <a
href="http://en.wikipedia.org/wiki/Antiplane_shear">antiplane shear</a> and <a
href="http://en.wikipedia.org/wiki/Plane_stress#Plane_stress">plan stress</a>.

But let's get back to the original problem.
How do we assemble the matrix for such an equation? A very long answer
with a number of different alternatives is given in the documentation of the
@ref vector_valued module. Historically, the solution shown below was the only
one available in the early years of the library. It turns out to also be the
fastest. On the other hand, if a few per cent of compute time do not matter,
there are simpler and probably more intuitive ways to assemble the linear
system than the one discussed below but that weren't available until several
years after this tutorial program was first written; if you are interested in
them, take a look at the @ref vector_valued module.

Let us go back to the question of how to assemble the linear system. The first
thing we need is some knowledge about how the shape functions work in the case
of vector-valued finite elements. Basically, this comes down to the following:
let $n$ be the number of shape functions for the scalar finite element of
which we build the vector element (for example, we will use bilinear functions
for each component of the vector-valued finite element, so the scalar finite
element is the <code>FE_Q(1)</code> element which we have used in previous
examples already, and $n=4$ in two space dimensions). Further, let $N$ be the
number of shape functions for the vector element; in two space dimensions, we
need $n$ shape functions for each component of the vector, so $N=2n$. Then,
the $i$th shape function of the vector element has the form
@f[
  \Phi_i({\mathbf x}) = \varphi_{\text{base}(i)}({\mathbf x})\ {\mathbf e}_{\text{comp}(i)},
@f]
where $e_l$ is the $l$th unit vector, $\text{comp}(i)$ is the function that tells
us which component of $\Phi_i$ is the one that is nonzero (for
each vector shape function, only one component is nonzero, and all others are
zero). $\varphi_{\text{base}(i)}(x)$ describes the space dependence of the shape
function, which is taken to be the $\text{base}(i)$-th shape function of the scalar
element. Of course, while $i$ is in the range $0,\ldots,N-1$, the functions
$\text{comp}(i)$ and $\text{base}(i)$ have the ranges $0,1$ (in 2D) and $0,\ldots,n-1$,
respectively.

For example (though this sequence of shape functions is not
guaranteed, and you should not rely on it),
the following layout could be used by the library:
@f{eqnarray*}
  \Phi_0({\mathbf x}) &=&
  \left(\begin{array}{c}
    \varphi_0({\mathbf x}) \\ 0
  \end{array}\right),
  \\
  \Phi_1({\mathbf x}) &=&
  \left(\begin{array}{c}
    0 \\ \varphi_0({\mathbf x})
  \end{array}\right),
  \\
  \Phi_2({\mathbf x}) &=&
  \left(\begin{array}{c}
    \varphi_1({\mathbf x}) \\ 0
  \end{array}\right),
  \\
  \Phi_3({\mathbf x}) &=&
  \left(\begin{array}{c}
    0 \\ \varphi_1({\mathbf x})
  \end{array}\right),
  \ldots
@f}
where here
@f[
  \text{comp}(0)=0, \quad  \text{comp}(1)=1, \quad  \text{comp}(2)=0, \quad  \text{comp}(3)=1, \quad  \ldots
@f]
@f[
  \text{base}(0)=0, \quad  \text{base}(1)=0, \quad  \text{base}(2)=1, \quad  \text{base}(3)=1, \quad  \ldots
@f]

In all but very rare cases, you will not need to know which shape function
$\varphi_{\text{base}(i)}$ of the scalar element belongs to a shape function $\Phi_i$
of the vector element. Let us therefore define
@f[
  \phi_i = \varphi_{\text{base}(i)}
@f]
by which we can write the vector shape function as
@f[
  \Phi_i({\mathbf x}) = \phi_{i}({\mathbf x})\ {\mathbf e}_{\text{comp}(i)}.
@f]
You can now safely forget about the function $\text{base}(i)$, at least for the rest
of this example program.

Now using this vector shape functions, we can write the discrete finite
element solution as
@f[
  {\mathbf u}_h({\mathbf x}) =
  \sum_i \Phi_i({\mathbf x})\ U_i
@f]
with scalar coefficients $U_i$. If we define an analog function ${\mathbf v}_h$ as
test function, we can write the discrete problem as follows: Find coefficients
$U_i$ such that
@f[
  a({\mathbf u}_h, {\mathbf v}_h) = ({\mathbf f}, {\mathbf v}_h)
  \qquad
  \forall {\mathbf v}_h.
@f]

If we insert the definition of the bilinear form and the representation of
${\mathbf u}_h$ and ${\mathbf v}_h$ into this formula:
@f{eqnarray*}
  \sum_{i,j}
    U_i V_j
  \sum_{k,l}
  \left\{
  \left(
    \lambda \partial_l (\Phi_i)_l, \partial_k (\Phi_j)_k
  \right)_\Omega
  +
  \left(
    \mu \partial_l (\Phi_i)_k, \partial_l (\Phi_j)_k
  \right)_\Omega
  +
  \left(
    \mu \partial_l (\Phi_i)_k, \partial_k (\Phi_j)_l
  \right)_\Omega
  \right\}
\\
=
  \sum_j V_j
  \sum_l
  \left(
    f_l,
    (\Phi_j)_l
  \right)_\Omega.
@f}
We note that here and in the following, the indices $k,l$ run over spatial
directions, i.e. $0\le k,l < d$, and that indices $i,j$ run over degrees
of freedom.

The local stiffness matrix on cell $K$ therefore has the following entries:
@f[
  A^K_{ij}
  =
  \sum_{k,l}
  \left\{
  \left(
    \lambda \partial_l (\Phi_i)_l, \partial_k (\Phi_j)_k
  \right)_K
  +
  \left(
    \mu \partial_l (\Phi_i)_k, \partial_l (\Phi_j)_k
  \right)_K
  +
  \left(
    \mu \partial_l (\Phi_i)_k, \partial_k (\Phi_j)_l
  \right)_K
  \right\},
@f]
where $i,j$ now are local degrees of freedom and therefore $0\le i,j < N$.
In these formulas, we always take some component of the vector shape functions
$\Phi_i$, which are of course given as follows (see their definition):
@f[
  (\Phi_i)_l = \phi_i \delta_{l,\text{comp}(i)},
@f]
with the Kronecker symbol $\delta_{nm}$. Due to this, we can delete some of
the sums over $k$ and $l$:
@f{eqnarray*}
  A^K_{ij}
  &=&
  \sum_{k,l}
  \Bigl\{
  \left(
    \lambda \partial_l \phi_i\ \delta_{l,\text{comp}(i)},
            \partial_k \phi_j\ \delta_{k,\text{comp}(j)}
  \right)_K
\\
  &\qquad\qquad& +
  \left(
    \mu \partial_l \phi_i\ \delta_{k,\text{comp}(i)},
        \partial_l \phi_j\ \delta_{k,\text{comp}(j)}
  \right)_K
  +
  \left(
    \mu \partial_l \phi_i\ \delta_{k,\text{comp}(i)},
        \partial_k \phi_j\ \delta_{l,\text{comp}(j)}
  \right)_K
  \Bigr\}
\\
  &=&
  \left(
    \lambda \partial_{\text{comp}(i)} \phi_i,
            \partial_{\text{comp}(j)} \phi_j
  \right)_K
  +
  \sum_l
  \left(
    \mu \partial_l \phi_i,
        \partial_l \phi_j
  \right)_K
  \ \delta_{\text{comp}(i),\text{comp}(j)}
  +
  \left(
    \mu \partial_{\text{comp}(j)} \phi_i,
        \partial_{\text{comp}(i)} \phi_j
  \right)_K
\\
  &=&
  \left(
    \lambda \partial_{\text{comp}(i)} \phi_i,
            \partial_{\text{comp}(j)} \phi_j
  \right)_K
  +
  \left(
    \mu \nabla \phi_i,
        \nabla \phi_j
  \right)_K
  \ \delta_{\text{comp}(i),\text{comp}(j)}
  +
  \left(
    \mu \partial_{\text{comp}(j)} \phi_i,
        \partial_{\text{comp}(i)} \phi_j
  \right)_K.
@f}

Likewise, the contribution of cell $K$ to the right hand side vector is
@f{eqnarray*}
  f^K_j
  &=&
  \sum_l
  \left(
    f_l,
    (\Phi_j)_l
  \right)_K
\\
  &=&
  \sum_l
  \left(
    f_l,
    \phi_j \delta_{l,\text{comp}(j)}
  \right)_K
\\
  &=&
  \left(
    f_{\text{comp}(j)},
    \phi_j
  \right)_K.
@f}

This is the form in which we will implement the local stiffness matrix and
right hand side vectors.

As a final note: in the step-17 example program, we will
revisit the elastic problem laid out here, and will show how to solve it in
%parallel on a cluster of computers. The resulting program will thus be able to
solve this problem to significantly higher accuracy, and more efficiently if
this is required. In addition, in step-20, @ref step_21
"step-21", as well as a few other of the later tutorial programs, we will
revisit some vector-valued problems and show a few techniques that may make it
simpler to actually go through all the stuff shown above, with
FiniteElement::system_to_component_index etc.


examples/step-8/doc/results.dox
<h1>Results</h1>


There is not much to be said about the results of this program, other than
that they look nice. All images were made using VisIt from the
output files that the program wrote to disk. The first two pictures show
the $x$- and $y$-displacements as scalar components:

<table width="100%">
<tr>
<td>
<img src="https://www.dealii.org/images/steps/developer/step-8.x.png" alt="">
</td>
<td>
<img src="https://www.dealii.org/images/steps/developer/step-8.y.png" alt="">
</td>
</tr>
</table>


You can clearly see the sources of $x$-displacement around $x=0.5$ and
$x=-0.5$, and of $y$-displacement at the origin.

What one frequently would like to do is to show the displacement as a vector
field, i.e., vectors that for each point illustrate the direction and magnitude
of displacement. Unfortunately, that's a bit more involved. To understand why
this is so, remember that we have just defined our finite element as a
collection of two  components (in <code>dim=2</code> dimensions). Nowhere have
we said that this is not just a pressure and a concentration (two scalar
quantities) but that the two components actually are the parts of a
vector-valued quantity, namely the displacement. Absent this knowledge, the
DataOut class assumes that all individual variables we print are separate
scalars, and VisIt and Paraview then faithfully assume that this is indeed what it is. In
other words, once we have written the data as scalars, there is nothing in
these programs that allows us to paste these two scalar fields back together as a
vector field. Where we would have to attack this problem is at the root,
namely in <code>ElasticProblem::output_results()</code>. We won't do so here but
instead refer the reader to the step-22 program where we show how to do this
for a more general situation. That said, we couldn't help generating the data
anyway that would show how this would look if implemented as discussed in
step-22. The vector field then looks like this (VisIt and Paraview
randomly select a few
hundred vertices from which to draw the vectors; drawing them from each
individual vertex would make the picture unreadable):

<img src="https://www.dealii.org/images/steps/developer/step-8.vectors.png" alt="">


We note that one may have intuitively expected the
solution to be symmetric about the $x$- and $y$-axes since the $x$- and
$y$-forces are symmetric with respect to these axes. However, the force
considered as a vector is not symmetric and consequently neither is
the solution.


examples/step-9/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>


In this example, our aims are the following:
<ol>
  <li>solve the advection equation $\beta \cdot \nabla u = f$;
  <li>show how we can use multiple threads to get results quicker if we have a
    multi-processor machine;
  <li>develop a simple refinement criterion.
</ol>
While the second aim is difficult to describe in general terms without
reference to the code, we will discuss the other two aims in the
following. The use of multiple threads will then be detailed at the
relevant places within the program. We will, however, follow the
general discussion of the WorkStream approach detailed in the
@ref threads "Parallel computing with multiple processors accessing shared memory"
documentation module.


<h3>Discretizing the advection equation</h3>

In the present example program, we want to numerically approximate the
solution of the advection equation
@f[
  \beta \cdot \nabla u = f,
@f]
where $\beta$ is a vector field that describes the advection direction and
speed (which may be dependent on the space variables if
$\beta=\beta(\mathbf x)$), $f$ is a source
function, and $u$ is the solution. The physical process that this
equation describes is that of a given flow field $\beta$, with which
another substance is transported, the density or concentration of
which is given by $u$. The equation does not contain diffusion of this
second species within its carrier substance, but there are source
terms.

It is obvious that at the inflow, the above equation needs to be
augmented by boundary conditions:
@f[
  u = g \qquad\qquad \mathrm{on}\ \partial\Omega_-,
@f]
where $\partial\Omega_-$ describes the inflow portion of the boundary and is
formally defined by
@f[
  \partial\Omega_-
  =
  \{{\mathbf x}\in \partial\Omega: \beta\cdot{\mathbf n}({\mathbf x}) < 0\},
@f]
and ${\mathbf n}({\mathbf x})$ being the outward normal to the domain at point
${\mathbf x}\in\partial\Omega$. This definition is quite intuitive, since
as ${\mathbf n}$ points outward, the scalar product with $\beta$ can only
be negative if the transport direction $\beta$ points inward, i.e. at
the inflow boundary. The mathematical theory states that we must not
pose any boundary condition on the outflow part of the boundary.

Unfortunately, the equation stated above cannot be solved in a stable way using
the standard finite element method. The problem is that
solutions to this equation possess insufficient regularity
perpendicular to the transport direction: while they are smooth along
the streamlines defined by the "wind field"
$\beta$, they may be discontinuous perpendicular to this
direction. This is easy to understand: what the equation $\beta \cdot
\nabla u = f$ means is in essence that the <i>rate of change of $u$ in
direction $\beta$ equals $f$</i>. But the equation has no implications
for the derivatives in the perpendicular direction, and consequently
if $u$ is discontinuous at a point on the inflow boundary, then this
discontinuity will simply be transported along the streamline of the
wind field that starts at this boundary point.
These discontinuities lead to numerical instabilities that
make a stable solution by a standard continuous finite element discretization
impossible.

A standard approach to address this difficulty is the <em>"streamline-upwind
Petrov-Galerkin"</em> (SUPG) method, sometimes also called the
streamline diffusion method. A good explanation of the method can be
found in @cite elman2005 . Formally, this method replaces the step
in which we derive the the weak form of the differential equation from
the strong form: Instead of multiplying the equation by a test
function $v$ and integrating over the domain, we instead multiply
by $v + \delta \beta\cdot\nabla v$, where $\delta$ is a
parameter that is chosen in the range of the (local) mesh width $h$;
good results are usually obtained by setting $\delta=0.1h$.
(Why this is called "streamline diffusion" will be explained below;
for the moment, let us simply take for granted that this is how we
derive a stable discrete formulation.)
The value for $\delta$ here is small enough
that we do not introduce excessive diffusion, but large enough that the
resulting problem is well-posed.

Using the test functions as defined above, an initial weak form of the
problem would ask for finding a function $u_h$ so that for all test
functions $v_h$ we have
@f[
  (\beta \cdot \nabla u_h, v_h + \delta \beta\cdot\nabla v_h)_\Omega
  =
  (f, v_h + \delta \beta\cdot\nabla v_h)_\Omega.
@f]
However, we would like to include inflow boundary conditions $u=g$
weakly into this problem, and this can be done by requiring that in
addition to the equation above we also have
@f[
  (u_h, w_h)_{\partial\Omega_-}
  =
  (g, w_h)_{\partial\Omega_-}
@f]
for all test functions $w_h$ that live on the boundary and that are
from a suitable test space. It turns out that a suitable space of test
functions happens to be $\beta\cdot {\mathbf n}$ times the traces of
the functions $v_h$ in the test space we already use for the
differential equation in the domain. Thus, we require that for all
test functions $v_h$ we have
@f[
  (u_h, \beta\cdot {\mathbf n} v_h)_{\partial\Omega_-}
  =
  (g, \beta\cdot {\mathbf n} v_h)_{\partial\Omega_-}.
@f]
Without attempting a justification (see again the literature on the finite
element method in general, and the streamline diffusion method in
particular), we can combine the equations for the differential
equation and the boundary values in the following
weak formulation of
our stabilized problem: find a discrete function $u_h$ such that
for all discrete test functions $v_h$ there holds
@f[
  (\beta \cdot \nabla u_h, v_h + \delta \beta\cdot\nabla v_h)_\Omega
  -
  (u_h, \beta\cdot {\mathbf n} v_h)_{\partial\Omega_-}
  =
  (f, v_h + \delta \beta\cdot\nabla v_h)_\Omega
  -
  (g, \beta\cdot {\mathbf n} v_h)_{\partial\Omega_-}.
@f]


One would think that this leads to a system matrix
to be inverted of the form
@f[
  a_{ij} =
  (\beta \cdot \nabla \varphi_i,
   \varphi_j + \delta \beta\cdot\nabla \varphi_j)_\Omega
  -
  (\varphi_i, \beta\cdot {\mathbf n} \varphi_j)_{\partial\Omega_-},
@f]
with basis functions $\varphi_i,\varphi_j$.  However, this is a
pitfall that happens to every numerical analyst at least once
(including the author): we have here expanded the solution
$u_h = \sum_i U_i \varphi_i$, but if we do so, we will have to solve the
problem
@f[
  U^T A = F^T,
@f]
where $U$ is the vector of expansion coefficients, i.e., we have to
solve the transpose problem of what we might have expected naively.

This is a point we made in the introduction of step-3. There, we argued that
to avoid this very kind of problem, one should get in the habit of always
multiplying with test functions <i>from the left</i> instead of from the right
to obtain the correct matrix right away. In order to obtain the form
of the linear system that we need, it is therefore best to rewrite the weak
formulation to
@f[
  (v_h + \delta \beta\cdot\nabla v_h, \beta \cdot \nabla u_h)_\Omega
  -
  (\beta\cdot {\mathbf n} v_h, u_h)_{\partial\Omega_-}
  =
  (v_h + \delta \beta\cdot\nabla v_h, f)_\Omega
  -
  (\beta\cdot {\mathbf n} v_h, g)_{\partial\Omega_-}
@f]
and then to obtain
@f[
  a_{ij} =
  (\varphi_i + \delta \beta \cdot \nabla \varphi_i,
   \beta\cdot\nabla \varphi_j)_\Omega
  -
  (\beta\cdot {\mathbf n} \varphi_i, \varphi_j)_{\partial\Omega_-},
@f]
as system matrix. We will assemble this matrix in the program.


<h3>Why is this method called "streamline diffusion"?</h3>

Looking at the bilinear form mentioned above, we see that the discrete
solution has to satisfy an equation of which the left hand side in
weak form has a domain term of the kind
@f[
  (v_h + \delta \beta\cdot\nabla v_h, \beta \cdot \nabla u_h)_\Omega,
@f]
or if we split this up, the form
@f[
  (v_h, \beta \cdot \nabla u_h)_\Omega
  +
  (\delta \beta\cdot\nabla v_h, \beta \cdot \nabla u_h)_\Omega.
@f]
If we wanted to see what strong form of the equation that would
correspond to, we need to integrate the second term. This yields the
following formulation, where for simplicity we'll ignore boundary
terms for now:
@f[
  (v_h, \beta \cdot \nabla u_h)_\Omega
  -
  \left(v_h, \delta \nabla \cdot \left[\beta \left(\beta \cdot \nabla
  u_h\right)\right]\right)_\Omega
  +
  \text{boundary terms}.
@f]
Let us assume for a moment that the wind field $\beta$ is
divergence-free, i.e., that $\nabla \cdot \beta = 0$. Then applying
the product rule to the derivative of the term in square brackets on
the right and using the divergence-freeness will give us the following:
@f[
  (v_h, \beta \cdot \nabla u_h)_\Omega
  -
  \left(v_h, \delta \left[\beta \cdot \nabla\right] \left[\beta \cdot \nabla
  \right]u_h\right)_\Omega
  +
  \text{boundary terms}.
@f]
That means that the strong form of the equation would be of the sort
@f[
  \beta \cdot \nabla u_h
  -
  \delta
  \left[\beta \cdot \nabla\right] \left[\beta \cdot \nabla
  \right] u_h.
@f]
What is important to recognize now is that $\beta\cdot\nabla$ is the
<em>derivative in direction $\beta$</em>. So, if we denote this by
$\beta\cdot\nabla=\frac{\partial}{\partial \beta}$ (in the same way as
we often write $\mathbf n\cdot\nabla=\frac{\partial}{\partial n}$ for
the derivative in normal direction at the boundary), then the strong
form of the equation is
@f[
  \beta \cdot \nabla u_h
  -
  \delta
  \frac{\partial^2}{\partial\beta^2} u_h.
@f]
In other words, the unusual choice of test function is equivalent to
the addition of term to the strong form that corresponds to a second
order (i.e., diffusion) differential operator in the direction of the wind
field $\beta$, i.e., in "streamline direction". A fuller account would
also have to explore the effect of the test function on boundary
values and why it is necessary to also use the same test function for
the right hand side, but the discussion above might make clear where
the name "streamline diffusion" for the method originates from.


<h3>Why is this method also called "Petrov-Galerkin"?</h3>

A "Galerkin method" is one where one obtains the weak formulation by
multiplying the equation by a test function $v$ (and then integrating
over $\Omega$) where the functions $v$ are from the same space as the
solution $u$ (though possibly with different boundary values). But
this is not strictly necessary: One could also imagine choosing the
test functions from a different set of functions, as long as that
different set has "as many dimensions" as the original set of
functions so that we end up with as many independent equations as
there are degrees of freedom (where all of this needs to be
appropriately defined in the infinite-dimensional case). Methods that
make use of this possibility (i.e., choose the set of test functions
differently than the set of solutions) are called "Petrov-Galerkin"
methods. In the current case, the test functions all have the form
$v+\beta\cdot\nabla v$ where $v$ is from the set of solutions.


<h3>Why is this method also called "streamline-upwind"?</h3>

[Upwind methods](https://en.wikipedia.org/wiki/Upwind_scheme) have a
long history in the derivation of stabilized schemes for advection
equations. Generally, the idea is that instead of looking at a
function "here", we look at it a small distance further "upstream" or "upwind",
i.e., where the information "here" originally came from. This might
suggest not considering $u(\mathbf x)$, but
something like $u(\mathbf x - \delta \beta)$. Or, equivalently upon
integration, we could evaluate $u(\mathbf x)$ and instead consider $v$
a bit downstream: $v(\mathbf x+\delta \beta)$. This would be cumbersome
for a variety of reasons: First, we would have to define what $v$
should be if $\mathbf x + \delta \beta$ happens to be outside
$\Omega$; second, computing integrals numerically would be much more
awkward since we no longer evaluate $u$ and $v$ at the same quadrature
points. But since we assume that $\delta$ is small, we can do a Taylor
expansion:
@f[
  v(\mathbf x + \delta \beta)
  \approx
  v(\mathbf x) + \delta \beta \cdot \nabla v(\mathbf x).
@f]
This form for the test function should by now look familiar.


<h3>Solving the linear system that corresponds to the advection equation</h3>

As the resulting matrix is no longer symmetric positive definite, we cannot
use the usual Conjugate Gradient method (implemented in the
SolverCG class) to solve the system. Instead, we use the GMRES (Generalized
Minimum RESidual) method (implemented in SolverGMRES) that is suitable
for problems of the kind we have here.


<h3>The test case</h3>

For the problem which we will solve in this tutorial program, we use
the following domain and functions (in $d=2$ space dimensions):
@f{eqnarray*}
  \Omega &=& [-1,1]^d \\
  \beta({\mathbf x})
  &=&
  \left(
    \begin{array}{c}2 \\ 1+\frac 45 \sin(8\pi x)\end{array}
  \right),
  \\
  s
  &=&
  0.1,
  \\
  f({\mathbf x})
  &=&
  \left\{
    \begin{array}{ll}
        \frac 1{10 s^d} &
        \mathrm{for}\ |{\mathbf x}-{\mathbf x}_0|<s, \\
        0 & \mathrm{else},
    \end{array}
  \right.
  \qquad\qquad
  {\mathbf x}_0
  =
  \left(
    \begin{array}{c} -\frac 34 \\ -\frac 34\end{array}
  \right),
  \\
  g
  &=&
  e^{5 (1 - |\mathbf x|^2)} \sin(16\pi|\mathbf x|^2).
@f}
For $d>2$, we extend $\beta$ and ${\mathbf x}_0$ by simply duplicating
the last of the components shown above one more time.

With all of this, the following comments are in order:
<ol>
<li> The advection field $\beta$ transports the solution roughly in
diagonal direction from lower left to upper right, but with a wiggle
structure superimposed.
<li> The right hand side adds to the field generated by the inflow
boundary conditions a blob in the lower left corner, which is then
transported along.
<li> The inflow boundary conditions impose a weighted sinusoidal
structure that is transported along with the flow field. Since
$|{\mathbf x}|\ge 1$ on the boundary, the weighting term never gets very large.
</ol>


<h3>A simple refinement criterion</h3>

In all previous examples with adaptive refinement, we have used an
error estimator first developed by Kelly et al., which assigns to each
cell $K$ the following indicator:
@f[
  \eta_K =
  \left(
    \frac {h_K}{24}
    \int_{\partial K}
      [\partial_n u_h]^2 \; d\sigma
  \right)^{1/2},
@f]
where $[\partial n u_h]$ denotes the jump of the normal derivatives
across a face $\gamma\subset\partial K$ of the cell $K$. It can be
shown that this error indicator uses a discrete analogue of the second
derivatives, weighted by a power of the cell size that is adjusted to
the linear elements assumed to be in use here:
@f[
  \eta_K \approx
  C h \| \nabla^2 u \|_K,
@f]
which itself is related to the error size in the energy norm.

The problem with this error indicator in the present case is that it
assumes that the exact solution possesses second derivatives. This is
already questionable for solutions to Laplace's problem in some cases,
although there most problems allow solutions in $H^2$. If solutions
are only in $H^1$, then the second derivatives would be singular in
some parts (of lower dimension) of the domain and the error indicators
would not reduce there under mesh refinement. Thus, the algorithm
would continuously refine the cells around these parts, i.e. would
refine into points or lines (in 2d).

However, for the present case, solutions are usually not even in $H^1$
(and this missing regularity is not the exceptional case as for
Laplace's equation), so the error indicator described above is not
really applicable. We will thus develop an indicator that is based on
a discrete approximation of the gradient. Although the gradient often
does not exist, this is the only criterion available to us, at least
as long as we use continuous elements as in the present
example. To start with, we note that given two cells $K$, $K'$ of
which the centers are connected by the vector ${\mathbf y}_{KK'}$, we can
approximate the directional derivative of a function $u$ as follows:
@f[
  \frac{{\mathbf y}_{KK'}^T}{|{\mathbf y}_{KK'}|} \nabla u
  \approx
  \frac{u(K') - u(K)}{|{\mathbf y}_{KK'}|},
@f]
where $u(K)$ and $u(K')$ denote $u$ evaluated at the centers of the
respective cells. We now multiply the above approximation by
${\mathbf y}_{KK'}/|{\mathbf y}_{KK'}|$ and sum over all neighbors $K'$ of $K$:
@f[
  \underbrace{
    \left(\sum_{K'} \frac{{\mathbf y}_{KK'} {\mathbf y}_{KK'}^T}
                         {|{\mathbf y}_{KK'}|^2}\right)}_{=:Y}
  \nabla u
  \approx
  \sum_{K'}
  \frac{{\mathbf y}_{KK'}}{|{\mathbf y}_{KK'}|}
  \frac{u(K') - u(K)}{|{\mathbf y}_{KK'}|}.
@f]
If the vectors ${\mathbf y}_{KK'}$ connecting $K$ with its neighbors span
the whole space (i.e. roughly: $K$ has neighbors in all directions),
then the term in parentheses in the left hand side expression forms a
regular matrix, which we can invert to obtain an approximation of the
gradient of $u$ on $K$:
@f[
  \nabla u
  \approx
  Y^{-1}
  \left(
    \sum_{K'}
    \frac{{\mathbf y}_{KK'}}{|{\mathbf y}_{KK'}|}
    \frac{u(K') - u(K)}{|{\mathbf y}_{KK'}|}
  \right).
@f]
We will denote the approximation on the right hand side by
$\nabla_h u(K)$, and we will use the following quantity as refinement
criterion:
@f[
  \eta_K = h^{1+d/2} |\nabla_h u_h(K)|,
@f]
which is inspired by the following (not rigorous) argument:
@f{eqnarray*}
  \|u-u_h\|^2_{L_2}
  &\le&
  C h^2 \|\nabla u\|^2_{L_2}
\\
  &\approx&
  C
  \sum_K
  h_K^2 \|\nabla u\|^2_{L_2(K)}
\\
  &\le&
  C
  \sum_K
  h_K^2 h_K^d \|\nabla u\|^2_{L_\infty(K)}
\\
  &\approx&
  C
  \sum_K
  h_K^{2+d} |\nabla_h u_h(K)|^2
@f}


examples/step-9/doc/results.dox
<h1>Results</h1>


The results of this program are not particularly spectacular. They
consist of the console output, some grid files, and the solution on
each of these grids. First for the console output:
@code
Cycle 0:
   Number of active cells:              64
   Number of degrees of freedom:        1681
   Iterations required for convergence: 298
   Max norm of residual:                3.60316e-12
Cycle 1:
   Number of active cells:              124
   Number of degrees of freedom:        3537
   Iterations required for convergence: 415
   Max norm of residual:                3.70682e-12
Cycle 2:
   Number of active cells:              247
   Number of degrees of freedom:        6734
   Iterations required for convergence: 543
   Max norm of residual:                7.19716e-13
Cycle 3:
   Number of active cells:              502
   Number of degrees of freedom:        14105
   Iterations required for convergence: 666
   Max norm of residual:                3.45628e-13
Cycle 4:
   Number of active cells:              1003
   Number of degrees of freedom:        27462
   Iterations required for convergence: 1064
   Max norm of residual:                1.86495e-13
Cycle 5:
   Number of active cells:              1993
   Number of degrees of freedom:        55044
   Iterations required for convergence: 1251
   Max norm of residual:                1.28765e-13
Cycle 6:
   Number of active cells:              3985
   Number of degrees of freedom:        108492
   Iterations required for convergence: 2035
   Max norm of residual:                6.78085e-14
Cycle 7:
   Number of active cells:              7747
   Number of degrees of freedom:        210612
   Iterations required for convergence: 2187
   Max norm of residual:                2.61457e-14
Cycle 8:
   Number of active cells:              15067
   Number of degrees of freedom:        406907
   Iterations required for convergence: 3079
   Max norm of residual:                2.9932e-14
Cycle 9:
   Number of active cells:              29341
   Number of degrees of freedom:        780591
   Iterations required for convergence: 3913
   Max norm of residual:                8.15689e-15
@endcode

Quite a number of cells are used on the finest level to resolve the features of
the solution. Here are the fourth and tenth grids:
<div class="twocolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-9-grid-3.png"
         alt="Fourth grid in the refinement cycle, showing some adaptivity to features."
         width="400" height="400">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-9-grid-9.png"
         alt="Tenth grid in the refinement cycle, showing that the waves are fully captured."
         width="400" height="400">
  </div>
</div>
and the fourth and tenth solutions:
<div class="twocolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-9-solution-3.png"
         alt="Fourth solution, showing that we resolve most features but some
         are sill unresolved and appear blury."
         width="400" height="400">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-9-solution-9.png"
         alt="Tenth solution, showing a fully resolved flow."
         width="400" height="400">
  </div>
</div>
and both the grid and solution zoomed in:
<div class="twocolumn" style="width: 80%">
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-9-solution-3-zoom.png"
         alt="Detail of the fourth solution, showing that we resolve most
         features but some are sill unresolved and appear blury. In particular,
         the larger cells need to be refined."
         width="400" height="400">
  </div>
  <div>
    <img src="https://www.dealii.org/images/steps/developer/step-9-solution-9-zoom.png"
         alt="Detail of the tenth solution, showing that we needed a lot more
         cells than were present in the fourth solution."
         width="400" height="400">
  </div>
</div>

The solution is created by that part that is transported along the wiggly
advection field from the left and lower boundaries to the top right, and the
part that is created by the source in the lower left corner, and the results of
which are also transported along. The grid shown above is well-adapted to
resolve these features. The comparison between plots shows that, even though we
are using a high-order approximation, we still need adaptive mesh refinement to
fully resolve the wiggles.


