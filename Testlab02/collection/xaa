examples/step-1/doc/intro.dox 

[1.x.0] 

[1.x.1] 

[1.x.2] 

Since this is the first tutorial program, let us comment first on how this tutorial and the rest of the deal.II documentation is supposed to work. The documentation for deal.II comes essentially at three different levels: 

- The tutorial: This is a collection of programs that shows how   deal.II is used in practice. It doesn't typically discuss individual   functions at the level of individual arguments, but rather wants to   give the big picture of how things work together. In other words, it   discusses "concepts": what are the building blocks of deal.II and   how are they used together in finite element programs. 

- The manual: This is the documentation of every single class and   every single (member) function in deal.II. You get there if, for   example, you click on the "Main page" or "Classes" tab at the top of   this page. This is the place where you would look up what the second   argument of  [2.x.0]  means,   to give just one slightly obscure example. You need this level of   documentation for when you know what you want to do, but forgot how   exactly the function was named, what its arguments are, or what it   returns. Note that you also get into the manual whenever you read   through the tutorial and click on any of the class or function   names, i.e. the tutorial contains a great many links into the manual   for whenever you need a more detailed description of a function or   class. On the other hand, the manual is not a good place to learn   deal.II since it gives you a microscopic view of things without   telling you how a function might fit into the bigger picture. 

- Modules: These are groups of classes and functions that work   together or have related functionality. If you click on the   "Modules" tab at the top of this page, you end up on a page that   lists a number of such groups. Each module discusses the underlying   principles of these classes; for example, the  [2.x.1]  module   talks about all sorts of different issues related to storing   sparsity patterns of matrices. This is documentation at an   intermediate level: they give you an overview of what's there in a   particular area. For example when you wonder what finite element   classes exist, you would take a look at the  [2.x.2]  module. The   modules are, of course, also cross-linked to the manual (and, at   times, to the tutorial); if you click on a class name, say on   Triangulation, would will also at the very top right under the class   name get a link to the modules this class is a member of if you want   to learn more about its context. 

Let's come back to the tutorial, since you are looking at the first program (or "step") of it. Each tutorial program is subdivided into the following sections: <ol>    [2.x.3]  [1.x.3] This is a discussion of what the program        does, including the mathematical model, and        what programming techniques are new compared to previous        tutorial programs.    [2.x.4]  [1.x.4] An extensively documented listing of the        source code. Here, we often document individual lines, or        blocks of code, and discuss what they do, how they do it, and        why. The comments frequently reference the introduction,        i.e. you have to understand [1.x.5] the program wants to achieve        (a goal discussed in the introduction) before you can        understand [1.x.6] it intends to get there.    [2.x.5]  [1.x.7] The output of the program, with comments and        interpretation. This section also frequently has a subsection        that gives suggestions on how to extend the program in various        direction; in the earlier programs, this is intended to give        you directions for little experiments designed to make your        familiar with deal.II, while in later programs it is more about        how to use more advanced numerical techniques.    [2.x.6]  [1.x.8] The source code stripped of        all comments. This is useful if you want to see the "big        picture" of the code, since the commented version of the        program has so much text in between that it is often difficult        to see the entire code of a single function on the screen at        once.  [2.x.7]  

The tutorials are not only meant to be static documentation, but you should play with them. To this end, go to the  [2.x.8]  directory (or whatever the number of the tutorial is that you're interested in) and type 

[1.x.9] 

The first command sets up the files that describe which include files this tutorial program depends on, how to compile it and how to run it. This command should find the installed deal.II libraries as well that were generated when you compiled and installed everything as described in the [1.x.10] file. If this command should fail to find the deal.II library, then you need to provide the path to the installation using the command 

[1.x.11] 

instead. 

The second of the commands above compiles the sources into an executable, while the last one executes it (strictly speaking,  [2.x.9]  will also compile the code if the executable doesn't exist yet, so you could have skipped the second command if you wanted). This is all that's needed to run the code and produce the output that is discussed in the "Results" section of the tutorial programs. This sequence needs to be repeated in all of the tutorial directories you want to play with. 

When learning the library, you need to play with it and see what happens. To this end, open the  [2.x.10]  source file with your favorite editor and modify it in some way, save it and run it as above. A few suggestions for possibly modifications are given at the end of the results section of this program, where we also provide a few links to other useful pieces of information. 




[1.x.12] 

This and several of the other tutorial programs are also discussed and demonstrated in [1.x.13] on deal.II and computational science. In particular, you can see the steps he executes to run this and other programs, and you will get a much better idea of the tools that can be used to work with deal.II. In particular, lectures 2 and 4 give an overview of deal.II and of the building blocks of any finite element code. ( [2.x.11]  

If you are not yet familiar with using Linux and running things on the command line, you may be interested in watching lectures 2.9 and 2.91. ( [2.x.12]  line and on what happens when compiling programs, respectively. 

Note that deal.II is actively developed, and in the course of this development we occasionally rename or deprecate functions or classes that are still referenced in these video lectures.  For example, the step-1 code shown in video lecture 5 uses a class HyperShellBoundary which was replaced with SphericalManifold class later on. Additionally, as of deal.II version 9.0,  [2.x.13]  now automatically attaches a SphericalManifold to the Triangulation. Otherwise the rest of the lecture material is relevant. 

[1.x.14] 

Let's come back to step-1, the current program. In this first example, we don't actually do very much, but show two techniques: what is the syntax to generate triangulation objects, and some elements of simple loops over all cells. We create two grids, one which is a regularly refined square (not very exciting, but a common starting grid for some problems), and one more geometric attempt: a ring-shaped domain, which is refined towards the inner edge. Through this, you will get to know three things every finite element program will have to have somewhere: An object of type Triangulation for the mesh; a call to the GridGenerator functions to generate a mesh; and loops over all cells that involve iterators (iterators are a generalization of pointers and are frequently used in the C++ standard library; in the context of deal.II, the  [2.x.14]  module talks about them). 

The program is otherwise small enough that it doesn't need a whole lot of introduction. 

 [2.x.15]  




[1.x.15] 

If you are reading through this tutorial program, chances are that you are interested in continuing to use deal.II for your own projects. Thus, you are about to embark on an exercise in programming using a large-scale scientific computing library. Unless you are already an experienced user of large-scale programming methods, this may be new territory for you &mdash; with all the new rules that go along with it such as the fact that you will have to deal with code written by others, that you may have to think about documenting your own code because you may not remember what exactly it is doing a year down the road (or because others will be using it as well), or coming up with ways to test that your program is doing the right thing. None of this is something that we typically train mathematicians, engineers, or scientists in but that is important when you start writing software of more than a few hundred lines. Remember: Producing software is not the same as just writing code. 

To make your life easier on this journey let us point to some resources that are worthwhile browsing through before you start any large-scale programming: 

- The [1.x.16] has a good number of answers to questions about   particular aspects of deal.II, but also to more general questions such as "How   do I debug scientific computing codes?" or "Can I train myself to write code   that has fewer bugs?". 

- You will benefit from becoming a better programmer. An excellent   resource to this end is the book   [Code Complete](https://en.wikipedia.org/wiki/Code_Complete)   by Steve McConnell  [2.x.16]  . It's already   a few years old, with the last edition published in 2004, but it has   lost none of its appeal as a guide to good programming practices,   and some of the principal developers use it as a group reading   project with every generation of their research group members. 

- The [1.x.17]   that provides introductions to many topics that are important to dealing   with software, such as version control, make files, testing, etc. It is   specifically written for scientists and engineers, not for computer   scientists, and has a focus on short, practical lessons. 

- The [1.x.18] has a lot of resources (and interesting blog posts) that   cover many aspects of writing scientific software. 

- The [1.x.19] also has resources on software development, in   particular for parallel computing. In the "Events" section on   that site are recorded tutorials and webinars that cover many   interesting topics. 

- An article on [1.x.20] that gives an introduction to   many of the ways by which you can make sure you are an efficient   programmer writing programs that work. 

As a general recommendation: If you expect to spend more than a few days writing software in the future, do yourself the favor of learning tools that can make your life more productive, in particular debuggers and integrated development environments. ( [2.x.17]  You will find that you will get the time spent learning these tools back severalfold soon by being more productive! Several of the video lectures referenced above show how to use tools such as integrated development environments or debuggers. 


examples/step-1/doc/results.dox 



[1.x.21] 

Running the program produces graphics of two grids (grid-1.svg and grid-2.svg). You can open these with most every web browser -- in the simplest case, just open the current directory in your file system explorer and click on the file. If you like working on the command line, you call your web browser with the file: `firefox grid-1.svg`, `google-chrome grid-1.svg`, or whatever the name of your browser is. If you do this, the two meshes should look like this: 

 [2.x.18]  

The left one, well, is not very exciting. The right one is &mdash; at least &mdash; unconventional. The pictures color-code the "refinement level" of each cell: How many times did a coarse mesh cell have to be subdivided to obtain the given cell. In the left image, this is boring since the mesh was refined globally a number of times, i.e., [1.x.22] cell was refined the same number of times. 

(While the second mesh is entirely artificial and made-up, and certainly not very practical in applications, to everyone's surprise it has found its way into the literature: see  [2.x.19] . Apparently it is good for some things at least.) 




[1.x.23] 

[1.x.24] 

This program obviously does not have a whole lot of functionality, but in particular the  [2.x.20]  function has a bunch of places where you can play with it. For example, you could modify the criterion by which we decide which cells to refine. An example would be to change the condition to this: 

[1.x.25] 

This would refine all cells for which the  [2.x.21] -coordinate of the cell's center is greater than zero (the  [2.x.22]  function that we call by dereferencing the  [2.x.23]  iterator returns a Point<2> object; subscripting  [2.x.24]  would give the  [2.x.25] -coordinate, subscripting  [2.x.26]  the  [2.x.27] -coordinate). By looking at the functions that TriaAccessor provides, you can also use more complicated criteria for refinement. 

In general, what you can do with operations of the form `cell->something()` is a bit difficult to find in the documentation because `cell` is not a pointer but an iterator. The functions you can call on a cell can be found in the documentation of the classes `TriaAccessor` (which has functions that can also be called on faces of cells or, more generally, all sorts of geometric objects that appear in a triangulation), and `CellAccessor` (which adds a few functions that are specific to *cells*). 

A more thorough description of the whole iterator concept can be found in the  [2.x.28]  documentation module. 




[1.x.26] 

Another possibility would be to generate meshes of entirely different geometries altogether. While for complex geometries there is no way around using meshes obtained from mesh generators, there is a good number of geometries for which deal.II can create meshes using the functions in the GridGenerator namespace. Many of these geometries (such as the one used in this example program) contain cells with curved faces: put another way, we expect the new vertices placed on the boundary to lie along a circle. deal.II handles complex geometries with the Manifold class (and classes inheriting from it); in particular, the functions in GridGenerator corresponding to non-Cartesian grids (such as  [2.x.29]  or  [2.x.30]  attach a Manifold object to the part of the triangulation that should be curved (SphericalManifold and CylindricalManifold, respectively) and use another manifold on the parts that should be flat (FlatManifold). See the documentation of Manifold or the  [2.x.31]  "manifold module" for descriptions of the design philosophy and interfaces of these classes. Take a look at what they provide and see how they could be used in a program like this. 

We also discuss a variety of other ways to create and manipulate meshes (and describe the process of attaching Manifolds) in step-49. 




[1.x.27] 

We close with a comment about modifying or writing programs with deal.II in general. When you start working with tutorial programs or your own applications, you will find that mistakes happen: your program will contain code that either aborts the program right away or bugs that simply lead to wrong results. In either case, you will find it extremely helpful to know how to work with a debugger: you may get by for a while by just putting debug output into your program, compiling it, and running it, but ultimately finding bugs with a debugger is much faster, much more convenient, and more reliable because you don't have to recompile the program all the time and because you can inspect the values of variables and how they change. 

Rather than postponing learning how to use a debugger till you really can't see any other way to find a bug, here's the one piece of advice we will provide in this program: learn how to use a debugger as soon as possible. It will be time well invested. ( [2.x.32]  Questions (FAQ) page linked to from the top-level [1.x.28] also provides a good number of hints on debugging deal.II programs. 




[1.x.29] 

It is often useful to include meshes into your theses or publications. For this, it may not be very useful to color-code the cells by refinement level, and to print the cell number onto each cell. But it doesn't have to be that way -- the GridOut class allows setting flags for each possible output format (see the classes in the GridOutFlags namespace) that control how exactly a mesh is plotted. You can of course also choose other output file formats such as VTK or VTU; this is particularly useful for 3d meshes where a 2d format such as SVG is not particular useful because it fixes a particular viewpoint onto the 3d object. As a consequence, you might want to explore other options in the GridOut class. 


examples/step-10/doc/intro.dox 

[1.x.30] 

[1.x.31] 


This is a rather short example which only shows some aspects of using higher order mappings. By  [2.x.33] mapping [2.x.34]  we mean the transformation between the unit cell (i.e. the unit line, square, or cube) to the cells in real space. In all the previous examples, we have implicitly used linear or d-linear mappings; you will not have noticed this at all, since this is what happens if you do not do anything special. However, if your domain has curved boundaries, there are cases where the piecewise linear approximation of the boundary (i.e. by straight line segments) is not sufficient, and you want that your computational domain is an approximation to the real domain using curved boundaries as well. If the boundary approximation uses piecewise quadratic parabolas to approximate the true boundary, then we say that this is a quadratic or  [2.x.35]  approximation. If we use piecewise graphs of cubic polynomials, then this is a  [2.x.36]  approximation, and so on. 




For some differential equations, it is known that piecewise linear approximations of the boundary, i.e.  [2.x.37]  mappings, are not sufficient if the boundary of the exact domain is curved. Examples are the biharmonic equation using  [2.x.38]  elements, or the Euler equations of gas dynamics on domains with curved reflective boundaries. In these cases, it is necessary to compute the integrals using a higher order mapping. If we do not use such a higher order mapping, the order of approximation of the boundary dominates the order of convergence of the entire numerical scheme, irrespective of the order of convergence of the discretization in the interior of the domain. 




Rather than demonstrating the use of higher order mappings with one of these more complicated examples, we do only a brief computation: calculating the value of  [2.x.39]  by two different methods. 




The first method uses a triangulated approximation of the circle with unit radius and integrates a unit magnitude constant function ( [2.x.40] ) over it. Of course, if the domain were the exact unit circle, then the area would be  [2.x.41] , but since we only use an approximation by piecewise polynomial segments, the value of the area we integrate over is not exactly  [2.x.42] . However, it is known that as we refine the triangulation, a  [2.x.43]  mapping approximates the boundary with an order  [2.x.44] , where  [2.x.45]  is the mesh size. We will check the values of the computed area of the circle and their convergence towards  [2.x.46]  under mesh refinement for different mappings. We will also find a convergence behavior that is surprising at first, but has a good explanation. 




The second method works similarly, but this time does not use the area of the triangulated unit circle, but rather its perimeter.  [2.x.47]  is then approximated by half of the perimeter, as we choose the radius equal to one. 




 [2.x.48]  This tutorial shows in essence how to choose a particular mapping for integrals, by attaching a particular geometry to the triangulation (as had already been done in step-1, for example) and then passing a mapping argument to the FEValues class that is used for all integrals in deal.II. The geometry we choose is a circle, for which deal.II already has a class (SphericalManifold) that can be used. If you want to define your own geometry, for example because it is complicated and cannot be described by the classes already available in deal.II, you will want to read through step-53. 


examples/step-10/doc/results.dox 



[1.x.32] 


The program performs two tasks, the first being to generate a visualization of the mapped domain, the second to compute pi by the two methods described. Let us first take a look at the generated graphics. They are generated in Gnuplot format, and can be viewed with the commands 

[1.x.33] 

or using one of the other filenames. The second line makes sure that the aspect ratio of the generated output is actually 1:1, i.e. a circle is drawn as a circle on your screen, rather than as an ellipse. The third line switches off the key in the graphic, as that will only print information (the filename) which is not that important right now. Similarly, the fourth and fifth disable tick marks. The plot is then generated with a specific line width ("lw", here set to 4) and line type ("lt", here chosen by saying that the line should be drawn using the RGB color "black"). 

The following table shows the triangulated computational domain for  [2.x.49] ,  [2.x.50] , and  [2.x.51]  mappings, for the original coarse grid (left), and a once uniformly refined grid (right). 

<div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_ball_0_q1.svg"          alt="Five-cell discretization of the disk."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_ball_1_q1.svg"          alt="20-cell discretization of the disk (i.e., five cells               refined once)."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_ball_0_q2.svg"          alt="Five-cell discretization of the disk with quadratic edges. The               boundary is nearly indistinguishable from the actual circle."          width="400" height="400"          >   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_ball_1_q2.svg"          alt="20-cell discretization with quadratic edges."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_ball_0_q3.svg"          alt="Five-cell discretization of the disk with cubic edges. The               boundary is nearly indistinguishable from the actual circle."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_ball_1_q3.svg"          alt="20-cell discretization with cubic edges."          width="400" height="400">   </div> </div> 

These pictures show the obvious advantage of higher order mappings: they approximate the true boundary quite well also on rather coarse meshes. To demonstrate this a little further, here is part of the upper right quarter circle of the coarse meshes with  [2.x.52]  and  [2.x.53]  mappings, where the dashed red line marks the actual circle: 

<div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_exact_vs_interpolate_q2.svg"          alt="Close-up of quadratic discretization. The distance between the          quadratic interpolant and the actual circle is small."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_exact_vs_interpolate_q3.svg"          alt="Close-up of cubic discretization. The distance between the          cubic interpolant and the actual circle is very small."          width="400" height="400">   </div> </div> 

Obviously the quadratic mapping approximates the boundary quite well, while for the cubic mapping the difference between approximated domain and true one is hardly visible already for the coarse grid. You can also see that the mapping only changes something at the outer boundaries of the triangulation. In the interior, all lines are still represented by linear functions, resulting in additional computations only on cells at the boundary. Higher order mappings are therefore usually not noticeably slower than lower order ones, because the additional computations are only performed on a small subset of all cells. 




The second purpose of the program was to compute the value of pi to good accuracy. This is the output of this part of the program: 

[1.x.34] 






One of the immediate observations from the output is that in all cases the values converge quickly to the true value of  [2.x.54] . Note that for the  [2.x.55]  mapping, we are already in the regime of roundoff errors and the convergence rate levels off, which is already quite a lot. However, also note that for the  [2.x.56]  mapping, even on the finest grid the accuracy is significantly worse than on the coarse grid for a  [2.x.57]  mapping! 




The last column of the output shows the convergence order, in powers of the mesh width  [2.x.58] . In the introduction, we had stated that the convergence order for a  [2.x.59]  mapping should be  [2.x.60] . However, in the example shown, the order is rather  [2.x.61] ! This at first surprising fact is explained by the properties of the  [2.x.62]  mapping. At order [1.x.35], it uses support points that are based on the [1.x.36]+1 point Gauss-Lobatto quadrature rule that selects the support points in such a way that the quadrature rule converges at order 2[1.x.37]. Even though these points are here only used for interpolation of a [1.x.38]th order polynomial, we get a superconvergence effect when numerically evaluating the integral, resulting in the observed high order of convergence. (This effect is also discussed in detail in the following publication: A. Bonito, A. Demlow, and J. Owen: "A priori error estimates for finite element approximations to eigenvalues and eigenfunctions of the Laplace-Beltrami operator", submitted, 2018.) 


examples/step-11/doc/intro.dox 

[1.x.39] 

[1.x.40] 

The problem we will be considering is the solution of Laplace's problem with Neumann boundary conditions only: 

[1.x.41] 

It is well known that if this problem is to have a solution, then the forces need to satisfy the compatibility condition 

[1.x.42] 

We will consider the special case that  [2.x.63]  is the circle of radius 1 around the origin, and  [2.x.64] ,  [2.x.65] . This choice satisfies the compatibility condition. 

The compatibility condition allows a solution of the above equation, but it nevertheless retains an ambiguity: since only derivatives of the solution appear in the equations, the solution is only determined up to a constant. For this reason, we have to pose another condition for the numerical solution, which fixes this constant. 

For this, there are various possibilities: <ol>  [2.x.66]  Fix one node of the discretization to zero or any other fixed value.   This amounts to an additional condition  [2.x.67] . Although this is   common practice, it is not necessarily a good idea, since we know that the   solutions of Laplace's equation are only in  [2.x.68] , which does not allow for   the definition of point values because it is not a subset of the continuous   functions. Therefore, even though fixing one node is allowed for   discretized functions, it is not for continuous functions, and one can   often see this in a resulting error spike at this point in the numerical   solution. 

 [2.x.69]  Fixing the mean value over the domain to zero or any other value. This   is allowed on the continuous level, since  [2.x.70]    by Sobolev's inequality, and thus also on the discrete level since we   there only consider subsets of  [2.x.71] . 

 [2.x.72]  Fixing the mean value over the boundary of the domain to zero or any   other value. This is also allowed on the continuous level, since    [2.x.73] , again by Sobolev's   inequality.  [2.x.74]  We will choose the last possibility, since we want to demonstrate another technique with it. 

While this describes the problem to be solved, we still have to figure out how to implement it. Basically, except for the additional mean value constraint, we have solved this problem several times, using Dirichlet boundary values, and we only need to drop the treatment of Dirichlet boundary nodes. The use of higher order mappings is also rather trivial and will be explained at the various places where we use it; in almost all conceivable cases, you will only consider the objects describing mappings as a black box which you need not worry about, because their only uses seem to be to be passed to places deep inside the library where functions know how to handle them (i.e. in the  [2.x.75]  classes and their descendants). 

The tricky point in this program is the use of the mean value constraint. Fortunately, there is a class in the library which knows how to handle such constraints, and we have used it quite often already, without mentioning its generality. Note that if we assume that the boundary nodes are spaced equally along the boundary, then the mean value constraint 

[1.x.43] 

can be written as 

[1.x.44] 

where the sum shall run over all degree of freedom indices which are located on the boundary of the computational domain. Let us denote by  [2.x.76]  that index on the boundary with the lowest number (or any other conveniently chosen index), then the constraint can also be represented by 

[1.x.45] 

This, luckily, is exactly the form of constraints for which the AffineConstraints class was designed. Note that we have used this class in several previous examples for the representation of hanging nodes constraints, which also have this form: there, the middle vertex shall have the mean of the values of the adjacent vertices. In general, the AffineConstraints class is designed to handle affine constraints of the form 

[1.x.46] 

where  [2.x.77]  denotes a matrix,  [2.x.78]  denotes a vector, and  [2.x.79]  the vector of nodal values. In this case, since  [2.x.80]  represents one homogeneous constraint,  [2.x.81]  is the zero vector. 

In this example, the mean value along the boundary allows just such a representation, with  [2.x.82]  being a matrix with just one row (i.e. there is only one constraint). In the implementation, we will create an AffineConstraints object, add one constraint (i.e. add another row to the matrix) referring to the first boundary node  [2.x.83] , and insert the weights with which all the other nodes contribute, which in this example happens to be just  [2.x.84] . 

Later, we will use this object to eliminate the first boundary node from the linear system of equations, reducing it to one which has a solution without the ambiguity of the constant shift value. One of the problems of the implementation will be that the explicit elimination of this node results in a number of additional elements in the matrix, of which we do not know in advance where they are located and how many additional entries will be in each of the rows of the matrix. We will show how we can use an intermediate object to work around this problem. 

But now on to the implementation of the program solving this problem... 


examples/step-11/doc/results.dox 



[1.x.47] 

This is what the program outputs: 

[1.x.48] 

As we expected, the convergence order for each of the different mappings is clearly quadratic in the mesh size. What  [2.x.85] is [2.x.86]  interesting, though, is that the error for a bilinear mapping (i.e. degree 1) is more than three times larger than that for the higher order mappings; it is therefore clearly advantageous in this case to use a higher order mapping, not because it improves the order of convergence but just to reduce the constant before the convergence order. On the other hand, using a cubic mapping only improves the result further by insignificant amounts, except on very coarse grids. 

We can also visualize the underlying meshes by using, for instance, ParaView. The image below shows initial meshes for different mapping degrees: 

 [2.x.87]  

Clearly, the effect is most pronounced when we go from the linear to quadratic mapping. This is also reflected in the error values given in the table above. The effect of going from quadratic to cubic degree is less dramatic, but still tangible owing to a more accurate description of the circular boundary. 

Next, let's look at the meshes after three global refinements 

 [2.x.88]  

Here, the differences are much less visible, especially for higher order mappings. Indeed, at this refinement level the error values reported in the table are essentially identical between mappings of degrees two and three. 


examples/step-12/doc/intro.dox 

 [2.x.89]  

[1.x.49] 

[1.x.50] 

[1.x.51] 

[1.x.52] 

This example is devoted to the  [2.x.90] discontinuous Galerkin method [2.x.91] , or in short, the DG method. It includes the following topics. <ol>    [2.x.92]  Discretization of the linear advection equation with the DG method.    [2.x.93]  Assembling of jump terms and other expressions on the interface between cells using FEInterfaceValues.    [2.x.94]  Assembling of the system matrix using the  [2.x.95]   [2.x.96]  

The particular concern of this program are the loops of DG methods. These turn out to be especially complex, primarily because for the face terms, we have to distinguish the cases of boundary, regular interior faces and interior faces with hanging nodes, respectively. The  [2.x.97]  handles the complexity on iterating over cells and faces and allows specifying "workers" for the different cell and face terms. The integration of face terms itself, including on adaptively refined faces, is done using the FEInterfaceValues class. 

[1.x.53] 

The model problem solved in this example is the linear advection equation 

[1.x.54] 

subject to the boundary conditions 

[1.x.55] 

on the inflow part  [2.x.98]  of the boundary  [2.x.99]  of the domain.  Here,  [2.x.100]  denotes a vector field,  [2.x.101]  the (scalar) solution function,  [2.x.102]  a boundary value function, 

[1.x.56] 

the inflow part of the boundary of the domain and  [2.x.103]  denotes the unit outward normal to the boundary  [2.x.104] . This equation is the conservative version of the advection equation already considered in step-9 of this tutorial. 


On each cell  [2.x.105] , we multiply by a test function  [2.x.106]  from the left and integrate by parts to get: 

[1.x.57] 

When summing this expression over all cells  [2.x.107] , the boundary integral is done over all internal and external faces and as such there are three cases: <ol>  [2.x.108]  outer boundary on the inflow (we replace  [2.x.109]  by given  [2.x.110] ):    [2.x.111]   [2.x.112]  outer boundary on the outflow:    [2.x.113]   [2.x.114]  inner faces (integral from two sides turns into jump, we use the upwind velocity):    [2.x.115]   [2.x.116]  

Here, the jump is defined as  [2.x.117] , where the superscripts refer to the left ('+') and right ('-') values at the face. The upwind value  [2.x.118]  is defined to be  [2.x.119]  if  [2.x.120]  and  [2.x.121]  otherwise. 

As a result, the mesh-dependent weak form reads: 

[1.x.58] 

Here,  [2.x.122]  is the set of all active cells of the triangulation and  [2.x.123]  is the set of all active interior faces. This formulation is known as the upwind discontinuous Galerkin method. 

In order to implement this bilinear form, we need to compute the cell terms (first sum) using the usual way to achieve integration on a cell, the interface terms (second sum) using FEInterfaceValues, and the boundary terms (the other two terms). The summation of all those is done by  [2.x.124]  




[1.x.59] 

We solve the advection equation on  [2.x.125]  with  [2.x.126]  representing a circular counterclockwise flow field, and  [2.x.127]  on  [2.x.128]  and  [2.x.129]  on  [2.x.130] . 

We solve on a sequence of meshes by refining the mesh adaptively by estimating the norm of the gradient on each cell. After solving on each mesh, we output the solution in vtk format and compute the  [2.x.131]  norm of the solution. As the exact solution is either 0 or 1, we can measure the magnitude of the overshoot of the numerical solution with this. 


examples/step-12/doc/results.dox 



[1.x.60] 


The output of this program consist of the console output and solutions in vtk format: 

[1.x.61] 



We show the solutions on the initial mesh, the mesh after two and after five adaptive refinement steps. 

 [2.x.132]   [2.x.133]   [2.x.134]  

And finally we show a plot of a 3d computation. 

 [2.x.135]  


[1.x.62] 

[1.x.63] 

In this program we have used discontinuous elements. It is a legitimate question to ask why not simply use the normal, continuous ones. Of course, to everyone with a background in numerical methods, the answer is obvious: the continuous Galerkin (cG) method is not stable for the transport equation, unless one specifically adds stabilization terms. The DG method, however, [1.x.64] stable. Illustrating this with the current program is not very difficult; in fact, only the following minor modifications are necessary: 

- Change the element to FE_Q instead of FE_DGQ. 

- Add handling of hanging node constraints in exactly the same way as step-6. 

- We need a different solver; the direct solver in step-29 is a convenient   choice. An experienced deal.II user will be able to do this in less than 10 minutes. 

While the 2d solution has been shown above, containing a number of small spikes at the interface that are, however, stable in height under mesh refinement, results look much different when using a continuous element: 

 [2.x.136]  

In refinement iteration 5, the image can't be plotted in a reasonable way any more as a 3d plot. We thus show a color plot with a range of  [2.x.137]  (the solution values of the exact solution lie in  [2.x.138] , of course). In any case, it is clear that the continuous Galerkin solution exhibits oscillatory behavior that gets worse and worse as the mesh is refined more and more. 

There are a number of strategies to stabilize the cG method, if one wants to use continuous elements for some reason. Discussing these methods is beyond the scope of this tutorial program; an interested reader could, for example, take a look at step-31. 




[1.x.65] 

[1.x.66] 

Given that the exact solution is known in this case, one interesting avenue for further extensions would be to confirm the order of convergence for this program. In the current case, the solution is non-smooth, and so we can not expect to get a particularly high order of convergence, even if we used higher order elements. But even if the solution [1.x.67] smooth, the equation is not elliptic and so it is not immediately clear that we should obtain a convergence order that equals that of the optimal interpolation estimates (i.e. for example that we would get  [2.x.139]  convergence in the  [2.x.140]  norm by using quadratic elements). 

In fact, for hyperbolic equations, theoretical predictions often indicate that the best one can hope for is an order one half below the interpolation estimate. For example, for the streamline diffusion method (an alternative method to the DG method used here to stabilize the solution of the transport equation), one can prove that for elements of degree  [2.x.141] , the order of convergence is  [2.x.142]  on arbitrary meshes. While the observed order is frequently  [2.x.143]  on uniformly refined meshes, one can construct so-called Peterson meshes on which the worse theoretical bound is actually attained. This should be relatively simple to verify, for example using the  [2.x.144]  function. 

A different direction is to observe that the solution of transport problems often has discontinuities and that therefore a mesh in which we [1.x.68] every cell in every coordinate direction may not be optimal. Rather, a better strategy would be to only cut cells in the direction parallel to the discontinuity. This is called [1.x.69] and is the subject of step-30. 


examples/step-12b/doc/intro.dox 

[1.x.70] 

[1.x.71] 

This is a variant of step-16 with the only change that we are using the MeshWorker framework with the pre-made LocalIntegrator helper classes instead of assembling the face terms using FEInterfaceValues. 

The details of this framework on how it is used in practice will be explained as part of this tutorial program. 

[1.x.72] 

The problem we solve here is the same as the one in step-12. 


examples/step-12b/doc/results.dox 



[1.x.73] 


The output of this program is very similar to step-16 and we are not repeating the output here. 

We show the solutions on the initial mesh, the mesh after two and after five adaptive refinement steps. 

 [2.x.145]   [2.x.146]   [2.x.147]  


Then we show the final grid (after 5 refinement steps) and the solution again, this time with a nicer 3d rendering (obtained using the  [2.x.148]  function and the VTK-based VisIt visualization program) that better shows the sharpness of the jump on the refined mesh and the over- and undershoots of the solution along the interface: 

 [2.x.149]   [2.x.150]  


And finally we show a plot of a 3d computation. 

 [2.x.151]  


[1.x.74] 

[1.x.75] 

For ideas for further extensions, please see see step-12. 


examples/step-13/doc/intro.dox 

[1.x.76] 

[1.x.77] 

[1.x.78] 


In this example program, we will not so much be concerned with describing new ways how to use deal.II and its facilities, but rather with presenting methods of writing modular and extensible finite element programs. The main reason for this is the size and complexity of modern research software: applications implementing modern error estimation concepts and adaptive solution methods tend to become rather large. For example, when this program was written in 2002, the three largest applications by the main authors of deal.II, are at the time of writing of this example program: <ol>  [2.x.152]  a program for solving conservation hyperbolic equations by the      Discontinuous Galerkin Finite Element method: 33,775 lines of      code;  [2.x.153]  a parameter estimation program: 28,980 lines of code;  [2.x.154]  a wave equation solver: 21,020 lines of code.  [2.x.155]  

(The library proper - without example programs and test suite - has slightly more than 150,000 lines of code as of spring 2002. It is of course several times larger now.) The sizes of these applications are at the edge of what one person, even an experienced programmer, can manage. 




The numbers above make one thing rather clear: monolithic programs that are not broken up into smaller, mostly independent pieces have no way of surviving, since even the author will quickly lose the overview of the various dependencies between different parts of a program. Only data encapsulation, for example using object oriented programming methods, and modularization by defining small but fixed interfaces can help structure data flow and mutual interdependencies. It is also an absolute prerequisite if more than one person is developing a program, since otherwise confusion will quickly prevail as one developer would need to know if another changed something about the internals of a different module if they were not cleanly separated. 




In previous examples, you have seen how the library itself is broken up into several complexes each building atop the underlying ones, but relatively independent of the other ones: <ol>  [2.x.156] the triangulation class complex, with associated iterator classes;  [2.x.157] the finite element classes;  [2.x.158] the DoFHandler class complex, with associated iterators, built on     the triangulation and finite element classes;  [2.x.159] the classes implementing mappings between unit and real cells;  [2.x.160] the FEValues class complex, built atop the finite elements and     mappings.  [2.x.161]  Besides these, and a large number of smaller classes, there are of course the following "tool" modules: <ol>  [2.x.162] output in various graphical formats;  [2.x.163] linear algebra classes.  [2.x.164]  These complexes can also be found as a flow chart on the front page of the deal.II manual website. 




The goal of this program is now to give an example of how a relatively simple finite element program could be structured such that we end up with a set of modules that are as independent of each other as possible. This allows to change the program at one end, without having to worry that it might break at the other, as long as we do not touch the interface through which the two ends communicate. The interface in C++, of course, is the declaration of abstract base classes. 




Here, we will implement (again) a Laplace solver, although with a number of differences compared to previous example programs: <ol>  [2.x.165] The classes that implement the process of numerically solving the     equation are no more responsible for driving the process of     "solving-estimating error-refining-solving again", but we delegate     this to external functions. This allows first to use it as a     building block in a larger context, where the solution of a     Laplace equation might only be one part (for example, in a     nonlinear problem, where Laplace equations might have to be solved     in each nonlinear step). It would also allow to build a framework     around this class that would allow using solvers for other     equations (but with the same external interface) instead, in case     some techniques shall be evaluated for different types of partial     differential equations.  [2.x.166] It splits the process of evaluating the computed solution to a     separate set of classes. The reason is that one is usually not     interested in the solution of a PDE per se, but rather in certain     aspects of it. For example, one might wish to compute the traction     at a certain boundary in elastic computations, or in the signal of     a seismic wave at a receiver position at a given     location. Sometimes, one might have an interest in several of     these aspects. Since the evaluation of a solution is something     that does not usually affect the process of solution, we split it     off into a separate module, to allow for the development of such     evaluation filters independently of the development of the solver     classes.  [2.x.167] Separate the classes that implement mesh refinement from the     classes that compute the solution.  [2.x.168] Separate the description of the test case with which we will     present the program, from the rest of the program.  [2.x.169] Parallelize the assembly of linear systems using the WorkStream     facilities. This follows the extensive description that can be     found in the  [2.x.170]  "Parallel computing with multiple processors accessing shared memory"     documentation module. The implementation essentially follows what     has already been described in step-9.  [2.x.171]  




The things the program does are not new. In fact, this is more like a melange of previous programs, cannibalizing various parts and functions from earlier examples. It is the way they are arranged in this program that should be the focus of the reader, i.e. the software design techniques used in the program to achieve the goal of implementing the desired mathematical method. However, we must stress that software design is in part also a subjective matter: different persons have different programming backgrounds and have different opinions about the "right" style of programming; this program therefore expresses only what the author considers useful practice, and is not necessarily a style that you have to adopt in order to write successful numerical software if you feel uncomfortable with the chosen ways. It should serve as a case study, however, inspiring the reader with ideas to the desired end. 




Once you have worked through the program, you will remark that it is already somewhat complex in its structure. Nevertheless, it only has about 850 lines of code, without comments. In real applications, there would of course be comments and class documentation, which would bring that to maybe 1200 lines. Yet, compared to the applications listed above, this is still small, as they are 20 to 25 times as large. For programs as large, a proper design right from the start is thus indispensable. Otherwise, it will have to be redesigned at one point in its life, once it becomes too large to be manageable. 




Despite of this, all three programs listed above have undergone major revisions, or even rewrites. The wave program, for example, was once entirely teared to parts when it was still significantly smaller, just to assemble it again in a more modular form. By that time, it had become impossible to add functionality without affecting older parts of the code (the main problem with the code was the data flow: in time dependent application, the major concern is when to store data to disk and when to reload it again; if this is not done in an organized fashion, then you end up with data released too early, loaded too late, or not released at all). Although the present example program thus draws from several years of experience, it is certainly not without flaws in its design, and in particular might not be suited for an application where the objective is different. It should serve as an inspiration for writing your own application in a modular way, to avoid the pitfalls of too closely coupled codes. 




[1.x.79] 


What the program actually does is not even the main point of this program, the structure of the program is more important. However, in a few words, a description would be: solve the Laplace equation for a given right hand side such that the solution is the function  [2.x.172] . The goal of the computation is to get the value of the solution at the point  [2.x.173] , and to compare the accuracy with which we resolve this value for two refinement criteria, namely global refinement and refinement by the error indicator by Kelly et al. which we have already used in previous examples. 




The results will, as usual, be discussed in the respective section of this document. In doing so, we will find a slightly irritating observation about the relative performance of the two refinement criteria. In a later example program, building atop this one, we will devise a different method that should hopefully perform better than the techniques discussed here. 




So much now for all the theoretical and anecdotal background. The best way of learning about a program is to look at it, so here it is: 


examples/step-13/doc/results.dox 



[1.x.80] 




The results of this program are not that interesting - after all its purpose was not to demonstrate some new mathematical idea, and also not how to program with deal.II, but rather to use the material which we have developed in the previous examples to form something which demonstrates a way to build modern finite element software in a modular and extensible way. 




Nevertheless, we of course show the results of the program. Of foremost interest is the point value computation, for which we had implemented the corresponding evaluation class. The results (i.e. the output) of the program looks as follows: 

[1.x.81] 




What surprises here is that the exact value is 1.59491554..., and that it is apparently surprisingly complicated to compute the solution even to only one per cent accuracy, although the solution is smooth (in fact infinitely often differentiable). This smoothness is shown in the graphical output generated by the program, here coarse grid and the first 9 refinement steps of the Kelly refinement indicator: 


 [2.x.174]  


While we're already at watching pictures, this is the eighth grid, as viewed from top: 


 [2.x.175]  


However, we are not yet finished with evaluation the point value computation. In fact, plotting the error  [2.x.176]  for the two refinement criteria yields the following picture: 


 [2.x.177]  





What  [2.x.178] is [2.x.179]  disturbing about this picture is that not only is the adaptive mesh refinement not better than global refinement as one would usually expect, it is even significantly worse since its convergence is irregular, preventing all extrapolation techniques when using the values of subsequent meshes! On the other hand, global refinement provides a perfect  [2.x.180]  or  [2.x.181]  convergence history and provides every opportunity to even improve on the point values by extrapolation. Global mesh refinement must therefore be considered superior in this example! This is even more surprising as the evaluation point is not somewhere in the left part where the mesh is coarse, but rather to the right and the adaptive refinement should refine the mesh around the evaluation point as well. 




We thus close the discussion of this example program with a question: 

<p align="center">   <strong> [2.x.182] What is wrong with adaptivity if it is not better than   global refinement? [2.x.183] </strong> 





 [2.x.184] Exercise at the end of this example: [2.x.185]  There is a simple reason for the bad and irregular behavior of the adapted mesh solutions. It is simple to find out by looking at the mesh around the evaluation point in each of the steps - the data for this is in the output files of the program. An exercise would therefore be to modify the mesh refinement routine such that the problem (once you remark it) is avoided. The second exercise is to check whether the results are then better than global refinement, and if so if even a better order of convergence (in terms of the number of degrees of freedom) is achieved, or only by a better constant. 




( [2.x.186] Very brief answers for the impatient: [2.x.187]  at steps with larger errors, the mesh is not regular at the point of evaluation, i.e. some of the adjacent cells have hanging nodes; this destroys some superapproximation effects of which the globally refined mesh can profit. Answer 2: this quick hack 

[1.x.82] 

in the refinement function of the Kelly refinement class right before executing refinement would improve the results (exercise: what does the code do?), making them consistently better than global refinement. Behavior is still irregular, though, so no results about an order of convergence are possible.) 


examples/step-14/doc/intro.dox 

[1.x.83] 

[1.x.84] 

[1.x.85] 

The Heidelberg group of Professor Rolf Rannacher, to which the three initial authors of the deal.II library belonged during their PhD time and partly also afterwards, has been involved with adaptivity and error estimation for finite element discretizations since the mid-1990ies. The main achievement is the development of error estimates for arbitrary functionals of the solution, and of optimal mesh refinement for its computation. 

We will not discuss the derivation of these concepts in too great detail, but will implement the main ideas in the present example program. For a thorough introduction into the general idea, we refer to the seminal work of Becker and Rannacher  [2.x.188] ,  [2.x.189] , and the overview article of the same authors in Acta Numerica  [2.x.190] ; the first introduces the concept of error estimation and adaptivity for general functional output for the Laplace equation, while the second gives many examples of applications of these concepts to a large number of other, more complicated equations. For applications to individual types of equations, see also the publications by Becker  [2.x.191] ,  [2.x.192] , Kanschat  [2.x.193] ,  [2.x.194] , Suttmeier  [2.x.195] ,  [2.x.196] ,  [2.x.197] ,  [2.x.198] , Bangerth  [2.x.199] ,  [2.x.200] ,  [2.x.201] ,  [2.x.202] , and Hartmann  [2.x.203] ,  [2.x.204] ,  [2.x.205] . All of these works, from the original introduction by Becker and Rannacher to individual contributions to particular equations, have later been summarized in a book by Bangerth and Rannacher that covers all of these topics, see  [2.x.206] . 


The basic idea is the following: in applications, one is not usually interested in the solution per se, but rather in certain aspects of it. For example, in simulations of flow problems, one may want to know the lift or drag of a body immersed in the fluid; it is this quantity that we want to know to best accuracy, and whether the rest of the solution of the describing equations is well resolved is not of primary interest. Likewise, in elasticity one might want to know about values of the stress at certain points to guess whether maximal load values of joints are safe, for example. Or, in radiative transfer problems, mean flux intensities are of interest. 

In all the cases just listed, it is the evaluation of a functional  [2.x.207]  of the solution which we are interested in, rather than the values of  [2.x.208]  everywhere. Since the exact solution  [2.x.209]  is not available, but only its numerical approximation  [2.x.210] , it is sensible to ask whether the computed value  [2.x.211]  is within certain limits of the exact value  [2.x.212] , i.e. we want to bound the error with respect to this functional,  [2.x.213] . 

For simplicity of exposition, we henceforth assume that both the quantity of interest  [2.x.214] , as well as the equation are linear, and we will in particular show the derivation for the Laplace equation with homogeneous Dirichlet boundary conditions, although the concept is much more general. For this general case, we refer to the references listed above.  The goal is to obtain bounds on the error,  [2.x.215] . For this, let us denote by  [2.x.216]  the solution of a dual problem, defined as follows: 

[1.x.86] 

where  [2.x.217]  is the bilinear form associated with the differential equation, and the test functions are chosen from the corresponding solution space. Then, taking as special test function  [2.x.218]  the error, we have that 

[1.x.87] 

and we can, by Galerkin orthogonality, rewrite this as 

[1.x.88] 

where  [2.x.219]  can be chosen from the discrete test space in whatever way we find convenient. 

Concretely, for Laplace's equation, the error identity reads 

[1.x.89] 

Because we want to use this formula not only to compute error, but also to refine the mesh, we need to rewrite the expression above as a sum over cells where each cell's contribution can then be used as an error indicator for this cell. Thus, we split the scalar products into terms for each cell, and integrate by parts on each of them: 

[1.x.90] 

Next we use that  [2.x.220] , and that for solutions of the Laplace equation, the solution is smooth enough that  [2.x.221]  is continuous almost everywhere -- so the terms involving  [2.x.222]  on one cell cancels with that on its neighbor, where the normal vector has the opposite sign. (The same is not true for  [2.x.223] , though.) At the boundary of the domain, where there is no neighbor cell with which this term could cancel, the weight  [2.x.224]  can be chosen as zero, and the whole term disappears. 

Thus, we have 

[1.x.91] 

In a final step, note that when taking the normal derivative of  [2.x.225] , we mean the value of this quantity as taken from this side of the cell (for the usual Lagrange elements, derivatives are not continuous across edges). We then rewrite the above formula by exchanging half of the edge integral of cell  [2.x.226]  with the neighbor cell  [2.x.227] , to obtain 

[1.x.92] 

Using that for the normal vectors on adjacent cells we have  [2.x.228] , we define the jump of the normal derivative by 

[1.x.93] 

and get the final form after setting the discrete function  [2.x.229] , which is by now still arbitrary, to the point interpolation of the dual solution,  [2.x.230] : 

[1.x.94] 



With this, we have obtained an exact representation of the error of the finite element discretization with respect to arbitrary (linear) functionals  [2.x.231] . Its structure is a weighted form of a residual estimator, as both  [2.x.232]  and  [2.x.233]  are cell and edge residuals that vanish on the exact solution, and  [2.x.234]  are weights indicating how important the residuals on a certain cell is for the evaluation of the given functional. Furthermore, it is a cell-wise quantity, so we can use it as a mesh refinement criterion. The question, is: how to evaluate it? After all, the evaluation requires knowledge of the dual solution  [2.x.235] , which carries the information about the quantity we want to know to best accuracy. 

In some, very special cases, this dual solution is known. For example, if the functional  [2.x.236]  is the point evaluation,  [2.x.237] , then the dual solution has to satisfy 

[1.x.95] 

with the Dirac delta function on the right hand side, and the dual solution is the Green's function with respect to the point  [2.x.238] . For simple geometries, this function is analytically known, and we could insert it into the error representation formula. 

However, we do not want to restrict ourselves to such special cases. Rather, we will compute the dual solution numerically, and approximate  [2.x.239]  by some numerically obtained  [2.x.240] . We note that it is not sufficient to compute this approximation  [2.x.241]  using the same method as used for the primal solution  [2.x.242] , since then  [2.x.243] , and the overall error estimate would be zero. Rather, the approximation  [2.x.244]  has to be from a larger space than the primal finite element space. There are various ways to obtain such an approximation (see the cited literature), and we will choose to compute it with a higher order finite element space. While this is certainly not the most efficient way, it is simple since we already have all we need to do that in place, and it also allows for simple experimenting. For more efficient methods, again refer to the given literature, in particular  [2.x.245] ,  [2.x.246] . 

With this, we end the discussion of the mathematical side of this program and turn to the actual implementation. 




 [2.x.247]  There are two steps above that do not seem necessary if all you care about is computing the error: namely, (i) the subtraction of  [2.x.248]  from  [2.x.249] , and (ii) splitting the integral into a sum of cells and integrating by parts on each. Indeed, neither of these two steps change  [2.x.250]  at all, as we only ever consider identities above until the substitution of  [2.x.251]  by  [2.x.252] . In other words, if you care only about [1.x.96]  [2.x.253] , then these steps are not necessary. On the other hand, if you want to use the error estimate also as a refinement criterion for each cell of the mesh, then it is necessary to (i) break the estimate into a sum of cells, and (ii) massage the formulas in such a way that each cell's contributions have something to do with the local error. (While the contortions above do not change the value of the [1.x.97]  [2.x.254] , they change the values we compute for each cell  [2.x.255] .) To this end, we want to write everything in the form "residual times dual weight" where a "residual" is something that goes to zero as the approximation becomes  [2.x.256]  better and better. For example, the quantity  [2.x.257]  is not a residual, since it simply converges to the (normal component of) the gradient of the exact solution. On the other hand,  [2.x.258]  is a residual because it converges to  [2.x.259] . All of the steps we have taken above in developing the final form of  [2.x.260]  have indeed had the goal of bringing the final formula into a form where each term converges to zero as the discrete solution  [2.x.261]  converges to  [2.x.262] . This then allows considering each cell's contribution as an "error indicator" that also converges to zero -- as it should as the mesh is refined. 




[1.x.98] 

The step-14 example program builds heavily on the techniques already used in the step-13 program. Its implementation of the dual weighted residual error estimator explained above is done by deriving a second class, properly called  [2.x.263]  base class, and having a class ( [2.x.264] ) that joins the two again and controls the solution of the primal and dual problem, and then uses both to compute the error indicator for mesh refinement. 

The program continues the modular concept of the previous example, by implementing the dual functional, describing quantity of interest, by an abstract base class, and providing two different functionals which implement this interface. Adding a different quantity of interest is thus simple. 

One of the more fundamental differences is the handling of data. A common case is that you develop a program that solves a certain equation, and test it with different right hand sides, different domains, different coefficients and boundary values, etc. Usually, these have to match, so that exact solutions are known, or that their combination makes sense at all. 

We demonstrate a way how this can be achieved in a simple, yet very flexible way. We will put everything that belongs to a certain setup into one class, and provide a little C++ mortar around it, so that entire setups (domains, coefficients, right hand sides, etc.) can be exchanged by only changing something in  [2.x.265] one [2.x.266]  place. 

Going this way a little further, we have also centralized all the other parameters that describe how the program is to work in one place, such as the order of the finite element, the maximal number of degrees of freedom, the evaluation objects that shall be executed on the computed solutions, and so on. This allows for simpler configuration of the program, and we will show in a later program how to use a library class that can handle setting these parameters by reading an input file. The general aim is to reduce the places within a program where one may have to look when wanting to change some parameter, as it has turned out in practice that one forgets where they are as programs grow. Furthermore, putting all options describing what the program does in a certain run into a file (that can be stored with the results) helps repeatability of results more than if the various flags were set somewhere in the program, where their exact values are forgotten after the next change to this place. 

Unfortunately, the program has become rather long. While this admittedly reduces its usefulness as an example program, we think that it is a very good starting point for development of a program for other kinds of problems, involving different equations than the Laplace equation treated here. Furthermore, it shows everything that we can show you about our way of a posteriori error estimation, and its structure should make it simple for you to adjust this method to other problems, other functionals, other geometries, coefficients, etc. 

The author believes that the present program is his masterpiece among the example programs, regarding the mathematical complexity, as well as the simplicity to add extensions. If you use this program as a basis for your own programs, we would kindly like to ask you to state this fact and the name of the author of the example program, Wolfgang Bangerth, in publications that arise from that, of your program consists in a considerable part of the example program. 


examples/step-14/doc/results.dox 



[1.x.99] 

[1.x.100] 


This program offers a lot of possibilities to play around. We can thus only show a small part of all possible results that can be obtained with the help of this program. However, you are encouraged to just try it out, by changing the settings in the main program. Here, we start by simply letting it run, unmodified: 

[1.x.101] 




First let's look what the program actually computed. On the seventh grid, primal and dual numerical solutions look like this (using a color scheme intended to evoke the snow-capped mountains of Colorado that the original author of this program now calls home):  [2.x.267]  Apparently, the region at the bottom left is so unimportant for the point value evaluation at the top right that the grid is left entirely unrefined there, even though the solution has singularities at the inner corner of that cell! Due to the symmetry in right hand side and domain, the solution should actually look like at the top right in all four corners, but the mesh refinement criterion involving the dual solution chose to refine them differently -- because we said that we really only care about a single function value somewhere at the top right. 




Here are some of the meshes that are produced in refinement cycles 0, 2, 4 (top row), and 5, 7, and 8 (bottom row): 

 [2.x.268]  

Note the subtle interplay between resolving the corner singularities, and resolving around the point of evaluation. It will be rather difficult to generate such a mesh by hand, as this would involve to judge quantitatively how much which of the four corner singularities should be resolved, and to set the weight compared to the vicinity of the evaluation point. 




The program prints the point value and the estimated error in this quantity. From extrapolating it, we can guess that the exact value is somewhere close to 0.0334473, plus or minus 0.0000001 (note that we get almost 6 valid digits from only 22,000 (primal) degrees of freedom. This number cannot be obtained from the value of the functional alone, but I have used the assumption that the error estimator is mostly exact, and extrapolated the computed value plus the estimated error, to get an approximation of the true value. Computing with more degrees of freedom shows that this assumption is indeed valid. 




From the computed results, we can generate two graphs: one that shows the convergence of the error  [2.x.269]  (taking the extrapolated value as correct) in the point value, and the value that we get by adding up computed value  [2.x.270]  and estimated error eta (if the error estimator  [2.x.271]  were exact, then the value  [2.x.272]  would equal the exact point value, and the error in this quantity would always be zero; however, since the error estimator is only a - good - approximation to the true error, we can by this only reduce the size of the error). In this graph, we also indicate the complexity  [2.x.273]  to show that mesh refinement acts optimal in this case. The second chart compares true and estimated error, and shows that the two are actually very close to each other, even for such a complicated quantity as the point value: 


 [2.x.274]  




[1.x.102] 


Since we have accepted quite some effort when using the mesh refinement driven by the dual weighted error estimator (for solving the dual problem, and for evaluating the error representation), it is worth while asking whether that effort was successful. To this end, we first compare the achieved error levels for different mesh refinement criteria. To generate this data, simply change the value of the mesh refinement criterion variable in the main program. The results are thus (for the weight in the Kelly indicator, we have chosen the function  [2.x.275] , where  [2.x.276]  is the distance to the evaluation point; it can be shown that this is the optimal weight if we neglect the effects of boundaries): 

 [2.x.277]  




Checking these numbers, we see that for global refinement, the error is proportional to  [2.x.278] , and for the dual estimator  [2.x.279] . Generally speaking, we see that the dual weighted error estimator is better than the other refinement indicators, at least when compared with those that have a similarly regular behavior. The Kelly indicator produces smaller errors, but jumps about the picture rather irregularly, with the error also changing signs sometimes. Therefore, its behavior does not allow to extrapolate the results to larger values of N. Furthermore, if we trust the error estimates of the dual weighted error estimator, the results can be improved by adding the estimated error to the computed values. In terms of reliability, the weighted estimator is thus better than the Kelly indicator, although the latter sometimes produces smaller errors. 




[1.x.103] 


Besides evaluating the values of the solution at a certain point, the program also offers the possibility to evaluate the x-derivatives at a certain point, and also to tailor mesh refinement for this. To let the program compute these quantities, simply replace the two occurrences of  [2.x.280]  in the main function by  [2.x.281] , and let the program run: 

[1.x.104] 






The solution looks roughly the same as before (the exact solution of course  [2.x.282] is [2.x.283]  the same, only the grid changed a little), but the dual solution is now different. A close-up around the point of evaluation shows this:  [2.x.284]  This time, the grids in refinement cycles 0, 5, 6, 7, 8, and 9 look like this: 

 [2.x.285]  

Note the asymmetry of the grids compared with those we obtained for the point evaluation. This is due to the fact that the domain and the primal solution may be symmetric about the diagonal, but the  [2.x.286] -derivative is not, and the latter enters the refinement criterion. 




Then, it is interesting to compare actually computed values of the quantity of interest (i.e. the x-derivative of the solution at one point) with a reference value of -0.0528223... plus or minus 0.0000005. We get this reference value by computing on finer grid after some more mesh refinements, with approximately 130,000 cells. Recall that if the error is  [2.x.287]  in the optimal case, then taking a mesh with ten times more cells gives us one additional digit in the result. 




In the left part of the following chart, you again see the convergence of the error towards this extrapolated value, while on the right you see a comparison of true and estimated error: 

 [2.x.288]  

After an initial phase where the true error changes its sign, the estimated error matches it quite well, again. Also note the dramatic improvement in the error when using the estimated error to correct the computed value of  [2.x.289] . 




[1.x.105] 


If instead of the  [2.x.290]  data set, we choose  [2.x.291]  in the main function, and choose  [2.x.292]  as the evaluation point, then we can redo the computations of the previous example program, to compare whether the results obtained with the help of the dual weighted error estimator are better than those we had previously. 




First, the meshes after 9 adaptive refinement cycles obtained with the point evaluation and derivative evaluation refinement criteria, respectively, look like this: 

 [2.x.293]  

The features of the solution can still be seen in the mesh, but since the solution is smooth, the singularities of the dual solution entirely dominate the mesh refinement criterion, and lead to strongly concentrated meshes. The solution after the seventh refinement step looks like the following: 

 [2.x.294]  

Obviously, the solution is worse at some places, but the mesh refinement process should have taken care that these places are not important for computing the point value. 





The next point is to compare the new (duality based) mesh refinement criterion with the old ones. These are the results: 

 [2.x.295]  




The results are, well, somewhat mixed. First, the Kelly indicator disqualifies itself by its unsteady behavior, changing the sign of the error several times, and with increasing errors under mesh refinement. The dual weighted error estimator has a monotone decrease in the error, and is better than the weighted Kelly and global refinement, but the margin is not as large as expected. This is, here, due to the fact the global refinement can exploit the regular structure of the meshes around the point of evaluation, which leads to a better order of convergence for the point error. However, if we had a mesh that is not locally rectangular, for example because we had to approximate curved boundaries, or if the coefficients were not constant, then this advantage of globally refinement meshes would vanish, while the good performance of the duality based estimator would remain. 







[1.x.106] 


The results here are not too clearly indicating the superiority of the dual weighted error estimation approach for mesh refinement over other mesh refinement criteria, such as the Kelly indicator. This is due to the relative simplicity of the shown applications. If you are not convinced yet that this approach is indeed superior, you are invited to browse through the literature indicated in the introduction, where plenty of examples are provided where the dual weighted approach can reduce the necessary numerical work by orders of magnitude, making this the only way to compute certain quantities to reasonable accuracies at all. 




Besides the objections you may raise against its use as a mesh refinement criterion, consider that accurate knowledge of the error in the quantity one might want to compute is of great use, since we can stop computations when we are satisfied with the accuracy. Using more traditional approaches, it is very difficult to get accurate estimates for arbitrary quantities, except for, maybe, the error in the energy norm, and we will then have no guarantee that the result we computed satisfies any requirements on its accuracy. Also, as was shown for the evaluation of point values and derivatives, the error estimate can be used to extrapolate the results, yielding much higher accuracy in the quantity we want to know. 




Leaving these mathematical considerations, we tried to write the program in a modular way, such that implementing another test case, or another evaluation and dual functional is simple. You are encouraged to take the program as a basis for your own experiments, and to play a little. 


examples/step-15/doc/intro.dox 

 [2.x.296]  

[1.x.107]  [2.x.297]  


[1.x.108] 

[1.x.109] 

[1.x.110] 

This program deals with an example of a non-linear elliptic partial differential equation, the [minimal surface equation](https://en.wikipedia.org/wiki/Minimal_surface). You can imagine the solution of this equation to describe the surface spanned by a soap film that is enclosed by a closed wire loop. We imagine the wire to not just be a planar loop, but in fact curved. The surface tension of the soap film will then reduce the surface to have minimal surface. The solution of the minimal surface equation describes this shape with the wire's vertical displacement as a boundary condition. For simplicity, we will here assume that the surface can be written as a graph  [2.x.298]  although it is clear that it is not very hard to construct cases where the wire is bent in such a way that the surface can only locally be constructed as a graph but not globally. 

Because the equation is non-linear, we can't solve it directly. Rather, we have to use Newton's method to compute the solution iteratively. 

 [2.x.299]  ( [2.x.300]  




[1.x.111] 

In a classical sense, the problem is given in the following form: 


  [1.x.112] 



 [2.x.301]  is the domain we get by projecting the wire's positions into  [2.x.302]  space. In this example, we choose  [2.x.303]  as the unit disk. 

As described above, we solve this equation using Newton's method in which we compute the  [2.x.304] th approximate solution from the  [2.x.305] th one, and use a damping parameter  [2.x.306]  to get better global convergence behavior:   [1.x.113] 

with   [1.x.114] 

and  [2.x.307]  the derivative of F in direction of  [2.x.308] : 

[1.x.115] 



Going through the motions to find out what  [2.x.309]  is, we find that we have to solve a linear elliptic PDE in every Newton step, with  [2.x.310]  as the solution of: 

  [1.x.116] 



In order to solve the minimal surface equation, we have to solve this equation repeatedly, once per Newton step. To solve this, we have to take a look at the boundary condition of this problem. Assuming that  [2.x.311]  already has the right boundary values, the Newton update  [2.x.312]  should have zero boundary conditions, in order to have the right boundary condition after adding both.  In the first Newton step, we are starting with the solution  [2.x.313] , the Newton update still has to deliver the right boundary condition to the solution  [2.x.314] . 


Summing up, we have to solve the PDE above with the boundary condition  [2.x.315]  in the first step and with  [2.x.316]  in all the following steps. 

 [2.x.317]  In some sense, one may argue that if the program already   implements  [2.x.318] , it is duplicative to also have to implement    [2.x.319] . As always, duplication tempts bugs and we would like   to avoid it. While we do not explore this issue in this program, we   will come back to it at the end of the [1.x.117] section below,   and specifically in step-72. 




[1.x.118] 

Starting with the strong formulation above, we get the weak formulation by multiplying both sides of the PDE with a test function  [2.x.320]  and integrating by parts on both sides:   [1.x.119] 

Here the solution  [2.x.321]  is a function in  [2.x.322] , subject to the boundary conditions discussed above. Reducing this space to a finite dimensional space with basis  [2.x.323] , we can write the solution: 

[1.x.120] 



Using the basis functions as test functions and defining  [2.x.324] , we can rewrite the weak formulation: 

[1.x.121] 



where the solution  [2.x.325]  is given by the coefficients  [2.x.326] . This linear system of equations can be rewritten as: 

[1.x.122] 



where the entries of the matrix  [2.x.327]  are given by: 

[1.x.123] 



and the right hand side  [2.x.328]  is given by: 

[1.x.124] 






[1.x.125] 

The matrix that corresponds to the Newton step above can be reformulated to show its structure a bit better. Rewriting it slightly, we get that it has the form 

[1.x.126] 

where the matrix  [2.x.329]  (of size  [2.x.330]  in  [2.x.331]  space dimensions) is given by the following expression: 

[1.x.127] 

From this expression, it is obvious that  [2.x.332]  is symmetric, and so  [2.x.333]  is symmetric as well. On the other hand,  [2.x.334]  is also positive definite, which confers the same property onto  [2.x.335] . This can be seen by noting that the vector  [2.x.336]  is an eigenvector of  [2.x.337]  with eigenvalue  [2.x.338]  while all vectors  [2.x.339]  that are perpendicular to  [2.x.340]  and each other are eigenvectors with eigenvalue  [2.x.341] . Since all eigenvalues are positive,  [2.x.342]  is positive definite and so is  [2.x.343] . We can thus use the CG method for solving the Newton steps. (The fact that the matrix  [2.x.344]  is symmetric and positive definite should not come as a surprise. It results from taking the derivative of an operator that results from taking the derivative of an energy functional: the minimal surface equation simply minimizes some non-quadratic energy. Consequently, the Newton matrix, as the matrix of second derivatives of a scalar energy, must be symmetric since the derivative with regard to the  [2.x.345] th and  [2.x.346] th degree of freedom should clearly commute. Likewise, if the energy functional is convex, then the matrix of second derivatives must be positive definite, and the direct calculation above simply reaffirms this.) 

It is worth noting, however, that the positive definiteness degenerates for problems where  [2.x.347]  becomes large. In other words, if we simply multiply all boundary values by 2, then to first order  [2.x.348]  and  [2.x.349]  will also be multiplied by two, but as a consequence the smallest eigenvalue of  [2.x.350]  will become smaller and the matrix will become more ill-conditioned. (More specifically, for  [2.x.351]  we have that  [2.x.352]  whereas  [2.x.353] ; thus, the condition number of  [2.x.354] , which is a multiplicative factor in the condition number of  [2.x.355]  grows like  [2.x.356] .) It is simple to verify with the current program that indeed multiplying the boundary values used in the current program by larger and larger values results in a problem that will ultimately no longer be solvable using the simple preconditioned CG method we use here. 




[1.x.128] 

As stated above, Newton's method works by computing a direction  [2.x.357]  and then performing the update  [2.x.358]  with a step length  [2.x.359] . It is a common observation that for strongly nonlinear models, Newton's method does not converge if we always choose  [2.x.360]  unless one starts with an initial guess  [2.x.361]  that is sufficiently close to the solution  [2.x.362]  of the nonlinear problem. In practice, we don't always have such an initial guess, and consequently taking full Newton steps (i.e., using  [2.x.363] ) does frequently not work. 

A common strategy therefore is to use a smaller step length for the first few steps while the iterate  [2.x.364]  is still far away from the solution  [2.x.365]  and as we get closer use larger values for  [2.x.366]  until we can finally start to use full steps  [2.x.367]  as we are close enough to the solution. The question is of course how to choose  [2.x.368] . There are basically two widely used approaches: line search and trust region methods. 

In this program, we simply always choose the step length equal to 0.1. This makes sure that for the testcase at hand we do get convergence although it is clear that by not eventually reverting to full step lengths we forego the rapid, quadratic convergence that makes Newton's method so appealing. Obviously, this is a point one eventually has to address if the program was made into one that is meant to solve more realistic problems. We will comment on this issue some more in the [1.x.129], and use an even better approach in step-77. 




[1.x.130] 

Overall, the program we have here is not unlike step-6 in many regards. The layout of the main class is essentially the same. On the other hand, the driving algorithm in the  [2.x.369]  function is different and works as follows: <ol>  [2.x.370]    Start with the function  [2.x.371]  and modify it in such a way   that the values of  [2.x.372]  along the boundary equal the correct   boundary values  [2.x.373]  (this happens in    [2.x.374] ). Set    [2.x.375] .  [2.x.376]  

 [2.x.377]    Compute the Newton update by solving the system  [2.x.378]    with boundary condition  [2.x.379]  on  [2.x.380] .  [2.x.381]  

 [2.x.382]    Compute a step length  [2.x.383] . In this program, we always set    [2.x.384] . To make things easier to extend later on, this   happens in a function of its own, namely in    [2.x.385] .   (The strategy of always choosing  [2.x.386]  is of course not   optimal -- we should choose a step length that works for a given   search direction -- but it requires a bit of work to do that. In the   end, we leave these sorts of things to external packages: step-77   does that.)  [2.x.387]  

 [2.x.388]    The new approximation of the solution is given by    [2.x.389] .  [2.x.390]  

 [2.x.391]    If  [2.x.392]  is a multiple of 5 then refine the mesh, transfer the   solution  [2.x.393]  to the new mesh and set the values of  [2.x.394]    in such a way that along the boundary we have    [2.x.395]  (again in    [2.x.396] ). Note that   this isn't automatically   guaranteed even though by construction we had that before mesh   refinement  [2.x.397]  because mesh refinement   adds new nodes to the mesh where we have to interpolate the old   solution to the new nodes upon bringing the solution from the old to   the new mesh. The values we choose by interpolation may be close to   the exact boundary conditions but are, in general, nonetheless not   the correct values.  [2.x.398]  

 [2.x.399]    Set  [2.x.400]  and go to step 2.  [2.x.401]   [2.x.402]  

The testcase we solve is chosen as follows: We seek to find the solution of minimal surface over the unit disk  [2.x.403]  where the surface attains the values  [2.x.404]  along the boundary. 


examples/step-15/doc/results.dox 



[1.x.131] 


The output of the program looks as follows: 

[1.x.132] 



Obviously, the scheme converges, if not very fast. We will come back to strategies for accelerating the method below. 

One can visualize the solution after each set of five Newton iterations, i.e., on each of the meshes on which we approximate the solution. This yields the following set of images: 

<div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step_15_solution_1.png"          alt="Solution after zero cycles with contour lines." width="230" height="273">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_15_solution_2.png"          alt="Solution after one cycle with contour lines." width="230" height="273">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_15_solution_3.png"          alt="Solution after two cycles with contour lines." width="230" height="273">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_15_solution_4.png"          alt="Solution after three cycles with contour lines." width="230" height="273">   </div> </div> 

It is clearly visible, that the solution minimizes the surface after each refinement. The solution converges to a picture one would imagine a soap bubble to be that is located inside a wire loop that is bent like the boundary. Also it is visible, how the boundary is smoothed out after each refinement. On the coarse mesh, the boundary doesn't look like a sine, whereas it does the finer the mesh gets. 

The mesh is mostly refined near the boundary, where the solution increases or decreases strongly, whereas it is coarsened on the inside of the domain, where nothing interesting happens, because there isn't much change in the solution. The ninth solution and mesh are shown here: 

<div class="onecolumn" style="width: 60%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step_15_solution_9.png"          alt="Grid and solution of the ninth cycle with contour lines." width="507" height="507">   </div> </div> 




[1.x.133] 

[1.x.134] 

The program shows the basic structure of a solver for a nonlinear, stationary problem. However, it does not converge particularly fast, for good reasons: 

- The program always takes a step size of 0.1. This precludes the rapid,   quadratic convergence for which Newton's method is typically chosen. 

- It does not connect the nonlinear iteration with the mesh refinement   iteration. 

Obviously, a better program would have to address these two points. We will discuss them in the following. 




[1.x.135] 

Newton's method has two well known properties: 

- It may not converge from arbitrarily chosen starting points. Rather, a   starting point has to be close enough to the solution to guarantee   convergence. However, we can enlarge the area from which Newton's method   converges by damping the iteration using a [1.x.136] 0< [2.x.405] . 

- It exhibits rapid convergence of quadratic order if (i) the step length is   chosen as  [2.x.406] , and (ii) it does in fact converge with this choice   of step length. 

A consequence of these two observations is that a successful strategy is to choose  [2.x.407]  for the initial iterations until the iterate has come close enough to allow for convergence with full step length, at which point we want to switch to  [2.x.408] . The question is how to choose  [2.x.409]  in an automatic fashion that satisfies these criteria. 

We do not want to review the literature on this topic here, but only briefly mention that there are two fundamental approaches to the problem: backtracking line search and trust region methods. The former is more widely used for partial differential equations and essentially does the following: 

- Compute a search direction 

- See if the resulting residual of  [2.x.410]  with    [2.x.411]  is "substantially smaller" than that of  [2.x.412]  alone. 

- If so, then take  [2.x.413] . 

- If not, try whether the residual is "substantially smaller" with    [2.x.414] . 

- If so, then take  [2.x.415] . 

- If not, try whether the residual is "substantially smaller" with    [2.x.416] . 

- Etc. One can of course choose other factors  [2.x.417]  than the  [2.x.418]  chosen above, for  [2.x.419] . It is obvious where the term "backtracking" comes from: we try a long step, but if that doesn't work we try a shorter step, and ever shorter step, etc. The function  [2.x.420]  is written the way it is to support exactly this kind of use case. 

Whether we accept a particular step length  [2.x.421]  depends on how we define "substantially smaller". There are a number of ways to do so, but without going into detail let us just mention that the most common ones are to use the Wolfe and Armijo-Goldstein conditions. For these, one can show the following: 

- There is always a step length  [2.x.422]  for which the conditions are   satisfied, i.e., the iteration never gets stuck as long as the problem is   convex. 

- If we are close enough to the solution, then the conditions allow for    [2.x.423] , thereby enabling quadratic convergence. 

We will not dwell on this here any further but leave the implementation of such algorithms as an exercise. We note, however, that when implemented correctly then it is a common observation that most reasonably nonlinear problems can be solved in anywhere between 5 and 15 Newton iterations to engineering accuracy &mdash; substantially fewer than we need with the current version of the program. 

More details on globalization methods including backtracking can be found, for example, in  [2.x.424]  and  [2.x.425] . 

A separate point, very much worthwhile making, however, is that in practice the implementation of efficient nonlinear solvers is about as complicated as the implementation of efficient finite element methods. One should not attempt to reinvent the wheel by implementing all of the necessary steps oneself. Substantial pieces of the puzzle are already available in the  [2.x.426]  function and could be used to this end. But, instead, just like building finite element solvers on libraries such as deal.II, one should be building nonlinear solvers on libraries such as [SUNDIALS](https://computing.llnl.gov/projects/sundials). In fact, deal.II has interfaces to SUNDIALS and in particular to its nonlinear solver sub-package KINSOL through the  [2.x.427]  class. It would not be very difficult to base the current problem on that interface -- indeed, that is what step-77 does. 




[1.x.137] 

We currently do exactly 5 iterations on each mesh. But is this optimal? One could ask the following questions: 

- Maybe it is worthwhile doing more iterations on the initial meshes since   there, computations are cheap. 

- On the other hand, we do not want to do too many iterations on every mesh:   yes, we could drive the residual to zero on every mesh, but that would only   mean that the nonlinear iteration error is far smaller than the   discretization error. 

- Should we use solve the linear systems in each Newton step with higher or   lower accuracy? 

Ultimately, what this boils down to is that we somehow need to couple the discretization error on the current mesh with the nonlinear residual we want to achieve with the Newton iterations on a given mesh, and to the linear iteration we want to achieve with the CG method within each Newton iterations. 

How to do this is, again, not entirely trivial, and we again leave it as a future exercise. 




[1.x.138] 

As outlined in the introduction, when solving a nonlinear problem of the form   [1.x.139] 

we use a Newton iteration that requires us to repeatedly solve the linear partial differential equation   [1.x.140] 

so that we can compute the update   [1.x.141] 

with the solution  [2.x.428]  of the Newton step. For the problem here, we could compute the derivative  [2.x.429]  by hand and obtained   [1.x.142] 

But this is already a sizable expression that is cumbersome both to derive and to implement. It is also, in some sense, duplicative: If we implement what  [2.x.430]  is somewhere in the code, then  [2.x.431]  is not an independent piece of information but is something that, at least in principle, a computer should be able to infer itself. Wouldn't it be nice if that could actually happen? That is, if we really only had to implement  [2.x.432] , and  [2.x.433]  was then somehow done implicitly? That is in fact possible, and runs under the name "automatic differentiation". step-71 discusses this very concept in general terms, and step-72 illustrates how this can be applied in practice for the very problem we are considering here. 


examples/step-16/doc/intro.dox 

 [2.x.434]  

[1.x.143] 

[1.x.144] 

[1.x.145] 


This example shows the basic usage of the multilevel functions in deal.II. It solves almost the same problem as used in step-6, but demonstrating the things one has to provide when using multigrid as a preconditioner. In particular, this requires that we define a hierarchy of levels, provide transfer operators from one level to the next and back, and provide representations of the Laplace operator on each level. 

In order to allow sufficient flexibility in conjunction with systems of differential equations and block preconditioners, quite a few different objects have to be created before starting the multilevel method, although most of what needs to be done is provided by deal.II itself. These are 

  - the object handling transfer between grids; we use the MGTransferPrebuilt     class for this that does almost all of the work inside the library, 

  - the solver on the coarsest level; here, we use MGCoarseGridHouseholder, 

  - the smoother on all other levels, which in our case will be the      [2.x.435]  class using SOR as the underlying method, 

  - and  [2.x.436]  a class having a special level multiplication, i.e. we     basically store one matrix per grid level and allow multiplication with it. 

Most of these objects will only be needed inside the function that actually solves the linear system. There, these objects are combined in an object of type Multigrid, containing the implementation of the V-cycle, which is in turn used by the preconditioner PreconditionMG, ready for plug-in into a linear solver of the LAC library. 

The multigrid method implemented here for adaptively refined meshes follows the outline in the  [2.x.437]  "Multigrid paper", which describes the underlying implementation in deal.II and also introduces a lot of the nomenclature. First, we have to distinguish between level meshes, namely cells that have the same refinement distance from the coarse mesh, and the leaf mesh consisting of active cells of the hierarchy (in older work we refer to this as the global mesh, but this term is overused). Most importantly, the leaf mesh is not identical with the level mesh on the finest level. The following image shows what we consider to be a "level mesh": 

<p align="center">    [2.x.438]   [2.x.439]  

The fine level in this mesh consists only of the degrees of freedom that are defined on the refined cells, but does not extend to that part of the domain that is not refined. While this guarantees that the overall effort grows as  [2.x.440]  as necessary for optimal multigrid complexity, it leads to problems when defining where to smooth and what boundary conditions to pose for the operators defined on individual levels if the level boundary is not an external boundary. These questions are discussed in detail in the article cited above. 

[1.x.146] 

The problem we solve here is similar to step-6, with two main differences: first, the multigrid preconditioner, obviously. We also change the discontinuity of the coefficients such that the local assembler does not look more complicated than necessary. 


examples/step-16/doc/results.dox 



[1.x.147] 

On the finest mesh, the solution looks like this: 

<p align="center">    [2.x.441]   [2.x.442]  

More importantly, we would like to see if the multigrid method really improved the solver performance. Therefore, here is the textual output: 

<pre> Cycle 0    Number of active cells:       80    Number of degrees of freedom: 89 (by level: 8, 25, 89)    Number of CG iterations: 8 

Cycle 1    Number of active cells:       158    Number of degrees of freedom: 183 (by level: 8, 25, 89, 138)    Number of CG iterations: 9 

Cycle 2    Number of active cells:       302    Number of degrees of freedom: 352 (by level: 8, 25, 89, 223, 160)    Number of CG iterations: 10 

Cycle 3    Number of active cells:       578    Number of degrees of freedom: 649 (by level: 8, 25, 89, 231, 494, 66)    Number of CG iterations: 10 

Cycle 4    Number of active cells:       1100    Number of degrees of freedom: 1218 (by level: 8, 25, 89, 274, 764, 417, 126)    Number of CG iterations: 10 

Cycle 5    Number of active cells:       2096    Number of degrees of freedom: 2317 (by level: 8, 25, 89, 304, 779, 1214, 817)    Number of CG iterations: 11 

Cycle 6    Number of active cells:       3986    Number of degrees of freedom: 4366 (by level: 8, 25, 89, 337, 836, 2270, 897, 1617)    Number of CG iterations: 10 

Cycle 7    Number of active cells:       7574    Number of degrees of freedom: 8350 (by level: 8, 25, 89, 337, 1086, 2835, 2268, 1789, 3217)    Number of CG iterations: 11 </pre> 

That's almost perfect multigrid performance: the linear residual gets reduced by 12 orders of magnitude in 10 iteration steps, and the results are almost independent of the mesh size. That's obviously in part due to the simple nature of the problem solved, but it shows the power of multigrid methods. 




[1.x.148] 


We encourage you to generate timings for the solve() call and compare to step-6. You will see that the multigrid method has quite an overhead on coarse meshes, but that it always beats other methods on fine meshes because of its optimal complexity. 

A close inspection of this program's performance shows that it is mostly dominated by matrix-vector operations. step-37 shows one way how this can be avoided by working with matrix-free methods. 

Another avenue would be to use algebraic multigrid methods. The geometric multigrid method used here can at times be a bit awkward to implement because it needs all those additional data structures, and it becomes even more difficult if the program is to run in %parallel on machines coupled through MPI, for example. In that case, it would be simpler if one could use a black-box preconditioner that uses some sort of multigrid hierarchy for good performance but can figure out level matrices and similar things by itself. Algebraic multigrid methods do exactly this, and we will use them in step-31 for the solution of a Stokes problem and in step-32 and step-40 for a parallel variation. That said, a parallel version of this example program with MPI can be found in step-50. 

Finally, one may want to think how to use geometric multigrid for other kinds of problems, specifically  [2.x.443]  "vector valued problems". This is the topic of step-56 where we use the techniques shown here for the Stokes equation. 


examples/step-16b/doc/intro.dox 

 [2.x.444]  

[1.x.149] 

[1.x.150] 

This is a variant of step-16 with the only change that we are using the MeshWorker framework with the pre-made LocalIntegrator helper classes instead of manually assembling the matrices. 

The details of this framework on how it is used in practice will be explained as part of this tutorial program. 

[1.x.151] 

The problem we solve here is the same as the one in step-16. 


examples/step-16b/doc/results.dox 



[1.x.152] 

As in step-16, the solution looks like this on the finest mesh: 

<p align="center">    [2.x.445]   [2.x.446]  

The output is formatted in a slightly different way compared to step-16 but is functionally the same and shows the same convergence properties: <pre>  [2.x.447]  0 DEAL::   Number of active cells:       20 DEAL::   Number of degrees of freedom: 25 (by level: 8, 25)  [2.x.448]  value 0.510691  [2.x.449]  step 6 value 4.59193e-14  [2.x.450]  1 DEAL::   Number of active cells:       44 DEAL::   Number of degrees of freedom: 55 (by level: 8, 25, 45)  [2.x.451]  value 0.440678  [2.x.452]  step 8 value 1.99419e-13  [2.x.453]  2 DEAL::   Number of active cells:       86 DEAL::   Number of degrees of freedom: 105 (by level: 8, 25, 69, 49)  [2.x.454]  value 0.371855  [2.x.455]  step 9 value 1.13984e-13  [2.x.456]  3 DEAL::   Number of active cells:       170 DEAL::   Number of degrees of freedom: 200 (by level: 8, 25, 77, 174)  [2.x.457]  value 0.318967  [2.x.458]  step 9 value 2.62112e-13  [2.x.459]  4 DEAL::   Number of active cells:       332 DEAL::   Number of degrees of freedom: 388 (by level: 8, 25, 86, 231, 204)  [2.x.460]  value 0.276534  [2.x.461]  step 10 value 1.69562e-13  [2.x.462]  5 DEAL::   Number of active cells:       632 DEAL::   Number of degrees of freedom: 714 (by level: 8, 25, 89, 231, 514, 141)  [2.x.463]  value 0.215300  [2.x.464]  step 10 value 6.47463e-13  [2.x.465]  6 DEAL::   Number of active cells:       1202 DEAL::   Number of degrees of freedom: 1332 (by level: 8, 25, 89, 282, 771, 435, 257)  [2.x.466]  value 0.175848  [2.x.467]  step 10 value 1.80664e-13  [2.x.468]  7 DEAL::   Number of active cells:       2288 DEAL::   Number of degrees of freedom: 2511 (by level: 8, 25, 89, 318, 779, 1420, 829, 30)  [2.x.469]  value 0.136724  [2.x.470]  step 11 value 9.73331e-14 </pre> 


examples/step-17/doc/intro.dox 

[1.x.153] 

[1.x.154] 

[1.x.155] 

This program does not introduce any new mathematical ideas; in fact, all it does is to do the exact same computations that step-8 already does, but it does so in a different manner: instead of using deal.II's own linear algebra classes, we build everything on top of classes deal.II provides that wrap around the linear algebra implementation of the [1.x.156] library. And since PETSc allows to distribute matrices and vectors across several computers within an MPI network, the resulting code will even be able to solve the problem in %parallel. If you don't know what PETSc is, then this would be a good time to take a quick glimpse at their homepage. 

As a prerequisite of this program, you need to have PETSc installed, and if you want to run in %parallel on a cluster, you also need [1.x.157] to partition meshes. The installation of deal.II together with these two additional libraries is described in the [1.x.158] file. 

Now, for the details: as mentioned, the program does not compute anything new, so the use of finite element classes, etc., is exactly the same as before. The difference to previous programs is that we have replaced almost all uses of classes  [2.x.471]  by their near-equivalents  [2.x.472]  and  [2.x.473]  that store data in a way so that every processor in the MPI network only stores a part of the matrix or vector. More specifically, each processor will only store those rows of the matrix that correspond to a degree of freedom it "owns". For vectors, they either store only elements that correspond to degrees of freedom the processor owns (this is what is necessary for the right hand side), or also some additional elements that make sure that every processor has access the solution components that live on the cells the processor owns (so-called  [2.x.474]  "locally active DoFs") or also on neighboring cells (so-called  [2.x.475]  "locally relevant DoFs"). 

The interface the classes from the PETScWrapper namespace provide is very similar to that of the deal.II linear algebra classes, but instead of implementing this functionality themselves, they simply pass on to their corresponding PETSc functions. The wrappers are therefore only used to give PETSc a more modern, object oriented interface, and to make the use of PETSc and deal.II objects as interchangeable as possible. The main point of using PETSc is that it can run in %parallel. We will make use of this by partitioning the domain into as many blocks ("subdomains") as there are processes in the MPI network. At the same time, PETSc also provides dummy MPI stubs, so you can run this program on a single machine if PETSc was configured without MPI. 




[1.x.159] 

Developing software to run in %parallel via MPI requires a bit of a change in mindset because one typically has to split up all data structures so that every processor only stores a piece of the entire problem. As a consequence, you can't typically access all components of a solution vector on each processor -- each processor may simply not have enough memory to hold the entire solution vector. Because data is split up or "distributed" across processors, we call the programming model used by MPI "distributed memory computing" (as opposed to "shared memory computing", which would mean that multiple processors can all access all data within one memory space, for example whenever multiple cores in a single machine work on a common task). Some of the fundamentals of distributed memory computing are discussed in the  [2.x.476]  "Parallel computing with multiple processors using distributed memory" documentation module, which is itself a sub-module of the  [2.x.477]  "Parallel computing" module. 

In general, to be truly able to scale to large numbers of processors, one needs to split between the available processors [1.x.160] data structure whose size scales with the size of the overall problem. (For a definition of what it means for a program to "scale", see  [2.x.478]  "this glossary entry".) This includes, for example, the triangulation, the matrix, and all global vectors (solution, right hand side). If one doesn't split all of these objects, one of those will be replicated on all processors and will eventually simply become too large if the problem size (and the number of available processors) becomes large. (On the other hand, it is completely fine to keep objects with a size that is independent of the overall problem size on every processor. For example, each copy of the executable will create its own finite element object, or the local matrix we use in the assembly.) 

In the current program (as well as in the related step-18), we will not go quite this far but present a gentler introduction to using MPI. More specifically, the only data structures we will parallelize are matrices and vectors. We do, however, not split up the Triangulation and DoFHandler classes: each process still has a complete copy of these objects, and all processes have exact copies of what the other processes have. We will then simply have to mark, in each copy of the triangulation on each of the processors, which processor owns which cells. This process is called "partitioning" a mesh into  [2.x.479]  "subdomains". 

For larger problems, having to store the [1.x.161] mesh on every processor will clearly yield a bottleneck. Splitting up the mesh is slightly, though not much more complicated (from a user perspective, though it is [1.x.162] more complicated under the hood) to achieve and we will show how to do this in step-40 and some other programs. There are numerous occasions where, in the course of discussing how a function of this program works, we will comment on the fact that it will not scale to large problems and why not. All of these issues will be addressed in step-18 and in particular step-40, which scales to very large numbers of processes. 

Philosophically, the way MPI operates is as follows. You typically run a program via 

[1.x.163] 

which means to run it on (say) 32 processors. (If you are on a cluster system, you typically need to [1.x.164] the program to run whenever 32 processors become available; this will be described in the documentation of your cluster. But under the hood, whenever those processors become available, the same call as above will generally be executed.) What this does is that the MPI system will start 32 [1.x.165] of the  [2.x.480]  executable. (The MPI term for each of these running executables is that you have 32  [2.x.481]  "MPI processes".) This may happen on different machines that can't even read from each others' memory spaces, or it may happen on the same machine, but the end result is the same: each of these 32 copies will run with some memory allocated to it by the operating system, and it will not directly be able to read the memory of the other 31 copies. In order to collaborate in a common task, these 32 copies then have to [1.x.166] with each other. MPI, short for [1.x.167], makes this possible by allowing programs to [1.x.168]. You can think of this as the mail service: you can put a letter to a specific address into the mail and it will be delivered. But that's the extent to which you can control things. If you want the receiver to do something with the content of the letter, for example return to you data you want from over there, then two things need to happen: (i) the receiver needs to actually go check whether there is anything in their mailbox, and (ii) if there is, react appropriately, for example by sending data back. If you wait for this return message but the original receiver was distracted and not paying attention, then you're out of luck: you'll simply have to wait until your requested over there will be worked on. In some cases, bugs will lead the original receiver to never check your mail, and in that case you will wait forever -- this is called a [1.x.169]. ( [2.x.482]  

In practice, one does not usually program at the level of sending and receiving individual messages, but uses higher level operations. For example, in the program we will use function calls that take a number from each processor, add them all up, and return the sum to all processors. Internally, this is implemented using individual messages, but to the user this is transparent. We call such operations [1.x.170] because [1.x.171] processors participate in them. Collectives allow us to write programs where not every copy of the executable is doing something completely different (this would be incredibly difficult to program) but where in essence all copies are doing the same thing (though on different data) for themselves, running through the same blocks of code; then they communicate data through collectives; and then go back to doing something for themselves again running through the same blocks of data. This is the key piece to being able to write programs, and it is the key component to making sure that programs can run on any number of processors, since we do not have to write different code for each of the participating processors. 

(This is not to say that programs are never written in ways where different processors run through different blocks of code in their copy of the executable. Programs internally also often communicate in other ways than through collectives. But in practice, %parallel finite element codes almost always follow the scheme where every copy of the program runs through the same blocks of code at the same time, interspersed by phases where all processors communicate with each other.) 

In reality, even the level of calling MPI collective functions is too low. Rather, the program below will not contain any direct calls to MPI at all, but only deal.II functions that hide this communication from users of the deal.II. This has the advantage that you don't have to learn the details of MPI and its rather intricate function calls. That said, you do have to understand the general philosophy behind MPI as outlined above. 




[1.x.172] 

The techniques this program then demonstrates are: 

- How to use the PETSc wrapper classes; this will already be visible in the   declaration of the principal class of this program,  [2.x.483] . 

- How to partition the mesh into subdomains; this happens in the    [2.x.484]  function. 

- How to parallelize operations for jobs running on an MPI network; here, this   is something one has to pay attention to in a number of places, most   notably in the   [2.x.485]  function. 

- How to deal with vectors that store only a subset of vector entries   and for which we have to ensure that they store what we need on the   current processors. See for example the    [2.x.486]    functions. 

- How to deal with status output from programs that run on multiple   processors at the same time. This is done via the  [2.x.487]    variable in the program, initialized in the constructor. 

Since all this can only be demonstrated using actual code, let us go straight to the code without much further ado. 


examples/step-17/doc/results.dox 



[1.x.173] 


If the program above is compiled and run on a single processor machine, it should generate results that are very similar to those that we already got with step-8. However, it becomes more interesting if we run it on a multicore machine or a cluster of computers. The most basic way to run MPI programs is using a command line like 

[1.x.174] 

to run the step-17 executable with 32 processors. 

(If you work on a cluster, then there is typically a step in between where you need to set up a job script and submit the script to a scheduler. The scheduler will execute the script whenever it can allocate 32 unused processors for your job. How to write such job scripts differs from cluster to cluster, and you should find the documentation of your cluster to see how to do this. On my system, I have to use the command  [2.x.488]  with a whole host of options to run a job in parallel.) 

Whether directly or through a scheduler, if you run this program on 8 processors, you should get output like the following: 

[1.x.175] 

(This run uses a few more refinement cycles than the code available in the examples/ directory. The run also used a version of METIS from 2004 that generated different partitionings; consequently, the numbers you get today are slightly different.) 

As can be seen, we can easily get to almost four million unknowns. In fact, the code's runtime with 8 processes was less than 7 minutes up to (and including) cycle 14, and 14 minutes including the second to last step. (These are numbers relevant to when the code was initially written, in 2004.) I lost the timing information for the last step, though, but you get the idea. All this is after release mode has been enabled by running  [2.x.489] , and with the generation of graphical output switched off for the reasons stated in the program comments above. ( [2.x.490]  The biggest 2d computations I did had roughly 7.1 million unknowns, and were done on 32 processes. It took about 40 minutes. Not surprisingly, the limiting factor for how far one can go is how much memory one has, since every process has to hold the entire mesh and DoFHandler objects, although matrices and vectors are split up. For the 7.1M computation, the memory consumption was about 600 bytes per unknown, which is not bad, but one has to consider that this is for every unknown, whether we store the matrix and vector entries locally or not. 




Here is some output generated in the 12th cycle of the program, i.e. with roughly 300,000 unknowns: 

 [2.x.491]  

As one would hope for, the x- (left) and y-displacements (right) shown here closely match what we already saw in step-8. As shown there and in step-22, we could as well have produced a vector plot of the displacement field, rather than plotting it as two separate scalar fields. What may be more interesting, though, is to look at the mesh and partition at this step: 

 [2.x.492]  

Again, the mesh (left) shows the same refinement pattern as seen previously. The right panel shows the partitioning of the domain across the 8 processes, each indicated by a different color. The picture shows that the subdomains are smaller where mesh cells are small, a fact that needs to be expected given that the partitioning algorithm tries to equilibrate the number of cells in each subdomain; this equilibration is also easily identified in the output shown above, where the number of degrees per subdomain is roughly the same. 




It is worth noting that if we ran the same program with a different number of processes, that we would likely get slightly different output: a different mesh, different number of unknowns and iterations to convergence. The reason for this is that while the matrix and right hand side are the same independent of the number of processes used, the preconditioner is not: it performs an ILU(0) on the chunk of the matrix of  [2.x.493] each processor separately [2.x.494] . Thus, it's effectiveness as a preconditioner diminishes as the number of processes increases, which makes the number of iterations increase. Since a different preconditioner leads to slight changes in the computed solution, this will then lead to slightly different mesh cells tagged for refinement, and larger differences in subsequent steps. The solution will always look very similar, though. 




Finally, here are some results for a 3d simulation. You can repeat these by changing 

[1.x.176] 

to 

[1.x.177] 

in the main function. If you then run the program in parallel, you get something similar to this (this is for a job with 16 processes): 

[1.x.178] 






The last step, going up to 1.5 million unknowns, takes about 55 minutes with 16 processes on 8 dual-processor machines (of the kind available in 2003). The graphical output generated by this job is rather large (cycle 5 already prints around 82 MB of data), so we contend ourselves with showing output from cycle 4: 

 [2.x.495]  




The left picture shows the partitioning of the cube into 16 processes, whereas the right one shows the x-displacement along two cutplanes through the cube. 




[1.x.179] 

[1.x.180] 

The program keeps a complete copy of the Triangulation and DoFHandler objects on every processor. It also creates complete copies of the solution vector, and it creates output on only one processor. All of this is obviously the bottleneck as far as parallelization is concerned. 

Internally, within deal.II, parallelizing the data structures used in hierarchic and unstructured triangulations is a hard problem, and it took us a few more years to make this happen. The step-40 tutorial program and the  [2.x.496]  documentation module talk about how to do these steps and what it takes from an application perspective. An obvious extension of the current program would be to use this functionality to completely distribute computations to many more processors than used here. 


examples/step-18/doc/intro.dox 

[1.x.181] 

[1.x.182] 


This tutorial program is another one in the series on the elasticity problem that we have already started with step-8 and step-17. It extends it into two different directions: first, it solves the quasistatic but time dependent elasticity problem for large deformations with a Lagrangian mesh movement approach. Secondly, it shows some more techniques for solving such problems using %parallel processing with PETSc's linear algebra. In addition to this, we show how to work around one of the two major bottlenecks of step-17, namely that we generated graphical output from only one process, and that this scaled very badly with larger numbers of processes and on large problems. (The other bottleneck, namely that every processor has to hold the entire mesh and DoFHandler, is addressed in step-40.) Finally, a good number of assorted improvements and techniques are demonstrated that have not been shown yet in previous programs. 

As before in step-17, the program runs just as fine on a single sequential machine as long as you have PETSc installed. Information on how to tell deal.II about a PETSc installation on your system can be found in the deal.II README file, which is linked to from the [1.x.183] in your installation of deal.II, or on [1.x.184]. 




[1.x.185] 

[1.x.186] 

In general, time-dependent small elastic deformations are described by the elastic wave equation 

[1.x.187] 

where  [2.x.497]  is the deformation of the body,  [2.x.498]  and  [2.x.499]  the density and attenuation coefficient, and  [2.x.500]  external forces. In addition, initial conditions 

[1.x.188] 

and Dirichlet (displacement) or Neumann (traction) boundary conditions need to be specified for a unique solution: 

[1.x.189] 

In above formulation,  [2.x.501]  is the symmetric gradient of the displacement, also called the  [2.x.502] strain [2.x.503] .  [2.x.504]  is a tensor of rank 4, called the  [2.x.505] stress-strain   tensor [2.x.506]  (the inverse of the [1.x.190]) that contains knowledge of the elastic strength of the material; its symmetry properties make sure that it maps symmetric tensors of rank 2 (&ldquo;matrices&rdquo; of dimension  [2.x.507] , where  [2.x.508]  is the spatial dimensionality) onto symmetric tensors of the same rank. We will comment on the roles of the strain and stress tensors more below. For the moment it suffices to say that we interpret the term  [2.x.509]  as the vector with components  [2.x.510] , where summation over indices  [2.x.511]  is implied. 

The quasistatic limit of this equation is motivated as follows: each small perturbation of the body, for example by changes in boundary condition or the forcing function, will result in a corresponding change in the configuration of the body. In general, this will be in the form of waves radiating away from the location of the disturbance. Due to the presence of the damping term, these waves will be attenuated on a time scale of, say,  [2.x.512] . Now, assume that all changes in external forcing happen on times scales that are much larger than  [2.x.513] . In that case, the dynamic nature of the change is unimportant: we can consider the body to always be in static equilibrium, i.e. we can assume that at all times the body satisfies 

[1.x.191] 

Note that the differential equation does not contain any time derivatives any more -- all time dependence is introduced through boundary conditions and a possibly time-varying force function  [2.x.514] . The changes in configuration can therefore be considered as being stationary instantaneously. An alternative view of this is that  [2.x.515]  is not really a time variable, but only a time-like parameter that governs the evolution of the problem. 

While these equations are sufficient to describe small deformations, computing large deformations is a little more complicated and, in general, leads to nonlinear equations such as those treated in step-44. In the following, let us consider some of the tools one would employ when simulating problems in which the deformation becomes [1.x.192]. 

 [2.x.516]  The model we will consider below is not founded on anything that would be mathematically sound: we will consider a model in which we produce a small deformation, deform the physical coordinates of the body by this deformation, and then consider the next loading step again as a linear problem. This isn't consistent, since the assumption of linearity implies that deformations are infinitesimal and so moving around the vertices of our mesh by a finite amount before solving the next linear problem is an inconsistent approach. We should therefore note that it is not surprising that the equations discussed below can't be found in the literature: [1.x.193] On the other hand, the implementation techniques we consider are very much what one would need to use when implementing a [1.x.194] model, as we will see in step-44. 


To come back to defining our "artificial" model, let us first introduce a tensorial stress variable  [2.x.517] , and write the differential equations in terms of the stress: 

[1.x.195] 

Note that these equations are posed on a domain  [2.x.518]  that changes with time, with the boundary moving according to the displacements  [2.x.519]  of the points on the boundary. To complete this system, we have to specify the incremental relationship between the stress and the strain, as follows: [1.x.196] 

[1.x.197] 

where a dot indicates a time derivative. Both the stress  [2.x.520]  and the strain  [2.x.521]  are symmetric tensors of rank 2. 




[1.x.198] 

Numerically, this system is solved as follows: first, we discretize the time component using a backward Euler scheme. This leads to a discrete equilibrium of force at time step  [2.x.522] : 

[1.x.199] 

where 

[1.x.200] 

and  [2.x.523]  the incremental displacement for time step  [2.x.524] . In addition, we have to specify initial data  [2.x.525] . This way, if we want to solve for the displacement increment, we have to solve the following system: 

[1.x.201] 

The weak form of this set of equations, which as usual is the basis for the finite element formulation, reads as follows: find  [2.x.526]  such that [1.x.202] 

[1.x.203] 

Using that  [2.x.527] , these equations can be simplified to 

[1.x.204] 



We note that, for simplicity, in the program we will always assume that there are no boundary forces, i.e.  [2.x.528] , and that the deformation of the body is driven by body forces  [2.x.529]  and prescribed boundary displacements  [2.x.530]  alone. It is also worth noting that when integrating by parts, we would get terms of the form  [2.x.531] , but that we replace them with the term involving the symmetric gradient  [2.x.532]  instead of  [2.x.533] . Due to the symmetry of  [2.x.534] , the two terms are mathematically equivalent, but the symmetric version avoids the potential for round-off errors making the resulting matrix slightly non-symmetric. 

The system at time step  [2.x.535] , to be solved on the old domain  [2.x.536] , has exactly the form of a stationary elastic problem, and is therefore similar to what we have already implemented in previous example programs. We will therefore not comment on the space discretization beyond saying that we again use lowest order continuous finite elements. 

There are differences, however: <ol>    [2.x.537]  We have to move (update) the mesh after each time step, in order to be   able to solve the next time step on a new domain; 

   [2.x.538]  We need to know  [2.x.539]  to compute the next incremental   displacement, i.e. we need to compute it at the end of the time step   to make sure it is available for the next time step. Essentially,   the stress variable is our window to the history of deformation of   the body.  [2.x.540]  These two operations are done in the functions  [2.x.541]  and  [2.x.542]  in the program. While moving the mesh is only a technicality, updating the stress is a little more complicated and will be discussed in the next section. 




[1.x.205] 

As indicated above, we need to have the stress variable  [2.x.543]  available when computing time step  [2.x.544] , and we can compute it using [1.x.206] 

[1.x.207] 

There are, despite the apparent simplicity of this equation, two questions that we need to discuss. The first concerns the way we store  [2.x.545] : even if we compute the incremental updates  [2.x.546]  using lowest-order finite elements, then its symmetric gradient  [2.x.547]  is in general still a function that is not easy to describe. In particular, it is not a piecewise constant function, and on general meshes (with cells that are not rectangles %parallel to the coordinate axes) or with non-constant stress-strain tensors  [2.x.548]  it is not even a bi- or trilinear function. Thus, it is a priori not clear how to store  [2.x.549]  in a computer program. 

To decide this, we have to see where it is used. The only place where we require the stress is in the term  [2.x.550] . In practice, we of course replace this term by numerical quadrature: 

[1.x.208] 

where  [2.x.551]  are the quadrature weights and  [2.x.552]  the quadrature points on cell  [2.x.553] . This should make clear that what we really need is not the stress  [2.x.554]  in itself, but only the values of the stress in the quadrature points on all cells. This, however, is a simpler task: we only have to provide a data structure that is able to hold one symmetric tensor of rank 2 for each quadrature point on all cells (or, since we compute in parallel, all quadrature points of all cells that the present MPI process &ldquo;owns&rdquo;). At the end of each time step we then only have to evaluate  [2.x.555] , multiply it by the stress-strain tensor  [2.x.556] , and use the result to update the stress  [2.x.557]  at quadrature point  [2.x.558] . 

The second complication is not visible in our notation as chosen above. It is due to the fact that we compute  [2.x.559]  on the domain  [2.x.560] , and then use this displacement increment to both update the stress as well as move the mesh nodes around to get to  [2.x.561]  on which the next increment is computed. What we have to make sure, in this context, is that moving the mesh does not only involve moving around the nodes, but also making corresponding changes to the stress variable: the updated stress is a variable that is defined with respect to the coordinate system of the material in the old domain, and has to be transferred to the new domain. The reason for this can be understood as follows: locally, the incremental deformation  [2.x.562]  can be decomposed into three parts, a linear translation (the constant part of the displacement increment field in the neighborhood of a point), a dilational component (that part of the gradient of the displacement field that has a nonzero divergence), and a rotation. A linear translation of the material does not affect the stresses that are frozen into it -- the stress values are simply translated along. The dilational or compressional change produces a corresponding stress update. However, the rotational component does not necessarily induce a nonzero stress update (think, in 2d, for example of the situation where  [2.x.563] , with which  [2.x.564] ). Nevertheless, if the material was prestressed in a certain direction, then this direction will be rotated along with the material.  To this end, we have to define a rotation matrix  [2.x.565]  that describes, in each point the rotation due to the displacement increments. It is not hard to see that the actual dependence of  [2.x.566]  on  [2.x.567]  can only be through the curl of the displacement, rather than the displacement itself or its full gradient (as mentioned above, the constant components of the increment describe translations, its divergence the dilational modes, and the curl the rotational modes). Since the exact form of  [2.x.568]  is cumbersome, we only state it in the program code, and note that the correct updating formula for the stress variable is then [1.x.209] 

[1.x.210] 



Both stress update and rotation are implemented in the function  [2.x.569]  of the example program. 




[1.x.211] 

In step-17, the main bottleneck for %parallel computations as far as run time is concerned was that only the first processor generated output for the entire domain. Since generating graphical output is expensive, this did not scale well when larger numbers of processors were involved. We will address this here. (For a definition of what it means for a program to "scale", see  [2.x.570]  "this glossary entry".) 

Basically, what we need to do is let every process generate graphical output for that subset of cells that it owns, write them into separate files and have a way to display all files for a certain timestep at the same time. This way the code produces one  [2.x.571]  file per process per time step. The two common VTK file viewers ParaView and VisIt both support opening more than one  [2.x.572]  file at once. To simplify the process of picking the correct files and allow moving around in time, both support record files that reference all files for a given timestep. Sadly, the record files have a different format between VisIt and Paraview, so we write out both formats. 

The code will generate the files  [2.x.573] , where  [2.x.574]  is the timestep number (starting from 1) and  [2.x.575]  is the process rank (starting from 0). These files contain the locally owned cells for the timestep and processor. The files  [2.x.576]  is the visit record for timestep  [2.x.577]  is the same for ParaView. (More recent versions of VisIt can actually read  [2.x.578]  files as well, but it doesn't hurt to output both kinds of record files.) Finally, the file  [2.x.579]  is a special record only supported by ParaView that references all time steps. So in ParaView, only solution.pvd needs to be opened, while one needs to select the group of all .visit files in VisIt for the same effect. 




[1.x.212] 

In step-17, we used a regular triangulation that was simply replicated on every processor, and a corresponding DoFHandler. Both had no idea that they were used in a %parallel context -- they just existed in their entirety on every processor, and we argued that this was eventually going to be a major memory bottleneck. 

We do not address this issue here (we will do so in step-40) but make the situation slightly more automated. In step-17, we created the triangulation and then manually "partitioned" it, i.e., we assigned  [2.x.580]  "subdomain ids" to every cell that indicated which  [2.x.581]  "MPI process" "owned" the cell. Here, we use a class  [2.x.582]  that at least does this part automatically: whenever you create or refine such a triangulation, it automatically partitions itself among all involved processes (which it knows about because you have to tell it about the  [2.x.583]  "MPI communicator" that connects these processes upon construction of the triangulation). Otherwise, the  [2.x.584]  looks, for all practical purposes, like a regular Triangulation object. 

The convenience of using this class does not only result from being able to avoid the manual call to  [2.x.585]  Rather, the DoFHandler class now also knows that you want to use it in a parallel context, and by default automatically enumerates degrees of freedom in such a way that all DoFs owned by process zero come before all DoFs owned by process 1, etc. In other words, you can also avoid the call to  [2.x.586]  

There are other benefits. For example, because the triangulation knows that it lives in a %parallel universe, it also knows that it "owns" certain cells (namely, those whose subdomain id equals its MPI rank; previously, the triangulation only stored these subdomain ids, but had no way to make sense of them). Consequently, in the assembly function, you can test whether a cell is "locally owned" (i.e., owned by the current process, see  [2.x.587] ) when you loop over all cells using the syntax 

[1.x.213] 

This knowledge extends to the DoFHandler object built on such triangulations, which can then identify which degrees of freedom are locally owned (see  [2.x.588] ) via calls such as  [2.x.589]  and  [2.x.590]  Finally, the DataOut class also knows how to deal with such triangulations and will simply skip generating graphical output on cells not locally owned. 

Of course, as has been noted numerous times in the discussion in step-17, keeping the entire triangulation on every process will not scale: large problems may simply not fit into each process's memory any more, even if we have sufficiently many processes around to solve them in a reasonable time. In such cases, the  [2.x.591]  is no longer a reasonable basis for computations and we will show in step-40 how the  [2.x.592]  class can be used to work around this, namely by letting each process store only a [1.x.214] of the triangulation. 




[1.x.215] 

The overall structure of the program can be inferred from the  [2.x.593]  function that first calls  [2.x.594]  for the first time step, and then  [2.x.595]  on all subsequent time steps. The difference between these functions is only that in the first time step we start on a coarse mesh, solve on it, refine the mesh adaptively, and then start again with a clean state on that new mesh. This procedure gives us a better starting mesh, although we should of course keep adapting the mesh as iterations proceed -- this isn't done in this program, but commented on below. 

The common part of the two functions treating time steps is the following sequence of operations on the present mesh:  [2.x.596]   [2.x.597]   [2.x.598] ]:   This first function is also the most interesting one. It assembles the   linear system corresponding to the discretized version of equation   [1.x.216]. This leads to a system matrix  [2.x.599]  built up of local contributions on each cell  [2.x.600]  with entries   [1.x.217] 

  In practice,  [2.x.601]  is computed using numerical quadrature according to the   formula   [1.x.218] 

  with quadrature points  [2.x.602]  and weights  [2.x.603] . We have built these   contributions before, in step-8 and step-17, but in both of these cases we   have done so rather clumsily by using knowledge of how the rank-4 tensor  [2.x.604]    is composed, and considering individual elements of the strain tensors    [2.x.605] . This is not really   convenient, in particular if we want to consider more complicated elasticity   models than the isotropic case for which  [2.x.606]  had the convenient form    [2.x.607] . While we in fact do not use a more complicated   form than this in the present program, we nevertheless want to write it in a   way that would easily allow for this. It is then natural to introduce   classes that represent symmetric tensors of rank 2 (for the strains and   stresses) and 4 (for the stress-strain tensor  [2.x.608] ). Fortunately, deal.II   provides these: the  [2.x.609]  class template   provides a full-fledged implementation of such tensors of rank  [2.x.610]    (which needs to be an even number) and dimension  [2.x.611] . 

  What we then need is two things: a way to create the stress-strain rank-4   tensor  [2.x.612]  as well as to create a symmetric tensor of rank 2 (the strain   tensor) from the gradients of a shape function  [2.x.613]  at a quadrature   point  [2.x.614]  on a given cell. At the top of the implementation of this   example program, you will find such functions. The first one,    [2.x.615] , takes two arguments corresponding to   the Lam&eacute; constants  [2.x.616]  and  [2.x.617]  and returns the stress-strain tensor   for the isotropic case corresponding to these constants (in the program, we   will choose constants corresponding to steel); it would be simple to replace   this function by one that computes this tensor for the anisotropic case, or   taking into account crystal symmetries, for example. The second one,    [2.x.618]  and indices    [2.x.619]  and  [2.x.620]  and returns the symmetric gradient, i.e. the strain,   corresponding to shape function  [2.x.621] , evaluated on the cell   on which the  [2.x.622]  object was last reinitialized. 

  Given this, the innermost loop of  [2.x.623]  computes the   local contributions to the matrix in the following elegant way (the variable    [2.x.624] , corresponding to the tensor  [2.x.625] , has   previously been initialized with the result of the first function above):   [1.x.219] 

  It is worth noting the expressive power of this piece of code, and to   compare it with the complications we had to go through in previous examples   for the elasticity problem. (To be fair, the SymmetricTensor class   template did not exist when these previous examples were written.) For   simplicity,  [2.x.626]  provides for the (double summation) product   between symmetric tensors of even rank here. 

  Assembling the local contributions   [1.x.220] 

  to the right hand side of [1.x.221] is equally   straightforward (note that we do not consider any boundary tractions  [2.x.627]  here). Remember that we only had to store the old stress in the   quadrature points of cells. In the program, we will provide a variable    [2.x.628]  that allows to access the stress    [2.x.629]  in each quadrature point. With this the code for the right   hand side looks as this, again rather elegant:   [1.x.222] 

  Note that in the multiplication  [2.x.630] , we have made use of the fact that for the chosen finite element, only   one vector component (namely  [2.x.631] ) of  [2.x.632]  is   nonzero, and that we therefore also have to consider only one component of    [2.x.633] . 

  This essentially concludes the new material we present in this function. It   later has to deal with boundary conditions as well as hanging node   constraints, but this parallels what we had to do previously in other   programs already. 

 [2.x.634]   [2.x.635] ]:   Unlike the previous one, this function is not really interesting, since it   does what similar functions have done in all previous tutorial programs --   solving the linear system using the CG method, using an incomplete LU   decomposition as a preconditioner (in the %parallel case, it uses an ILU of   each processor's block separately). It is virtually unchanged   from step-17. 

 [2.x.636]   [2.x.637]  [via    [2.x.638] ]: Based on the displacement field  [2.x.639]  computed before, we update the stress values in all quadrature points   according to [1.x.223] and [1.x.224],   including the rotation of the coordinate system. 

 [2.x.640]   [2.x.641] : Given the solution computed before, in this   function we deform the mesh by moving each vertex by the displacement vector   field evaluated at this particular vertex. 

 [2.x.642]   [2.x.643] : This function simply outputs the solution   based on what we have said above, i.e. every processor computes output only   for its own portion of the domain. In addition to the solution, we also compute the norm of   the stress averaged over all the quadrature points on each cell.  [2.x.644]  

With this general structure of the code, we only have to define what case we want to solve. For the present program, we have chosen to simulate the quasistatic deformation of a vertical cylinder for which the bottom boundary is fixed and the top boundary is pushed down at a prescribed vertical velocity. However, the horizontal velocity of the top boundary is left unspecified -- one can imagine this situation as a well-greased plate pushing from the top onto the cylinder, the points on the top boundary of the cylinder being allowed to slide horizontally along the surface of the plate, but forced to move downward by the plate. The inner and outer boundaries of the cylinder are free and not subject to any prescribed deflection or traction. In addition, gravity acts on the body. 

The program text will reveal more about how to implement this situation, and the results section will show what displacement pattern comes out of this simulation. 


examples/step-18/doc/results.dox 



[1.x.225] 


Running the program takes a good while if one uses debug mode; it takes about eleven minutes on my i7 desktop. Fortunately, the version compiled with optimizations is much faster; the program only takes about a minute and a half after recompiling with the command <tt>make release</tt> on the same machine, a much more reasonable time. 


If run, the program prints the following output, explaining what it is doing during all that time: 

[1.x.226] 

In other words, it is computing on 12,000 cells and with some 52,000 unknowns. Not a whole lot, but enough for a coupled three-dimensional problem to keep a computer busy for a while. At the end of the day, this is what we have for output: 

[1.x.227] 




If we visualize these files with VisIt or Paraview, we get to see the full picture of the disaster our forced compression wreaks on the cylinder (colors in the images encode the norm of the stress in the material): 


<div class="threecolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0002.0000.png"            alt="Time = 2"            width="400">     </div>     <div class="text" align="center">       Time = 2     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0005.0000.png"            alt="Time = 5"            width="400">     </div>     <div class="text" align="center">       Time = 5     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0007.0000.png"            alt="Time = 7"            width="400">     </div>     <div class="text" align="center">       Time = 7     </div>   </div> </div> 


<div class="threecolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0008.0000.png"            alt="Time = 8"            width="400">     </div>     <div class="text" align="center">       Time = 8     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0009.0000.png"            alt="Time = 9"            width="400">     </div>     <div class="text" align="center">       Time = 9     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0010.0000.png"            alt="Time = 10"            width="400">     </div>     <div class="text" align="center">       Time = 10     </div>   </div> </div> 


As is clearly visible, as we keep compressing the cylinder, it starts to bow out near the fully constrained bottom surface and, after about eight time units, buckle in an azimuthally symmetric manner. 


Although the result appears plausible for the symmetric geometry and loading, it is yet to be established whether or not the computation is fully converged. In order to see whether it is, we ran the program again with one more global refinement at the beginning and with the time step halved. This would have taken a very long time on a single machine, so we used a proper workstation and ran it on 16 processors in parallel. The beginning of the output now looks like this: 

[1.x.228] 

That's quite a good number of unknowns, given that we are in 3d. The output of this program are 16 files for each time step: 

[1.x.229] 




Here are first the mesh on which we compute as well as the partitioning for the 16 processors: 


<div class="twocolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-000mesh.png"            alt="Discretization"            width="400">     </div>     <div class="text" align="center">       Discretization     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0002.p.png"            alt="Parallel partitioning"            width="400">     </div>     <div class="text" align="center">       Parallel partitioning     </div>   </div> </div> 


Finally, here is the same output as we have shown before for the much smaller sequential case: 

<div class="threecolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0002.s.png"            alt="Time = 2"            width="400">     </div>     <div class="text" align="center">       Time = 2     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0005.s.png"            alt="Time = 5"            width="400">     </div>     <div class="text" align="center">       Time = 5     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0007.s.png"            alt="Time = 7"            width="400">     </div>     <div class="text" align="center">       Time = 7     </div>   </div> </div> 


<div class="threecolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0008.s.png"            alt="Time = 8"            width="400">     </div>     <div class="text" align="center">       Time = 8     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0009.s.png"            alt="Time = 9"            width="400">     </div>     <div class="text" align="center">       Time = 9     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0010.s.png"            alt="Time = 10"            width="400">     </div>     <div class="text" align="center">       Time = 10     </div>   </div> </div> 


As before, we observe that at high axial compression the cylinder begins to buckle, but this time ultimately collapses on itself. In contrast to our first run, towards the end of the simulation the deflection pattern becomes nonsymmetric (the central bulge deflects laterally). The model clearly does not provide for this (all our forces and boundary deflections are symmetric) but the effect is probably physically correct anyway: in reality, small inhomogeneities in the body's material properties would lead it to buckle to one side to evade the forcing; in numerical simulations, small perturbations such as numerical round-off or an inexact solution of a linear system by an iterative solver could have the same effect. Another typical source for asymmetries in adaptive computations is that only a certain fraction of cells is refined in each step, which may lead to asymmetric meshes even if the original coarse mesh was symmetric. 


If one compares this with the previous run, the results both qualitatively and quantitatively different. The previous computation was therefore certainly not converged, though we can't say for sure anything about the present one. One would need an even finer computation to find out. However, the point may be moot: looking at the last picture in detail, it is pretty obvious that not only is the linear small deformation model we chose completely inadequate, but for a realistic simulation we would also need to make sure that the body does not intersect itself during deformation (if we continued compressing the cylinder we would observe some self-intersection). Without such a formulation we cannot expect anything to make physical sense, even if it produces nice pictures! 




[1.x.230] 

The program as is does not really solve an equation that has many applications in practice: quasi-static material deformation based on a purely elastic law is almost boring. However, the program may serve as the starting point for more interesting experiments, and that indeed was the initial motivation for writing it. Here are some suggestions of what the program is missing and in what direction it may be extended: 

[1.x.231] 

 The most obvious extension is to use a more realistic material model for large-scale quasistatic deformation. The natural choice for this would be plasticity, in which a nonlinear relationship between stress and strain replaces equation [1.x.232]. Plasticity models are usually rather complicated to program since the stress-strain dependence is generally non-smooth. The material can be thought of being able to withstand only a maximal stress (the yield stress) after which it starts to &ldquo;flow&rdquo;. A mathematical description to this can be given in the form of a variational inequality, which alternatively can be treated as minimizing the elastic energy 

[1.x.233] 

subject to the constraint 

[1.x.234] 

on the stress. This extension makes the problem to be solved in each time step nonlinear, so we need another loop within each time step. 

Without going into further details of this model, we refer to the excellent book by Simo and Hughes on &ldquo;Computational Inelasticity&rdquo; for a comprehensive overview of computational strategies for solving plastic models. Alternatively, a brief but concise description of an algorithm for plasticity is given in an article by S. Commend, A. Truty, and Th. Zimmermann;  [2.x.645] . 




[1.x.235] 

The formulation we have chosen, i.e. using piecewise (bi-, tri-)linear elements for all components of the displacement vector, and treating the stress as a variable dependent on the displacement is appropriate for most materials. However, this so-called displacement-based formulation becomes unstable and exhibits spurious modes for incompressible or nearly-incompressible materials. While fluids are usually not elastic (in most cases, the stress depends on velocity gradients, not displacement gradients, although there are exceptions such as electro-rheologic fluids), there are a few solids that are nearly incompressible, for example rubber. Another case is that many plasticity models ultimately let the material become incompressible, although this is outside the scope of the present program. 

Incompressibility is characterized by Poisson's ratio 

[1.x.236] 

where  [2.x.646]  are the Lam&eacute; constants of the material. Physical constraints indicate that  [2.x.647]  (the condition also follows from mathematical stability considerations). If  [2.x.648]  approaches  [2.x.649] , then the material becomes incompressible. In that case, pure displacement-based formulations are no longer appropriate for the solution of such problems, and stabilization techniques have to be employed for a stable and accurate solution. The book and paper cited above give indications as to how to do this, but there is also a large volume of literature on this subject; a good start to get an overview of the topic can be found in the references of the paper by H.-Y. Duan and Q. Lin;  [2.x.650] . 




[1.x.237] 

In the present form, the program only refines the initial mesh a number of times, but then never again. For any kind of realistic simulation, one would want to extend this so that the mesh is refined and coarsened every few time steps instead. This is not hard to do, in fact, but has been left for future tutorial programs or as an exercise, if you wish. 

The main complication one has to overcome is that one has to transfer the data that is stored in the quadrature points of the cells of the old mesh to the new mesh, preferably by some sort of projection scheme. The general approach to this would go like this: 

- At the beginning, the data is only available in the quadrature points of   individual cells, not as a finite element field that is defined everywhere. 

- So let us find a finite element field that [1.x.238] defined everywhere so   that we can later interpolate it to the quadrature points of the new   mesh. In general, it will be difficult to find a continuous finite element   field that matches the values in the quadrature points exactly because the   number of degrees of freedom of these fields does not match the number of   quadrature points there are, and the nodal values of this global field will   either be over- or underdetermined. But it is usually not very difficult to   find a discontinuous field that matches the values in the quadrature points;   for example, if you have a QGauss(2) quadrature formula (i.e. 4 points per   cell in 2d, 8 points in 3d), then one would use a finite element of kind   FE_DGQ(1), i.e. bi-/tri-linear functions as these have 4 degrees of freedom   per cell in 2d and 8 in 3d. 

- There are functions that can make this conversion from individual points to   a global field simpler. The following piece of pseudo-code should help if   you use a QGauss(2) quadrature formula. Note that the multiplication by the   projection matrix below takes a vector of scalar components, i.e., we can only   convert one set of scalars at a time from the quadrature points to the degrees   of freedom and vice versa. So we need to store each component of stress separately,   which requires  [2.x.651]  vectors. We'll store this set of vectors in a 2D array to   make it easier to read off components in the same way you would the stress tensor.   Thus, we'll loop over the components of stress on each cell and store   these values in the global history field. (The prefix  [2.x.652]    indicates that we work with quantities related to the history variables defined   in the quadrature points.)   [1.x.239] 



- Now that we have a global field, we can refine the mesh and transfer the   history_field vector as usual using the SolutionTransfer class. This will   interpolate everything from the old to the new mesh. 

- In a final step, we have to get the data back from the now interpolated   global field to the quadrature points on the new mesh. The following code   will do that:   [1.x.240] 



It becomes a bit more complicated once we run the program in parallel, since then each process only stores this data for the cells it owned on the old mesh. That said, using a parallel vector for  [2.x.653]  will do the trick if you put a call to  [2.x.654]  after the transfer from quadrature points into the global vector. 




[1.x.241] 

At present, the program makes no attempt to make sure that a cell, after moving its vertices at the end of the time step, still has a valid geometry (i.e. that its Jacobian determinant is positive and bounded away from zero everywhere). It is, in fact, not very hard to set boundary values and forcing terms in such a way that one gets distorted and inverted cells rather quickly. Certainly, in some cases of large deformation, this is unavoidable with a mesh of finite mesh size, but in some other cases this should be preventable by appropriate mesh refinement and/or a reduction of the time step size. The program does not do that, but a more sophisticated version definitely should employ some sort of heuristic defining what amount of deformation of cells is acceptable, and what isn't. 


examples/step-19/doc/intro.dox 



 [2.x.655]  

[1.x.242] 

 [2.x.656]  Support for particles exists in deal.II primarily due to the initial   efforts of Rene Gassmoeller. Please acknowledge this work by citing   the publication  [2.x.657]  if you use particle functionality in your   own work. 

[1.x.243] 

[1.x.244] 

The finite element method in general, and deal.II in particular, were invented to solve partial differential equations -- in other words, to solve [continuum mechanics](https://en.wikipedia.org/wiki/Continuum_mechanics) problems. On the other hand, sometimes one wants to solve problems in which it is useful to track individual objects ("particles") and how their positions evolve. If this simply leads to a set of ordinary differential equations, for example if you want to track the positions of the planets in the solar system over time, then deal.II is clearly not your right tool. On the other hand, if this evolution is due to the interaction with the solution of partial differential equation, or if having a mesh to determine which particles interact with others (such as in the [smoothed particle hydrodynamics (SPH)](https://en.wikipedia.org/wiki/Smoothed-particle_hydrodynamics) method), then deal.II has support for you. 

The case we will consider here is how electrically charged particles move through an electric field. As motivation, we will consider [cathode rays](https://en.wikipedia.org/wiki/Cathode_ray): Electrons emitted by a heated piece of metal that is negatively charged (the "cathode"), and that are then accelerated by an electric field towards the positively charged electrode (the "anode"). The anode is typically ring-shaped so that the majority of electrons can fly through the hole in the form of an electron beam. In the olden times, they might then have illuminated the screen of a TV built from a [cathode ray tube](https://en.wikipedia.org/wiki/Cathode-ray_tube). Today, instead, electron beams are useful in [X-ray machines](https://en.wikipedia.org/wiki/X-ray_tube), [electron beam lithography](https://en.wikipedia.org/wiki/Electron-beam_lithography), [electron beam welding](https://en.wikipedia.org/wiki/Electron-beam_welding), and a number of other areas. 

The equations we will then consider are as follows: First, we need to describe the electric field. This is most easily accomplished by noting that the electric potential  [2.x.658]  satisfied the equation 

[1.x.245] 

where  [2.x.659]  is the dielectric constant of vacuum, and  [2.x.660]  is the charge density. This is augmented by boundary conditions that we will choose as follows: 

[1.x.246] 

In other words, we prescribe voltages  [2.x.661]  and  [2.x.662]  at the two electrodes and insulating (Neumann) boundary conditions elsewhere. Since the dynamics of the particles are purely due to the electric field  [2.x.663] , we could as well have prescribed  [2.x.664]  and  [2.x.665]  at the two electrodes -- all that matters is the voltage difference at the two electrodes. 

Given this electric potential  [2.x.666]  and the electric field  [2.x.667] , we can describe the trajectory of the  [2.x.668] th particle using the differential equation 

[1.x.247] 

where  [2.x.669]  are the mass and electric charge of each particle. In practice, it is convenient to write this as a system of first-order differential equations in the position  [2.x.670]  and velocity  [2.x.671] : 

[1.x.248] 

The deal.II class we will use to deal with particles,  [2.x.672]  stores particles in a way so that the position  [2.x.673]  is part of the  [2.x.674]  data structures. (It stores particles sorted by cell they are in, and consequently needs to know where each particle is.) The velocity  [2.x.675] , on the other hand, is of no concern to  [2.x.676]  and consequently we will store it as a "property" of each particle that we will update in each time step. Properties can also be used to store any other quantity we might care about each particle: its charge, or if they were larger than just an electron, its color, mass, attitude in space, chemical composition, etc. 

There remain two things to discuss to complete the model: Where particles start and what the charge density  [2.x.677]  is. 

First, historically, cathode rays used very large electric fields to pull electrons out of the metal. This produces only a relatively small current. One can do better by heating the cathode: a statistical fraction of electrons in that case have enough thermal energy to leave the metal; the electric field then just has to be strong enough to pull them away from the attraction of their host body. We will model this in the following way: We will create a new particle if (i) the electric field points away from the electrode, i.e., if  [2.x.678]  where  [2.x.679]  is the normal vector at a face pointing out of the domain (into the electrode), and (ii) the electric field exceeds a threshold value  [2.x.680] . This is surely not a sufficiently accurate model for what really happens, but is good enough for our current tutorial program. 

Second, in principle we would have to model the charge density via 

[1.x.249] 



 [2.x.681]  The issue now is that in reality, a cathode ray tube in an old television yields a current of somewhere around a few milli-Amperes. In the much higher energy beams of particle accelerators, the current may only be a few nano-Ampere. But an Ampere is  [2.x.682]  electrons flowing per second. Now, as you will see in the results section, we really only simulate a few microseconds ( [2.x.683]  seconds), but that still results in very very large numbers of electrons -- far more than we can hope to simulate with a program as small as the current one. As a consequence, let us presume that each particle represents  [2.x.684]  electrons. Then the particle mass and charge are also  [2.x.685]  and  [2.x.686]  and the equations we have to solve are 

[1.x.250] 

which is of course exactly the same as above. On the other hand, the charge density for these "clumps" of electrons is given by 

[1.x.251] 

It is this form that we will implement in the program, where  [2.x.687]  is chosen rather large in the program to ensure that the particles actually affect the electric field. (This may not be realistic in practice: In most cases, there are just not enough electrons to actually affect the overall electric field. But realism is not our goal here.) 




 [2.x.688]  One may wonder why the equation for the electric field (or, rather, the electric potential) has no time derivative whereas the equations for the electron positions do. In essence, this is a modeling assumption: We assume that the particles move so slowly that at any given time the electric field is in equilibrium. This is saying, in other words, that the velocity of the electrons is much less than the speed of light. In yet other words, we can rephrase this in terms of the electrode voltage  [2.x.689] : Since every volt of electric potential accelerates electrons by approximately 600 km/s (neglecting relativistic effects), requiring  [2.x.690]  is equivalent to saying that  [2.x.691] . Under this assumption (and the assumption that the total number of electrons is small), one can also neglect the creation of magnetic fields by the moving charges, which would otherwise also affect the movement of the electrons. 




[1.x.252] 

The equations outlined above form a set of coupled differential equations. Let us bring them all together in one place again to make that clear: 

[1.x.253] 

Because of the awkward dependence of the electric potential on the particle locations, we don't want to solve this as a coupled system but instead use a decoupled approach where we first solve for the potential in each time step and then the particle locations. (One could also do it the other way around, of course.) This is very much in the same spirit as we do in step-21, step-31, and step-32, to name just a few, and can all be understood in the context of the operator splitting methods discussed in step-58. 

So, if we denote by an upper index  [2.x.692]  the time step, and if we use a simple time discretization for the ODE, then this means that we have to solve the following set of equations in each time step: 

[1.x.254] 

There are of course many better ways to do a time discretization (for example the simple [leapfrog scheme](https://en.wikipedia.org/wiki/Leapfrog_integration)) but this isn't the point of the tutorial program, and so we will be content with what we have here. (We will comment on a piece of this puzzle in the [1.x.255] section of this program, however.) 

There remains the question of how we should choose the time step size  [2.x.693] . The limitation here is that the  [2.x.694]  class needs to keep track of which cell each particle is in. This is particularly an issue if we are running computations in parallel (say, in step-70) because in that case every process only stores those cells it owns plus one layer of "ghost cells". That's not relevant here, but in general we should make sure that over the course of each time step, a particle moves only from one cell to any of its immediate neighbors (face, edge, or vertex neighbors). If we can ensure that, then  [2.x.695]  is guaranteed to be able to figure out which cell a particle ends up in. To do this, a useful rule of thumb is that we should choose the time step so that for all particles the expected distance the particle moves by is less than one cell diameter: 

[1.x.256] 

or equivalently 

[1.x.257] 

Here,  [2.x.696]  is the length of the shortest edge of the cell on which particle  [2.x.697]  is located -- in essence, a measure of the size of a cell. 

On the other hand, a particle might already be at the boundary of one cell and the neighboring cell might be once further refined. So then the time to cross that *neighboring* cell would actually be half the amount above, suggesting 

[1.x.258] 



But even that is not good enough: The formula above updates the particle positions in each time using the formula 

[1.x.259] 

that is, using the *current* velocity  [2.x.698] . But we don't have the current velocity yet at the time when we need to choose  [2.x.699]  -- which is after we have updated the potential  [2.x.700]  but before we update the velocity from  [2.x.701]  to  [2.x.702] . All we have is  [2.x.703] . So we need an additional safety factor for our final choice: 

[1.x.260] 

How large should  [2.x.704]  be? That depends on how much of underestimate  [2.x.705]  might be compared to  [2.x.706] , and that is actually quite easy to assess: A particle created in one time step with zero velocity will roughly pick up equal velocity increments in each successive time step if the electric field it encounters along the way were roughly constant. So the maximal difference between  [2.x.707]  and  [2.x.708]  would be a factor of two. As a consequence, we will choose  [2.x.709] . 

There is only one other case we ought to consider: What happens in the very first time step? There, any particles to be moved along have just been created, but they have a zero velocity. So we don't know what velocity we should choose for them. Of course, in all other time steps there are also particles that have just been created, but in general, the particles with the highest velocity limit the time step size and so the newly created particles with their zero velocity don't matter. But if we *only* have such particles? 

In that case, we can use the following approximation: If a particle starts at  [2.x.710] , then the update formula tells us that 

[1.x.261] 

and consequently 

[1.x.262] 

which we can write as 

[1.x.263] 

Not wanting to move a particle by more than  [2.x.711]  then implies that we should choose the time step as 

[1.x.264] 

Using the same argument about neighboring cells possibly being smaller by a factor of two then leads to the final formula for time step zero: 

[1.x.265] 



Strictly speaking, we would have to evaluate the electric potential  [2.x.712]  at the location of each particle, but a good enough approximation is to use the maximum of the values at the vertices of the respective cell. (Why the vertices and not the midpoint? Because the gradient of the solution of the Laplace equation, i.e., the electric field, is largest in corner singularities which are located at the vertices of cells.) This has the advantage that we can make good use of the FEValues functionality which can recycle pre-computed material as long as the quadrature points are the same from one cell to the next. 

We could always run this kind of scheme to estimate the difference between  [2.x.713]  and  [2.x.714] , but it relies on evaluating the electric field  [2.x.715]  on each cell, and that is expensive. As a consequence, we will limit this approach to the very first time step. 




[1.x.266] 

Having discussed the time discretization, the discussion of the spatial discretization is going to be short: We use quadratic finite elements, i.e., the space  [2.x.716] , to approximate the electric potential  [2.x.717] . The mesh is adapted a couple of times during the initial time step. All of this is entirely standard if you have read step-6, and the implementation does not provide for any kind of surprise. 




[1.x.267] 

Adding and moving particles is, in practice, not very difficult in deal.II. To add one, the `create_particles()` function of this program simply uses a code snippet of the following form: 

[1.x.268] 

In other words, it is not all that different from inserting an object into a  [2.x.718]  or  [2.x.719]  Create the object, set its properties (here, the current location, its reference cell location, and its id) and call `insert_particle`. The only thing that may be surprising is the reference location: In order to evaluate things such as  [2.x.720] , it is necessary to evaluate finite element fields at locations  [2.x.721] . But this requires evaluating the finite element shape functions at points on the reference cell  [2.x.722] . To make this efficient, every particle doesn't just store its location and the cell it is on, but also what location that point corresponds to in the cell's reference coordinate system. 

Updating a particle's position is then no more difficult: One just has to call 

[1.x.269] 

We do this in the `move_particles()` function. The only difference is that we then have to tell the  [2.x.723]  class to also find what cell that position corresponds to (and, when computing in parallel, which process owns this cell). For efficiency reason, this is most easily done after updating all particles' locations, and is achieved via the  [2.x.724]  function. 

There are, of course, times where a particle may leave the domain in question. In that case,  [2.x.725]  can not find a surrounding cell and simply deletes the particle. But, it is often useful to track the number of particles that have been lost this way, and for this the  [2.x.726]  class offers a "signal" that one can attach to. We show how to do this in the constructor of the main class to count how many particles were lost in each time step. Specifically, the way this works is that the  [2.x.727]  class has a "signal" to which one can attach a function that will be executed whenever the signal is triggered. Here, this looks as follows: 

[1.x.270] 

That's a bit of a mouthful, but what's happening is this: We declare a lambda function that "captures" the `this` pointer (so that we can access member functions of the surrounding object inside the lambda function), and that takes two arguments: 

- A reference to the particle that has been "lost". 

- A reference to the cell it was on last. The lambda function then simply calls the  [2.x.728]  function with these arguments. When we attach this lambda function to the signal, the  [2.x.729]  function will trigger the signal for every particle for which it can't find a new home. This gives us the chance to record where the particle is, and to record statistics on it. 




 [2.x.730]  In this tutorial program, we insert particles by hand and at   locations we specifically choose based on conditions that include   the solution of the electrostatic problem. But there are other cases   where one primarily wants to use particles as passive objects, for   example to trace and visualize the flow field of a fluid flow   problem. In those cases, there are numerous functions in the    [2.x.731]  namespace that can generate particles   automatically. One of the functions of this namespace is also used   in the step-70 tutorial program, for example. 




[1.x.271] 

The test case here is not meant to be a realistic depiction of a cathode ray tube, but it has the right general characteristics and the point is, in any case, only to demonstrate how one would implement deal.II codes that use particles. 

The following picture shows the geometry that we're going to use: 

<p align="center">   <img src="https://www.dealii.org/images/steps/developer/step-19.geometry.png"        alt="The geometry used in this program"        width="600">  [2.x.732]  

In this picture, the parts of the boundary marked in red and blue are the cathode, held at an electric potential  [2.x.733] . The part of the cathode shown in red is the part that is heated, leading to electrons leaving the metal and then being accelerated by the electric field (a few electric field lines are also shown). The green part of the boundary is the anode, held at  [2.x.734] . The rest of the boundary satisfies a Neumann boundary condition. 

This setup mimics real devices. The re-entrant corner results in an electric potential  [2.x.735]  whose derivative (the electric field  [2.x.736] ) has a singularity -- in other words, it becomes very large in the vicinity of the corner, allowing it to rip electrons away from the metal. These electrons are then accelerated towards the (green) anode which has a hole in the middle through which the electrons can escape the device and fly on to hit the screen, where they excite the "phosphor" to then emit the light that we see from these old-style TV screens. The non-heated part of the cathode is not subject to the emission of electrons -- in the code, we will mark this as the "focussing element" of the tube, because its negative electric voltage repels the electrons and makes sure that they do not just fly away from the heated part of the cathode perpendicular to the boundary, but in fact bend their paths towards the anode on the right. 

The electric field lines also shown in the picture illustrate that the electric field connects the negative and positive electrodes, respectively. The accelerating force the electrons experience is along these field lines. Finally, the picture shows the mesh used in the computation, illustrating that there are singularities at the tip of the re-rentrant corner as well as at all places where the boundary conditions change; these singularities are visible because the mesh is refined in these locations. 

Of practical interest is to figure out which fraction of the electrons emitted from the cathode actually make it through the hole in the anode -- electrons that just bounce into the anode itself are not actually doing anything useful other than converting electricity into heat. As a consequence, in the `track_lost_particle()` function (which is called for each particle that leaves the domain, see above), we will estimate where it might have left the domain and report this in the output. 




 [2.x.737]  It is worth repeating that neither the geometry used here, nor in fact any other aspect of this program is intended to represent anything even half-way realistic. Tutorial programs are our tools to teach how deal.II works, and we often use situations for which we have some kind of intuition since this helps us interpret the output of a program, but that's about the extent to which we intend the program to do anything of use besides being a teaching tool. 


examples/step-19/doc/results.dox 



[1.x.272] 

When this program is run, it produces output that looks as follows: ``` Timestep 1   Field degrees of freedom:                                 4989   Total number of particles in simulation:  20   Number of particles lost this time step:  0 

  Now at t=2.12647e-07, dt=2.12647e-07. 

Timestep 2   Field degrees of freedom:                 4989   Total number of particles in simulation:  24   Number of particles lost this time step:  0 

  Now at t=4.14362e-07, dt=2.01715e-07. 

Timestep 3   Field degrees of freedom:                 4989   Total number of particles in simulation:  28   Number of particles lost this time step:  0 

  Now at t=5.96019e-07, dt=1.81657e-07. 

Timestep 4   Field degrees of freedom:                 4989   Total number of particles in simulation:  32   Number of particles lost this time step:  0 

  Now at t=7.42634e-07, dt=1.46614e-07. 


... 


  Timestep 1000   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  6   Fraction of particles lost through anode: 0.0601266 

  Now at t=4.93276e-05, dt=4.87463e-08. 

Timestep 1001   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  0   Fraction of particles lost through anode: 0.0601266 

  Now at t=4.93759e-05, dt=4.82873e-08. 


... 


Timestep 2091   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  0   Fraction of particles lost through anode: 0.0503338 

  Now at t=9.99237e-05, dt=4.26254e-08. 

Timestep 2092   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  0   Fraction of particles lost through anode: 0.0503338 

  Now at t=9.99661e-05, dt=4.24442e-08. 

Timestep 2093   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  2   Fraction of particles lost through anode: 0.050308 

  Now at t=0.0001, dt=3.38577e-08. ``` 

Picking a random few time steps, we can visualize the solution in the form of streamlines for the electric field and dots for the electrons: <div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-19.solution.0000.png"          alt="The solution at time step 0 (t=0 seconds)."          width="500">      [2.x.738]      Solution at time step 0 (t=0 seconds).      [2.x.739]    </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-19.solution.1400.png"          alt="The solution at time step 1400 (t=0.000068 seconds)."          width="500">      [2.x.740]      Solution at time step 1400 (t=0.000068 seconds).      [2.x.741]    </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-19.solution.0700.png"          alt="The solution at time step 700 (t=0.000035 seconds)."          width="500">      [2.x.742]      Solution at time step 700 (t=0.000035 seconds).      [2.x.743]    </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-19.solution.2092.png"          alt="The solution at time step 2092 (t=0.0001 seconds)."          width="500">      [2.x.744]      Solution at time step 2092 (t=0.0001 seconds).      [2.x.745]    </div> </div> 

That said, a more appropriate way to visualize the results of this program are by creating a video that shows how these electrons move, and how the electric field changes in response to their motion: 

[1.x.273] 



What you can see here is how the "focus element" of the boundary with its negative voltage repels the electrons and makes sure that they do not just fly away perpendicular from the cathode (as they do in the initial part of their trajectories). It also shows how the electric field lines move around over time, in response to the charges flying by -- in other words, the feedback the particles have on the electric field that itself drives the motion of the electrons. 

The movie suggests that electrons move in "bunches" or "bursts". One element of this appearance is an artifact of how the movie was created: Every frame of the movie corresponds to one time step, but the time step length varies. More specifically, the fastest particle moving through the smallest cell determines the length of the time step (see the discussion in the introduction), and consequently time steps are small whenever a (fast) particle moves through the small cells at the right edge of the domain; time steps are longer again once the particle has left the domain. This slowing-accelerating effect can easily be visualized by plotting the time step length shown in the screen output. 

The second part of this is real, however: The simulation creates a large group of particles in the beginning, and fewer after about the 300th time step. This is probably because of the negative charge of the particles in the simulation: They reduce the magnitude of the electric field at the (also negatively charged electrode) and consequently reduce the number of points on the cathode at which the magnitude exceeds the threshold necessary to draw an electron out of the electrode. 


[1.x.274] 

[1.x.275] 

[1.x.276] 

The `assemble_system()`, `move_particles()`, and `update_timestep_size()` functions all call  [2.x.746]  and  [2.x.747]  that query information about the particles located on the current cell. While this is convenient, it's also inefficient. To understand why this is so, one needs to know how particles are stored in  [2.x.748]  namely, in a data structure in which particles are ordered in some kind of linear fashion sorted by the cell they are on. Consequently, in order to find the particles associated with a given cell, these functions need to search for the first (and possibly last) particle on a given cell -- an effort that costs  [2.x.749]  operations where  [2.x.750]  is the number of particles. But this is repeated on every cell; assuming that for large computations, the number of cells and particles are roughly proportional, the accumulated cost of these function calls is then  [2.x.751]  and consequently larger than the  [2.x.752]  cost that we should shoot for with all parts of a program. 

We can make this cheaper, though. First, instead of calling  [2.x.753]  we might first call  [2.x.754]  and then compute the number of particles on a cell by just computing the distance of the last to the first particle on the current cell: 

[1.x.277] 

The first of these calls is of course still  [2.x.755] , but at least the second call only takes a compute time proportional to the number of particles on the current cell and so, when accumulated over all cells, has a cost of  [2.x.756] . 

But we can even get rid of the first of these calls with some proper algorithm design. That's because particles are ordered in the same way as cells, and so we can just walk them as we move along on the cells. The following outline of an algorithm does this: 

[1.x.278] 



In this code, we touch every cell exactly once and we never have to search the big data structure for the first or last particle on each cell. As a consequence, the algorithm costs a total of  [2.x.757]  for a complete sweep of all particles and all cells. 

It would not be very difficult to implement this scheme for all three of the functions in this program that have this issue. 




[1.x.279] 

The program already computes the fraction of the electrons that leave the domain through the hole in the anode. But there are other quantities one might be interested in. For example, the average velocity of these particles. It would not be very difficult to obtain each particle's velocity from its properties, in the same way as we do in the `move_particles()` function, and compute statistics from it. 




[1.x.280] 

As discussed above, there is a varying time difference between different frames of the video because we create output for every time step. A better way to create movies would be to generate a new output file in fixed time intervals, regardless of how many time steps lie between each such point. 




[1.x.281] 

The problem we are considering in this program is a coupled, multiphysics problem. But the way we solve it is by first computing the (electric) potential field and then update the particle locations. This is what is called an "operator-splitting method", a concept we will investigate in more detail in step-58. 

While it is awkward to think of a way to solve this problem that does not involve splitting the problem into a PDE piece and a particles piece, one *can* (and probably should!) think of a better way to update the particle locations. Specifically, the equations we use to update the particle location are 

[1.x.282] 

This corresponds to a simple forward-Euler time discretization -- a method of first order accuracy in the time step size  [2.x.758]  that we know we should avoid because we can do better. Rather, one might want to consider a scheme such as the [leapfrog scheme](https://en.wikipedia.org/wiki/Leapfrog_integration) or more generally [symplectic integrators](https://en.wikipedia.org/wiki/Symplectic_integrator) such as the [Verlet scheme](https://en.wikipedia.org/wiki/Verlet_integration). 




[1.x.283] 

In release mode, the program runs in about 3.5 minutes on one of the author's laptops at the time of writing this. That's acceptable. But what if we wanted to make the simulation three-dimensional? If we wanted to not use a maximum of around 100 particles at any given time (as happens with the parameters used here) but 100,000? If we needed a substantially finer mesh? 

In those cases, one would want to run the program not just on a single processor, but in fact on as many as one has available. This requires parallelization both the PDE solution as well as over particles. In practice, while there are substantial challenges to making this efficient and scale well, these challenges are all addressed in deal.II itself. For example, step-40 shows how to parallelize the finite element part, and step-70 shows how one can then also parallelize the particles part. 


examples/step-2/doc/intro.dox 

[1.x.284] 

[1.x.285] 

 [2.x.759]  

After we have created a grid in the previous example, we now show how to define degrees of freedom on this mesh. For this example, we will use the lowest order ( [2.x.760] ) finite elements, for which the degrees of freedom are associated with the vertices of the mesh. Later examples will demonstrate higher order elements where degrees of freedom are not necessarily associated with vertices any more, but can be associated with edges, faces, or cells. 

The term "degree of freedom" is commonly used in the finite element community to indicate two slightly different, but related things. The first is that we'd like to represent the finite element solution as a linear combination of shape functions, in the form  [2.x.761] . Here,  [2.x.762]  is a vector of expansion coefficients. Because we don't know their values yet (we will compute them as the solution of a linear or nonlinear system), they are called "unknowns" or "degrees of freedom". The second meaning of the term can be explained as follows: A mathematical description of finite element problems is often to say that we are looking for a finite dimensional function  [2.x.763]  that satisfies some set of equations (e.g.  [2.x.764]  for all test functions  [2.x.765] ). In other words, all we say here that the solution needs to lie in some space  [2.x.766] . However, to actually solve this problem on a computer we need to choose a basis of this space; this is the set of shape functions  [2.x.767]  we have used above in the expansion of  [2.x.768]  with coefficients  [2.x.769] . There are of course many bases of the space  [2.x.770] , but we will specifically choose the one that is described by the finite element functions that are traditionally defined locally on the cells of the mesh. Describing "degrees of freedom" in this context requires us to simply [1.x.286] the basis functions of the space  [2.x.771] . For  [2.x.772]  elements this means simply enumerating the vertices of the mesh in some way, but for higher order elements, one also has to enumerate the shape functions that are associated with edges, faces, or cell interiors of the mesh. In other words, the enumeration of degrees of freedom is an entirely separate thing from the indices we use for vertices. The class that provides this enumeration of the basis functions of  [2.x.773]  is called DoFHandler. 

Defining degrees of freedom ("DoF"s in short) on a mesh is a rather simple task, since the library does all the work for you. Essentially, all you have to do is create a finite element object (from one of the many finite element classes deal.II already has, see for example the  [2.x.774]  documentation) and give it to a DoFHandler object through the  [2.x.775]  function ("distributing DoFs" is the term we use to describe the process of [1.x.287] the basis functions as discussed above). The DoFHandler is a class that knows which degrees of freedom live where, i.e., it can answer questions like "how many degrees of freedom are there globally" and "on this cell, give me the global indices of the shape functions that live here". This is the sort of information you need when determining how big your system matrix should be, and when copying the contributions of a single cell into the global matrix. 

[1.x.288] 

The next step would then be to compute a matrix and right hand side corresponding to a particular differential equation using this finite element and mesh. We will keep this step for the step-3 program and rather talk about one practical aspect of a finite element program, namely that finite element matrices are always very sparse: almost all entries in these matrices are zero. 

To be more precise, we say that a matrix is sparse if the number of nonzero entries [1.x.289] in the matrix is bounded by a number that is independent of the overall number of degrees of freedom. For example, the simple 5-point stencil of a finite difference approximation of the Laplace equation leads to a sparse matrix since the number of nonzero entries per row is five, and therefore independent of the total size of the matrix. For more complicated problems -- say, the Stokes problem of step-22 -- and in particular in 3d, the number of entries per row may be several hundred. But the important point is that this number is independent of the overall size of the problem: If you refine the mesh, the maximal number of unknowns per row remains the same. 

Sparsity is one of the distinguishing feature of the finite element method compared to, say, approximating the solution of a partial differential equation using a Taylor expansion and matching coefficients, or using a Fourier basis. 

In practical terms, it is the sparsity of matrices that enables us to solve problems with millions or billions of unknowns. To understand this, note that a matrix with  [2.x.776]  rows, each with a fixed upper bound for the number of nonzero entries, requires  [2.x.777]  memory locations for storage, and a matrix-vector multiplication also requires only  [2.x.778]  operations. Consequently, if we had a linear solver that requires only a fixed number of matrix-vector multiplications to come up with the solution of a linear system with this matrix, then we would have a solver that can find the values of all  [2.x.779]  unknowns with optimal complexity, i.e., with a total of  [2.x.780]  operations. It is clear that this wouldn't be possible if the matrix were not sparse (because then the number of entries in the matrix would have to be  [2.x.781]  with some  [2.x.782] , and doing a fixed number of matrix-vector products would take  [2.x.783]  operations), but it also requires very specialized solvers such as multigrid methods to satisfy the requirement that the solution requires only a fixed number of matrix-vector multiplications. We will frequently look at the question of what solver to use in the remaining programs of this tutorial. 

The sparsity is generated by the fact that finite element shape functions are defined [1.x.290] on individual cells, rather than globally, and that the local differential operators in the bilinear form only couple shape functions whose support overlaps. (The "support" of a function is the area where it is nonzero. For the finite element method, the support of a shape function is generally the cells adjacent to the vertex, edge, or face it is defined on.) In other words, degrees of freedom  [2.x.784]  and  [2.x.785]  that are not defined on the same cell do not overlap, and consequently the matrix entry  [2.x.786]  will be zero.  (In some cases such as the Discontinuous Galerkin method, shape functions may also connect to neighboring cells through face integrals. But finite element methods do not generally couple shape functions beyond the immediate neighbors of a cell on which the function is defined.) 




[1.x.291] 

By default, the DoFHandler class enumerates degrees of freedom on a mesh in a rather random way; consequently, the sparsity pattern is also not optimized for any particular purpose. To show this, the code below will demonstrate a simple way to output the "sparsity pattern" that corresponds to a DoFHandler, i.e., an object that represents all of the potentially nonzero elements of a matrix one may build when discretizing a partial differential equation on a mesh and its DoFHandler. This lack of structure in the sparsity pattern will be apparent from the pictures we show below. 

For most applications and algorithms, the exact way in which degrees of freedom are numbered does not matter. For example, the Conjugate Gradient method we use to solve linear systems does not care. On the other hand, some algorithms do care: in particular, some preconditioners such as SSOR will work better if they can walk through degrees of freedom in a particular order, and it would be nice if we could just sort them in such a way that SSOR can iterate through them from zero to  [2.x.787]  in this order. Other examples include computing incomplete LU or Cholesky factorizations, or if we care about the block structure of matrices (see step-20 for an example). deal.II therefore has algorithms that can re-enumerate degrees of freedom in particular ways in namespace DoFRenumbering. Renumbering can be thought of as choosing a different, permuted basis of the finite element space. The sparsity pattern and matrices that result from this renumbering are therefore also simply a permutation of rows and columns compared to the ones we would get without explicit renumbering. 

In the program below, we will use the algorithm of Cuthill and McKee to do so. We will show the sparsity pattern for both the original enumeration of degrees of freedom and of the renumbered version below, in the [1.x.292]. 


examples/step-2/doc/results.dox 



[1.x.293] 

The program has, after having been run, produced two sparsity patterns. We can visualize them by opening the  [2.x.788]  files in a web browser. 

The results then look like this (every point denotes an entry which might be nonzero; of course the fact whether the entry actually is zero or not depends on the equation under consideration, but the indicated positions in the matrix tell us which shape functions can and which can't couple when discretizing a local, i.e. differential, equation):  [2.x.789]  

The different regions in the left picture, indicated by kinks in the lines and single dots on the left and top, represent the degrees of freedom on the different refinement levels of the triangulation.  As can be seen in the right picture, the sparsity pattern is much better clustered around the main diagonal of the matrix after renumbering. Although this might not be apparent, the number of nonzero entries is the same in both pictures, of course. 




[1.x.294] 

Just as with step-1, you may want to play with the program a bit to familiarize yourself with deal.II. For example, in the  [2.x.790]  function, we use linear finite elements (that's what the argument "1" to the FE_Q object is). Explore how the sparsity pattern changes if you use higher order elements, for example cubic or quintic ones (by using 3 and 5 as the respective arguments). 

You could also explore how the sparsity pattern changes by refining the mesh. You will see that not only the size of the matrix changes, but also its bandwidth (the distance from the diagonal of those nonzero elements of the matrix that are farthest away from the diagonal), though the ratio of bandwidth to size typically shrinks, i.e. the matrix clusters more around the diagonal. 

Another idea of experiments would be to try other renumbering strategies than Cuthill-McKee from the DoFRenumbering namespace and see how they affect the sparsity pattern. 

You can also visualize the output using [1.x.295] (one of the simpler visualization programs; maybe not the easiest to use since it is command line driven, but also universally available on all Linux and other Unix-like systems) by changing from  [2.x.791] : 

[1.x.296] 



Another practice based on [1.x.297] is trying to print out the mesh with locations and numbering of the support points. For that, you need to include header files for GridOut and MappingQ1. The code for this is: 

[1.x.298] 

After we run the code, we get a file called gnuplot.gpl. To view this file, we can run the following code in the command line: 

[1.x.299]. With that, you will get a picture similar to  [2.x.792]  depending on the mesh you are looking at. For more information, see  [2.x.793]  


examples/step-20/doc/intro.dox 

[1.x.300] 

[1.x.301] 

 [2.x.794]  

This program is devoted to two aspects: the use of mixed finite elements -- in particular Raviart-Thomas elements -- and using block matrices to define solvers, preconditioners, and nested versions of those that use the substructure of the system matrix. The equation we are going to solve is again the Poisson equation, though with a matrix-valued coefficient: 

[1.x.302] 

 [2.x.795]  is assumed to be uniformly positive definite, i.e., there is  [2.x.796]  such that the eigenvalues  [2.x.797]  of  [2.x.798]  satisfy  [2.x.799] . The use of the symbol  [2.x.800]  instead of the usual  [2.x.801]  for the solution variable will become clear in the next section. 

After discussing the equation and the formulation we are going to use to solve it, this introduction will cover the use of block matrices and vectors, the definition of solvers and preconditioners, and finally the actual test case we are going to solve. 

We are going to extend this tutorial program in step-21 to solve not only the mixed Laplace equation, but add another equation that describes the transport of a mixture of two fluids. 

The equations covered here fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.802]  module. 




[1.x.303] 

In the form above, the Poisson equation (i.e., the Laplace equation with a nonzero right hand side) is generally considered a good model equation for fluid flow in porous media. Of course, one typically models fluid flow through the [1.x.304] or, if fluid velocities are slow or the viscosity is large, the [1.x.305] (which we cover in step-22). In the first of these two models, the forces that act are inertia and viscous friction, whereas in the second it is only viscous friction -- i.e., forces that one fluid particle exerts on a nearby one. This is appropriate if you have free flow in a large domain, say a pipe, a river, or in the air. On the other hand, if the fluid is confined in pores, then friction forces exerted by the pore walls on the fluid become more and more important and internal viscous friction becomes less and less important. Modeling this then first leads to the [1.x.306] if both effects are important, and in the limit of very small pores to the [1.x.307]. The latter is just a different name for the Poisson or Laplace equation, connotating it with the area to which one wants to apply it: slow flow in a porous medium. In essence it says that the velocity is proportional to the negative pressure gradient that drives the fluid through the porous medium. 

The Darcy equation models this pressure that drives the flow. (Because the solution variable is a pressure, we here use the name  [2.x.803]  instead of the name  [2.x.804]  more commonly used for the solution of partial differential equations.) Typical applications of this view of the Laplace equation are then modeling groundwater flow, or the flow of hydrocarbons in oil reservoirs. In these applications,  [2.x.805]  is the permeability tensor, i.e., a measure for how much resistance the soil or rock matrix asserts on the fluid flow. 

In the applications named above, a desirable feature for a numerical scheme is that it should be locally conservative, i.e., that whatever flows into a cell also flows out of it (or the difference is equal to the integral over the source terms over each cell, if the sources are nonzero). However, as it turns out, the usual discretizations of the Laplace equation (such as those used in step-3, step-4, or step-6) do not satisfy this property. But, one can achieve this by choosing a different formulation of the problem and a particular combination of finite element spaces. 




[1.x.308] 

To this end, one first introduces a second variable, called the velocity,  [2.x.806] . By its definition, the velocity is a vector in the negative direction of the pressure gradient, multiplied by the permeability tensor. If the permeability tensor is proportional to the unit matrix, this equation is easy to understand and intuitive: the higher the permeability, the higher the velocity; and the velocity is proportional to the gradient of the pressure, going from areas of high pressure to areas of low pressure (thus the negative sign). 

With this second variable, one then finds an alternative version of the Laplace equation, called the [1.x.309]: 

[1.x.310] 

Here, we have multiplied the equation defining the velocity  [2.x.807]  by  [2.x.808]  because this makes the set of equations symmetric: one of the equations has the gradient, the second the negative divergence, and these two are of course adjoints of each other, resulting in a symmetric bilinear form and a consequently symmetric system matrix under the common assumption that  [2.x.809]  is a symmetric tensor. 

The weak formulation of this problem is found by multiplying the two equations with test functions and integrating some terms by parts: 

[1.x.311] 

where 

[1.x.312] 

Here,  [2.x.810]  is the outward normal vector at the boundary. Note how in this formulation, Dirichlet boundary values of the original problem are incorporated in the weak form. 

To be well-posed, we have to look for solutions and test functions in the space  [2.x.811]  for  [2.x.812] , [2.x.813] , and  [2.x.814]  for  [2.x.815] . It is a well-known fact stated in almost every book on finite element theory that if one chooses discrete finite element spaces for the approximation of  [2.x.816]  inappropriately, then the resulting discrete problem is instable and the discrete solution will not converge to the exact solution. (Some details on the problem considered here -- which falls in the class of "saddle-point problems" 

-- can be found on the Wikipedia page on the [1.x.313].) 

To overcome this, a number of different finite element pairs for  [2.x.817]  have been developed that lead to a stable discrete problem. One such pair is to use the Raviart-Thomas spaces  [2.x.818]  for the velocity  [2.x.819]  and discontinuous elements of class  [2.x.820]  for the pressure  [2.x.821] . For details about these spaces, we refer in particular to the book on mixed finite element methods by Brezzi and Fortin, but many other books on the theory of finite elements, for example the classic book by Brenner and Scott, also state the relevant results. In any case, with appropriate choices of function spaces, the discrete formulation reads as follows: Find  [2.x.822]  so that 

[1.x.314] 




Before continuing, let us briefly pause and show that the choice of function spaces above provides us with the desired local conservation property. In particular, because the pressure space consists of discontinuous piecewise polynomials, we can choose the test function  [2.x.823]  as the function that is equal to one on any given cell  [2.x.824]  and zero everywhere else. If we also choose  [2.x.825]  everywhere (remember that the weak form above has to hold for [1.x.315] discrete test functions  [2.x.826] ), then putting these choices of test functions into the weak formulation above implies in particular that 

[1.x.316] 

which we can of course write in more explicit form as 

[1.x.317] 

Applying the divergence theorem results in the fact that  [2.x.827]  has to satisfy, for every choice of cell  [2.x.828] , the relationship 

[1.x.318] 

If you now recall that  [2.x.829]  was the velocity, then the integral on the left is exactly the (discrete) flux across the boundary of the cell  [2.x.830] . The statement is then that the flux must be equal to the integral over the sources within  [2.x.831] . In particular, if there are no sources (i.e.,  [2.x.832]  in  [2.x.833] ), then the statement is that [1.x.319] flux is zero, i.e., whatever flows into a cell must flow out of it through some other part of the cell boundary. This is what we call [1.x.320] because it holds for every cell. 

On the other hand, the usual continuous  [2.x.834]  elements would not result in this kind of property when used for the pressure (as, for example, we do in step-43) because one can not choose a discrete test function  [2.x.835]  that is one on a cell  [2.x.836]  and zero everywhere else: It would be discontinuous and consequently not in the finite element space. (Strictly speaking, all we can say is that the proof above would not work for continuous elements. Whether these elements might still result in local conservation is a different question as one could think that a different kind of proof might still work; in reality, however, the property really does not hold.) 




[1.x.321] 

The deal.II library (of course) implements Raviart-Thomas elements  [2.x.837]  of arbitrary order  [2.x.838] , as well as discontinuous elements  [2.x.839] . If we forget about their particular properties for a second, we then have to solve a discrete problem 

[1.x.322] 

with the bilinear form and right hand side as stated above, and  [2.x.840] ,  [2.x.841] . Both  [2.x.842]  and  [2.x.843]  are from the space  [2.x.844] , where  [2.x.845]  is itself a space of  [2.x.846] -dimensional functions to accommodate for the fact that the flow velocity is vector-valued. The necessary question then is: how do we do this in a program? 

Vector-valued elements have already been discussed in previous tutorial programs, the first time and in detail in step-8. The main difference there was that the vector-valued space  [2.x.847]  is uniform in all its components: the  [2.x.848]  components of the displacement vector are all equal and from the same function space. What we could therefore do was to build  [2.x.849]  as the outer product of the  [2.x.850]  times the usual  [2.x.851]  finite element space, and by this make sure that all our shape functions have only a single non-zero vector component. Instead of dealing with vector-valued shape functions, all we did in step-8 was therefore to look at the (scalar) only non-zero component and use the  [2.x.852]  call to figure out which component this actually is. 

This doesn't work with Raviart-Thomas elements: following from their construction to satisfy certain regularity properties of the space  [2.x.853] , the shape functions of  [2.x.854]  are usually nonzero in all their vector components at once. For this reason, were  [2.x.855]  applied to determine the only nonzero component of shape function  [2.x.856] , an exception would be generated. What we really need to do is to get at  [2.x.857] all [2.x.858]  vector components of a shape function. In deal.II diction, we call such finite elements  [2.x.859] non-primitive [2.x.860] , whereas finite elements that are either scalar or for which every vector-valued shape function is nonzero only in a single vector component are called  [2.x.861] primitive [2.x.862] . 

So what do we have to do for non-primitive elements? To figure this out, let us go back in the tutorial programs, almost to the very beginnings. There, we learned that we use the  [2.x.863]  class to determine the values and gradients of shape functions at quadrature points. For example, we would call  [2.x.864]  to obtain the value of the  [2.x.865] th shape function on the quadrature point with number  [2.x.866] . Later, in step-8 and other tutorial programs, we learned that this function call also works for vector-valued shape functions (of primitive finite elements), and that it returned the value of the only non-zero component of shape function  [2.x.867]  at quadrature point  [2.x.868] . 

For non-primitive shape functions, this is clearly not going to work: there is no single non-zero vector component of shape function  [2.x.869] , and the call to  [2.x.870]  would consequently not make much sense. However, deal.II offers a second function call,  [2.x.871]  that returns the value of the  [2.x.872]  at quadrature point  [2.x.873]  is an index between zero and the number of vector components of the present finite element; for example, the element we will use to describe velocities and pressures is going to have  [2.x.874]  components. It is worth noting that this function call can also be used for primitive shape functions: it will simply return zero for all components except one; for non-primitive shape functions, it will in general return a non-zero value for more than just one component. 

We could now attempt to rewrite the bilinear form above in terms of vector components. For example, in 2d, the first term could be rewritten like this (note that  [2.x.875] ): 

[1.x.323] 

If we implemented this, we would get code like this: 

[1.x.324] 



This is, at best, tedious, error prone, and not dimension independent. There are obvious ways to make things dimension independent, but in the end, the code is simply not pretty. What would be much nicer is if we could simply extract the  [2.x.876]  and  [2.x.877]  components of a shape function  [2.x.878] . In the program we do that in the following way: 

[1.x.325] 



This is, in fact, not only the first term of the bilinear form, but the whole thing (sans boundary contributions). 

What this piece of code does is, given an  [2.x.879]  object, to extract the values of the first  [2.x.880]  components of shape function  [2.x.881]  at quadrature points  [2.x.882] , that is the velocity components of that shape function. Put differently, if we write shape functions  [2.x.883]  as the tuple  [2.x.884] , then the function returns the velocity part of this tuple. Note that the velocity is of course a  [2.x.885] -dimensional tensor, and that the function returns a corresponding object. Similarly, where we subscript with the pressure extractor, we extract the scalar pressure component. The whole mechanism is described in more detail in the  [2.x.886]  module. 

In practice, it turns out that we can do a bit better if we evaluate the shape functions, their gradients and divergences only once per outermost loop, and store the result, as this saves us a few otherwise repeated computations (it is possible to save even more repeated operations by calculating all relevant quantities in advance and then only inserting the results in the actual loop, see step-22 for a realization of that approach). The final result then looks like this, working in every space dimension: 

[1.x.326] 



This very closely resembles the form in which we have originally written down the bilinear form and right hand side. 

There is one final term that we have to take care of: the right hand side contained the term  [2.x.887] , constituting the weak enforcement of pressure boundary conditions. We have already seen in step-7 how to deal with face integrals: essentially exactly the same as with domain integrals, except that we have to use the FEFaceValues class instead of  [2.x.888] . To compute the boundary term we then simply have to loop over all boundary faces and integrate there. The mechanism works in the same way as above, i.e. the extractor classes also work on FEFaceValues objects: 

[1.x.327] 



You will find the exact same code as above in the sources for the present program. We will therefore not comment much on it below. 




[1.x.328] 

After assembling the linear system we are faced with the task of solving it. The problem here is that the matrix possesses two undesirable properties: 

- It is [1.x.329],   i.e., it has both positive and negative eigenvalues.   We don't want to prove this property here, but note that this is true   for all matrices of the form    [2.x.889]    such as the one here where  [2.x.890]  is positive definite. 

- The matrix has a zero block at the bottom right (there is no term in   the bilinear form that couples the pressure  [2.x.891]  with the   pressure test function  [2.x.892] ). 

At least it is symmetric, but the first issue above still means that the Conjugate Gradient method is not going to work since it is only applicable to problems in which the matrix is symmetric and positive definite. We would have to resort to other iterative solvers instead, such as MinRes, SymmLQ, or GMRES, that can deal with indefinite systems. However, then the next problem immediately surfaces: Due to the zero block, there are zeros on the diagonal and none of the usual, "simple" preconditioners (Jacobi, SSOR) will work as they require division by diagonal elements. 

For the matrix sizes we expect to run with this program, the by far simplest approach would be to just use a direct solver (in particular, the SparseDirectUMFPACK class that is bundled with deal.II). step-29 goes this route and shows that solving [1.x.330] linear system can be done in just 3 or 4 lines of code. 

But then, this is a tutorial: We teach how to do things. Consequently, in the following, we will introduce some techniques that can be used in cases like these. Namely, we will consider the linear system as not consisting of one large matrix and vectors, but we will want to decompose matrices into [1.x.331] that correspond to the individual operators that appear in the system. We note that the resulting solver is not optimal -- there are much better ways to efficiently compute the system, for example those explained in the results section of step-22 or the one we use in step-43 for a problem similar to the current one. Here, our goal is simply to introduce new solution techniques and how they can be implemented in deal.II. 




[1.x.332] 

In view of the difficulties using standard solvers and preconditioners mentioned above, let us take another look at the matrix. If we sort our degrees of freedom so that all velocity come before all pressure variables, then we can subdivide the linear system  [2.x.893]  into the following blocks: 

[1.x.333] 

where  [2.x.894]  are the values of velocity and pressure degrees of freedom, respectively,  [2.x.895]  is the mass matrix on the velocity space,  [2.x.896]  corresponds to the negative divergence operator, and  [2.x.897]  is its transpose and corresponds to the gradient. 

By block elimination, we can then re-order this system in the following way (multiply the first row of the system by  [2.x.898]  and then subtract the second row from it): 

[1.x.334] 

Here, the matrix  [2.x.899]  (called the [1.x.335] of  [2.x.900] ) is obviously symmetric and, owing to the positive definiteness of  [2.x.901]  and the fact that  [2.x.902]  has full column rank,  [2.x.903]  is also positive definite. 

Consequently, if we could compute  [2.x.904] , we could apply the Conjugate Gradient method to it. However, computing  [2.x.905]  is expensive because it requires us to compute the inverse of the (possibly large) matrix  [2.x.906] ; and  [2.x.907]  is in fact also a full matrix because even though  [2.x.908]  is sparse, its inverse  [2.x.909]  will generally be a dense matrix. On the other hand, the CG algorithm doesn't require us to actually have a representation of  [2.x.910] : It is sufficient to form matrix-vector products with it. We can do so in steps, using the fact that matrix products are associative (i.e., we can set parentheses in such a way that the product is more convenient to compute): To compute  [2.x.911] , we <ol>   [2.x.912]  compute  [2.x.913] ;   [2.x.914]  solve  [2.x.915]  for  [2.x.916] , using the CG method applied to the   positive definite and symmetric mass matrix  [2.x.917] ;   [2.x.918]  compute  [2.x.919]  to obtain  [2.x.920] .  [2.x.921]  Note how we evaluate the expression  [2.x.922]  right to left to avoid matrix-matrix products; this way, all we have to do is evaluate matrix-vector products. 

In the following, we will then have to come up with ways to represent the matrix  [2.x.923]  so that it can be used in a Conjugate Gradient solver, as well as to define ways in which we can precondition the solution of the linear system involving  [2.x.924] , and deal with solving linear systems with the matrix  [2.x.925]  (the second step above). 

 [2.x.926]  The key point in this consideration is to recognize that to implement an iterative solver such as CG or GMRES, we never actually need the actual [1.x.336] of a matrix! All that is required is that we can form matrix-vector products. The same is true for preconditioners. In deal.II we encode this requirement by only requiring that matrices and preconditioners given to solver classes have a  [2.x.927]  member function that does the matrix-vector product. How a class chooses to implement this function is not important to the solver. Consequently, classes can implement it by, for example, doing a sequence of products and linear solves as discussed above. 




[1.x.337] 

deal.II includes support for describing such linear operations in a very general way. This is done with the LinearOperator class that, like  [2.x.928]  "the MatrixType concept", defines a minimal interface for [1.x.338] a linear operation to a vector: 

[1.x.339] 

The key difference between a LinearOperator and an ordinary matrix is however that a LinearOperator does not allow any further access to the underlying object. All you can do with a LinearOperator is to apply its "action" to a vector! We take the opportunity to introduce the LinearOperator concept at this point because it is a very useful tool that allows you to construct complex solvers and preconditioners in a very intuitive manner. 

As a first example let us construct a LinearOperator object that represents  [2.x.929] . This means that whenever the  [2.x.930]  function of this operator is called it has to solve a linear system. This requires us to specify a solver (and corresponding) preconditioner. Assuming that  [2.x.931]  is a reference to the upper left block of the system matrix we can write: 

[1.x.340] 

Rather than using a SolverControl we use the ReductionControl class here that stops iterations when either an absolute tolerance is reached (for which we choose  [2.x.932] ) or when the residual is reduced by a certain factor (here,  [2.x.933] ). In contrast the SolverControl class only checks for absolute tolerances. We have to use ReductionControl in our case to work around a minor issue: The right hand sides that we  will feed to  [2.x.934]  are essentially formed by residuals that naturally decrease vastly in norm as the outer iterations progress. This makes control by an absolute tolerance very error prone. 

We now have a LinearOperator  [2.x.935]  that we can use to construct more complicated operators such as the Schur complement  [2.x.936] . Assuming that  [2.x.937]  is a reference to the upper right block constructing a LinearOperator  [2.x.938]  is a matter of two lines: 

[1.x.341] 

Here, the multiplication of three LinearOperator objects yields a composite object  [2.x.939]  function first applies  [2.x.940] , then  [2.x.941]  (i.e. solving an equation with  [2.x.942] ), and finally  [2.x.943]  to any given input vector. In that sense  [2.x.944]  is similar to the following code: 

[1.x.342] 

( [2.x.945]  are two temporary vectors). The key point behind this approach is the fact that we never actually create an inner product of matrices. Instead, whenever we have to perform a matrix vector multiplication with  [2.x.946]  we simply run all individual  [2.x.947]  operations in above sequence. 

 [2.x.948]  We could have achieved the same goal of creating a "matrix like" object by implementing a specialized class  [2.x.949]  that provides a suitable  [2.x.950]  function. Skipping over some details this might have looked like the following: 

[1.x.343] 

Even though both approaches are exactly equivalent, the LinearOperator class has a big advantage over this manual approach. It provides so-called [1.x.344][1.x.345]: Mathematically, we think about  [2.x.951]  as being the composite matrix  [2.x.952]  and the LinearOperator class allows you to write this out more or less verbatim, 

[1.x.346] 

The manual approach on the other hand obscures this fact. 

All that is left for us to do now is to form the right hand sides of the two equations defining  [2.x.953]  and  [2.x.954] , and then solve them with the Schur complement matrix and the mass matrix, respectively. For example the right hand side of the first equation reads  [2.x.955] . This could be implemented as follows: 

[1.x.347] 

Again, this is a perfectly valid approach, but the fact that deal.II requires us to manually resize the final and temporary vector, and that every operation takes up a new line makes this hard to read. This is the point where a second class in the linear operator framework can will help us. Similarly in spirit to LinearOperator, a PackagedOperation stores a "computation": 

[1.x.348] 

The class allows [1.x.349] of expressions involving vectors and linear operators. This is done by storing the computational expression and only performing the computation when either the object is converted to a vector object, or  [2.x.956]  (or  [2.x.957]  is invoked by hand. Assuming that  [2.x.958]  are the two vectors of the right hand side we can simply write: 

[1.x.350] 

Here,  [2.x.959]  is a PackagedOperation that [1.x.351] the computation we specified. It does not create a vector with the actual result immediately. 

With these prerequisites at hand, solving for  [2.x.960]  and  [2.x.961]  is a matter of creating another solver and inverse: 

[1.x.352] 



 [2.x.962]  The functionality that we developed in this example step by hand is already readily available in the library. Have a look at schur_complement(), condense_schur_rhs(), and postprocess_schur_solution(). 




[1.x.353] 

One may ask whether it would help if we had a preconditioner for the Schur complement  [2.x.963] . The general answer, as usual, is: of course. The problem is only, we don't know anything about this Schur complement matrix. We do not know its entries, all we know is its action. On the other hand, we have to realize that our solver is expensive since in each iteration we have to do one matrix-vector product with the Schur complement, which means that we have to do invert the mass matrix once in each iteration. 

There are different approaches to preconditioning such a matrix. On the one extreme is to use something that is cheap to apply and therefore has no real impact on the work done in each iteration. The other extreme is a preconditioner that is itself very expensive, but in return really brings down the number of iterations required to solve with  [2.x.964] . 

We will try something along the second approach, as much to improve the performance of the program as to demonstrate some techniques. To this end, let us recall that the ideal preconditioner is, of course,  [2.x.965] , but that is unattainable. However, how about 

[1.x.354] 

as a preconditioner? That would mean that every time we have to do one preconditioning step, we actually have to solve with  [2.x.966] . At first, this looks almost as expensive as solving with  [2.x.967]  right away. However, note that in the inner iteration, we do not have to calculate  [2.x.968] , but only the inverse of its diagonal, which is cheap. 

Thankfully, the LinearOperator framework makes this very easy to write out. We already used a Jacobi preconditioner ( [2.x.969] ) for the  [2.x.970]  matrix earlier. So all that is left to do is to write out how the approximate Schur complement should look like: 

[1.x.355] 

Note how this operator differs in simply doing one Jacobi sweep (i.e. multiplying with the inverses of the diagonal) instead of multiplying with the full  [2.x.971] . (This is how a single Jacobi preconditioner step with  [2.x.972]  is defined: it is the multiplication with the inverse of the diagonal of  [2.x.973] ; in other words, the operation  [2.x.974]  on a vector  [2.x.975]  is exactly what PreconditionJacobi does.) 

With all this we almost have the preconditioner completed: it should be the inverse of the approximate Schur complement. We implement this again by creating a linear operator with inverse_operator() function. This time however we would like to choose a relatively modest tolerance for the CG solver (that inverts  [2.x.976] ). The reasoning is that  [2.x.977] , so we actually do not need to invert it exactly. This, however creates a subtle problem:  [2.x.978]  will be used in the final outer CG iteration to create an orthogonal basis. But for this to work, it must be precisely the same linear operation for every invocation. We ensure this by using an IterationNumberControl that allows us to fix the number of CG iterations that are performed to a fixed small number (in our case 30): 

[1.x.356] 



That's all! 

Obviously, applying this inverse of the approximate Schur complement is a very expensive preconditioner, almost as expensive as inverting the Schur complement itself. We can expect it to significantly reduce the number of outer iterations required for the Schur complement. In fact it does: in a typical run on 7 times refined meshes using elements of order 0, the number of outer iterations drops from 592 to 39. On the other hand, we now have to apply a very expensive preconditioner 25 times. A better measure is therefore simply the run-time of the program: on a current laptop (as of January 2019), it drops from 3.57 to 2.05 seconds for this test case. That doesn't seem too impressive, but the savings become more pronounced on finer meshes and with elements of higher order. For example, an seven times refined mesh and using elements of order 2 (which amounts to about 0.4 million degrees of freedom) yields an improvement of 1134 to 83 outer iterations, at a runtime of 168 seconds to 40 seconds. Not earth shattering, but significant. 




[1.x.357] 

In this tutorial program, we will solve the Laplace equation in mixed formulation as stated above. Since we want to monitor convergence of the solution inside the program, we choose right hand side, boundary conditions, and the coefficient so that we recover a solution function known to us. In particular, we choose the pressure solution 

[1.x.358] 

and for the coefficient we choose the unit matrix  [2.x.979]  for simplicity. Consequently, the exact velocity satisfies 

[1.x.359] 

This solution was chosen since it is exactly divergence free, making it a realistic test case for incompressible fluid flow. By consequence, the right hand side equals  [2.x.980] , and as boundary values we have to choose  [2.x.981] . 

For the computations in this program, we choose  [2.x.982] . You can find the resulting solution in the [1.x.360], after the commented program. 


examples/step-20/doc/results.dox 



[1.x.361] 

[1.x.362] 


If we run the program as is, we get this output for the  [2.x.983]  mesh we use (for a total of 1024 cells with 1024 pressure degrees of freedom since we use piecewise constants, and 2112 velocities because the Raviart-Thomas element defines one degree per freedom per face and there are  [2.x.984]  faces parallel to the  [2.x.985] -axis and the same number parallel to the  [2.x.986] -axis): 

[1.x.363] 



The fact that the number of iterations is so small, of course, is due to the good (but expensive!) preconditioner we have developed. To get confidence in the solution, let us take a look at it. The following three images show (from left to right) the x-velocity, the y-velocity, and the pressure: 

 [2.x.987]  




Let us start with the pressure: it is highest at the left and lowest at the right, so flow will be from left to right. In addition, though hardly visible in the graph, we have chosen the pressure field such that the flow left-right flow first channels towards the center and then outward again. Consequently, the x-velocity has to increase to get the flow through the narrow part, something that can easily be seen in the left image. The middle image represents inward flow in y-direction at the left end of the domain, and outward flow in y-direction at the right end of the domain. 




As an additional remark, note how the x-velocity in the left image is only continuous in x-direction, whereas the y-velocity is continuous in y-direction. The flow fields are discontinuous in the other directions. This very obviously reflects the continuity properties of the Raviart-Thomas elements, which are, in fact, only in the space H(div) and not in the space  [2.x.988] . Finally, the pressure field is completely discontinuous, but that should not surprise given that we have chosen  [2.x.989]  as the finite element for that solution component. 




[1.x.364] 


The program offers two obvious places where playing and observing convergence is in order: the degree of the finite elements used (passed to the constructor of the  [2.x.990] ), and the refinement level (determined in  [2.x.991] ). What one can do is to change these values and observe the errors computed later on in the course of the program run. 




If one does this, one finds the following pattern for the  [2.x.992]  error in the pressure variable:  [2.x.993]  

The theoretically expected convergence orders are very nicely reflected by the experimentally observed ones indicated in the last row of the table. 




One can make the same experiment with the  [2.x.994]  error in the velocity variables:  [2.x.995]  The result concerning the convergence order is the same here. 




[1.x.365] 

[1.x.366] 

[1.x.367] 

Realistic flow computations for ground water or oil reservoir simulations will not use a constant permeability. Here's a first, rather simple way to change this situation: we use a permeability that decays very rapidly away from a central flowline until it hits a background value of 0.001. This is to mimic the behavior of fluids in sandstone: in most of the domain, the sandstone is homogeneous and, while permeable to fluids, not overly so; on the other stone, the stone has cracked, or faulted, along one line, and the fluids flow much easier along this large crack. Here is how we could implement something like this: 

[1.x.368] 

Remember that the function returns the inverse of the permeability tensor. 




With a significantly higher mesh resolution, we can visualize this, here with x- and y-velocity: 

 [2.x.996]  

It is obvious how fluids flow essentially only along the middle line, and not anywhere else. 




Another possibility would be to use a random permeability field. A simple way to achieve this would be to scatter a number of centers around the domain and then use a permeability field that is the sum of (negative) exponentials for each of these centers. Flow would then try to hop from one center of high permeability to the next one. This is an entirely unscientific attempt at describing a random medium, but one possibility to implement this behavior would look like this: 

[1.x.369] 



A piecewise constant interpolation of the diagonal elements of the inverse of this tensor (i.e., of  [2.x.997] ) looks as follows: 

 [2.x.998]  


With a permeability field like this, we would get x-velocities and pressures as follows: 

 [2.x.999]  

We will use these permeability fields again in step-21 and step-43. 




[1.x.370] 

As mentioned in the introduction, the Schur complement solver used here is not the best one conceivable (nor is it intended to be a particularly good one). Better ones can be found in the literature and can be built using the same block matrix techniques that were introduced here. We pick up on this theme again in step-22, where we first build a Schur complement solver for the Stokes equation as we did here, and then in the [1.x.371] section discuss better ways based on solving the system as a whole but preconditioning based on individual blocks. We will also come back to this in step-43. 


examples/step-21/doc/intro.dox 

[1.x.372] [1.x.373] 

This program grew out of a student project by Yan Li at Texas A&amp;M University. Most of the work for this program is by her. 

In this project, we propose a numerical simulation for two phase flow problems in porous media. This problem includes one elliptic equation and one nonlinear, time dependent transport equation. This is therefore also the first time-dependent tutorial program (besides the somewhat strange time-dependence of  [2.x.1000]  "step-18"). 

The equations covered here are an extension of the material already covered in step-20. In particular, they fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.1001]  module. 




[1.x.374] 

Modeling of two phase flow in porous media is important for both environmental remediation and the management of petroleum and groundwater reservoirs. Practical situations involving two phase flow include the dispersal of a nonaqueous phase liquid in an aquifer, or the joint movement of a mixture of fluids such as oil and water in a reservoir. Simulation models, if they are to provide realistic predictions, must accurately account for these effects. 

To derive the governing equations, consider two phase flow in a reservoir  [2.x.1002]  under the assumption that the movement of fluids is dominated by viscous effects; i.e. we neglect the effects of gravity, compressibility, and capillary pressure. Porosity will be considered to be constant. We will denote variables referring to either of the two phases using subscripts  [2.x.1003]  and  [2.x.1004] , short for water and oil. The derivation of the equations holds for other pairs of fluids as well, however. 

The velocity with which molecules of each of the two phases move is determined by Darcy's law that states that the velocity is proportional to the pressure gradient: 

[1.x.375] 

where  [2.x.1005]  is the velocity of phase  [2.x.1006] ,  [2.x.1007]  is the permeability tensor,  [2.x.1008]  is the relative permeability of phase  [2.x.1009] ,  [2.x.1010]  is the pressure and  [2.x.1011]  is the viscosity of phase  [2.x.1012] . Finally,  [2.x.1013]  is the saturation (volume fraction), i.e. a function with values between 0 and 1 indicating the composition of the mixture of fluids. In general, the coefficients  [2.x.1014]  may be spatially dependent variables, and we will always treat them as non-constant functions in the following. 

We combine Darcy's law with the statement of conservation of mass for each phase, 

[1.x.376] 

with a source term for each phase. By summing over the two phases, we can express the governing equations in terms of the so-called pressure equation: 

[1.x.377] 

Here,  [2.x.1015]  is the sum source term, and 

[1.x.378] 

is the total mobility. 

So far, this looks like an ordinary stationary, Poisson-like equation that we can solve right away with the techniques of the first few tutorial programs (take a look at step-6, for example, for something very similar). However, we have not said anything yet about the saturation, which of course is going to change as the fluids move around. 

The second part of the equations is the description of the dynamics of the saturation, i.e., how the relative concentration of the two fluids changes with time. The saturation equation for the displacing fluid (water) is given by the following conservation law: 

[1.x.379] 

which can be rewritten by using the product rule of the divergence operator in the previous equation: 

[1.x.380] 

Here,  [2.x.1016]  is the total influx introduced above, and  [2.x.1017]  is the flow rate of the displacing fluid (water). These two are related to the fractional flow  [2.x.1018]  in the following way: 

[1.x.381] 

where the fractional flow is often parameterized via the (heuristic) expression 

[1.x.382] 

Putting it all together yields the saturation equation in the following, advected form: 

[1.x.383] 

where  [2.x.1019]  is the total velocity 

[1.x.384] 

Note that the advection equation contains the term  [2.x.1020]  rather than  [2.x.1021]  to indicate that the saturation is not simply transported along; rather, since the two phases move with different velocities, the saturation can actually change even in the advected coordinate system. To see this, rewrite  [2.x.1022]  to observe that the [1.x.385] velocity with which the phase with saturation  [2.x.1023]  is transported is  [2.x.1024]  whereas the other phase is transported at velocity  [2.x.1025] .  [2.x.1026]  is consequently often referred to as the [1.x.386]. 

In summary, what we get are the following two equations: 

[1.x.387] 

Here,  [2.x.1027]  are now time dependent functions: while at every time instant the flow field is in equilibrium with the pressure (i.e. we neglect dynamic accelerations), the saturation is transported along with the flow and therefore changes over time, in turn affected the flow field again through the dependence of the first equation on  [2.x.1028] . 

This set of equations has a peculiar character: one of the two equations has a time derivative, the other one doesn't. This corresponds to the character that the pressure and velocities are coupled through an instantaneous constraint, whereas the saturation evolves over finite time scales. 

Such systems of equations are called Differential Algebraic Equations (DAEs), since one of the equations is a differential equation, the other is not (at least not with respect to the time variable) and is therefore an "algebraic" equation. (The notation comes from the field of ordinary differential equations, where everything that does not have derivatives with respect to the time variable is necessarily an algebraic equation.) This class of equations contains pretty well-known cases: for example, the time dependent Stokes and Navier-Stokes equations (where the algebraic constraint is that the divergence of the flow field,  [2.x.1029] , must be zero) as well as the time dependent Maxwell equations (here, the algebraic constraint is that the divergence of the electric displacement field equals the charge density,  [2.x.1030]  and that the divergence of the magnetic flux density is zero:  [2.x.1031] ); even the quasistatic model of step-18 falls into this category. We will see that the different character of the two equations will inform our discretization strategy for the two equations. 




[1.x.388] 

In the reservoir simulation community, it is common to solve the equations derived above by going back to the first order, mixed formulation. To this end, we re-introduce the total velocity  [2.x.1032]  and write the equations in the following form: 

[1.x.389] 

This formulation has the additional benefit that we do not have to express the total velocity  [2.x.1033]  appearing in the transport equation as a function of the pressure, but can rather take the primary variable for it. Given the saddle point structure of the first two equations and their similarity to the mixed Laplace formulation we have introduced in step-20, it will come as no surprise that we will use a mixed discretization again. 

But let's postpone this for a moment. The first business we have with these equations is to think about the time discretization. In reservoir simulation, there is a rather standard algorithm that we will use here. It first solves the pressure using an implicit equation, then the saturation using an explicit time stepping scheme. The algorithm is called IMPES for IMplicit Pressure Explicit Saturation and was first proposed a long time ago: by Sheldon et al. in 1959 and Stone and Gardner in 1961 (J. W. Sheldon, B. Zondek and W. T. Cardwell: [1.x.390], Trans. SPE AIME, 216 (1959), pp. 290-296; H. L. Stone and A. O. Gardner Jr: [1.x.391], Trans. SPE AIME, 222 (1961), pp. 92-104). In a slightly modified form, this algorithm can be written as follows: for each time step, solve 

[1.x.392] 

where  [2.x.1034]  is the length of a time step. Note how we solve the implicit pressure-velocity system that only depends on the previously computed saturation  [2.x.1035] , and then do an explicit time step for  [2.x.1036]  that only depends on the previously known  [2.x.1037]  and the just computed  [2.x.1038] . This way, we never have to iterate for the nonlinearities of the system as we would have if we used a fully implicit method. (In a more modern perspective, this should be seen as an "operator splitting" method. step-58 has a long description of the idea behind this.) 

We can then state the problem in weak form as follows, by multiplying each equation with test functions  [2.x.1039] ,  [2.x.1040] , and  [2.x.1041]  and integrating terms by parts: 

[1.x.393] 

Note that in the first term, we have to prescribe the pressure  [2.x.1042]  on the boundary  [2.x.1043]  as boundary values for our problem.  [2.x.1044]  denotes the unit outward normal vector to  [2.x.1045] , as usual. 

For the saturation equation, we obtain after integrating by parts 

[1.x.394] 

Using the fact that  [2.x.1046] , we can rewrite the cell term to get an equation as follows: 

[1.x.395] 

We introduce an object of type DiscreteTime in order to keep track of the current value of time and time step in the code. This class encapsulates many complexities regarding adjusting time step size and stopping at a specified final time. 




[1.x.396] 

In each time step, we then apply the mixed finite method of  [2.x.1047]  "step-20" to the velocity and pressure. To be well-posed, we choose Raviart-Thomas spaces  [2.x.1048]  for  [2.x.1049]  and discontinuous elements of class  [2.x.1050]  for  [2.x.1051] . For the saturation, we will also choose  [2.x.1052]  spaces. 

Since we have discontinuous spaces, we have to think about how to evaluate terms on the interfaces between cells, since discontinuous functions are not really defined there. In particular, we have to give a meaning to the last term on the left hand side of the saturation equation. To this end, let us define that we want to evaluate it in the following sense: 

[1.x.397] 

where  [2.x.1053]  denotes the inflow boundary and  [2.x.1054]  is the outflow part of the boundary. The quantities  [2.x.1055]  then correspond to the values of these variables on the present cell, whereas  [2.x.1056]  (needed on the inflow part of the boundary of  [2.x.1057] ) are quantities taken from the neighboring cell. Some more context on discontinuous element techniques and evaluation of fluxes can also be found in step-12 and step-12b. 




[1.x.398] 

The linear solvers used in this program are a straightforward extension of the ones used in step-20 (but without LinearOperator). Essentially, we simply have to extend everything from two to three solution components. If we use the discrete spaces mentioned above and put shape functions into the bilinear forms, we arrive at the following linear system to be solved for time step  [2.x.1058] : 

[1.x.399] 

where the individual matrices and vectors are defined as follows using shape functions  [2.x.1059]  (of type Raviart Thomas  [2.x.1060] ) for velocities and  [2.x.1061]  (of type  [2.x.1062] ) for both pressures and saturations: 

[1.x.400] 



 [2.x.1063]  Due to historical accidents, the role of matrices  [2.x.1064]  and  [2.x.1065]  has been reverted in this program compared to step-20. In other words, here  [2.x.1066]  refers to the divergence and  [2.x.1067]  to the gradient operators when it was the other way around in step-20. 

The system above presents a complication: Since the matrix  [2.x.1068]  depends on  [2.x.1069]  implicitly (the velocities are needed to determine which parts of the boundaries  [2.x.1070]  of cells are influx or outflux parts), we can only assemble this matrix after we have solved for the velocities. 

The solution scheme then involves the following steps: <ol>    [2.x.1071] Solve for the pressure  [2.x.1072]  using the Schur complement   technique introduced in step-20. 

   [2.x.1073] Solve for the velocity  [2.x.1074]  as also discussed in   step-20. 

   [2.x.1075] Compute the term  [2.x.1076] , using   the just computed velocities. 

   [2.x.1077] Solve for the saturation  [2.x.1078] .  [2.x.1079]  

In this scheme, we never actually build the matrix  [2.x.1080] , but rather generate the right hand side of the third equation once we are ready to do so. 

In the program, we use a variable  [2.x.1081]  to store the solution of the present time step. At the end of each step, we copy its content, i.e. all three of its block components, into the variable  [2.x.1082]  for use in the next time step. 




[1.x.401] 

A general rule of thumb in hyperbolic transport equations like the equation we have to solve for the saturation equation is that if we use an explicit time stepping scheme, then we should use a time step such that the distance that a particle can travel within one time step is no larger than the diameter of a single cell. In other words, here, we should choose 

[1.x.402] 

Fortunately, we are in a position where we can do that: we only need the time step when we want to assemble the right hand side of the saturation equation, which is after we have already solved for  [2.x.1083] . All we therefore have to do after solving for the velocity is to loop over all quadrature points in the domain and determine the maximal magnitude of the velocity. We can then set the time step for the saturation equation to 

[1.x.403] 



Why is it important to do this? If we don't, then we will end up with lots of places where our saturation is larger than one or less than zero, as can easily be verified. (Remember that the saturation corresponds to something like the water fraction in the fluid mixture, and therefore must physically be between 0 and 1.) On the other hand, if we choose our time step according to the criterion listed above, this only happens very very infrequently &mdash; in fact only once for the entire run of the program. However, to be on the safe side, however, we run a function  [2.x.1084]  at the end of each time step, that simply projects the saturation back onto the interval  [2.x.1085] , should it have gotten out of the physical range. This is useful since the functions  [2.x.1086]  and  [2.x.1087]  do not represent anything physical outside this range, and we should not expect the program to do anything useful once we have negative saturations or ones larger than one. 

Note that we will have similar restrictions on the time step also in step-23 and step-24 where we solve the time dependent wave equation, another hyperbolic problem. We will also come back to the issue of time step choice below in the section on [1.x.404]. 




[1.x.405] 

For simplicity, this program assumes that there is no source,  [2.x.1088] , and that the heterogeneous porous medium is isotropic  [2.x.1089] . The first one of these is a realistic assumption in oil reservoirs: apart from injection and production wells, there are usually no mechanisms for fluids to appear or disappear out of the blue. The second one is harder to justify: on a microscopic level, most rocks are isotropic, because they consist of a network of interconnected pores. However, this microscopic scale is out of the range of today's computer simulations, and we have to be content with simulating things on the scale of meters. On that scale, however, fluid transport typically happens through a network of cracks in the rock, rather than through pores. However, cracks often result from external stress fields in the rock layer (for example from tectonic faulting) and the cracks are therefore roughly aligned. This leads to a situation where the permeability is often orders of magnitude larger in the direction parallel to the cracks than perpendicular to the cracks. A problem typically faces in reservoir simulation, however, is that the modeler doesn't know the direction of cracks because oil reservoirs are not accessible to easy inspection. The only solution in that case is to assume an effective, isotropic permeability. 

Whatever the matter, both of these restrictions, no sources and isotropy, would be easy to lift with a few lines of code in the program. 

Next, for simplicity, our numerical simulation will be done on the unit cell  [2.x.1090]  for  [2.x.1091] . Our initial conditions are  [2.x.1092] ; in the oil reservoir picture, where  [2.x.1093]  would indicate the water saturation, this means that the reservoir contains pure oil at the beginning. Note that we do not need any initial conditions for pressure or velocity, since the equations do not contain time derivatives of these variables. Finally, we impose the following pressure boundary conditions: 

[1.x.406] 

Since the pressure and velocity solve a mixed form Poisson equation, the imposed pressure leads to a resulting flow field for the velocity. On the other hand, this flow field determines whether a piece of the boundary is of inflow or outflow type, which is of relevance because we have to impose boundary conditions for the saturation on the inflow part of the boundary, 

[1.x.407] 

On this inflow boundary, we impose the following saturation values: 

[1.x.408] 

In other words, we have pure water entering the reservoir at the left, whereas the other parts of the boundary are in contact with undisturbed parts of the reservoir and whenever influx occurs on these boundaries, pure oil will enter. 

In our simulations, we choose the total mobility as 

[1.x.409] 

where we use  [2.x.1094]  for the viscosity. In addition, the fractional flow of water is given by 

[1.x.410] 



 [2.x.1095]  Coming back to this testcase in step-43 several years later revealed an oddity in the setup of this testcase. To this end, consider that we can rewrite the advection equation for the saturation as  [2.x.1096] . Now, at the initial time, we have  [2.x.1097] , and with the given choice of function  [2.x.1098] , we happen to have  [2.x.1099] . In other words, at  [2.x.1100] , the equation reduces to  [2.x.1101]  for all  [2.x.1102] , so the saturation is zero everywhere and it is going to stay zero everywhere! This is despite the fact that  [2.x.1103]  is not necessarily zero: the combined fluid is moving, but we've chosen our partial flux  [2.x.1104]  in such a way that infinitesimal amounts of wetting fluid also only move at infinitesimal speeds (i.e., they stick to the medium more than the non-wetting phase in which they are embedded). That said, how can we square this with the knowledge that wetting fluid is invading from the left, leading to the flow patterns seen in the [1.x.411]? That's where we get into mathematics: Equations like the transport equation we are considering here have infinitely many solutions, but only one of them is physical: the one that results from the so-called viscosity limit, called the [1.x.412]. The thing is that with discontinuous elements we arrive at this viscosity limit because using a numerical flux introduces a finite amount of artificial viscosity into the numerical scheme. On the other hand, in step-43, we use an artificial viscosity that is proportional to  [2.x.1105]  on every cell, which at the initial time is zero. Thus, the saturation there is zero and remains zero; the solution we then get is [1.x.413] solution of the advection equation, but the method does not converge to the viscosity solution without further changes. We will therefore use a different initial condition in that program. 


Finally, to come back to the description of the testcase, we will show results for computations with the two permeability functions introduced at the end of the results section of  [2.x.1106]  "step-20":  [2.x.1107]     [2.x.1108] A function that models a single, winding crack that snakes through the   domain. In analogy to step-20, but taking care of the slightly   different geometry we have here, we describe this by the following function:   [1.x.414] 

  Taking the maximum is necessary to ensure that the ratio between maximal and   minimal permeability remains bounded. If we don't do that, permeabilities   will span many orders of magnitude. On the other hand, the ratio between   maximal and minimal permeability is a factor in the condition number of the   Schur complement matrix, and if too large leads to problems for which our   linear solvers will no longer converge properly. 

   [2.x.1109] A function that models a somewhat random medium. Here, we choose   [1.x.415] 

  where the centers  [2.x.1110]  are  [2.x.1111]  randomly chosen locations inside   the domain. This function models a domain in which there are  [2.x.1112]  centers of   higher permeability (for example where rock has cracked) embedded in a   matrix of more pristine, unperturbed background rock. Note that here we have   cut off the permeability function both above and below to ensure a bounded   condition number.  [2.x.1113]  


examples/step-21/doc/results.dox 



[1.x.416] 

The code as presented here does not actually compute the results found on the web page. The reason is, that even on a decent computer it runs more than a day. If you want to reproduce these results, modify the end time of the DiscreteTime object to `250` within the constructor of TwoPhaseFlowProblem. 

If we run the program, we get the following kind of output: 

[1.x.417] 

As we can see, the time step is pretty much constant right from the start, which indicates that the velocities in the domain are not strongly dependent on changes in saturation, although they certainly are through the factor  [2.x.1114]  in the pressure equation. 

Our second observation is that the number of CG iterations needed to solve the pressure Schur complement equation drops from 22 to 17 between the first and the second time step (in fact, it remains around 17 for the rest of the computations). The reason is actually simple: Before we solve for the pressure during a time step, we don't reset the  [2.x.1115]  variable to zero. The pressure (and the other variables) therefore have the previous time step's values at the time we get into the CG solver. Since the velocities and pressures don't change very much as computations progress, the previous time step's pressure is actually a good initial guess for this time step's pressure. Consequently, the number of iterations we need once we have computed the pressure once is significantly reduced. 

The final observation concerns the number of iterations needed to solve for the saturation, i.e. one. This shouldn't surprise us too much: the matrix we have to solve with is the mass matrix. However, this is the mass matrix for the  [2.x.1116]  element of piecewise constants where no element couples with the degrees of freedom on neighboring cells. The matrix is therefore a diagonal one, and it is clear that we should be able to invert this matrix in a single CG iteration. 


With all this, here are a few movies that show how the saturation progresses over time. First, this is for the single crack model, as implemented in the  [2.x.1117]  class: 

 [2.x.1118]  

As can be seen, the water rich fluid snakes its way mostly along the high-permeability zone in the middle of the domain, whereas the rest of the domain is mostly impermeable. This and the next movie are generated using  [2.x.1119] , leading to a  [2.x.1120]  mesh with some 16,000 cells and about 66,000 unknowns in total. 


The second movie shows the saturation for the random medium model of class  [2.x.1121] , where we have randomly distributed centers of high permeability and fluid hops from one of these zones to the next: 

 [2.x.1122]  


Finally, here is the same situation in three space dimensions, on a mesh with  [2.x.1123] , which produces a mesh of some 32,000 cells and 167,000 degrees of freedom: 

 [2.x.1124]  

To repeat these computations, all you have to do is to change the line 

[1.x.418] 

in the main function to 

[1.x.419] 

The visualization uses a cloud technique, where the saturation is indicated by colored but transparent clouds for each cell. This way, one can also see somewhat what happens deep inside the domain. A different way of visualizing would have been to show isosurfaces of the saturation evolving over time. There are techniques to plot isosurfaces transparently, so that one can see several of them at the same time like the layers of an onion. 

So why don't we show such isosurfaces? The problem lies in the way isosurfaces are computed: they require that the field to be visualized is continuous, so that the isosurfaces can be generated by following contours at least across a single cell. However, our saturation field is piecewise constant and discontinuous. If we wanted to plot an isosurface for a saturation  [2.x.1125] , chances would be that there is no single point in the domain where that saturation is actually attained. If we had to define isosurfaces in that context at all, we would have to take the interfaces between cells, where one of the two adjacent cells has a saturation greater than and the other cell a saturation less than 0.5. However, it appears that most visualization programs are not equipped to do this kind of transformation. 


[1.x.420] 

[1.x.421] 

There are a number of areas where this program can be improved. Three of them are listed below. All of them are, in fact, addressed in a tutorial program that forms the continuation of the current one: step-43. 




[1.x.422] 

At present, the program is not particularly fast: the 2d random medium computation took about a day for the 1,000 or so time steps. The corresponding 3d computation took almost two days for 800 time steps. The reason why it isn't faster than this is twofold. First, we rebuild the entire matrix in every time step, although some parts such as the  [2.x.1126] ,  [2.x.1127] , and  [2.x.1128]  blocks never change. 

Second, we could do a lot better with the solver and preconditioners. Presently, we solve the Schur complement  [2.x.1129]  with a CG method, using  [2.x.1130]  as a preconditioner. Applying this preconditioner is expensive, since it involves solving a linear system each time. This may have been appropriate for  [2.x.1131]  "step-20", where we have to solve the entire problem only once. However, here we have to solve it hundreds of times, and in such cases it is worth considering a preconditioner that is more expensive to set up the first time, but cheaper to apply later on. 

One possibility would be to realize that the matrix we use as preconditioner,  [2.x.1132]  is still sparse, and symmetric on top of that. If one looks at the flow field evolve over time, we also see that while  [2.x.1133]  changes significantly over time, the pressure hardly does and consequently  [2.x.1134] . In other words, the matrix for the first time step should be a good preconditioner also for all later time steps.  With a bit of back-and-forthing, it isn't hard to actually get a representation of it as a SparseMatrix object. We could then hand it off to the SparseMIC class to form a sparse incomplete Cholesky decomposition. To form this decomposition is expensive, but we have to do it only once in the first time step, and can then use it as a cheap preconditioner in the future. We could do better even by using the SparseDirectUMFPACK class that produces not only an incomplete, but a complete decomposition of the matrix, which should yield an even better preconditioner. 

Finally, why use the approximation  [2.x.1135]  to precondition  [2.x.1136] ? The latter matrix, after all, is the mixed form of the Laplace operator on the pressure space, for which we use linear elements. We could therefore build a separate matrix  [2.x.1137]  on the side that directly corresponds to the non-mixed formulation of the Laplacian, for example using the bilinear form  [2.x.1138] . We could then form an incomplete or complete decomposition of this non-mixed matrix and use it as a preconditioner of the mixed form. 

Using such techniques, it can reasonably be expected that the solution process will be faster by at least an order of magnitude. 




[1.x.423] 

In the introduction we have identified the time step restriction 

[1.x.424] 

that has to hold globally, i.e. for all  [2.x.1139] . After discretization, we satisfy it by choosing 

[1.x.425] 



This restriction on the time step is somewhat annoying: the finer we make the mesh the smaller the time step; in other words, we get punished twice: each time step is more expensive to solve and we have to do more time steps. 

This is particularly annoying since the majority of the additional work is spent solving the implicit part of the equations, i.e. the pressure-velocity system, whereas it is the hyperbolic transport equation for the saturation that imposes the time step restriction. 

To avoid this bottleneck, people have invented a number of approaches. For example, they may only re-compute the pressure-velocity field every few time steps (or, if you want, use different time step sizes for the pressure/velocity and saturation equations). This keeps the time step restriction on the cheap explicit part while it makes the solution of the implicit part less frequent. Experiments in this direction are certainly worthwhile; one starting point for such an approach is the paper by Zhangxin Chen, Guanren Huan and Baoyan Li: [1.x.426], Transport in Porous Media, 54 (2004), pp. 361&mdash;376. There are certainly many other papers on this topic as well, but this one happened to land on our desk a while back. 




[1.x.427] 

Adaptivity would also clearly help. Looking at the movies, one clearly sees that most of the action is confined to a relatively small part of the domain (this particularly obvious for the saturation, but also holds for the velocities and pressures). Adaptivity can therefore be expected to keep the necessary number of degrees of freedom low, or alternatively increase the accuracy. 

On the other hand, adaptivity for time dependent problems is not a trivial thing: we would have to change the mesh every few time steps, and we would have to transport our present solution to the next mesh every time we change it (something that the SolutionTransfer class can help with). These are not insurmountable obstacles, but they do require some additional coding and more than we felt comfortable was worth packing into this tutorial program. 


examples/step-22/doc/intro.dox 

 [2.x.1140]  

[1.x.428] 




[1.x.429] 

[1.x.430] 

This program deals with the Stokes system of equations which reads as follows in non-dimensionalized form: 

[1.x.431] 

where  [2.x.1141]  denotes the velocity of a fluid,  [2.x.1142]  is its pressure,  [2.x.1143]  are external forces, and  [2.x.1144]   is the rank-2 tensor of symmetrized gradients; a component-wise definition of it is  [2.x.1145] . 

The Stokes equations describe the steady-state motion of a slow-moving, viscous fluid such as honey, rocks in the earth mantle, or other cases where inertia does not play a significant role. If a fluid is moving fast enough that inertia forces are significant compared to viscous friction, the Stokes equations are no longer valid; taking into account inertia effects then leads to the nonlinear Navier-Stokes equations. However, in this tutorial program, we will focus on the simpler Stokes system. 

Note that when deriving the more general compressible Navier-Stokes equations, the diffusion is modeled as the divergence of the stress tensor 

[1.x.432] 

where  [2.x.1146]  is the viscosity of the fluid. With the assumption of  [2.x.1147]  (assume constant viscosity and non-dimensionalize the equation by dividing out  [2.x.1148] ) and assuming incompressibility ( [2.x.1149] ), we arrive at the formulation from above: 

[1.x.433] 

A different formulation uses the Laplace operator ( [2.x.1150] ) instead of the symmetrized gradient. A big difference here is that the different components of the velocity do not couple. If you assume additional regularity of the solution  [2.x.1151]  (second partial derivatives exist and are continuous), the formulations are equivalent: 

[1.x.434] 

This is because the  [2.x.1152] th entry of   [2.x.1153]  is given by: 

[1.x.435] 

If you can not assume the above mentioned regularity, or if your viscosity is not a constant, the equivalence no longer holds. Therefore, we decided to stick with the more physically accurate symmetric tensor formulation in this tutorial. 


To be well-posed, we will have to add boundary conditions to the equations. What boundary conditions are readily possible here will become clear once we discuss the weak form of the equations. 

The equations covered here fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.1154]  module. 




[1.x.436] 

The weak form of the equations is obtained by writing it in vector form as 

[1.x.437] 

forming the dot product from the left with a vector-valued test function  [2.x.1155]  and integrating over the domain  [2.x.1156] , yielding the following set of equations: 

[1.x.438] 

which has to hold for all test functions  [2.x.1157] . 

A generally good rule of thumb is that if one [1.x.439] reduce how many derivatives are taken on any variable in the formulation, then one [1.x.440] in fact do that using integration by parts. (This is motivated by the theory of [1.x.441], and in particular the difference between strong and [1.x.442].) We have already done that for the Laplace equation, where we have integrated the second derivative by parts to obtain the weak formulation that has only one derivative on both test and trial function. 

In the current context, we integrate by parts the second term: 

[1.x.443] 

Likewise, we integrate by parts the first term to obtain 

[1.x.444] 

where the scalar product between two tensor-valued quantities is here defined as 

[1.x.445] 

Using this, we have now reduced the requirements on our variables to first derivatives for  [2.x.1158]  and no derivatives at all for  [2.x.1159] . 

Because the scalar product between a general tensor like  [2.x.1160]  and a symmetric tensor like  [2.x.1161]  equals the scalar product between the symmetrized forms of the two, we can also write the bilinear form above as follows: 

[1.x.446] 

We will deal with the boundary terms in the next section, but it is already clear from the domain terms 

[1.x.447] 

of the bilinear form that the Stokes equations yield a symmetric bilinear form, and consequently a symmetric (if indefinite) system matrix. 




[1.x.448] 

 [2.x.1162]  ( [2.x.1163]  

The weak form just derived immediately presents us with different possibilities for imposing boundary conditions: <ol>  [2.x.1164] Dirichlet velocity boundary conditions: On a part      [2.x.1165]  we may impose Dirichlet conditions     on the velocity  [2.x.1166] : 

    [1.x.449] 

    Because test functions  [2.x.1167]  come from the tangent space of     the solution variable, we have that  [2.x.1168]  on  [2.x.1169]      and consequently that     [1.x.450] 

    In other words, as usual, strongly imposed boundary values do not     appear in the weak form. 

    It is noteworthy that if we impose Dirichlet boundary values on the entire     boundary, then the pressure is only determined up to a constant. An     algorithmic realization of that would use similar tools as have been seen in     step-11. 

 [2.x.1170] Neumann-type or natural boundary conditions: On the rest of the boundary      [2.x.1171] , let us re-write the     boundary terms as follows:     [1.x.451] 

    In other words, on the Neumann part of the boundary we can     prescribe values for the total stress:     [1.x.452] 

    If the boundary is subdivided into Dirichlet and Neumann parts      [2.x.1172] , this then leads to the following weak form:     [1.x.453] 




 [2.x.1173] Robin-type boundary conditions: Robin boundary conditions are a mixture of     Dirichlet and Neumann boundary conditions. They would read     [1.x.454] 

    with a rank-2 tensor (matrix)  [2.x.1174] . The associated weak form is     [1.x.455] 



 [2.x.1175] Partial boundary conditions: It is possible to combine Dirichlet and     Neumann boundary conditions by only enforcing each of them for certain     components of the velocity. For example, one way to impose artificial     boundary conditions is to require that the flow is perpendicular to the     boundary, i.e. the tangential component  [2.x.1176]  be zero, thereby constraining      [2.x.1177] -1 components of the velocity. The remaining component can     be constrained by requiring that the normal component of the normal     stress be zero, yielding the following set of boundary conditions:     [1.x.456] 



    An alternative to this is when one wants the flow to be [1.x.457]     rather than perpendicular to the boundary (in deal.II, the      [2.x.1178]  function can do this for     you). This is frequently the case for problems with a free boundary     (e.g. at the surface of a river or lake if vertical forces of the flow are     not large enough to actually deform the surface), or if no significant     friction is exerted by the boundary on the fluid (e.g. at the interface     between earth mantle and earth core where two fluids meet that are     stratified by different densities but that both have small enough     viscosities to not introduce much tangential stress on each other).     In formulas, this means that     [1.x.458] 

    the first condition (which needs to be imposed strongly) fixing a single     component of the velocity, with the second (which would be enforced in the     weak form) fixing the remaining two components.  [2.x.1179]  

Despite this wealth of possibilities, we will only use Dirichlet and (homogeneous) Neumann boundary conditions in this tutorial program. 




[1.x.459] 

As developed above, the weak form of the equations with Dirichlet and Neumann boundary conditions on  [2.x.1180]  and  [2.x.1181]  reads like this: find  [2.x.1182]  so that 

[1.x.460] 

for all test functions  [2.x.1183] . 

These equations represent a symmetric [1.x.461]. It is well known that then a solution only exists if the function spaces in which we search for a solution have to satisfy certain conditions, typically referred to as the Babuska-Brezzi or Ladyzhenskaya-Babuska-Brezzi (LBB) conditions. The continuous function spaces above satisfy these. However, when we discretize the equations by replacing the continuous variables and test functions by finite element functions in finite dimensional spaces  [2.x.1184] , we have to make sure that  [2.x.1185]  also satisfy the LBB conditions. This is similar to what we had to do in step-20. 

For the Stokes equations, there are a number of possible choices to ensure that the finite element spaces are compatible with the LBB condition. A simple and accurate choice that we will use here is  [2.x.1186] , i.e. use elements one order higher for the velocities than for the pressures. 

This then leads to the following discrete problem: find  [2.x.1187]  so that 

[1.x.462] 

for all test functions  [2.x.1188] . Assembling the linear system associated with this problem follows the same lines used in  [2.x.1189]  "step-20", step-21, and explained in detail in the  [2.x.1190]  module. 




[1.x.463] 

The weak form of the discrete equations naturally leads to the following linear system for the nodal values of the velocity and pressure fields: 

[1.x.464] 

Like in step-20 and step-21, we will solve this system of equations by forming the Schur complement, i.e. we will first find the solution  [2.x.1191]  of 

[1.x.465] 

and then 

[1.x.466] 

The way we do this is pretty much exactly like we did in these previous tutorial programs, i.e. we use the same classes  [2.x.1192]  and  [2.x.1193]  again. There are two significant differences, however: 

<ol>  [2.x.1194]  First, in the mixed Laplace equation we had to deal with the question of how to precondition the Schur complement  [2.x.1195] , which was spectrally equivalent to the Laplace operator on the pressure space (because  [2.x.1196]  represents the gradient operator,  [2.x.1197]  its adjoint  [2.x.1198] , and  [2.x.1199]  the identity (up to the material parameter  [2.x.1200] ), so  [2.x.1201]  is something like  [2.x.1202] ). Consequently, the matrix is badly conditioned for small mesh sizes and we had to come up with an elaborate preconditioning scheme for the Schur complement. 

 [2.x.1203]  Second, every time we multiplied with  [2.x.1204]  we had to solve with the mass matrix  [2.x.1205] . This wasn't particularly difficult, however, since the mass matrix is always well conditioned and so simple to invert using CG and a little bit of preconditioning.  [2.x.1206]  In other words, preconditioning the inner solver for  [2.x.1207]  was simple whereas preconditioning the outer solver for  [2.x.1208]  was complicated. 

Here, the situation is pretty much exactly the opposite. The difference stems from the fact that the matrix at the heart of the Schur complement does not stem from the identity operator but from a variant of the Laplace operator,  [2.x.1209]  (where  [2.x.1210]  is the symmetric gradient) acting on a vector field. In the investigation of this issue we largely follow the paper D. Silvester and A. Wathen: "Fast iterative solution of stabilised Stokes systems part II. Using general block preconditioners." (SIAM J. Numer. Anal., 31 (1994), pp. 1352-1367), which is available online [1.x.467]. Principally, the difference in the matrix at the heart of the Schur complement has two consequences: 

<ol>  [2.x.1211]  First, it makes the outer preconditioner simple: the Schur complement corresponds to the operator  [2.x.1212]  on the pressure space; forgetting about the fact that we deal with symmetric gradients instead of the regular one, the Schur complement is something like  [2.x.1213] , which, even if not mathematically entirely concise, is spectrally equivalent to the identity operator (a heuristic argument would be to commute the operators into  [2.x.1214] ). It turns out that it isn't easy to solve this Schur complement in a straightforward way with the CG method: using no preconditioner, the condition number of the Schur complement matrix depends on the size ratios of the largest to the smallest cells, and one still needs on the order of 50-100 CG iterations. However, there is a simple cure: precondition with the mass matrix on the pressure space and we get down to a number between 5-15 CG iterations, pretty much independently of the structure of the mesh (take a look at the [1.x.468] of this program to see that indeed the number of CG iterations does not change as we refine the mesh). 

So all we need in addition to what we already have is the mass matrix on the pressure variables and we will store it in a separate object. 




 [2.x.1215]  While the outer preconditioner has become simpler compared to the mixed Laplace case discussed in step-20, the issue of the inner solver has become more complicated. In the mixed Laplace discretization, the Schur complement has the form  [2.x.1216] . Thus, every time we multiplied with the Schur complement, we had to solve a linear system  [2.x.1217] ; this isn't too complicated there, however, since the mass matrix  [2.x.1218]  on the pressure space is well-conditioned. 


On the other hand, for the Stokes equation we consider here, the Schur complement is  [2.x.1219]  where the matrix  [2.x.1220]  is related to the Laplace operator (it is, in fact, the matrix corresponding to the bilinear form  [2.x.1221] ). Thus, solving with  [2.x.1222]  is a lot more complicated: the matrix is badly conditioned and we know that we need many iterations unless we have a very good preconditioner. What is worse, we have to solve with  [2.x.1223]  every time we multiply with the Schur complement, which is 5-15 times using the preconditioner described above. 

Because we have to solve with  [2.x.1224]  several times, it pays off to spend a bit more time once to create a good preconditioner for this matrix. So here's what we're going to do: if in 2d, we use the ultimate preconditioner, namely a direct sparse LU decomposition of the matrix. This is implemented using the SparseDirectUMFPACK class that uses the UMFPACK direct solver to compute the decomposition. To use it, you will have to build deal.II with UMFPACK support (which is the default); see the [1.x.469] for instructions. With this, the inner solver converges in one iteration. 

In 2d, we can do this sort of thing because even reasonably large problems rarely have more than a few 100,000 unknowns with relatively few nonzero entries per row. Furthermore, the bandwidth of matrices in 2d is  [2.x.1225]  and therefore moderate. For such matrices, sparse factors can be computed in a matter of a few seconds. (As a point of reference, computing the sparse factors of a matrix of size  [2.x.1226]  and bandwidth  [2.x.1227]  takes  [2.x.1228]  operations. In 2d, this is  [2.x.1229] ; though this is a higher complexity than, for example, assembling the linear system which takes  [2.x.1230] , the constant for computing the decomposition is so small that it doesn't become the dominating factor in the entire program until we get to very large %numbers of unknowns in the high 100,000s or more.) 

The situation changes in 3d, because there we quickly have many more unknowns and the bandwidth of matrices (which determines the number of nonzero entries in sparse LU factors) is  [2.x.1231] , and there are many more entries per row as well. This makes using a sparse direct solver such as UMFPACK inefficient: only for problem sizes of a few 10,000 to maybe 100,000 unknowns can a sparse decomposition be computed using reasonable time and memory resources. 

What we do in that case is to use an incomplete LU decomposition (ILU) as a preconditioner, rather than actually computing complete LU factors. As it so happens, deal.II has a class that does this: SparseILU. Computing the ILU takes a time that only depends on the number of nonzero entries in the sparse matrix (or that we are willing to fill in the LU factors, if these should be more than the ones in the matrix), but is independent of the bandwidth of the matrix. It is therefore an operation that can efficiently also be computed in 3d. On the other hand, an incomplete LU decomposition, by definition, does not represent an exact inverse of the matrix  [2.x.1232] . Consequently, preconditioning with the ILU will still require more than one iteration, unlike preconditioning with the sparse direct solver. The inner solver will therefore take more time when multiplying with the Schur complement: an unavoidable trade-off.  [2.x.1233]  

In the program below, we will make use of the fact that the SparseILU and SparseDirectUMFPACK classes have a very similar interface and can be used interchangeably. All that we need is a switch class that, depending on the dimension, provides a type that is either of the two classes mentioned above. This is how we do that: 

[1.x.470] 



From here on, we can refer to the type <code>typename  [2.x.1234]  and automatically get the correct preconditioner class. Because of the similarity of the interfaces of the two classes, we will be able to use them interchangeably using the same syntax in all places. 




[1.x.471] 

The discussions above showed *one* way in which the linear system that results from the Stokes equations can be solved, and because the tutorial programs are teaching tools that makes sense. But is this the way this system of equations *should* be solved? 

The answer to this is no. The primary bottleneck with the approach, already identified above, is that we have to repeatedly solve linear systems with  [2.x.1235]  inside the Schur complement, and because we don't have a good preconditioner for the Schur complement, these solves just have to happen too often. A better approach is to use a block decomposition, which is based on an observation of Silvester and Wathen  [2.x.1236]  and explained in much greater detail in  [2.x.1237]  . An implementation of this alternative approach is discussed below, in the section on a [1.x.472] in the results section of this program. 




[1.x.473] 

Above, we have claimed that the linear system has the form 

[1.x.474] 

i.e., in particular that there is a zero block at the bottom right of the matrix. This then allowed us to write the Schur complement as  [2.x.1238] . But this is not quite correct. 

Think of what would happen if there are constraints on some pressure variables (see the  [2.x.1239]  "Constraints on degrees of freedom" documentation module), for example because we use adaptively refined meshes and continuous pressure finite elements so that there are hanging nodes. Another cause for such constraints are Dirichlet boundary conditions on the pressure. Then the AffineConstraints class, upon copying the local contributions to the matrix into the global linear system will zero out rows and columns corresponding to constrained degrees of freedom and put a positive entry on the diagonal. (You can think of this entry as being one for simplicity, though in reality it is a value of the same order of magnitude as the other matrix entries.) In other words, the bottom right block is really not empty at all: It has a few entries on the diagonal, one for each constrained pressure degree of freedom, and a correct description of the linear system we have to solve is that it has the form 

[1.x.475] 

where  [2.x.1240]  is the zero matrix with the exception of the positive diagonal entries for the constrained degrees of freedom. The correct Schur complement would then in fact be the matrix  [2.x.1241]  instead of the one stated above. 

Thinking about this makes us, first, realize that the resulting Schur complement is now indefinite because  [2.x.1242]  is symmetric and positive definite whereas  [2.x.1243]  is a positive semidefinite, and subtracting the latter from the former may no longer be positive definite. This is annoying because we could no longer employ the Conjugate Gradient method on this true Schur complement. That said, we could fix the issue in  [2.x.1244]  by simply putting *negative* values onto the diagonal for the constrained pressure variables -- because we really only put something nonzero to ensure that the resulting matrix is not singular; we really didn't care whether that entry is positive or negative. So if the entries on the diagonal of  [2.x.1245]  were negative, then  [2.x.1246]  would again be a symmetric and positive definite matrix. 

But, secondly, the code below doesn't actually do any of that: It happily solves the linear system with the wrong Schur complement  [2.x.1247]  that just ignores the issue altogether. Why does this even work? To understand why this is so, recall that when writing local contributions into the global matrix,  [2.x.1248]  zeros out the rows and columns that correspond to constrained degrees of freedom. This means that  [2.x.1249]  has some zero rows, and  [2.x.1250]  zero columns. As a consequence, if one were to multiply out what the entries of  [2.x.1251]  are, one would realize that it has zero rows and columns for all constrained pressure degrees of freedom, including a zero on the diagonal. The nonzero entries of  [2.x.1252]  would fit into exactly those zero diagonal locations, and ensure that  [2.x.1253]  is invertible. Not doing so, strictly speaking, means that  [2.x.1254]  remains singular: It is symmetric and positive definite on the subset of non-constrained pressure degrees of freedom, and simply the zero matrix on the constrained pressures. Why does the Conjugate Gradient method work for this matrix? Because  [2.x.1255]  also makes sure that the right hand side entries that correspond to these zero rows of the matrix are *also* zero, i.e., the right hand side is compatible. 

What this means is that whatever the values of the solution vector for these constrained pressure degrees of freedom, these rows will always have a zero residual and, if one were to consider what the CG algorithm does internally, just never produce any updates to the solution vector. In other words, the CG algorithm just *ignores* these rows, despite the fact that the matrix is singular. This only works because these degrees of freedom are entirely decoupled from the rest of the linear system (because the entire row and corresponding column are zero). At the end of the solution process, the constrained pressure values in the solution vector therefore remain exactly as they were when we started the call to the solver; they are finally overwritten with their correct values when we call  [2.x.1256]  after the CG solver is done. 

The upshot of this discussion is that the assumption that the bottom right block of the big matrix is zero is a bit simplified, but that just going with it does not actually lead to any practical problems worth addressing. 




[1.x.476] 

The domain, right hand side and boundary conditions we implement below relate to a problem in geophysics: there, one wants to compute the flow field of magma in the earth's interior under a mid-ocean rift. Rifts are places where two continental plates are very slowly drifting apart (a few centimeters per year at most), leaving a crack in the earth crust that is filled with magma from below. Without trying to be entirely realistic, we model this situation by solving the following set of equations and boundary conditions on the domain  [2.x.1257] : 

[1.x.477] 

and using natural boundary conditions  [2.x.1258]  everywhere else. In other words, at the left part of the top surface we prescribe that the fluid moves with the continental plate to the left at speed  [2.x.1259] , that it moves to the right on the right part of the top surface, and impose natural flow conditions everywhere else. If we are in 2d, the description is essentially the same, with the exception that we omit the second component of all vectors stated above. 

As will become apparent in the [1.x.478], the flow field will pull material from below and move it to the left and right ends of the domain, as expected. The discontinuity of velocity boundary conditions will produce a singularity in the pressure at the center of the top surface that sucks material all the way to the top surface to fill the gap left by the outward motion of material at this location. 




[1.x.479] 

[1.x.480] 

In all the previous tutorial programs, we used the AffineConstraints object merely for handling hanging node constraints (with exception of step-11). However, the class can also be used to implement Dirichlet boundary conditions, as we will show in this program, by fixing some node values  [2.x.1260] . Note that these are inhomogeneous constraints, and we have to pay some special attention to that. The way we are going to implement this is to first read in the boundary values into the AffineConstraints object by using the call 

[1.x.481] 



very similar to how we were making the list of boundary nodes before (note that we set Dirichlet conditions only on boundaries with boundary flag 1). The actual application of the boundary values is then handled by the AffineConstraints object directly, without any additional interference. 

We could then proceed as before, namely by filling the matrix, and then calling a condense function on the constraints object of the form 

[1.x.482] 



Note that we call this on the system matrix and system right hand side simultaneously, since resolving inhomogeneous constraints requires knowledge about both the matrix entries and the right hand side. For efficiency reasons, though, we choose another strategy: all the constraints collected in the AffineConstraints object can be resolved on the fly while writing local data into the global matrix, by using the call 

[1.x.483] 



This technique is further discussed in the step-27 tutorial program. All we need to know here is that this functions does three things at once: it writes the local data into the global matrix and right hand side, it distributes the hanging node constraints and additionally implements (inhomogeneous) Dirichlet boundary conditions. That's nice, isn't it? 

We can conclude that the AffineConstraints class provides an alternative to using  [2.x.1261]  for implementing Dirichlet boundary conditions. 


[1.x.484][1.x.485] 

Frequently, a sparse matrix contains a substantial amount of elements that actually are zero when we are about to start a linear solve. Such elements are introduced when we eliminate constraints or implement Dirichlet conditions, where we usually delete all entries in constrained rows and columns, i.e., we set them to zero. The fraction of elements that are present in the sparsity pattern, but do not really contain any information, can be up to one fourth of the total number of elements in the matrix for the 3D application considered in this tutorial program. Remember that matrix-vector products or preconditioners operate on all the elements of a sparse matrix (even those that are zero), which is an inefficiency we will avoid here. 

An advantage of directly resolving constrained degrees of freedom is that we can avoid having most of the entries that are going to be zero in our sparse matrix &mdash; we do not need constrained entries during matrix construction (as opposed to the traditional algorithms, which first fill the matrix, and only resolve constraints afterwards). This will save both memory and time when forming matrix-vector products. The way we are going to do that is to pass the information about constraints to the function that generates the sparsity pattern, and then set a <tt>false</tt> argument specifying that we do not intend to use constrained entries: 

[1.x.486] 

This functions obviates, by the way, also the call to the <tt>condense()</tt> function on the sparsity pattern. 




[1.x.487] 

The program developed below has seen a lot of TLC. We have run it over and over under profiling tools (mainly [1.x.488]'s cachegrind and callgrind tools, as well as the KDE [1.x.489] program for visualization) to see where the bottlenecks are. This has paid off: through this effort, the program has become about four times as fast when considering the runtime of the refinement cycles zero through three, reducing the overall number of CPU instructions executed from 869,574,060,348 to 199,853,005,625. For higher refinement levels, the gain is probably even larger since some algorithms that are not  [2.x.1262]  have been eliminated. 

Essentially, there are currently two algorithms in the program that do not scale linearly with the number of degrees of freedom: renumbering of degrees of freedom (which is  [2.x.1263] , and the linear solver (which is  [2.x.1264] ). As for the first, while reordering degrees of freedom may not scale linearly, it is an indispensable part of the overall algorithm as it greatly improves the quality of the sparse ILU, easily making up for the time spent on computing the renumbering; graphs and timings to demonstrate this are shown in the documentation of the DoFRenumbering namespace, also underlining the choice of the Cuthill-McKee reordering algorithm chosen below. 

As for the linear solver: as mentioned above, our implementation here uses a Schur complement formulation. This is not necessarily the very best choice but demonstrates various important techniques available in deal.II. The question of which solver is best is again discussed in the [1.x.490] of this program, along with code showing alternative solvers and a comparison of their results. 

Apart from this, many other algorithms have been tested and improved during the creation of this program. For example, in building the sparsity pattern, we originally used a (now no longer existing) BlockCompressedSparsityPattern object that added one element at a time; however, its data structures were poorly adapted for the large numbers of nonzero entries per row created by our discretization in 3d, leading to a quadratic behavior. Replacing the internal algorithms in deal.II to set many elements at a time, and using a BlockCompressedSimpleSparsityPattern (which has, as of early 2015, been in turn replaced by BlockDynamicSparsityPattern) as a better adapted data structure, removed this bottleneck at the price of a slightly higher memory consumption. Likewise, the implementation of the decomposition step in the SparseILU class was very inefficient and has been replaced by one that is about 10 times faster. Even the vmult function of the SparseILU has been improved to save about twenty percent of time. Small improvements were applied here and there. Moreover, the AffineConstraints object has been used to eliminate a lot of entries in the sparse matrix that are eventually going to be zero, see [1.x.491]. 

A profile of how many CPU instructions are spent at the various different places in the program during refinement cycles zero through three in 3d is shown here: 

 [2.x.1265]  

As can be seen, at this refinement level approximately three quarters of the instruction count is spent on the actual solver (the  [2.x.1266]  calls on the left, the  [2.x.1267]  call in the middle for the Schur complement solve, and another box representing the multiplications with SparseILU and SparseMatrix in the solve for [1.x.492]). About one fifth of the instruction count is spent on matrix assembly and sparse ILU computation (box in the lower right corner) and the rest on other things. Since floating point operations such as in the  [2.x.1268]  calls typically take much longer than many of the logical operations and table lookups in matrix assembly, the fraction of the run time taken up by matrix assembly is actually significantly less than the fraction of instructions, as will become apparent in the comparison we make in the results section. 

For higher refinement levels, the boxes representing the solver as well as the blue box at the top right stemming from reordering algorithm are going to grow at the expense of the other parts of the program, since they don't scale linearly. The fact that at this moderate refinement level (3168 cells and 93176 degrees of freedom) the linear solver already makes up about three quarters of the instructions is a good sign that most of the algorithms used in this program are well-tuned and that major improvements in speeding up the program are most likely not to come from hand-optimizing individual aspects but by changing solver algorithms. We will address this point in the discussion of results below as well. 

As a final point, and as a point of reference, the following picture also shows how the profile looked at an early stage of optimizing this program: 

 [2.x.1269]  

As mentioned above, the runtime of this version was about four times as long as for the first profile, with the SparseILU decomposition taking up about 30% of the instruction count, and operations an early, inefficient version of DynamicSparsityPattern about 10%. Both these bottlenecks have since been completely removed. 


examples/step-22/doc/results.dox 

[1.x.493] 

[1.x.494] 

[1.x.495] 

[1.x.496] 

Running the program with the space dimension set to 2 in the  [2.x.1270]  function yields the following output (in "release mode",  [2.x.1271]  

[1.x.497] 



The entire computation above takes about 2 seconds on a reasonably quick (for 2015 standards) machine. 

What we see immediately from this is that the number of (outer) iterations does not increase as we refine the mesh. This confirms the statement in the introduction that preconditioning the Schur complement with the mass matrix indeed yields a matrix spectrally equivalent to the identity matrix (i.e. with eigenvalues bounded above and below independently of the mesh size or the relative sizes of cells). In other words, the mass matrix and the Schur complement are spectrally equivalent. 

In the images below, we show the grids for the first six refinement steps in the program.  Observe how the grid is refined in regions where the solution rapidly changes: On the upper boundary, we have Dirichlet boundary conditions that are -1 in the left half of the line and 1 in the right one, so there is an abrupt change at  [2.x.1272] . Likewise, there are changes from Dirichlet to Neumann data in the two upper corners, so there is need for refinement there as well: 

 [2.x.1273]  

Finally, following is a plot of the flow field. It shows fluid transported along with the moving upper boundary and being replaced by material coming from below: 

 [2.x.1274]  

This plot uses the capability of VTK-based visualization programs (in this case of VisIt) to show vector data; this is the result of us declaring the velocity components of the finite element in use to be a set of vector components, rather than independent scalar components in the  [2.x.1275]  function of this tutorial program. 




[1.x.498] 

In 3d, the screen output of the program looks like this: 

[1.x.499] 



Again, we see that the number of outer iterations does not increase as we refine the mesh. Nevertheless, the compute time increases significantly: for each of the iterations above separately, it takes about 0.14 seconds, 0.63 seconds, 4.8 seconds, 35 seconds, 2 minutes and 33 seconds, and 13 minutes and 12 seconds. This overall superlinear (in the number of unknowns) increase in runtime is due to the fact that our inner solver is not  [2.x.1276] : a simple experiment shows that as we keep refining the mesh, the average number of ILU-preconditioned CG iterations to invert the velocity-velocity block  [2.x.1277]  increases. 

We will address the question of how possibly to improve our solver [1.x.500]. 

As for the graphical output, the grids generated during the solution look as follow: 

 [2.x.1278]  

Again, they show essentially the location of singularities introduced by boundary conditions. The vector field computed makes for an interesting graph: 

 [2.x.1279]  

The isocontours shown here as well are those of the pressure variable, showing the singularity at the point of discontinuous velocity boundary conditions. 




[1.x.501] 

As explained during the generation of the sparsity pattern, it is important to have the numbering of degrees of freedom in mind when using preconditioners like incomplete LU decompositions. This is most conveniently visualized using the distribution of nonzero elements in the stiffness matrix. 

If we don't do anything special to renumber degrees of freedom (i.e., without using  [2.x.1280]  but with using  [2.x.1281]  to ensure that degrees of freedom are appropriately sorted into their corresponding blocks of the matrix and vector), then we get the following image after the first adaptive refinement in two dimensions: 

 [2.x.1282]  

In order to generate such a graph, you have to insert a piece of code like the following to the end of the setup step. 

[1.x.502] 



It is clearly visible that the nonzero entries are spread over almost the whole matrix.  This makes preconditioning by ILU inefficient: ILU generates a Gaussian elimination (LU decomposition) without fill-in elements, which means that more tentative fill-ins left out will result in a worse approximation of the complete decomposition. 

In this program, we have thus chosen a more advanced renumbering of components.  The renumbering with  [2.x.1283]  and grouping the components into velocity and pressure yields the following output: 

 [2.x.1284]  

It is apparent that the situation has improved a lot. Most of the elements are now concentrated around the diagonal in the (0,0) block in the matrix. Similar effects are also visible for the other blocks. In this case, the ILU decomposition will be much closer to the full LU decomposition, which improves the quality of the preconditioner. (It may be interesting to note that the sparse direct solver UMFPACK does some %internal renumbering of the equations before actually generating a sparse LU decomposition; that procedure leads to a very similar pattern to the one we got from the Cuthill-McKee algorithm.) 

Finally, we want to have a closer look at a sparsity pattern in 3D. We show only the (0,0) block of the matrix, again after one adaptive refinement. Apart from the fact that the matrix size has increased, it is also visible that there are many more entries in the matrix. Moreover, even for the optimized renumbering, there will be a considerable amount of tentative fill-in elements. This illustrates why UMFPACK is not a good choice in 3D - a full decomposition needs many new entries that  eventually won't fit into the physical memory (RAM): 

 [2.x.1285]  




[1.x.503] 

[1.x.504][1.x.505] 

We have seen in the section of computational results that the number of outer iterations does not depend on the mesh size, which is optimal in a sense of scalability. This does, however, not apply to the solver as a whole, as mentioned above: We did not look at the number of inner iterations when generating the inverse of the matrix  [2.x.1286]  and the mass matrix  [2.x.1287] . Of course, this is unproblematic in the 2D case where we precondition  [2.x.1288]  with a direct solver and the  [2.x.1289]  operation of the inverse matrix structure will converge in one single CG step, but this changes in 3D where we only use an ILU preconditioner.  There, the number of required preconditioned CG steps to invert  [2.x.1290]  increases as the mesh is refined, and each  [2.x.1291]  operation involves on average approximately 14, 23, 36, 59, 75 and 101 inner CG iterations in the refinement steps shown above. (On the other hand, the number of iterations for applying the inverse pressure mass matrix is always around five, both in two and three dimensions.)  To summarize, most work is spent on solving linear systems with the same matrix  [2.x.1292]  over and over again. What makes this look even worse is the fact that we actually invert a matrix that is about 95 percent the size of the total system matrix and stands for 85 percent of the non-zero entries in the sparsity pattern. Hence, the natural question is whether it is reasonable to solve a linear system with matrix  [2.x.1293]  for about 15 times when calculating the solution to the block system. 

The answer is, of course, that we can do that in a few other (most of the time better) ways. Nevertheless, it has to be remarked that an indefinite system as the one at hand puts indeed much higher demands on the linear algebra than standard elliptic problems as we have seen in the early tutorial programs. The improvements are still rather unsatisfactory, if one compares with an elliptic problem of similar size. Either way, we will introduce below a number of improvements to the linear solver, a discussion that we will re-consider again with additional options in the step-31 program. 

[1.x.506][1.x.507] A first attempt to improve the speed of the linear solution process is to choose a dof reordering that makes the ILU being closer to a full LU decomposition, as already mentioned in the in-code comments. The DoFRenumbering namespace compares several choices for the renumbering of dofs for the Stokes equations. The best result regarding the computing time was found for the King ordering, which is accessed through the call  [2.x.1294]  With that program, the inner solver needs considerably less operations, e.g. about 62 inner CG iterations for the inversion of  [2.x.1295]  at cycle 4 compared to about 75 iterations with the standard Cuthill-McKee-algorithm. Also, the computing time at cycle 4 decreased from about 17 to 11 minutes for the  [2.x.1296]  call. However, the King ordering (and the orderings provided by the  [2.x.1297]  namespace in general) has a serious drawback - it uses much more memory than the in-build deal versions, since it acts on abstract graphs rather than the geometry provided by the triangulation. In the present case, the renumbering takes about 5 times as much memory, which yields an infeasible algorithm for the last cycle in 3D with 1.2 million unknowns. 

[1.x.508] Another idea to improve the situation even more would be to choose a preconditioner that makes CG for the (0,0) matrix  [2.x.1298]  converge in a mesh-independent number of iterations, say 10 to 30. We have seen such a candidate in step-16: multigrid. 

[1.x.509] [1.x.510] Even with a good preconditioner for  [2.x.1299] , we still need to solve of the same linear system repeatedly (with different right hand sides, though) in order to make the Schur complement solve converge. The approach we are going to discuss here is how inner iteration and outer iteration can be combined. If we persist in calculating the Schur complement, there is no other possibility. 

The alternative is to attack the block system at once and use an approximate Schur complement as efficient preconditioner. The idea is as follows: If we find a block preconditioner  [2.x.1300]  such that the matrix 

[1.x.511] 

is simple, then an iterative solver with that preconditioner will converge in a few iterations. Using the Schur complement  [2.x.1301] , one finds that 

[1.x.512] 

would appear to be a good choice since 

[1.x.513] 

This is the approach taken by the paper by Silvester and Wathen referenced to in the introduction (with the exception that Silvester and Wathen use right preconditioning). In this case, a Krylov-based iterative method would converge in one step only if exact inverses of  [2.x.1302]  and  [2.x.1303]  were applied, since all the eigenvalues are one (and the number of iterations in such a method is bounded by the number of distinct eigenvalues). Below, we will discuss the choice of an adequate solver for this problem. First, we are going to have a closer look at the implementation of the preconditioner. 

Since  [2.x.1304]  is aimed to be a preconditioner only, we shall use approximations to the inverse of the Schur complement  [2.x.1305]  and the matrix  [2.x.1306] . Hence, the Schur complement will be approximated by the pressure mass matrix  [2.x.1307] , and we use a preconditioner to  [2.x.1308]  (without an InverseMatrix class around it) for approximating  [2.x.1309] . 

Here comes the class that implements the block Schur complement preconditioner. The  [2.x.1310]  operation for block vectors according to the derivation above can be specified by three successive operations: 

[1.x.514] 



Since we act on the whole block system now, we have to live with one disadvantage: we need to perform the solver iterations on the full block system instead of the smaller pressure space. 

Now we turn to the question which solver we should use for the block system. The first observation is that the resulting preconditioned matrix cannot be solved with CG since it is neither positive definite nor symmetric. 

The deal.II libraries implement several solvers that are appropriate for the problem at hand. One choice is the solver  [2.x.1311]  "BiCGStab", which was used for the solution of the unsymmetric advection problem in step-9. The second option, the one we are going to choose, is  [2.x.1312]  "GMRES" (generalized minimum residual). Both methods have their pros and cons - there are problems where one of the two candidates clearly outperforms the other, and vice versa. [1.x.515]'s article on the GMRES method gives a comparative presentation. A more comprehensive and well-founded comparison can be read e.g. in the book by J.W. Demmel (Applied Numerical Linear Algebra, SIAM, 1997, section 6.6.6). 

For our specific problem with the ILU preconditioner for  [2.x.1313] , we certainly need to perform hundreds of iterations on the block system for large problem sizes (we won't beat CG!). Actually, this disfavors GMRES: During the GMRES iterations, a basis of Krylov vectors is successively built up and some operations are performed on these vectors. The more vectors are in this basis, the more operations and memory will be needed. The number of operations scales as  [2.x.1314]  and memory as  [2.x.1315] , where  [2.x.1316]  is the number of vectors in the Krylov basis and  [2.x.1317]  the size of the (block) matrix. To not let these demands grow excessively, deal.II limits the size  [2.x.1318]  of the basis to 30 vectors by default. Then, the basis is rebuilt. This implementation of the GMRES method is called GMRES(k), with default  [2.x.1319] . What we have gained by this restriction, namely a bound on operations and memory requirements, will be compensated by the fact that we use an incomplete basis - this will increase the number of required iterations. 

BiCGStab, on the other hand, won't get slower when many iterations are needed (one iteration uses only results from one preceding step and not all the steps as GMRES). Besides the fact the BiCGStab is more expensive per step since two matrix-vector products are needed (compared to one for CG or GMRES), there is one main reason which makes BiCGStab not appropriate for this problem: The preconditioner applies the inverse of the pressure mass matrix by using the InverseMatrix class. Since the application of the inverse matrix to a vector is done only in approximative way (an exact inverse is too expensive), this will also affect the solver. In the case of BiCGStab, the Krylov vectors will not be orthogonal due to that perturbation. While this is uncritical for a small number of steps (up to about 50), it ruins the performance of the solver when these perturbations have grown to a significant magnitude in the coarse of iterations. 

We did some experiments with BiCGStab and found it to be faster than GMRES up to refinement cycle 3 (in 3D), but it became very slow for cycles 4 and 5 (even slower than the original Schur complement), so the solver is useless in this situation. Choosing a sharper tolerance for the inverse matrix class ( [2.x.1320]  instead of  [2.x.1321] ) made BiCGStab perform well also for cycle 4, but did not change the failure on the very large problems. 

GMRES is of course also effected by the approximate inverses, but it is not as sensitive to orthogonality and retains a relatively good performance also for large sizes, see the results below. 

With this said, we turn to the realization of the solver call with GMRES with  [2.x.1322]  temporary vectors: 

[1.x.516] 



Obviously, one needs to add the include file  [2.x.1323]  "<lac/solver_gmres.h>" in order to make this run. We call the solver with a BlockVector template in order to enable GMRES to operate on block vectors and matrices. Note also that we need to set the (1,1) block in the system matrix to zero (we saved the pressure mass matrix there which is not part of the problem) after we copied the information to another matrix. 

Using the Timer class, we collect some statistics that compare the runtime of the block solver with the one from the problem implementation above. Besides the solution with the two options we also check if the solutions of the two variants are close to each other (i.e. this solver gives indeed the same solution as we had before) and calculate the infinity norm of the vector difference. 

Let's first see the results in 2D: 

[1.x.517] 



We see that there is no huge difference in the solution time between the block Schur complement preconditioner solver and the Schur complement itself. The reason is simple: we used a direct solve as preconditioner for  [2.x.1324]  - so we cannot expect any gain by avoiding the inner iterations. We see that the number of iterations has slightly increased for GMRES, but all in all the two choices are fairly similar. 

The picture of course changes in 3D: 

[1.x.518] 



Here, the block preconditioned solver is clearly superior to the Schur complement, but the advantage gets less for more mesh points. This is because GMRES(k) scales worse with the problem size than CG, as we discussed above.  Nonetheless, the improvement by a factor of 3-6 for moderate problem sizes is quite impressive. 




[1.x.519] An ultimate linear solver for this problem could be imagined as a combination of an optimal preconditioner for  [2.x.1325]  (e.g. multigrid) and the block preconditioner described above, which is the approach taken in the step-31 and step-32 tutorial programs (where we use an algebraic multigrid method) and step-56 (where we use a geometric multigrid method). 




[1.x.520] Another possibility that can be taken into account is to not set up a block system, but rather solve the system of velocity and pressure all at once. The options are direct solve with UMFPACK (2D) or GMRES with ILU preconditioning (3D). It should be straightforward to try that. 




[1.x.521] 

The program can of course also serve as a basis to compute the flow in more interesting cases. The original motivation to write this program was for it to be a starting point for some geophysical flow problems, such as the movement of magma under places where continental plates drift apart (for example mid-ocean ridges). Of course, in such places, the geometry is more complicated than the examples shown above, but it is not hard to accommodate for that. 

For example, by using the following modification of the boundary values function 

[1.x.522] 

and the following way to generate the mesh as the domain  [2.x.1326]  

[1.x.523] 

then we get images where the fault line is curved:  [2.x.1327]  


examples/step-23/doc/intro.dox 

[1.x.524] 

[1.x.525] 

 [2.x.1328]  

This is the first of a number of tutorial programs that will finally cover "real" time-dependent problems, not the slightly odd form of time dependence found in step-18 or the DAE model of step-21. In particular, this program introduces the wave equation in a bounded domain. Later, step-24 will consider an example of absorbing boundary conditions, and  [2.x.1329]  "step-25" a kind of nonlinear wave equation producing solutions called solitons. 

The wave equation in its prototypical form reads as follows: find  [2.x.1330]  that satisfies 

[1.x.526] 

Note that since this is an equation with second-order time derivatives, we need to pose two initial conditions, one for the value and one for the time derivative of the solution. 

Physically, the equation describes the motion of an elastic medium. In 2-d, one can think of how a membrane moves if subjected to a force. The Dirichlet boundary conditions above indicate that the membrane is clamped at the boundary at a height  [2.x.1331]  (this height might be moving as well &mdash; think of people holding a blanket and shaking it up and down). The first initial condition equals the initial deflection of the membrane, whereas the second one gives its velocity. For example, one could think of pushing the membrane down with a finger and then letting it go at  [2.x.1332]  (nonzero deflection but zero initial velocity), or hitting it with a hammer at  [2.x.1333]  (zero deflection but nonzero velocity). Both cases would induce motion in the membrane. 




[1.x.527] 

[1.x.528] There is a long-standing debate in the numerical analysis community over whether a discretization of time dependent equations should involve first discretizing the time variable leading to a stationary PDE at each time step that is then solved using standard finite element techniques (this is called the Rothe method), or whether one should first discretize the spatial variables, leading to a large system of ordinary differential equations that can then be handled by one of the usual ODE solvers (this is called the method of lines). 

Both of these methods have advantages and disadvantages. Traditionally, people have preferred the method of lines, since it allows to use the very well developed machinery of high-order ODE solvers available for the rather stiff ODEs resulting from this approach, including step length control and estimation of the temporal error. 

On the other hand, Rothe's method becomes awkward when using higher-order time stepping method, since one then has to write down a PDE that couples the solution of the present time step not only with that at the previous time step, but possibly also even earlier solutions, leading to a significant number of terms. 

For these reasons, the method of lines was the method of choice for a long time. However, it has one big drawback: if we discretize the spatial variable first, leading to a large ODE system, we have to choose a mesh once and for all. If we are willing to do this, then this is a legitimate and probably superior approach. 

If, on the other hand, we are looking at the wave equation and many other time dependent problems, we find that the character of a solution changes as time progresses. For example, for the wave equation, we may have a single wave travelling through the domain, where the solution is smooth or even constant in front of and behind the wave &mdash; adaptivity would be really useful for such cases, but the key is that the area where we need to refine the mesh changes from time step to time step! 

If we intend to go that way, i.e. choose a different mesh for each time step (or set of time steps), then the method of lines is not appropriate any more: instead of getting one ODE system with a number of variables equal to the number of unknowns in the finite element mesh, our number of unknowns now changes all the time, a fact that standard ODE solvers are certainly not prepared to deal with at all. On the other hand, for the Rothe method, we just get a PDE for each time step that we may choose to discretize independently of the mesh used for the previous time step; this approach is not without perils and difficulties, but at least is a sensible and well-defined procedure. 

For all these reasons, for the present program, we choose to use the Rothe method for discretization, i.e. we first discretize in time and then in space. We will not actually use adaptive meshes at all, since this involves a large amount of additional code, but we will comment on this some more in the [1.x.529]. 




[1.x.530] 

Given these considerations, here is how we will proceed: let us first define a simple time stepping method for this second order problem, and then in a second step do the spatial discretization, i.e. we will follow Rothe's approach. 

For the first step, let us take a little detour first: in order to discretize a second time derivative, we can either discretize it directly, or we can introduce an additional variable and transform the system into a first order system. In many cases, this turns out to be equivalent, but dealing with first order systems is often simpler. To this end, let us introduce 

[1.x.531] 

and call this variable the [1.x.532] for obvious reasons. We can then reformulate the original wave equation as follows: 

[1.x.533] 

The advantage of this formulation is that it now only contains first time derivatives for both variables, for which it is simple to write down time stepping schemes. Note that we do not have boundary conditions for  [2.x.1334]  at first. However, we could enforce  [2.x.1335]  on the boundary. It turns out in numerical examples that this is actually necessary: without doing so the solution doesn't look particularly wrong, but the Crank-Nicolson scheme does not conserve energy if one doesn't enforce these boundary conditions. 

With this formulation, let us introduce the following time discretization where a superscript  [2.x.1336]  indicates the number of a time step and  [2.x.1337]  is the length of the present time step: 

[1.x.534] Note how we introduced a parameter  [2.x.1338]  here. If we chose  [2.x.1339] , for example, the first equation would reduce to  [2.x.1340] , which is well-known as the forward or explicit Euler method. On the other hand, if we set  [2.x.1341] , then we would get  [2.x.1342] , which corresponds to the backward or implicit Euler method. Both these methods are first order accurate methods. They are simple to implement, but they are not really very accurate. 

The third case would be to choose  [2.x.1343] . The first of the equations above would then read  [2.x.1344] . This method is known as the Crank-Nicolson method and has the advantage that it is second order accurate. In addition, it has the nice property that it preserves the energy in the solution (physically, the energy is the sum of the kinetic energy of the particles in the membrane plus the potential energy present due to the fact that it is locally stretched; this quantity is a conserved one in the continuous equation, but most time stepping schemes do not conserve it after time discretization). Since  [2.x.1345]  also appears in the equation for  [2.x.1346] , the Crank-Nicolson scheme is also implicit. 

In the program, we will leave  [2.x.1347]  as a parameter, so that it will be easy to play with it. The results section will show some numerical evidence comparing the different schemes. 

The equations above (called the [1.x.535] equations because we have only discretized the time, but not space), can be simplified a bit by eliminating  [2.x.1348]  from the first equation and rearranging terms. We then get 

[1.x.536] In this form, we see that if we are given the solution  [2.x.1349]  of the previous timestep, that we can then solve for the variables  [2.x.1350]  separately, i.e. one at a time. This is convenient. In addition, we recognize that the operator in the first equation is positive definite, and the second equation looks particularly simple. 




[1.x.537] 

We have now derived equations that relate the approximate (semi-discrete) solution  [2.x.1351]  and its time derivative  [2.x.1352]  at time  [2.x.1353]  with the solutions  [2.x.1354]  of the previous time step at  [2.x.1355] . The next step is to also discretize the spatial variable using the usual finite element methodology. To this end, we multiply each equation with a test function, integrate over the entire domain, and integrate by parts where necessary. This leads to 

[1.x.538] 

It is then customary to approximate  [2.x.1356] , where  [2.x.1357]  are the shape functions used for the discretization of the  [2.x.1358] -th time step and  [2.x.1359]  are the unknown nodal values of the solution. Similarly,  [2.x.1360] . Finally, we have the solutions of the previous time step,  [2.x.1361]  and  [2.x.1362] . Note that since the solution of the previous time step has already been computed by the time we get to time step  [2.x.1363] ,  [2.x.1364]  are known. Furthermore, note that the solutions of the previous step may have been computed on a different mesh, so we have to use shape functions  [2.x.1365] . 

If we plug these expansions into above equations and test with the test functions from the present mesh, we get the following linear system: 

[1.x.539] where 

[1.x.540] 



If we solve these two equations, we can move the solution one step forward and go on to the next time step. 

It is worth noting that if we choose the same mesh on each time step (as we will in fact do in the program below), then we have the same shape functions on time step  [2.x.1366]  and  [2.x.1367] , i.e.  [2.x.1368] . Consequently, we get  [2.x.1369]  and  [2.x.1370] . On the other hand, if we had used different shape functions, then we would have to compute integrals that contain shape functions defined on two meshes. This is a somewhat messy process that we omit here, but that is treated in some detail in step-28. 

Under these conditions (i.e. a mesh that doesn't change), one can optimize the solution procedure a bit by basically eliminating the solution of the second linear system. We will discuss this in the introduction of the  [2.x.1371]  "step-25" program. 

[1.x.541] 

One way to compare the quality of a time stepping scheme is to see whether the numerical approximation preserves conservation properties of the continuous equation. For the wave equation, the natural quantity to look at is the energy. By multiplying the wave equation by  [2.x.1372] , integrating over  [2.x.1373] , and integrating by parts where necessary, we find that 

[1.x.542] 

By consequence, in absence of body forces and constant boundary values, we get that 

[1.x.543] 

is a conserved quantity, i.e. one that doesn't change with time. We will compute this quantity after each time step. It is straightforward to see that if we replace  [2.x.1374]  by its finite element approximation, and  [2.x.1375]  by the finite element approximation of the velocity  [2.x.1376] , then 

[1.x.544] 

As we will see in the results section, the Crank-Nicolson scheme does indeed conserve the energy, whereas neither the forward nor the backward Euler scheme do. 




[1.x.545] 

One of the reasons why the wave equation is nasty to solve numerically is that explicit time discretizations are only stable if the time step is small enough. In particular, it is coupled to the spatial mesh width  [2.x.1377] . For the lowest order discretization we use here, the relationship reads 

[1.x.546] 

where  [2.x.1378]  is the wave speed, which in our formulation of the wave equation has been normalized to one. Consequently, unless we use the implicit schemes with  [2.x.1379] , our solutions will not be numerically stable if we violate this restriction. Implicit schemes do not have this restriction for stability, but they become inaccurate if the time step is too large. 

This condition was first recognized by Courant, Friedrichs, and Lewy &mdash; in 1928, long before computers became available for numerical computations! (This result appeared in the German language article R. Courant, K. Friedrichs and H. Lewy: [1.x.547], Mathematische Annalen, vol. 100, no. 1, pages 32-74, 1928.) This condition on the time step is most frequently just referred to as the [1.x.548] condition. Intuitively, the CFL condition says that the time step must not be larger than the time it takes a wave to cross a single cell. 

In the program, we will refine the square  [2.x.1380]  seven times uniformly, giving a mesh size of  [2.x.1381] , which is what we set the time step to. The fact that we set the time step and mesh size individually in two different places is error prone: it is too easy to refine the mesh once more but forget to also adjust the time step.  [2.x.1382]  "step-24" shows a better way how to keep these things in sync. 




[1.x.549] 

Although the program has all the hooks to deal with nonzero initial and boundary conditions and body forces, we take a simple case where the domain is a square  [2.x.1383]  and 

[1.x.550] 

This corresponds to a membrane initially at rest and clamped all around, where someone is waving a part of the clamped boundary once up and down, thereby shooting a wave into the domain. 


examples/step-23/doc/results.dox 



[1.x.551] 

When the program is run, it produces the following output: 

[1.x.552] 



What we see immediately is that the energy is a constant at least after  [2.x.1384]  (until which the boundary source term  [2.x.1385]  is nonzero, injecting energy into the system). 

In addition to the screen output, the program writes the solution of each time step to an output file. If we process them adequately and paste them into a movie, we get the following: 

 [2.x.1386]  

The movie shows the generated wave nice traveling through the domain and back, being reflected at the clamped boundary. Some numerical noise is trailing the wave, an artifact of a too-large mesh size that can be reduced by reducing the mesh width and the time step. 


[1.x.553] 

[1.x.554] 

If you want to explore a bit, try out some of the following things:  [2.x.1387]     [2.x.1388] Varying  [2.x.1389] . This gives different time stepping schemes, some of   which are stable while others are not. Take a look at how the energy   evolves. 

   [2.x.1390] Different initial and boundary conditions, right hand sides. 

   [2.x.1391] More complicated domains or more refined meshes. Remember that the time   step needs to be bounded by the mesh width, so changing the mesh should   always involve also changing the time step. We will come back to this issue   in step-24. 

   [2.x.1392] Variable coefficients: In real media, the wave speed is often   variable. In particular, the "real" wave equation in realistic media would   read   [1.x.555] 

  where  [2.x.1393]  is the density of the material, and  [2.x.1394]  is related to the   stiffness coefficient. The wave speed is then  [2.x.1395] . 

  To make such a change, we would have to compute the mass and Laplace   matrices with a variable coefficient. Fortunately, this isn't too hard: the   functions  [2.x.1396]  and    [2.x.1397]  have additional default parameters that can   be used to pass non-constant coefficient functions to them. The required   changes are therefore relatively small. On the other hand, care must be   taken again to make sure the time step is within the allowed range. 

   [2.x.1398] In the in-code comments, we discussed the fact that the matrices for   solving for  [2.x.1399]  and  [2.x.1400]  need to be reset in every time because of   boundary conditions, even though the actual content does not change. It is   possible to avoid copying by not eliminating columns in the linear systems,   which is implemented by appending a  [2.x.1401]  argument to the call:   [1.x.556] 



   [2.x.1402] deal.II being a library that supports adaptive meshes it would of course be   nice if this program supported change the mesh every few time steps. Given the   structure of the solution &mdash; a wave that travels through the domain &mdash;   it would seem appropriate if we only refined the mesh where the wave currently is,   and not simply everywhere. It is intuitively clear that we should be able to   save a significant amount of cells this way. (Though upon further thought one   realizes that this is really only the case in the initial stages of the simulation.   After some time, for wave phenomena, the domain is filled with reflections of   the initial wave going in every direction and filling every corner of the domain.   At this point, there is in general little one can gain using local mesh   refinement.) 

  To make adaptively changing meshes possible, there are basically two routes.   The "correct" way would be to go back to the weak form we get using Rothe's   method. For example, the first of the two equations to be solved in each time   step looked like this:   [1.x.557]   Now, note that we solve for  [2.x.1403]  on mesh  [2.x.1404] , and   consequently the test functions  [2.x.1405]  have to be from the space    [2.x.1406]  as well. As discussed in the introduction, terms like    [2.x.1407]  then require us to integrate the solution of the   previous step (which may have been computed on a different mesh    [2.x.1408] ) against the test functions of the current mesh,   leading to a matrix  [2.x.1409] . This process of integrating shape   functions from different meshes is, at best, awkward. It can be done   but because it is difficult to ensure that  [2.x.1410]  and    [2.x.1411]  differ by at most one level of refinement, one   has to recursively match cells from both meshes. It is feasible to   do this, but it leads to lengthy and not entirely obvious code. 

  The second approach is the following: whenever we change the mesh,   we simply interpolate the solution from the last time step on the old   mesh to the new mesh, using the SolutionTransfer class. In other words,   instead of the equation above, we would solve   [1.x.558]   where  [2.x.1412]  interpolates a given function onto mesh  [2.x.1413] .   This is a much simpler approach because, in each time step, we no   longer have to worry whether  [2.x.1414]  were computed on the   same mesh as we are using now or on a different mesh. Consequently,   the only changes to the code necessary are the addition of a function   that computes the error, marks cells for refinement, sets up a   SolutionTransfer object, transfers the solution to the new mesh, and   rebuilds matrices and right hand side vectors on the new mesh. Neither   the functions building the matrices and right hand sides, nor the   solvers need to be changed. 

  While this second approach is, strictly speaking,   not quite correct in the Rothe framework (it introduces an addition source   of error, namely the interpolation), it is nevertheless what   almost everyone solving time dependent equations does. We will use this   method in step-31, for example.  [2.x.1415]  


examples/step-24/doc/intro.dox 

[1.x.559] 

[1.x.560] 

This program grew out of a student project by Xing Jin at Texas A&amp;M University. Most of the work for this program is by her. Some of the work on this tutorial program has been funded by NSF under grant DMS-0604778. 

The program is part of a project that aims to simulate thermoacoustic tomography imaging. In thermoacoustic tomography, pulsed electromagnetic energy is delivered into biological issues. Tissues absorb some of this energy and those parts of the tissue that absorb the most energy generate thermoacoustic waves through thermoelastic expansion. For imaging, one uses that different kinds of tissue, most importantly healthy and diseased tissue, absorb different amounts of energy and therefore expand at different rates. The experimental setup is to measure the amplitude of the pressure waves generated by these sources on the surface of the tissue and try to reconstruct the source distributions, which is indicative for the distribution of absorbers and therefore of different kinds of tissue. Part of this project is to compare simulated data with actual measurements, so one has to solve the "forward problem", i.e. the wave equation that describes the propagation of pressure waves in tissue. This program is therefore a continuation of  [2.x.1416]  "step-23", where the wave equation was first introduced. 




[1.x.561] 

The temperature at a given location, neglecting thermal diffusion, can be stated as 

[1.x.562] 



Here  [2.x.1417]  is the density;  [2.x.1418]  is the specific heat;  [2.x.1419]  is the temperature rise due to the delivered microwave energy; and  [2.x.1420]  is the heating function defined as the thermal energy per time and volume transformed from deposited microwave energy. 

Let us assume that tissues have heterogeneous dielectric properties but homogeneous acoustic properties. The basic acoustic generation equation in an acoustically homogeneous medium can be described as follows: if  [2.x.1421]  is the vector-valued displacement, then tissue certainly reacts to changes in pressure by acceleration: 

[1.x.563] 

Furthermore, it contracts due to excess pressure and expands based on changes in temperature: 

[1.x.564] 

Here,  [2.x.1422]  is a thermoexpansion coefficient. 

Let us now make the assumption that heating only happens on a time scale much shorter than wave propagation through tissue (i.e. the temporal length of the microwave pulse that heats the tissue is much shorter than the time it takes a wave to cross the domain). In that case, the heating rate  [2.x.1423]  can be written as  [2.x.1424]  (where  [2.x.1425]  is a map of absorption strengths for microwave energy and  [2.x.1426]  is the Dirac delta function), which together with the first equation above will yield an instantaneous jump in the temperature  [2.x.1427]  at time  [2.x.1428] . Using this assumption, and taking all equations together, we can rewrite and combine the above as follows: 

[1.x.565] 

where  [2.x.1429] . 

This somewhat strange equation with the derivative of a Dirac delta function on the right hand side can be rewritten as an initial value problem as follows: 

[1.x.566] 

(A derivation of this transformation into an initial value problem is given at the end of this introduction as an appendix.) 

In the inverse problem, it is the initial condition  [2.x.1430]  that one would like to recover, since it is a map of absorption strengths for microwave energy, and therefore presumably an indicator to discern healthy from diseased tissue. 

In real application, the thermoacoustic source is very small as compared to the medium.  The propagation path of the thermoacoustic waves can then be approximated as from the source to the infinity. Furthermore, detectors are only a limited distance from the source. One only needs to evaluate the values when the thermoacoustic waves pass through the detectors, although they do continue beyond. This is therefore a problem where we are only interested in a small part of an infinite medium, and we do not want waves generated somewhere to be reflected at the boundary of the domain which we consider interesting. Rather, we would like to simulate only that part of the wave field that is contained inside the domain of interest, and waves that hit the boundary of that domain to simply pass undisturbed through the boundary. In other words, we would like the boundary to absorb any waves that hit it. 

In general, this is a hard problem: Good absorbing boundary conditions are nonlinear and/or numerically very expensive. We therefore opt for a simple first order approximation to absorbing boundary conditions that reads 

[1.x.567] 

Here,  [2.x.1431]  is the normal derivative at the boundary. It should be noted that this is not a particularly good boundary condition, but it is one of the very few that are reasonably simple to implement. 




[1.x.568] 

As in step-23, one first introduces a second variable, which is defined as the derivative of the pressure potential: 

[1.x.569] 



With the second variable, one then transforms the forward problem into two separate equations: 

[1.x.570] 

with initial conditions: 

[1.x.571] 

Note that we have introduced a right hand side  [2.x.1432]  here to show how to derive these formulas in the general case, although in the application to the thermoacoustic problem  [2.x.1433] . 

The semi-discretized, weak version of this model, using the general  [2.x.1434]  scheme introduced in step-23 is then: 

[1.x.572] 

where  [2.x.1435]  is an arbitrary test function, and where we have used the absorbing boundary condition to integrate by parts: absorbing boundary conditions are incorporated into the weak form by using 

[1.x.573] 



From this we obtain the discrete model by introducing a finite number of shape functions, and get 

[1.x.574] 

The matrices  [2.x.1436]  and  [2.x.1437]  are here as in step-23, and the boundary mass matrix 

[1.x.575] 

results from the use of absorbing boundary conditions. 

Above two equations can be rewritten in a matrix form with the pressure and its derivative as an unknown vector: 

[1.x.576] 



where 

[1.x.577] 



By simple transformations, one then obtains two equations for the pressure potential and its derivative, just as in the previous tutorial program: 

[1.x.578] 






[1.x.579] 

Compared to step-23, this programs adds the treatment of a simple absorbing boundary conditions. In addition, it deals with data obtained from actual experimental measurements. To this end, we need to evaluate the solution at points at which the experiment also evaluates a real pressure field. We will see how to do that using the  [2.x.1438]  function further down below. 




[1.x.580] 

In the derivation of the initial value problem for the wave equation, we initially found that the equation had the derivative of a Dirac delta function as a right hand side: 

[1.x.581] 

In order to see how to transform this single equation into the usual statement of a PDE with initial conditions, let us make the assumption that the physically quite reasonable medium is at rest initially, i.e.  [2.x.1439]  for  [2.x.1440] . Next, let us form the indefinite integral with respect to time of both sides: 

[1.x.582] 

This immediately leads to the statement 

[1.x.583] 

where  [2.x.1441]  is such that  [2.x.1442] . Next, we form the (definite) integral over time from  [2.x.1443]  to  [2.x.1444]  to find 

[1.x.584] 

If we use the property of the delta function that  [2.x.1445] , and assume that  [2.x.1446]  is a continuous function in time, we find as we let  [2.x.1447]  go to zero that 

[1.x.585] 

In other words, using that  [2.x.1448] , we retrieve the initial condition 

[1.x.586] 

At the same time, we know that for every  [2.x.1449]  the delta function is zero, so for  [2.x.1450]  we get the equation 

[1.x.587] 

Consequently, we have obtained a representation of the wave equation and one initial condition from the original somewhat strange equation. 

Finally, because we here have an equation with two time derivatives, we still need a second initial condition. To this end, let us go back to the equation 

[1.x.588] 

and integrate it in time from  [2.x.1451]  to  [2.x.1452] . This leads to 

[1.x.589] 

Using integration by parts of the form 

[1.x.590] 

where we use that  [2.x.1453]  and inserting  [2.x.1454] , we see that in fact 

[1.x.591] 



Now, let  [2.x.1455] . Assuming that  [2.x.1456]  is a continuous function in time, we see that 

[1.x.592] 

and consequently 

[1.x.593] 

However, we have assumed that  [2.x.1457] . Consequently, we obtain as the second initial condition that 

[1.x.594] 

completing the system of equations. 


examples/step-24/doc/results.dox 



[1.x.595] 

The program writes both graphical data for each time step as well as the values evaluated at each detector location to disk. We then draw them in plots. Experimental data were also collected for comparison. Currently our experiments have only been done in two dimensions by circularly scanning a single detector. The tissue sample here is a thin slice in the  [2.x.1458]  plane ( [2.x.1459] ), and we assume that signals from other  [2.x.1460]  directions won't contribute to the data. Consequently, we only have to compare our experimental data with two dimensional simulated data. 

[1.x.596] 

This movie shows the thermoacoustic waves generated by a single small absorber propagating in the medium (in our simulation, we assume the medium is mineral oil, which has a acoustic speed of 1.437  [2.x.1461] ): 

 [2.x.1462]  

For a single absorber, we of course have to change the  [2.x.1463]  class accordingly. 

Next, let us compare experimental and computational results. The visualization uses a technique long used in seismology, where the data of each detector is plotted all in one graph. The way this is done is by offsetting each detector's signal a bit compared to the previous one. For example, here is a plot of the first four detectors (from bottom to top, with time in microseconds running from left to right) using the source setup used in the program, to make things a bit more interesting compared to the present case of only a single source: 

 [2.x.1464]  

One thing that can be seen, for example, is that the arrival of the second and fourth signals shifts to earlier times for greater detector numbers (i.e. the topmost ones), but not the first and the third; this can be interpreted to mean that the origin of these signals must be closer to the latter detectors than to the former ones. 

If we stack not only 4, but all 160 detectors in one graph, the individual lines blur, but where they run together they create a pattern of darker or lighter grayscales.  The following two figures show the results obtained at the detector locations stacked in that way. The left figure is obtained from experiments, and the right is the simulated data. In the experiment, a single small strong absorber was embedded in weaker absorbing tissue: 

 [2.x.1465]  

It is obvious that the source location is closer to the detectors at angle  [2.x.1466] . All the other signals that can be seen in the experimental data result from the fact that there are weak absorbers also in the rest of the tissue, which surrounds the signals generated by the small strong absorber in the center. On the other hand, in the simulated data, we only simulate the small strong absorber. 

In reality, detectors have limited bandwidth. The thermoacoustic waves passing through the detector will therefore be filtered. By using a high-pass filter (implemented in MATLAB and run against the data file produced by this program), the simulated results can be made to look closer to the experimental data: 

 [2.x.1467]  

In our simulations, we see spurious signals behind the main wave that result from numerical artifacts. This problem can be alleviated by using finer mesh, resulting in the following plot: 

 [2.x.1468]  




[1.x.597] 

To further verify the program, we will also show simulation results for multiple absorbers. This corresponds to the case that is actually implemented in the program. The following movie shows the propagation of the generated thermoacoustic waves in the medium by multiple absorbers: 

 [2.x.1469]  

Experimental data and our simulated data are compared in the following two figures:  [2.x.1470]  

Note that in the experimental data, the first signal (i.e. the left-most dark line) results from absorption at the tissue boundary, and therefore reaches the detectors first and before any of the signals from the interior. This signal is also faintly visible at the end of the traces, around 30  [2.x.1471] , which indicates that the signal traveled through the entire tissue to reach detectors at the other side, after all the signals originating from the interior have reached them. 

As before, the numerical result better matches experimental ones by applying a bandwidth filter that matches the actual behavior of detectors (left) and by choosing a finer mesh (right): 

 [2.x.1472]  

One of the important differences between the left and the right figure is that the curves look much less "angular" at the right. The angularity comes from the fact that while waves in the continuous equation travel equally fast in all directions, this isn't the case after discretization: there, waves that travel diagonal to cells move at slightly different speeds to those that move parallel to mesh lines. This anisotropy leads to wave fronts that aren't perfectly circular (and would produce sinusoidal signals in the stacked plots), but are bulged out in certain directions. To make things worse, the circular mesh we use (see for example step-6 for a view of the coarse mesh) is not isotropic either. The net result is that the signal fronts are not sinusoidal unless the mesh is sufficiently fine. The right image is a lot better in this respect, though artifacts in the form of trailing spurious waves can still be seen. 


examples/step-25/doc/intro.dox 

[1.x.598] [1.x.599] 

This program grew out of a student project by Ivan Christov at Texas A&amp;M University. Most of the work for this program is by him. 

The goal of this program is to solve the sine-Gordon soliton equation in 1, 2 or 3 spatial dimensions. The motivation for solving this equation is that very little is known about the nature of the solutions in 2D and 3D, even though the 1D case has been studied extensively. 

Rather facetiously, the sine-Gordon equation's moniker is a pun on the so-called Klein-Gordon equation, which is a relativistic version of the Schrödinger equation for particles with non-zero mass. The resemblance is not just superficial, the sine-Gordon equation has been shown to model some unified-field phenomena such as interaction of subatomic particles (see, e.g., Perring &amp; Skyrme in Nuclear %Physics [1.x.600]) and the Josephson (quantum) effect in superconductor junctions (see, e.g., [1.x.601]). Furthermore, from the mathematical standpoint, since the sine-Gordon equation is "completely integrable," it is a candidate for study using the usual methods such as the inverse scattering transform. Consequently, over the years, many interesting solitary-wave, and even stationary, solutions to the sine-Gordon equation have been found. In these solutions, particles correspond to localized features. For more on the sine-Gordon equation, the inverse scattering transform and other methods for finding analytical soliton equations, the reader should consult the following "classical" references on the subject: G. L. Lamb's [1.x.602] (Chapter 5, Section 2) and G. B. Whitham's [1.x.603] (Chapter 17, Sections 10-13). 

 [2.x.1473]  We will cover a separate nonlinear equation from quantum   mechanics, the Nonlinear Schr&ouml;dinger Equation, in step-58. 

[1.x.604] The sine-Gordon initial-boundary-value problem (IBVP) we wish to solve consists of the following equations: 

[1.x.605] It is a nonlinear equation similar to the wave equation we discussed in step-23 and step-24. We have chosen to enforce zero Neumann boundary conditions in order for waves to reflect off the boundaries of our domain. It should be noted, however, that Dirichlet boundary conditions are not appropriate for this problem. Even though the solutions to the sine-Gordon equation are localized, it only makes sense to specify (Dirichlet) boundary conditions at  [2.x.1474] , otherwise either a solution does not exist or only the trivial solution  [2.x.1475]  exists. 

However, the form of the equation above is not ideal for numerical discretization. If we were to discretize the second-order time derivative directly and accurately, then  we would need a large stencil (i.e., several time steps would need to be kept in the memory), which could become expensive. Therefore, in complete analogy to what we did in step-23 and step-24, we split the second-order (in time) sine-Gordon equation into a system of two first-order (in time) equations, which we call the split, or velocity, formulation. To this end, by setting  [2.x.1476] , it is easy to see that the sine-Gordon equation is equivalent to 

[1.x.606] 

[1.x.607] Now, we can discretize the split formulation in time using the  [2.x.1477] -method, which has a stencil of only two time steps. By choosing a  [2.x.1478] , the latter discretization allows us to choose from a continuum of schemes. In particular, if we pick  [2.x.1479]  or  [2.x.1480] , we obtain the first-order accurate explicit or implicit Euler method, respectively. Another important choice is  [2.x.1481] , which gives the second-order accurate Crank-Nicolson scheme. Henceforth, a superscript  [2.x.1482]  denotes the values of the variables at the  [2.x.1483]  time step, i.e. at  [2.x.1484] , where  [2.x.1485]  is the (fixed) time step size. Thus, the split formulation of the time-discretized sine-Gordon equation becomes 

[1.x.608] 

We can simplify the latter via a bit of algebra. Eliminating  [2.x.1486]  from the first equation and rearranging, we obtain 

[1.x.609] 

It may seem as though we can just proceed to discretize the equations in space at this point. While this is true for the second equation (which is linear in  [2.x.1487] ), this would not work for all  [2.x.1488]  since the first equation above is nonlinear. Therefore, a nonlinear solver must be implemented, then the equations can be discretized in space and solved. 

To this end, we can use Newton's method. Given the nonlinear equation  [2.x.1489] , we produce successive approximations to  [2.x.1490]  as follows: 

[1.x.610] The iteration can be initialized with the old time step, i.e.  [2.x.1491] , and eventually it will produce a solution to the first equation of the split formulation (see above). For the time discretization of the sine-Gordon equation under consideration here, we have that 

[1.x.611] Notice that while  [2.x.1492]  is a function,  [2.x.1493]  is an operator. 

[1.x.612] With hindsight, we choose both the solution and the test space to be  [2.x.1494] . Hence, multiplying by a test function  [2.x.1495]  and integrating, we obtain the following variational (or weak) formulation of the split formulation (including the nonlinear solver for the first equation) at each time step: 

[1.x.613] Note that the we have used integration by parts and the zero Neumann boundary conditions on all terms involving the Laplacian operator. Moreover,  [2.x.1496]  and  [2.x.1497]  are as defined above, and  [2.x.1498]  denotes the usual  [2.x.1499]  inner product over the domain  [2.x.1500] , i.e.  [2.x.1501] . Finally, notice that the first equation is, in fact, the definition of an iterative procedure, so it is solved multiple times during each time step until a stopping criterion is met. 

[1.x.614] Using the Finite Element Method, we discretize the variational formulation in space. To this end, let  [2.x.1502]  be a finite-dimensional  [2.x.1503] -conforming finite element space ( [2.x.1504] ) with nodal basis  [2.x.1505] . Now, we can expand all functions in the weak formulation (see above) in terms of the nodal basis. Henceforth, we shall denote by a capital letter the vector of coefficients (in the nodal basis) of a function denoted by the same letter in lower case; e.g.,  [2.x.1506]  where  [2.x.1507]  and  [2.x.1508] . Thus, the finite-dimensional version of the variational formulation requires that we solve the following matrix equations at each time step: 

[1.x.615] 

Above, the matrix  [2.x.1509]  and the vector  [2.x.1510]  denote the discrete versions of the gadgets discussed above, i.e., 

[1.x.616] Again, note that the first matrix equation above is, in fact, the definition of an iterative procedure, so it is solved multiple times until a stopping criterion is met. Moreover,  [2.x.1511]  is the mass matrix, i.e.  [2.x.1512] ,  [2.x.1513]  is the Laplace matrix, i.e.  [2.x.1514] ,  [2.x.1515]  is the nonlinear term in the equation that defines our auxiliary velocity variable, i.e.  [2.x.1516] , and  [2.x.1517]  is the nonlinear term in the Jacobian matrix of  [2.x.1518] , i.e.  [2.x.1519] . 

What solvers can we use for the first equation? Let's look at the matrix we have to invert: 

[1.x.617] 

for some  [2.x.1520]  that depends on the present and previous solution. First, note that the matrix is symmetric. In addition, if the time step  [2.x.1521]  is small enough, i.e. if  [2.x.1522] , then the matrix is also going to be positive definite. In the program below, this will always be the case, so we will use the Conjugate Gradient method together with the SSOR method as preconditioner. We should keep in mind, however, that this will fail if we happen to use a bigger time step. Fortunately, in that case the solver will just throw an exception indicating a failure to converge, rather than silently producing a wrong result. If that happens, then we can simply replace the CG method by something that can handle indefinite symmetric systems. The GMRES solver is typically the standard method for all "bad" linear systems, but it is also a slow one. Possibly better would be a solver that utilizes the symmetry, such as, for example, SymmLQ, which is also implemented in deal.II. 

This program uses a clever optimization over step-23 and  [2.x.1523]  "step-24": If you read the above formulas closely, it becomes clear that the velocity  [2.x.1524]  only ever appears in products with the mass matrix. In step-23 and step-24, we were, therefore, a bit wasteful: in each time step, we would solve a linear system with the mass matrix, only to multiply the solution of that system by  [2.x.1525]  again in the next time step. This can, of course, be avoided, and we do so in this program. 




[1.x.618] 

There are a few analytical solutions for the sine-Gordon equation, both in 1D and 2D. In particular, the program as is computes the solution to a problem with a single kink-like solitary wave initial condition.  This solution is given by Leibbrandt in \e Phys. \e Rev. \e Lett. \b 41(7), and is implemented in the  [2.x.1526]  class. 

It should be noted that this closed-form solution, strictly speaking, only holds for the infinite-space initial-value problem (not the Neumann initial-boundary-value problem under consideration here). However, given that we impose \e zero Neumann boundary conditions, we expect that the solution to our initial-boundary-value problem would be close to the solution of the infinite-space initial-value problem, if reflections of waves off the boundaries of our domain do \e not occur. In practice, this is of course not the case, but we can at least assume that this were so. 

The constants  [2.x.1527]  and  [2.x.1528]  in the 2D solution and  [2.x.1529] ,  [2.x.1530]  and  [2.x.1531]  in the 3D solution are called the B&auml;cklund transformation parameters. They control such things as the orientation and steepness of the kink. For the purposes of testing the code against the exact solution, one should choose the parameters so that the kink is aligned with the grid. 

The solutions that we implement in the  [2.x.1532]  class are these:  [2.x.1533]     [2.x.1534] In 1D:   [1.x.619] 

  where we choose  [2.x.1535] . 

  In 1D, more interesting analytical solutions are known. Many of them are   listed on http://mathworld.wolfram.com/Sine-GordonEquation.html . 

   [2.x.1536] In 2D:   [1.x.620] 

  where  [2.x.1537]  is defined as   [1.x.621] 

  and where we choose  [2.x.1538] . 

   [2.x.1539] In 3D:   [1.x.622] 

  where  [2.x.1540]  is defined as   [1.x.623] 

  and where we choose  [2.x.1541] .  [2.x.1542]  


Since it makes it easier to play around, the  [2.x.1543]  class that is used to set &mdash; surprise! &mdash; the initial values of our simulation simply queries the class that describes the exact solution for the value at the initial time, rather than duplicating the effort to implement a solution function. 


examples/step-25/doc/results.dox 



[1.x.624] The explicit Euler time stepping scheme  ( [2.x.1544] ) performs adequately for the problems we wish to solve. Unfortunately, a rather small time step has to be chosen due to stability issues ---  [2.x.1545]  appears to work for most the simulations we performed. On the other hand, the Crank-Nicolson scheme ( [2.x.1546] ) is unconditionally stable, and (at least for the case of the 1D breather) we can pick the time step to be as large as  [2.x.1547]  without any ill effects on the solution. The implicit Euler scheme ( [2.x.1548] ) is "exponentially damped," so it is not a good choice for solving the sine-Gordon equation, which is conservative. However, some of the damped schemes in the continuum that is offered by the  [2.x.1549] -method were useful for eliminating spurious oscillations due to boundary effects. 

In the simulations below, we solve the sine-Gordon equation on the interval  [2.x.1550]  in 1D and on the square  [2.x.1551]  in 2D. In each case, the respective grid is refined uniformly 6 times, i.e.  [2.x.1552] . 

[1.x.625] The first example we discuss is the so-called 1D (stationary) breather solution of the sine-Gordon equation. The breather has the following closed-form expression, as mentioned in the Introduction: 

[1.x.626] where  [2.x.1553] ,  [2.x.1554]  and  [2.x.1555]  are constants. In the simulation below, we have chosen  [2.x.1556] ,  [2.x.1557] ,  [2.x.1558] . Moreover, it is know that the period of oscillation of the breather is  [2.x.1559] , hence we have chosen  [2.x.1560]  and  [2.x.1561]  so that we can observe three oscillations of the solution. Then, taking  [2.x.1562] ,  [2.x.1563]  and  [2.x.1564] , the program computed the following solution. 

 [2.x.1565]  

Though not shown how to do this in the program, another way to visualize the (1+1)-d solution is to use output generated by the DataOutStack class; it allows to "stack" the solutions of individual time steps, so that we get 2D space-time graphs from 1D time-dependent solutions. This produces the space-time plot below instead of the animation above. 

 [2.x.1566]  

Furthermore, since the breather is an analytical solution of the sine-Gordon equation, we can use it to validate our code, although we have to assume that the error introduced by our choice of Neumann boundary conditions is small compared to the numerical error. Under this assumption, one could use the  [2.x.1567]  function to compute the difference between the numerical solution and the function described by the  [2.x.1568]  class of this program. For the simulation shown in the two images above, the  [2.x.1569]  norm of the error in the finite element solution at each time step remained on the order of  [2.x.1570] . Hence, we can conclude that the numerical method has been implemented correctly in the program. 




[1.x.627] 

The only analytical solution to the sine-Gordon equation in (2+1)D that can be found in the literature is the so-called kink solitary wave. It has the following closed-form expression:   [1.x.628] 

with   [1.x.629] 

where  [2.x.1571] ,  [2.x.1572]  and  [2.x.1573]  are constants. In the simulation below we have chosen  [2.x.1574] . Notice that if  [2.x.1575]  the kink is stationary, hence it would make a good solution against which we can validate the program in 2D because no reflections off the boundary of the domain occur. 

The simulation shown below was performed with  [2.x.1576] ,  [2.x.1577] ,  [2.x.1578] ,  [2.x.1579]  and  [2.x.1580] . The  [2.x.1581]  norm of the error of the finite element solution at each time step remained on the order of  [2.x.1582] , showing that the program is working correctly in 2D, as well as 1D. Unfortunately, the solution is not very interesting, nonetheless we have included a snapshot of it below for completeness. 

 [2.x.1583]  

Now that we have validated the code in 1D and 2D, we move to a problem where the analytical solution is unknown. 

To this end, we rotate the kink solution discussed above about the  [2.x.1584]  axis: we let   [2.x.1585] . The latter results in a solitary wave that is not aligned with the grid, so reflections occur at the boundaries of the domain immediately. For the simulation shown below, we have taken  [2.x.1586] ,  [2.x.1587] ,  [2.x.1588] ,  [2.x.1589]  and  [2.x.1590] . Moreover, we had to pick  [2.x.1591]  because for any  [2.x.1592]  oscillations arose at the boundary, which are likely due to the scheme and not the equation, thus picking a value of  [2.x.1593]  a good bit into the "exponentially damped" spectrum of the time stepping schemes assures these oscillations are not created. 

 [2.x.1594]  

Another interesting solution to the sine-Gordon equation (which cannot be obtained analytically) can be produced by using two 1D breathers to construct the following separable 2D initial condition: 

[1.x.630] where  [2.x.1595] ,  [2.x.1596]  as in the 1D case we discussed above. For the simulation shown below, we have chosen  [2.x.1597] ,  [2.x.1598] ,  [2.x.1599]  and  [2.x.1600] . The solution is pretty interesting 

--- it acts like a breather (as far as the pictures are concerned); however, it appears to break up and reassemble, rather than just oscillate. 

 [2.x.1601]  


[1.x.631] 

[1.x.632] 

It is instructive to change the initial conditions. Most choices will not lead to solutions that stay localized (in the soliton community, such solutions are called "stationary", though the solution does change with time), but lead to solutions where the wave-like character of the equation dominates and a wave travels away from the location of a localized initial condition. For example, it is worth playing around with the  [2.x.1602]  class, by replacing the call to the  [2.x.1603]  class by something like this function: 

[1.x.633] 

if  [2.x.1604] , and  [2.x.1605]  outside this region. 

A second area would be to investigate whether the scheme is energy-preserving. For the pure wave equation, discussed in  [2.x.1606]  "step-23", this is the case if we choose the time stepping parameter such that we get the Crank-Nicolson scheme. One could do a similar thing here, noting that the energy in the sine-Gordon solution is defined as 

[1.x.634] 

(We use  [2.x.1607]  instead of  [2.x.1608]  in the formula to ensure that all contributions to the energy are positive, and so that decaying solutions have finite energy on unbounded domains.) 

Beyond this, there are two obvious areas: 

- Clearly, adaptivity (i.e. time-adaptive grids) would be of interest   to problems like these. Their complexity leads us to leave this out   of this program again, though the general comments in the   introduction of  [2.x.1609]  "step-23" remain true. 

- Faster schemes to solve this problem. While computers today are   plenty fast enough to solve 2d and, frequently, even 3d stationary   problems within not too much time, time dependent problems present   an entirely different class of problems. We address this topic in   step-48 where we show how to solve this problem in parallel and   without assembling or inverting any matrix at all. 


examples/step-26/doc/intro.dox 

[1.x.635] 

[1.x.636] 

 [2.x.1610]  ( [2.x.1611]  


This program implements the heat equation 

[1.x.637] 

In some sense, this equation is simpler than the ones we have discussed in the preceding programs step-23, step-24, step-25, namely the wave equation. This is due to the fact that the heat equation smoothes out the solution over time, and is consequently more forgiving in many regards. For example, when using implicit time stepping methods, we can actually take large time steps, we have less trouble with the small disturbances we introduce through adapting the mesh every few time steps, etc. 

Our goal here will be to solve the equations above using the theta-scheme that discretizes the equation in time using the following approach, where we would like  [2.x.1612]  to approximate  [2.x.1613]  at some time  [2.x.1614] : 

[1.x.638] 

Here,  [2.x.1615]  is the time step size. The theta-scheme generalizes the explicit Euler ( [2.x.1616] ), implicit Euler ( [2.x.1617] ) and Crank-Nicolson ( [2.x.1618] ) time discretizations. Since the latter has the highest convergence order, we will choose  [2.x.1619]  in the program below, but make it so that playing with this parameter remains simple. (If you are interested in playing with higher order methods, take a look at step-52.) 

Given this time discretization, space discretization happens as it always does, by multiplying with test functions, integrating by parts, and then restricting everything to a finite dimensional subspace. This yields the following set of fully discrete equations after multiplying through with  [2.x.1620] : 

[1.x.639] 

where  [2.x.1621]  is the mass matrix and  [2.x.1622]  is the stiffness matrix that results from discretizing the Laplacian. Bringing all known quantities to the right hand side yields the linear system we have to solve in every step: 

[1.x.640] 

The linear system on the left hand side is symmetric and positive definite, so we should have no trouble solving it with the Conjugate Gradient method. 

We can start the iteration above if we have the set of nodal coefficients  [2.x.1623]  at the initial time. Here, we take the ones we get by interpolating the initial values  [2.x.1624]  onto the mesh used for the first time step. We will also need to choose a time step; we will here just choose it as fixed, but clearly advanced simulators will want to choose it adaptively. We will briefly come back to this in the [1.x.641]. 




[1.x.642] 

When solving the wave equation and its variants in the previous few programs, we kept the mesh fixed. Just as for stationary equations, one can make a good case that this is not the smartest approach and that significant savings can be had by adapting the mesh. There are, however, significant difficulties compared to the stationary case. Let us go through them in turn: 

 [2.x.1625]     [2.x.1626] [1.x.643]: For stationary problems, the   general approach is "make the mesh as fine as it is necessary". For problems   with singularities, this often leads to situations where we get many levels   of refinement into corners or along interfaces. The very first tutorial to   use adaptive meshes, step-6, is a point in case already. 

  However, for time dependent problems, we typically need to choose the time   step related to the mesh size. For explicit time discretizations, this is   obvious, since we need to respect a CFL condition that ties the time step   size to the smallest mesh size. For implicit time discretizations, no such   hard restriction exists, but in practice we still want to make the time step   smaller if we make the mesh size smaller since we typically have error   estimates of the form  [2.x.1627]  where  [2.x.1628]  are the   convergence orders of the time and space discretization, respectively. We   can only make the error small if we decrease both terms. Ideally, an   estimate like this would suggest to choose  [2.x.1629] . Because, at   least for problems with non-smooth solutions, the error is typically   localized in the cells with the smallest mesh size, we have to indeed choose    [2.x.1630] , using the [1.x.644] mesh size. 

  The consequence is that refining the mesh further in one place implies not   only the moderate additional effort of increasing the number of degrees of   freedom slightly, but also the much larger effort of having the solve the   [1.x.645] linear system more often because of the smaller time step. 

  In practice, one typically deals with this by acknowledging that we can not   make the time step arbitrarily small, and consequently can not make the   local mesh size arbitrarily small. Rather, we set a maximal level of   refinement and when we flag cells for refinement, we simply do not refine   those cells whose children would exceed this maximal level of refinement. 

  There is a similar problem in that we will choose a right hand side that   will switch on in different parts of the domain at different times. To avoid   being caught flat footed with too coarse a mesh in areas where we suddenly   need a finer mesh, we will also enforce in our program a [1.x.646] mesh   refinement level. 

   [2.x.1631] [1.x.647]: Let us consider again the   semi-discrete equations we have written down above:   [1.x.648] 

  We can here consider  [2.x.1632]  as data since it has presumably been computed   before. Now, let us replace   [1.x.649] 

  multiply with test functions  [2.x.1633]  and integrate by parts   where necessary. In a process as outlined above, this would yield   [1.x.650] 

  Now imagine that we have changed the mesh between time steps  [2.x.1634]  and    [2.x.1635] . Then the problem is that the basis functions we use for  [2.x.1636]  and    [2.x.1637]  are different! This pertains to the terms on the right hand side,   the first of which we could more clearly write as (the second follows the   same pattern)   [1.x.651] 

  If the meshes used in these two time steps are the same, then    [2.x.1638]  forms a square mass matrix    [2.x.1639] . However, if the meshes are not the same, then in general the matrix   is rectangular. Worse, it is difficult to even compute these integrals   because if we loop over the cells of the mesh at time step  [2.x.1640] , then we need   to evaluate  [2.x.1641]  at the quadrature points of these cells, but   they do not necessarily correspond to the cells of the mesh at time step    [2.x.1642]  and  [2.x.1643]  is not defined via these cells; the same of   course applies if we wanted to compute the integrals via integration on the   cells of mesh  [2.x.1644] . 

  In any case, what we have to face is a situation where we need to integrate   shape functions defined on two different meshes. This can be done, and is in   fact demonstrated in step-28, but the process is at best described by the   word "awkward". 

  In practice, one does not typically want to do this. Rather, we avoid the   whole situation by interpolating the solution from the old to the new mesh   every time we adapt the mesh. In other words, rather than solving the   equations above, we instead solve the problem   [1.x.652] 

  where  [2.x.1645]  is the interpolation operator onto the finite element space   used in time step  [2.x.1646] . This is not the optimal approach since it introduces   an additional error besides time and space discretization, but it is a   pragmatic one that makes it feasible to do time adapting meshes.  [2.x.1647]  




[1.x.653] 

There are a number of things one can typically get wrong when implementing a finite element code. In particular, for time dependent problems, the following are common sources of bugs: 

- The time integration, for example by getting the coefficients in front of   the terms involving the current and previous time steps wrong (e.g., mixing   up a factor  [2.x.1648]  for  [2.x.1649] ). 

- Handling the right hand side, for example forgetting a factor of  [2.x.1650]  or    [2.x.1651] . 

- Mishandling the boundary values, again for example forgetting a factor of    [2.x.1652]  or  [2.x.1653] , or forgetting to apply nonzero boundary values not only   to the right hand side but also to the system matrix. 

A less common problem is getting the initial conditions wrong because one can typically see that it is wrong by just outputting the first time step. In any case, in order to verify the correctness of the code, it is helpful to have a testing protocol that allows us to verify each of these components separately. This means: 

- Testing the code with nonzero initial conditions but zero right hand side   and boundary values and verifying that the time evolution is correct. 

- Then testing with zero initial conditions and boundary values but nonzero   right hand side and again ensuring correctness. 

- Finally, testing with zero initial conditions and right hand side but   nonzero boundary values. 

This sounds complicated, but fortunately, for linear partial differential equations without coefficients (or constant coefficients) like the one here, there is a fairly standard protocol that rests on the following observation: if you choose as your domain a square  [2.x.1654]  (or, with slight modifications, a rectangle), then the exact solution can be written as 

[1.x.654] 

(with integer constants  [2.x.1655] ) if only the initial condition, right hand side and boundary values are all of the form  [2.x.1656]  as well. This is due to the fact that the function  [2.x.1657]  is an eigenfunction of the Laplace operator and allows us to compute things like the time factor  [2.x.1658]  analytically and, consequently, compare with what we get numerically. 

As an example, let us consider the situation where we have  [2.x.1659]  and  [2.x.1660] . With the claim (ansatz) of the form for  [2.x.1661]  above, we get that 

[1.x.655] 

For this to be equal to  [2.x.1662] , we need that 

[1.x.656] 

and due to the initial conditions,  [2.x.1663] . This differential equation can be integrated to yield 

[1.x.657] 

In other words, if the initial condition is a product of sines, then the solution has exactly the same shape of a product of sines that decays to zero with a known time dependence. This is something that is easy to test if you have a sufficiently fine mesh and sufficiently small time step. 

What is typically going to happen if you get the time integration scheme wrong (e.g., by having the wrong factors of  [2.x.1664]  or  [2.x.1665]  in front of the various terms) is that you don't get the right temporal behavior of the solution. Double check the various factors until you get the right behavior. You may also want to verify that the temporal decay rate (as determined, for example, by plotting the value of the solution at a fixed point) does not double or halve each time you double or halve the time step or mesh size. You know that it's not the handling of the boundary conditions or right hand side because these were both zero. 

If you have so verified that the time integrator is correct, take the situation where the right hand side is nonzero but the initial conditions are zero:  [2.x.1666]  and  [2.x.1667] . Again, 

[1.x.658] 

and for this to be equal to  [2.x.1668] , we need that 

[1.x.659] 

and due to the initial conditions,  [2.x.1669] . Integrating this equation in time yields 

[1.x.660] 



Again, if you have the wrong factors of  [2.x.1670]  or  [2.x.1671]  in front of the right hand side terms you will either not get the right temporal behavior of the solution, or it will converge to a maximum value other than  [2.x.1672] . 

Once we have verified that the time integration and right hand side handling are correct using this scheme, we can go on to verifying that we have the boundary values correct, using a very similar approach. 




[1.x.661] 

Solving the heat equation on a simple domain with a simple right hand side almost always leads to solutions that are exceedingly boring, since they become very smooth very quickly and then do not move very much any more. Rather, we here solve the equation on the L-shaped domain with zero Dirichlet boundary values and zero initial conditions, but as right hand side we choose 

[1.x.662] 

Here, 

[1.x.663] 

In other words, in every period of length  [2.x.1673] , the right hand side first flashes on in domain 1, then off completely, then on in domain 2, then off completely again. This pattern is probably best observed via the little animation of the solution shown in the [1.x.664]. 

If you interpret the heat equation as finding the spatially and temporally variable temperature distribution of a conducting solid, then the test case above corresponds to an L-shaped body where we keep the boundary at zero temperature, and heat alternatingly in two parts of the domain. While heating is in effect, the temperature rises in these places, after which it diffuses and diminishes again. The point of these initial conditions is that they provide us with a solution that has singularities both in time (when sources switch on and off) as well as time (at the reentrant corner as well as at the edges and corners of the regions where the source acts). 


examples/step-26/doc/results.dox 



[1.x.665] 

As in many of the tutorials, the actual output of the program matters less than how we arrived there. Nonetheless, here it is: 

[1.x.666] 



Maybe of more interest is a visualization of the solution and the mesh on which it was computed: 

 [2.x.1674]  

The movie shows how the two sources switch on and off and how the mesh reacts to this. It is quite obvious that the mesh as is is probably not the best we could come up with. We'll get back to this in the next section. 


[1.x.667] 

[1.x.668] 

There are at least two areas where one can improve this program significantly: adaptive time stepping and a better choice of the mesh. 

[1.x.669] 

Having chosen an implicit time stepping scheme, we are not bound by any CFL-like condition on the time step. Furthermore, because the time scales on which change happens on a given cell in the heat equation are not bound to the cells diameter (unlike the case with the wave equation, where we had a fixed speed of information transport that couples the temporal and spatial scales), we can choose the time step as we please. Or, better, choose it as we deem necessary for accuracy. 

Looking at the solution, it is clear that the action does not happen uniformly over time: a lot is changing around the time we switch on a source, things become less dramatic once a source is on for a little while, and we enter a long phase of decline when both sources are off. During these times, we could surely get away with a larger time step than before without sacrificing too much accuracy. 

The literature has many suggestions on how to choose the time step size adaptively. Much can be learned, for example, from the way ODE solvers choose their time steps. One can also be inspired by a posteriori error estimators that can, ideally, be written in a way that the consist of a temporal and a spatial contribution to the overall error. If the temporal one is too large, we should choose a smaller time step. Ideas in this direction can be found, for example, in the PhD thesis of a former principal developer of deal.II, Ralf Hartmann, published by the University of Heidelberg, Germany, in 2002. 




[1.x.670] 

We here use one of the simpler time stepping methods, namely the second order in time Crank-Nicolson method. However, more accurate methods such as Runge-Kutta methods are available and should be used as they do not represent much additional effort. It is not difficult to implement this for the current program, but a more systematic treatment is also given in step-52. 




[1.x.671] 

If you look at the meshes in the movie above, it is clear that they are not particularly well suited to the task at hand. In fact, they look rather random. 

There are two factors at play. First, there are some islands where cells have been refined but that are surrounded by non-refined cells (and there are probably also a few occasional coarsened islands). These are not terrible, as they most of the time do not affect the approximation quality of the mesh, but they also don't help because so many of their additional degrees of freedom are in fact constrained by hanging node constraints. That said, this is easy to fix: the Triangulation class takes an argument to its constructor indicating a level of "mesh smoothing". Passing one of many possible flags, this instructs the triangulation to refine some additional cells, or not to refine some cells, so that the resulting mesh does not have these artifacts. 

The second problem is more severe: the mesh appears to lag the solution. The underlying reason is that we only adapt the mesh once every fifth time step, and only allow for a single refinement in these cases. Whenever a source switches on, the solution had been very smooth in this area before and the mesh was consequently rather coarse. This implies that the next time step when we refine the mesh, we will get one refinement level more in this area, and five time steps later another level, etc. But this is not enough: first, we should refine immediately when a source switches on (after all, in the current context we at least know what the right hand side is), and we should allow for more than one refinement level. Of course, all of this can be done using deal.II, it just requires a bit of algorithmic thinking in how to make this work! 




[1.x.672] 

To increase the accuracy and resolution of your simulation in time, one typically decreases the time step size  [2.x.1675] . If you start playing around with the time step in this particular example, you will notice that the solution becomes partly negative, if  [2.x.1676]  is below a certain threshold. This is not what we would expect to happen (in nature). 

To get an idea of this behavior mathematically, let us consider a general, fully discrete problem: 

[1.x.673] 

The general form of the  [2.x.1677] th equation then reads: 

[1.x.674] 

where  [2.x.1678]  is the set of degrees of freedom that DoF  [2.x.1679]  couples with (i.e., for which either the matrix  [2.x.1680]  or matrix  [2.x.1681]  has a nonzero entry at position  [2.x.1682] ). If all coefficients fulfill the following conditions: 

[1.x.675] 

all solutions  [2.x.1683]  keep their sign from the previous ones  [2.x.1684] , and consequently from the initial values  [2.x.1685] . See e.g. [1.x.676] for more information on positivity preservation. 

Depending on the PDE to solve and the time integration scheme used, one is able to deduce conditions for the time step  [2.x.1686] . For the heat equation with the Crank-Nicolson scheme, [1.x.677] have translated it to the following ones: 

[1.x.678] 

where  [2.x.1687]  denotes the mass matrix and  [2.x.1688]  the stiffness matrix with  [2.x.1689]  for  [2.x.1690] , respectively. With  [2.x.1691] , we can formulate bounds for the global time step  [2.x.1692]  as follows: 

[1.x.679] 

In other words, the time step is constrained by [1.x.680] in case of a Crank-Nicolson scheme. These bounds should be considered along with the CFL condition to ensure significance of the performed simulations. 

Being unable to make the time step as small as we want to get more accuracy without losing the positivity property is annoying. It raises the question of whether we can at least [1.x.681] the minimal time step we can choose  to ensure positivity preservation in this particular tutorial. Indeed, we can use the SparseMatrix objects for both mass and stiffness that are created via the MatrixCreator functions. Iterating through each entry via SparseMatrixIterators lets us check for diagonal and off-diagonal entries to set a proper time step dynamically. For quadratic matrices, the diagonal element is stored as the first member of a row (see SparseMatrix documentation). An exemplary code snippet on how to grab the entries of interest from the  [2.x.1693]  is shown below. 

[1.x.682] 



Using the information so computed, we can bound the time step via the formulas above. 


examples/step-27/doc/intro.dox 

[1.x.683] 

[1.x.684] 

This tutorial program attempts to show how to use  [2.x.1694] -finite element methods with deal.II. It solves the Laplace equation and so builds only on the first few tutorial programs, in particular on step-4 for dimension independent programming and step-6 for adaptive mesh refinement. 

The  [2.x.1695] -finite element method was proposed in the early 1980s by Babu&scaron;ka and Guo as an alternative to either (i) mesh refinement (i.e., decreasing the mesh parameter  [2.x.1696]  in a finite element computation) or (ii) increasing the polynomial degree  [2.x.1697]  used for shape functions. It is based on the observation that increasing the polynomial degree of the shape functions reduces the approximation error if the solution is sufficiently smooth. On the other hand, it is well known that even for the generally well-behaved class of elliptic problems, higher degrees of regularity can not be guaranteed in the vicinity of boundaries, corners, or where coefficients are discontinuous; consequently, the approximation can not be improved in these areas by increasing the polynomial degree  [2.x.1698]  but only by refining the mesh, i.e., by reducing the mesh size  [2.x.1699] . These differing means to reduce the error have led to the notion of  [2.x.1700] -finite elements, where the approximating finite element spaces are adapted to have a high polynomial degree  [2.x.1701]  wherever the solution is sufficiently smooth, while the mesh width  [2.x.1702]  is reduced at places wherever the solution lacks regularity. It was already realized in the first papers on this method that  [2.x.1703] -finite elements can be a powerful tool that can guarantee that the error is reduced not only with some negative power of the number of degrees of freedom, but in fact exponentially. 

In order to implement this method, we need several things above and beyond what a usual finite element program needs, and in particular above what we have introduced in the tutorial programs leading up to step-6. In particular, we will have to discuss the following aspects:  [2.x.1704]     [2.x.1705] Instead of using the same finite element on all cells, we now will want   a collection of finite element objects, and associate each cell with one   of these objects in this collection. [2.x.1706]  

   [2.x.1707] Degrees of freedom will then have to be allocated on each cell depending   on what finite element is associated with this particular cell. Constraints   will have to be generated in the same way as for hanging nodes, but we now   also have to deal with the case where two neighboring cells have different   finite elements assigned. [2.x.1708]  

   [2.x.1709] We will need to be able to assemble cell and face contributions   to global matrices and right hand side vectors. [2.x.1710]  

   [2.x.1711] After solving the resulting linear system, we will want to   analyze the solution. In particular, we will want to compute error   indicators that tell us whether a given cell should be refined   and/or whether the polynomial degree of the shape functions used on   it should be increased. [2.x.1712]   [2.x.1713]  

We will discuss all these aspects in the following subsections of this introduction. It will not come as a big surprise that most of these tasks are already well supported by functionality provided by the deal.II, and that we will only have to provide the logic of what the program should do, not exactly how all this is going to happen. 

In deal.II, the  [2.x.1714] -functionality is largely packaged into the hp-namespace. This namespace provides classes that handle  [2.x.1715] -discretizations, assembling matrices and vectors, and other tasks. We will get to know many of them further down below. In addition, most of the functions in the DoFTools, and VectorTools namespaces accept  [2.x.1716] -objects in addition to the non- [2.x.1717] -ones. Much of the  [2.x.1718] -implementation is also discussed in the  [2.x.1719]  documentation module and the links found there. 

It may be worth giving a slightly larger perspective at the end of this first part of the introduction.  [2.x.1720] -functionality has been implemented in a number of different finite element packages (see, for example, the list of references cited in the  [2.x.1721]  "hp-paper"). However, by and large, most of these packages have implemented it only for the (i) the 2d case, and/or (ii) the discontinuous Galerkin method. The latter is a significant simplification because discontinuous finite elements by definition do not require continuity across faces between cells and therefore do not require the special treatment otherwise necessary whenever finite elements of different polynomial degree meet at a common face. In contrast, deal.II implements the most general case, i.e., it allows for continuous and discontinuous elements in 1d, 2d, and 3d, and automatically handles the resulting complexity. In particular, it handles computing the constraints (similar to hanging node constraints) of elements of different degree meeting at a face or edge. The many algorithmic and data structure techniques necessary for this are described in the  [2.x.1722]  "hp-paper" for those interested in such detail. 

We hope that providing such a general implementation will help explore the potential of  [2.x.1723] -methods further. 




[1.x.685] 

Now on again to the details of how to use the  [2.x.1724] -functionality in deal.II. The first aspect we have to deal with is that now we do not have only a single finite element any more that is used on all cells, but a number of different elements that cells can choose to use. For this, deal.II introduces the concept of a [1.x.686], implemented in the class  [2.x.1725]  In essence, such a collection acts like an object of type  [2.x.1726] , but with a few more bells and whistles and a memory management better suited to the task at hand. As we will later see, we will also use similar quadrature collections, and &mdash; although we don't use them here &mdash; there is also the concept of mapping collections. All of these classes are described in the  [2.x.1727]  overview. 

In this tutorial program, we will use continuous Lagrange elements of orders 2 through 7 (in 2d) or 2 through 5 (in 3d). The collection of used elements can then be created as follows: 

[1.x.687] 






[1.x.688][1.x.689] 

The next task we have to consider is what to do with the list of finite element objects we want to use. In previous tutorial programs, starting with step-2, we have seen that the DoFHandler class is responsible for making the connection between a mesh (described by a Triangulation object) and a finite element, by allocating the correct number of degrees of freedom for each vertex, face, edge, and cell of the mesh. 

The situation here is a bit more complicated since we do not just have a single finite element object, but rather may want to use different elements on different cells. We therefore need two things: (i) a version of the DoFHandler class that can deal with this situation, and (ii) a way to tell the DoFHandler which element to use on which cell. 

The first of these two things is implemented in the [1.x.690]-mode of the DoFHandler class: rather than associating it with a triangulation and a single finite element object, it is associated with a triangulation and a finite element collection. The second part is achieved by a loop over all cells of this DoFHandler and for each cell setting the index of the finite element within the collection that shall be used on this cell. We call the index of the finite element object within the collection that shall be used on a cell the cell's [1.x.691] to indicate that this is the finite element that is active on this cell, whereas all the other elements of the collection are inactive on it. The general outline of this reads like this: 

[1.x.692] 



Dots in the call to  [2.x.1728]  indicate that we will have to have some sort of strategy later on to decide which element to use on which cell; we will come back to this later. The main point here is that the first and last line of this code snippet is pretty much exactly the same as for the non- [2.x.1729] -case. 

Another complication arises from the fact that this time we do not simply have hanging nodes from local mesh refinement, but we also have to deal with the case that if there are two cells with different active finite element indices meeting at a face (for example a Q2 and a Q3 element) then we have to compute additional constraints on the finite element field to ensure that it is continuous. This is conceptually very similar to how we compute hanging node constraints, and in fact the code looks exactly the same: 

[1.x.693] 

In other words, the  [2.x.1730]  deals not only with hanging node constraints, but also with  [2.x.1731] -constraints at the same time. 




[1.x.694] 

Following this, we have to set up matrices and vectors for the linear system of the correct size and assemble them. Setting them up works in exactly the same way as for the non- [2.x.1732] -case. Assembling requires a bit more thought. 

The main idea is of course unchanged: we have to loop over all cells, assemble local contributions, and then copy them into the global objects. As discussed in some detail first in step-3, deal.II has the FEValues class that pulls the finite element description, mapping, and quadrature formula together and aids in evaluating values and gradients of shape functions as well as other information on each of the quadrature points mapped to the real location of a cell. Every time we move on to a new cell we re-initialize this FEValues object, thereby asking it to re-compute that part of the information that changes from cell to cell. It can then be used to sum up local contributions to bilinear form and right hand side. 

In the context of  [2.x.1733] -finite element methods, we have to deal with the fact that we do not use the same finite element object on each cell. In fact, we should not even use the same quadrature object for all cells, but rather higher order quadrature formulas for cells where we use higher order finite elements. Similarly, we may want to use higher order mappings on such cells as well. 

To facilitate these considerations, deal.II has a class  [2.x.1734]  that does what we need in the current context. The difference is that instead of a single finite element, quadrature formula, and mapping, it takes collections of these objects. It's use is very much like the regular FEValues class, i.e., the interesting part of the loop over all cells would look like this: 

[1.x.695] 



In this tutorial program, we will always use a Q1 mapping, so the mapping collection argument to the  [2.x.1735]  construction will be omitted. Inside the loop, we first initialize the  [2.x.1736]  object for the current cell. The second, third and fourth arguments denote the index within their respective collections of the quadrature, mapping, and finite element objects we wish to use on this cell. These arguments can be omitted (and are in the program below), in which case  [2.x.1737]  is used for this index. The order of these arguments is chosen in this way because one may sometimes want to pick a different quadrature or mapping object from their respective collections, but hardly ever a different finite element than the one in use on this cell, i.e., one with an index different from  [2.x.1738] . The finite element collection index is therefore the last default argument so that it can be conveniently omitted. 

What this  [2.x.1739]  call does is the following: the  [2.x.1740]  class checks whether it has previously already allocated a non- [2.x.1741] -FEValues object for this combination of finite element, quadrature, and mapping objects. If not, it allocates one. It then re-initializes this object for the current cell, after which there is now a FEValues object for the selected finite element, quadrature and mapping usable on the current cell. A reference to this object is then obtained using the call  [2.x.1742] , and will be used in the usual fashion to assemble local contributions. 




[1.x.696] 

One of the central pieces of the adaptive finite element method is that we inspect the computed solution (a posteriori) with an indicator that tells us which are the cells where the error is largest, and then refine them. In many of the other tutorial programs, we use the KellyErrorEstimator class to get an indication of the size of the error on a cell, although we also discuss more complicated strategies in some programs, most importantly in step-14. 

In any case, as long as the decision is only "refine this cell" or "do not refine this cell", the actual refinement step is not particularly challenging. However, here we have a code that is capable of hp-refinement, i.e., we suddenly have two choices whenever we detect that the error on a certain cell is too large for our liking: we can refine the cell by splitting it into several smaller ones, or we can increase the polynomial degree of the shape functions used on it. How do we know which is the more promising strategy? Answering this question is the central problem in  [2.x.1743] -finite element research at the time of this writing. 

In short, the question does not appear to be settled in the literature at this time. There are a number of more or less complicated schemes that address it, but there is nothing like the KellyErrorEstimator that is universally accepted as a good, even if not optimal, indicator of the error. Most proposals use the fact that it is beneficial to increase the polynomial degree whenever the solution is locally smooth whereas it is better to refine the mesh wherever it is rough. However, the questions of how to determine the local smoothness of the solution as well as the decision when a solution is smooth enough to allow for an increase in  [2.x.1744]  are certainly big and important ones. 

In the following, we propose a simple estimator of the local smoothness of a solution. As we will see in the results section, this estimator has flaws, in particular as far as cells with local hanging nodes are concerned. We therefore do not intend to present the following ideas as a complete solution to the problem. Rather, it is intended as an idea to approach it that merits further research and investigation. In other words, we do not intend to enter a sophisticated proposal into the fray about answers to the general question. However, to demonstrate our approach to  [2.x.1745] -finite elements, we need a simple indicator that does generate some useful information that is able to drive the simple calculations this tutorial program will perform. 




[1.x.697] 

Our approach here is simple: for a function  [2.x.1746]  to be in the Sobolev space  [2.x.1747]  on a cell  [2.x.1748] , it has to satisfy the condition 

[1.x.698] 

Assuming that the cell  [2.x.1749]  is not degenerate, i.e., that the mapping from the unit cell to cell  [2.x.1750]  is sufficiently regular, above condition is of course equivalent to 

[1.x.699] 

where  [2.x.1751]  is the function  [2.x.1752]  mapped back onto the unit cell  [2.x.1753] . From here, we can do the following: first, let us define the Fourier series of  [2.x.1754]  as 

[1.x.700] 

with Fourier vectors  [2.x.1755]  in 2d,  [2.x.1756]  in 3d, etc, and  [2.x.1757] . The coefficients of expansion  [2.x.1758]  can be obtained using  [2.x.1759] -orthogonality of the exponential basis 

[1.x.701] 

that leads to the following expression 

[1.x.702] 

It becomes clear that we can then write the  [2.x.1760]  norm of  [2.x.1761]  as 

[1.x.703] 

In other words, if this norm is to be finite (i.e., for  [2.x.1762]  to be in  [2.x.1763] ), we need that 

[1.x.704] 

Put differently: the higher regularity  [2.x.1764]  we want, the faster the Fourier coefficients have to go to zero. If you wonder where the additional exponent  [2.x.1765]  comes from: we would like to make use of the fact that  [2.x.1766]  if the sequence  [2.x.1767]  for any  [2.x.1768] . The problem is that we here have a summation not only over a single variable, but over all the integer multiples of  [2.x.1769]  that are located inside the  [2.x.1770] -dimensional sphere, because we have vector components  [2.x.1771] . In the same way as we prove that the sequence  [2.x.1772]  above converges by replacing the sum by an integral over the entire line, we can replace our  [2.x.1773] -dimensional sum by an integral over  [2.x.1774] -dimensional space. Now we have to note that between distance  [2.x.1775]  and  [2.x.1776] , there are, up to a constant,  [2.x.1777]  modes, in much the same way as we can transform the volume element  [2.x.1778]  into  [2.x.1779] . Consequently, it is no longer  [2.x.1780]  that has to decay as  [2.x.1781] , but it is in fact  [2.x.1782] . A comparison of exponents yields the result. 

We can turn this around: Assume we are given a function  [2.x.1783]  of unknown smoothness. Let us compute its Fourier coefficients  [2.x.1784]  and see how fast they decay. If they decay as 

[1.x.705] 

then consequently the function we had here was in  [2.x.1785] . 




[1.x.706] 

So what do we have to do to estimate the local smoothness of  [2.x.1786]  on a cell  [2.x.1787] ? Clearly, the first step is to compute the Fourier coefficients of our solution. Fourier series being infinite series, we simplify our task by only computing the first few terms of the series, such that  [2.x.1788]  with a cut-off  [2.x.1789] . Let us parenthetically remark that we want to choose  [2.x.1790]  large enough so that we capture at least the variation of those shape functions that vary the most. On the other hand, we should not choose  [2.x.1791]  too large: clearly, a finite element function, being a polynomial, is in  [2.x.1792]  on any given cell, so the coefficients will have to decay exponentially at one point; since we want to estimate the smoothness of the function this polynomial approximates, not of the polynomial itself, we need to choose a reasonable cutoff for  [2.x.1793] . Either way, computing this series is not particularly hard: from the definition 

[1.x.707] 

we see that we can compute the coefficient  [2.x.1794]  as 

[1.x.708] 

where  [2.x.1795]  is the value of the  [2.x.1796] th degree of freedom on this cell. In other words, we can write it as a matrix-vector product 

[1.x.709] 

with the matrix 

[1.x.710] 

This matrix is easily computed for a given number of shape functions  [2.x.1797]  and Fourier modes  [2.x.1798] . Consequently, finding the coefficients  [2.x.1799]  is a rather trivial job. To simplify our life even further, we will use  [2.x.1800]  class which does exactly this. 

The next task is that we have to estimate how fast these coefficients decay with  [2.x.1801] . The problem is that, of course, we have only finitely many of these coefficients in the first place. In other words, the best we can do is to fit a function  [2.x.1802]  to our data points  [2.x.1803] , for example by determining  [2.x.1804]  via a least-squares procedure: 

[1.x.711] 

However, the problem with this is that it leads to a nonlinear problem, a fact that we would like to avoid. On the other hand, we can transform the problem into a simpler one if we try to fit the logarithm of our coefficients to the logarithm of  [2.x.1805] , like this: 

[1.x.712] 

Using the usual facts about logarithms, we see that this yields the problem 

[1.x.713] 

where  [2.x.1806] . This is now a problem for which the optimality conditions  [2.x.1807] , are linear in  [2.x.1808] . We can write these conditions as follows: 

[1.x.714] 

This linear system is readily inverted to yield 

[1.x.715] 

and 

[1.x.716] 



This is nothing else but linear regression fit and to do that we will use  [2.x.1809]  While we are not particularly interested in the actual value of  [2.x.1810] , the formula above gives us a mean to calculate the value of the exponent  [2.x.1811]  that we can then use to determine that  [2.x.1812]  is in  [2.x.1813]  with  [2.x.1814] . 

These steps outlined above are applicable to many different scenarios, which motivated the introduction of a generic function  [2.x.1815]  in deal.II, that combines all the tasks described in this section in one simple function call. We will use it in the implementation of this program. 




[1.x.717] 

In the formulas above, we have derived the Fourier coefficients  [2.x.1816] . Because  [2.x.1817]  is a vector, we will get a number of Fourier coefficients  [2.x.1818]  for the same absolute value  [2.x.1819] , corresponding to the Fourier transform in different directions. If we now consider a function like  [2.x.1820]  then we will find lots of large Fourier coefficients in  [2.x.1821] -direction because the function is non-smooth in this direction, but fast-decaying Fourier coefficients in  [2.x.1822] -direction because the function is smooth there. The question that arises is this: if we simply fit our polynomial decay  [2.x.1823]  to [1.x.718] Fourier coefficients, we will fit it to a smoothness [1.x.719]. Is this what we want? Or would it be better to only consider the largest coefficient  [2.x.1824]  for all  [2.x.1825]  with the same magnitude, essentially trying to determine the smoothness of the solution in that spatial direction in which the solution appears to be roughest? 

One can probably argue for either case. The issue would be of more interest if deal.II had the ability to use anisotropic finite elements, i.e., ones that use different polynomial degrees in different spatial directions, as they would be able to exploit the directionally variable smoothness much better. Alas, this capability does not exist at the time of writing this tutorial program. 

Either way, because we only have isotopic finite element classes, we adopt the viewpoint that we should tailor the polynomial degree to the lowest amount of regularity, in order to keep numerical efforts low. Consequently, instead of using the formula 

[1.x.720] 

To calculate  [2.x.1826]  as shown above, we have to slightly modify all sums: instead of summing over all Fourier modes, we only sum over those for which the Fourier coefficient is the largest one among all  [2.x.1827]  with the same magnitude  [2.x.1828] , i.e., all sums above have to replaced by the following sums: 

[1.x.721] 

This is the form we will implement in the program. 




[1.x.722] 

One may ask whether it is a problem that we only compute the Fourier transform on the [1.x.723] (rather than the real cell) of the solution. After all, we stretch the solution by a factor  [2.x.1829]  during the transformation, thereby shifting the Fourier frequencies by a factor of  [2.x.1830] . This is of particular concern since we may have neighboring cells with mesh sizes  [2.x.1831]  that differ by a factor of 2 if one of them is more refined than the other. The concern is also motivated by the fact that, as we will see in the results section below, the estimated smoothness of the solution should be a more or less continuous function, but exhibits jumps at locations where the mesh size jumps. It therefore seems natural to ask whether we have to compensate for the transformation. 

The short answer is "no". In the process outlined above, we attempt to find coefficients  [2.x.1832]  that minimize the sum of squares of the terms 

[1.x.724] 

To compensate for the transformation means not attempting to fit a decay  [2.x.1833]  with respect to the Fourier frequencies  [2.x.1834]  [1.x.725], but to fit the coefficients  [2.x.1835]  computed on the reference cell [1.x.726], where  [2.x.1836]  is the norm of the transformation operator (i.e., something like the diameter of the cell). In other words, we would have to minimize the sum of squares of the terms 

[1.x.727] 

instead. However, using fundamental properties of the logarithm, this is simply equivalent to minimizing 

[1.x.728] 

In other words, this and the original least squares problem will produce the same best-fit exponent  [2.x.1837] , though the offset will in one case be  [2.x.1838]  and in the other  [2.x.1839] . However, since we are not interested in the offset at all but only in the exponent, it doesn't matter whether we scale Fourier frequencies in order to account for mesh size effects or not, the estimated smoothness exponent will be the same in either case. 




[1.x.729] 

[1.x.730] 

One of the problems with  [2.x.1840] -methods is that the high polynomial degree of shape functions together with the large number of constrained degrees of freedom leads to matrices with large numbers of nonzero entries in some rows. At the same time, because there are areas where we use low polynomial degree and consequently matrix rows with relatively few nonzero entries. Consequently, allocating the sparsity pattern for these matrices is a challenge: we cannot simply assemble a SparsityPattern by starting with an estimate of the bandwidth without using a lot of extra memory. 

The way in which we create a SparsityPattern for the underlying linear system is tightly coupled to the strategy we use to enforce constraints. deal.II supports handling constraints in linear systems in two ways: <ol>    [2.x.1841] Assembling the matrix without regard to the constraints and applying them   afterwards with  [2.x.1842]  or [2.x.1843]     [2.x.1844] Applying constraints as we assemble the system with    [2.x.1845]   [2.x.1846]  Most programs built on deal.II use the  [2.x.1847]  function to allocate a DynamicSparsityPattern that takes constraints into account. The system matrix then uses a SparsityPattern copied over from the DynamicSparsityPattern. This method is explained in step-2 and used in most tutorial programs. 

The early tutorial programs use first or second degree finite elements, so removing entries in the sparsity pattern corresponding to constrained degrees of freedom does not have a large impact on the overall number of zeros explicitly stored by the matrix. However, since as many as a third of the degrees of freedom may be constrained in an hp-discretization (and, with higher degree elements, these constraints can couple one DoF to as many as ten or twenty other DoFs), it is worthwhile to take these constraints into consideration since the resulting matrix will be much sparser (and, therefore, matrix-vector products or factorizations will be substantially faster too). 




[1.x.731] 

A second problem particular to  [2.x.1848] -methods arises because we have so many constrained degrees of freedom: typically up to about one third of all degrees of freedom (in 3d) are constrained because they either belong to cells with hanging nodes or because they are on cells adjacent to cells with a higher or lower polynomial degree. This is, in fact, not much more than the fraction of constrained degrees of freedom in non- [2.x.1849] -mode, but the difference is that each constrained hanging node is constrained not only against the two adjacent degrees of freedom, but is constrained against many more degrees of freedom. 

It turns out that the strategy presented first in step-6 to eliminate the constraints while computing the element matrices and vectors with  [2.x.1850]  is the most efficient approach also for this case. The alternative strategy to first build the matrix without constraints and then "condensing" away constrained degrees of freedom is considerably more expensive. It turns out that building the sparsity pattern by this inefficient algorithm requires at least  [2.x.1851]  in the number of unknowns, whereas an ideal finite element program would of course only have algorithms that are linear in the number of unknowns. Timing the sparsity pattern creation as well as the matrix assembly shows that the algorithm presented in step-6 (and used in the code below) is indeed faster. 

In our program, we will also treat the boundary conditions as (possibly inhomogeneous) constraints and eliminate the matrix rows and columns to those as well. All we have to do for this is to call the function that interpolates the Dirichlet boundary conditions already in the setup phase in order to tell the AffineConstraints object about them, and then do the transfer from local to global data on matrix and vector simultaneously. This is exactly what we've shown in step-6. 




[1.x.732] 

The test case we will solve with this program is a re-take of the one we already look at in step-14: we solve the Laplace equation 

[1.x.733] 

in 2d, with  [2.x.1852] , and with zero Dirichlet boundary values for  [2.x.1853] . We do so on the domain  [2.x.1854] , i.e., a square with a square hole in the middle. 

The difference to step-14 is of course that we use  [2.x.1855] -finite elements for the solution. The test case is of interest because it has re-entrant corners in the corners of the hole, at which the solution has singularities. We therefore expect that the solution will be smooth in the interior of the domain, and rough in the vicinity of the singularities. The hope is that our refinement and smoothness indicators will be able to see this behavior and refine the mesh close to the singularities, while the polynomial degree is increased away from it. As we will see in the results section, this is indeed the case. 


examples/step-27/doc/results.dox 



[1.x.734] 

In this section, we discuss a few results produced from running the current tutorial program. More results, in particular the extension to 3d calculations and determining how much compute time the individual components of the program take, are given in the  [2.x.1856]  "hp-paper". 

When run, this is what the program produces: 

[1.x.735] 



The first thing we learn from this is that the number of constrained degrees of freedom is on the order of 20-25% of the total number of degrees of freedom, at least on the later grids when we have elements of relatively high order (in 3d, the fraction of constrained degrees of freedom can be up to 30%). This is, in fact, on the same order of magnitude as for non- [2.x.1857] -discretizations. For example, in the last step of the step-6 program, we have 18353 degrees of freedom, 4432 of which are constrained. The difference is that in the latter program, each constrained hanging node is constrained against only the two adjacent degrees of freedom, whereas in the  [2.x.1858] -case, constrained nodes are constrained against many more degrees of freedom. Note also that the current program also includes nodes subject to Dirichlet boundary conditions in the list of constraints. In cycle 0, all the constraints are actually because of boundary conditions. 

Of maybe more interest is to look at the graphical output. First, here is the solution of the problem: 

<img src="https://www.dealii.org/images/steps/developer/step-27-solution.png"      alt="Elevation plot of the solution, showing the lack of regularity near           the interior (reentrant) corners."      width="200" height="200"> 

Secondly, let us look at the sequence of meshes generated: 

<div class="threecolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-00.svg"          alt="Triangulation containing reentrant corners without adaptive refinement."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-01.svg"          alt="Triangulation containing reentrant corners with one level of          refinement. New cells are placed near the corners."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-02.svg"          alt="Triangulation containing reentrant corners with two levels of          refinement. New cells are placed near the corners."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-03.svg"          alt="Triangulation containing reentrant corners with three levels of          refinement. New cells are placed near the corners."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-04.svg"          alt="Triangulation containing reentrant corners with four levels of          refinement. New cells are placed near the corners."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-05.svg"          alt="Triangulation containing reentrant corners with five levels of          refinement. New cells are placed near the corners."          width="200" height="200">   </div> </div> 

It is clearly visible how the mesh is refined near the corner singularities, as one would expect it. More interestingly, we should be curious to see the distribution of finite element polynomial degrees to these mesh cells, where the lightest color corresponds to degree two and the darkest one corresponds to degree seven: 

<div class="threecolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-00.svg"          alt="Initial grid where all cells contain just biquadratic functions."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-01.svg"          alt="Depiction of local approximation degrees after one refinement."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-02.svg"          alt="Depiction of local approximation degrees after two refinements."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-03.svg"          alt="Depiction of local approximation degrees after three refinements."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-04.svg"          alt="Depiction of local approximation degrees after four refinements."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-05.svg"          alt="Depiction of local approximation degrees after five refinements."          width="200" height="200">   </div> </div> 

While this is certainly not a perfect arrangement, it does make some sense: we use low order elements close to boundaries and corners where regularity is low. On the other hand, higher order elements are used where (i) the error was at one point fairly large, i.e., mainly in the general area around the corner singularities and in the top right corner where the solution is large, and (ii) where the solution is smooth, i.e., far away from the boundary. 

This arrangement of polynomial degrees of course follows from our smoothness estimator. Here is the estimated smoothness of the solution, with darker colors indicating least smoothness and lighter indicating the smoothest areas: 

<div class="threecolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-00.svg"          alt="Estimated regularity per cell on the initial grid."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-01.svg"          alt="Depiction of the estimated regularity per cell after one refinement."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-02.svg"          alt="Depiction of the estimated regularity per cell after two refinements."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-03.svg"          alt="Depiction of the estimated regularity per cell after three refinements."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-04.svg"          alt="Depiction of the estimated regularity per cell after four refinements."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-05.svg"          alt="Depiction of the estimated regularity per cell after five refinements."          width="200" height="200">   </div> </div> 

The primary conclusion one can draw from this is that the loss of regularity at the internal corners is a highly localized phenomenon; it only seems to impact the cells adjacent to the corner itself, so when we refine the mesh the black coloring is no longer visible. Besides the corners, this sequence of plots implies that the smoothness estimates are somewhat independent of the mesh refinement, particularly when we are far away from boundaries. It is also obvious that the smoothness estimates are independent of the actual size of the solution (see the picture of the solution above), as it should be. A point of larger concern, however, is that one realizes on closer inspection that the estimator we have overestimates the smoothness of the solution on cells with hanging nodes. This in turn leads to higher polynomial degrees in these areas, skewing the allocation of finite elements onto cells. 

We have no good explanation for this effect at the moment. One theory is that the numerical solution on cells with hanging nodes is, of course, constrained and therefore not entirely free to explore the function space to get close to the exact solution. This lack of degrees of freedom may manifest itself by yielding numerical solutions on these cells with suppressed oscillation, meaning a higher degree of smoothness. The estimator picks this signal up and the estimated smoothness overestimates the actual value. However, a definite answer to what is going on currently eludes the authors of this program. 

The bigger question is, of course, how to avoid this problem. Possibilities include estimating the smoothness not on single cells, but cell assemblies or patches surrounding each cell. It may also be possible to find simple correction factors for each cell depending on the number of constrained degrees of freedom it has. In either case, there are ample opportunities for further research on finding good  [2.x.1859] -refinement criteria. On the other hand, the main point of the current program was to demonstrate using the  [2.x.1860] -technology in deal.II, which is unaffected by our use of a possible sub-optimal refinement criterion. 




[1.x.736] 

[1.x.737] 

[1.x.738] 

This tutorial demonstrates only one particular strategy to decide between  [2.x.1861] - and  [2.x.1862] -adaptation. In fact, there are many more ways to automatically decide on the adaptation type, of which a few are already implemented in deal.II:  [2.x.1863]     [2.x.1864] [1.x.739] This is the strategy currently   implemented in this tutorial. For more information on this strategy, see   the general documentation of the  [2.x.1865]  namespace. [2.x.1866]  

   [2.x.1867] [1.x.740] This strategy is quite similar   to the current one, but uses Legendre series expansion rather than the   Fourier one: instead of sinusoids as basis functions, this strategy uses   Legendre polynomials. Of course, since we approximate the solution using a   finite-dimensional polynomial on each cell, the expansion of the solution in   Legendre polynomials is also finite and, consequently, when we talk about the   "decay" of this expansion, we can only consider the finitely many nonzero   coefficients of this expansion, rather than think about it in asymptotic terms.   But, if we have enough of these coefficients, we can certainly think of the   decay of these coefficients as characteristic of the decay of the coefficients   of the exact solution (which is, in general, not polynomial and so will have an   infinite Legendre expansion), and considering the coefficients we have should   reveal something about the properties of the exact solution. 

  The transition from the Fourier strategy to the Legendre one is quite simple:   You just need to change the series expansion class and the corresponding   smoothness estimation function to be part of the proper namespaces    [2.x.1868]  and  [2.x.1869]  This strategy is used   in step-75. For the theoretical background of this strategy, consult the   general documentation of the  [2.x.1870]  namespace, as well   as  [2.x.1871]  ,  [2.x.1872]  and  [2.x.1873]  . [2.x.1874]  

   [2.x.1875] [1.x.741] The last strategy is quite different   from the other two. In theory, we know how the error will converge   after changing the discretization of the function space. With    [2.x.1876] -refinement the solution converges algebraically as already pointed   out in step-7. If the solution is sufficiently smooth, though, we   expect that the solution will converge exponentially with increasing   polynomial degree of the finite element. We can compare a proper   prediction of the error with the actual error in the following step to   see if our choice of adaptation type was justified. 

  The transition to this strategy is a bit more complicated. For this, we need   an initialization step with pure  [2.x.1877] - or  [2.x.1878] -refinement and we need to   transfer the predicted errors over adapted meshes. The extensive   documentation of the  [2.x.1879]  function describes not   only the theoretical details of this approach, but also presents a blueprint   on how to implement this strategy in your code. For more information, see    [2.x.1880]  . 

  Note that with this particular function you cannot predict the error for   the next time step in time-dependent problems. Therefore, this strategy   cannot be applied to this type of problem without further ado. Alternatively,   the following approach could be used, which works for all the other   strategies as well: start each time step with a coarse mesh, keep refining   until happy with the result, and only then move on to the next time step. [2.x.1881]   [2.x.1882]  

Try implementing one of these strategies into this tutorial and observe the subtle changes to the results. You will notice that all strategies are capable of identifying the singularities near the reentrant corners and will perform  [2.x.1883] -refinement in these regions, while preferring  [2.x.1884] -refinement in the bulk domain. A detailed comparison of these strategies is presented in  [2.x.1885]  . 




[1.x.742] 

All functionality presented in this tutorial already works for both sequential and parallel applications. It is possible without too much effort to change to either the  [2.x.1886]  or the  [2.x.1887]  classes. If you feel eager to try it, we recommend reading step-18 for the former and step-40 for the latter case first for further background information on the topic, and then come back to this tutorial to try out your newly acquired skills. 

We go one step further in step-75: Here, we combine hp-adapative and MatrixFree methods in combination with  [2.x.1888]  objects. 


examples/step-28/doc/intro.dox 

 [2.x.1889]  

[1.x.743][1.x.744][1.x.745] 

 [2.x.1890]  


[1.x.746] [1.x.747] In this example, we intend to solve the multigroup diffusion approximation of the neutron transport equation. Essentially, the way to view this is as follows: In a nuclear reactor, neutrons are speeding around at different energies, get absorbed or scattered, or start a new fission event. If viewed at long enough length scales, the movement of neutrons can be considered a diffusion process. 

A mathematical description of this would group neutrons into energy bins, and consider the balance equations for the neutron fluxes in each of these bins, or energy groups. The scattering, absorption, and fission events would then be operators within the diffusion equation describing the neutron fluxes. Assume we have energy groups  [2.x.1891] , where by convention we assume that the neutrons with the highest energy are in group 1 and those with the lowest energy in group  [2.x.1892] . Then the neutron flux of each group satisfies the following equations: 

[1.x.748] 

augmented by appropriate boundary conditions. Here,  [2.x.1893]  is the velocity of neutrons within group  [2.x.1894] . In other words, the change in time in flux of neutrons in group  [2.x.1895]  is governed by the following processes:  [2.x.1896]   [2.x.1897]  Diffusion  [2.x.1898] . Here,  [2.x.1899]  is the   (spatially variable) diffusion coefficient.  [2.x.1900]  Absorption  [2.x.1901]  (note the   negative sign). The coefficient  [2.x.1902]  is called the [1.x.749].  [2.x.1903]  Nuclear fission  [2.x.1904] .   The production of neutrons of energy  [2.x.1905]  is   proportional to the flux of neutrons of energy  [2.x.1906]  times the   probability  [2.x.1907]  that neutrons of energy  [2.x.1908]  cause a fission   event times the number  [2.x.1909]  of neutrons produced in each fission event   times the probability that a neutron produced in this event has energy    [2.x.1910] .  [2.x.1911]  is called the [1.x.750] and    [2.x.1912]  the [1.x.751]. We will denote the term    [2.x.1913]  as the [1.x.752] in the program.  [2.x.1914]  Scattering  [2.x.1915]    of neutrons of energy  [2.x.1916]  producing neutrons   of energy  [2.x.1917] .  [2.x.1918]  is called the [1.x.753]. The case of elastic, in-group scattering  [2.x.1919]  exists, too, but   we subsume this into the removal cross section. The case  [2.x.1920]  is called   down-scattering, since a neutron loses energy in such an event. On the   other hand,  [2.x.1921]  corresponds to up-scattering: a neutron gains energy in   a scattering event from the thermal motion of the atoms surrounding it;   up-scattering is therefore only an important process for neutrons with   kinetic energies that are already on the same order as the thermal kinetic   energy (i.e. in the sub  [2.x.1922]  range).  [2.x.1923]  An extraneous source  [2.x.1924] .  [2.x.1925]  

For realistic simulations in reactor analysis, one may want to split the continuous spectrum of neutron energies into many energy groups, often up to 100. However, if neutron energy spectra are known well enough for some type of reactor (for example Pressurized Water Reactors, PWR), it is possible to obtain satisfactory results with only 2 energy groups. 

In the program shown in this tutorial program, we provide the structure to compute with as many energy groups as desired. However, to keep computing times moderate and in order to avoid tabulating hundreds of coefficients, we only provide the coefficients for above equations for a two-group simulation, i.e.  [2.x.1926] . We do, however, consider a realistic situation by assuming that the coefficients are not constant, but rather depend on the materials that are assembled into reactor fuel assemblies in rather complicated ways (see below). 




[1.x.754] 

If we consider all energy groups at once, we may write above equations in the following operator form: 

[1.x.755] 

where  [2.x.1927]  are sinking, fission, and scattering operators, respectively.  [2.x.1928]  here includes both the diffusion and removal terms. Note that  [2.x.1929]  is symmetric, whereas  [2.x.1930]  and  [2.x.1931]  are not. 

It is well known that this equation admits a stable solution if all eigenvalues of the operator  [2.x.1932]  are negative. This can be readily seen by multiplying the equation by  [2.x.1933]  and integrating over the domain, leading to 

[1.x.756] 

Stability means that the solution does not grow, i.e. we want the left hand side to be less than zero, which is the case if the eigenvalues of the operator on the right are all negative. For obvious reasons, it is not very desirable if a nuclear reactor produces neutron fluxes that grow exponentially, so eigenvalue analyses are the bread-and-butter of nuclear engineers. The main point of the program is therefore to consider the eigenvalue problem 

[1.x.757] 

where we want to make sure that all eigenvalues are positive. Note that  [2.x.1934] , being the diffusion operator plus the absorption (removal), is positive definite; the condition that all eigenvalues are positive therefore means that we want to make sure that fission and inter-group scattering are weak enough to not shift the spectrum into the negative. 

In nuclear engineering, one typically looks at a slightly different formulation of the eigenvalue problem. To this end, we do not just multiply with  [2.x.1935]  and integrate, but rather multiply with  [2.x.1936] . We then get the following evolution equation: 

[1.x.758] 

Stability is then guaranteed if the eigenvalues of the following problem are all negative: 

[1.x.759] 

which is equivalent to the eigenvalue problem 

[1.x.760] 

The typical formulation in nuclear engineering is to write this as 

[1.x.761] 

where  [2.x.1937] . Intuitively,  [2.x.1938]  is something like the multiplication factor for neutrons per typical time scale and should be less than or equal to one for stable operation of a reactor: if it is less than one, the chain reaction will die down, whereas nuclear bombs for example have a  [2.x.1939] -eigenvalue larger than one. A stable reactor should have  [2.x.1940] . 

For those who wonder how this can be achieved in practice without inadvertently getting slightly larger than one and triggering a nuclear bomb: first, fission processes happen on different time scales. While most neutrons are released very quickly after a fission event, a small number of neutrons are only released by daughter nuclei after several further decays, up to 10-60 seconds after the fission was initiated. If one is therefore slightly beyond  [2.x.1941] , one therefore has many seconds to react until all the neutrons created in fission re-enter the fission cycle. Nevertheless, control rods in nuclear reactors absorbing neutrons -- and therefore reducing  [2.x.1942]  -- are designed in such a way that they are all the way in the reactor in at most 2 seconds. 

One therefore has on the order of 10-60 seconds to regulate the nuclear reaction if  [2.x.1943]  should be larger than one for some time, as indicated by a growing neutron flux. Regulation can be achieved by continuously monitoring the neutron flux, and if necessary increase or reduce neutron flux by moving neutron-absorbing control rods a few millimeters into or out of the reactor. On a longer scale, the water cooling the reactor contains boron, a good neutron absorber. Every few hours, boron concentrations are adjusted by adding boron or diluting the coolant. 

Finally, some of the absorption and scattering reactions have some stability built in; for example, higher neutron fluxes result in locally higher temperatures, which lowers the density of water and therefore reduces the number of scatterers that are necessary to moderate neutrons from high to low energies before they can start fission events themselves. 

In this tutorial program, we solve above  [2.x.1944] -eigenvalue problem for two energy groups, and we are looking for the largest multiplication factor  [2.x.1945] , which is proportional to the inverse of the minimum eigenvalue plus one. To solve the eigenvalue problem, we generally use a modified version of the [1.x.762]. The algorithm looks like this: 

<ol>  [2.x.1946]  Initialize  [2.x.1947]  and  [2.x.1948]  with  [2.x.1949]    and  [2.x.1950]  and let  [2.x.1951] . 

 [2.x.1952]  Define the so-called [1.x.763] by   [1.x.764] 



 [2.x.1953]  Solve for all group fluxes  [2.x.1954]  using   [1.x.765] 



 [2.x.1955]  Update   [1.x.766] 



 [2.x.1956]  Compare  [2.x.1957]  with  [2.x.1958] .   If the change greater than a prescribed tolerance then set  [2.x.1959]  repeat   the iteration starting at step 2, otherwise end the iteration.  [2.x.1960]  

Note that in this scheme, we do not solve group fluxes exactly in each power iteration, but rather consider previously compute  [2.x.1961]  only for down-scattering events  [2.x.1962] . Up-scattering is only treated by using old iterators  [2.x.1963] , in essence assuming that the scattering operator is triangular. This is physically motivated since up-scattering does not play a too important role in neutron scattering. In addition, practices shows that the inverse power iteration is stable even using this simplification. 

Note also that one can use lots of extrapolation techniques to accelerate the power iteration laid out above. However, none of these are implemented in this example. 




[1.x.767] 

One may wonder whether it is appropriate to solve for the solutions of the individual energy group equations on the same meshes. The question boils down to this: will  [2.x.1964]  and  [2.x.1965]  have similar smoothness properties? If this is the case, then it is appropriate to use the same mesh for the two; a typical application could be chemical combustion, where typically the concentrations of all or most chemical species change rapidly within the flame front. As it turns out, and as will be apparent by looking at the graphs shown in the results section of this tutorial program, this isn't the case here, however: since the diffusion coefficient is different for different energy groups, fast neutrons (in bins with a small group number  [2.x.1966] ) have a very smooth flux function, whereas slow neutrons (in bins with a large group number) are much more affected by the local material properties and have a correspondingly rough solution if the coefficient are rough as in the case we compute here. Consequently, we will want to use different meshes to compute each energy group. 

This has two implications that we will have to consider: First, we need to find a way to refine the meshes individually. Second, assembling the source terms for the inverse power iteration, where we have to integrate solution  [2.x.1967]  defined on mesh  [2.x.1968]  against the shape functions defined on mesh  [2.x.1969] , becomes a much more complicated task. 




[1.x.768] 

We use the usual paradigm: solve on a given mesh, then evaluate an error indicator for each cell of each mesh we have. Because it is so convenient, we again use the a posteriori error estimator by Kelly, Gago, Zienkiewicz and Babuska which approximates the error per cell by integrating the jump of the gradient of the solution along the faces of each cell. Using this, we obtain indicators 

[1.x.769] 

where  [2.x.1970]  is the triangulation used in the solution of  [2.x.1971] . The question is what to do with this. For one, it is clear that refining only those cells with the highest error indicators might lead to bad results. To understand this, it is important to realize that  [2.x.1972]  scales with the second derivative of  [2.x.1973] . In other words, if we have two energy groups  [2.x.1974]  whose solutions are equally smooth but where one is larger by a factor of 10,000, for example, then only the cells of that mesh will be refined, whereas the mesh for the solution of small magnitude will remain coarse. This is probably not what one wants, since we can consider both components of the solution equally important. 

In essence, we would therefore have to scale  [2.x.1975]  by an importance factor  [2.x.1976]  that says how important it is to resolve  [2.x.1977]  to any given accuracy. Such important factors can be computed using duality techniques (see, for example, the step-14 tutorial program, and the reference to the book by Bangerth and Rannacher cited there). We won't go there, however, and simply assume that all energy groups are equally important, and will therefore normalize the error indicators  [2.x.1978]  for group  [2.x.1979]  by the maximum of the solution  [2.x.1980] . We then refine the cells whose errors satisfy 

[1.x.770] 

and coarsen the cells where 

[1.x.771] 

We chose  [2.x.1981]  and  [2.x.1982]  in the code. Note that this will, of course, lead to different meshes for the different energy groups. 

The strategy above essentially means the following: If for energy group  [2.x.1983]  there are many cells  [2.x.1984]  on which the error is large, for example because the solution is globally very rough, then many cells will be above the threshold. On the other hand, if there are a few cells with large and many with small errors, for example because the solution is overall rather smooth except at a few places, then only the few cells with large errors will be refined. Consequently, the strategy allows for meshes that track the global smoothness properties of the corresponding solutions rather well. 




[1.x.772] 

As pointed out above, the multigroup refinement strategy results in different meshes for the different solutions  [2.x.1985] . So what's the problem? In essence it goes like this: in step 3 of the eigenvalue iteration, we have form the weak form for the equation to compute  [2.x.1986]  as usual by multiplication with test functions  [2.x.1987]  defined on the mesh for energy group  [2.x.1988] ; in the process, we have to compute the right hand side vector that contains terms of the following form: 

[1.x.773] 

where  [2.x.1989]  is one of the coefficient functions  [2.x.1990]  or  [2.x.1991]  used in the right hand side of eigenvalue equation. The difficulty now is that  [2.x.1992]  is defined on the mesh for energy group  [2.x.1993] , i.e. it can be expanded as  [2.x.1994] , with basis functions  [2.x.1995]  defined on mesh  [2.x.1996] . The contribution to the right hand side can therefore be written as 

[1.x.774] 

On the other hand, the test functions  [2.x.1997]  are defined on mesh  [2.x.1998] . This means that we can't just split the integral  [2.x.1999]  into integrals over the cells of either mesh  [2.x.2000]  or  [2.x.2001] , since the respectively other basis functions may not be defined on these cells. 

The solution to this problem lies in the fact that both the meshes for  [2.x.2002]  and  [2.x.2003]  are derived by adaptive refinement from a common coarse mesh. We can therefore always find a set of cells, which we denote by  [2.x.2004] , that satisfy the following conditions:  [2.x.2005]   [2.x.2006]  the union of the cells covers the entire domain, and  [2.x.2007]  a cell  [2.x.2008]  is active on at least   one of the two meshes.  [2.x.2009]  A way to construct this set is to take each cell of coarse mesh and do the following steps: (i) if the cell is active on either  [2.x.2010]  or  [2.x.2011] , then add this cell to the set; (ii) otherwise, i.e. if this cell has children on both meshes, then do step (i) for each of the children of this cell. In fact, deal.II has a function  [2.x.2012]  that computes exactly this set of cells that are active on at least one of two meshes. 

With this, we can write above integral as follows: 

[1.x.775] 

 In the code, we compute the right hand side in the function  [2.x.2013] , where (among other things) we loop over the set of common most refined cells, calling the function  [2.x.2014]  on each pair of these cells. 

By construction, there are now three cases to be considered: <ol>  [2.x.2015]  The cell  [2.x.2016]  is active on both meshes, i.e. both the basis   functions  [2.x.2017]  as well as  [2.x.2018]  are defined on  [2.x.2019] .  [2.x.2020]  The cell  [2.x.2021]  is active on mesh  [2.x.2022] , but not  [2.x.2023] , i.e. the    [2.x.2024]   are defined on  [2.x.2025] , whereas the  [2.x.2026]  are defined   on children of  [2.x.2027] .  [2.x.2028]  The cell  [2.x.2029]  is active on mesh  [2.x.2030] , but not  [2.x.2031] , with opposite   conclusions than in (ii).  [2.x.2032]  

To compute the right hand side above, we then need to have different code for these three cases, as follows: <ol>  [2.x.2033]  If the cell  [2.x.2034]  is active on both meshes, then we can directly   evaluate the integral. In fact, we don't even have to bother with the basis   functions  [2.x.2035] , since all we need is the values of  [2.x.2036]  at   the quadrature points. We can do this using the    [2.x.2037]  function. This is done directly in   the  [2.x.2038]  function. 

 [2.x.2039]  If the cell  [2.x.2040]  is active on mesh  [2.x.2041] , but not  [2.x.2042] , then the   basis functions  [2.x.2043]  are only defined either on the children    [2.x.2044] , or on children of these children if cell  [2.x.2045]    is refined more than once on mesh  [2.x.2046] . 

  Let us assume for a second that  [2.x.2047]  is only once more refined on mesh  [2.x.2048]    than on mesh  [2.x.2049] . Using the fact that we use embedded finite element spaces   where each basis function on one mesh can be written as a linear combination   of basis functions on the next refined mesh, we can expand the restriction   of  [2.x.2050]  to child cell  [2.x.2051]  into the basis functions defined on that   child cell (i.e. on cells on which the basis functions  [2.x.2052]  are   defined):   [1.x.776] 

  Here, and in the following, summation over indices appearing twice is   implied. The matrix  [2.x.2053]  is the matrix that interpolated data from a cell   to its  [2.x.2054] -th child. 

  Then we can write the contribution of cell  [2.x.2055]  to the right hand side   component  [2.x.2056]  as   [1.x.777] 

  In matrix notation, this can be written as   [1.x.778] 

  where  [2.x.2057]  is   the weighted mass matrix on child  [2.x.2058]  of cell  [2.x.2059] . 

  The next question is what happens if a child  [2.x.2060]  of  [2.x.2061]  is not   active. Then, we have to apply the process recursively, i.e. we have to   interpolate the basis functions  [2.x.2062]  onto child  [2.x.2063]  of  [2.x.2064] , then   onto child  [2.x.2065]  of that cell, onto child  [2.x.2066]  of that one, etc,   until we find an active cell. We then have to sum up all the contributions   from all the children, grandchildren, etc, of cell  [2.x.2067] , with contributions   of the form   [1.x.779] 

  or   [1.x.780] 

  etc. We do this process recursively, i.e. if we sit on cell  [2.x.2068]  and see that   it has children on grid  [2.x.2069] , then we call a function    [2.x.2070]  with an identity matrix; the function will   multiply it's argument from the left with the prolongation matrix; if the   cell has further children, it will call itself with this new matrix,   otherwise it will perform the integration. 

 [2.x.2071]  The last case is where  [2.x.2072]  is active on mesh  [2.x.2073]  but not mesh    [2.x.2074] . In that case, we have to express basis function  [2.x.2075]  in   terms of the basis functions defined on the children of cell  [2.x.2076] , rather   than  [2.x.2077]  as before. This of course works in exactly the same   way. If the children of  [2.x.2078]  are active on mesh  [2.x.2079] , then   leading to the expression   [1.x.781] 

  In matrix notation, this expression now reads as   [1.x.782] 

  and correspondingly for cases where cell  [2.x.2080]  is refined more than once on   mesh  [2.x.2081] :   [1.x.783] 

  or   [1.x.784] 

  etc. In other words, the process works in exactly the same way as before,   except that we have to take the transpose of the prolongation matrices and   need to multiply it to the mass matrix from the other side.  [2.x.2082]  


The expressions for cases (ii) and (iii) can be understood as repeatedly interpolating either the left or right basis functions in the scalar product  [2.x.2083]  onto child cells, and then finally forming the inner product (the mass matrix) on the final cell. To make the symmetry in these cases more obvious, we can write them like this: for case (ii), we have 

[1.x.785] 

whereas for case (iii) we get 

[1.x.786] 






[1.x.787] 

A nuclear reactor core is composed of different types of assemblies. An assembly is essentially the smallest unit that can be moved in and out of a reactor, and is usually rectangular or square. However, assemblies are not fixed units, as they are assembled from a complex lattice of different fuel rods, control rods, and instrumentation elements that are held in place relative to each other by spacers that are permanently attached to the rods. To make things more complicated, there are different kinds of assemblies that are used at the same time in a reactor, where assemblies differ in the type and arrangement of rods they are made up of. 

Obviously, the arrangement of assemblies as well as the arrangement of rods inside them affect the distribution of neutron fluxes in the reactor (a fact that will be obvious by looking at the solution shown below in the results sections of this program). Fuel rods, for example, differ from each other in the enrichment of U-235 or Pu-239. Control rods, on the other hand, have zero fission, but nonzero scattering and absorption cross sections. 

This whole arrangement would make the description or spatially dependent material parameters very complicated. It will not become much simpler, but we will make one approximation: we merge the volume inhabited by each cylindrical rod and the surrounding water into volumes of quadratic cross section into so-called `pin cells' for which homogenized material data are obtained with nuclear database and knowledge of neutron spectrum. The homogenization makes all material data piecewise constant on the solution domain for a reactor with fresh fuel. Spatially dependent material parameters are then looked up for the quadratic assembly in which a point is located, and then for the quadratic pin cell within this assembly. 

In this tutorial program, we simulate a quarter of a reactor consisting of  [2.x.2084]  assemblies. We use symmetry (Neumann) boundary conditions to reduce the problem to one quarter of the domain, and consequently only simulate a  [2.x.2085]  set of assemblies. Two of them will be UO [2.x.2086]  fuel, the other two of them MOX fuel. Each of these assemblies consists of  [2.x.2087]  rods of different compositions. In total, we therefore create a  [2.x.2088]  lattice of rods. To make things simpler later on, we reflect this fact by creating a coarse mesh of  [2.x.2089]  cells (even though the domain is a square, for which we would usually use a single cell). In deal.II, each cell has a  [2.x.2090]  which one may use to associated each cell with a particular number identifying the material from which this cell's volume is made of; we will use this material ID to identify which of the 8 different kinds of rods that are used in this testcase make up a particular cell. Note that upon mesh refinement, the children of a cell inherit the material ID, making it simple to track the material even after mesh refinement. 

The arrangement of the rods will be clearly visible in the images shown in the results section. The cross sections for materials and for both energy groups are taken from a OECD/NEA benchmark problem. The detailed configuration and material data is given in the code. 




[1.x.788] 

As a coarse overview of what exactly the program does, here is the basic layout: starting on a coarse mesh that is the same for each energy group, we compute inverse eigenvalue iterations to compute the  [2.x.2091] -eigenvalue on a given set of meshes. We stop these iterations when the change in the eigenvalue drops below a certain tolerance, and then write out the meshes and solutions for each energy group for inspection by a graphics program. Because the meshes for the solutions are different, we have to generate a separate output file for each energy group, rather than being able to add all energy group solutions into the same file. 

After this, we evaluate the error indicators as explained in one of the sections above for each of the meshes, and refine and coarsen the cells of each mesh independently. Since the eigenvalue iterations are fairly expensive, we don't want to start all over on the new mesh; rather, we use the SolutionTransfer class to interpolate the solution on the previous mesh to the next one upon mesh refinement. A simple experiment will convince you that this is a lot cheaper than if we omitted this step. After doing so, we resume our eigenvalue iterations on the next set of meshes. 

The program is controlled by a parameter file, using the ParameterHandler class. We will show a parameter file in the results section of this tutorial. For the moment suffice it to say that it controls the polynomial degree of the finite elements used, the number of energy groups (even though all that is presently implemented are the coefficients for a 2-group problem), the tolerance where to stop the inverse eigenvalue iteration, and the number of refinement cycles we will do. 


examples/step-28/doc/results.dox 



[1.x.789] 

We can run the program with the following input file: 

[1.x.790] 

The output of this program then consists of the console output, a file named `convergence_table' to record main results of mesh iteration, and the graphical output in vtu format. 

The console output looks like this: 

[1.x.791] 



We see that power iteration does converge faster after cycle 0 due to the initialization with solution from last mesh iteration. The contents of `convergence_table' are, 

[1.x.792] 

The meanings of columns are: number of mesh iteration, numbers of degrees of  freedom of fast energy group, numbers of DoFs of thermal group, converged k-effective and the ratio between maximum of fast flux and maximum of thermal one. 

The grids of fast and thermal energy groups at mesh iteration #9 look as follows: 

 [2.x.2092]  &nbsp;  [2.x.2093]  

We see that the grid of thermal group is much finer than the one of fast group. The solutions on these grids are, (Note: flux are normalized with total fission source equal to 1) 

 [2.x.2094]  &nbsp;  [2.x.2095]  

Then we plot the convergence data with polynomial order being equal to 1,2 and 3. 

 [2.x.2096]  

The estimated `exact' k-effective = 0.906834721253 which is simply from last mesh iteration of polynomial order 3 minus 2e-10. We see that h-adaptive calculations deliver an algebraic convergence. And the higher polynomial order is, the faster mesh iteration converges. In our problem, we need smaller number of DoFs to achieve same accuracy with higher polynomial order. 


examples/step-29/doc/intro.dox 

 [2.x.2097]  

[1.x.793] 

[1.x.794] In order to run this program, deal.II must be configured to use the UMFPACK sparse direct solver. Refer to the [1.x.795] for instructions how to do this. 


[1.x.796] 

[1.x.797] 


A question that comes up frequently is how to solve problems involving complex valued functions with deal.II. For many problems, instead of working with complex valued finite elements directly, it is often more convenient to split complex valued functions into their real and imaginary parts and use separate scalar finite element fields for discretizing each one of them. Basically this amounts to viewing a single complex valued equation as a system of two real valued equations. This short example demonstrates how this can be implemented in deal.II by using an  [2.x.2098]  object to stack two finite element fields representing real and imaginary parts. (The opposite approach, keeping everything complex-valued, is demonstrated in a different tutorial program: see step-58 for this.) When split into real and imaginary parts, the equations covered here fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.2099]  module. 

In addition to this discussion, we also discuss the ParameterHandler class, which provides a convenient way for reading parameters from a configuration file at runtime without the need to recompile the program code. 




[1.x.798] 

The original purpose of this program is to simulate the focusing properties of an ultrasound wave generated by a transducer lens with variable geometry. Recent applications in medical imaging use ultrasound waves not only for imaging purposes, but also to excite certain local effects in a material, like changes in optical properties, that can then be measured by other imaging techniques. A vital ingredient for these methods is the ability to focus the intensity of the ultrasound wave in a particular part of the material, ideally in a point, to be able to examine the properties of the material at that particular location. 

To derive a model for this problem, we think of ultrasound as a pressure wave governed by the wave equation: 

[1.x.799] 

where  [2.x.2100]  is the wave speed (that for simplicity we assume to be constant),  [2.x.2101] . The boundary  [2.x.2102]  is divided into two parts  [2.x.2103]  and  [2.x.2104] , with  [2.x.2105]  representing the transducer lens and  [2.x.2106]  an absorbing boundary (that is, we want to choose boundary conditions on  [2.x.2107]  in such a way that they imitate a larger domain). On  [2.x.2108] , the transducer generates a wave of constant frequency  [2.x.2109]  and constant amplitude (that we chose to be 1 here): 

[1.x.800] 



If there are no other (interior or boundary) sources, and since the only source has frequency  [2.x.2110] , then the solution admits a separation of variables of the form  [2.x.2111] . The complex-valued function  [2.x.2112]  describes the spatial dependency of amplitude and phase (relative to the source) of the waves of frequency  [2.x.2113] , with the amplitude being the quantity that we are interested in. By plugging this form of the solution into the wave equation, we see that for  [2.x.2114]  we have 

[1.x.801] 



For finding suitable conditions on  [2.x.2115]  that model an absorbing boundary, consider a wave of the form  [2.x.2116]  with frequency  [2.x.2117]  traveling in direction  [2.x.2118] . In order for  [2.x.2119]  to solve the wave equation,  [2.x.2120]  must hold. Suppose that this wave hits the boundary in  [2.x.2121]  at a right angle, i.e.  [2.x.2122]  with  [2.x.2123]  denoting the outer unit normal of  [2.x.2124]  in  [2.x.2125] . Then at  [2.x.2126] , this wave satisfies the equation 

[1.x.802] 

Hence, by enforcing the boundary condition 

[1.x.803] 

waves that hit the boundary  [2.x.2127]  at a right angle will be perfectly absorbed. On the other hand, those parts of the wave field that do not hit a boundary at a right angle do not satisfy this condition and enforcing it as a boundary condition will yield partial reflections, i.e. only parts of the wave will pass through the boundary as if it wasn't here whereas the remaining fraction of the wave will be reflected back into the domain. 

If we are willing to accept this as a sufficient approximation to an absorbing boundary we finally arrive at the following problem for  [2.x.2128] : 

[1.x.804] 

This is a Helmholtz equation (similar to the one in step-7, but this time with ''the bad sign'') with Dirichlet data on  [2.x.2129]  and mixed boundary conditions on  [2.x.2130] . Because of the condition on  [2.x.2131] , we cannot just treat the equations for real and imaginary parts of  [2.x.2132]  separately. What we can do however is to view the PDE for  [2.x.2133]  as a system of two PDEs for the real and imaginary parts of  [2.x.2134] , with the boundary condition on  [2.x.2135]  representing the coupling terms between the two components of the system. This works along the following lines: Let  [2.x.2136] , then in terms of  [2.x.2137]  and  [2.x.2138]  we have the following system: 

[1.x.805] 



For test functions  [2.x.2139]  with  [2.x.2140] , after the usual multiplication, integration over  [2.x.2141]  and applying integration by parts, we get the weak formulation 

[1.x.806] 



We choose finite element spaces  [2.x.2142]  and  [2.x.2143]  with bases  [2.x.2144]  and look for approximate solutions 

[1.x.807] 

Plugging into the variational form yields the equation system 

[1.x.808] 

In matrix notation: 

[1.x.809] 

(One should not be fooled by the right hand side being zero here, that is because we haven't included the Dirichlet boundary data yet.) Because of the alternating sign in the off-diagonal blocks, we can already see that this system is non-symmetric, in fact it is even indefinite. Of course, there is no necessity to choose the spaces  [2.x.2145]  and  [2.x.2146]  to be the same. However, we expect real and imaginary part of the solution to have similar properties and will therefore indeed take  [2.x.2147]  in the implementation, and also use the same basis functions  [2.x.2148]  for both spaces. The reason for the notation using different symbols is just that it allows us to distinguish between shape functions for  [2.x.2149]  and  [2.x.2150] , as this distinction plays an important role in the implementation. 




[1.x.810] 

For the computations, we will consider wave propagation in the unit square, with ultrasound generated by a transducer lens that is shaped like a segment of the circle with center at  [2.x.2151]  and a radius slightly greater than  [2.x.2152] ; this shape should lead to a focusing of the sound wave at the center of the circle. Varying  [2.x.2153]  changes the "focus" of the lens and affects the spatial distribution of the intensity of  [2.x.2154] , where our main concern is how well  [2.x.2155]  is focused. 

In the program below, we will implement the complex-valued Helmholtz equations using the formulation with split real and imaginary parts. We will also discuss how to generate a domain that looks like a square with a slight bulge simulating the transducer (in the  [2.x.2156]  function), and how to generate graphical output that not only contains the solution components  [2.x.2157]  and  [2.x.2158] , but also the magnitude  [2.x.2159]  directly in the output file (in  [2.x.2160] ). Finally, we use the ParameterHandler class to easily read parameters like the focal distance  [2.x.2161] , wave speed  [2.x.2162] , frequency  [2.x.2163] , and a number of other parameters from an input file at run-time, rather than fixing those parameters in the source code where we would have to re-compile every time we want to change parameters. 


examples/step-29/doc/results.dox 

[1.x.811] 

[1.x.812] 

The current program reads its run-time parameters from an input file called  [2.x.2164]  that looks like this: 

[1.x.813] 



As can be seen, we set  [2.x.2165] , which amounts to a focus of the transducer lens at  [2.x.2166] ,  [2.x.2167] . The coarse mesh is refined 5 times, resulting in 160x160 cells, and the output is written in vtu format. The parameter reader understands many more parameters pertaining in particular to the generation of output, but we need none of these parameters here and therefore stick with their default values. 

Here's the console output of the program in debug mode: 

[1.x.814] 



(Of course, execution times will differ if you run the program locally.) The fact that most of the time is spent on assembling the system matrix and generating output is due to the many assertions that need to be checked in debug mode. In release mode these parts of the program run much faster whereas solving the linear system is hardly sped up at all: 

[1.x.815] 



The graphical output of the program looks as follows: 


 [2.x.2168]  

The first two pictures show the real and imaginary parts of  [2.x.2169] , whereas the last shows the intensity  [2.x.2170] . One can clearly see that the intensity is focused around the focal point of the lens (0.5, 0.3), and that the focus is rather sharp in  [2.x.2171] -direction but more blurred in  [2.x.2172] -direction, which is a consequence of the geometry of the focusing lens, its finite aperture, and the wave nature of the problem. 

Because colorful graphics are always fun, and to stress the focusing effects some more, here is another set of images highlighting how well the intensity is actually focused in  [2.x.2173] -direction: 

 [2.x.2174]  


As a final note, the structure of the program makes it easy to determine which parts of the program scale nicely as the mesh is refined and which parts don't. Here are the run times for 5, 6, and 7 global refinements: 

[1.x.816] 



Each time we refine the mesh once, so the number of cells and degrees of freedom roughly quadruples from each step to the next. As can be seen, generating the grid, setting up degrees of freedom, assembling the linear system, and generating output scale pretty closely to linear, whereas solving the linear system is an operation that requires 8 times more time each time the number of degrees of freedom is increased by a factor of 4, i.e. it is  [2.x.2175] . This can be explained by the fact that (using optimal ordering) the bandwidth of a finite element matrix is  [2.x.2176] , and the effort to solve a banded linear system using LU decomposition is  [2.x.2177] . This also explains why the program does run in 3d as well (after changing the dimension on the  [2.x.2178]  object), but scales very badly and takes extraordinary patience before it finishes solving the linear system on a mesh with appreciable resolution, even though all the other parts of the program scale very nicely. 




[1.x.817] 

[1.x.818] 

An obvious possible extension for this program is to run it in 3d &mdash; after all, the world around us is three-dimensional, and ultrasound beams propagate in three-dimensional media. You can try this by simply changing the template parameter of the principal class in  [2.x.2179]  and running it. This won't get you very far, though: certainly not if you do 5 global refinement steps as set in the parameter file. You'll simply run out of memory as both the mesh (with its  [2.x.2180]  cells) and in particular the sparse direct solver take too much memory. You can solve with 3 global refinement steps, however, if you have a bit of time: in early 2011, the direct solve takes about half an hour. What you'll notice, however, is that the solution is completely wrong: the mesh size is simply not small enough to resolve the solution's waves accurately, and you can see this in plots of the solution. Consequently, this is one of the cases where adaptivity is indispensable if you don't just want to throw a bigger (presumably %parallel) machine at the problem. 


examples/step-3/doc/intro.dox 

[1.x.819] 

[1.x.820] 

 [2.x.2181]  

[1.x.821] 

This is the first example where we actually use finite elements to compute something. We will solve a simple version of Poisson's equation with zero boundary values, but a nonzero right hand side: 

[1.x.822] 

We will solve this equation on the square,  [2.x.2182] , for which you've already learned how to generate a mesh in step-1 and step-2. In this program, we will also only consider the particular case  [2.x.2183]  and come back to how to implement the more general case in the next tutorial program, step-4. 

If you've learned about the basics of the finite element method, you will remember the steps we need to take to approximate the solution  [2.x.2184]  by a finite dimensional approximation. Specifically, we first need to derive the weak form of the equation above, which we obtain by multiplying the equation by a test function  [2.x.2185]  [1.x.823] (we will come back to the reason for multiplying from the left and not from the right below) and integrating over the domain  [2.x.2186] : 

[1.x.824] 

This can be integrated by parts: 

[1.x.825] 

The test function  [2.x.2187]  has to satisfy the same kind of boundary conditions (in mathematical terms: it needs to come from the tangent space of the set in which we seek the solution), so on the boundary  [2.x.2188]  and consequently the weak form we are looking for reads 

[1.x.826] 

where we have used the common notation  [2.x.2189] . The problem then asks for a function  [2.x.2190]  for which this statement is true for all test functions  [2.x.2191]  from the appropriate space (which here is the space  [2.x.2192] ). 

Of course we can't find such a function on a computer in the general case, and instead we seek an approximation  [2.x.2193] , where the  [2.x.2194]  are unknown expansion coefficients we need to determine (the "degrees of freedom" of this problem), and  [2.x.2195]  are the finite element shape functions we will use. To define these shape functions, we need the following: 

- A mesh on which to define shape functions. You have already seen how to   generate and manipulate the objects that describe meshes in step-1 and   step-2. 

- A finite element that describes the shape functions we want to use on the   reference cell (which in deal.II is always the unit interval  [2.x.2196] , the   unit square  [2.x.2197]  or the unit cube  [2.x.2198] , depending on which space   dimension you work in). In step-2, we had already used an object of type   FE_Q<2>, which denotes the usual Lagrange elements that define shape   functions by interpolation on support points. The simplest one is   FE_Q<2>(1), which uses polynomial degree 1. In 2d, these are often referred   to as [1.x.827], since they are linear in each of the two coordinates   of the reference cell. (In 1d, they would be [1.x.828] and in 3d   [1.x.829]; however, in the deal.II documentation, we will frequently   not make this distinction and simply always call these functions "linear".) 

- A DoFHandler object that enumerates all the degrees of freedom on the mesh,   taking the reference cell description the finite element object provides as   the basis. You've also already seen how to do this in step-2. 

- A mapping that tells how the shape functions on the real cell are obtained   from the shape functions defined by the finite element class on the   reference cell. By default, unless you explicitly say otherwise, deal.II   will use a (bi-, tri-)linear mapping for this, so in most cases you don't   have to worry about this step. 

Through these steps, we now have a set of functions  [2.x.2199] , and we can define the weak form of the discrete problem: Find a function  [2.x.2200] , i.e., find the expansion coefficients  [2.x.2201]  mentioned above, so that 

[1.x.830] 

Note that we here follow the convention that everything is counted starting at zero, as common in C and C++. This equation can be rewritten as a linear system if you insert the representation  [2.x.2202]  and then observe that 

[1.x.831] 

With this, the problem reads: Find a vector  [2.x.2203]  so that 

[1.x.832] 

where the matrix  [2.x.2204]  and the right hand side  [2.x.2205]  are defined as 

[1.x.833] 






[1.x.834] 

Before we move on with describing how these quantities can be computed, note that if we had multiplied the original equation from the [1.x.835] by a test function rather than from the left, then we would have obtained a linear system of the form 

[1.x.836] 

with a row vector  [2.x.2206] . By transposing this system, this is of course equivalent to solving 

[1.x.837] 

which here is the same as above since  [2.x.2207] . But in general is not, and in order to avoid any sort of confusion, experience has shown that simply getting into the habit of multiplying the equation from the left rather than from the right (as is often done in the mathematical literature) avoids a common class of errors as the matrix is automatically correct and does not need to be transposed when comparing theory and implementation. See step-9 for the first example in this tutorial where we have a non-symmetric bilinear form for which it makes a difference whether we multiply from the right or from the left. 




[1.x.838] 

Now we know what we need (namely: objects that hold the matrix and vectors, as well as ways to compute  [2.x.2208] ), and we can look at what it takes to make that happen: 

- The object for  [2.x.2209]  is of type SparseMatrix while those for  [2.x.2210]  and  [2.x.2211]  are of   type Vector. We will see in the program below what classes are used to solve   linear systems. 

- We need a way to form the integrals. In the finite element method, this is   most commonly done using quadrature, i.e. the integrals are replaced by a   weighted sum over a set of points on each cell. That is, we first split the   integral over  [2.x.2212]  into integrals over all cells,   [1.x.839] 

  and then approximate each cell's contribution by quadrature:   [1.x.840] 

  where  [2.x.2213]  is the  [2.x.2214] th quadrature point on cell  [2.x.2215] , and  [2.x.2216]    the  [2.x.2217] th quadrature weight. There are different parts to what is needed in   doing this, and we will discuss them in turn next. 

- First, we need a way to describe the location  [2.x.2218]  of quadrature   points and their weights  [2.x.2219] . They are usually mapped from the reference   cell in the same way as shape functions, i.e., implicitly using the   MappingQ1 class or, if you explicitly say so, through one of the other   classes derived from Mapping. The locations and weights on the reference   cell are described by objects derived from the Quadrature base   class. Typically, one chooses a quadrature formula (i.e. a set of points and   weights) so that the quadrature exactly equals the integral in the matrix;   this can be achieved because all factors in the integral are polynomial, and   is done by Gaussian quadrature formulas, implemented in the QGauss class. 

- We then need something that can help us evaluate  [2.x.2220]    on cell  [2.x.2221] . This is what the FEValues class does: it takes a finite element   objects to describe  [2.x.2222]  on the reference cell, a quadrature object to   describe the quadrature points and weights, and a mapping object (or   implicitly takes the MappingQ1 class) and provides values and derivatives of   the shape functions on the real cell  [2.x.2223]  as well as all sorts of other   information needed for integration, at the quadrature points located on  [2.x.2224] . 

FEValues really is the central class in the assembly process. One way you can view it is as follows: The FiniteElement and derived classes describe shape [1.x.841], i.e., infinite dimensional objects: functions have values at every point. We need this for theoretical reasons because we want to perform our analysis with integrals over functions. However, for a computer, this is a very difficult concept, since they can in general only deal with a finite amount of information, and so we replace integrals by sums over quadrature points that we obtain by mapping (the Mapping object) using  points defined on a reference cell (the Quadrature object) onto points on the real cell. In essence, we reduce the problem to one where we only need a finite amount of information, namely shape function values and derivatives, quadrature weights, normal vectors, etc, exclusively at a finite set of points. The FEValues class is the one that brings the three components together and provides this finite set of information on a particular cell  [2.x.2225] . You will see it in action when we assemble the linear system below. 

It is noteworthy that all of this could also be achieved if you simply created these three objects yourself in an application program, and juggled the information yourself. However, this would neither be simpler (the FEValues class provides exactly the kind of information you actually need) nor faster: the FEValues class is highly optimized to only compute on each cell the particular information you need; if anything can be re-used from the previous cell, then it will do so, and there is a lot of code in that class to make sure things are cached wherever this is advantageous. 

The final piece of this introduction is to mention that after a linear system is obtained, it is solved using an iterative solver and then postprocessed: we create an output file using the DataOut class that can then be visualized using one of the common visualization programs. 

 [2.x.2226]  The preceding overview of all the important steps of any finite element implementation has its counterpart in deal.II: The library can naturally be grouped into a number of "modules" that cover the basic concepts just outlined. You can access these modules through the tab at the top of this page. An overview of the most fundamental groups of concepts is also available on the [1.x.842]. 




[1.x.843] 

Although this is the simplest possible equation you can solve using the finite element method, this program shows the basic structure of most finite element programs and also serves as the template that almost all of the following programs will essentially follow. Specifically, the main class of this program looks like this: 

[1.x.844] 



This follows the object oriented programming mantra of [1.x.845], i.e. we do our best to hide almost all internal details of this class in private members that are not accessible to the outside. 

Let's start with the member variables: These follow the building blocks we have outlined above in the bullet points, namely we need a Triangulation and a DoFHandler object, and a finite element object that describes the kinds of shape functions we want to use. The second group of objects relate to the linear algebra: the system matrix and right hand side as well as the solution vector, and an object that describes the sparsity pattern of the matrix. This is all this class needs (and the essentials that any solver for a stationary PDE requires) and that needs to survive throughout the entire program. In contrast to this, the FEValues object we need for assembly is only required throughout assembly, and so we create it as a local object in the function that does that and destroy it again at its end. 

Secondly, let's look at the member functions. These, as well, already form the common structure that almost all following tutorial programs will use:  [2.x.2227]     [2.x.2228]   [2.x.2229] : This is what one could call a        [1.x.846]. As its name suggests, it sets up the        object that stores the triangulation. In later examples, it could also        deal with boundary conditions, geometries, etc.    [2.x.2230]   [2.x.2231] : This then is the function in which all the        other data structures are set up that are needed to solve the        problem. In particular, it will initialize the DoFHandler object and        correctly size the various objects that have to do with the linear        algebra. This function is often separated from the preprocessing        function above because, in a time dependent program, it may be called        at least every few time steps whenever the mesh        is adaptively refined (something we will see how to do in step-6). On        the other hand, setting up the mesh itself in the preprocessing        function above is done only once at the beginning of the program and        is, therefore, separated into its own function.    [2.x.2232]   [2.x.2233] : This, then is where the contents of the        matrix and right hand side are computed, as discussed at length in the        introduction above. Since doing something with this linear system is        conceptually very different from computing its entries, we separate it        from the following function.    [2.x.2234]   [2.x.2235] : This then is the function in which we compute the        solution  [2.x.2236]  of the linear system  [2.x.2237] . In the current program, this        is a simple task since the matrix is so simple, but it will become a        significant part of a program's size whenever the problem is not so        trivial any more (see, for example, step-20, step-22, or step-31 once        you've learned a bit more about the library).    [2.x.2238]   [2.x.2239] : Finally, when you have computed a        solution, you probably want to do something with it. For example, you        may want to output it in a format that can be visualized, or you may        want to compute quantities you are interested in: say, heat fluxes in a        heat exchanger, air friction coefficients of a wing, maximum bridge        loads, or simply the value of the numerical solution at a point. This        function is therefore the place for postprocessing your solution.  [2.x.2240]  All of this is held together by the single public function (other than the constructor), namely the  [2.x.2241]  function. It is the one that is called from the place where an object of this type is created, and it is the one that calls all the other functions in their proper order. Encapsulating this operation into the  [2.x.2242]  function, rather than calling all the other functions from  [2.x.2243]  makes sure that you can change how the separation of concerns within this class is implemented. For example, if one of the functions becomes too big, you can split it up into two, and the only places you have to be concerned about changing as a consequence are within this very same class, and not anywhere else. 

As mentioned above, you will see this general structure &mdash; sometimes with variants in spelling of the functions' names, but in essentially this order of separation of functionality &mdash; again in many of the following tutorial programs. 




[1.x.847] 

deal.II defines a number of integral %types via alias in namespace  [2.x.2244]  (In the previous sentence, the word "integral" is used as the [1.x.848] that corresponds to the noun "integer". It shouldn't be confused with the [1.x.849] "integral" that represents the area or volume under a curve or surface. The adjective "integral" is widely used in the C++ world in contexts such as "integral type", "integral constant", etc.) In particular, in this program you will see  [2.x.2245]  in a couple of places: an integer type that is used to denote the [1.x.850] index of a degree of freedom, i.e., the index of a particular degree of freedom within the DoFHandler object that is defined on top of a triangulation (as opposed to the index of a particular degree of freedom within a particular cell). For the current program (as well as almost all of the tutorial programs), you will have a few thousand to maybe a few million unknowns globally (and, for  [2.x.2246]  elements, you will have 4 [1.x.851] in 2d and 8 in 3d). Consequently, a data type that allows to store sufficiently large numbers for global DoF indices is  [2.x.2247]  given that it allows to store numbers between 0 and slightly more than 4 billion (on most systems, where integers are 32-bit). In fact, this is what  [2.x.2248]  is. 

So, why not just use  [2.x.2249]  right away? deal.II used to do this until version 7.3. However, deal.II supports very large computations (via the framework discussed in step-40) that may have more than 4 billion unknowns when spread across a few thousand processors. Consequently, there are situations where  [2.x.2250]  is not sufficiently large and we need a 64-bit unsigned integral type. To make this possible, we introduced  [2.x.2251]  which by default is defined as simply <code>unsigned int</code> whereas it is possible to define it as <code>unsigned long long int</code> if necessary, by passing a particular flag during configuration (see the ReadMe file). 

This covers the technical aspect. But there is also a documentation purpose: everywhere in the library and codes that are built on it, if you see a place using the data type  [2.x.2252]  you immediately know that the quantity that is being referenced is, in fact, a global dof index. No such meaning would be apparent if we had just used  [2.x.2253]  (which may also be a local index, a boundary indicator, a material id, etc.). Immediately knowing what a variable refers to also helps avoid errors: it's quite clear that there must be a bug if you see an object of type  [2.x.2254]  being assigned to variable of type  [2.x.2255]  even though they are both represented by unsigned integers and the compiler will, consequently, not complain. 

In more practical terms what the presence of this type means is that during assembly, we create a  [2.x.2256]  matrix (in 2d, using a  [2.x.2257]  element) of the contributions of the cell we are currently sitting on, and then we need to add the elements of this matrix to the appropriate elements of the global (system) matrix. For this, we need to get at the global indices of the degrees of freedom that are local to the current cell, for which we will always use the following piece of the code: 

[1.x.852] 

where  [2.x.2258]  is declared as 

[1.x.853] 

The name of this variable might be a bit of a misnomer -- it stands for "the global indices of those degrees of freedom locally defined on the current cell" -- but variables that hold this information are universally named this way throughout the library. 

 [2.x.2259]   [2.x.2260]  is not the only type defined in this namespace. Rather, there is a whole family, including  [2.x.2261]   [2.x.2262]  and  [2.x.2263]  All of these are alias for integer data types but, as explained above, they are used throughout the library so that (i) the intent of a variable becomes more easily discerned, and (ii) so that it becomes possible to change the actual type to a larger one if necessary without having to go through the entire library and figure out whether a particular use of  [2.x.2264]  corresponds to, say, a material indicator. 


examples/step-3/doc/results.dox 



[1.x.854] 

The output of the program looks as follows: 

[1.x.855] 



The first two lines is what we wrote to  [2.x.2265] . The last two lines were generated without our intervention by the CG solver. The first two lines state the residual at the start of the iteration, while the last line tells us that the solver needed 47 iterations to bring the norm of the residual to 5.3e-13, i.e. below the threshold 1e-12 which we have set in the `solve' function. We will show in the next program how to suppress this output, which is sometimes useful for debugging purposes, but often clutters up the screen display. 

Apart from the output shown above, the program generated the file  [2.x.2266] , which is in the VTK format that is widely used by many visualization programs today -- including the two heavy-weights [1.x.856] and [1.x.857] that are the most commonly used programs for this purpose today. 

Using VisIt, it is not very difficult to generate a picture of the solution like this:  [2.x.2267]  It shows both the solution and the mesh, elevated above the  [2.x.2268] - [2.x.2269]  plane based on the value of the solution at each point. Of course the solution here is not particularly exciting, but that is a result of both what the Laplace equation represents and the right hand side  [2.x.2270]  we have chosen for this program: The Laplace equation describes (among many other uses) the vertical deformation of a membrane subject to an external (also vertical) force. In the current example, the membrane's borders are clamped to a square frame with no vertical variation; a constant force density will therefore intuitively lead to a membrane that simply bulges upward -- like the one shown above. 

VisIt and Paraview both allow playing with various kinds of visualizations of the solution. Several video lectures show how to use these programs.  [2.x.2271]  




[1.x.858] 

[1.x.859] 

If you want to play around a little bit with this program, here are a few suggestions:  [2.x.2272]  

 [2.x.2273]     [2.x.2274]    Change the geometry and mesh: In the program, we have generated a square   domain and mesh by using the  [2.x.2275]    function. However, the  [2.x.2276]  has a good number of other   functions as well. Try an L-shaped domain, a ring, or other domains you find   there.    [2.x.2277]  

   [2.x.2278]    Change the boundary condition: The code uses the  [2.x.2279]    function to generate zero boundary conditions. However, you may want to try   non-zero constant boundary values using    [2.x.2280]  instead of    [2.x.2281]  to have unit Dirichlet boundary   values. More exotic functions are described in the documentation of the   Functions namespace, and you may pick one to describe your particular boundary   values.    [2.x.2282]  

   [2.x.2283]  Modify the type of boundary condition: Presently, what happens   is that we use Dirichlet boundary values all around, since the   default is that all boundary parts have boundary indicator zero, and   then we tell the    [2.x.2284]  function to   interpolate boundary values to zero on all boundary components with   indicator zero.   [2.x.2285]  We can change this behavior if we assign parts   of the boundary different indicators. For example, try this   immediately after calling  [2.x.2286]    [1.x.860] 



  What this does is it first asks the triangulation to   return an iterator that points to the first active cell. Of course,   this being the coarse mesh for the triangulation of a square, the   triangulation has only a single cell at this moment, and it is   active. Next, we ask the cell to return an iterator to its first   face, and then we ask the face to reset the boundary indicator of   that face to 1. What then follows is this: When the mesh is refined,   faces of child cells inherit the boundary indicator of their   parents, i.e. even on the finest mesh, the faces on one side of the   square have boundary indicator 1. Later, when we get to   interpolating boundary conditions, the    [2.x.2287]  call will only produce boundary   values for those faces that have zero boundary indicator, and leave   those faces alone that have a different boundary indicator. What   this then does is to impose Dirichlet boundary conditions on the   former, and homogeneous Neumann conditions on the latter (i.e. zero   normal derivative of the solution, unless one adds additional terms   to the right hand side of the variational equality that deal with   potentially non-zero Neumann conditions). You will see this if you   run the program. 

  An alternative way to change the boundary indicator is to label   the boundaries based on the Cartesian coordinates of the face centers.   For example, we can label all of the cells along the top and   bottom boundaries with a boundary indicator 1 by checking to   see if the cell centers' y-coordinates are within a tolerance   (here 1e-12) of -1 and 1. Try this immediately after calling    [2.x.2288]  as before:   [1.x.861] 

  Although this code is a bit longer than before, it is useful for   complex geometries, as it does not require knowledge of face labels. 

   [2.x.2289]    A slight variation of the last point would be to set different boundary   values as above, but then use a different boundary value function for   boundary indicator one. In practice, what you have to do is to add a second   call to  [2.x.2290]  for boundary indicator one:   [1.x.862] 

  If you have this call immediately after the first one to this function, then   it will interpolate boundary values on faces with boundary indicator 1 to the   unit value, and merge these interpolated values with those previously   computed for boundary indicator 0. The result will be that we will get   discontinuous boundary values, zero on three sides of the square, and one on   the fourth. 

   [2.x.2291]    Observe convergence: We will only discuss computing errors in norms in   step-7, but it is easy to check that computations converge   already here. For example, we could evaluate the value of the solution in a   single point and compare the value for different %numbers of global   refinement (the number of global refinement steps is set in    [2.x.2292]  above). To evaluate the   solution at a point, say at  [2.x.2293] , we could add the   following code to the  [2.x.2294]  function:   [1.x.863] 

  For 1 through 9 global refinement steps, we then get the following sequence   of point values:    [2.x.2295]    By noticing that the difference between each two consecutive values reduces   by about a factor of 4, we can conjecture that the "correct" value may be    [2.x.2296] . In fact, if we assumed this to be   the correct value, we could show that the sequence above indeed shows  [2.x.2297]  convergence &mdash; theoretically, the convergence order should be    [2.x.2298]  but the symmetry of the domain and the mesh may lead   to the better convergence order observed. 

  A slight variant of this would be to repeat the test with quadratic   elements. All you need to do is to set the polynomial degree of the finite   element to two in the constructor    [2.x.2299] . 

   [2.x.2300] Convergence of the mean: A different way to see that the solution   actually converges (to something &mdash; we can't tell whether it's really   the correct value!) is to compute the mean of the solution. To this end, add   the following code to  [2.x.2301] :   [1.x.864] 

  The documentation of the function explains what the second and fourth   parameters mean, while the first and third should be obvious. Doing the same   study again where we change the number of global refinement steps, we get   the following result:    [2.x.2302]    Again, the difference between two adjacent values goes down by about a   factor of four, indicating convergence as  [2.x.2303] .  [2.x.2304]  




[1.x.865] 

%HDF5 is a commonly used format that can be read by many scripting languages (e.g. R or Python). It is not difficult to get deal.II to produce some %HDF5 files that can then be used in external scripts to postprocess some of the data generated by this program. Here are some ideas on what is possible. 




[1.x.866] 

To fully make use of the automation we first need to introduce a private variable for the number of global refinement steps  [2.x.2305] , which will be used for the output filename. In  [2.x.2306]  with 

[1.x.867] 

The deal.II library has two different %HDF5 bindings, one in the HDF5 namespace (for interfacing to general-purpose data files) and another one in DataOut (specifically for writing files for the visualization of solutions). Although the HDF5 deal.II binding supports both serial and MPI, the %HDF5 DataOut binding only supports parallel output. For this reason we need to initialize an MPI communicator with only one processor. This is done by adding the following code. 

[1.x.868] 

Next we change the  [2.x.2307]  output routine as described in the DataOutBase namespace documentation: 

[1.x.869] 

The resulting file can then be visualized just like the VTK file that the original version of the tutorial produces; but, since %HDF5 is a more general file format, it can also easily be processed in scripting languages for other purposes. 




[1.x.870] 

After outputting the solution, the file can be opened again to include more datasets.  This allows us to keep all the necessary information of our experiment in a single result file, which can then be read and processed by some postprocessing script. (Have a look at  [2.x.2308]  for further information on the possible output options.) 

To make this happen, we first include the necessary header into our file: 

[1.x.871] 

Adding the following lines to the end of our output routine adds the information about the value of the solution at a particular point, as well as the mean value of the solution, to our %HDF5 file: 

[1.x.872] 






[1.x.873] 

The data put into %HDF5 files above can then be used from scripting languages for further postprocessing. In the following, let us show how this can, in particular, be done with the [1.x.874], a widely used language in statistical data analysis. (Similar things can also be done in Python, for example.) If you are unfamiliar with R and ggplot2 you could check out the data carpentry course on R [1.x.875]. Furthermore, since most search engines struggle with searches of the form "R + topic", we recommend using the specializes service [1.x.876] instead. 

The most prominent difference between R and other languages is that the assignment operator (`a = 5`) is typically written as `a <- 5`. As the latter is considered standard we will use it in our examples as well. To open the `.h5` file in R you have to install the [1.x.877] package, which is a part of the Bioconductor package. 

First we will include all necessary packages and have a look at how the data is structured in our file. 

[1.x.878] 

This gives the following output 

[1.x.879] 

The datasets can be accessed by  [2.x.2309] . The function  [2.x.2310]  gives us the dimensions of the matrix that is used to store our cells. We can see the following three matrices, as well as the two additional data points we added.  [2.x.2311]   [2.x.2312]   [2.x.2313] : a 4x1024 matrix that stores the  (C++) vertex indices for each cell  [2.x.2314]   [2.x.2315] : a 2x1089 matrix storing the position values (x,y) for our cell vertices  [2.x.2316]   [2.x.2317] : a 1x1089 matrix storing the values of our solution at each vertex  [2.x.2318]  Now we can use this data to generate various plots. Plotting with ggplot2 usually splits into two steps. At first the data needs to be manipulated and added to a  [2.x.2319] . After that, a  [2.x.2320]  object is constructed and manipulated by adding plot elements to it. 

 [2.x.2321]  contain all the information we need to plot our grid. The following code wraps all the data into one dataframe for plotting our grid: 

[1.x.880] 



With the finished dataframe we have everything we need to plot our grid: 

[1.x.881] 



The contents of this file then look as follows (not very exciting, but you get the idea):  [2.x.2322]  

We can also visualize the solution itself, and this is going to look more interesting. To make a 2D pseudocolor plot of our solution we will use  [2.x.2323] . This function needs a structured grid, i.e. uniform in x and y directions. Luckily our data at this point is structured in the right way. The following code plots a pseudocolor representation of our surface into a new PDF: 

[1.x.882] 

This is now going to look as follows:  [2.x.2324]  

For plotting the converge curves we need to re-run the C++ code multiple times with different values for  [2.x.2325]  starting from 1. Since every file only contains a single data point we need to loop over them and concatenate the results into a single vector. 

[1.x.883] 

As we are not interested in the values themselves but rather in the error compared to a "exact" solution we will assume our highest refinement level to be that solution and omit it from the data. 

[1.x.884] 

Now we have all the data available to generate our plots. It is often useful to plot errors on a log-log scale, which is accomplished in the following code: 

[1.x.885] 

This results in the following plot that shows how the errors in the mean value and the solution value at the chosen point nicely converge to zero:  [2.x.2326]  


examples/step-30/doc/intro.dox 

[1.x.886] 

[1.x.887] 




[1.x.888] 

This example is devoted to  [2.x.2327] anisotropic refinement [2.x.2328] , which extends to possibilities of local refinement. In most parts, this is a modification of the step-12 tutorial program, we use the same DG method for a linear transport equation. This program will cover the following topics: <ol>    [2.x.2329]   [2.x.2330] Anisotropic refinement [2.x.2331] : What is the meaning of anisotropic refinement?    [2.x.2332]   [2.x.2333] Implementation [2.x.2334] : Necessary modifications of code to work with anisotropically refined meshes.    [2.x.2335]   [2.x.2336] Jump indicator [2.x.2337] : A simple indicator for anisotropic refinement in   the context of DG methods.  [2.x.2338]  The discretization itself will not be discussed, and neither will implementation techniques not specific to anisotropic refinement used here. Please refer to step-12 for this. 

Please note, at the moment of writing this tutorial program, anisotropic refinement is only fully implemented for discontinuous Galerkin Finite Elements. This may later change (or may already have). 




 [2.x.2339]  While this program is a modification of step-12, it is an adaptation of a version of step-12 written early on in the history of deal.II when the MeshWorker framework wasn't available yet. Consequently, it bears little resemblance to the step-12 as it exists now, apart from the fact that it solves the same equation with the same discretization. 




[1.x.889] 

All the adaptive processes in the preceding tutorial programs were based on  [2.x.2340] isotropic [2.x.2341]  refinement of cells, which cuts all edges in half and forms new cells of these split edges (plus some additional edges, faces and vertices, of course). In deal.II,  [2.x.2342] anisotropic refinement [2.x.2343]  refers to the process of splitting only part of the edges while leaving the others unchanged. Consider a simple square cell, for example: 

[1.x.890] 

After the usual refinement it will consist of four children and look like this: 

[1.x.891] 

The new anisotropic refinement may take two forms: either we can split the edges which are parallel to the horizontal x-axis, resulting in these two child cells: 

[1.x.892] 

or we can split the two edges which run along the y-axis, resulting again in two children, which look that way, however: 

[1.x.893] 

All refinement cases of cells are described by an enumeration  [2.x.2344]  and the above anisotropic cases are called  [2.x.2345]  and  [2.x.2346]  for obvious reasons. The isotropic refinement case is called  [2.x.2347]  in 2D and can be requested from the RefinementCase class via  [2.x.2348]  

In 3D, there is a third axis which can be split, the z-axis, and thus we have an additional refinement case  [2.x.2349]  here. Isotropic refinement will now refine a cell along the x-, y- and z-axes and thus be referred to as  [2.x.2350]  cut_xyz. Additional cases  [2.x.2351]   [2.x.2352]  and  [2.x.2353]  exist, which refine a cell along two of the axes, but not along the third one. Given a hex cell with x-axis running to the right, y-axis 'into the page' and z-axis to the top, 

[1.x.894] 

we have the isotropic refinement case, 

[1.x.895] 

three anisotropic cases which refine only one axis: 

[1.x.896] 

and three cases which refine two of the three axes: 

[1.x.897] 

For 1D problems, anisotropic refinement can make no difference, as there is only one coordinate direction for a cell, so it is not possible to split it in any other way than isotropically. 

[1.x.898] Adaptive local refinement is used to obtain fine meshes which are well adapted to solving the problem at hand efficiently. In short, the size of cells which produce a large error is reduced to obtain a better approximation of the solution to the problem at hand. However, a lot of problems contain anisotropic features. Prominent examples are shocks or boundary layers in compressible viscous flows. An efficient mesh approximates these features with cells of higher aspect ratio which are oriented according to the mentioned features. Using only isotropic refinement, the aspect ratios of the original mesh cells are preserved, as they are inherited by the children of a cell. Thus, starting from an isotropic mesh, a boundary layer will be refined in order to catch the rapid variation of the flow field in the wall normal direction, thus leading to cells with very small edge lengths both in normal and tangential direction. Usually, much higher edge lengths in tangential direction and thus significantly less cells could be used without a significant loss in approximation accuracy. An anisotropic refinement process can modify the aspect ratio from mother to child cells by a factor of two for each refinement step. In the course of several refinements, the aspect ratio of the fine cells can be optimized, saving a considerable number of cells and correspondingly degrees of freedom and thus computational resources, memory as well as CPU time. 

[1.x.899] 

Most of the time, when we do finite element computations, we only consider one cell at a time, for example to calculate cell contributions to the global matrix, or to interpolate boundary values. However, sometimes we have to look at how cells are related in our algorithms. Relationships between cells come in two forms: neighborship and mother-child relationship. For the case of isotropic refinement, deal.II uses certain conventions (invariants) for cell relationships that are always maintained. For example, a refined cell always has exactly  [2.x.2354]  children. And (except for the 1d case), two neighboring cells may differ by at most one refinement level: they are equally often refined or one of them is exactly once more refined, leaving exactly one hanging node on the common face. Almost all of the time these invariants are only of concern in the internal implementation of the library. However, there are cases where knowledge of them is also relevant to an application program. 

In the current context, it is worth noting that the kind of mesh refinement affects some of the most fundamental assumptions. Consequently, some of the usual code found in application programs will need modifications to exploit the features of meshes which were created using anisotropic refinement. For those interested in how deal.II evolved, it may be of interest that the loosening of such invariants required some incompatible changes. For example, the library used to have a member  [2.x.2355]  that specified how many children a cell has once it is refined. For isotropic refinement, this number is equal to  [2.x.2356] , as mentioned above. However, for anisotropic refinement, this number does not exist, as is can be either two or four in 2D and two, four or eight in 3D, and the member  [2.x.2357]  has consequently been removed. It has now been replaced by  [2.x.2358]  which specifies the [1.x.900] number of children a cell can have. How many children a refined cell has was previously available as static information, but now it depends on the actual refinement state of a cell and can be retrieved using  [2.x.2359]  a call that works equally well for both isotropic and anisotropic refinement. A very similar situation can be found for faces and their subfaces: the pertinent information can be queried using  [2.x.2360]  or  [2.x.2361] , depending on the context. 

Another important aspect, and the most important one in this tutorial, is the treatment of neighbor-relations when assembling jump terms on the faces between cells. Looking at the documentation of the assemble_system functions in step-12 we notice, that we need to decide if a neighboring cell is coarser, finer or on the same (refinement) level as our current cell. These decisions do not work in the same way for anisotropic refinement as the information given by the  [2.x.2362] level [2.x.2363]  of a cell is not enough to completely characterize anisotropic cells; for example, are the terminal children of a two-dimensional cell that is first cut in  [2.x.2364] -direction and whose children are then cut in  [2.x.2365] -direction on level 2, or are they on level 1 as they would be if the cell would have been refined once isotropically, resulting in the same set of finest cells? 

After anisotropic refinement, a coarser neighbor is not necessarily exactly one level below ours, but can pretty much have any level relative to the current one; in fact, it can even be on a higher level even though it is coarser. Thus the decisions have to be made on a different basis, whereas the intention of the decisions stays the same. 

In the following, we will discuss the cases that can happen when we want to compute contributions to the matrix (or right hand side) of the form 

[1.x.901] 

or similar; remember that we integrate terms like this using the FEFaceValues and FESubfaceValues classes. We will also show how to write code that works for both isotropic and anisotropic refinement: 

 [2.x.2366]  

   [2.x.2367]   [2.x.2368] Finer neighbor [2.x.2369] : If we are on an active cell and want   to integrate over a face  [2.x.2370] , the first   possibility is that the neighbor behind this face is more refined,   i.e. has children occupying only part of the   common face. In this case, the face   under consideration has to be a refined one, which can determine by   asking  [2.x.2371] . If this is true, we need to   loop over   all subfaces and get the neighbors' child behind this subface, so that we can   reinit an FEFaceValues object with the neighbor and an FESubfaceValues object   with our cell and the respective subface. 

  For isotropic refinement, this kind is reasonably simple because we   know that an invariant of the isotropically refined adaptive meshes   in deal.II is that neighbors can only differ by exactly one   refinement level. However, this isn't quite true any more for   anisotropically refined meshes, in particular in 3d; there,   the active cell we are interested on the other side of  [2.x.2372]  might not   actually be a child of our   neighbor, but perhaps a grandchild or even a farther offspring. Fortunately,   this complexity is hidden in the internals of the library. All we need to do   is call the  [2.x.2373]    function. Still, in 3D there are two cases which need special consideration:    [2.x.2374]       [2.x.2375]  If the neighbor is refined more than once anisotropically, it might be   that here are not two or four but actually three subfaces to   consider. Imagine   the following refinement process of the (two-dimensional) face of   the (three-dimensional) neighbor cell we are considering: first the   face is refined along x, later on only the left subface is refined along y. 

[1.x.902] 

     Here the number of subfaces is three. It is important to note the subtle   differences between, for a face,  [2.x.2376]  and    [2.x.2377]  The first function returns the number of   immediate children, which would be two for the above example, whereas the   second returns the number of active offspring (i.e., including children,   grandchildren, and further descendants), which is the correct three in   the example above. Using  [2.x.2378]  works for   isotropic and anisotropic as well as 2D and 3D cases, so it should always be   used. It should be noted that if any of the cells behind the two   small subfaces on the left side of the rightmost image is further   refined, then the current cell (i.e. the side from which we are   viewing this common face) is going to be refined as well: this is so   because otherwise the invariant of having only one hanging node per   edge would be violated. 

     [2.x.2379]  It might be, that the neighbor is coarser, but still has children which   are finer than our current cell. This situation can occur if two equally   coarse cells are refined, where one of the cells has two children at the face   under consideration and the other one four. The cells in the next graphic are   only separated from each other to show the individual refinement cases. 

[1.x.903] 



  Here, the left two cells resulted from an anisotropic bisection of   the mother cell in  [2.x.2380] -direction, whereas the right four cells   resulted from a simultaneous anisotropic refinement in both the  [2.x.2381] -   and  [2.x.2382] -directions.   The left cell marked with # has two finer neighbors marked with +, but the   actual neighbor of the left cell is the complete right mother cell, as the   two cells marked with + are finer and their direct mother is the one   large cell.    [2.x.2383]  

  However, fortunately,  [2.x.2384]  takes care of   these situations by itself, if you loop over the correct number of subfaces,   in the above example this is two. The  [2.x.2385]  function   takes care of this too, so that the resulting state is always correct. There   is one little caveat, however: For reiniting the neighbors FEFaceValues object   you need to know the index of the face that points toward the current   cell. Usually you assume that the neighbor you get directly is as coarse or as   fine as you, if it has children, thus this information can be obtained with    [2.x.2386]  If the neighbor is coarser, however, you   would have to use the first value in  [2.x.2387]    instead. In order to make this easy for you, there is    [2.x.2388]  which does the correct thing for you and   returns the desired result. 

   [2.x.2389]   [2.x.2390] Neighbor is as fine as our cell [2.x.2391] : After we ruled out all cases in   which there are finer children, we only need to decide, whether the neighbor   is coarser here. For this, there is the    [2.x.2392]  function which returns a boolean. In   order to get the relevant case of a neighbor of the same coarseness we would   use  [2.x.2393] . The code inside this   block can be left untouched. However, there is one thing to mention here: If   we want to use a rule, which cell should assemble certain terms on a given   face we might think of the rule presented in step-12. We know that we have to   leave out the part about comparing our cell's level with that of the neighbor   and replace it with the test for a coarser neighbor presented above. However,   we also have to consider the possibility that neighboring cells of same   coarseness have the same index (on different levels). Thus we have to include   the case where the cells have the same index, and give an additional   condition, which of the cells should assemble the terms, e.g. we can choose   the cell with lower level. The details of this concept can be seen in the   implementation below. 

   [2.x.2394]   [2.x.2395] Coarser neighbor [2.x.2396] : The remaining case is obvious: If there are no   refined neighbors and the neighbor is not as fine as the current cell, then it must   be coarser. Thus we can leave the old condition phrase, simply using    [2.x.2397] . The  [2.x.2398]    function takes care of all the complexity of anisotropic refinement combined   with possible non standard face orientation, flip and rotation on general 3D meshes. 

 [2.x.2399]  

[1.x.904] When a triangulation is refined, cells which were not flagged for refinement may be refined nonetheless. This is due to additional smoothing algorithms which are either necessary or requested explicitly. In particular, the restriction that there be at most one hanging node on each edge frequently forces the refinement of additional cells neighboring ones that are already finer and are flagged for further refinement. 

However, deal.II also implements a number of algorithms that make sure that resulting meshes are smoother than just the bare minimum, for example ensuring that there are no isolated refined cells surrounded by non-refined ones, since the additional degrees of freedom on these islands would almost all be constrained by hanging node constraints. (See the documentation of the Triangulation class and its  [2.x.2400]  member for more information on mesh smoothing.) 

Most of the smoothing algorithms that were originally developed for the isotropic case have been adapted to work in a very similar way for both anisotropic and isotropic refinement. There are two algorithms worth mentioning, however: <ol>    [2.x.2401]   [2.x.2402] : In an isotropic environment,   this algorithm tries to ensure a good approximation quality by reducing the   difference in refinement level of cells meeting at a common vertex. However,   there is no clear corresponding concept for anisotropic refinement, thus this   algorithm may not be used in combination with anisotropic refinement. This   restriction is enforced by an assertion which throws an error as soon as the   algorithm is called on a triangulation which has been refined anisotropically. 

   [2.x.2403]   [2.x.2404] : If refinement is introduced to   limit the number of hanging nodes, the additional cells are often not needed   to improve the approximation quality. This is especially true for DG   methods. If you set the flag  [2.x.2405]  the   smoothing algorithm tries to minimize the number of probably unneeded   additional cells by using anisotropic refinement for the smoothing. If you set   this smoothing flag you might get anisotropically refined cells, even if you   never set a single refinement flag to anisotropic refinement. Be aware that   you should only use this flag, if your code respects the possibility of   anisotropic meshes. Combined with a suitable anisotropic indicator this flag   can help save additional cells and thus effort.  [2.x.2406]  




[1.x.905] 

Using the benefits of anisotropic refinement requires an indicator to catch anisotropic features of the solution and exploit them for the refinement process. Generally the anisotropic refinement process will consist of several steps: <ol>    [2.x.2407]  Calculate an error indicator.    [2.x.2408]  Use the error indicator to flag cells for refinement, e.g. using a fixed   number or fraction of cells. Those cells will be flagged for isotropic   refinement automatically.    [2.x.2409]  Evaluate a distinct anisotropic indicator only on the flagged cells.    [2.x.2410]  Use the anisotropic indicator to set a new, anisotropic refinement flag   for cells where this is appropriate, leave the flags unchanged otherwise.    [2.x.2411]  Call  [2.x.2412]  to perform the   requested refinement, using the requested isotropic and anisotropic flags.  [2.x.2413]  This approach is similar to the one we have used in step-27 for hp-refinement and has the great advantage of flexibility: Any error indicator can be used in the anisotropic process, i.e. if you have quite involved a posteriori goal-oriented error indicators available you can use them as easily as a simple Kelly error estimator. The anisotropic part of the refinement process is not influenced by this choice. Furthermore, simply leaving out the third and forth steps leads to the same isotropic refinement you used to get before any anisotropic changes in deal.II or your application program. As a last advantage, working only on cells flagged for refinement results in a faster evaluation of the anisotropic indicator, which can become noticeable on finer meshes with a lot of cells if the indicator is quite involved. 

Here, we use a very simple approach which is only applicable to DG methods. The general idea is quite simple: DG methods allow the discrete solution to jump over the faces of a cell, whereas it is smooth within each cell. Of course, in the limit we expect that the jumps tend to zero as we refine the mesh and approximate the true solution better and better. Thus, a large jump across a given face indicates that the cell should be refined (at least) orthogonally to that face, whereas a small jump does not lead to this conclusion. It is possible, of course, that the exact solution is not smooth and that it also features a jump. In that case, however, a large jump over one face indicates, that this face is more or less parallel to the jump and in the vicinity of it, thus again we would expect a refinement orthogonal to the face under consideration to be effective. 

The proposed indicator calculates the average jump  [2.x.2414] , i.e. the mean value of the absolute jump  [2.x.2415]  of the discrete solution  [2.x.2416]  over the two faces  [2.x.2417] ,  [2.x.2418] ,  [2.x.2419]  orthogonal to coordinate direction  [2.x.2420]  on the unit cell. 

[1.x.906] 

If the average jump in one direction is larger than the average of the jumps in the other directions by a certain factor  [2.x.2421] , i.e. if  [2.x.2422] , the cell is refined only along that particular direction  [2.x.2423] , otherwise the cell is refined isotropically. 

Such a criterion is easily generalized to systems of equations: the absolute value of the jump would be replaced by an appropriate norm of the vector-valued jump. 




[1.x.907] 

We solve the linear transport equation presented in step-12. The domain is extended to cover  [2.x.2424]  in 2D, where the flow field  [2.x.2425]  describes a counterclockwise quarter circle around the origin in the right half of the domain and is parallel to the x-axis in the left part of the domain. The inflow boundary is again located at  [2.x.2426]  and along the positive part of the x-axis, and the boundary conditions are chosen as in step-12. 


examples/step-30/doc/results.dox 



[1.x.908] 


The output of this program consist of the console output, the SVG files containing the grids, and the solutions given in VTU format. 

[1.x.909] 



This text output shows the reduction in the number of cells which results from the successive application of anisotropic refinement. After the last refinement step the savings have accumulated so much that almost four times as many cells and thus degrees of freedom are needed in the isotropic case. The time needed for assembly scales with a similar factor. 

The first interesting part is of course to see how the meshes look like. On the left are the isotropically refined ones, on the right the anisotropic ones (colors indicate the refinement level of cells): 

 [2.x.2427]  


The other interesting thing is, of course, to see the solution on these two sequences of meshes. Here they are, on the refinement cycles 1 and 4, clearly showing that the solution is indeed composed of [1.x.910] piecewise polynomials: 

 [2.x.2428]  

We see, that the solution on the anisotropically refined mesh is very similar to the solution obtained on the isotropically refined mesh. Thus the anisotropic indicator seems to effectively select the appropriate cells for anisotropic refinement. 

The pictures also explain why the mesh is refined as it is. In the whole left part of the domain refinement is only performed along the  [2.x.2429] -axis of cells. In the right part of the domain the refinement is dominated by isotropic refinement, as the anisotropic feature of the solution - the jump from one to zero - is not well aligned with the mesh where the advection direction takes a turn. However, at the bottom and closest (to the observer) parts of the quarter circle this jumps again becomes more and more aligned with the mesh and the refinement algorithm reacts by creating anisotropic cells of increasing aspect ratio. 

It might seem that the necessary alignment of anisotropic features and the coarse mesh can decrease performance significantly for real world problems. That is not wrong in general: If one were, for example, to apply anisotropic refinement to problems in which shocks appear (e.g., the equations solved in step-69), then it many cases the shock is not aligned with the mesh and anisotropic refinement will help little unless one also introduces techniques to move the mesh in alignment with the shocks. On the other hand, many steep features of solutions are due to boundary layers. In those cases, the mesh is already aligned with the anisotropic features because it is of course aligned with the boundary itself, and anisotropic refinement will almost always increase the efficiency of computations on adapted grids for these cases. 


examples/step-31/doc/intro.dox 

 [2.x.2430]  

[1.x.911] 


[1.x.912] 

[1.x.913] 

[1.x.914] 

This program deals with an interesting physical problem: how does a fluid (i.e., a liquid or gas) behave if it experiences differences in buoyancy caused by temperature differences? It is clear that those parts of the fluid that are hotter (and therefore lighter) are going to rise up and those that are cooler (and denser) are going to sink down with gravity. 

In cases where the fluid moves slowly enough such that inertial effects can be neglected, the equations that describe such behavior are the Boussinesq equations that read as follows: 

[1.x.915] 

These equations fall into the class of vector-valued problems (a toplevel overview of this topic can be found in the  [2.x.2431]  module). Here,  [2.x.2432]  is the velocity field,  [2.x.2433]  the pressure, and  [2.x.2434]  the temperature of the fluid.  [2.x.2435]  is the symmetric gradient of the velocity. As can be seen, velocity and pressure solve a Stokes equation describing the motion of an incompressible fluid, an equation we have previously considered in step-22; we will draw extensively on the experience we have gained in that program, in particular with regard to efficient linear Stokes solvers. 

The forcing term of the fluid motion is the buoyancy of the fluid, expressed as the product of the density  [2.x.2436] , the thermal expansion coefficient  [2.x.2437] , the temperature  [2.x.2438]  and the gravity vector  [2.x.2439]  pointing downward. (A derivation of why the right hand side looks like it looks is given in the introduction of step-32.) While the first two equations describe how the fluid reacts to temperature differences by moving around, the third equation states how the fluid motion affects the temperature field: it is an advection diffusion equation, i.e., the temperature is attached to the fluid particles and advected along in the flow field, with an additional diffusion (heat conduction) term. In many applications, the diffusion coefficient is fairly small, and the temperature equation is in fact transport, not diffusion dominated and therefore in character more hyperbolic than elliptic; we will have to take this into account when developing a stable discretization. 

In the equations above, the term  [2.x.2440]  on the right hand side denotes the heat sources and may be a spatially and temporally varying function.  [2.x.2441]  and  [2.x.2442]  denote the viscosity and diffusivity coefficients, which we assume constant for this tutorial program. The more general case when  [2.x.2443]  depends on the temperature is an important factor in physical applications: Most materials become more fluid as they get hotter (i.e.,  [2.x.2444]  decreases with  [2.x.2445] ); sometimes, as in the case of rock minerals at temperatures close to their melting point,  [2.x.2446]  may change by orders of magnitude over the typical range of temperatures. 

We note that the Stokes equation above could be nondimensionalized by introducing the [1.x.916]  [2.x.2447]  using a typical length scale  [2.x.2448] , typical temperature difference  [2.x.2449] , density  [2.x.2450] , thermal diffusivity  [2.x.2451] , and thermal conductivity  [2.x.2452] .  [2.x.2453]  is a dimensionless number that describes the ratio of heat transport due to convection induced by buoyancy changes from temperature differences, and of heat transport due to thermal diffusion. A small Rayleigh number implies that buoyancy is not strong relative to viscosity and fluid motion  [2.x.2454]  is slow enough so that heat diffusion  [2.x.2455]  is the dominant heat transport term. On the other hand, a fluid with a high Rayleigh number will show vigorous convection that dominates heat conduction. 

For most fluids for which we are interested in computing thermal convection, the Rayleigh number is very large, often  [2.x.2456]  or larger. From the structure of the equations, we see that this will lead to large pressure differences and large velocities. Consequently, the convection term in the convection-diffusion equation for  [2.x.2457]  will also be very large and an accurate solution of this equation will require us to choose small time steps. Problems with large Rayleigh numbers are therefore hard to solve numerically for similar reasons that make solving the [1.x.917] hard to solve when the [1.x.918] is large. 

Note that a large Rayleigh number does not necessarily involve large velocities in absolute terms. For example, the Rayleigh number in the earth mantle is larger than  [2.x.2458] . Yet the velocities are small: the material is in fact solid rock but it is so hot and under pressure that it can flow very slowly, on the order of at most a few centimeters per year. Nevertheless, this can lead to mixing over time scales of many million years, a time scale much shorter than for the same amount of heat to be distributed by thermal conductivity and a time scale of relevance to affect the evolution of the earth's interior and surface structure. 

 [2.x.2459]  If you are interested in using the program as the basis for your own experiments, you will also want to take a look at its continuation in step-32. Furthermore, step-32 later was developed into the much larger open source code ASPECT (see https://aspect.geodynamics.org/ ) that can solve realistic problems and that you may want to investigate before trying to morph step-31 into something that can solve whatever you want to solve. 




[1.x.919] 

Since the Boussinesq equations are derived under the assumption that inertia of the fluid's motion does not play a role, the flow field is at each time entirely determined by buoyancy difference at that time, not by the flow field at previous times. This is reflected by the fact that the first two equations above are the steady state Stokes equation that do not contain a time derivative. Consequently, we do not need initial conditions for either velocities or pressure. On the other hand, the temperature field does satisfy an equation with a time derivative, so we need initial conditions for  [2.x.2460] . 

As for boundary conditions: if  [2.x.2461]  then the temperature satisfies a second order differential equation that requires boundary data all around the boundary for all times. These can either be a prescribed boundary temperature  [2.x.2462]  (Dirichlet boundary conditions), or a prescribed thermal flux  [2.x.2463] ; in this program, we will use an insulated boundary condition, i.e., prescribe no thermal flux:  [2.x.2464] . 

Similarly, the velocity field requires us to pose boundary conditions. These may be no-slip no-flux conditions  [2.x.2465]  on  [2.x.2466]  if the fluid sticks to the boundary, or no normal flux conditions  [2.x.2467]  if the fluid can flow along but not across the boundary, or any number of other conditions that are physically reasonable. In this program, we will use no normal flux conditions. 




[1.x.920] 

Like the equations solved in step-21, we here have a system of differential-algebraic equations (DAE): with respect to the time variable, only the temperature equation is a differential equation whereas the Stokes system for  [2.x.2468]  and  [2.x.2469]  has no time-derivatives and is therefore of the sort of an algebraic constraint that has to hold at each time instant. The main difference to step-21 is that the algebraic constraint there was a mixed Laplace system of the form 

[1.x.921] 

where now we have a Stokes system 

[1.x.922] 

where  [2.x.2470]  is an operator similar to the Laplacian  [2.x.2471]  applied to a vector field. 

Given the similarity to what we have done in step-21, it may not come as a surprise that we choose a similar approach, although we will have to make adjustments for the change in operator in the top-left corner of the differential operator. 




[1.x.923] 

The structure of the problem as a DAE allows us to use the same strategy as we have already used in step-21, i.e., we use a time lag scheme: we first solve the temperature equation (using an extrapolated velocity field), and then insert the new temperature solution into the right hand side of the velocity equation. The way we implement this in our code looks at things from a slightly different perspective, though. We first solve the Stokes equations for velocity and pressure using the temperature field from the previous time step, which means that we get the velocity for the previous time step. In other words, we first solve the Stokes system for time step  [2.x.2472]  as 

[1.x.924] 

and then the temperature equation with an extrapolated velocity field to time  [2.x.2473] . 

In contrast to step-21, we'll use a higher order time stepping scheme here, namely the [1.x.925] that replaces the time derivative  [2.x.2474]  by the (one-sided) difference quotient  [2.x.2475]  with  [2.x.2476]  the time step size. This gives the discretized-in-time temperature equation 

[1.x.926] 

Note how the temperature equation is solved semi-explicitly: diffusion is treated implicitly whereas advection is treated explicitly using an extrapolation (or forward-projection) of temperature and velocity, including the just-computed velocity  [2.x.2477] . The forward-projection to the current time level  [2.x.2478]  is derived from a Taylor expansion,  [2.x.2479] . We need this projection for maintaining the order of accuracy of the BDF-2 scheme. In other words, the temperature fields we use in the explicit right hand side are second order approximations of the current temperature field &mdash; not quite an explicit time stepping scheme, but by character not too far away either. 

The introduction of the temperature extrapolation limits the time step by a [1.x.927] just like it was in  [2.x.2480]  "step-21". (We wouldn't have had that stability condition if we treated the advection term implicitly since the BDF-2 scheme is A-stable, at the price that we needed to build a new temperature matrix at each time step.) We will discuss the exact choice of time step in the [1.x.928], but for the moment of importance is that this CFL condition means that the time step size  [2.x.2481]  may change from time step to time step, and that we have to modify the above formula slightly. If  [2.x.2482]  are the time steps sizes of the current and previous time step, then we use the approximations 

[1.x.929] 

and 

[1.x.930] 

and above equation is generalized as follows: 

[1.x.931] 



where  [2.x.2483]  denotes the extrapolation of velocity  [2.x.2484]  and temperature  [2.x.2485]  to time level  [2.x.2486] , using the values at the two previous time steps. That's not an easy to read equation, but will provide us with the desired higher order accuracy. As a consistency check, it is easy to verify that it reduces to the same equation as above if  [2.x.2487] . 

As a final remark we note that the choice of a higher order time stepping scheme of course forces us to keep more time steps in memory; in particular, we here will need to have  [2.x.2488]  around, a vector that we could previously discard. This seems like a nuisance that we were able to avoid previously by using only a first order time stepping scheme, but as we will see below when discussing the topic of stabilization, we will need this vector anyway and so keeping it around for time discretization is essentially for free and gives us the opportunity to use a higher order scheme. 




[1.x.932] 

Like solving the mixed Laplace equations, solving the Stokes equations requires us to choose particular pairs of finite elements for velocities and pressure variables. Because this has already been discussed in step-22, we only cover this topic briefly: Here, we use the stable pair  [2.x.2489] . These are continuous elements, so we can form the weak form of the Stokes equation without problem by integrating by parts and substituting continuous functions by their discrete counterparts: 

[1.x.933] 

for all test functions  [2.x.2490] . The first term of the first equation is considered as the inner product between tensors, i.e.  [2.x.2491] . Because the second tensor in this product is symmetric, the anti-symmetric component of  [2.x.2492]  plays no role and it leads to the entirely same form if we use the symmetric gradient of  [2.x.2493]  instead. Consequently, the formulation we consider and that we implement is 

[1.x.934] 



This is exactly the same as what we already discussed in step-22 and there is not much more to say about this here. 




[1.x.935] 

The more interesting question is what to do with the temperature advection-diffusion equation. By default, not all discretizations of this equation are equally stable unless we either do something like upwinding, stabilization, or all of this. One way to achieve this is to use discontinuous elements (i.e., the FE_DGQ class that we used, for example, in the discretization of the transport equation in step-12, or in discretizing the pressure in step-20 and step-21) and to define a flux at the interface between cells that takes into account upwinding. If we had a pure advection problem this would probably be the simplest way to go. However, here we have some diffusion as well, and the discretization of the Laplace operator with discontinuous elements is cumbersome because of the significant number of additional terms that need to be integrated on each face between cells. Discontinuous elements also have the drawback that the use of numerical fluxes introduces an additional numerical diffusion that acts everywhere, whereas we would really like to minimize the effect of numerical diffusion to a minimum and only apply it where it is necessary to stabilize the scheme. 

A better alternative is therefore to add some nonlinear viscosity to the model. Essentially, what this does is to transform the temperature equation from the form 

[1.x.936] 

to something like 

[1.x.937] 

where  [2.x.2494]  is an addition viscosity (diffusion) term that only acts in the vicinity of shocks and other discontinuities.  [2.x.2495]  is chosen in such a way that if  [2.x.2496]  satisfies the original equations, the additional viscosity is zero. 

To achieve this, the literature contains a number of approaches. We will here follow one developed by Guermond and Popov that builds on a suitably defined residual and a limiting procedure for the additional viscosity. To this end, let us define a residual  [2.x.2497]  as follows: 

[1.x.938] 

where we will later choose the stabilization exponent  [2.x.2498]  from within the range  [2.x.2499] . Note that  [2.x.2500]  will be zero if  [2.x.2501]  satisfies the temperature equation, since then the term in parentheses will be zero. Multiplying terms out, we get the following, entirely equivalent form: 

[1.x.939] 



With this residual, we can now define the artificial viscosity as a piecewise constant function defined on each cell  [2.x.2502]  with diameter  [2.x.2503]  separately as follows: 

[1.x.940] 



Here,  [2.x.2504]  is a stabilization constant (a dimensional analysis reveals that it is unitless and therefore independent of scaling; we will discuss its choice in the [1.x.941]) and  [2.x.2505]  is a normalization constant that must have units  [2.x.2506] . We will choose it as  [2.x.2507] , where  [2.x.2508]  is the range of present temperature values (remember that buoyancy is driven by temperature variations, not the absolute temperature) and  [2.x.2509]  is a dimensionless constant. To understand why this method works consider this: If on a particular cell  [2.x.2510]  the temperature field is smooth, then we expect the residual to be small there (in fact to be on the order of  [2.x.2511] ) and the stabilization term that injects artificial diffusion will there be of size  [2.x.2512]  &mdash; i.e., rather small, just as we hope it to be when no additional diffusion is necessary. On the other hand, if we are on or close to a discontinuity of the temperature field, then the residual will be large; the minimum operation in the definition of  [2.x.2513]  will then ensure that the stabilization has size  [2.x.2514]  &mdash; the optimal amount of artificial viscosity to ensure stability of the scheme. 

Whether or not this scheme really works is a good question. Computations by Guermond and Popov have shown that this form of stabilization actually performs much better than most of the other stabilization schemes that are around (for example streamline diffusion, to name only the simplest one). Furthermore, for  [2.x.2515]  they can even prove that it produces better convergence orders for the linear transport equation than for example streamline diffusion. For  [2.x.2516] , no theoretical results are currently available, but numerical tests indicate that the results are considerably better than for  [2.x.2517] . 

A more practical question is how to introduce this artificial diffusion into the equations we would like to solve. Note that the numerical viscosity  [2.x.2518]  is temperature-dependent, so the equation we want to solve is nonlinear in  [2.x.2519]  &mdash; not what one desires from a simple method to stabilize an equation, and even less so if we realize that  [2.x.2520]  is nondifferentiable in  [2.x.2521] . However, there is no reason to despair: we still have to discretize in time and we can treat the term explicitly. 

In the definition of the stabilization parameter, we approximate the time derivative by  [2.x.2522] . This approximation makes only use of available time data and this is the reason why we need to store data of two previous time steps (which enabled us to use the BDF-2 scheme without additional storage cost). We could now simply evaluate the rest of the terms at  [2.x.2523] , but then the discrete residual would be nothing else than a backward Euler approximation, which is only first order accurate. So, in case of smooth solutions, the residual would be still of the order  [2.x.2524] , despite the second order time accuracy in the outer BDF-2 scheme and the spatial FE discretization. This is certainly not what we want to have (in fact, we desired to have small residuals in regions where the solution behaves nicely), so a bit more care is needed. The key to this problem is to observe that the first derivative as we constructed it is actually centered at  [2.x.2525] . We get the desired second order accurate residual calculation if we evaluate all spatial terms at  [2.x.2526]  by using the approximation  [2.x.2527] , which means that we calculate the nonlinear viscosity as a function of this intermediate temperature,  [2.x.2528] . Note that this evaluation of the residual is nothing else than a Crank-Nicholson scheme, so we can be sure that now everything is alright. One might wonder whether it is a problem that the numerical viscosity now is not evaluated at time  [2.x.2529]  (as opposed to the rest of the equation). However, this offset is uncritical: For smooth solutions,  [2.x.2530]  will vary continuously, so the error in time offset is  [2.x.2531]  times smaller than the nonlinear viscosity itself, i.e., it is a small higher order contribution that is left out. That's fine because the term itself is already at the level of discretization error in smooth regions. 

Using the BDF-2 scheme introduced above, this yields for the simpler case of uniform time steps of size  [2.x.2532] : 

[1.x.942] 

On the left side of this equation remains the term from the time derivative and the original (physical) diffusion which we treat implicitly (this is actually a nice term: the matrices that result from the left hand side are the mass matrix and a multiple of the Laplace matrix &mdash; both are positive definite and if the time step size  [2.x.2533]  is small, the sum is simple to invert). On the right hand side, the terms in the first line result from the time derivative; in the second line is the artificial diffusion at time  [2.x.2534] ; the third line contains the advection term, and the fourth the sources. Note that the artificial diffusion operates on the extrapolated temperature at the current time in the same way as we have discussed the advection works in the section on time stepping. 

The form for nonuniform time steps that we will have to use in reality is a bit more complicated (which is why we showed the simpler form above first) and reads: 

[1.x.943] 



After settling all these issues, the weak form follows naturally from the strong form shown in the last equation, and we immediately arrive at the weak form of the discretized equations: 

[1.x.944] 

for all discrete test functions  [2.x.2535] . Here, the diffusion term has been integrated by parts, and we have used that we will impose no thermal flux,  [2.x.2536] . 

This then results in a matrix equation of form 

[1.x.945] 

which given the structure of matrix on the left (the sum of two positive definite matrices) is easily solved using the Conjugate Gradient method. 




[1.x.946] 

As explained above, our approach to solving the joint system for velocities/pressure on the one hand and temperature on the other is to use an operator splitting where we first solve the Stokes system for the velocities and pressures using the old temperature field, and then solve for the new temperature field using the just computed velocity field. (A more extensive discussion of operator splitting methods can be found in step-58.) 




[1.x.947] 

Solving the linear equations coming from the Stokes system has been discussed in great detail in step-22. In particular, in the results section of that program, we have discussed a number of alternative linear solver strategies that turned out to be more efficient than the original approach. The best alternative identified there we to use a GMRES solver preconditioned by a block matrix involving the Schur complement. Specifically, the Stokes operator leads to a block structured matrix 

[1.x.948] 

and as discussed there a good preconditioner is 

[1.x.949] 

where  [2.x.2537]  is the Schur complement of the Stokes operator  [2.x.2538] . Of course, this preconditioner is not useful because we can't form the various inverses of matrices, but we can use the following as a preconditioner: 

[1.x.950] 

where  [2.x.2539]  are approximations to the inverse matrices. In particular, it turned out that  [2.x.2540]  is spectrally equivalent to the mass matrix and consequently replacing  [2.x.2541]  by a CG solver applied to the mass matrix on the pressure space was a good choice. In a small deviation from step-22, we here have a coefficient  [2.x.2542]  in the momentum equation, and by the same derivation as there we should arrive at the conclusion that it is the weighted mass matrix with entries  [2.x.2543]  that we should be using. 

It was more complicated to come up with a good replacement  [2.x.2544] , which corresponds to the discretized symmetric Laplacian of the vector-valued velocity field, i.e.  [2.x.2545] . In step-22 we used a sparse LU decomposition (using the SparseDirectUMFPACK class) of  [2.x.2546]  for  [2.x.2547]  &mdash; the perfect preconditioner &mdash; in 2d, but for 3d memory and compute time is not usually sufficient to actually compute this decomposition; consequently, we only use an incomplete LU decomposition (ILU, using the SparseILU class) in 3d. 

For this program, we would like to go a bit further. To this end, note that the symmetrized bilinear form on vector fields,  [2.x.2548]  is not too far away from the nonsymmetrized version,  [2.x.2549]  (note that the factor 2 has disappeared in this form). The latter, however, has the advantage that the  [2.x.2550]  vector components of the test functions are not coupled (well, almost, see below), i.e., the resulting matrix is block-diagonal: one block for each vector component, and each of these blocks is equal to the Laplace matrix for this vector component. So assuming we order degrees of freedom in such a way that first all  [2.x.2551] -components of the velocity are numbered, then the  [2.x.2552] -components, and then the  [2.x.2553] -components, then the matrix  [2.x.2554]  that is associated with this slightly different bilinear form has the form 

[1.x.951] 

where  [2.x.2555]  is a Laplace matrix of size equal to the number of shape functions associated with each component of the vector-valued velocity. With this matrix, one could be tempted to define our preconditioner for the velocity matrix  [2.x.2556]  as follows: 

[1.x.952] 

where  [2.x.2557]  is a preconditioner for the Laplace matrix &mdash; something where we know very well how to build good preconditioners! 

In reality, the story is not quite as simple: To make the matrix  [2.x.2558]  definite, we need to make the individual blocks  [2.x.2559]  definite by applying boundary conditions. One can try to do so by applying Dirichlet boundary conditions all around the boundary, and then the so-defined preconditioner  [2.x.2560]  turns out to be a good preconditioner for  [2.x.2561]  if the latter matrix results from a Stokes problem where we also have Dirichlet boundary conditions on the velocity components all around the domain, i.e., if we enforce  [2.x.2562] . 

Unfortunately, this "if" is an "if and only if": in the program below we will want to use no-flux boundary conditions of the form  [2.x.2563]  (i.e., flow %parallel to the boundary is allowed, but no flux through the boundary). In this case, it turns out that the block diagonal matrix defined above is not a good preconditioner because it neglects the coupling of components at the boundary. A better way to do things is therefore if we build the matrix  [2.x.2564]  as the vector Laplace matrix  [2.x.2565]  and then apply the same boundary condition as we applied to  [2.x.2566] . If this is a Dirichlet boundary condition all around the domain, the  [2.x.2567]  will decouple to three diagonal blocks as above, and if the boundary conditions are of the form  [2.x.2568]  then this will introduce a coupling of degrees of freedom at the boundary but only there. This, in fact, turns out to be a much better preconditioner than the one introduced above, and has almost all the benefits of what we hoped to get. 


To sum this whole story up, we can observe:  [2.x.2569]     [2.x.2570]  Compared to building a preconditioner from the original matrix  [2.x.2571]    resulting from the symmetric gradient as we did in step-22,   we have to expect that the preconditioner based on the Laplace bilinear form   performs worse since it does not take into account the coupling between   vector components. 

   [2.x.2572] On the other hand, preconditioners for the Laplace matrix are typically   more mature and perform better than ones for vector problems. For example,   at the time of this writing, Algebraic %Multigrid (AMG) algorithms are very   well developed for scalar problems, but not so for vector problems. 

   [2.x.2573] In building this preconditioner, we will have to build up the   matrix  [2.x.2574]  and its preconditioner. While this means that we   have to store an additional matrix we didn't need before, the   preconditioner  [2.x.2575]  is likely going to need much less   memory than storing a preconditioner for the coupled matrix    [2.x.2576] . This is because the matrix  [2.x.2577]  has only a third of the   entries per row for all rows corresponding to interior degrees of   freedom, and contains coupling between vector components only on   those parts of the boundary where the boundary conditions introduce   such a coupling. Storing the matrix is therefore comparatively   cheap, and we can expect that computing and storing the   preconditioner  [2.x.2578]  will also be much cheaper compared to   doing so for the fully coupled matrix.  [2.x.2579]  




[1.x.953] 

This is the easy part: The matrix for the temperature equation has the form  [2.x.2580] , where  [2.x.2581]  are mass and stiffness matrices on the temperature space, and  [2.x.2582]  are constants related the time stepping scheme and the current and previous time step. This being the sum of a symmetric positive definite and a symmetric positive semidefinite matrix, the result is also symmetric positive definite. Furthermore,  [2.x.2583]  is a number proportional to the time step, and so becomes small whenever the mesh is fine, damping the effect of the then ill-conditioned stiffness matrix. 

As a consequence, inverting this matrix with the Conjugate Gradient algorithm, using a simple preconditioner, is trivial and very cheap compared to inverting the Stokes matrix. 




[1.x.954] 

[1.x.955] 

One of the things worth explaining up front about the program below is the use of two different DoFHandler objects. If one looks at the structure of the equations above and the scheme for their solution, one realizes that there is little commonality that keeps the Stokes part and the temperature part together. In all previous tutorial programs in which we have discussed  [2.x.2584]  "vector-valued problems" we have always only used a single finite element with several vector components, and a single DoFHandler object. Sometimes, we have substructured the resulting matrix into blocks to facilitate particular solver schemes; this was, for example, the case in the step-22 program for the Stokes equations upon which the current program is based. 

We could of course do the same here. The linear system that we would get would look like this: 

[1.x.956] 

The problem with this is: We never use the whole matrix at the same time. In fact, it never really exists at the same time: As explained above,  [2.x.2585]  and  [2.x.2586]  depend on the already computed solution  [2.x.2587] , in the first case through the time step (that depends on  [2.x.2588]  because it has to satisfy a CFL condition). So we can only assemble it once we've already solved the top left  [2.x.2589]  block Stokes system, and once we've moved on to the temperature equation we don't need the Stokes part any more; the fact that we build an object for a matrix that never exists as a whole in memory at any given time led us to jumping through some hoops in step-21, so let's not repeat this sort of error. Furthermore, we don't actually build the matrix  [2.x.2590] : Because by the time we get to the temperature equation we already know  [2.x.2591] , and because we have to assemble the right hand side  [2.x.2592]  at this time anyway, we simply move the term  [2.x.2593]  to the right hand side and assemble it along with all the other terms there. What this means is that there does not remain a part of the matrix where temperature variables and Stokes variables couple, and so a global enumeration of all degrees of freedom is no longer important: It is enough if we have an enumeration of all Stokes degrees of freedom, and of all temperature degrees of freedom independently. 

In essence, there is consequently not much use in putting [1.x.957] into a block matrix (though there are of course the same good reasons to do so for the  [2.x.2594]  Stokes part), or, for that matter, in putting everything into the same DoFHandler object. 

But are there [1.x.958] to doing so? These exist, though they may not be obvious at first. The main problem is that if we need to create one global finite element that contains velocity, pressure, and temperature shape functions, and use this to initialize the DoFHandler. But we also use this finite element object to initialize all FEValues or FEFaceValues objects that we use. This may not appear to be that big a deal, but imagine what happens when, for example, we evaluate the residual  [2.x.2595]  that we need to compute the artificial viscosity  [2.x.2596] .  For this, we need the Laplacian of the temperature, which we compute using the tensor of second derivatives (Hessians) of the shape functions (we have to give the  [2.x.2597]  flag to the FEValues object for this). Now, if we have a finite that contains the shape functions for velocities, pressures, and temperatures, that means that we have to compute the Hessians of [1.x.959] shape functions, including the many higher order shape functions for the velocities. That's a lot of computations that we don't need, and indeed if one were to do that (as we had in an early version of the program), assembling the right hand side took about a quarter of the overall compute time. 

So what we will do is to use two different finite element objects, one for the Stokes components and one for the temperatures. With this come two different DoFHandlers, two sparsity patterns and two matrices for the Stokes and temperature parts, etc. And whenever we have to assemble something that contains both temperature and Stokes shape functions (in particular the right hand sides of Stokes and temperature equations), then we use two FEValues objects initialized with two cell iterators that we walk in %parallel through the two DoFHandler objects associated with the same Triangulation object; for these two FEValues objects, we use of course the same quadrature objects so that we can iterate over the same set of quadrature points, but each FEValues object will get update flags only according to what it actually needs to compute. In particular, when we compute the residual as above, we only ask for the values of the Stokes shape functions, but also the Hessians of the temperature shape functions &mdash; much cheaper indeed, and as it turns out: assembling the right hand side of the temperature equation is now a component of the program that is hardly measurable. 

With these changes, timing the program yields that only the following operations are relevant for the overall run time:  [2.x.2598]     [2.x.2599] Solving the Stokes system: 72% of the run time.    [2.x.2600] Assembling the Stokes preconditioner and computing the algebraic       multigrid hierarchy using the Trilinos ML package: 11% of the       run time.    [2.x.2601] The function  [2.x.2602] : 7%       of overall run time.    [2.x.2603] Assembling the Stokes and temperature right hand side vectors as       well as assembling the matrices: 7%.  [2.x.2604]  In essence this means that all bottlenecks apart from the algebraic multigrid have been removed. 




[1.x.960] 

In much the same way as we used PETSc to support our linear algebra needs in step-17 and step-18, we use interfaces to the [1.x.961] library (see the deal.II README file for installation instructions) in this program. Trilinos is a very large collection of everything that has to do with linear and nonlinear algebra, as well as all sorts of tools around that (and looks like it will grow in many other directions in the future as well). 

The main reason for using Trilinos, similar to our exploring PETSc, is that it is a very powerful library that provides a lot more tools than deal.II's own linear algebra library. That includes, in particular, the ability to work in %parallel on a cluster, using MPI, and a wider variety of preconditioners. In the latter class, one of the most interesting capabilities is the existence of the Trilinos ML package that implements an Algebraic Multigrid (AMG) method. We will use this preconditioner to precondition the second order operator part of the momentum equation. The ability to solve problems in %parallel will be explored in step-32, using the same problem as discussed here. 

PETSc, which we have used in step-17 and step-18, is certainly a powerful library, providing a large number of functions that deal with matrices, vectors, and iterative solvers and preconditioners, along with lots of other stuff, most of which runs quite well in %parallel. It is, however, a few years old already than Trilinos, written in C, and generally not quite as easy to use as some other libraries. As a consequence, deal.II has also acquired interfaces to Trilinos, which shares a lot of the same functionality with PETSc. It is, however, a project that is several years younger, is written in C++ and by people who generally have put a significant emphasis on software design. 




[1.x.962] 

The case we want to solve here is as follows: we solve the Boussinesq equations described above with  [2.x.2605] , i.e., a relatively slow moving fluid that has virtually no thermal diffusive conductivity and transports heat mainly through convection. On the boundary, we will require no-normal flux for the velocity ( [2.x.2606] ) and for the temperature ( [2.x.2607] ). This is one of the cases discussed in the introduction of step-22 and fixes one component of the velocity while allowing flow to be %parallel to the boundary. There remain  [2.x.2608]  components to be fixed, namely the tangential components of the normal stress; for these, we choose homogeneous conditions which means that we do not have to anything special. Initial conditions are only necessary for the temperature field, and we choose it to be constant zero. 

The evolution of the problem is then entirely driven by the right hand side  [2.x.2609]  of the temperature equation, i.e., by heat sources and sinks. Here, we choose a setup invented in advance of a Christmas lecture: real candles are of course prohibited in U.S. class rooms, but virtual ones are allowed. We therefore choose three spherical heat sources unequally spaced close to the bottom of the domain, imitating three candles. The fluid located at these sources, initially at rest, is then heated up and as the temperature rises gains buoyancy, rising up; more fluid is dragged up and through the sources, leading to three hot plumes that rise up until they are captured by the recirculation of fluid that sinks down on the outside, replacing the air that rises due to heating. 


examples/step-31/doc/results.dox 



[1.x.963] 

[1.x.964] 

When you run the program in 2d, the output will look something like this: <code> <pre> Number of active cells: 256 (on 5 levels) Number of degrees of freedom: 3556 (2178+289+1089) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.919118    9 CG iterations for temperature.    Temperature range: -0.16687 1.30011 

Number of active cells: 280 (on 6 levels) Number of degrees of freedom: 4062 (2490+327+1245) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.459559    9 CG iterations for temperature.    Temperature range: -0.0982971 0.598503 

Number of active cells: 520 (on 7 levels) Number of degrees of freedom: 7432 (4562+589+2281) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.229779    9 CG iterations for temperature.    Temperature range: -0.0551098 0.294493 

Number of active cells: 1072 (on 8 levels) Number of degrees of freedom: 15294 (9398+1197+4699) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.11489    9 CG iterations for temperature.    Temperature range: -0.0273524 0.156861 

Number of active cells: 2116 (on 9 levels) Number of degrees of freedom: 30114 (18518+2337+9259) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.0574449    9 CG iterations for temperature.    Temperature range: -0.014993 0.0738328 

Timestep 1:  t=0.0574449    Assembling...    Solving...    56 GMRES iterations for Stokes subsystem.    Time step: 0.0574449    9 CG iterations for temperature.    Temperature range: -0.0273934 0.14488 

... </pre> </code> 

In the beginning we refine the mesh several times adaptively and always return to time step zero to restart on the newly refined mesh. Only then do we start the actual time iteration. 

The program runs for a while. The temperature field for time steps 0, 500, 1000, 1500, 2000, 3000, 4000, and 5000 looks like this (note that the color scale used for the temperature is not always the same): 

 [2.x.2610]  

The visualizations shown here were generated using a version of the example which did not enforce the constraints after transferring the mesh. 

As can be seen, we have three heat sources that heat fluid and therefore produce a buoyancy effect that lets hots pockets of fluid rise up and swirl around. By a chimney effect, the three streams are pressed together by fluid that comes from the outside and wants to join the updraft party. Note that because the fluid is initially at rest, those parts of the fluid that were initially over the sources receive a longer heating time than that fluid that is later dragged over the source by the fully developed flow field. It is therefore hotter, a fact that can be seen in the red tips of the three plumes. Note also the relatively fine features of the flow field, a result of the sophisticated transport stabilization of the temperature equation we have chosen. 

In addition to the pictures above, the following ones show the adaptive mesh and the flow field at the same time steps: 

 [2.x.2611]  




[1.x.965] 

The same thing can of course be done in 3d by changing the template parameter to the BoussinesqFlowProblem object in  [2.x.2612]  from 2 to 3, so that the output now looks like follows: 

<code> <pre> Number of active cells: 64 (on 3 levels) Number of degrees of freedom: 3041 (2187+125+729) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 2.45098    9 CG iterations for temperature.    Temperature range: -0.675683 4.94725 

Number of active cells: 288 (on 4 levels) Number of degrees of freedom: 12379 (8943+455+2981) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 1.22549    9 CG iterations for temperature.    Temperature range: -0.527701 2.25764 

Number of active cells: 1296 (on 5 levels) Number of degrees of freedom: 51497 (37305+1757+12435) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.612745    10 CG iterations for temperature.    Temperature range: -0.496942 0.847395 

Number of active cells: 5048 (on 6 levels) Number of degrees of freedom: 192425 (139569+6333+46523) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.306373    10 CG iterations for temperature.    Temperature range: -0.267683 0.497739 

Timestep 1:  t=0.306373    Assembling...    Solving...    27 GMRES iterations for Stokes subsystem.    Time step: 0.306373    10 CG iterations for temperature.    Temperature range: -0.461787 0.958679 

... </pre> </code> 

Visualizing the temperature isocontours at time steps 0, 50, 100, 150, 200, 300, 400, 500, 600, 700, and 800 yields the following plots: 

 [2.x.2613]  

That the first picture looks like three hedgehogs stems from the fact that our scheme essentially projects the source times the first time step size onto the mesh to obtain the temperature field in the first time step. Since the source function is discontinuous, we need to expect over- and undershoots from this project. This is in fact what happens (it's easier to check this in 2d) and leads to the crumpled appearance of the isosurfaces.  The visualizations shown here were generated using a version of the example which did not enforce the constraints after transferring the mesh. 




[1.x.966] 

The program as is has three parameters that we don't have much of a theoretical handle on how to choose in an optimal way. These are:  [2.x.2614]     [2.x.2615] The time step must satisfy a CFL condition        [2.x.2616] . Here,  [2.x.2617]  is       dimensionless, but what is the right value?    [2.x.2618] In the computation of the artificial viscosity, 

[1.x.967] 

      with  [2.x.2619] .       Here, the choice of the dimensionless %numbers  [2.x.2620]  is of       interest.  [2.x.2621]  In all of these cases, we will have to expect that the correct choice of each value depends on that of the others, and most likely also on the space dimension and polynomial degree of the finite element used for the temperature. Below we'll discuss a few numerical experiments to choose constants  [2.x.2622]  and  [2.x.2623] . 

Below, we will not discuss the choice of  [2.x.2624] . In the program, we set it to  [2.x.2625] . The reason for this value is a bit complicated and has more to do with the history of the program than reasoning: while the correct formula for the global scaling parameter  [2.x.2626]  is shown above, the program (including the version shipped with deal.II 6.2) initially had a bug in that we computed  [2.x.2627]  instead, where we had set the scaling parameter to one. Since we only computed on the unit square/cube where  [2.x.2628] , this was entirely equivalent to using the correct formula with  [2.x.2629] . Since this value for  [2.x.2630]  appears to work just fine for the current program, we corrected the formula in the program and set  [2.x.2631]  to a value that reproduces exactly the results we had before. We will, however, revisit this issue again in step-32. 

Now, however, back to the discussion of what values of  [2.x.2632]  and  [2.x.2633]  to choose: 




[1.x.968][1.x.969] 

These two constants are definitely linked in some way. The reason is easy to see: In the case of a pure advection problem,  [2.x.2634] , any explicit scheme has to satisfy a CFL condition of the form  [2.x.2635] . On the other hand, for a pure diffusion problem,  [2.x.2636] , explicit schemes need to satisfy a condition  [2.x.2637] . So given the form of  [2.x.2638]  above, an advection diffusion problem like the one we have to solve here will result in a condition of the form  [2.x.2639] . It follows that we have to face the fact that we might want to choose  [2.x.2640]  larger to improve the stability of the numerical scheme (by increasing the amount of artificial diffusion), but we have to pay a price in the form of smaller, and consequently more time steps. In practice, one would therefore like to choose  [2.x.2641]  as small as possible to keep the transport problem sufficiently stabilized while at the same time trying to choose the time step as large as possible to reduce the overall amount of work. 

The find the right balance, the only way is to do a few computational experiments. Here's what we did: We modified the program slightly to allow less mesh refinement (so we don't always have to wait that long) and to choose  [2.x.2642]  to eliminate the effect of the constant  [2.x.2643]  (we know that solutions are stable by using this version of  [2.x.2644]  as an artificial viscosity, but that we can improve things -- i.e. make the solution sharper -- by using the more complicated formula for this artificial viscosity). We then run the program for different values  [2.x.2645]  and observe maximal and minimal temperatures in the domain. What we expect to see is this: If we choose the time step too big (i.e. choose a  [2.x.2646]  bigger than theoretically allowed) then we will get exponential growth of the temperature. If we choose  [2.x.2647]  too small, then the transport stabilization becomes insufficient and the solution will show significant oscillations but not exponential growth. 




[1.x.970] 

Here is what we get for  [2.x.2648] , and  [2.x.2649] , different choices of  [2.x.2650] , and bilinear elements ( [2.x.2651] ) in 2d: 

 [2.x.2652]  

The way to interpret these graphs goes like this: for  [2.x.2653]  and  [2.x.2654] , we see exponential growth or at least large variations, but if we choose  [2.x.2655]  or smaller, then the scheme is stable though a bit wobbly. For more artificial diffusion, we can choose  [2.x.2656]  or smaller for  [2.x.2657] ,  [2.x.2658]  or smaller for  [2.x.2659] , and again need  [2.x.2660]  for  [2.x.2661]  (this time because much diffusion requires a small time step). 

So how to choose? If we were simply interested in a large time step, then we would go with  [2.x.2662]  and  [2.x.2663] . On the other hand, we're also interested in accuracy and here it may be of interest to actually investigate what these curves show. To this end note that we start with a zero temperature and that our sources are positive &mdash; so we would intuitively expect that the temperature can never drop below zero. But it does, a consequence of Gibb's phenomenon when using continuous elements to approximate a discontinuous solution. We can therefore see that choosing  [2.x.2664]  too small is bad: too little artificial diffusion leads to over- and undershoots that aren't diffused away. On the other hand, for large  [2.x.2665] , the minimum temperature drops below zero at the beginning but then quickly diffuses back to zero. 

On the other hand, let's also look at the maximum temperature. Watching the movie of the solution, we see that initially the fluid is at rest. The source keeps heating the same volume of fluid whose temperature increases linearly at the beginning until its buoyancy is able to move it upwards. The hottest part of the fluid is therefore transported away from the solution and fluid taking its place is heated for only a short time before being moved out of the source region, therefore remaining cooler than the initial bubble. If  [2.x.2666]  (in the program it is nonzero but very small) then the hottest part of the fluid should be advected along with the flow with its temperature constant. That's what we can see in the graphs with the smallest  [2.x.2667] : Once the maximum temperature is reached, it hardly changes any more. On the other hand, the larger the artificial diffusion, the more the hot spot is diffused. Note that for this criterion, the time step size does not play a significant role. 

So to sum up, likely the best choice would appear to be  [2.x.2668]  and  [2.x.2669] . The curve is a bit wobbly, but overall pictures looks pretty reasonable with the exception of some over and undershoots close to the start time due to Gibb's phenomenon. 




[1.x.971] 

One can repeat the same sequence of experiments for higher order elements as well. Here are the graphs for bi-quadratic shape functions ( [2.x.2670] ) for the temperature, while we retain the  [2.x.2671]  stable Taylor-Hood element for the Stokes system: 

 [2.x.2672]  

Again, small values of  [2.x.2673]  lead to less diffusion but we have to choose the time step very small to keep things under control. Too large values of  [2.x.2674]  make for more diffusion, but again require small time steps. The best value would appear to be  [2.x.2675] , as for the  [2.x.2676]  element, and then we have to choose  [2.x.2677]  &mdash; exactly half the size for the  [2.x.2678]  element, a fact that may not be surprising if we state the CFL condition as the requirement that the time step be small enough so that the distance transport advects in each time step is no longer than one [1.x.972] away (which for  [2.x.2679]  elements is  [2.x.2680] , but for  [2.x.2681]  elements is  [2.x.2682] ). It turns out that  [2.x.2683]  needs to be slightly larger for obtaining stable results also late in the simulation at times larger than 60, so we actually choose it as  [2.x.2684]  in the code. 




[1.x.973] 

One can repeat these experiments in 3d and find the optimal time step for each value of  [2.x.2685]  and find the best value of  [2.x.2686] . What one finds is that for the same  [2.x.2687]  already used in 2d, the time steps needs to be a bit smaller, by around a factor of 1.2 or so. This is easily explained: the time step restriction is  [2.x.2688]  where  [2.x.2689]  is the [1.x.974] of the cell. However, what is really needed is the distance between mesh points, which is  [2.x.2690] . So a more appropriate form would be  [2.x.2691] . 

The second find is that one needs to choose  [2.x.2692]  slightly bigger (about  [2.x.2693]  or so). This then again reduces the time step we can take. 







[1.x.975] 

Concluding, from the simple computations above,  [2.x.2694]  appears to be a good choice for the stabilization parameter in 2d, and  [2.x.2695]  in 3d. In a dimension independent way, we can model this as  [2.x.2696] . If one does longer computations (several thousand time steps) on finer meshes, one realizes that the time step size is not quite small enough and that for stability one will have to reduce the above values a bit more (by about a factor of  [2.x.2697] ). 

As a consequence, a formula that reconciles 2d, 3d, and variable polynomial degree and takes all factors in account reads as follows: 

[1.x.976] 

In the first form (in the center of the equation),  [2.x.2698]  is a universal constant,  [2.x.2699]  is the factor that accounts for the difference between cell diameter and grid point separation,  [2.x.2700]  accounts for the increase in  [2.x.2701]  with space dimension,  [2.x.2702]  accounts for the distance between grid points for higher order elements, and  [2.x.2703]  for the local speed of transport relative to the cell size. This is the formula that we use in the program. 

As for the question of whether to use  [2.x.2704]  or  [2.x.2705]  elements for the temperature, the following considerations may be useful: First, solving the temperature equation is hardly a factor in the overall scheme since almost the entire compute time goes into solving the Stokes system in each time step. Higher order elements for the temperature equation are therefore not a significant drawback. On the other hand, if one compares the size of the over- and undershoots the solution produces due to the discontinuous source description, one notices that for the choice of  [2.x.2706]  and  [2.x.2707]  as above, the  [2.x.2708]  solution dips down to around  [2.x.2709] , whereas the  [2.x.2710]  solution only goes to  [2.x.2711]  (remember that the exact solution should never become negative at all. This means that the  [2.x.2712]  solution is significantly more accurate; the program therefore uses these higher order elements, despite the penalty we pay in terms of smaller time steps. 




[1.x.977] 

There are various ways to extend the current program. Of particular interest is, of course, to make it faster and/or increase the resolution of the program, in particular in 3d. This is the topic of the step-32 tutorial program which will implement strategies to solve this problem in %parallel on a cluster. It is also the basis of the much larger open source code ASPECT (see https://aspect.geodynamics.org/ ) that can solve realistic problems and that constitutes the further development of step-32. 

Another direction would be to make the fluid flow more realistic. The program was initially written to simulate various cases simulating the convection of material in the earth's mantle, i.e. the zone between the outer earth core and the solid earth crust: there, material is heated from below and cooled from above, leading to thermal convection. The physics of this fluid are much more complicated than shown in this program, however: The viscosity of mantle material is strongly dependent on the temperature, i.e.  [2.x.2713] , with the dependency frequently modeled as a viscosity that is reduced exponentially with rising temperature. Secondly, much of the dynamics of the mantle is determined by chemical reactions, primarily phase changes of the various crystals that make up the mantle; the buoyancy term on the right hand side of the Stokes equations then depends not only on the temperature, but also on the chemical composition at a given location which is advected by the flow field but also changes as a function of pressure and temperature. We will investigate some of these effects in later tutorial programs as well. 


examples/step-32/doc/intro.dox 

 [2.x.2714]  

[1.x.978][1.x.979][1.x.980][1.x.981][1.x.982] 


[1.x.983] 

[1.x.984] 

This program does pretty much exactly what step-31 already does: it solves the Boussinesq equations that describe the motion of a fluid whose temperature is not in equilibrium. As such, all the equations we have described in step-31 still hold: we solve the same general partial differential equation (with only minor modifications to adjust for more realism in the problem setting), using the same finite element scheme, the same time stepping algorithm, and more or less the same stabilization method for the temperature advection-diffusion equation. As a consequence, you may first want to understand that program &mdash; and its implementation &mdash; before you work on the current one. 

The difference between step-31 and the current program is that here we want to do things in %parallel, using both the availability of many machines in a cluster (with parallelization based on MPI) as well as many processor cores within a single machine (with parallelization based on threads). This program's main job is therefore to introduce the changes that are necessary to utilize the availability of these %parallel compute resources. In this regard, it builds on the step-40 program that first introduces the necessary classes for much of the %parallel functionality, and on step-55 that shows how this is done for a vector-valued problem. 

In addition to these changes, we also use a slightly different preconditioner, and we will have to make a number of changes that have to do with the fact that we want to solve a [1.x.985] problem here, not a model problem. The latter, in particular, will require that we think about scaling issues as well as what all those parameters and coefficients in the equations under consideration actually mean. We will discuss first the issues that affect changes in the mathematical formulation and solver structure, then how to parallelize things, and finally the actual testcase we will consider. 




[1.x.986] 

In step-31, we used the following Stokes model for the velocity and pressure field: 

[1.x.987] 

The right hand side of the first equation appears a wee bit unmotivated. Here's how things should really be. We need the external forces that act on the fluid, which we assume are given by gravity only. In the current case, we assume that the fluid does expand slightly for the purposes of this gravity force, but not enough that we need to modify the incompressibility condition (the second equation). What this means is that for the purpose of the right hand side, we can assume that  [2.x.2715] . An assumption that may not be entirely justified is that we can assume that the changes of density as a function of temperature are small, leading to an expression of the form  [2.x.2716] , i.e., the density equals  [2.x.2717]  at reference temperature and decreases linearly as the temperature increases (as the material expands). The force balance equation then looks properly written like this: 

[1.x.988] 

Now note that the gravity force results from a gravity potential as  [2.x.2718] , so that we can re-write this as follows: 

[1.x.989] 

The second term on the right is time independent, and so we could introduce a new "dynamic" pressure  [2.x.2719]  with which the Stokes equations would read: 

[1.x.990] 

This is exactly the form we used in step-31, and it was appropriate to do so because all changes in the fluid flow are only driven by the dynamic pressure that results from temperature differences. (In other words: Any contribution to the right hand side that results from taking the gradient of a scalar field have no effect on the velocity field.) 

On the other hand, we will here use the form of the Stokes equations that considers the total pressure instead: 

[1.x.991] 

There are several advantages to this: 

- This way we can plot the pressure in our program in such a way that it   actually shows the total pressure that includes the effects of   temperature differences as well as the static pressure of the   overlying rocks. Since the pressure does not appear any further in any   of the other equations, whether to use one or the other is more a   matter of taste than of correctness. The flow field is exactly the   same, but we get a pressure that we can now compare with values that   are given in geophysical books as those that hold at the bottom of the   earth mantle, for example. 

- If we wanted to make the model even more realistic, we would have to take   into account that many of the material parameters (e.g. the viscosity, the   density, etc) not only depend on the temperature but also the   [1.x.992] pressure. 

- The model above assumed a linear dependence  [2.x.2720]  and assumed that  [2.x.2721]  is small. In   practice, this may not be so. In fact, realistic models are   certainly not linear, and  [2.x.2722]  may also not be small for at least   part of the temperature range because the density's behavior is   substantially dependent not only on thermal expansion but by phase   changes. 

- A final reason to do this is discussed in the results section and   concerns possible extensions to the model we use here. It has to do   with the fact that the temperature equation (see below) we use here does not   include a term that contains the pressure. It should, however:   rock, like gas, heats up as you compress it. Consequently,   material that rises up cools adiabatically, and cold material that   sinks down heats adiabatically. We discuss this further below. 

 [2.x.2723]  There is, however, a downside to this procedure. In the earth, the dynamic pressure is several orders of magnitude smaller than the total pressure. If we use the equations above and solve all variables to, say, 4 digits of accuracy, then we may be able to get the velocity and the total pressure right, but we will have no accuracy at all if we compute the dynamic pressure by subtracting from the total pressure the static part  [2.x.2724] . If, for example, the dynamic pressure is six orders of magnitude smaller than the static pressure, then we need to solve the overall pressure to at least seven digits of accuracy to get anything remotely accurate. That said, in practice this turns out not to be a limiting factor. 




[1.x.993] 

Remember that we want to solve the following set of equations: 

[1.x.994] 

augmented by appropriate boundary and initial conditions. As discussed in step-31, we will solve this set of equations by solving for a Stokes problem first in each time step, and then moving the temperature equation forward by one time interval. 

The problem under consideration in this current section is with the Stokes problem: if we discretize it as usual, we get a linear system 

[1.x.995] 

which in this program we will solve with a FGMRES solver. This solver iterates until the residual of these linear equations is below a certain tolerance, i.e., until 

[1.x.996] 

This does not make any sense from the viewpoint of physical units: the quantities involved here have physical units so that the first part of the residual has units  [2.x.2725]  (most easily established by considering the term  [2.x.2726]  and considering that the pressure has units  [2.x.2727]  and the integration yields a factor of  [2.x.2728] ), whereas the second part of the residual has units  [2.x.2729] . Taking the norm of this residual vector would yield a quantity with units  [2.x.2730] . This, quite obviously, does not make sense, and we should not be surprised that doing so is eventually going to come back hurting us. 

So why is this an issue here, but not in step-31? The reason back there is that everything was nicely balanced: velocities were on the order of one, the pressure likewise, the viscosity was one, and the domain had a diameter of  [2.x.2731] . As a result, while nonsensical, nothing bad happened. On the other hand, as we will explain below, things here will not be that simply scaled:  [2.x.2732]  will be around  [2.x.2733] , velocities on the order of  [2.x.2734] , pressure around  [2.x.2735] , and the diameter of the domain is  [2.x.2736] . In other words, the order of magnitude for the first equation is going to be  [2.x.2737] , whereas the second equation will be around  [2.x.2738] . Well, so what this will lead to is this: if the solver wants to make the residual small, it will almost entirely focus on the first set of equations because they are so much bigger, and ignore the divergence equation that describes mass conservation. That's exactly what happens: unless we set the tolerance to extremely small values, the resulting flow field is definitely not divergence free. As an auxiliary problem, it turns out that it is difficult to find a tolerance that always works; in practice, one often ends up with a tolerance that requires 30 or 40 iterations for most time steps, and 10,000 for some others. 

So what's a numerical analyst to do in a case like this? The answer is to start at the root and first make sure that everything is mathematically consistent first. In our case, this means that if we want to solve the system of Stokes equations jointly, we have to scale them so that they all have the same physical dimensions. In our case, this means multiplying the second equation by something that has units  [2.x.2739] ; one choice is to multiply with  [2.x.2740]  where  [2.x.2741]  is a typical lengthscale in our domain (which experiments show is best chosen to be the diameter of plumes &mdash; around 10 km &mdash; rather than the diameter of the domain). Using these %numbers for  [2.x.2742]  and  [2.x.2743] , this factor is around  [2.x.2744] . So, we now get this for the Stokes system: 

[1.x.997] 

The trouble with this is that the result is not symmetric any more (we have  [2.x.2745]  at the bottom left, but not its transpose operator at the top right). This, however, can be cured by introducing a scaled pressure  [2.x.2746] , and we get the scaled equations 

[1.x.998] 

This is now symmetric. Obviously, we can easily recover the original pressure  [2.x.2747]  from the scaled pressure  [2.x.2748]  that we compute as a result of this procedure. 

In the program below, we will introduce a factor  [2.x.2749]  that corresponds to  [2.x.2750] , and we will use this factor in the assembly of the system matrix and preconditioner. Because it is annoying and error prone, we will recover the unscaled pressure immediately following the solution of the linear system, i.e., the solution vector's pressure component will immediately be unscaled to retrieve the physical pressure. Since the solver uses the fact that we can use a good initial guess by extrapolating the previous solutions, we also have to scale the pressure immediately [1.x.999] solving. 




[1.x.1000] 

In this tutorial program, we apply a variant of the preconditioner used in step-31. That preconditioner was built to operate on the system matrix  [2.x.2751]  in block form such that the product matrix 

[1.x.1001] 

is of a form that Krylov-based iterative solvers like GMRES can solve in a few iterations. We then replaced the exact inverse of  [2.x.2752]  by the action of an AMG preconditioner  [2.x.2753]  based on a vector Laplace matrix, approximated the Schur complement  [2.x.2754]  by a mass matrix  [2.x.2755]  on the pressure space and wrote an <tt>InverseMatrix</tt> class for implementing the action of  [2.x.2756]  on vectors. In the InverseMatrix class, we used a CG solve with an incomplete Cholesky (IC) preconditioner for performing the inner solves. 

An observation one can make is that we use just the action of a preconditioner for approximating the velocity inverse  [2.x.2757]  (and the outer GMRES iteration takes care of the approximate character of the inverse), whereas we use a more or less [1.x.1002] inverse for  [2.x.2758] , realized by a fully converged CG solve. This appears unbalanced, but there's system to this madness: almost all the effort goes into the upper left block to which we apply the AMG preconditioner, whereas even an exact inversion of the pressure mass matrix costs basically nothing. Consequently, if it helps us reduce the overall number of iterations somewhat, then this effort is well spent. 

That said, even though the solver worked well for step-31, we have a problem here that is a bit more complicated (cells are deformed, the pressure varies by orders of magnitude, and we want to plan ahead for more complicated physics), and so we'll change a few things slightly: 

- For more complex problems, it turns out that using just a single AMG V-cycle   as preconditioner is not always sufficient. The outer solver converges just   fine most of the time in a reasonable number of iterations (say, less than   50) but there are the occasional time step where it suddenly takes 700 or   so. What exactly is going on there is hard to determine, but the problem can   be avoided by using a more accurate solver for the top left   block. Consequently, we'll want to use a CG iteration to invert the top left   block of the preconditioner matrix, and use the AMG as a preconditioner for   the CG solver. 

- The downside of this is that, of course, the Stokes preconditioner becomes   much more expensive (approximately 10 times more expensive than when we just   use a single V-cycle). Our strategy then is this: let's do up to 30 GMRES   iterations with just the V-cycle as a preconditioner and if that doesn't   yield convergence, then take the best approximation of the Stokes solution   obtained after this first round of iterations and use that as the starting   guess for iterations where we use the full inner solver with a rather   lenient tolerance as preconditioner. In all our experiments this leads to   convergence in only a few additional iterations. 

- One thing we need to pay attention to is that when using a CG with a lenient   tolerance in the preconditioner, then  [2.x.2759]  is no longer a   linear function of  [2.x.2760]  (it is, of course, if we have a very stringent   tolerance in our solver, or if we only apply a single V-cycle). This is a   problem since now our preconditioner is no longer a linear operator; in   other words, every time GMRES uses it the preconditioner looks   different. The standard GMRES solver can't deal with this, leading to slow   convergence or even breakdown, but the F-GMRES variant is designed to deal   with exactly this kind of situation and we consequently use it. 

- On the other hand, once we have settled on using F-GMRES we can relax the   tolerance used in inverting the preconditioner for  [2.x.2761] . In step-31, we ran a   preconditioned CG method on  [2.x.2762]  until the residual had been reduced   by 7 orders of magnitude. Here, we can again be more lenient because we know   that the outer preconditioner doesn't suffer. 

- In step-31, we used a left preconditioner in which we first invert the top   left block of the preconditioner matrix, then apply the bottom left   (divergence) one, and then invert the bottom right. In other words, the   application of the preconditioner acts as a lower left block triangular   matrix. Another option is to use a right preconditioner that here would be   upper right block triangulation, i.e., we first invert the bottom right   Schur complement, apply the top right (gradient) operator and then invert   the elliptic top left block. To a degree, which one to choose is a matter of   taste. That said, there is one significant advantage to a right   preconditioner in GMRES-type solvers: the residual with which we determine   whether we should stop the iteration is the true residual, not the norm of   the preconditioned equations. Consequently, it is much simpler to compare it   to the stopping criterion we typically use, namely the norm of the right   hand side vector. In writing this code we found that the scaling issues we   discussed above also made it difficult to determine suitable stopping   criteria for left-preconditioned linear systems, and consequently this   program uses a right preconditioner. 

- In step-31, we used an IC (incomplete Cholesky) preconditioner for the   pressure mass matrix in the Schur complement preconditioner and for the   solution of the temperature system. Here, we could in principle do the same,   but we do choose an even simpler preconditioner, namely a Jacobi   preconditioner for both systems. This is because here we target at massively   %parallel computations, where the decompositions for IC/ILU would have to be   performed block-wise for the locally owned degrees of freedom on each   processor. This means, that the preconditioner gets more like a Jacobi   preconditioner anyway, so we rather start from that variant straight   away. Note that we only use the Jacobi preconditioners for CG solvers with   mass matrices, where they give optimal ([1.x.1003]-independent) convergence   anyway, even though they usually require about twice as many iterations as   an IC preconditioner. 

As a final note, let us remark that in step-31 we computed the Schur complement  [2.x.2763]  by approximating  [2.x.2764] . Now, however, we have re-scaled the  [2.x.2765]  and  [2.x.2766]  operators. So  [2.x.2767]  should now approximate  [2.x.2768] . We use the discrete form of the right hand side of this as our approximation  [2.x.2769]  to  [2.x.2770] . 




[1.x.1004] 

Similarly to step-31, we will use an artificial viscosity for stabilization based on a residual of the equation.  As a difference to step-31, we will provide two slightly different definitions of the stabilization parameter. For  [2.x.2771] , we use the same definition as in step-31: 

[1.x.1005] 

where we compute the viscosity from a residual  [2.x.2772]  of the equation, limited by a diffusion proportional to the mesh size  [2.x.2773]  in regions where the residual is large (around steep gradients). This definition has been shown to work well for the given case,  [2.x.2774]  in step-31, but it is usually less effective as the diffusion for  [2.x.2775] . For that case, we choose a slightly more readable definition of the viscosity, 

[1.x.1006] 

where the first term gives again the maximum dissipation (similarly to a first order upwind scheme), 

[1.x.1007] 

and the entropy viscosity is defined as 

[1.x.1008] 



This formula is described in the article [1.x.1009] Compared to the case  [2.x.2776] , the residual is computed from the temperature entropy,  [2.x.2777]  with  [2.x.2778]  an average temperature (we choose the mean between the maximum and minimum temperature in the computation), which gives the following formula 

[1.x.1010] 

The denominator in the formula for  [2.x.2779]  is computed as the global deviation of the entropy from the space-averaged entropy  [2.x.2780] . As in step-31, we evaluate the artificial viscosity from the temperature and velocity at two previous time levels, in order to avoid a nonlinearity in its definition. 

The above definitions of the viscosity are simple, but depend on two parameters, namely  [2.x.2781]  and  [2.x.2782] .  For the current program, we want to go about this issue a bit more systematically for both parameters in the case  [2.x.2783] , using the same line of reasoning with which we chose two other parameters in our discretization,  [2.x.2784]  and  [2.x.2785] , in the results section of step-31. In particular, remember that we would like to make the artificial viscosity as small as possible while keeping it as large as necessary. In the following, let us describe the general strategy one may follow. The computations shown here were done with an earlier version of the program and so the actual numerical values you get when running the program may no longer match those shown here; that said, the general approach remains valid and has been used to find the values of the parameters actually used in the program. 

To see what is happening, note that below we will impose boundary conditions for the temperature between 973 and 4273 Kelvin, and initial conditions are also chosen in this range; for these considerations, we run the program without %internal heat sources or sinks, and consequently the temperature should always be in this range, barring any %internal oscillations. If the minimal temperature drops below 973 Kelvin, then we need to add stabilization by either increasing  [2.x.2786]  or decreasing  [2.x.2787] . 

As we did in step-31, we first determine an optimal value of  [2.x.2788]  by using the "traditional" formula 

[1.x.1011] 

which we know to be stable if only  [2.x.2789]  is large enough. Doing a couple hundred time steps (on a coarser mesh than the one shown in the program, and with a different viscosity that affects transport velocities and therefore time step sizes) in 2d will produce the following graph: 

 [2.x.2790]  

As can be seen, values  [2.x.2791]  are too small whereas  [2.x.2792]  appears to work, at least to the time horizon shown here. As a remark on the side, there are at least two questions one may wonder here: First, what happens at the time when the solution becomes unstable? Looking at the graphical output, we can see that with the unreasonably coarse mesh chosen for these experiments, around time  [2.x.2793]  seconds the plumes of hot material that have been rising towards the cold outer boundary and have then spread sideways are starting to get close to each other, squeezing out the cold material in-between. This creates a layer of cells into which fluids flows from two opposite sides and flows out toward a third, apparently a scenario that then produce these instabilities without sufficient stabilization. Second: In step-31, we used  [2.x.2794] ; why does this not work here? The answer to this is not entirely clear -- stabilization parameters are certainly known to depend on things like the shape of cells, for which we had squares in step-31 but have trapezoids in the current program. Whatever the exact cause, we at least have a value of  [2.x.2795] , namely 0.052 for 2d, that works for the current program. A similar set of experiments can be made in 3d where we find that  [2.x.2796]  is a good choice &mdash; neatly leading to the formula  [2.x.2797] . 

With this value fixed, we can go back to the original formula for the viscosity  [2.x.2798]  and play with the constant  [2.x.2799] , making it as large as possible in order to make  [2.x.2800]  as small as possible. This gives us a picture like this: 

 [2.x.2801]  

Consequently,  [2.x.2802]  would appear to be the right value here. While this graph has been obtained for an exponent  [2.x.2803] , in the program we use  [2.x.2804]  instead, and in that case one has to re-tune the parameter (and observe that  [2.x.2805]  appears in the numerator and not in the denominator). It turns out that  [2.x.2806]  works with  [2.x.2807] . 




[1.x.1012] 

The standard Taylor-Hood discretization for Stokes, using the  [2.x.2808]  element, is globally conservative, i.e.  [2.x.2809] . This can easily be seen: the weak form of the divergence equation reads  [2.x.2810] . Because the pressure space does contain the function  [2.x.2811] , we get 

[1.x.1013] 

by the divergence theorem. This property is important: if we want to use the velocity field  [2.x.2812]  to transport along other quantities (such as the temperature in the current equations, but it could also be concentrations of chemical substances or entirely artificial tracer quantities) then the conservation property guarantees that the amount of the quantity advected remains constant. 

That said, there are applications where this [1.x.1014] property is not enough. Rather, we would like that it holds [1.x.1015], on every cell. This can be achieved by using the space  [2.x.2813]  for discretization, where we have replaced the [1.x.1016] space of tensor product polynomials of degree  [2.x.2814]  for the pressure by the [1.x.1017] space of the complete polynomials of the same degree. (Note that tensor product polynomials in 2d contain the functions  [2.x.2815] , whereas the complete polynomials only have the functions  [2.x.2816] .) This space turns out to be stable for the Stokes equation. 

Because the space is discontinuous, we can now in particular choose the test function  [2.x.2817] , i.e. the characteristic function of cell  [2.x.2818] . We then get in a similar fashion as above 

[1.x.1018] 

showing the conservation property for cell  [2.x.2819] . This clearly holds for each cell individually. 

There are good reasons to use this discretization. As mentioned above, this element guarantees conservation of advected quantities on each cell individually. A second advantage is that the pressure mass matrix we use as a preconditioner in place of the Schur complement becomes block diagonal and consequently very easy to invert. However, there are also downsides. For one, there are now more pressure variables, increasing the overall size of the problem, although this doesn't seem to cause much harm in practice. More importantly, though, the fact that now the divergence integrated over each cell is zero when it wasn't before does not guarantee that the divergence is pointwise smaller. In fact, as one can easily verify, the  [2.x.2820]  norm of the divergence is [1.x.1019] for this than for the standard Taylor-Hood discretization. (However, both converge at the same rate to zero, since it is easy to see that  [2.x.2821] .) It is therefore not a priori clear that the error is indeed smaller just because we now have more degrees of freedom. 

Given these considerations, it remains unclear which discretization one should prefer. Consequently, we leave that up to the user and make it a parameter in the input file which one to use. 




[1.x.1020] 

In the program, we will use a spherical shell as domain. This means that the inner and outer boundary of the domain are no longer "straight" (by which we usually mean that they are bilinear surfaces that can be represented by the FlatManifold class). Rather, they are curved and it seems prudent to use a curved approximation in the program if we are already using higher order finite elements for the velocity. Consequently, we will introduce a member variable of type MappingQ that denotes such a mapping (step-10 and step-11 introduce such mappings for the first time) and that we will use in all computations on cells that are adjacent to the boundary. Since this only affects a relatively small fraction of cells, the additional effort is not very large and we will take the luxury of using a quartic mapping for these cel