examples/step-59/doc/intro.dox
<br>

<i>
This program was contributed by Katharina Kormann and Martin
Kronbichler.

This work was partly supported by the German Research Foundation (DFG) through
the project "High-order discontinuous Galerkin for the exa-scale" (ExaDG)
within the priority program "Software for Exascale Computing" (SPPEXA). </i>

<a name="Intro"></a>
<h1>Introduction</h1>

Matrix-free operator evaluation enables very efficient implementations of
discretization with high-order polynomial bases due to a method called sum
factorization. This concept has been introduced in the step-37 and step-48
tutorial programs. In this tutorial program, we extend those concepts to
discontinuous Galerkin (DG) schemes that include face integrals, a class of
methods where high orders are particularly widespread.

The underlying idea of the matrix-free evaluation is the same as for
continuous elements: The matrix-vector product that appears in an iterative
solver or multigrid smoother is not implemented by a classical sparse matrix
kernel, but instead applied implicitly by the evaluation of the underlying
integrals on the fly. For tensor product shape functions that are integrated
with a tensor product quadrature rule, this evaluation is particularly
efficient by using the sum-factorization technique, which decomposes the
initially $(k+1)^{2d}$ operations for interpolation involving $(k+1)^d$ vector
entries with associated shape functions at degree $k$ in $d$ dimensions to
$(k+1)^d$ quadrature points into $d$ one-dimensional operations of cost
$(k+1)^{d+1}$ each. In 3D, this reduces the order of complexity by two powers
in $k$. When measured as the complexity per degree of freedom, the complexity
is $\mathcal O(k)$ in the polynomial degree. Due to the presence of face
integrals in DG, and due to the fact that operations on quadrature points
involve more memory transfer, which both scale as $\mathcal O(1)$, the
observed complexity is often constant for moderate $k\leq 10$. This means that
a high order method can be evaluated with the same throughput in terms of
degrees of freedom per second as a low-order method.

More information on the algorithms are available in the preprint
<br>
<a href="https://arxiv.org/abs/1711.03590">Fast matrix-free evaluation of
discontinuous Galerkin finite element operators</a> by Martin Kronbichler and
Katharina Kormann, arXiv:1711.03590.

<h3>The symmetric interior penalty formulation for the Laplacian</h3>

For this tutorial program, we exemplify the matrix-free DG framework for the
interior penalty discretization of the Laplacian, i.e., the same scheme as the
one used for the step-39 tutorial program. The discretization of the Laplacian
is given by the following weak form
@f{align*}
&\sum_{K\in\text{cells}} \left(\nabla v_h, \nabla u_h\right)_{K}+\\
&\sum_{F\in\text{faces}}\Big(-\left<\jump{v_h}, \average{\nabla u_h}\right>_{F} - \left<\average{\nabla v_h}, \jump{u_h}\right>_{F} + \left<\jump{v_h}, \sigma \jump{u_h}\right>_{F}\Big) \\
&= \sum_{K\in\text{cells}}\left(v_h, f\right)_{K},
@f}
where $\jump{v} = v^- \mathbf{n}^- + v^+ \mathbf{n}^+ = \mathbf n^{-}
\left(v^- - v^+\right)$ denotes the directed jump of the quantity $v$ from the
two associated cells $K^-$ and $K^+$, and $\average{v}=\frac{v^- + v^+}{2}$
is the average from both sides.

The terms in the equation represent the cell integral after integration by
parts, the primal consistency term that arises at the element interfaces due
to integration by parts and insertion of an average flux, the adjoint
consistency term that is added for restoring symmetry of the underlying
matrix, and a penalty term with factor $\sigma$, whose magnitude is equal the
length of the cells in direction normal to face multiplied by $k(k+1)$, see
step-39. The penalty term is chosen such that an inverse estimate holds and
the final weak form is coercive, i.e., positive definite in the discrete
setting. The adjoint consistency term and the penalty term involve the jump
$\jump{u_h}$ at the element interfaces, which disappears for the analytic
solution $u$. Thus, these terms are consistent with the original PDE, ensuring
that the method can retain optimal orders of convergence.

In the implementation below, we implement the weak form above by moving the
normal vector $\mathbf{n}^-$ from the jump terms to the derivatives to form a
<i>normal</i> derivative of the form $\mathbf{n}^-\cdot \nabla u_h$. This
makes the implementation on quadrature points slightly more efficient because
we only need to work with scalar terms rather than tensors, and is
mathematically equivalent.

For boundary conditions, we use the so-called mirror principle that defines
<i>artificial</i> exterior values $u^+$ by extrapolation from the interior
solution $u^-$ combined with the given boundary data, setting $u^+ = -u^- + 2
g_\text{D}$ and $\mathbf{n}^-\cdot \nabla u^+ = \mathbf{n}^-\cdot \nabla u^-$
on Dirichlet boundaries and $u^+=u^-$ and $\mathbf{n}^-\cdot \nabla u^+ =
-\mathbf{n}^-\cdot \nabla u^- + 2 g_\text{N}$ on Neumann boundaries, for given
Dirichlet values $g_\text{D}$ and Neumann values $g_\text{N}$. These
expressions are then inserted in the above weak form. Contributions involving
the known quantities $g_\text{D}$ and $g_\text{N}$ are eventually moved to the
right hand side, whereas the unknown value $u^-$ is retained on the left hand
side and contributes to the matrix terms similarly as interior faces. Upon
these manipulations, the same weak form as in step-39 is obtained.

<h3>Face integration support in MatrixFree and FEFaceEvaluation</h3>

The matrix-free framework of deal.II provides the necessary infrastructure to
implement the action of the discretized equation above. As opposed to the
MatrixFree::cell_loop() that we used in step-37 and step-48, we now build a
code in terms of MatrixFree::loop() that takes three function pointers, one
for the cell integrals, one for the inner face integrals, and one for the
boundary face integrals (in analogy to the design of MeshWorker used in the
step-39 tutorial program). In each of these three functions, we then implement
the respective terms on the quadrature points. For interpolation between the
vector entries and the values and gradients on quadrature points, we use the
class FEEvaluation for cell contributions and FEFaceEvaluation for face
contributions. The basic usage of these functions has been discussed
extensively in the step-37 tutorial program.

In MatrixFree::loop(), all interior faces are visited exactly once, so one
must make sure to compute the contributions from both the test functions
$v_h^-$ and $v_h^+$. Given the fact that the test functions on both sides are
indeed independent, the weak form above effectively means that we submit the
same contribution to both an FEFaceEvaluation object called `phi_inner` and
`phi_outer` for testing with the normal derivative of the test function, and
values with opposite sign for testing with the values of the test function,
because the latter involves opposite signs due to the jump term. For faces
between cells of different refinement level, the integration is done from the
refined side, and FEFaceEvaluation automatically performs interpolation to a
subface on the coarse side. Thus, a hanging node never appears explicitly in a
user implementation of a weak form.

The fact that each face is visited exactly once also applies to those faces at
subdomain boundaries between different processors when parallelized with MPI,
where one cell belongs to one processor and one to the other. The setup in
MatrixFree::reinit() splits the faces between the two sides, and eventually
only reports the faces actually handled locally in
MatrixFree::n_inner_face_batches() and MatrixFree::n_boundary_face_batches(),
respectively. Note that, in analogy to the cell integrals discussed in
step-37, deal.II applies vectorization over several faces to use SIMD, working
on something we call a <i>batch of faces</i> with a single instruction. The
face batches are independent from the cell batches, even though the time at
which face integrals are processed is kept close to the time when the cell
integrals of the respective cells are processed, in order to increase the data
locality.

Another thing that is new in this program is the fact that we no longer split
the vector access like FEEvaluation::read_dof_values() or
FEEvaluation::distribute_local_to_global() from the evaluation and integration
steps, but call combined functions FEEvaluation::gather_evaluate() and
FEEvaluation::integrate_scatter(), respectively. This is useful for face
integrals because, depending on what gets evaluated on the faces, not all
vector entries of a cell must be touched in the first place. Think for example
of the case of the nodal element FE_DGQ with node points on the element
surface: If we are interested in the shape function values on a face, only
$(k+ 1)^{d-1}$ degrees of freedom contribute to them in a non-trivial way (in
a more technical way of speaking, only $(k+1)^{d-1}$ shape functions have a
nonzero support on the face and return true for
FiniteElement::has_support_on_face()). When compared to the $(k+1)^d$ degrees
of freedom of a cell, this is one power less.

Now of course we are not interested in only the function values, but also the
derivatives on the cell. Fortunately, there is an element in deal.II that
extends this property of reduced access also for derivatives on faces, the
FE_DGQHermite element.

<h3>The FE_DGQHermite element</h3>

The element FE_DGQHermite belongs to the family of FE_DGQ elements, i.e., its
shape functions are a tensor product of 1D polynomials and the element is
fully discontinuous. As opposed to the nodal character in the usual FE_DGQ
element, the FE_DGQHermite element is a mixture of nodal contributions and
derivative contributions based on a Hermite-like concept. The underlying
polynomial class is Polynomials::HermiteLikeInterpolation and can be
summarized as follows: For cubic polynomials, we use two polynomials to
represent the function value and first derivative at the left end of the unit
interval, $x=0$, and two polynomials to represent the function value and first
derivative and the right end of the unit interval, $x=1$. At the opposite
ends, both the value and first derivative of the shape functions are zero,
ensuring that only two out of the four basis functions contribute to values
and derivative on the respective end. However, we deviate from the classical
Hermite interpolation in not strictly assigning one degree of freedom for the
value and one for the first derivative, but rather allow the first derivative
to be a linear combination of the first and the second shape function. This is
done to improve the conditioning of the interpolation. Also, when going to
degrees beyond three, we add node points in the element interior in a
Lagrange-like fashion, combined with double zeros in the points $x=0$ and
$x=1$. The position of these extra nodes is determined by the zeros of some
Jacobi polynomials as explained in the description of the class
Polynomials::HermiteLikeInterpolation.

Using this element, we only need to access $2(k+1)^{d-1}$ degrees of freedom
for computing both values and derivatives on a face. The check whether the
Hermite property is fulfilled is done transparently inside
FEFaceEvaluation::gather_evaluate() and FEFaceEvaluation::integrate_scatter()
that check the type of the basis and reduce the access to data if
possible. Obviously, this would not be possible if we had separated
FEFaceEvaluation::read_dof_values() from FEFaceEvaluation::evaluate(), because
the amount of entries we need to read depends on the type of the derivative
(only values, first derivative, etc.) and thus must be given to
`read_dof_values()`.

This optimization is not only useful for computing the face integrals, but
also for the MPI ghost layer exchange: In a naive exchange, we would need to
send all degrees of freedom of a cell to another processor if the other
processor is responsible for computing the face's contribution. Since we know
that only some of the degrees of freedom in the evaluation with
FEFaceEvaluation are touched, it is natural to only exchange the relevant
ones. The MatrixFree::loop() function has support for a selected data exchange
when combined with LinearAlgebra::distributed::Vector. To make this happen, we
need to tell the loop what kind of evaluation on faces we are going to do,
using an argument of type MatrixFree::DataAccessOnFaces, as can be seen in the
implementation of `LaplaceOperator::vmult()` below. The way data is exchanged
in that case is as follows: The ghost layer data in the vector still pretends
to represent all degrees of freedom, such that FEFaceEvaluation can continue
to read the values as if the cell were a locally owned one. The data exchange
routines take care of the task for packing and unpacking the data into this
format. While this sounds pretty complicated, we will show in the results
section below that this really pays off by comparing the performance to a
baseline code that does not specify the data access on faces.

<h3>An approximate block-Jacobi smoother using the fast diagonalization method</h3>

In the tradition of the step-37 program, we again solve a Poisson problem with
a geometric multigrid preconditioner inside a conjugate gradient
solver. Instead of computing the diagonal and use the basic
PreconditionChebyshev as a smoother, we choose a different strategy in this
tutorial program. We implement a block-Jacobi preconditioner, where a block
refers to all degrees of freedom on a cell. Rather than building the full cell
matrix and applying its LU factorization (or inverse) in the preconditioner
&mdash; an operation that would be heavily memory bandwidth bound and thus
pretty slow &mdash; we approximate the inverse of the block by a special
technique called fast diagonalization method.

The idea of the method is to take use of the structure of the cell matrix. In
case of the Laplacian with constant coefficients discretized on a Cartesian
mesh, the cell matrix $L$ can be written as
@f{align*}{
L &= A_1 \otimes M_0 + M_1 \otimes A_0
@f}
in 2D and
@f{align*}{
L &= A_2 \otimes M_1 \otimes M_0 + M_2 \otimes A_1 \otimes M_0 + M_2 \otimes M_1 \otimes A_0
@f}
in 3D. The matrices $A_0$ and $A_1$ denote the 1D Laplace matrix (including
the cell and face term associated to the current cell values $u^-_h$ and
$v^-_h$) and $M_0$ and $M_1$ are the mass matrices. Note that this simple
tensor product structure is lost once there are non-constant coefficients on
the cell or the geometry is not constant any more. We mention that a similar
setup could also be used to replace the computed integrals with this final
tensor product form of the matrices, which would cut the operations for the
operator evaluation into less than half. However, given the fact that this
only holds for Cartesian cells and constant coefficients, which is a pretty
narrow case, we refrain from pursuing this idea.

Interestingly, the exact inverse of the matrix $L$ can be found through tensor
products due to a method introduced by <a
href="http://dl.acm.org/citation.cfm?id=2716130">R. E. Lynch, J. R. Rice,
D. H. Thomas, Direct solution of partial difference equations by tensor
product methods, Numerische Mathematik 6, 185-199</a> from 1964,
@f{align*}{
L^{-1} &= S_1 \otimes S_0 (\Lambda_1 \otimes I + I \otimes \Lambda_0)^{-1}
S_1^\mathrm T \otimes S_0^\mathrm T,
@f}
where $S_d$ is the matrix of eigenvectors to the generalized eigenvalue problem
in the given tensor direction $d$:
@f{align*}{
A_d s  &= \lambda M_d s, \quad d = 0, \ldots,\mathrm{dim-1},
@f}
and $\Lambda_d$ is the diagonal matrix representing the generalized
eigenvalues $\lambda$. Note that the vectors $s$ are such that they
simultaneously diagonalize $A_d$ and $M_d$, i.e. $S_d^{\mathrm T} A_d S_d =
\Lambda_d$ and $S_d^{\mathrm T} M_d S_d = I$.

The deal.II library implements a class using this concept, called
TensorProductMatrixSymmetricSum.

For the sake of this program, we stick with constant coefficients and
Cartesian meshes, even though an approximate version based on tensor products
would still be possible for a more general mesh, and the operator evaluation
itself is of course generic. Also, we do not bother with adaptive meshes where
the multigrid algorithm would need to get access to flux matrices over the
edges of different refinement, as explained in step-39. One thing we do,
however, is to still wrap our block-Jacobi preconditioner inside
PreconditionChebyshev. That class relieves us from finding an appropriate
relaxation parameter (which would be around 0.7 in 2D and 0.5 in 3D for the
block-Jacobi smoother), and often increases smoothing efficiency a bit over
plain Jacobi smoothing in that it enables lower the time to solution when
setting the degree of the Chebyshev polynomial to one or two.

Note that the block-Jacobi smoother has an additional benefit: The fast
diagonalization method can also be interpreted as a change from the
Hermite-like polynomials underlying FE_DGQHermite to a basis where the cell
Laplacian is diagonal. Thus, it cancels the effect of the basis, and we get
the same iteration counts irrespective of whether we use FE_DGQHermite or
FE_DGQ. This is in contrast to using the PreconditionChebyshev class with only
the diagonal (a point-Jacobi scheme), where FE_DGQ and FE_DGQHermite do indeed
behave differently and FE_DGQ needs 2-5 less iterations than FE_DGQHermite,
despite the modification made to the Hermite-like shape functions to ensure a
good conditioning.


examples/step-59/doc/results.dox
<h1>Results</h1>

<h3>Program output</h3>

Like in step-37, we evaluate the multigrid solver in terms of run time.  In
two space dimensions with elements of degree 8, a possible output could look
as follows:
@code
Running with 12 MPI processes, element FE_DGQHermite<2>(8)

Cycle 0
Number of degrees of freedom: 5184
Total setup time              0.0282445 s
Time solve (14 iterations)    0.0110712 s
Verification via L2 error:    1.66232e-07

Cycle 1
Number of degrees of freedom: 20736
Total setup time              0.0126282 s
Time solve (14 iterations)    0.0157021 s
Verification via L2 error:    2.91505e-10

Cycle 2
Number of degrees of freedom: 82944
Total setup time              0.0227573 s
Time solve (14 iterations)    0.026568 s
Verification via L2 error:    6.64514e-13

Cycle 3
Number of degrees of freedom: 331776
Total setup time              0.0604685 s
Time solve (14 iterations)    0.0628356 s
Verification via L2 error:    5.57513e-13

Cycle 4
Number of degrees of freedom: 1327104
Total setup time              0.154359 s
Time solve (13 iterations)    0.219555 s
Verification via L2 error:    3.08139e-12

Cycle 5
Number of degrees of freedom: 5308416
Total setup time              0.467764 s
Time solve (13 iterations)    1.1821 s
Verification via L2 error:    3.90334e-12

Cycle 6
Number of degrees of freedom: 21233664
Total setup time              1.73263 s
Time solve (13 iterations)    5.21054 s
Verification via L2 error:    4.94543e-12
@endcode

Like in step-37, the number of CG iterations remains constant with increasing
problem size. The iteration counts are a bit higher, which is because we use a
lower degree of the Chebyshev polynomial (2 vs 5 in step-37) and because the
interior penalty discretization has a somewhat larger spread in
eigenvalues. Nonetheless, 13 iterations to reduce the residual by 12 orders of
magnitude, or almost a factor of 9 per iteration, indicates an overall very
efficient method. In particular, we can solve a system with 21 million degrees
of freedom in 5 seconds when using 12 cores, which is a very good
efficiency. Of course, in 2D we are well inside the regime of roundoff for a
polynomial degree of 8 &ndash; as a matter of fact, around 83k DoFs or 0.025s
would have been enough to fully converge this (simple) analytic solution
here.

Not much changes if we run the program in three spatial dimensions, except for
the fact that we now use do something more useful with the higher polynomial
degree and increasing mesh sizes, as the roundoff errors are only obtained at
the finest mesh. Still, it is remarkable that we can solve a 3D Laplace
problem with a wave of three periods to roundoff accuracy on a twelve-core
machine pretty easily - using about 3.5 GB of memory in total for the second
to largest case with 24m DoFs, taking not more than eight seconds. The largest
case uses 30GB of memory with 191m DoFs.

@code
Running with 12 MPI processes, element FE_DGQHermite<3>(8)

Cycle 0
Number of degrees of freedom: 5832
Total setup time              0.0210681 s
Time solve (15 iterations)    0.0956945 s
Verification via L2 error:    0.0297194

Cycle 1
Number of degrees of freedom: 46656
Total setup time              0.0452428 s
Time solve (15 iterations)    0.113827 s
Verification via L2 error:    9.55733e-05

Cycle 2
Number of degrees of freedom: 373248
Total setup time              0.190423 s
Time solve (15 iterations)    0.218309 s
Verification via L2 error:    2.6868e-07

Cycle 3
Number of degrees of freedom: 2985984
Total setup time              0.627914 s
Time solve (15 iterations)    1.0595 s
Verification via L2 error:    4.6918e-10

Cycle 4
Number of degrees of freedom: 23887872
Total setup time              2.85215 s
Time solve (15 iterations)    8.30576 s
Verification via L2 error:    9.38583e-13

Cycle 5
Number of degrees of freedom: 191102976
Total setup time              16.1324 s
Time solve (15 iterations)    65.57 s
Verification via L2 error:    3.17875e-13
@endcode

<h3>Comparison of efficiency at different polynomial degrees</h3>

In the introduction and in-code comments, it was mentioned several times that
high orders are treated very efficiently with the FEEvaluation and
FEFaceEvaluation evaluators. Now, we want to substantiate these claims by
looking at the throughput of the 3D multigrid solver for various polynomial
degrees. We collect the times as follows: We first run a solver at problem
size close to ten million, indicated in the first four table rows, and record
the timings. Then, we normalize the throughput by recording the number of
million degrees of freedom solved per second (MDoFs/s) to be able to compare
the efficiency of the different degrees, which is computed by dividing the
number of degrees of freedom by the solver time.

<table align="center" class="doxtable">
  <tr>
   <th>degree</th>
   <th>1</th>
   <th>2</th>
   <th>3</th>
   <th>4</th>
   <th>5</th>
   <th>6</th>
   <th>7</th>
   <th>8</th>
   <th>9</th>
   <th>10</th>
   <th>11</th>
   <th>12</th>
  </tr>
  <tr>
   <th>Number of DoFs</th>
   <td>2097152</td>
   <td>7077888</td>
   <td>16777216</td>
   <td>32768000</td>
   <td>7077888</td>
   <td>11239424</td>
   <td>16777216</td>
   <td>23887872</td>
   <td>32768000</td>
   <td>43614208</td>
   <td>7077888</td>
   <td>8998912</td>
  </tr>
  <tr>
   <th>Number of iterations</th>
   <td>13</td>
   <td>12</td>
   <td>12</td>
   <td>12</td>
   <td>13</td>
   <td>13</td>
   <td>15</td>
   <td>15</td>
   <td>17</td>
   <td>19</td>
   <td>18</td>
   <td>18</td>
  </tr>
  <tr>
   <th>Solver time [s]</th>
   <td>0.713</td>
   <td>2.150</td>
   <td>4.638</td>
   <td>8.803</td>
   <td>2.041</td>
   <td>3.295</td>
   <td>5.723</td>
   <td>8.306</td>
   <td>12.75</td>
   <td>19.25</td>
   <td>3.530</td>
   <td>4.814</td>
  </tr>
  <tr>
   <th>MDoFs/s</th>
   <td>2.94</td>
   <td>3.29</td>
   <td>3.62</td>
   <td>3.72</td>
   <td>3.47</td>
   <td>3.41</td>
   <td>2.93</td>
   <td>2.88</td>
   <td>2.57</td>
   <td>2.27</td>
   <td>2.01</td>
   <td>1.87</td>
  </tr>
</table>

We clearly see how the efficiency per DoF initially improves until it reaches
a maximum for the polynomial degree $k=4$. This effect is surprising, not only
because higher polynomial degrees often yield a vastly better solution, but
especially also when having matrix-based schemes in mind where the denser
coupling at higher degree leads to a monotonously decreasing throughput (and a
drastic one in 3D, with $k=4$ being more than ten times slower than
$k=1$!). For higher degrees, the throughput decreases a bit, which is both due
to an increase in the number of iterations (going from 12 at $k=2,3,4$ to 19
at $k=10$) and due to the $\mathcal O(k)$ complexity of operator
evaluation. Nonetheless, efficiency as the time to solution would be still
better for higher polynomial degrees because they have better convergence rates (at least
for problems as simple as this one): For $k=12$, we reach roundoff accuracy
already with 1 million DoFs (solver time less than a second), whereas for $k=8$
we need 24 million DoFs and 8 seconds. For $k=5$, the error is around
$10^{-9}$ with 57m DoFs and thus still far away from roundoff, despite taking 16
seconds.

Note that the above numbers are a bit pessimistic because they include the
time it takes the Chebyshev smoother to compute an eigenvalue estimate, which
is around 10 percent of the solver time. If the system is solved several times
(as e.g. common in fluid dynamics), this eigenvalue cost is only paid once and
faster times become available.

<h3>Evaluation of efficiency of ingredients</h3>

Finally, we take a look at some of the special ingredients presented in this
tutorial program, namely the FE_DGQHermite basis in particular and the
specification of MatrixFree::DataAccessOnFaces. In the following table, the
third row shows the optimized solver above, the fourth row shows the timings
with only the MatrixFree::DataAccessOnFaces set to `unspecified` rather than
the optimal `gradients`, and the last one with replacing FE_DGQHermite by the
basic FE_DGQ elements where both the MPI exchange are more expensive and the
operations done by FEFaceEvaluation::gather_evaluate() and
FEFaceEvaluation::integrate_scatter().

<table align="center" class="doxtable">
  <tr>
   <th>degree</th>
   <th>1</th>
   <th>2</th>
   <th>3</th>
   <th>4</th>
   <th>5</th>
   <th>6</th>
   <th>7</th>
   <th>8</th>
   <th>9</th>
   <th>10</th>
   <th>11</th>
   <th>12</th>
  </tr>
  <tr>
   <th>Number of DoFs</th>
   <td>2097152</td>
   <td>7077888</td>
   <td>16777216</td>
   <td>32768000</td>
   <td>7077888</td>
   <td>11239424</td>
   <td>16777216</td>
   <td>23887872</td>
   <td>32768000</td>
   <td>43614208</td>
   <td>7077888</td>
   <td>8998912</td>
  </tr>
  <tr>
   <th>Solver time optimized as in tutorial [s]</th>
   <td>0.713</td>
   <td>2.150</td>
   <td>4.638</td>
   <td>8.803</td>
   <td>2.041</td>
   <td>3.295</td>
   <td>5.723</td>
   <td>8.306</td>
   <td>12.75</td>
   <td>19.25</td>
   <td>3.530</td>
   <td>4.814</td>
  </tr>
  <tr>
   <th>Solver time MatrixFree::DataAccessOnFaces::unspecified [s]</th>
   <td>0.711</td>
   <td>2.151</td>
   <td>4.675</td>
   <td>8.968</td>
   <td>2.243</td>
   <td>3.655</td>
   <td>6.277</td>
   <td>9.082</td>
   <td>13.50</td>
   <td>20.05</td>
   <td>3.817</td>
   <td>5.178</td>
  </tr>
  <tr>
   <th>Solver time FE_DGQ [s]</th>
   <td>0.712</td>
   <td>2.041</td>
   <td>5.066</td>
   <td>9.335</td>
   <td>2.379</td>
   <td>3.802</td>
   <td>6.564</td>
   <td>9.714</td>
   <td>14.54</td>
   <td>22.76</td>
   <td>4.148</td>
   <td>5.857</td>
  </tr>
</table>

The data in the table shows that not using MatrixFree::DataAccessOnFaces
increases costs by around 10% for higher polynomial degrees. For lower
degrees, the difference is obviously less pronounced because the
volume-to-surface ratio is more beneficial and less data needs to be
exchanged. The difference is larger when looking at the matrix-vector product
only, rather than the full multigrid solver shown here, with around 20% worse
timings just because of the MPI communication.

For $k=1$ and $k=2$, the Hermite-like basis functions do obviously not really
pay off (indeed, for $k=1$ the polynomials are exactly the same as for FE_DGQ)
and the results are similar as with the FE_DGQ basis. However, for degrees
starting at three, we see an increasing advantage for FE_DGQHermite, showing
the effectiveness of these basis functions.

<h3>Possibilities for extension</h3>

As mentioned in the introduction, the fast diagonalization method is tied to a
Cartesian mesh with constant coefficients. If we wanted to solve
variable-coefficient problems, we would need to invest a bit more time in the
design of the smoother parameters by selecting proper generalizations (e.g.,
approximating the inverse on the nearest box-shaped element).

Another way of extending the program would be to include support for adaptive
meshes, for which interface operations at edges of different refinement
level become necessary, as discussed in step-39.


