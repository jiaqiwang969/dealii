examples/step-55/doc/intro.dox
<br>

<i>This program was contributed by Timo Heister. Special thanks to Sander
Rhebergen for the inspiration to finally write this tutorial.

This material is based upon work partially supported by National Science
Foundation grant DMS1522191 and the Computational Infrastructure in
Geodynamics initiative (CIG), through the National Science Foundation under
Award No. EAR-0949446 and The University of California-Davis.

The authors would like to thank the Isaac Newton Institute for
Mathematical Sciences, Cambridge, for support and hospitality during
the programme Melt in the Mantle where work on this tutorial was
undertaken. This work was supported by EPSRC grant no EP/K032208/1.
</i>


@note As a prerequisite of this program, you need to have PETSc or Trilinos
and the p4est library installed. The installation of deal.II together with
these additional libraries is described in the <a href="../../readme.html"
target="body">README</a> file.

<a name="Intro"></a>
<h1>Introduction</h1>

Building on step-40, this tutorial shows how to solve linear PDEs with several
components in parallel using MPI with PETSc or Trilinos for the linear
algebra. For this, we return to the Stokes equations as discussed in
step-22. The motivation for writing this tutorial is to provide an
intermediate step (pun intended) between step-40 (parallel Laplace) and
step-32 (parallel coupled Stokes with Boussinesq for a time dependent
problem).

The learning outcomes for this tutorial are:

- You are able to solve PDEs with several variables in parallel and can
  apply this to different problems.

- You understand the concept of optimal preconditioners and are able to check
  this for a particular problem.

- You are able to construct manufactured solutions using the free computer
  algreba system SymPy (https://sympy.org).

- You can implement various other tasks for parallel programs: error
  computation, writing graphical output, etc.

- You can visualize vector fields, stream lines, and contours of vector
  quantities.

We are solving for a velocity $\textbf{u}$ and pressure $p$ that satisfy the
Stokes equation, which reads
@f{eqnarray*}
  - \triangle \textbf{u} + \nabla p &=& \textbf{f}, \\
  -\textrm{div}\; \textbf{u} &=& 0.
@f}


<h3>Optimal preconditioners</h3>

Make sure that you read (even better: try) what is described in "Block Schur
complement preconditioner" in the "Possible Extensions" section in step-22.
Like described there, we are going to solve the block system using a Krylov
method and a block preconditioner.

Our goal here is to construct a very simple (maybe the simplest?) optimal
preconditioner for the linear system. A preconditioner is called "optimal" or
"of optimal complexity", if the number of iterations of the preconditioned
system is independent of the mesh size $h$. You can extend that definition to
also require indepence of the number of processors used (we will discuss that
in the results section), the computational domain and the mesh quality, the
test case itself, the polynomial degree of the finite element space, and more.

Why is a constant number of iterations considered to be "optimal"? Assume the
discretized PDE gives a linear system with N unknowns. Because the matrix
coming from the FEM discretization is sparse, a matrix-vector product can be
done in O(N) time. A preconditioner application can also only be O(N) at best
(for example doable with multigrid methods). If the number of iterations
required to solve the linear system is independent of $h$ (and therefore N),
the total cost of solving the system will be O(N). It is not possible to beat
this complexity, because even looking at all the entries of the right-hand
side already takes O(N) time. For more information see @cite elman2005,
Chapter 2.5 (Multigrid).

The preconditioner described here is even simpler than the one described in
step-22 and will typically require more iterations and consequently time to
solve. When considering preconditioners, optimality is not the only important
metric. But an optimal and expensive preconditioner is typically more
desirable than a cheaper, non-optimal one. This is because, eventually, as the
mesh size becomes smaller and smaller and linear problems become bigger and
bigger, the former will eventually beat the latter.

<h3>The solver and preconditioner</h3>

We precondition the linear system
@f{eqnarray*}
  \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
  \left(\begin{array}{c}
    U \\ P
  \end{array}\right)
  =
  \left(\begin{array}{c}
    F \\ 0
  \end{array}\right),
@f}

with the block diagonal preconditioner
@f{eqnarray*}
  P^{-1}
  =
  \left(\begin{array}{cc}
    A & 0 \\ 0 & S
  \end{array}\right) ^{-1},
  =
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ 0 & S^{-1}
  \end{array}\right),
@f}
where $S=-BA^{-1} B^T$ is the Schur complement.

With this choice of $P$, assuming that we handle $A^{-1}$ and $S^{-1}$ exactly
(which is an "idealized" situation), the preconditioned linear system has
three distinct eigenvalues independent of $h$ and is therefore "optimal".  See
section 6.2.1 (especially p. 292) in @cite elman2005. For comparison,
using the ideal version of the upper block-triangular preconditioner in
step-22 (also used in step-56) would have all eigenvalues be equal to one.

We will use approximations of the inverse operations in $P^{-1}$ that are
(nearly) independent of $h$. In this situation, one can again show, that the
eigenvalues are independent of $h$. For the Krylov method we choose MINRES,
which is attractive for the analysis (iteration count is proven to be
independent of $h$, see the remainder of the chapter 6.2.1 in the book
mentioned above), great from the computational standpoint (simpler and cheaper
than GMRES for example), and applicable (matrix and preconditioner are
symmetric).

For the approximations we will use a CG solve with the mass matrix in the
pressure space for approximating the action of $S^{-1}$. Note that the mass
matrix is spectrally equivalent to $S$. We can expect the number of CG
iterations to be independent of $h$, even with a simple preconditioner like
ILU.

For the approximation of the velocity block $A$ we will perform a single AMG
V-cycle. In practice this choice is not exactly independent of $h$, which can
explain the slight increase in iteration numbers. A possible explanation is
that the coarsest level will be solved exactly and the number of levels and
size of the coarsest matrix is not predictable.


<h3>The testcase</h3>

We will construct a manufactured solution based on the classical Kovasznay problem,
see @cite kovasznay1948laminar. Here
is an image of the solution colored by the x velocity including
streamlines of the velocity:

 <img src="https://www.dealii.org/images/steps/developer/step-55.solution.png" alt="">

We have to cheat here, though, because we are not solving the non-linear
Navier-Stokes equations, but the linear Stokes system without convective
term. Therefore, to recreate the exact same solution, we use the method of
manufactured solutions with the solution of the Kovasznay problem. This will
effectively move the convective term into the right-hand side $f$.

The right-hand side is computed using the script "reference.py" and we use
the exact solution for boundary conditions and error computation.


examples/step-55/doc/results.dox
<h1>Results</h1>

As expected from the discussion above, the number of iterations is independent
of the number of processors and only very slightly dependent on $h$:

<table>
<tr>
  <th colspan="2">PETSc</th>
  <th colspan="8">number of processors</th>
</tr>
<tr>
  <th>cycle</th>
  <th>dofs</th>
  <th>1</th>
  <th>2</th>
  <th>4</th>
  <th>8</th>
  <th>16</th>
  <th>32</th>
  <th>64</th>
  <th>128</th>
</tr>
<tr>
  <td>0</td>
  <td>659</td>
  <td>49</td>
  <td>49</td>
  <td>49</td>
  <td>51</td>
  <td>51</td>
  <td>51</td>
  <td>49</td>
  <td>49</td>
</tr>
<tr>
  <td>1</td>
  <td>2467</td>
  <td>52</td>
  <td>52</td>
  <td>52</td>
  <td>52</td>
  <td>52</td>
  <td>54</td>
  <td>54</td>
  <td>53</td>
</tr>
<tr>
  <td>2</td>
  <td>9539</td>
  <td>56</td>
  <td>56</td>
  <td>56</td>
  <td>54</td>
  <td>56</td>
  <td>56</td>
  <td>54</td>
  <td>56</td>
</tr>
<tr>
  <td>3</td>
  <td>37507</td>
  <td>57</td>
  <td>57</td>
  <td>57</td>
  <td>57</td>
  <td>57</td>
  <td>56</td>
  <td>57</td>
  <td>56</td>
</tr>
<tr>
  <td>4</td>
  <td>148739</td>
  <td>58</td>
  <td>59</td>
  <td>57</td>
  <td>59</td>
  <td>57</td>
  <td>57</td>
  <td>57</td>
  <td>57</td>
</tr>
<tr>
  <td>5</td>
  <td>592387</td>
  <td>60</td>
  <td>60</td>
  <td>59</td>
  <td>59</td>
  <td>59</td>
  <td>59</td>
  <td>59</td>
  <td>59</td>
</tr>
<tr>
  <td>6</td>
  <td>2364419</td>
  <td>62</td>
  <td>62</td>
  <td>61</td>
  <td>61</td>
  <td>61</td>
  <td>61</td>
  <td>61</td>
  <td>61</td>
</tr>
</table>

<table>
<tr>
  <th colspan="2">Trilinos</th>
  <th colspan="8">number of processors</th>
</tr>
<tr>
  <th>cycle</th>
  <th>dofs</th>
  <th>1</th>
  <th>2</th>
  <th>4</th>
  <th>8</th>
  <th>16</th>
  <th>32</th>
  <th>64</th>
  <th>128</th>
</tr>
<tr>
  <td>0</td>
  <td>659</td>
  <td>37</td>
  <td>37</td>
  <td>37</td>
  <td>37</td>
  <td>37</td>
  <td>37</td>
  <td>37</td>
  <td>37</td>
</tr>
<tr>
  <td>1</td>
  <td>2467</td>
  <td>92</td>
  <td>89</td>
  <td>89</td>
  <td>82</td>
  <td>86</td>
  <td>81</td>
  <td>78</td>
  <td>78</td>
</tr>
<tr>
  <td>2</td>
  <td>9539</td>
  <td>102</td>
  <td>99</td>
  <td>96</td>
  <td>95</td>
  <td>95</td>
  <td>88</td>
  <td>83</td>
  <td>95</td>
</tr>
<tr>
  <td>3</td>
  <td>37507</td>
  <td>107</td>
  <td>105</td>
  <td>104</td>
  <td>99</td>
  <td>100</td>
  <td>96</td>
  <td>96</td>
  <td>90</td>
</tr>
<tr>
  <td>4</td>
  <td>148739</td>
  <td>112</td>
  <td>112</td>
  <td>111</td>
  <td>111</td>
  <td>127</td>
  <td>126</td>
  <td>115</td>
  <td>117</td>
</tr>
<tr>
  <td>5</td>
  <td>592387</td>
  <td>116</td>
  <td>115</td>
  <td>114</td>
  <td>112</td>
  <td>118</td>
  <td>120</td>
  <td>131</td>
  <td>130</td>
</tr>
<tr>
  <td>6</td>
  <td>2364419</td>
  <td>130</td>
  <td>126</td>
  <td>120</td>
  <td>120</td>
  <td>121</td>
  <td>122</td>
  <td>121</td>
  <td>123</td>
</tr>
</table>

While the PETSc results show a constant number of iterations, the iterations
increase when using Trilinos. This is likely because of the different settings
used for the AMG preconditioner. For performance reasons we do not allow
coarsening below a couple thousand unknowns. As the coarse solver is an exact
solve (we are using LU by default), a change in number of levels will
influence the quality of a V-cycle. Therefore, a V-cycle is closer to an exact
solver for smaller problem sizes.

<a name="extensions"></a>
<h3>Possibilities for extensions</h3>

<h4>Investigate Trilinos iterations</h4>

Play with the smoothers, smoothing steps, and other properties for the
Trilinos AMG to achieve an optimal preconditioner.

<h4>Solve the Oseen problem instead of the Stokes system</h4>

This change requires changing the outer solver to GMRES or BiCGStab, because
the system is no longer symmetric.

You can prescribe the exact flow solution as $b$ in the convective term $b
\cdot \nabla u$. This should give the same solution as the original problem,
if you set the right hand side to zero.

<h4>Adaptive refinement</h4>

So far, this tutorial program refines the mesh globally in each step.
Replacing the code in StokesProblem::refine_grid() by something like
@code
Vector<float> estimated_error_per_cell(triangulation.n_active_cells());

FEValuesExtractors::Vector velocities(0);
KellyErrorEstimator<dim>::estimate(
  dof_handler,
  QGauss<dim - 1>(fe.degree + 1),
  std::map<types::boundary_id, const Function<dim> *>(),
  locally_relevant_solution,
  estimated_error_per_cell,
  fe.component_mask(velocities));
parallel::distributed::GridRefinement::refine_and_coarsen_fixed_number(
  triangulation, estimated_error_per_cell, 0.3, 0.0);
triangulation.execute_coarsening_and_refinement();
@endcode
makes it simple to explore adaptive mesh refinement.


