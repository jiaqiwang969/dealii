examples/step-18/doc/intro.dox
<a name="Intro"></a>
<h1>Introduction</h1>


This tutorial program is another one in the series on the elasticity problem
that we have already started with step-8 and step-17. It extends it into two
different directions: first, it solves the quasistatic but time dependent
elasticity problem for large deformations with a Lagrangian mesh movement
approach. Secondly, it shows some more techniques for solving such problems
using %parallel processing with PETSc's linear algebra. In addition to this,
we show how to work around one of the two major bottlenecks of step-17, namely
that we generated graphical output from only one process, and that this scaled
very badly with larger numbers of processes and on large problems. (The other
bottleneck, namely that every processor has to hold the entire mesh and
DoFHandler, is addressed in step-40.) Finally, a
good number of assorted improvements and techniques are demonstrated that have
not been shown yet in previous programs.

As before in step-17, the program runs just as fine on a single sequential
machine as long as you have PETSc installed. Information on how to tell
deal.II about a PETSc installation on your system can be found in the deal.II
README file, which is linked to from the <a href="../../index.html">main
documentation page</a>
in your installation of deal.II, or on <a href="http://www.dealii.org/">the
deal.II webpage</a>.


<h3>Quasistatic elastic deformation</h3>

<h4>Motivation of the model</h4>

In general, time-dependent small elastic deformations are described by the
elastic wave equation
@f[
  \rho \frac{\partial^2 \mathbf{u}}{\partial t^2}
  + c \frac{\partial \mathbf{u}}{\partial t}
  - \textrm{div}\  ( C \varepsilon(\mathbf{u})) = \mathbf{f}
  \qquad
  \textrm{in}\ \Omega,
@f]
where $\mathbf{u}=\mathbf{u} (\mathbf{x},t)$ is the deformation of the body, $\rho$
and $c$ the density and attenuation coefficient, and $\mathbf{f}$ external forces.
In addition, initial conditions
@f[
  \mathbf{u}(\cdot, 0) = \mathbf{u}_0(\cdot)
  \qquad
  \textrm{on}\ \Omega,
@f]
and Dirichlet (displacement) or Neumann (traction) boundary conditions need
to be specified for a unique solution:
@f{eqnarray*}
  \mathbf{u}(\mathbf{x},t) &=& \mathbf{d}(\mathbf{x},t)
  \qquad
  \textrm{on}\ \Gamma_D\subset\partial\Omega,
  \\
  \mathbf{n} \ C \varepsilon(\mathbf{u}(\mathbf{x},t)) &=& \mathbf{b}(\mathbf{x},t)
  \qquad
  \textrm{on}\ \Gamma_N=\partial\Omega\backslash\Gamma_D.
@f}
In above formulation, $\varepsilon(\mathbf{u})= \frac 12 (\nabla \mathbf{u} + \nabla
\mathbf{u}^T)$ is the symmetric gradient of the displacement, also called the
<em>strain</em>. $C$ is a tensor of rank 4, called the <em>stress-strain
  tensor</em> (the inverse of the <a
  href="https://en.wikipedia.org/wiki/Hooke%27s_law#Hooke's_law_for_continuous_media"><em>compliance
  tensor</em></a>)
that contains knowledge of the elastic strength of the material; its
symmetry properties make sure that it maps symmetric tensors of rank 2
(&ldquo;matrices&rdquo; of dimension $d$, where $d$ is the spatial dimensionality) onto
symmetric tensors of the same rank. We will comment on the roles of the strain
and stress tensors more below. For the moment it suffices to say that we
interpret the term $\textrm{div}\  ( C \varepsilon(\mathbf{u}))$ as the vector with
components $\frac \partial{\partial x_j} C_{ijkl} \varepsilon(\mathbf{u})_{kl}$,
where summation over indices $j,k,l$ is implied.

The quasistatic limit of this equation is motivated as follows: each small
perturbation of the body, for example by changes in boundary condition or the
forcing function, will result in a corresponding change in the configuration
of the body. In general, this will be in the form of waves radiating away from
the location of the disturbance. Due to the presence of the damping term,
these waves will be attenuated on a time scale of, say, $\tau$. Now, assume
that all changes in external forcing happen on times scales that are
much larger than $\tau$. In that case, the dynamic nature of the change is
unimportant: we can consider the body to always be in static equilibrium,
i.e. we can assume that at all times the body satisfies
@f{eqnarray*}
  - \textrm{div}\  ( C \varepsilon(\mathbf{u})) &=& \mathbf{f}(\mathbf{x},t)
  \qquad
  \textrm{in}\ \Omega,
  \\
  \mathbf{u}(\mathbf{x},t) &=& \mathbf{d}(\mathbf{x},t)
  \qquad
  \textrm{on}\ \Gamma_D,
  \\
  \mathbf{n} \ C \varepsilon(\mathbf{u}(\mathbf{x},t)) &=& \mathbf{b}(\mathbf{x},t)
  \qquad
  \textrm{on}\ \Gamma_N.
@f}
Note that the differential equation does not contain any time derivatives any
more -- all time dependence is introduced through boundary conditions and a
possibly time-varying force function $\mathbf{f}(\mathbf{x},t)$. The changes in
configuration can therefore be considered as being stationary
instantaneously. An alternative view of this is that $t$ is not really a time
variable, but only a time-like parameter that governs the evolution of the
problem.

While these equations are sufficient to describe small deformations, computing
large deformations is a little more complicated and, in general, leads
to nonlinear equations such as those treated in step-44. In the
following, let us consider some of the tools one would employ when
simulating problems in which the deformation becomes <i>large</i>.

@note The model we will consider below is not founded on anything that
would be mathematically sound: we will consider a model in which we
produce a small deformation, deform the physical coordinates of the
body by this deformation, and then consider the next loading step
again as a linear problem. This isn't consistent, since the assumption
of linearity implies that deformations are infinitesimal and so moving
around the vertices of our mesh by a finite amount before solving the
next linear problem is an inconsistent approach. We should therefore
note that it is not surprising that the equations discussed below
can't be found in the literature: <b>The model considered here has
little to do with reality!</b> On the other hand, the implementation
techniques we consider are very much what one would need to use when
implementing a <i>real</i> model, as we will see in step-44.


To come back to defining our "artificial" model, let us first
introduce a tensorial stress variable $\sigma$, and write the differential
equations in terms of the stress:
@f{eqnarray*}
  - \textrm{div}\  \sigma &=& \mathbf{f}(\mathbf{x},t)
  \qquad
  \textrm{in}\ \Omega(t),
  \\
  \mathbf{u}(\mathbf{x},t) &=& \mathbf{d}(\mathbf{x},t)
  \qquad
  \textrm{on}\ \Gamma_D\subset\partial\Omega(t),
  \\
  \mathbf{n} \ C \varepsilon(\mathbf{u}(\mathbf{x},t)) &=& \mathbf{b}(\mathbf{x},t)
  \qquad
  \textrm{on}\ \Gamma_N=\partial\Omega(t)\backslash\Gamma_D.
@f}
Note that these equations are posed on a domain $\Omega(t)$ that
changes with time, with the boundary moving according to the
displacements $\mathbf{u}(\mathbf{x},t)$ of the points on the boundary. To
complete this system, we have to specify the incremental relationship between
the stress and the strain, as follows:
<a name="step_18.stress-strain"></a>
@f[
  \dot\sigma = C \varepsilon (\dot{\mathbf{u}}),
  \qquad
  \qquad
  \textrm{[stress-strain]}
@f]
where a dot indicates a time derivative. Both the stress $\sigma$ and the
strain $\varepsilon(\mathbf{u})$ are symmetric tensors of rank 2.


<h4>Time discretization</h4>

Numerically, this system is solved as follows: first, we discretize
the time component using a backward Euler scheme. This leads to a
discrete equilibrium of force at time step $n$:
@f[
  -\textrm{div}\  \sigma^n = f^n,
@f]
where
@f[
  \sigma^n = \sigma^{n-1} + C \varepsilon (\Delta \mathbf{u}^n),
@f]
and $\Delta \mathbf{u}^n$ the incremental displacement for time step
$n$. In addition, we have to specify initial data $\mathbf{u}(\cdot,0)=\mathbf{u}_0$.
This way, if we want to solve for the displacement increment, we
have to solve the following system:
@f{align*}
  - \textrm{div}\   C \varepsilon(\Delta\mathbf{u}^n) &= \mathbf{f} + \textrm{div}\  \sigma^{n-1}
  \qquad
  &&\textrm{in}\ \Omega(t_{n-1}),
  \\
  \Delta \mathbf{u}^n(\mathbf{x},t) &= \mathbf{d}(\mathbf{x},t_n) - \mathbf{d}(\mathbf{x},t_{n-1})
  \qquad
  &&\textrm{on}\ \Gamma_D\subset\partial\Omega(t_{n-1}),
  \\
  \mathbf{n} \ C \varepsilon(\Delta \mathbf{u}^n(\mathbf{x},t)) &= \mathbf{b}(\mathbf{x},t_n)-\mathbf{b}(\mathbf{x},t_{n-1})
  \qquad
  &&\textrm{on}\ \Gamma_N=\partial\Omega(t_{n-1})\backslash\Gamma_D.
@f}
The weak form of this set of equations, which as usual is the basis for the
finite element formulation, reads as follows: find $\Delta \mathbf{u}^n \in
\{v\in H^1(\Omega(t_{n-1}))^d: v|_{\Gamma_D}=\mathbf{d}(\cdot,t_n) - \mathbf{d}(\cdot,t_{n-1})\}$
such that
<a name="step_18.linear-system"></a>
@f{align*}
  (C \varepsilon(\Delta\mathbf{u}^n), \varepsilon(\varphi) )_{\Omega(t_{n-1})}
  &=
  (\mathbf{f}, \varphi)_{\Omega(t_{n-1})}
  -(\sigma^{n-1},\varepsilon(\varphi))_{\Omega(t_{n-1})}
  \\
  &\qquad
  +(\mathbf{b}(\mathbf{x},t_n)-\mathbf{b}(\mathbf{x},t_{n-1}), \varphi)_{\Gamma_N}
  +(\sigma^{n-1} \mathbf{n}, \varphi)_{\Gamma_N}
  \\
  &\qquad\qquad
  \forall \varphi \in \{\mathbf{v}\in H^1(\Omega(t_{n-1}))^d: \mathbf{v}|_{\Gamma_D}=0\}.
@f}
Using that $\sigma^{n-1} \mathbf{n}
            = [C \varepsilon(\mathbf{u}^{n-1})] \mathbf{n}
            = \mathbf{b}(\mathbf x, t_{n-1})$,
these equations can be simplified to
@f{align*}
  (C \varepsilon(\Delta\mathbf{u}^n), \varepsilon(\varphi) )_{\Omega(t_{n-1})}
  &=
  (\mathbf{f}, \varphi)_{\Omega(t_{n-1})}
  -(\sigma^{n-1},\varepsilon(\varphi))_{\Omega(t_{n-1})}
  +(\mathbf{b}(\mathbf{x},t_n),t_{n-1}), \varphi)_{\Gamma_N}
  \\
  &\qquad\qquad
  \forall \varphi \in \{\mathbf{v}\in H^1(\Omega(t_{n-1}))^d: \mathbf{v}|_{\Gamma_D}=0\}.
  \qquad
  \qquad
  \textrm{[linear-system]}
@f}

We note that, for simplicity, in the program we will always assume that there
are no boundary forces, i.e. $\mathbf{b} = 0$, and that the deformation of the
body is driven by body forces $\mathbf{f}$ and prescribed boundary displacements
$\mathbf{d}$ alone. It is also worth noting that when integrating by parts, we
would get terms of the form $(C \varepsilon(\Delta\mathbf{u}^n), \nabla \varphi
)_{\Omega(t_{n-1})}$, but that we replace them with the term involving the
symmetric gradient $\varepsilon(\varphi)$ instead of $\nabla\varphi$. Due to
the symmetry of $C$, the two terms are mathematically equivalent, but
the symmetric version avoids the potential for round-off errors making
the resulting matrix slightly non-symmetric.

The system at time step $n$, to be solved on the old domain
$\Omega(t_{n-1})$, has exactly the form of a stationary elastic
problem, and is therefore similar to what we have already implemented
in previous example programs. We will therefore not comment on the
space discretization beyond saying that we again use lowest order
continuous finite elements.

There are differences, however:
<ol>
  <li> We have to move (update) the mesh after each time step, in order to be
  able to solve the next time step on a new domain;

  <li> We need to know $\sigma^{n-1}$ to compute the next incremental
  displacement, i.e. we need to compute it at the end of the time step
  to make sure it is available for the next time step. Essentially,
  the stress variable is our window to the history of deformation of
  the body.
</ol>
These two operations are done in the functions <code>move_mesh</code> and
<code>update_quadrature_point_history</code> in the program. While moving
the mesh is only a technicality, updating the stress is a little more
complicated and will be discussed in the next section.


<h4>Updating the stress variable</h4>

As indicated above, we need to have the stress variable $\sigma^n$ available
when computing time step $n+1$, and we can compute it using
<a name="step_18.stress-update"></a>
@f[
  \sigma^n = \sigma^{n-1} + C \varepsilon (\Delta \mathbf{u}^n).
  \qquad
  \qquad
  \textrm{[stress-update]}
@f]
There are, despite the apparent simplicity of this equation, two questions
that we need to discuss. The first concerns the way we store $\sigma^n$: even
if we compute the incremental updates $\Delta\mathbf{u}^n$ using lowest-order
finite elements, then its symmetric gradient $\varepsilon(\Delta\mathbf{u}^n)$ is
in general still a function that is not easy to describe. In particular, it is
not a piecewise constant function, and on general meshes (with cells that are
not rectangles %parallel to the coordinate axes) or with non-constant
stress-strain tensors $C$ it is not even a bi- or trilinear function. Thus, it
is a priori not clear how to store $\sigma^n$ in a computer program.

To decide this, we have to see where it is used. The only place where we
require the stress is in the term
$(\sigma^{n-1},\varepsilon(\varphi))_{\Omega(t_{n-1})}$. In practice, we of
course replace this term by numerical quadrature:
@f[
  (\sigma^{n-1},\varepsilon(\varphi))_{\Omega(t_{n-1})}
  =
  \sum_{K\subset {T}}
  (\sigma^{n-1},\varepsilon(\varphi))_K
  \approx
  \sum_{K\subset {T}}
  \sum_q
  w_q \ \sigma^{n-1}(\mathbf{x}_q) : \varepsilon(\varphi(\mathbf{x}_q),
@f]
where $w_q$ are the quadrature weights and $\mathbf{x}_q$ the quadrature points on
cell $K$. This should make clear that what we really need is not the stress
$\sigma^{n-1}$ in itself, but only the values of the stress in the quadrature
points on all cells. This, however, is a simpler task: we only have to provide
a data structure that is able to hold one symmetric tensor of rank 2 for each
quadrature point on all cells (or, since we compute in parallel, all
quadrature points of all cells that the present MPI process &ldquo;owns&rdquo;). At the
end of each time step we then only have to evaluate $\varepsilon(\Delta \mathbf{u}^n(\mathbf{x}_q))$, multiply it by the stress-strain tensor $C$, and use the
result to update the stress $\sigma^n(\mathbf{x}_q)$ at quadrature point $q$.

The second complication is not visible in our notation as chosen above. It is
due to the fact that we compute $\Delta u^n$ on the domain $\Omega(t_{n-1})$,
and then use this displacement increment to both update the stress as well as
move the mesh nodes around to get to $\Omega(t_n)$ on which the next increment
is computed. What we have to make sure, in this context, is that moving the
mesh does not only involve moving around the nodes, but also making
corresponding changes to the stress variable: the updated stress is a variable
that is defined with respect to the coordinate system of the material in the
old domain, and has to be transferred to the new domain. The reason for this
can be understood as follows: locally, the incremental deformation $\Delta\mathbf{u}$ can be decomposed into three parts, a linear translation (the constant part
of the displacement increment field in the neighborhood of a point), a
dilational
component (that part of the gradient of the displacement field that has a
nonzero divergence), and a rotation. A linear translation of the material does
not affect the stresses that are frozen into it -- the stress values are
simply translated along. The dilational or compressional change produces a
corresponding stress update. However, the rotational component does not
necessarily induce a nonzero stress update (think, in 2d, for example of the
situation where $\Delta\mathbf{u}=(y, -x)^T$, with which $\varepsilon(\Delta
\mathbf{u})=0$). Nevertheless, if the material was prestressed in a certain
direction, then this direction will be rotated along with the material.  To
this end, we have to define a rotation matrix $R(\Delta \mathbf{u}^n)$ that
describes, in each point the rotation due to the displacement increments. It
is not hard to see that the actual dependence of $R$ on $\Delta \mathbf{u}^n$ can
only be through the curl of the displacement, rather than the displacement
itself or its full gradient (as mentioned above, the constant components of
the increment describe translations, its divergence the dilational modes, and
the curl the rotational modes). Since the exact form of $R$ is cumbersome, we
only state it in the program code, and note that the correct updating formula
for the stress variable is then
<a name="step_18.stress-update+rot"></a>
@f[
  \sigma^n
  =
  R(\Delta \mathbf{u}^n)^T
  [\sigma^{n-1} + C \varepsilon (\Delta \mathbf{u}^n)]
  R(\Delta \mathbf{u}^n).
  \qquad
  \qquad
  \textrm{[stress-update+rot]}
@f]

Both stress update and rotation are implemented in the function
<code>update_quadrature_point_history</code> of the example program.


<h3>Parallel graphical output</h3>

In step-17, the main bottleneck for %parallel computations as far as run time
is concerned
was that only the first processor generated output for the entire domain.
Since generating graphical output is expensive, this did not scale well when
larger numbers of processors were involved. We will address this here. (For a
definition of what it means for a program to "scale", see
@ref GlossParallelScaling "this glossary entry".)

Basically, what we need to do is let every process
generate graphical output for that subset of cells that it owns, write them
into separate files and have a way to display all files for a certain timestep
at the same time. This way the code produces one <code>.vtu</code> file per process per
time step. The two common VTK file viewers ParaView and VisIt both support
opening more than one <code>.vtu</code> file at once. To simplify the process of picking
the correct files and allow moving around in time, both support record files
that reference all files for a given timestep. Sadly, the record files have a
different format between VisIt and Paraview, so we write out both formats.

The code will generate the files <code>solution-TTTT.NNN.vtu</code>,
where <code>TTTT</code> is the timestep number (starting from 1) and
<code>NNN</code> is the process rank (starting from
0). These files contain the locally owned cells for the timestep and
processor. The files <code>solution-TTTT.visit</code> is the visit record
for timestep <code>TTTT</code>, while <code>solution-TTTT.pvtu</code> is
the same for ParaView. (More recent versions of VisIt can actually read
<code>.pvtu</code> files as well, but it doesn't hurt to output both
kinds of record files.) Finally, the file
<code>solution.pvd</code> is a special record only supported by ParaView that references
all time steps. So in ParaView, only solution.pvd needs to be opened, while
one needs to select the group of all .visit files in VisIt for the same
effect.


<h3>A triangulation with automatic partitioning</h3>

In step-17, we used a regular triangulation that was simply replicated on
every processor, and a corresponding DoFHandler. Both had no idea that they
were used in a %parallel context -- they just existed in their entirety
on every processor, and we argued that this was eventually going to be a
major memory bottleneck.

We do not address this issue here (we will do so in step-40) but make
the situation slightly more automated. In step-17, we created the triangulation
and then manually "partitioned" it, i.e., we assigned
@ref GlossSubdomainId "subdomain ids" to every cell that indicated which
@ref GlossMPIProcess "MPI process" "owned" the cell. Here, we use a class
parallel::shared::Triangulation that at least does this part automatically:
whenever you create or refine such a triangulation, it automatically
partitions itself among all involved processes (which it knows about because
you have to tell it about the @ref GlossMPICommunicator "MPI communicator"
that connects these processes upon construction of the triangulation).
Otherwise, the parallel::shared::Triangulation looks, for all practical
purposes, like a regular Triangulation object.

The convenience of using this class does not only result from being able
to avoid the manual call to GridTools::partition(). Rather, the DoFHandler
class now also knows that you want to use it in a parallel context, and
by default automatically enumerates degrees of freedom in such a way
that all DoFs owned by process zero come before all DoFs owned by process 1,
etc. In other words, you can also avoid the call to
DoFRenumbering::subdomain_wise().

There are other benefits. For example, because the triangulation knows that
it lives in a %parallel universe, it also knows that it "owns" certain
cells (namely, those whose subdomain id equals its MPI rank; previously,
the triangulation only stored these subdomain ids, but had no way to
make sense of them). Consequently, in the assembly function, you can
test whether a cell is "locally owned" (i.e., owned by the current
process, see @ref GlossLocallyOwnedCell) when you loop over all cells
using the syntax
@code
  if (cell->is_locally_owned())
@endcode
This knowledge extends to the DoFHandler object built on such triangulations,
which can then identify which degrees of freedom are locally owned
(see @ref GlossLocallyOwnedDof) via calls such as
DoFHandler::compute_n_locally_owned_dofs_per_processor() and
DoFTools::extract_locally_relevant_dofs(). Finally, the DataOut class
also knows how to deal with such triangulations and will simply skip
generating graphical output on cells not locally owned.

Of course, as has been noted numerous times in the discussion in step-17,
keeping the entire triangulation on every process will not scale: large
problems may simply not fit into each process's memory any more, even if
we have sufficiently many processes around to solve them in a reasonable
time. In such cases, the parallel::shared::Triangulation is no longer
a reasonable basis for computations and we will show in step-40 how the
parallel::distributed::Triangulation class can be used to work around
this, namely by letting each process store only a <i>part</i> of the
triangulation.


<h3>Overall structure of the program</h3>

The overall structure of the program can be inferred from the <code>run()</code>
function that first calls <code>do_initial_timestep()</code> for the first time
step, and then <code>do_timestep()</code> on all subsequent time steps. The
difference between these functions is only that in the first time step we
start on a coarse mesh, solve on it, refine the mesh adaptively, and then
start again with a clean state on that new mesh. This procedure gives us a
better starting mesh, although we should of course keep adapting the mesh as
iterations proceed -- this isn't done in this program, but commented on below.

The common part of the two functions treating time steps is the following
sequence of operations on the present mesh:
<ul>
<li> <code>assemble_system ()</code> [via <code>solve_timestep ()</code>]:
  This first function is also the most interesting one. It assembles the
  linear system corresponding to the discretized version of equation
  <a href="#step_18.linear-system">[linear-system]</a>. This leads to a system matrix $A_{ij} = \sum_K
  A^K_{ij}$ built up of local contributions on each cell $K$ with entries
  @f[
    A^K_{ij} = (C \varepsilon(\varphi_j), \varepsilon(\varphi_i))_K;
  @f]
  In practice, $A^K$ is computed using numerical quadrature according to the
  formula
  @f[
    A^K_{ij} = \sum_q w_q [\varepsilon(\varphi_i(\mathbf{x}_q)) : C :
                           \varepsilon(\varphi_j(\mathbf{x}_q))],
  @f]
  with quadrature points $\mathbf{x}_q$ and weights $w_q$. We have built these
  contributions before, in step-8 and step-17, but in both of these cases we
  have done so rather clumsily by using knowledge of how the rank-4 tensor $C$
  is composed, and considering individual elements of the strain tensors
  $\varepsilon(\varphi_i),\varepsilon(\varphi_j)$. This is not really
  convenient, in particular if we want to consider more complicated elasticity
  models than the isotropic case for which $C$ had the convenient form
  $C_{ijkl}  = \lambda \delta_{ij} \delta_{kl} + \mu (\delta_{ik} \delta_{jl}
  + \delta_{il} \delta_{jk})$. While we in fact do not use a more complicated
  form than this in the present program, we nevertheless want to write it in a
  way that would easily allow for this. It is then natural to introduce
  classes that represent symmetric tensors of rank 2 (for the strains and
  stresses) and 4 (for the stress-strain tensor $C$). Fortunately, deal.II
  provides these: the <code>SymmetricTensor<rank,dim></code> class template
  provides a full-fledged implementation of such tensors of rank <code>rank</code>
  (which needs to be an even number) and dimension <code>dim</code>.

  What we then need is two things: a way to create the stress-strain rank-4
  tensor $C$ as well as to create a symmetric tensor of rank 2 (the strain
  tensor) from the gradients of a shape function $\varphi_i$ at a quadrature
  point $\mathbf{x}_q$ on a given cell. At the top of the implementation of this
  example program, you will find such functions. The first one,
  <code>get_stress_strain_tensor</code>, takes two arguments corresponding to
  the Lam&eacute; constants $\lambda$ and $\mu$ and returns the stress-strain tensor
  for the isotropic case corresponding to these constants (in the program, we
  will choose constants corresponding to steel); it would be simple to replace
  this function by one that computes this tensor for the anisotropic case, or
  taking into account crystal symmetries, for example. The second one,
  <code>get_strain</code> takes an object of type <code>FEValues</code> and indices
  $i$ and $q$ and returns the symmetric gradient, i.e. the strain,
  corresponding to shape function $\varphi_i(\mathbf{x}_q)$, evaluated on the cell
  on which the <code>FEValues</code> object was last reinitialized.

  Given this, the innermost loop of <code>assemble_system</code> computes the
  local contributions to the matrix in the following elegant way (the variable
  <code>stress_strain_tensor</code>, corresponding to the tensor $C$, has
  previously been initialized with the result of the first function above):
  @code
for (unsigned int i=0; i<dofs_per_cell; ++i)
  for (unsigned int j=0; j<dofs_per_cell; ++j)
    for (unsigned int q_point=0; q_point<n_q_points;
         ++q_point)
      {
        const SymmetricTensor<2,dim>
          eps_phi_i = get_strain (fe_values, i, q_point),
          eps_phi_j = get_strain (fe_values, j, q_point);

        cell_matrix(i,j)
          += (eps_phi_i * stress_strain_tensor * eps_phi_j *
              fe_values.JxW (q_point));
      }
  @endcode
  It is worth noting the expressive power of this piece of code, and to
  compare it with the complications we had to go through in previous examples
  for the elasticity problem. (To be fair, the SymmetricTensor class
  template did not exist when these previous examples were written.) For
  simplicity, <code>operator*</code> provides for the (double summation) product
  between symmetric tensors of even rank here.

  Assembling the local contributions
  @f{eqnarray*}
      f^K_i &=&
      (\mathbf{f}, \varphi_i)_K -(\sigma^{n-1},\varepsilon(\varphi_i))_K
      \\
      &\approx&
      \sum_q
      w_q \left\{
        \mathbf{f}(\mathbf{x}_q) \cdot \varphi_i(\mathbf{x}_q) -
        \sigma^{n-1}_q : \varepsilon(\varphi_i(\mathbf{x}_q))
      \right\}
  @f}
  to the right hand side of <a href="#step_18.linear-system">[linear-system]</a> is equally
  straightforward (note that we do not consider any boundary tractions $\mathbf{b}$ here). Remember that we only had to store the old stress in the
  quadrature points of cells. In the program, we will provide a variable
  <code>local_quadrature_points_data</code> that allows to access the stress
  $\sigma^{n-1}_q$ in each quadrature point. With this the code for the right
  hand side looks as this, again rather elegant:
  @code
for (unsigned int i=0; i<dofs_per_cell; ++i)
  {
    const unsigned int
      component_i = fe.system_to_component_index(i).first;

    for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
      {
        const SymmetricTensor<2,dim> &old_stress
          = local_quadrature_points_data[q_point].old_stress;

        cell_rhs(i) += (body_force_values[q_point](component_i) *
                        fe_values.shape_value (i,q_point)
                        -
                        old_stress *
                        get_strain (fe_values,i,q_point)) *
                       fe_values.JxW (q_point);
      }
  }
  @endcode
  Note that in the multiplication $\mathbf{f}(\mathbf{x}_q) \cdot \varphi_i(\mathbf{x}_q)$, we have made use of the fact that for the chosen finite element, only
  one vector component (namely <code>component_i</code>) of $\varphi_i$ is
  nonzero, and that we therefore also have to consider only one component of
  $\mathbf{f}(\mathbf{x}_q)$.

  This essentially concludes the new material we present in this function. It
  later has to deal with boundary conditions as well as hanging node
  constraints, but this parallels what we had to do previously in other
  programs already.

<li> <code>solve_linear_problem ()</code> [via <code>solve_timestep ()</code>]:
  Unlike the previous one, this function is not really interesting, since it
  does what similar functions have done in all previous tutorial programs --
  solving the linear system using the CG method, using an incomplete LU
  decomposition as a preconditioner (in the %parallel case, it uses an ILU of
  each processor's block separately). It is virtually unchanged
  from step-17.

<li> <code>update_quadrature_point_history ()</code> [via
  <code>solve_timestep ()</code>]: Based on the displacement field $\Delta \mathbf{u}^n$ computed before, we update the stress values in all quadrature points
  according to <a href="#step_18.stress-update">[stress-update]</a> and <a href="#step_18.stress-update+rot">[stress-update+rot]</a>,
  including the rotation of the coordinate system.

<li> <code>move_mesh ()</code>: Given the solution computed before, in this
  function we deform the mesh by moving each vertex by the displacement vector
  field evaluated at this particular vertex.

<li> <code>output_results ()</code>: This function simply outputs the solution
  based on what we have said above, i.e. every processor computes output only
  for its own portion of the domain. In addition to the solution, we also compute the norm of
  the stress averaged over all the quadrature points on each cell.
</ul>

With this general structure of the code, we only have to define what case we
want to solve. For the present program, we have chosen to simulate the
quasistatic deformation of a vertical cylinder for which the bottom boundary
is fixed and the top boundary is pushed down at a prescribed vertical
velocity. However, the horizontal velocity of the top boundary is left
unspecified -- one can imagine this situation as a well-greased plate pushing
from the top onto the cylinder, the points on the top boundary of the cylinder
being allowed to slide horizontally along the surface of the plate, but forced
to move downward by the plate. The inner and outer boundaries of the cylinder
are free and not subject to any prescribed deflection or traction. In
addition, gravity acts on the body.

The program text will reveal more about how to implement this situation, and
the results section will show what displacement pattern comes out of this
simulation.


examples/step-18/doc/results.dox
<h1>Results</h1>


Running the program takes a good while if one uses debug mode; it takes about
eleven minutes on my i7 desktop. Fortunately, the version compiled with
optimizations is much faster; the program only takes about a minute and a half
after recompiling with the command <tt>make release</tt> on the same machine, a
much more reasonable time.


If run, the program prints the following output, explaining what it is
doing during all that time:
@verbatim
\$ time make run
[ 66%] Built target \step-18
[100%] Run \step-18 with Release configuration
Timestep 1 at time 1
  Cycle 0:
    Number of active cells:       3712 (by partition: 3712)
    Number of degrees of freedom: 17226 (by partition: 17226)
    Assembling system... norm of rhs is 1.88062e+10
    Solver converged in 103 iterations.
    Updating quadrature point data...
  Cycle 1:
    Number of active cells:       12812 (by partition: 12812)
    Number of degrees of freedom: 51738 (by partition: 51738)
    Assembling system... norm of rhs is 1.86145e+10
    Solver converged in 121 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 2 at time 2
    Assembling system... norm of rhs is 1.84169e+10
    Solver converged in 122 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 3 at time 3
    Assembling system... norm of rhs is 1.82355e+10
    Solver converged in 122 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 4 at time 4
    Assembling system... norm of rhs is 1.80728e+10
    Solver converged in 117 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 5 at time 5
    Assembling system... norm of rhs is 1.79318e+10
    Solver converged in 116 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 6 at time 6
    Assembling system... norm of rhs is 1.78171e+10
    Solver converged in 115 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 7 at time 7
    Assembling system... norm of rhs is 1.7737e+10
    Solver converged in 112 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 8 at time 8
    Assembling system... norm of rhs is 1.77127e+10
    Solver converged in 111 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 9 at time 9
    Assembling system... norm of rhs is 1.78207e+10
    Solver converged in 113 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 10 at time 10
    Assembling system... norm of rhs is 1.83544e+10
    Solver converged in 115 iterations.
    Updating quadrature point data...
    Moving mesh...

[100%] Built target run
make run  176.82s user 0.15s system 198% cpu 1:28.94 total
@endverbatim
In other words, it is computing on 12,000 cells and with some 52,000
unknowns. Not a whole lot, but enough for a coupled three-dimensional
problem to keep a computer busy for a while. At the end of the day,
this is what we have for output:
@verbatim
\$ ls -l *vtu *visit
-rw-r--r-- 1 drwells users 1706059 Feb 13 19:36 solution-0010.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:36 solution-0010.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:36 solution-0010.visit
-rw-r--r-- 1 drwells users 1707907 Feb 13 19:36 solution-0009.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:36 solution-0009.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:36 solution-0009.visit
-rw-r--r-- 1 drwells users 1703771 Feb 13 19:35 solution-0008.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0008.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0008.visit
-rw-r--r-- 1 drwells users 1693671 Feb 13 19:35 solution-0007.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0007.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0007.visit
-rw-r--r-- 1 drwells users 1681847 Feb 13 19:35 solution-0006.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0006.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0006.visit
-rw-r--r-- 1 drwells users 1670115 Feb 13 19:35 solution-0005.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0005.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0005.visit
-rw-r--r-- 1 drwells users 1658559 Feb 13 19:35 solution-0004.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0004.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0004.visit
-rw-r--r-- 1 drwells users 1639983 Feb 13 19:35 solution-0003.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0003.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0003.visit
-rw-r--r-- 1 drwells users 1625851 Feb 13 19:35 solution-0002.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:35 solution-0002.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:35 solution-0002.visit
-rw-r--r-- 1 drwells users 1616035 Feb 13 19:34 solution-0001.000.vtu
-rw-r--r-- 1 drwells users     761 Feb 13 19:34 solution-0001.pvtu
-rw-r--r-- 1 drwells users      33 Feb 13 19:34 solution-0001.visit
@endverbatim


If we visualize these files with VisIt or Paraview, we get to see the full picture
of the disaster our forced compression wreaks on the cylinder (colors in the
images encode the norm of the stress in the material):


<div class="threecolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0002.0000.png"
           alt="Time = 2"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 2
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0005.0000.png"
           alt="Time = 5"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 5
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0007.0000.png"
           alt="Time = 7"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 7
    </div>
  </div>
</div>


<div class="threecolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0008.0000.png"
           alt="Time = 8"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 8
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0009.0000.png"
           alt="Time = 9"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 9
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0010.0000.png"
           alt="Time = 10"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 10
    </div>
  </div>
</div>


As is clearly visible, as we keep compressing the cylinder, it starts
to bow out near the fully constrained bottom surface and, after about eight
time units, buckle in an azimuthally symmetric manner.


Although the result appears plausible for the symmetric geometry and loading,
it is yet to be established whether or not the computation is fully converged.
In order to see whether it is, we ran the program again with one more global
refinement at the beginning and with the time step halved. This would have
taken a very long time on a single machine, so we used a proper workstation and
ran it on 16 processors in parallel. The beginning of the output now looks like
this:
@verbatim
Timestep 1 at time 0.5
  Cycle 0:
    Number of active cells:       29696 (by partition: 1808+1802+1894+1881+1870+1840+1884+1810+1876+1818+1870+1884+1854+1903+1816+1886)
    Number of degrees of freedom: 113100 (by partition: 6936+6930+7305+7116+7326+6869+7331+6786+7193+6829+7093+7162+6920+7280+6843+7181)
    Assembling system... norm of rhs is 1.10765e+10
    Solver converged in 209 iterations.
    Updating quadrature point data...
  Cycle 1:
    Number of active cells:       102034 (by partition: 6387+6202+6421+6341+6408+6201+6428+6428+6385+6294+6506+6244+6417+6527+6299+6546)
    Number of degrees of freedom: 359337 (by partition: 23255+21308+24774+24019+22304+21415+22430+22184+22298+21796+22396+21592+22325+22553+21977+22711)
    Assembling system... norm of rhs is 1.35759e+10
    Solver converged in 268 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 2 at time 1
    Assembling system... norm of rhs is 1.34674e+10
    Solver converged in 267 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 3 at time 1.5
    Assembling system... norm of rhs is 1.33607e+10
    Solver converged in 265 iterations.
    Updating quadrature point data...
    Moving mesh...

Timestep 4 at time 2
    Assembling system... norm of rhs is 1.32558e+10
    Solver converged in 263 iterations.
    Updating quadrature point data...
    Moving mesh...

[...]

Timestep 20 at time 10
    Assembling system... norm of rhs is 1.47755e+10
    Solver converged in 425 iterations.
    Updating quadrature point data...
    Moving mesh...
@endverbatim
That's quite a good number of unknowns, given that we are in 3d. The output of
this program are 16 files for each time step:
@verbatim
\$ ls -l solution-0001*
-rw-r--r-- 1 wellsd2 user 761065 Feb 13 21:09 solution-0001.000.vtu
-rw-r--r-- 1 wellsd2 user 759277 Feb 13 21:09 solution-0001.001.vtu
-rw-r--r-- 1 wellsd2 user 761217 Feb 13 21:09 solution-0001.002.vtu
-rw-r--r-- 1 wellsd2 user 761605 Feb 13 21:09 solution-0001.003.vtu
-rw-r--r-- 1 wellsd2 user 756917 Feb 13 21:09 solution-0001.004.vtu
-rw-r--r-- 1 wellsd2 user 752669 Feb 13 21:09 solution-0001.005.vtu
-rw-r--r-- 1 wellsd2 user 735217 Feb 13 21:09 solution-0001.006.vtu
-rw-r--r-- 1 wellsd2 user 750065 Feb 13 21:09 solution-0001.007.vtu
-rw-r--r-- 1 wellsd2 user 760273 Feb 13 21:09 solution-0001.008.vtu
-rw-r--r-- 1 wellsd2 user 777265 Feb 13 21:09 solution-0001.009.vtu
-rw-r--r-- 1 wellsd2 user 772469 Feb 13 21:09 solution-0001.010.vtu
-rw-r--r-- 1 wellsd2 user 760833 Feb 13 21:09 solution-0001.011.vtu
-rw-r--r-- 1 wellsd2 user 782241 Feb 13 21:09 solution-0001.012.vtu
-rw-r--r-- 1 wellsd2 user 748905 Feb 13 21:09 solution-0001.013.vtu
-rw-r--r-- 1 wellsd2 user 738413 Feb 13 21:09 solution-0001.014.vtu
-rw-r--r-- 1 wellsd2 user 762133 Feb 13 21:09 solution-0001.015.vtu
-rw-r--r-- 1 wellsd2 user   1421 Feb 13 21:09 solution-0001.pvtu
-rw-r--r-- 1 wellsd2 user    364 Feb 13 21:09 solution-0001.visit
@endverbatim


Here are first the mesh on which we compute as well as the partitioning
for the 16 processors:


<div class="twocolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-000mesh.png"
           alt="Discretization"
           width="400">
    </div>
    <div class="text" align="center">
      Discretization
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0002.p.png"
           alt="Parallel partitioning"
           width="400">
    </div>
    <div class="text" align="center">
      Parallel partitioning
    </div>
  </div>
</div>


Finally, here is the same output as we have shown before for the much smaller
sequential case:

<div class="threecolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0002.s.png"
           alt="Time = 2"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 2
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0005.s.png"
           alt="Time = 5"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 5
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0007.s.png"
           alt="Time = 7"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 7
    </div>
  </div>
</div>


<div class="threecolumn" style="width: 80%">
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0008.s.png"
           alt="Time = 8"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 8
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0009.s.png"
           alt="Time = 9"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 9
    </div>
  </div>
  <div class="parent">
    <div class="img" align="center">
      <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0010.s.png"
           alt="Time = 10"
           width="400">
    </div>
    <div class="text" align="center">
      Time = 10
    </div>
  </div>
</div>


As before, we observe that at high axial compression the cylinder begins
to buckle, but this time ultimately collapses on itself. In contrast to our
first run, towards the end of the simulation the deflection pattern becomes
nonsymmetric (the central bulge deflects laterally). The model clearly does not
provide for this (all our forces and boundary deflections are symmetric) but the
effect is probably physically correct anyway: in reality, small inhomogeneities
in the body's material properties would lead it to buckle to one side
to evade the forcing; in numerical simulations, small perturbations
such as numerical round-off or an inexact solution of a linear system
by an iterative solver could have the same effect. Another typical source for
asymmetries in adaptive computations is that only a certain fraction of cells
is refined in each step, which may lead to asymmetric meshes even if the
original coarse mesh was symmetric.


If one compares this with the previous run, the results both qualitatively
and quantitatively different. The previous computation was
therefore certainly not converged, though we can't say for sure anything about
the present one. One would need an even finer computation to find out. However,
the point may be moot: looking at the last picture in detail, it is pretty
obvious that not only is the linear small deformation model we chose completely
inadequate, but for a realistic simulation we would also need to make sure that
the body does not intersect itself during deformation (if we continued
compressing the cylinder we would observe some self-intersection).
Without such a formulation we cannot expect anything to make physical sense,
even if it produces nice pictures!


<h3>Possibilities for extensions</h3>

The program as is does not really solve an equation that has many applications
in practice: quasi-static material deformation based on a purely elastic law
is almost boring. However, the program may serve as the starting point for
more interesting experiments, and that indeed was the initial motivation for
writing it. Here are some suggestions of what the program is missing and in
what direction it may be extended:

<h5>Plasticity models</h5>

 The most obvious extension is to use a more
realistic material model for large-scale quasistatic deformation. The natural
choice for this would be plasticity, in which a nonlinear relationship between
stress and strain replaces equation <a href="#step_18.stress-strain">[stress-strain]</a>. Plasticity
models are usually rather complicated to program since the stress-strain
dependence is generally non-smooth. The material can be thought of being able
to withstand only a maximal stress (the yield stress) after which it starts to
&ldquo;flow&rdquo;. A mathematical description to this can be given in the form of a
variational inequality, which alternatively can be treated as minimizing the
elastic energy
@f[
  E(\mathbf{u}) =
  (\varepsilon(\mathbf{u}), C\varepsilon(\mathbf{u}))_{\Omega}
  - (\mathbf{f}, \mathbf{u})_{\Omega} - (\mathbf{b}, \mathbf{u})_{\Gamma_N},
@f]
subject to the constraint
@f[
  f(\sigma(\mathbf{u})) \le 0
@f]
on the stress. This extension makes the problem to be solved in each time step
nonlinear, so we need another loop within each time step.

Without going into further details of this model, we refer to the excellent
book by Simo and Hughes on &ldquo;Computational Inelasticity&rdquo; for a
comprehensive overview of computational strategies for solving plastic
models. Alternatively, a brief but concise description of an algorithm for
plasticity is given in an article by S. Commend, A. Truty, and Th. Zimmermann;
@cite CTZ04.


<h5>Stabilization issues</h5>

The formulation we have chosen, i.e. using
piecewise (bi-, tri-)linear elements for all components of the displacement
vector, and treating the stress as a variable dependent on the displacement is
appropriate for most materials. However, this so-called displacement-based
formulation becomes unstable and exhibits spurious modes for incompressible or
nearly-incompressible materials. While fluids are usually not elastic (in most
cases, the stress depends on velocity gradients, not displacement gradients,
although there are exceptions such as electro-rheologic fluids), there are a
few solids that are nearly incompressible, for example rubber. Another case is
that many plasticity models ultimately let the material become incompressible,
although this is outside the scope of the present program.

Incompressibility is characterized by Poisson's ratio
@f[
  \nu = \frac{\lambda}{2(\lambda+\mu)},
@f]
where $\lambda,\mu$ are the Lam&eacute; constants of the material.
Physical constraints indicate that $-1\le \nu\le \frac 12$ (the condition
also follows from mathematical stability considerations). If $\nu$
approaches $\frac 12$, then the material becomes incompressible. In that
case, pure displacement-based formulations are no longer appropriate for the
solution of such problems, and stabilization techniques have to be employed
for a stable and accurate solution. The book and paper cited above give
indications as to how to do this, but there is also a large volume of
literature on this subject; a good start to get an overview of the topic can
be found in the references of the paper by H.-Y. Duan and Q. Lin; @cite DL05.


<h5>Refinement during timesteps</h5>

In the present form, the program
only refines the initial mesh a number of times, but then never again. For any
kind of realistic simulation, one would want to extend this so that the mesh
is refined and coarsened every few time steps instead. This is not hard to do,
in fact, but has been left for future tutorial programs or as an exercise, if
you wish.

The main complication one has to overcome is that one has to
transfer the data that is stored in the quadrature points of the cells of the
old mesh to the new mesh, preferably by some sort of projection scheme. The
general approach to this would go like this:

- At the beginning, the data is only available in the quadrature points of
  individual cells, not as a finite element field that is defined everywhere.

- So let us find a finite element field that <i>is</i> defined everywhere so
  that we can later interpolate it to the quadrature points of the new
  mesh. In general, it will be difficult to find a continuous finite element
  field that matches the values in the quadrature points exactly because the
  number of degrees of freedom of these fields does not match the number of
  quadrature points there are, and the nodal values of this global field will
  either be over- or underdetermined. But it is usually not very difficult to
  find a discontinuous field that matches the values in the quadrature points;
  for example, if you have a QGauss(2) quadrature formula (i.e. 4 points per
  cell in 2d, 8 points in 3d), then one would use a finite element of kind
  FE_DGQ(1), i.e. bi-/tri-linear functions as these have 4 degrees of freedom
  per cell in 2d and 8 in 3d.

- There are functions that can make this conversion from individual points to
  a global field simpler. The following piece of pseudo-code should help if
  you use a QGauss(2) quadrature formula. Note that the multiplication by the
  projection matrix below takes a vector of scalar components, i.e., we can only
  convert one set of scalars at a time from the quadrature points to the degrees
  of freedom and vice versa. So we need to store each component of stress separately,
  which requires <code>dim*dim</code> vectors. We'll store this set of vectors in a 2D array to
  make it easier to read off components in the same way you would the stress tensor.
  Thus, we'll loop over the components of stress on each cell and store
  these values in the global history field. (The prefix <code>history_</code>
  indicates that we work with quantities related to the history variables defined
  in the quadrature points.)
  @code
    FE_DGQ<dim>     history_fe (1);
    DoFHandler<dim> history_dof_handler (triangulation);
    history_dof_handler.distribute_dofs (history_fe);

    std::vector< std::vector< Vector<double> > >
                 history_field (dim, std::vector< Vector<double> >(dim)),
                 local_history_values_at_qpoints (dim, std::vector< Vector<double> >(dim)),
                 local_history_fe_values (dim, std::vector< Vector<double> >(dim));

    for (unsigned int i=0; i<dim; i++)
      for (unsigned int j=0; j<dim; j++)
      {
        history_field[i][j].reinit(history_dof_handler.n_dofs());
        local_history_values_at_qpoints[i][j].reinit(quadrature.size());
        local_history_fe_values[i][j].reinit(history_fe.n_dofs_per_cell());
      }

    FullMatrix<double> qpoint_to_dof_matrix (history_fe.dofs_per_cell,
                                             quadrature.size());
    FETools::compute_projection_from_quadrature_points_matrix
              (history_fe,
               quadrature, quadrature,
               qpoint_to_dof_matrix);

    typename DoFHandler<dim>::active_cell_iterator cell = dof_handler.begin_active(),
                                                   endc = dof_handler.end(),
                                                   dg_cell = history_dof_handler.begin_active();

    for (; cell!=endc; ++cell, ++dg_cell)
      {

        PointHistory<dim> *local_quadrature_points_history
          = reinterpret_cast<PointHistory<dim> *>(cell->user_pointer());

        Assert (local_quadrature_points_history >= &quadrature_point_history.front(),
                ExcInternalError());
        Assert (local_quadrature_points_history < &quadrature_point_history.back(),
                ExcInternalError());

        for (unsigned int i=0; i<dim; i++)
          for (unsigned int j=0; j<dim; j++)
          {
            for (unsigned int q=0; q<quadrature.size(); ++q)
              local_history_values_at_qpoints[i][j](q)
                = local_quadrature_points_history[q].old_stress[i][j];

            qpoint_to_dof_matrix.vmult (local_history_fe_values[i][j],
                                        local_history_values_at_qpoints[i][j]);

            dg_cell->set_dof_values (local_history_fe_values[i][j],
                                     history_field[i][j]);
          }
      }
  @endcode

- Now that we have a global field, we can refine the mesh and transfer the
  history_field vector as usual using the SolutionTransfer class. This will
  interpolate everything from the old to the new mesh.

- In a final step, we have to get the data back from the now interpolated
  global field to the quadrature points on the new mesh. The following code
  will do that:
  @code
    FullMatrix<double> dof_to_qpoint_matrix (quadrature.size(),
                                             history_fe.n_dofs_per_cell());
    FETools::compute_interpolation_to_quadrature_points_matrix
              (history_fe,
               quadrature,
               dof_to_qpoint_matrix);

    typename DoFHandler<dim>::active_cell_iterator cell = dof_handler.begin_active(),
                                                   endc = dof_handler.end(),
                                                   dg_cell = history_dof_handler.begin_active();

    for (; cell != endc; ++cell, ++dg_cell)
    {
      PointHistory<dim> *local_quadrature_points_history
       = reinterpret_cast<PointHistory<dim> *>(cell->user_pointer());

      Assert (local_quadrature_points_history >= &quadrature_point_history.front(),
              ExcInternalError());
      Assert (local_quadrature_points_history < &quadrature_point_history.back(),
              ExcInternalError());

      for (unsigned int i=0; i<dim; i++)
        for (unsigned int j=0; j<dim; j++)
        {
          dg_cell->get_dof_values (history_field[i][j],
                                   local_history_fe_values[i][j]);

          dof_to_qpoint_matrix.vmult (local_history_values_at_qpoints[i][j],
                                      local_history_fe_values[i][j]);

          for (unsigned int q=0; q<quadrature.size(); ++q)
            local_quadrature_points_history[q].old_stress[i][j]
              = local_history_values_at_qpoints[i][j](q);
      }
  @endcode

It becomes a bit more complicated once we run the program in parallel, since
then each process only stores this data for the cells it owned on the old
mesh. That said, using a parallel vector for <code>history_field</code> will
do the trick if you put a call to <code>compress</code> after the transfer
from quadrature points into the global vector.


<h5>Ensuring mesh regularity</h5>

At present, the program makes no attempt
to make sure that a cell, after moving its vertices at the end of the time
step, still has a valid geometry (i.e. that its Jacobian determinant is
positive and bounded away from zero everywhere). It is, in fact, not very hard
to set boundary values and forcing terms in such a way that one gets distorted
and inverted cells rather quickly. Certainly, in some cases of large
deformation, this is unavoidable with a mesh of finite mesh size, but in some
other cases this should be preventable by appropriate mesh refinement and/or a
reduction of the time step size. The program does not do that, but a more
sophisticated version definitely should employ some sort of heuristic defining
what amount of deformation of cells is acceptable, and what isn't.


