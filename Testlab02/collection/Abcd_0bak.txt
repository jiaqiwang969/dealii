examples/step-1/doc/intro.dox 

[1.x.0] 

[1.x.1] 

[1.x.2] 

Since this is the first tutorial program, let us comment first on how this tutorial and the rest of the deal.II documentation is supposed to work. The documentation for deal.II comes essentially at three different levels: 

- The tutorial: This is a collection of programs that shows how   deal.II is used in practice. It doesn't typically discuss individual   functions at the level of individual arguments, but rather wants to   give the big picture of how things work together. In other words, it   discusses "concepts": what are the building blocks of deal.II and   how are they used together in finite element programs. 

- The manual: This is the documentation of every single class and   every single (member) function in deal.II. You get there if, for   example, you click on the "Main page" or "Classes" tab at the top of   this page. This is the place where you would look up what the second   argument of  [2.x.0]  means,   to give just one slightly obscure example. You need this level of   documentation for when you know what you want to do, but forgot how   exactly the function was named, what its arguments are, or what it   returns. Note that you also get into the manual whenever you read   through the tutorial and click on any of the class or function   names, i.e. the tutorial contains a great many links into the manual   for whenever you need a more detailed description of a function or   class. On the other hand, the manual is not a good place to learn   deal.II since it gives you a microscopic view of things without   telling you how a function might fit into the bigger picture. 

- Modules: These are groups of classes and functions that work   together or have related functionality. If you click on the   "Modules" tab at the top of this page, you end up on a page that   lists a number of such groups. Each module discusses the underlying   principles of these classes; for example, the  [2.x.1]  module   talks about all sorts of different issues related to storing   sparsity patterns of matrices. This is documentation at an   intermediate level: they give you an overview of what's there in a   particular area. For example when you wonder what finite element   classes exist, you would take a look at the  [2.x.2]  module. The   modules are, of course, also cross-linked to the manual (and, at   times, to the tutorial); if you click on a class name, say on   Triangulation, would will also at the very top right under the class   name get a link to the modules this class is a member of if you want   to learn more about its context. 

Let's come back to the tutorial, since you are looking at the first program (or "step") of it. Each tutorial program is subdivided into the following sections: <ol>    [2.x.3]  [1.x.3] This is a discussion of what the program        does, including the mathematical model, and        what programming techniques are new compared to previous        tutorial programs.    [2.x.4]  [1.x.4] An extensively documented listing of the        source code. Here, we often document individual lines, or        blocks of code, and discuss what they do, how they do it, and        why. The comments frequently reference the introduction,        i.e. you have to understand [1.x.5] the program wants to achieve        (a goal discussed in the introduction) before you can        understand [1.x.6] it intends to get there.    [2.x.5]  [1.x.7] The output of the program, with comments and        interpretation. This section also frequently has a subsection        that gives suggestions on how to extend the program in various        direction; in the earlier programs, this is intended to give        you directions for little experiments designed to make your        familiar with deal.II, while in later programs it is more about        how to use more advanced numerical techniques.    [2.x.6]  [1.x.8] The source code stripped of        all comments. This is useful if you want to see the "big        picture" of the code, since the commented version of the        program has so much text in between that it is often difficult        to see the entire code of a single function on the screen at        once.  [2.x.7]  

The tutorials are not only meant to be static documentation, but you should play with them. To this end, go to the  [2.x.8]  directory (or whatever the number of the tutorial is that you're interested in) and type 

[1.x.9] 

The first command sets up the files that describe which include files this tutorial program depends on, how to compile it and how to run it. This command should find the installed deal.II libraries as well that were generated when you compiled and installed everything as described in the [1.x.10] file. If this command should fail to find the deal.II library, then you need to provide the path to the installation using the command 

[1.x.11] 

instead. 

The second of the commands above compiles the sources into an executable, while the last one executes it (strictly speaking,  [2.x.9]  will also compile the code if the executable doesn't exist yet, so you could have skipped the second command if you wanted). This is all that's needed to run the code and produce the output that is discussed in the "Results" section of the tutorial programs. This sequence needs to be repeated in all of the tutorial directories you want to play with. 

When learning the library, you need to play with it and see what happens. To this end, open the  [2.x.10]  source file with your favorite editor and modify it in some way, save it and run it as above. A few suggestions for possibly modifications are given at the end of the results section of this program, where we also provide a few links to other useful pieces of information. 




[1.x.12] 

This and several of the other tutorial programs are also discussed and demonstrated in [1.x.13] on deal.II and computational science. In particular, you can see the steps he executes to run this and other programs, and you will get a much better idea of the tools that can be used to work with deal.II. In particular, lectures 2 and 4 give an overview of deal.II and of the building blocks of any finite element code. ( [2.x.11]  

If you are not yet familiar with using Linux and running things on the command line, you may be interested in watching lectures 2.9 and 2.91. ( [2.x.12]  line and on what happens when compiling programs, respectively. 

Note that deal.II is actively developed, and in the course of this development we occasionally rename or deprecate functions or classes that are still referenced in these video lectures.  For example, the step-1 code shown in video lecture 5 uses a class HyperShellBoundary which was replaced with SphericalManifold class later on. Additionally, as of deal.II version 9.0,  [2.x.13]  now automatically attaches a SphericalManifold to the Triangulation. Otherwise the rest of the lecture material is relevant. 

[1.x.14] 

Let's come back to step-1, the current program. In this first example, we don't actually do very much, but show two techniques: what is the syntax to generate triangulation objects, and some elements of simple loops over all cells. We create two grids, one which is a regularly refined square (not very exciting, but a common starting grid for some problems), and one more geometric attempt: a ring-shaped domain, which is refined towards the inner edge. Through this, you will get to know three things every finite element program will have to have somewhere: An object of type Triangulation for the mesh; a call to the GridGenerator functions to generate a mesh; and loops over all cells that involve iterators (iterators are a generalization of pointers and are frequently used in the C++ standard library; in the context of deal.II, the  [2.x.14]  module talks about them). 

The program is otherwise small enough that it doesn't need a whole lot of introduction. 

 [2.x.15]  




[1.x.15] 

If you are reading through this tutorial program, chances are that you are interested in continuing to use deal.II for your own projects. Thus, you are about to embark on an exercise in programming using a large-scale scientific computing library. Unless you are already an experienced user of large-scale programming methods, this may be new territory for you &mdash; with all the new rules that go along with it such as the fact that you will have to deal with code written by others, that you may have to think about documenting your own code because you may not remember what exactly it is doing a year down the road (or because others will be using it as well), or coming up with ways to test that your program is doing the right thing. None of this is something that we typically train mathematicians, engineers, or scientists in but that is important when you start writing software of more than a few hundred lines. Remember: Producing software is not the same as just writing code. 

To make your life easier on this journey let us point to some resources that are worthwhile browsing through before you start any large-scale programming: 

- The [1.x.16] has a good number of answers to questions about   particular aspects of deal.II, but also to more general questions such as "How   do I debug scientific computing codes?" or "Can I train myself to write code   that has fewer bugs?". 

- You will benefit from becoming a better programmer. An excellent   resource to this end is the book   [Code Complete](https://en.wikipedia.org/wiki/Code_Complete)   by Steve McConnell  [2.x.16]  . It's already   a few years old, with the last edition published in 2004, but it has   lost none of its appeal as a guide to good programming practices,   and some of the principal developers use it as a group reading   project with every generation of their research group members. 

- The [1.x.17]   that provides introductions to many topics that are important to dealing   with software, such as version control, make files, testing, etc. It is   specifically written for scientists and engineers, not for computer   scientists, and has a focus on short, practical lessons. 

- The [1.x.18] has a lot of resources (and interesting blog posts) that   cover many aspects of writing scientific software. 

- The [1.x.19] also has resources on software development, in   particular for parallel computing. In the "Events" section on   that site are recorded tutorials and webinars that cover many   interesting topics. 

- An article on [1.x.20] that gives an introduction to   many of the ways by which you can make sure you are an efficient   programmer writing programs that work. 

As a general recommendation: If you expect to spend more than a few days writing software in the future, do yourself the favor of learning tools that can make your life more productive, in particular debuggers and integrated development environments. ( [2.x.17]  You will find that you will get the time spent learning these tools back severalfold soon by being more productive! Several of the video lectures referenced above show how to use tools such as integrated development environments or debuggers. 


examples/step-1/doc/results.dox 



[1.x.21] 

Running the program produces graphics of two grids (grid-1.svg and grid-2.svg). You can open these with most every web browser -- in the simplest case, just open the current directory in your file system explorer and click on the file. If you like working on the command line, you call your web browser with the file: `firefox grid-1.svg`, `google-chrome grid-1.svg`, or whatever the name of your browser is. If you do this, the two meshes should look like this: 

 [2.x.18]  

The left one, well, is not very exciting. The right one is &mdash; at least &mdash; unconventional. The pictures color-code the "refinement level" of each cell: How many times did a coarse mesh cell have to be subdivided to obtain the given cell. In the left image, this is boring since the mesh was refined globally a number of times, i.e., [1.x.22] cell was refined the same number of times. 

(While the second mesh is entirely artificial and made-up, and certainly not very practical in applications, to everyone's surprise it has found its way into the literature: see  [2.x.19] . Apparently it is good for some things at least.) 




[1.x.23] 

[1.x.24] 

This program obviously does not have a whole lot of functionality, but in particular the  [2.x.20]  function has a bunch of places where you can play with it. For example, you could modify the criterion by which we decide which cells to refine. An example would be to change the condition to this: 

[1.x.25] 

This would refine all cells for which the  [2.x.21] -coordinate of the cell's center is greater than zero (the  [2.x.22]  function that we call by dereferencing the  [2.x.23]  iterator returns a Point<2> object; subscripting  [2.x.24]  would give the  [2.x.25] -coordinate, subscripting  [2.x.26]  the  [2.x.27] -coordinate). By looking at the functions that TriaAccessor provides, you can also use more complicated criteria for refinement. 

In general, what you can do with operations of the form `cell->something()` is a bit difficult to find in the documentation because `cell` is not a pointer but an iterator. The functions you can call on a cell can be found in the documentation of the classes `TriaAccessor` (which has functions that can also be called on faces of cells or, more generally, all sorts of geometric objects that appear in a triangulation), and `CellAccessor` (which adds a few functions that are specific to *cells*). 

A more thorough description of the whole iterator concept can be found in the  [2.x.28]  documentation module. 




[1.x.26] 

Another possibility would be to generate meshes of entirely different geometries altogether. While for complex geometries there is no way around using meshes obtained from mesh generators, there is a good number of geometries for which deal.II can create meshes using the functions in the GridGenerator namespace. Many of these geometries (such as the one used in this example program) contain cells with curved faces: put another way, we expect the new vertices placed on the boundary to lie along a circle. deal.II handles complex geometries with the Manifold class (and classes inheriting from it); in particular, the functions in GridGenerator corresponding to non-Cartesian grids (such as  [2.x.29]  or  [2.x.30]  attach a Manifold object to the part of the triangulation that should be curved (SphericalManifold and CylindricalManifold, respectively) and use another manifold on the parts that should be flat (FlatManifold). See the documentation of Manifold or the  [2.x.31]  "manifold module" for descriptions of the design philosophy and interfaces of these classes. Take a look at what they provide and see how they could be used in a program like this. 

We also discuss a variety of other ways to create and manipulate meshes (and describe the process of attaching Manifolds) in step-49. 




[1.x.27] 

We close with a comment about modifying or writing programs with deal.II in general. When you start working with tutorial programs or your own applications, you will find that mistakes happen: your program will contain code that either aborts the program right away or bugs that simply lead to wrong results. In either case, you will find it extremely helpful to know how to work with a debugger: you may get by for a while by just putting debug output into your program, compiling it, and running it, but ultimately finding bugs with a debugger is much faster, much more convenient, and more reliable because you don't have to recompile the program all the time and because you can inspect the values of variables and how they change. 

Rather than postponing learning how to use a debugger till you really can't see any other way to find a bug, here's the one piece of advice we will provide in this program: learn how to use a debugger as soon as possible. It will be time well invested. ( [2.x.32]  Questions (FAQ) page linked to from the top-level [1.x.28] also provides a good number of hints on debugging deal.II programs. 




[1.x.29] 

It is often useful to include meshes into your theses or publications. For this, it may not be very useful to color-code the cells by refinement level, and to print the cell number onto each cell. But it doesn't have to be that way -- the GridOut class allows setting flags for each possible output format (see the classes in the GridOutFlags namespace) that control how exactly a mesh is plotted. You can of course also choose other output file formats such as VTK or VTU; this is particularly useful for 3d meshes where a 2d format such as SVG is not particular useful because it fixes a particular viewpoint onto the 3d object. As a consequence, you might want to explore other options in the GridOut class. 


examples/step-10/doc/intro.dox 

[1.x.30] 

[1.x.31] 


This is a rather short example which only shows some aspects of using higher order mappings. By  [2.x.33] mapping [2.x.34]  we mean the transformation between the unit cell (i.e. the unit line, square, or cube) to the cells in real space. In all the previous examples, we have implicitly used linear or d-linear mappings; you will not have noticed this at all, since this is what happens if you do not do anything special. However, if your domain has curved boundaries, there are cases where the piecewise linear approximation of the boundary (i.e. by straight line segments) is not sufficient, and you want that your computational domain is an approximation to the real domain using curved boundaries as well. If the boundary approximation uses piecewise quadratic parabolas to approximate the true boundary, then we say that this is a quadratic or  [2.x.35]  approximation. If we use piecewise graphs of cubic polynomials, then this is a  [2.x.36]  approximation, and so on. 




For some differential equations, it is known that piecewise linear approximations of the boundary, i.e.  [2.x.37]  mappings, are not sufficient if the boundary of the exact domain is curved. Examples are the biharmonic equation using  [2.x.38]  elements, or the Euler equations of gas dynamics on domains with curved reflective boundaries. In these cases, it is necessary to compute the integrals using a higher order mapping. If we do not use such a higher order mapping, the order of approximation of the boundary dominates the order of convergence of the entire numerical scheme, irrespective of the order of convergence of the discretization in the interior of the domain. 




Rather than demonstrating the use of higher order mappings with one of these more complicated examples, we do only a brief computation: calculating the value of  [2.x.39]  by two different methods. 




The first method uses a triangulated approximation of the circle with unit radius and integrates a unit magnitude constant function ( [2.x.40] ) over it. Of course, if the domain were the exact unit circle, then the area would be  [2.x.41] , but since we only use an approximation by piecewise polynomial segments, the value of the area we integrate over is not exactly  [2.x.42] . However, it is known that as we refine the triangulation, a  [2.x.43]  mapping approximates the boundary with an order  [2.x.44] , where  [2.x.45]  is the mesh size. We will check the values of the computed area of the circle and their convergence towards  [2.x.46]  under mesh refinement for different mappings. We will also find a convergence behavior that is surprising at first, but has a good explanation. 




The second method works similarly, but this time does not use the area of the triangulated unit circle, but rather its perimeter.  [2.x.47]  is then approximated by half of the perimeter, as we choose the radius equal to one. 




 [2.x.48]  This tutorial shows in essence how to choose a particular mapping for integrals, by attaching a particular geometry to the triangulation (as had already been done in step-1, for example) and then passing a mapping argument to the FEValues class that is used for all integrals in deal.II. The geometry we choose is a circle, for which deal.II already has a class (SphericalManifold) that can be used. If you want to define your own geometry, for example because it is complicated and cannot be described by the classes already available in deal.II, you will want to read through step-53. 


examples/step-10/doc/results.dox 



[1.x.32] 


The program performs two tasks, the first being to generate a visualization of the mapped domain, the second to compute pi by the two methods described. Let us first take a look at the generated graphics. They are generated in Gnuplot format, and can be viewed with the commands 

[1.x.33] 

or using one of the other filenames. The second line makes sure that the aspect ratio of the generated output is actually 1:1, i.e. a circle is drawn as a circle on your screen, rather than as an ellipse. The third line switches off the key in the graphic, as that will only print information (the filename) which is not that important right now. Similarly, the fourth and fifth disable tick marks. The plot is then generated with a specific line width ("lw", here set to 4) and line type ("lt", here chosen by saying that the line should be drawn using the RGB color "black"). 

The following table shows the triangulated computational domain for  [2.x.49] ,  [2.x.50] , and  [2.x.51]  mappings, for the original coarse grid (left), and a once uniformly refined grid (right). 

<div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_ball_0_q1.svg"          alt="Five-cell discretization of the disk."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_ball_1_q1.svg"          alt="20-cell discretization of the disk (i.e., five cells               refined once)."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_ball_0_q2.svg"          alt="Five-cell discretization of the disk with quadratic edges. The               boundary is nearly indistinguishable from the actual circle."          width="400" height="400"          >   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_ball_1_q2.svg"          alt="20-cell discretization with quadratic edges."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_ball_0_q3.svg"          alt="Five-cell discretization of the disk with cubic edges. The               boundary is nearly indistinguishable from the actual circle."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_ball_1_q3.svg"          alt="20-cell discretization with cubic edges."          width="400" height="400">   </div> </div> 

These pictures show the obvious advantage of higher order mappings: they approximate the true boundary quite well also on rather coarse meshes. To demonstrate this a little further, here is part of the upper right quarter circle of the coarse meshes with  [2.x.52]  and  [2.x.53]  mappings, where the dashed red line marks the actual circle: 

<div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_exact_vs_interpolate_q2.svg"          alt="Close-up of quadratic discretization. The distance between the          quadratic interpolant and the actual circle is small."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_10_exact_vs_interpolate_q3.svg"          alt="Close-up of cubic discretization. The distance between the          cubic interpolant and the actual circle is very small."          width="400" height="400">   </div> </div> 

Obviously the quadratic mapping approximates the boundary quite well, while for the cubic mapping the difference between approximated domain and true one is hardly visible already for the coarse grid. You can also see that the mapping only changes something at the outer boundaries of the triangulation. In the interior, all lines are still represented by linear functions, resulting in additional computations only on cells at the boundary. Higher order mappings are therefore usually not noticeably slower than lower order ones, because the additional computations are only performed on a small subset of all cells. 




The second purpose of the program was to compute the value of pi to good accuracy. This is the output of this part of the program: 

[1.x.34] 






One of the immediate observations from the output is that in all cases the values converge quickly to the true value of  [2.x.54] . Note that for the  [2.x.55]  mapping, we are already in the regime of roundoff errors and the convergence rate levels off, which is already quite a lot. However, also note that for the  [2.x.56]  mapping, even on the finest grid the accuracy is significantly worse than on the coarse grid for a  [2.x.57]  mapping! 




The last column of the output shows the convergence order, in powers of the mesh width  [2.x.58] . In the introduction, we had stated that the convergence order for a  [2.x.59]  mapping should be  [2.x.60] . However, in the example shown, the order is rather  [2.x.61] ! This at first surprising fact is explained by the properties of the  [2.x.62]  mapping. At order [1.x.35], it uses support points that are based on the [1.x.36]+1 point Gauss-Lobatto quadrature rule that selects the support points in such a way that the quadrature rule converges at order 2[1.x.37]. Even though these points are here only used for interpolation of a [1.x.38]th order polynomial, we get a superconvergence effect when numerically evaluating the integral, resulting in the observed high order of convergence. (This effect is also discussed in detail in the following publication: A. Bonito, A. Demlow, and J. Owen: "A priori error estimates for finite element approximations to eigenvalues and eigenfunctions of the Laplace-Beltrami operator", submitted, 2018.) 


examples/step-11/doc/intro.dox 

[1.x.39] 

[1.x.40] 

The problem we will be considering is the solution of Laplace's problem with Neumann boundary conditions only: 

[1.x.41] 

It is well known that if this problem is to have a solution, then the forces need to satisfy the compatibility condition 

[1.x.42] 

We will consider the special case that  [2.x.63]  is the circle of radius 1 around the origin, and  [2.x.64] ,  [2.x.65] . This choice satisfies the compatibility condition. 

The compatibility condition allows a solution of the above equation, but it nevertheless retains an ambiguity: since only derivatives of the solution appear in the equations, the solution is only determined up to a constant. For this reason, we have to pose another condition for the numerical solution, which fixes this constant. 

For this, there are various possibilities: <ol>  [2.x.66]  Fix one node of the discretization to zero or any other fixed value.   This amounts to an additional condition  [2.x.67] . Although this is   common practice, it is not necessarily a good idea, since we know that the   solutions of Laplace's equation are only in  [2.x.68] , which does not allow for   the definition of point values because it is not a subset of the continuous   functions. Therefore, even though fixing one node is allowed for   discretized functions, it is not for continuous functions, and one can   often see this in a resulting error spike at this point in the numerical   solution. 

 [2.x.69]  Fixing the mean value over the domain to zero or any other value. This   is allowed on the continuous level, since  [2.x.70]    by Sobolev's inequality, and thus also on the discrete level since we   there only consider subsets of  [2.x.71] . 

 [2.x.72]  Fixing the mean value over the boundary of the domain to zero or any   other value. This is also allowed on the continuous level, since    [2.x.73] , again by Sobolev's   inequality.  [2.x.74]  We will choose the last possibility, since we want to demonstrate another technique with it. 

While this describes the problem to be solved, we still have to figure out how to implement it. Basically, except for the additional mean value constraint, we have solved this problem several times, using Dirichlet boundary values, and we only need to drop the treatment of Dirichlet boundary nodes. The use of higher order mappings is also rather trivial and will be explained at the various places where we use it; in almost all conceivable cases, you will only consider the objects describing mappings as a black box which you need not worry about, because their only uses seem to be to be passed to places deep inside the library where functions know how to handle them (i.e. in the  [2.x.75]  classes and their descendants). 

The tricky point in this program is the use of the mean value constraint. Fortunately, there is a class in the library which knows how to handle such constraints, and we have used it quite often already, without mentioning its generality. Note that if we assume that the boundary nodes are spaced equally along the boundary, then the mean value constraint 

[1.x.43] 

can be written as 

[1.x.44] 

where the sum shall run over all degree of freedom indices which are located on the boundary of the computational domain. Let us denote by  [2.x.76]  that index on the boundary with the lowest number (or any other conveniently chosen index), then the constraint can also be represented by 

[1.x.45] 

This, luckily, is exactly the form of constraints for which the AffineConstraints class was designed. Note that we have used this class in several previous examples for the representation of hanging nodes constraints, which also have this form: there, the middle vertex shall have the mean of the values of the adjacent vertices. In general, the AffineConstraints class is designed to handle affine constraints of the form 

[1.x.46] 

where  [2.x.77]  denotes a matrix,  [2.x.78]  denotes a vector, and  [2.x.79]  the vector of nodal values. In this case, since  [2.x.80]  represents one homogeneous constraint,  [2.x.81]  is the zero vector. 

In this example, the mean value along the boundary allows just such a representation, with  [2.x.82]  being a matrix with just one row (i.e. there is only one constraint). In the implementation, we will create an AffineConstraints object, add one constraint (i.e. add another row to the matrix) referring to the first boundary node  [2.x.83] , and insert the weights with which all the other nodes contribute, which in this example happens to be just  [2.x.84] . 

Later, we will use this object to eliminate the first boundary node from the linear system of equations, reducing it to one which has a solution without the ambiguity of the constant shift value. One of the problems of the implementation will be that the explicit elimination of this node results in a number of additional elements in the matrix, of which we do not know in advance where they are located and how many additional entries will be in each of the rows of the matrix. We will show how we can use an intermediate object to work around this problem. 

But now on to the implementation of the program solving this problem... 


examples/step-11/doc/results.dox 



[1.x.47] 

This is what the program outputs: 

[1.x.48] 

As we expected, the convergence order for each of the different mappings is clearly quadratic in the mesh size. What  [2.x.85] is [2.x.86]  interesting, though, is that the error for a bilinear mapping (i.e. degree 1) is more than three times larger than that for the higher order mappings; it is therefore clearly advantageous in this case to use a higher order mapping, not because it improves the order of convergence but just to reduce the constant before the convergence order. On the other hand, using a cubic mapping only improves the result further by insignificant amounts, except on very coarse grids. 

We can also visualize the underlying meshes by using, for instance, ParaView. The image below shows initial meshes for different mapping degrees: 

 [2.x.87]  

Clearly, the effect is most pronounced when we go from the linear to quadratic mapping. This is also reflected in the error values given in the table above. The effect of going from quadratic to cubic degree is less dramatic, but still tangible owing to a more accurate description of the circular boundary. 

Next, let's look at the meshes after three global refinements 

 [2.x.88]  

Here, the differences are much less visible, especially for higher order mappings. Indeed, at this refinement level the error values reported in the table are essentially identical between mappings of degrees two and three. 


examples/step-12/doc/intro.dox 

 [2.x.89]  

[1.x.49] 

[1.x.50] 

[1.x.51] 

[1.x.52] 

This example is devoted to the  [2.x.90] discontinuous Galerkin method [2.x.91] , or in short, the DG method. It includes the following topics. <ol>    [2.x.92]  Discretization of the linear advection equation with the DG method.    [2.x.93]  Assembling of jump terms and other expressions on the interface between cells using FEInterfaceValues.    [2.x.94]  Assembling of the system matrix using the  [2.x.95]   [2.x.96]  

The particular concern of this program are the loops of DG methods. These turn out to be especially complex, primarily because for the face terms, we have to distinguish the cases of boundary, regular interior faces and interior faces with hanging nodes, respectively. The  [2.x.97]  handles the complexity on iterating over cells and faces and allows specifying "workers" for the different cell and face terms. The integration of face terms itself, including on adaptively refined faces, is done using the FEInterfaceValues class. 

[1.x.53] 

The model problem solved in this example is the linear advection equation 

[1.x.54] 

subject to the boundary conditions 

[1.x.55] 

on the inflow part  [2.x.98]  of the boundary  [2.x.99]  of the domain.  Here,  [2.x.100]  denotes a vector field,  [2.x.101]  the (scalar) solution function,  [2.x.102]  a boundary value function, 

[1.x.56] 

the inflow part of the boundary of the domain and  [2.x.103]  denotes the unit outward normal to the boundary  [2.x.104] . This equation is the conservative version of the advection equation already considered in step-9 of this tutorial. 


On each cell  [2.x.105] , we multiply by a test function  [2.x.106]  from the left and integrate by parts to get: 

[1.x.57] 

When summing this expression over all cells  [2.x.107] , the boundary integral is done over all internal and external faces and as such there are three cases: <ol>  [2.x.108]  outer boundary on the inflow (we replace  [2.x.109]  by given  [2.x.110] ):    [2.x.111]   [2.x.112]  outer boundary on the outflow:    [2.x.113]   [2.x.114]  inner faces (integral from two sides turns into jump, we use the upwind velocity):    [2.x.115]   [2.x.116]  

Here, the jump is defined as  [2.x.117] , where the superscripts refer to the left ('+') and right ('-') values at the face. The upwind value  [2.x.118]  is defined to be  [2.x.119]  if  [2.x.120]  and  [2.x.121]  otherwise. 

As a result, the mesh-dependent weak form reads: 

[1.x.58] 

Here,  [2.x.122]  is the set of all active cells of the triangulation and  [2.x.123]  is the set of all active interior faces. This formulation is known as the upwind discontinuous Galerkin method. 

In order to implement this bilinear form, we need to compute the cell terms (first sum) using the usual way to achieve integration on a cell, the interface terms (second sum) using FEInterfaceValues, and the boundary terms (the other two terms). The summation of all those is done by  [2.x.124]  




[1.x.59] 

We solve the advection equation on  [2.x.125]  with  [2.x.126]  representing a circular counterclockwise flow field, and  [2.x.127]  on  [2.x.128]  and  [2.x.129]  on  [2.x.130] . 

We solve on a sequence of meshes by refining the mesh adaptively by estimating the norm of the gradient on each cell. After solving on each mesh, we output the solution in vtk format and compute the  [2.x.131]  norm of the solution. As the exact solution is either 0 or 1, we can measure the magnitude of the overshoot of the numerical solution with this. 


examples/step-12/doc/results.dox 



[1.x.60] 


The output of this program consist of the console output and solutions in vtk format: 

[1.x.61] 



We show the solutions on the initial mesh, the mesh after two and after five adaptive refinement steps. 

 [2.x.132]   [2.x.133]   [2.x.134]  

And finally we show a plot of a 3d computation. 

 [2.x.135]  


[1.x.62] 

[1.x.63] 

In this program we have used discontinuous elements. It is a legitimate question to ask why not simply use the normal, continuous ones. Of course, to everyone with a background in numerical methods, the answer is obvious: the continuous Galerkin (cG) method is not stable for the transport equation, unless one specifically adds stabilization terms. The DG method, however, [1.x.64] stable. Illustrating this with the current program is not very difficult; in fact, only the following minor modifications are necessary: 

- Change the element to FE_Q instead of FE_DGQ. 

- Add handling of hanging node constraints in exactly the same way as step-6. 

- We need a different solver; the direct solver in step-29 is a convenient   choice. An experienced deal.II user will be able to do this in less than 10 minutes. 

While the 2d solution has been shown above, containing a number of small spikes at the interface that are, however, stable in height under mesh refinement, results look much different when using a continuous element: 

 [2.x.136]  

In refinement iteration 5, the image can't be plotted in a reasonable way any more as a 3d plot. We thus show a color plot with a range of  [2.x.137]  (the solution values of the exact solution lie in  [2.x.138] , of course). In any case, it is clear that the continuous Galerkin solution exhibits oscillatory behavior that gets worse and worse as the mesh is refined more and more. 

There are a number of strategies to stabilize the cG method, if one wants to use continuous elements for some reason. Discussing these methods is beyond the scope of this tutorial program; an interested reader could, for example, take a look at step-31. 




[1.x.65] 

[1.x.66] 

Given that the exact solution is known in this case, one interesting avenue for further extensions would be to confirm the order of convergence for this program. In the current case, the solution is non-smooth, and so we can not expect to get a particularly high order of convergence, even if we used higher order elements. But even if the solution [1.x.67] smooth, the equation is not elliptic and so it is not immediately clear that we should obtain a convergence order that equals that of the optimal interpolation estimates (i.e. for example that we would get  [2.x.139]  convergence in the  [2.x.140]  norm by using quadratic elements). 

In fact, for hyperbolic equations, theoretical predictions often indicate that the best one can hope for is an order one half below the interpolation estimate. For example, for the streamline diffusion method (an alternative method to the DG method used here to stabilize the solution of the transport equation), one can prove that for elements of degree  [2.x.141] , the order of convergence is  [2.x.142]  on arbitrary meshes. While the observed order is frequently  [2.x.143]  on uniformly refined meshes, one can construct so-called Peterson meshes on which the worse theoretical bound is actually attained. This should be relatively simple to verify, for example using the  [2.x.144]  function. 

A different direction is to observe that the solution of transport problems often has discontinuities and that therefore a mesh in which we [1.x.68] every cell in every coordinate direction may not be optimal. Rather, a better strategy would be to only cut cells in the direction parallel to the discontinuity. This is called [1.x.69] and is the subject of step-30. 


examples/step-12b/doc/intro.dox 

[1.x.70] 

[1.x.71] 

This is a variant of step-16 with the only change that we are using the MeshWorker framework with the pre-made LocalIntegrator helper classes instead of assembling the face terms using FEInterfaceValues. 

The details of this framework on how it is used in practice will be explained as part of this tutorial program. 

[1.x.72] 

The problem we solve here is the same as the one in step-12. 


examples/step-12b/doc/results.dox 



[1.x.73] 


The output of this program is very similar to step-16 and we are not repeating the output here. 

We show the solutions on the initial mesh, the mesh after two and after five adaptive refinement steps. 

 [2.x.145]   [2.x.146]   [2.x.147]  


Then we show the final grid (after 5 refinement steps) and the solution again, this time with a nicer 3d rendering (obtained using the  [2.x.148]  function and the VTK-based VisIt visualization program) that better shows the sharpness of the jump on the refined mesh and the over- and undershoots of the solution along the interface: 

 [2.x.149]   [2.x.150]  


And finally we show a plot of a 3d computation. 

 [2.x.151]  


[1.x.74] 

[1.x.75] 

For ideas for further extensions, please see see step-12. 


examples/step-13/doc/intro.dox 

[1.x.76] 

[1.x.77] 

[1.x.78] 


In this example program, we will not so much be concerned with describing new ways how to use deal.II and its facilities, but rather with presenting methods of writing modular and extensible finite element programs. The main reason for this is the size and complexity of modern research software: applications implementing modern error estimation concepts and adaptive solution methods tend to become rather large. For example, when this program was written in 2002, the three largest applications by the main authors of deal.II, are at the time of writing of this example program: <ol>  [2.x.152]  a program for solving conservation hyperbolic equations by the      Discontinuous Galerkin Finite Element method: 33,775 lines of      code;  [2.x.153]  a parameter estimation program: 28,980 lines of code;  [2.x.154]  a wave equation solver: 21,020 lines of code.  [2.x.155]  

(The library proper - without example programs and test suite - has slightly more than 150,000 lines of code as of spring 2002. It is of course several times larger now.) The sizes of these applications are at the edge of what one person, even an experienced programmer, can manage. 




The numbers above make one thing rather clear: monolithic programs that are not broken up into smaller, mostly independent pieces have no way of surviving, since even the author will quickly lose the overview of the various dependencies between different parts of a program. Only data encapsulation, for example using object oriented programming methods, and modularization by defining small but fixed interfaces can help structure data flow and mutual interdependencies. It is also an absolute prerequisite if more than one person is developing a program, since otherwise confusion will quickly prevail as one developer would need to know if another changed something about the internals of a different module if they were not cleanly separated. 




In previous examples, you have seen how the library itself is broken up into several complexes each building atop the underlying ones, but relatively independent of the other ones: <ol>  [2.x.156] the triangulation class complex, with associated iterator classes;  [2.x.157] the finite element classes;  [2.x.158] the DoFHandler class complex, with associated iterators, built on     the triangulation and finite element classes;  [2.x.159] the classes implementing mappings between unit and real cells;  [2.x.160] the FEValues class complex, built atop the finite elements and     mappings.  [2.x.161]  Besides these, and a large number of smaller classes, there are of course the following "tool" modules: <ol>  [2.x.162] output in various graphical formats;  [2.x.163] linear algebra classes.  [2.x.164]  These complexes can also be found as a flow chart on the front page of the deal.II manual website. 




The goal of this program is now to give an example of how a relatively simple finite element program could be structured such that we end up with a set of modules that are as independent of each other as possible. This allows to change the program at one end, without having to worry that it might break at the other, as long as we do not touch the interface through which the two ends communicate. The interface in C++, of course, is the declaration of abstract base classes. 




Here, we will implement (again) a Laplace solver, although with a number of differences compared to previous example programs: <ol>  [2.x.165] The classes that implement the process of numerically solving the     equation are no more responsible for driving the process of     "solving-estimating error-refining-solving again", but we delegate     this to external functions. This allows first to use it as a     building block in a larger context, where the solution of a     Laplace equation might only be one part (for example, in a     nonlinear problem, where Laplace equations might have to be solved     in each nonlinear step). It would also allow to build a framework     around this class that would allow using solvers for other     equations (but with the same external interface) instead, in case     some techniques shall be evaluated for different types of partial     differential equations.  [2.x.166] It splits the process of evaluating the computed solution to a     separate set of classes. The reason is that one is usually not     interested in the solution of a PDE per se, but rather in certain     aspects of it. For example, one might wish to compute the traction     at a certain boundary in elastic computations, or in the signal of     a seismic wave at a receiver position at a given     location. Sometimes, one might have an interest in several of     these aspects. Since the evaluation of a solution is something     that does not usually affect the process of solution, we split it     off into a separate module, to allow for the development of such     evaluation filters independently of the development of the solver     classes.  [2.x.167] Separate the classes that implement mesh refinement from the     classes that compute the solution.  [2.x.168] Separate the description of the test case with which we will     present the program, from the rest of the program.  [2.x.169] Parallelize the assembly of linear systems using the WorkStream     facilities. This follows the extensive description that can be     found in the  [2.x.170]  "Parallel computing with multiple processors accessing shared memory"     documentation module. The implementation essentially follows what     has already been described in step-9.  [2.x.171]  




The things the program does are not new. In fact, this is more like a melange of previous programs, cannibalizing various parts and functions from earlier examples. It is the way they are arranged in this program that should be the focus of the reader, i.e. the software design techniques used in the program to achieve the goal of implementing the desired mathematical method. However, we must stress that software design is in part also a subjective matter: different persons have different programming backgrounds and have different opinions about the "right" style of programming; this program therefore expresses only what the author considers useful practice, and is not necessarily a style that you have to adopt in order to write successful numerical software if you feel uncomfortable with the chosen ways. It should serve as a case study, however, inspiring the reader with ideas to the desired end. 




Once you have worked through the program, you will remark that it is already somewhat complex in its structure. Nevertheless, it only has about 850 lines of code, without comments. In real applications, there would of course be comments and class documentation, which would bring that to maybe 1200 lines. Yet, compared to the applications listed above, this is still small, as they are 20 to 25 times as large. For programs as large, a proper design right from the start is thus indispensable. Otherwise, it will have to be redesigned at one point in its life, once it becomes too large to be manageable. 




Despite of this, all three programs listed above have undergone major revisions, or even rewrites. The wave program, for example, was once entirely teared to parts when it was still significantly smaller, just to assemble it again in a more modular form. By that time, it had become impossible to add functionality without affecting older parts of the code (the main problem with the code was the data flow: in time dependent application, the major concern is when to store data to disk and when to reload it again; if this is not done in an organized fashion, then you end up with data released too early, loaded too late, or not released at all). Although the present example program thus draws from several years of experience, it is certainly not without flaws in its design, and in particular might not be suited for an application where the objective is different. It should serve as an inspiration for writing your own application in a modular way, to avoid the pitfalls of too closely coupled codes. 




[1.x.79] 


What the program actually does is not even the main point of this program, the structure of the program is more important. However, in a few words, a description would be: solve the Laplace equation for a given right hand side such that the solution is the function  [2.x.172] . The goal of the computation is to get the value of the solution at the point  [2.x.173] , and to compare the accuracy with which we resolve this value for two refinement criteria, namely global refinement and refinement by the error indicator by Kelly et al. which we have already used in previous examples. 




The results will, as usual, be discussed in the respective section of this document. In doing so, we will find a slightly irritating observation about the relative performance of the two refinement criteria. In a later example program, building atop this one, we will devise a different method that should hopefully perform better than the techniques discussed here. 




So much now for all the theoretical and anecdotal background. The best way of learning about a program is to look at it, so here it is: 


examples/step-13/doc/results.dox 



[1.x.80] 




The results of this program are not that interesting - after all its purpose was not to demonstrate some new mathematical idea, and also not how to program with deal.II, but rather to use the material which we have developed in the previous examples to form something which demonstrates a way to build modern finite element software in a modular and extensible way. 




Nevertheless, we of course show the results of the program. Of foremost interest is the point value computation, for which we had implemented the corresponding evaluation class. The results (i.e. the output) of the program looks as follows: 

[1.x.81] 




What surprises here is that the exact value is 1.59491554..., and that it is apparently surprisingly complicated to compute the solution even to only one per cent accuracy, although the solution is smooth (in fact infinitely often differentiable). This smoothness is shown in the graphical output generated by the program, here coarse grid and the first 9 refinement steps of the Kelly refinement indicator: 


 [2.x.174]  


While we're already at watching pictures, this is the eighth grid, as viewed from top: 


 [2.x.175]  


However, we are not yet finished with evaluation the point value computation. In fact, plotting the error  [2.x.176]  for the two refinement criteria yields the following picture: 


 [2.x.177]  





What  [2.x.178] is [2.x.179]  disturbing about this picture is that not only is the adaptive mesh refinement not better than global refinement as one would usually expect, it is even significantly worse since its convergence is irregular, preventing all extrapolation techniques when using the values of subsequent meshes! On the other hand, global refinement provides a perfect  [2.x.180]  or  [2.x.181]  convergence history and provides every opportunity to even improve on the point values by extrapolation. Global mesh refinement must therefore be considered superior in this example! This is even more surprising as the evaluation point is not somewhere in the left part where the mesh is coarse, but rather to the right and the adaptive refinement should refine the mesh around the evaluation point as well. 




We thus close the discussion of this example program with a question: 

<p align="center">   <strong> [2.x.182] What is wrong with adaptivity if it is not better than   global refinement? [2.x.183] </strong> 





 [2.x.184] Exercise at the end of this example: [2.x.185]  There is a simple reason for the bad and irregular behavior of the adapted mesh solutions. It is simple to find out by looking at the mesh around the evaluation point in each of the steps - the data for this is in the output files of the program. An exercise would therefore be to modify the mesh refinement routine such that the problem (once you remark it) is avoided. The second exercise is to check whether the results are then better than global refinement, and if so if even a better order of convergence (in terms of the number of degrees of freedom) is achieved, or only by a better constant. 




( [2.x.186] Very brief answers for the impatient: [2.x.187]  at steps with larger errors, the mesh is not regular at the point of evaluation, i.e. some of the adjacent cells have hanging nodes; this destroys some superapproximation effects of which the globally refined mesh can profit. Answer 2: this quick hack 

[1.x.82] 

in the refinement function of the Kelly refinement class right before executing refinement would improve the results (exercise: what does the code do?), making them consistently better than global refinement. Behavior is still irregular, though, so no results about an order of convergence are possible.) 


examples/step-14/doc/intro.dox 

[1.x.83] 

[1.x.84] 

[1.x.85] 

The Heidelberg group of Professor Rolf Rannacher, to which the three initial authors of the deal.II library belonged during their PhD time and partly also afterwards, has been involved with adaptivity and error estimation for finite element discretizations since the mid-1990ies. The main achievement is the development of error estimates for arbitrary functionals of the solution, and of optimal mesh refinement for its computation. 

We will not discuss the derivation of these concepts in too great detail, but will implement the main ideas in the present example program. For a thorough introduction into the general idea, we refer to the seminal work of Becker and Rannacher  [2.x.188] ,  [2.x.189] , and the overview article of the same authors in Acta Numerica  [2.x.190] ; the first introduces the concept of error estimation and adaptivity for general functional output for the Laplace equation, while the second gives many examples of applications of these concepts to a large number of other, more complicated equations. For applications to individual types of equations, see also the publications by Becker  [2.x.191] ,  [2.x.192] , Kanschat  [2.x.193] ,  [2.x.194] , Suttmeier  [2.x.195] ,  [2.x.196] ,  [2.x.197] ,  [2.x.198] , Bangerth  [2.x.199] ,  [2.x.200] ,  [2.x.201] ,  [2.x.202] , and Hartmann  [2.x.203] ,  [2.x.204] ,  [2.x.205] . All of these works, from the original introduction by Becker and Rannacher to individual contributions to particular equations, have later been summarized in a book by Bangerth and Rannacher that covers all of these topics, see  [2.x.206] . 


The basic idea is the following: in applications, one is not usually interested in the solution per se, but rather in certain aspects of it. For example, in simulations of flow problems, one may want to know the lift or drag of a body immersed in the fluid; it is this quantity that we want to know to best accuracy, and whether the rest of the solution of the describing equations is well resolved is not of primary interest. Likewise, in elasticity one might want to know about values of the stress at certain points to guess whether maximal load values of joints are safe, for example. Or, in radiative transfer problems, mean flux intensities are of interest. 

In all the cases just listed, it is the evaluation of a functional  [2.x.207]  of the solution which we are interested in, rather than the values of  [2.x.208]  everywhere. Since the exact solution  [2.x.209]  is not available, but only its numerical approximation  [2.x.210] , it is sensible to ask whether the computed value  [2.x.211]  is within certain limits of the exact value  [2.x.212] , i.e. we want to bound the error with respect to this functional,  [2.x.213] . 

For simplicity of exposition, we henceforth assume that both the quantity of interest  [2.x.214] , as well as the equation are linear, and we will in particular show the derivation for the Laplace equation with homogeneous Dirichlet boundary conditions, although the concept is much more general. For this general case, we refer to the references listed above.  The goal is to obtain bounds on the error,  [2.x.215] . For this, let us denote by  [2.x.216]  the solution of a dual problem, defined as follows: 

[1.x.86] 

where  [2.x.217]  is the bilinear form associated with the differential equation, and the test functions are chosen from the corresponding solution space. Then, taking as special test function  [2.x.218]  the error, we have that 

[1.x.87] 

and we can, by Galerkin orthogonality, rewrite this as 

[1.x.88] 

where  [2.x.219]  can be chosen from the discrete test space in whatever way we find convenient. 

Concretely, for Laplace's equation, the error identity reads 

[1.x.89] 

Because we want to use this formula not only to compute error, but also to refine the mesh, we need to rewrite the expression above as a sum over cells where each cell's contribution can then be used as an error indicator for this cell. Thus, we split the scalar products into terms for each cell, and integrate by parts on each of them: 

[1.x.90] 

Next we use that  [2.x.220] , and that for solutions of the Laplace equation, the solution is smooth enough that  [2.x.221]  is continuous almost everywhere -- so the terms involving  [2.x.222]  on one cell cancels with that on its neighbor, where the normal vector has the opposite sign. (The same is not true for  [2.x.223] , though.) At the boundary of the domain, where there is no neighbor cell with which this term could cancel, the weight  [2.x.224]  can be chosen as zero, and the whole term disappears. 

Thus, we have 

[1.x.91] 

In a final step, note that when taking the normal derivative of  [2.x.225] , we mean the value of this quantity as taken from this side of the cell (for the usual Lagrange elements, derivatives are not continuous across edges). We then rewrite the above formula by exchanging half of the edge integral of cell  [2.x.226]  with the neighbor cell  [2.x.227] , to obtain 

[1.x.92] 

Using that for the normal vectors on adjacent cells we have  [2.x.228] , we define the jump of the normal derivative by 

[1.x.93] 

and get the final form after setting the discrete function  [2.x.229] , which is by now still arbitrary, to the point interpolation of the dual solution,  [2.x.230] : 

[1.x.94] 



With this, we have obtained an exact representation of the error of the finite element discretization with respect to arbitrary (linear) functionals  [2.x.231] . Its structure is a weighted form of a residual estimator, as both  [2.x.232]  and  [2.x.233]  are cell and edge residuals that vanish on the exact solution, and  [2.x.234]  are weights indicating how important the residuals on a certain cell is for the evaluation of the given functional. Furthermore, it is a cell-wise quantity, so we can use it as a mesh refinement criterion. The question, is: how to evaluate it? After all, the evaluation requires knowledge of the dual solution  [2.x.235] , which carries the information about the quantity we want to know to best accuracy. 

In some, very special cases, this dual solution is known. For example, if the functional  [2.x.236]  is the point evaluation,  [2.x.237] , then the dual solution has to satisfy 

[1.x.95] 

with the Dirac delta function on the right hand side, and the dual solution is the Green's function with respect to the point  [2.x.238] . For simple geometries, this function is analytically known, and we could insert it into the error representation formula. 

However, we do not want to restrict ourselves to such special cases. Rather, we will compute the dual solution numerically, and approximate  [2.x.239]  by some numerically obtained  [2.x.240] . We note that it is not sufficient to compute this approximation  [2.x.241]  using the same method as used for the primal solution  [2.x.242] , since then  [2.x.243] , and the overall error estimate would be zero. Rather, the approximation  [2.x.244]  has to be from a larger space than the primal finite element space. There are various ways to obtain such an approximation (see the cited literature), and we will choose to compute it with a higher order finite element space. While this is certainly not the most efficient way, it is simple since we already have all we need to do that in place, and it also allows for simple experimenting. For more efficient methods, again refer to the given literature, in particular  [2.x.245] ,  [2.x.246] . 

With this, we end the discussion of the mathematical side of this program and turn to the actual implementation. 




 [2.x.247]  There are two steps above that do not seem necessary if all you care about is computing the error: namely, (i) the subtraction of  [2.x.248]  from  [2.x.249] , and (ii) splitting the integral into a sum of cells and integrating by parts on each. Indeed, neither of these two steps change  [2.x.250]  at all, as we only ever consider identities above until the substitution of  [2.x.251]  by  [2.x.252] . In other words, if you care only about [1.x.96]  [2.x.253] , then these steps are not necessary. On the other hand, if you want to use the error estimate also as a refinement criterion for each cell of the mesh, then it is necessary to (i) break the estimate into a sum of cells, and (ii) massage the formulas in such a way that each cell's contributions have something to do with the local error. (While the contortions above do not change the value of the [1.x.97]  [2.x.254] , they change the values we compute for each cell  [2.x.255] .) To this end, we want to write everything in the form "residual times dual weight" where a "residual" is something that goes to zero as the approximation becomes  [2.x.256]  better and better. For example, the quantity  [2.x.257]  is not a residual, since it simply converges to the (normal component of) the gradient of the exact solution. On the other hand,  [2.x.258]  is a residual because it converges to  [2.x.259] . All of the steps we have taken above in developing the final form of  [2.x.260]  have indeed had the goal of bringing the final formula into a form where each term converges to zero as the discrete solution  [2.x.261]  converges to  [2.x.262] . This then allows considering each cell's contribution as an "error indicator" that also converges to zero -- as it should as the mesh is refined. 




[1.x.98] 

The step-14 example program builds heavily on the techniques already used in the step-13 program. Its implementation of the dual weighted residual error estimator explained above is done by deriving a second class, properly called  [2.x.263]  base class, and having a class ( [2.x.264] ) that joins the two again and controls the solution of the primal and dual problem, and then uses both to compute the error indicator for mesh refinement. 

The program continues the modular concept of the previous example, by implementing the dual functional, describing quantity of interest, by an abstract base class, and providing two different functionals which implement this interface. Adding a different quantity of interest is thus simple. 

One of the more fundamental differences is the handling of data. A common case is that you develop a program that solves a certain equation, and test it with different right hand sides, different domains, different coefficients and boundary values, etc. Usually, these have to match, so that exact solutions are known, or that their combination makes sense at all. 

We demonstrate a way how this can be achieved in a simple, yet very flexible way. We will put everything that belongs to a certain setup into one class, and provide a little C++ mortar around it, so that entire setups (domains, coefficients, right hand sides, etc.) can be exchanged by only changing something in  [2.x.265] one [2.x.266]  place. 

Going this way a little further, we have also centralized all the other parameters that describe how the program is to work in one place, such as the order of the finite element, the maximal number of degrees of freedom, the evaluation objects that shall be executed on the computed solutions, and so on. This allows for simpler configuration of the program, and we will show in a later program how to use a library class that can handle setting these parameters by reading an input file. The general aim is to reduce the places within a program where one may have to look when wanting to change some parameter, as it has turned out in practice that one forgets where they are as programs grow. Furthermore, putting all options describing what the program does in a certain run into a file (that can be stored with the results) helps repeatability of results more than if the various flags were set somewhere in the program, where their exact values are forgotten after the next change to this place. 

Unfortunately, the program has become rather long. While this admittedly reduces its usefulness as an example program, we think that it is a very good starting point for development of a program for other kinds of problems, involving different equations than the Laplace equation treated here. Furthermore, it shows everything that we can show you about our way of a posteriori error estimation, and its structure should make it simple for you to adjust this method to other problems, other functionals, other geometries, coefficients, etc. 

The author believes that the present program is his masterpiece among the example programs, regarding the mathematical complexity, as well as the simplicity to add extensions. If you use this program as a basis for your own programs, we would kindly like to ask you to state this fact and the name of the author of the example program, Wolfgang Bangerth, in publications that arise from that, of your program consists in a considerable part of the example program. 


examples/step-14/doc/results.dox 



[1.x.99] 

[1.x.100] 


This program offers a lot of possibilities to play around. We can thus only show a small part of all possible results that can be obtained with the help of this program. However, you are encouraged to just try it out, by changing the settings in the main program. Here, we start by simply letting it run, unmodified: 

[1.x.101] 




First let's look what the program actually computed. On the seventh grid, primal and dual numerical solutions look like this (using a color scheme intended to evoke the snow-capped mountains of Colorado that the original author of this program now calls home):  [2.x.267]  Apparently, the region at the bottom left is so unimportant for the point value evaluation at the top right that the grid is left entirely unrefined there, even though the solution has singularities at the inner corner of that cell! Due to the symmetry in right hand side and domain, the solution should actually look like at the top right in all four corners, but the mesh refinement criterion involving the dual solution chose to refine them differently -- because we said that we really only care about a single function value somewhere at the top right. 




Here are some of the meshes that are produced in refinement cycles 0, 2, 4 (top row), and 5, 7, and 8 (bottom row): 

 [2.x.268]  

Note the subtle interplay between resolving the corner singularities, and resolving around the point of evaluation. It will be rather difficult to generate such a mesh by hand, as this would involve to judge quantitatively how much which of the four corner singularities should be resolved, and to set the weight compared to the vicinity of the evaluation point. 




The program prints the point value and the estimated error in this quantity. From extrapolating it, we can guess that the exact value is somewhere close to 0.0334473, plus or minus 0.0000001 (note that we get almost 6 valid digits from only 22,000 (primal) degrees of freedom. This number cannot be obtained from the value of the functional alone, but I have used the assumption that the error estimator is mostly exact, and extrapolated the computed value plus the estimated error, to get an approximation of the true value. Computing with more degrees of freedom shows that this assumption is indeed valid. 




From the computed results, we can generate two graphs: one that shows the convergence of the error  [2.x.269]  (taking the extrapolated value as correct) in the point value, and the value that we get by adding up computed value  [2.x.270]  and estimated error eta (if the error estimator  [2.x.271]  were exact, then the value  [2.x.272]  would equal the exact point value, and the error in this quantity would always be zero; however, since the error estimator is only a - good - approximation to the true error, we can by this only reduce the size of the error). In this graph, we also indicate the complexity  [2.x.273]  to show that mesh refinement acts optimal in this case. The second chart compares true and estimated error, and shows that the two are actually very close to each other, even for such a complicated quantity as the point value: 


 [2.x.274]  




[1.x.102] 


Since we have accepted quite some effort when using the mesh refinement driven by the dual weighted error estimator (for solving the dual problem, and for evaluating the error representation), it is worth while asking whether that effort was successful. To this end, we first compare the achieved error levels for different mesh refinement criteria. To generate this data, simply change the value of the mesh refinement criterion variable in the main program. The results are thus (for the weight in the Kelly indicator, we have chosen the function  [2.x.275] , where  [2.x.276]  is the distance to the evaluation point; it can be shown that this is the optimal weight if we neglect the effects of boundaries): 

 [2.x.277]  




Checking these numbers, we see that for global refinement, the error is proportional to  [2.x.278] , and for the dual estimator  [2.x.279] . Generally speaking, we see that the dual weighted error estimator is better than the other refinement indicators, at least when compared with those that have a similarly regular behavior. The Kelly indicator produces smaller errors, but jumps about the picture rather irregularly, with the error also changing signs sometimes. Therefore, its behavior does not allow to extrapolate the results to larger values of N. Furthermore, if we trust the error estimates of the dual weighted error estimator, the results can be improved by adding the estimated error to the computed values. In terms of reliability, the weighted estimator is thus better than the Kelly indicator, although the latter sometimes produces smaller errors. 




[1.x.103] 


Besides evaluating the values of the solution at a certain point, the program also offers the possibility to evaluate the x-derivatives at a certain point, and also to tailor mesh refinement for this. To let the program compute these quantities, simply replace the two occurrences of  [2.x.280]  in the main function by  [2.x.281] , and let the program run: 

[1.x.104] 






The solution looks roughly the same as before (the exact solution of course  [2.x.282] is [2.x.283]  the same, only the grid changed a little), but the dual solution is now different. A close-up around the point of evaluation shows this:  [2.x.284]  This time, the grids in refinement cycles 0, 5, 6, 7, 8, and 9 look like this: 

 [2.x.285]  

Note the asymmetry of the grids compared with those we obtained for the point evaluation. This is due to the fact that the domain and the primal solution may be symmetric about the diagonal, but the  [2.x.286] -derivative is not, and the latter enters the refinement criterion. 




Then, it is interesting to compare actually computed values of the quantity of interest (i.e. the x-derivative of the solution at one point) with a reference value of -0.0528223... plus or minus 0.0000005. We get this reference value by computing on finer grid after some more mesh refinements, with approximately 130,000 cells. Recall that if the error is  [2.x.287]  in the optimal case, then taking a mesh with ten times more cells gives us one additional digit in the result. 




In the left part of the following chart, you again see the convergence of the error towards this extrapolated value, while on the right you see a comparison of true and estimated error: 

 [2.x.288]  

After an initial phase where the true error changes its sign, the estimated error matches it quite well, again. Also note the dramatic improvement in the error when using the estimated error to correct the computed value of  [2.x.289] . 




[1.x.105] 


If instead of the  [2.x.290]  data set, we choose  [2.x.291]  in the main function, and choose  [2.x.292]  as the evaluation point, then we can redo the computations of the previous example program, to compare whether the results obtained with the help of the dual weighted error estimator are better than those we had previously. 




First, the meshes after 9 adaptive refinement cycles obtained with the point evaluation and derivative evaluation refinement criteria, respectively, look like this: 

 [2.x.293]  

The features of the solution can still be seen in the mesh, but since the solution is smooth, the singularities of the dual solution entirely dominate the mesh refinement criterion, and lead to strongly concentrated meshes. The solution after the seventh refinement step looks like the following: 

 [2.x.294]  

Obviously, the solution is worse at some places, but the mesh refinement process should have taken care that these places are not important for computing the point value. 





The next point is to compare the new (duality based) mesh refinement criterion with the old ones. These are the results: 

 [2.x.295]  




The results are, well, somewhat mixed. First, the Kelly indicator disqualifies itself by its unsteady behavior, changing the sign of the error several times, and with increasing errors under mesh refinement. The dual weighted error estimator has a monotone decrease in the error, and is better than the weighted Kelly and global refinement, but the margin is not as large as expected. This is, here, due to the fact the global refinement can exploit the regular structure of the meshes around the point of evaluation, which leads to a better order of convergence for the point error. However, if we had a mesh that is not locally rectangular, for example because we had to approximate curved boundaries, or if the coefficients were not constant, then this advantage of globally refinement meshes would vanish, while the good performance of the duality based estimator would remain. 







[1.x.106] 


The results here are not too clearly indicating the superiority of the dual weighted error estimation approach for mesh refinement over other mesh refinement criteria, such as the Kelly indicator. This is due to the relative simplicity of the shown applications. If you are not convinced yet that this approach is indeed superior, you are invited to browse through the literature indicated in the introduction, where plenty of examples are provided where the dual weighted approach can reduce the necessary numerical work by orders of magnitude, making this the only way to compute certain quantities to reasonable accuracies at all. 




Besides the objections you may raise against its use as a mesh refinement criterion, consider that accurate knowledge of the error in the quantity one might want to compute is of great use, since we can stop computations when we are satisfied with the accuracy. Using more traditional approaches, it is very difficult to get accurate estimates for arbitrary quantities, except for, maybe, the error in the energy norm, and we will then have no guarantee that the result we computed satisfies any requirements on its accuracy. Also, as was shown for the evaluation of point values and derivatives, the error estimate can be used to extrapolate the results, yielding much higher accuracy in the quantity we want to know. 




Leaving these mathematical considerations, we tried to write the program in a modular way, such that implementing another test case, or another evaluation and dual functional is simple. You are encouraged to take the program as a basis for your own experiments, and to play a little. 


examples/step-15/doc/intro.dox 

 [2.x.296]  

[1.x.107]  [2.x.297]  


[1.x.108] 

[1.x.109] 

[1.x.110] 

This program deals with an example of a non-linear elliptic partial differential equation, the [minimal surface equation](https://en.wikipedia.org/wiki/Minimal_surface). You can imagine the solution of this equation to describe the surface spanned by a soap film that is enclosed by a closed wire loop. We imagine the wire to not just be a planar loop, but in fact curved. The surface tension of the soap film will then reduce the surface to have minimal surface. The solution of the minimal surface equation describes this shape with the wire's vertical displacement as a boundary condition. For simplicity, we will here assume that the surface can be written as a graph  [2.x.298]  although it is clear that it is not very hard to construct cases where the wire is bent in such a way that the surface can only locally be constructed as a graph but not globally. 

Because the equation is non-linear, we can't solve it directly. Rather, we have to use Newton's method to compute the solution iteratively. 

 [2.x.299]  ( [2.x.300]  




[1.x.111] 

In a classical sense, the problem is given in the following form: 


  [1.x.112] 



 [2.x.301]  is the domain we get by projecting the wire's positions into  [2.x.302]  space. In this example, we choose  [2.x.303]  as the unit disk. 

As described above, we solve this equation using Newton's method in which we compute the  [2.x.304] th approximate solution from the  [2.x.305] th one, and use a damping parameter  [2.x.306]  to get better global convergence behavior:   [1.x.113] 

with   [1.x.114] 

and  [2.x.307]  the derivative of F in direction of  [2.x.308] : 

[1.x.115] 



Going through the motions to find out what  [2.x.309]  is, we find that we have to solve a linear elliptic PDE in every Newton step, with  [2.x.310]  as the solution of: 

  [1.x.116] 



In order to solve the minimal surface equation, we have to solve this equation repeatedly, once per Newton step. To solve this, we have to take a look at the boundary condition of this problem. Assuming that  [2.x.311]  already has the right boundary values, the Newton update  [2.x.312]  should have zero boundary conditions, in order to have the right boundary condition after adding both.  In the first Newton step, we are starting with the solution  [2.x.313] , the Newton update still has to deliver the right boundary condition to the solution  [2.x.314] . 


Summing up, we have to solve the PDE above with the boundary condition  [2.x.315]  in the first step and with  [2.x.316]  in all the following steps. 

 [2.x.317]  In some sense, one may argue that if the program already   implements  [2.x.318] , it is duplicative to also have to implement    [2.x.319] . As always, duplication tempts bugs and we would like   to avoid it. While we do not explore this issue in this program, we   will come back to it at the end of the [1.x.117] section below,   and specifically in step-72. 




[1.x.118] 

Starting with the strong formulation above, we get the weak formulation by multiplying both sides of the PDE with a test function  [2.x.320]  and integrating by parts on both sides:   [1.x.119] 

Here the solution  [2.x.321]  is a function in  [2.x.322] , subject to the boundary conditions discussed above. Reducing this space to a finite dimensional space with basis  [2.x.323] , we can write the solution: 

[1.x.120] 



Using the basis functions as test functions and defining  [2.x.324] , we can rewrite the weak formulation: 

[1.x.121] 



where the solution  [2.x.325]  is given by the coefficients  [2.x.326] . This linear system of equations can be rewritten as: 

[1.x.122] 



where the entries of the matrix  [2.x.327]  are given by: 

[1.x.123] 



and the right hand side  [2.x.328]  is given by: 

[1.x.124] 






[1.x.125] 

The matrix that corresponds to the Newton step above can be reformulated to show its structure a bit better. Rewriting it slightly, we get that it has the form 

[1.x.126] 

where the matrix  [2.x.329]  (of size  [2.x.330]  in  [2.x.331]  space dimensions) is given by the following expression: 

[1.x.127] 

From this expression, it is obvious that  [2.x.332]  is symmetric, and so  [2.x.333]  is symmetric as well. On the other hand,  [2.x.334]  is also positive definite, which confers the same property onto  [2.x.335] . This can be seen by noting that the vector  [2.x.336]  is an eigenvector of  [2.x.337]  with eigenvalue  [2.x.338]  while all vectors  [2.x.339]  that are perpendicular to  [2.x.340]  and each other are eigenvectors with eigenvalue  [2.x.341] . Since all eigenvalues are positive,  [2.x.342]  is positive definite and so is  [2.x.343] . We can thus use the CG method for solving the Newton steps. (The fact that the matrix  [2.x.344]  is symmetric and positive definite should not come as a surprise. It results from taking the derivative of an operator that results from taking the derivative of an energy functional: the minimal surface equation simply minimizes some non-quadratic energy. Consequently, the Newton matrix, as the matrix of second derivatives of a scalar energy, must be symmetric since the derivative with regard to the  [2.x.345] th and  [2.x.346] th degree of freedom should clearly commute. Likewise, if the energy functional is convex, then the matrix of second derivatives must be positive definite, and the direct calculation above simply reaffirms this.) 

It is worth noting, however, that the positive definiteness degenerates for problems where  [2.x.347]  becomes large. In other words, if we simply multiply all boundary values by 2, then to first order  [2.x.348]  and  [2.x.349]  will also be multiplied by two, but as a consequence the smallest eigenvalue of  [2.x.350]  will become smaller and the matrix will become more ill-conditioned. (More specifically, for  [2.x.351]  we have that  [2.x.352]  whereas  [2.x.353] ; thus, the condition number of  [2.x.354] , which is a multiplicative factor in the condition number of  [2.x.355]  grows like  [2.x.356] .) It is simple to verify with the current program that indeed multiplying the boundary values used in the current program by larger and larger values results in a problem that will ultimately no longer be solvable using the simple preconditioned CG method we use here. 




[1.x.128] 

As stated above, Newton's method works by computing a direction  [2.x.357]  and then performing the update  [2.x.358]  with a step length  [2.x.359] . It is a common observation that for strongly nonlinear models, Newton's method does not converge if we always choose  [2.x.360]  unless one starts with an initial guess  [2.x.361]  that is sufficiently close to the solution  [2.x.362]  of the nonlinear problem. In practice, we don't always have such an initial guess, and consequently taking full Newton steps (i.e., using  [2.x.363] ) does frequently not work. 

A common strategy therefore is to use a smaller step length for the first few steps while the iterate  [2.x.364]  is still far away from the solution  [2.x.365]  and as we get closer use larger values for  [2.x.366]  until we can finally start to use full steps  [2.x.367]  as we are close enough to the solution. The question is of course how to choose  [2.x.368] . There are basically two widely used approaches: line search and trust region methods. 

In this program, we simply always choose the step length equal to 0.1. This makes sure that for the testcase at hand we do get convergence although it is clear that by not eventually reverting to full step lengths we forego the rapid, quadratic convergence that makes Newton's method so appealing. Obviously, this is a point one eventually has to address if the program was made into one that is meant to solve more realistic problems. We will comment on this issue some more in the [1.x.129], and use an even better approach in step-77. 




[1.x.130] 

Overall, the program we have here is not unlike step-6 in many regards. The layout of the main class is essentially the same. On the other hand, the driving algorithm in the  [2.x.369]  function is different and works as follows: <ol>  [2.x.370]    Start with the function  [2.x.371]  and modify it in such a way   that the values of  [2.x.372]  along the boundary equal the correct   boundary values  [2.x.373]  (this happens in    [2.x.374] ). Set    [2.x.375] .  [2.x.376]  

 [2.x.377]    Compute the Newton update by solving the system  [2.x.378]    with boundary condition  [2.x.379]  on  [2.x.380] .  [2.x.381]  

 [2.x.382]    Compute a step length  [2.x.383] . In this program, we always set    [2.x.384] . To make things easier to extend later on, this   happens in a function of its own, namely in    [2.x.385] .   (The strategy of always choosing  [2.x.386]  is of course not   optimal -- we should choose a step length that works for a given   search direction -- but it requires a bit of work to do that. In the   end, we leave these sorts of things to external packages: step-77   does that.)  [2.x.387]  

 [2.x.388]    The new approximation of the solution is given by    [2.x.389] .  [2.x.390]  

 [2.x.391]    If  [2.x.392]  is a multiple of 5 then refine the mesh, transfer the   solution  [2.x.393]  to the new mesh and set the values of  [2.x.394]    in such a way that along the boundary we have    [2.x.395]  (again in    [2.x.396] ). Note that   this isn't automatically   guaranteed even though by construction we had that before mesh   refinement  [2.x.397]  because mesh refinement   adds new nodes to the mesh where we have to interpolate the old   solution to the new nodes upon bringing the solution from the old to   the new mesh. The values we choose by interpolation may be close to   the exact boundary conditions but are, in general, nonetheless not   the correct values.  [2.x.398]  

 [2.x.399]    Set  [2.x.400]  and go to step 2.  [2.x.401]   [2.x.402]  

The testcase we solve is chosen as follows: We seek to find the solution of minimal surface over the unit disk  [2.x.403]  where the surface attains the values  [2.x.404]  along the boundary. 


examples/step-15/doc/results.dox 



[1.x.131] 


The output of the program looks as follows: 

[1.x.132] 



Obviously, the scheme converges, if not very fast. We will come back to strategies for accelerating the method below. 

One can visualize the solution after each set of five Newton iterations, i.e., on each of the meshes on which we approximate the solution. This yields the following set of images: 

<div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step_15_solution_1.png"          alt="Solution after zero cycles with contour lines." width="230" height="273">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_15_solution_2.png"          alt="Solution after one cycle with contour lines." width="230" height="273">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_15_solution_3.png"          alt="Solution after two cycles with contour lines." width="230" height="273">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_15_solution_4.png"          alt="Solution after three cycles with contour lines." width="230" height="273">   </div> </div> 

It is clearly visible, that the solution minimizes the surface after each refinement. The solution converges to a picture one would imagine a soap bubble to be that is located inside a wire loop that is bent like the boundary. Also it is visible, how the boundary is smoothed out after each refinement. On the coarse mesh, the boundary doesn't look like a sine, whereas it does the finer the mesh gets. 

The mesh is mostly refined near the boundary, where the solution increases or decreases strongly, whereas it is coarsened on the inside of the domain, where nothing interesting happens, because there isn't much change in the solution. The ninth solution and mesh are shown here: 

<div class="onecolumn" style="width: 60%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step_15_solution_9.png"          alt="Grid and solution of the ninth cycle with contour lines." width="507" height="507">   </div> </div> 




[1.x.133] 

[1.x.134] 

The program shows the basic structure of a solver for a nonlinear, stationary problem. However, it does not converge particularly fast, for good reasons: 

- The program always takes a step size of 0.1. This precludes the rapid,   quadratic convergence for which Newton's method is typically chosen. 

- It does not connect the nonlinear iteration with the mesh refinement   iteration. 

Obviously, a better program would have to address these two points. We will discuss them in the following. 




[1.x.135] 

Newton's method has two well known properties: 

- It may not converge from arbitrarily chosen starting points. Rather, a   starting point has to be close enough to the solution to guarantee   convergence. However, we can enlarge the area from which Newton's method   converges by damping the iteration using a [1.x.136] 0< [2.x.405] . 

- It exhibits rapid convergence of quadratic order if (i) the step length is   chosen as  [2.x.406] , and (ii) it does in fact converge with this choice   of step length. 

A consequence of these two observations is that a successful strategy is to choose  [2.x.407]  for the initial iterations until the iterate has come close enough to allow for convergence with full step length, at which point we want to switch to  [2.x.408] . The question is how to choose  [2.x.409]  in an automatic fashion that satisfies these criteria. 

We do not want to review the literature on this topic here, but only briefly mention that there are two fundamental approaches to the problem: backtracking line search and trust region methods. The former is more widely used for partial differential equations and essentially does the following: 

- Compute a search direction 

- See if the resulting residual of  [2.x.410]  with    [2.x.411]  is "substantially smaller" than that of  [2.x.412]  alone. 

- If so, then take  [2.x.413] . 

- If not, try whether the residual is "substantially smaller" with    [2.x.414] . 

- If so, then take  [2.x.415] . 

- If not, try whether the residual is "substantially smaller" with    [2.x.416] . 

- Etc. One can of course choose other factors  [2.x.417]  than the  [2.x.418]  chosen above, for  [2.x.419] . It is obvious where the term "backtracking" comes from: we try a long step, but if that doesn't work we try a shorter step, and ever shorter step, etc. The function  [2.x.420]  is written the way it is to support exactly this kind of use case. 

Whether we accept a particular step length  [2.x.421]  depends on how we define "substantially smaller". There are a number of ways to do so, but without going into detail let us just mention that the most common ones are to use the Wolfe and Armijo-Goldstein conditions. For these, one can show the following: 

- There is always a step length  [2.x.422]  for which the conditions are   satisfied, i.e., the iteration never gets stuck as long as the problem is   convex. 

- If we are close enough to the solution, then the conditions allow for    [2.x.423] , thereby enabling quadratic convergence. 

We will not dwell on this here any further but leave the implementation of such algorithms as an exercise. We note, however, that when implemented correctly then it is a common observation that most reasonably nonlinear problems can be solved in anywhere between 5 and 15 Newton iterations to engineering accuracy &mdash; substantially fewer than we need with the current version of the program. 

More details on globalization methods including backtracking can be found, for example, in  [2.x.424]  and  [2.x.425] . 

A separate point, very much worthwhile making, however, is that in practice the implementation of efficient nonlinear solvers is about as complicated as the implementation of efficient finite element methods. One should not attempt to reinvent the wheel by implementing all of the necessary steps oneself. Substantial pieces of the puzzle are already available in the  [2.x.426]  function and could be used to this end. But, instead, just like building finite element solvers on libraries such as deal.II, one should be building nonlinear solvers on libraries such as [SUNDIALS](https://computing.llnl.gov/projects/sundials). In fact, deal.II has interfaces to SUNDIALS and in particular to its nonlinear solver sub-package KINSOL through the  [2.x.427]  class. It would not be very difficult to base the current problem on that interface -- indeed, that is what step-77 does. 




[1.x.137] 

We currently do exactly 5 iterations on each mesh. But is this optimal? One could ask the following questions: 

- Maybe it is worthwhile doing more iterations on the initial meshes since   there, computations are cheap. 

- On the other hand, we do not want to do too many iterations on every mesh:   yes, we could drive the residual to zero on every mesh, but that would only   mean that the nonlinear iteration error is far smaller than the   discretization error. 

- Should we use solve the linear systems in each Newton step with higher or   lower accuracy? 

Ultimately, what this boils down to is that we somehow need to couple the discretization error on the current mesh with the nonlinear residual we want to achieve with the Newton iterations on a given mesh, and to the linear iteration we want to achieve with the CG method within each Newton iterations. 

How to do this is, again, not entirely trivial, and we again leave it as a future exercise. 




[1.x.138] 

As outlined in the introduction, when solving a nonlinear problem of the form   [1.x.139] 

we use a Newton iteration that requires us to repeatedly solve the linear partial differential equation   [1.x.140] 

so that we can compute the update   [1.x.141] 

with the solution  [2.x.428]  of the Newton step. For the problem here, we could compute the derivative  [2.x.429]  by hand and obtained   [1.x.142] 

But this is already a sizable expression that is cumbersome both to derive and to implement. It is also, in some sense, duplicative: If we implement what  [2.x.430]  is somewhere in the code, then  [2.x.431]  is not an independent piece of information but is something that, at least in principle, a computer should be able to infer itself. Wouldn't it be nice if that could actually happen? That is, if we really only had to implement  [2.x.432] , and  [2.x.433]  was then somehow done implicitly? That is in fact possible, and runs under the name "automatic differentiation". step-71 discusses this very concept in general terms, and step-72 illustrates how this can be applied in practice for the very problem we are considering here. 


examples/step-16/doc/intro.dox 

 [2.x.434]  

[1.x.143] 

[1.x.144] 

[1.x.145] 


This example shows the basic usage of the multilevel functions in deal.II. It solves almost the same problem as used in step-6, but demonstrating the things one has to provide when using multigrid as a preconditioner. In particular, this requires that we define a hierarchy of levels, provide transfer operators from one level to the next and back, and provide representations of the Laplace operator on each level. 

In order to allow sufficient flexibility in conjunction with systems of differential equations and block preconditioners, quite a few different objects have to be created before starting the multilevel method, although most of what needs to be done is provided by deal.II itself. These are 

  - the object handling transfer between grids; we use the MGTransferPrebuilt     class for this that does almost all of the work inside the library, 

  - the solver on the coarsest level; here, we use MGCoarseGridHouseholder, 

  - the smoother on all other levels, which in our case will be the      [2.x.435]  class using SOR as the underlying method, 

  - and  [2.x.436]  a class having a special level multiplication, i.e. we     basically store one matrix per grid level and allow multiplication with it. 

Most of these objects will only be needed inside the function that actually solves the linear system. There, these objects are combined in an object of type Multigrid, containing the implementation of the V-cycle, which is in turn used by the preconditioner PreconditionMG, ready for plug-in into a linear solver of the LAC library. 

The multigrid method implemented here for adaptively refined meshes follows the outline in the  [2.x.437]  "Multigrid paper", which describes the underlying implementation in deal.II and also introduces a lot of the nomenclature. First, we have to distinguish between level meshes, namely cells that have the same refinement distance from the coarse mesh, and the leaf mesh consisting of active cells of the hierarchy (in older work we refer to this as the global mesh, but this term is overused). Most importantly, the leaf mesh is not identical with the level mesh on the finest level. The following image shows what we consider to be a "level mesh": 

<p align="center">    [2.x.438]   [2.x.439]  

The fine level in this mesh consists only of the degrees of freedom that are defined on the refined cells, but does not extend to that part of the domain that is not refined. While this guarantees that the overall effort grows as  [2.x.440]  as necessary for optimal multigrid complexity, it leads to problems when defining where to smooth and what boundary conditions to pose for the operators defined on individual levels if the level boundary is not an external boundary. These questions are discussed in detail in the article cited above. 

[1.x.146] 

The problem we solve here is similar to step-6, with two main differences: first, the multigrid preconditioner, obviously. We also change the discontinuity of the coefficients such that the local assembler does not look more complicated than necessary. 


examples/step-16/doc/results.dox 



[1.x.147] 

On the finest mesh, the solution looks like this: 

<p align="center">    [2.x.441]   [2.x.442]  

More importantly, we would like to see if the multigrid method really improved the solver performance. Therefore, here is the textual output: 

<pre> Cycle 0    Number of active cells:       80    Number of degrees of freedom: 89 (by level: 8, 25, 89)    Number of CG iterations: 8 

Cycle 1    Number of active cells:       158    Number of degrees of freedom: 183 (by level: 8, 25, 89, 138)    Number of CG iterations: 9 

Cycle 2    Number of active cells:       302    Number of degrees of freedom: 352 (by level: 8, 25, 89, 223, 160)    Number of CG iterations: 10 

Cycle 3    Number of active cells:       578    Number of degrees of freedom: 649 (by level: 8, 25, 89, 231, 494, 66)    Number of CG iterations: 10 

Cycle 4    Number of active cells:       1100    Number of degrees of freedom: 1218 (by level: 8, 25, 89, 274, 764, 417, 126)    Number of CG iterations: 10 

Cycle 5    Number of active cells:       2096    Number of degrees of freedom: 2317 (by level: 8, 25, 89, 304, 779, 1214, 817)    Number of CG iterations: 11 

Cycle 6    Number of active cells:       3986    Number of degrees of freedom: 4366 (by level: 8, 25, 89, 337, 836, 2270, 897, 1617)    Number of CG iterations: 10 

Cycle 7    Number of active cells:       7574    Number of degrees of freedom: 8350 (by level: 8, 25, 89, 337, 1086, 2835, 2268, 1789, 3217)    Number of CG iterations: 11 </pre> 

That's almost perfect multigrid performance: the linear residual gets reduced by 12 orders of magnitude in 10 iteration steps, and the results are almost independent of the mesh size. That's obviously in part due to the simple nature of the problem solved, but it shows the power of multigrid methods. 




[1.x.148] 


We encourage you to generate timings for the solve() call and compare to step-6. You will see that the multigrid method has quite an overhead on coarse meshes, but that it always beats other methods on fine meshes because of its optimal complexity. 

A close inspection of this program's performance shows that it is mostly dominated by matrix-vector operations. step-37 shows one way how this can be avoided by working with matrix-free methods. 

Another avenue would be to use algebraic multigrid methods. The geometric multigrid method used here can at times be a bit awkward to implement because it needs all those additional data structures, and it becomes even more difficult if the program is to run in %parallel on machines coupled through MPI, for example. In that case, it would be simpler if one could use a black-box preconditioner that uses some sort of multigrid hierarchy for good performance but can figure out level matrices and similar things by itself. Algebraic multigrid methods do exactly this, and we will use them in step-31 for the solution of a Stokes problem and in step-32 and step-40 for a parallel variation. That said, a parallel version of this example program with MPI can be found in step-50. 

Finally, one may want to think how to use geometric multigrid for other kinds of problems, specifically  [2.x.443]  "vector valued problems". This is the topic of step-56 where we use the techniques shown here for the Stokes equation. 


examples/step-16b/doc/intro.dox 

 [2.x.444]  

[1.x.149] 

[1.x.150] 

This is a variant of step-16 with the only change that we are using the MeshWorker framework with the pre-made LocalIntegrator helper classes instead of manually assembling the matrices. 

The details of this framework on how it is used in practice will be explained as part of this tutorial program. 

[1.x.151] 

The problem we solve here is the same as the one in step-16. 


examples/step-16b/doc/results.dox 



[1.x.152] 

As in step-16, the solution looks like this on the finest mesh: 

<p align="center">    [2.x.445]   [2.x.446]  

The output is formatted in a slightly different way compared to step-16 but is functionally the same and shows the same convergence properties: <pre>  [2.x.447]  0 DEAL::   Number of active cells:       20 DEAL::   Number of degrees of freedom: 25 (by level: 8, 25)  [2.x.448]  value 0.510691  [2.x.449]  step 6 value 4.59193e-14  [2.x.450]  1 DEAL::   Number of active cells:       44 DEAL::   Number of degrees of freedom: 55 (by level: 8, 25, 45)  [2.x.451]  value 0.440678  [2.x.452]  step 8 value 1.99419e-13  [2.x.453]  2 DEAL::   Number of active cells:       86 DEAL::   Number of degrees of freedom: 105 (by level: 8, 25, 69, 49)  [2.x.454]  value 0.371855  [2.x.455]  step 9 value 1.13984e-13  [2.x.456]  3 DEAL::   Number of active cells:       170 DEAL::   Number of degrees of freedom: 200 (by level: 8, 25, 77, 174)  [2.x.457]  value 0.318967  [2.x.458]  step 9 value 2.62112e-13  [2.x.459]  4 DEAL::   Number of active cells:       332 DEAL::   Number of degrees of freedom: 388 (by level: 8, 25, 86, 231, 204)  [2.x.460]  value 0.276534  [2.x.461]  step 10 value 1.69562e-13  [2.x.462]  5 DEAL::   Number of active cells:       632 DEAL::   Number of degrees of freedom: 714 (by level: 8, 25, 89, 231, 514, 141)  [2.x.463]  value 0.215300  [2.x.464]  step 10 value 6.47463e-13  [2.x.465]  6 DEAL::   Number of active cells:       1202 DEAL::   Number of degrees of freedom: 1332 (by level: 8, 25, 89, 282, 771, 435, 257)  [2.x.466]  value 0.175848  [2.x.467]  step 10 value 1.80664e-13  [2.x.468]  7 DEAL::   Number of active cells:       2288 DEAL::   Number of degrees of freedom: 2511 (by level: 8, 25, 89, 318, 779, 1420, 829, 30)  [2.x.469]  value 0.136724  [2.x.470]  step 11 value 9.73331e-14 </pre> 


examples/step-17/doc/intro.dox 

[1.x.153] 

[1.x.154] 

[1.x.155] 

This program does not introduce any new mathematical ideas; in fact, all it does is to do the exact same computations that step-8 already does, but it does so in a different manner: instead of using deal.II's own linear algebra classes, we build everything on top of classes deal.II provides that wrap around the linear algebra implementation of the [1.x.156] library. And since PETSc allows to distribute matrices and vectors across several computers within an MPI network, the resulting code will even be able to solve the problem in %parallel. If you don't know what PETSc is, then this would be a good time to take a quick glimpse at their homepage. 

As a prerequisite of this program, you need to have PETSc installed, and if you want to run in %parallel on a cluster, you also need [1.x.157] to partition meshes. The installation of deal.II together with these two additional libraries is described in the [1.x.158] file. 

Now, for the details: as mentioned, the program does not compute anything new, so the use of finite element classes, etc., is exactly the same as before. The difference to previous programs is that we have replaced almost all uses of classes  [2.x.471]  by their near-equivalents  [2.x.472]  and  [2.x.473]  that store data in a way so that every processor in the MPI network only stores a part of the matrix or vector. More specifically, each processor will only store those rows of the matrix that correspond to a degree of freedom it "owns". For vectors, they either store only elements that correspond to degrees of freedom the processor owns (this is what is necessary for the right hand side), or also some additional elements that make sure that every processor has access the solution components that live on the cells the processor owns (so-called  [2.x.474]  "locally active DoFs") or also on neighboring cells (so-called  [2.x.475]  "locally relevant DoFs"). 

The interface the classes from the PETScWrapper namespace provide is very similar to that of the deal.II linear algebra classes, but instead of implementing this functionality themselves, they simply pass on to their corresponding PETSc functions. The wrappers are therefore only used to give PETSc a more modern, object oriented interface, and to make the use of PETSc and deal.II objects as interchangeable as possible. The main point of using PETSc is that it can run in %parallel. We will make use of this by partitioning the domain into as many blocks ("subdomains") as there are processes in the MPI network. At the same time, PETSc also provides dummy MPI stubs, so you can run this program on a single machine if PETSc was configured without MPI. 




[1.x.159] 

Developing software to run in %parallel via MPI requires a bit of a change in mindset because one typically has to split up all data structures so that every processor only stores a piece of the entire problem. As a consequence, you can't typically access all components of a solution vector on each processor -- each processor may simply not have enough memory to hold the entire solution vector. Because data is split up or "distributed" across processors, we call the programming model used by MPI "distributed memory computing" (as opposed to "shared memory computing", which would mean that multiple processors can all access all data within one memory space, for example whenever multiple cores in a single machine work on a common task). Some of the fundamentals of distributed memory computing are discussed in the  [2.x.476]  "Parallel computing with multiple processors using distributed memory" documentation module, which is itself a sub-module of the  [2.x.477]  "Parallel computing" module. 

In general, to be truly able to scale to large numbers of processors, one needs to split between the available processors [1.x.160] data structure whose size scales with the size of the overall problem. (For a definition of what it means for a program to "scale", see  [2.x.478]  "this glossary entry".) This includes, for example, the triangulation, the matrix, and all global vectors (solution, right hand side). If one doesn't split all of these objects, one of those will be replicated on all processors and will eventually simply become too large if the problem size (and the number of available processors) becomes large. (On the other hand, it is completely fine to keep objects with a size that is independent of the overall problem size on every processor. For example, each copy of the executable will create its own finite element object, or the local matrix we use in the assembly.) 

In the current program (as well as in the related step-18), we will not go quite this far but present a gentler introduction to using MPI. More specifically, the only data structures we will parallelize are matrices and vectors. We do, however, not split up the Triangulation and DoFHandler classes: each process still has a complete copy of these objects, and all processes have exact copies of what the other processes have. We will then simply have to mark, in each copy of the triangulation on each of the processors, which processor owns which cells. This process is called "partitioning" a mesh into  [2.x.479]  "subdomains". 

For larger problems, having to store the [1.x.161] mesh on every processor will clearly yield a bottleneck. Splitting up the mesh is slightly, though not much more complicated (from a user perspective, though it is [1.x.162] more complicated under the hood) to achieve and we will show how to do this in step-40 and some other programs. There are numerous occasions where, in the course of discussing how a function of this program works, we will comment on the fact that it will not scale to large problems and why not. All of these issues will be addressed in step-18 and in particular step-40, which scales to very large numbers of processes. 

Philosophically, the way MPI operates is as follows. You typically run a program via 

[1.x.163] 

which means to run it on (say) 32 processors. (If you are on a cluster system, you typically need to [1.x.164] the program to run whenever 32 processors become available; this will be described in the documentation of your cluster. But under the hood, whenever those processors become available, the same call as above will generally be executed.) What this does is that the MPI system will start 32 [1.x.165] of the  [2.x.480]  executable. (The MPI term for each of these running executables is that you have 32  [2.x.481]  "MPI processes".) This may happen on different machines that can't even read from each others' memory spaces, or it may happen on the same machine, but the end result is the same: each of these 32 copies will run with some memory allocated to it by the operating system, and it will not directly be able to read the memory of the other 31 copies. In order to collaborate in a common task, these 32 copies then have to [1.x.166] with each other. MPI, short for [1.x.167], makes this possible by allowing programs to [1.x.168]. You can think of this as the mail service: you can put a letter to a specific address into the mail and it will be delivered. But that's the extent to which you can control things. If you want the receiver to do something with the content of the letter, for example return to you data you want from over there, then two things need to happen: (i) the receiver needs to actually go check whether there is anything in their mailbox, and (ii) if there is, react appropriately, for example by sending data back. If you wait for this return message but the original receiver was distracted and not paying attention, then you're out of luck: you'll simply have to wait until your requested over there will be worked on. In some cases, bugs will lead the original receiver to never check your mail, and in that case you will wait forever -- this is called a [1.x.169]. ( [2.x.482]  

In practice, one does not usually program at the level of sending and receiving individual messages, but uses higher level operations. For example, in the program we will use function calls that take a number from each processor, add them all up, and return the sum to all processors. Internally, this is implemented using individual messages, but to the user this is transparent. We call such operations [1.x.170] because [1.x.171] processors participate in them. Collectives allow us to write programs where not every copy of the executable is doing something completely different (this would be incredibly difficult to program) but where in essence all copies are doing the same thing (though on different data) for themselves, running through the same blocks of code; then they communicate data through collectives; and then go back to doing something for themselves again running through the same blocks of data. This is the key piece to being able to write programs, and it is the key component to making sure that programs can run on any number of processors, since we do not have to write different code for each of the participating processors. 

(This is not to say that programs are never written in ways where different processors run through different blocks of code in their copy of the executable. Programs internally also often communicate in other ways than through collectives. But in practice, %parallel finite element codes almost always follow the scheme where every copy of the program runs through the same blocks of code at the same time, interspersed by phases where all processors communicate with each other.) 

In reality, even the level of calling MPI collective functions is too low. Rather, the program below will not contain any direct calls to MPI at all, but only deal.II functions that hide this communication from users of the deal.II. This has the advantage that you don't have to learn the details of MPI and its rather intricate function calls. That said, you do have to understand the general philosophy behind MPI as outlined above. 




[1.x.172] 

The techniques this program then demonstrates are: 

- How to use the PETSc wrapper classes; this will already be visible in the   declaration of the principal class of this program,  [2.x.483] . 

- How to partition the mesh into subdomains; this happens in the    [2.x.484]  function. 

- How to parallelize operations for jobs running on an MPI network; here, this   is something one has to pay attention to in a number of places, most   notably in the   [2.x.485]  function. 

- How to deal with vectors that store only a subset of vector entries   and for which we have to ensure that they store what we need on the   current processors. See for example the    [2.x.486]    functions. 

- How to deal with status output from programs that run on multiple   processors at the same time. This is done via the  [2.x.487]    variable in the program, initialized in the constructor. 

Since all this can only be demonstrated using actual code, let us go straight to the code without much further ado. 


examples/step-17/doc/results.dox 



[1.x.173] 


If the program above is compiled and run on a single processor machine, it should generate results that are very similar to those that we already got with step-8. However, it becomes more interesting if we run it on a multicore machine or a cluster of computers. The most basic way to run MPI programs is using a command line like 

[1.x.174] 

to run the step-17 executable with 32 processors. 

(If you work on a cluster, then there is typically a step in between where you need to set up a job script and submit the script to a scheduler. The scheduler will execute the script whenever it can allocate 32 unused processors for your job. How to write such job scripts differs from cluster to cluster, and you should find the documentation of your cluster to see how to do this. On my system, I have to use the command  [2.x.488]  with a whole host of options to run a job in parallel.) 

Whether directly or through a scheduler, if you run this program on 8 processors, you should get output like the following: 

[1.x.175] 

(This run uses a few more refinement cycles than the code available in the examples/ directory. The run also used a version of METIS from 2004 that generated different partitionings; consequently, the numbers you get today are slightly different.) 

As can be seen, we can easily get to almost four million unknowns. In fact, the code's runtime with 8 processes was less than 7 minutes up to (and including) cycle 14, and 14 minutes including the second to last step. (These are numbers relevant to when the code was initially written, in 2004.) I lost the timing information for the last step, though, but you get the idea. All this is after release mode has been enabled by running  [2.x.489] , and with the generation of graphical output switched off for the reasons stated in the program comments above. ( [2.x.490]  The biggest 2d computations I did had roughly 7.1 million unknowns, and were done on 32 processes. It took about 40 minutes. Not surprisingly, the limiting factor for how far one can go is how much memory one has, since every process has to hold the entire mesh and DoFHandler objects, although matrices and vectors are split up. For the 7.1M computation, the memory consumption was about 600 bytes per unknown, which is not bad, but one has to consider that this is for every unknown, whether we store the matrix and vector entries locally or not. 




Here is some output generated in the 12th cycle of the program, i.e. with roughly 300,000 unknowns: 

 [2.x.491]  

As one would hope for, the x- (left) and y-displacements (right) shown here closely match what we already saw in step-8. As shown there and in step-22, we could as well have produced a vector plot of the displacement field, rather than plotting it as two separate scalar fields. What may be more interesting, though, is to look at the mesh and partition at this step: 

 [2.x.492]  

Again, the mesh (left) shows the same refinement pattern as seen previously. The right panel shows the partitioning of the domain across the 8 processes, each indicated by a different color. The picture shows that the subdomains are smaller where mesh cells are small, a fact that needs to be expected given that the partitioning algorithm tries to equilibrate the number of cells in each subdomain; this equilibration is also easily identified in the output shown above, where the number of degrees per subdomain is roughly the same. 




It is worth noting that if we ran the same program with a different number of processes, that we would likely get slightly different output: a different mesh, different number of unknowns and iterations to convergence. The reason for this is that while the matrix and right hand side are the same independent of the number of processes used, the preconditioner is not: it performs an ILU(0) on the chunk of the matrix of  [2.x.493] each processor separately [2.x.494] . Thus, it's effectiveness as a preconditioner diminishes as the number of processes increases, which makes the number of iterations increase. Since a different preconditioner leads to slight changes in the computed solution, this will then lead to slightly different mesh cells tagged for refinement, and larger differences in subsequent steps. The solution will always look very similar, though. 




Finally, here are some results for a 3d simulation. You can repeat these by changing 

[1.x.176] 

to 

[1.x.177] 

in the main function. If you then run the program in parallel, you get something similar to this (this is for a job with 16 processes): 

[1.x.178] 






The last step, going up to 1.5 million unknowns, takes about 55 minutes with 16 processes on 8 dual-processor machines (of the kind available in 2003). The graphical output generated by this job is rather large (cycle 5 already prints around 82 MB of data), so we contend ourselves with showing output from cycle 4: 

 [2.x.495]  




The left picture shows the partitioning of the cube into 16 processes, whereas the right one shows the x-displacement along two cutplanes through the cube. 




[1.x.179] 

[1.x.180] 

The program keeps a complete copy of the Triangulation and DoFHandler objects on every processor. It also creates complete copies of the solution vector, and it creates output on only one processor. All of this is obviously the bottleneck as far as parallelization is concerned. 

Internally, within deal.II, parallelizing the data structures used in hierarchic and unstructured triangulations is a hard problem, and it took us a few more years to make this happen. The step-40 tutorial program and the  [2.x.496]  documentation module talk about how to do these steps and what it takes from an application perspective. An obvious extension of the current program would be to use this functionality to completely distribute computations to many more processors than used here. 


examples/step-18/doc/intro.dox 

[1.x.181] 

[1.x.182] 


This tutorial program is another one in the series on the elasticity problem that we have already started with step-8 and step-17. It extends it into two different directions: first, it solves the quasistatic but time dependent elasticity problem for large deformations with a Lagrangian mesh movement approach. Secondly, it shows some more techniques for solving such problems using %parallel processing with PETSc's linear algebra. In addition to this, we show how to work around one of the two major bottlenecks of step-17, namely that we generated graphical output from only one process, and that this scaled very badly with larger numbers of processes and on large problems. (The other bottleneck, namely that every processor has to hold the entire mesh and DoFHandler, is addressed in step-40.) Finally, a good number of assorted improvements and techniques are demonstrated that have not been shown yet in previous programs. 

As before in step-17, the program runs just as fine on a single sequential machine as long as you have PETSc installed. Information on how to tell deal.II about a PETSc installation on your system can be found in the deal.II README file, which is linked to from the [1.x.183] in your installation of deal.II, or on [1.x.184]. 




[1.x.185] 

[1.x.186] 

In general, time-dependent small elastic deformations are described by the elastic wave equation 

[1.x.187] 

where  [2.x.497]  is the deformation of the body,  [2.x.498]  and  [2.x.499]  the density and attenuation coefficient, and  [2.x.500]  external forces. In addition, initial conditions 

[1.x.188] 

and Dirichlet (displacement) or Neumann (traction) boundary conditions need to be specified for a unique solution: 

[1.x.189] 

In above formulation,  [2.x.501]  is the symmetric gradient of the displacement, also called the  [2.x.502] strain [2.x.503] .  [2.x.504]  is a tensor of rank 4, called the  [2.x.505] stress-strain   tensor [2.x.506]  (the inverse of the [1.x.190]) that contains knowledge of the elastic strength of the material; its symmetry properties make sure that it maps symmetric tensors of rank 2 (&ldquo;matrices&rdquo; of dimension  [2.x.507] , where  [2.x.508]  is the spatial dimensionality) onto symmetric tensors of the same rank. We will comment on the roles of the strain and stress tensors more below. For the moment it suffices to say that we interpret the term  [2.x.509]  as the vector with components  [2.x.510] , where summation over indices  [2.x.511]  is implied. 

The quasistatic limit of this equation is motivated as follows: each small perturbation of the body, for example by changes in boundary condition or the forcing function, will result in a corresponding change in the configuration of the body. In general, this will be in the form of waves radiating away from the location of the disturbance. Due to the presence of the damping term, these waves will be attenuated on a time scale of, say,  [2.x.512] . Now, assume that all changes in external forcing happen on times scales that are much larger than  [2.x.513] . In that case, the dynamic nature of the change is unimportant: we can consider the body to always be in static equilibrium, i.e. we can assume that at all times the body satisfies 

[1.x.191] 

Note that the differential equation does not contain any time derivatives any more -- all time dependence is introduced through boundary conditions and a possibly time-varying force function  [2.x.514] . The changes in configuration can therefore be considered as being stationary instantaneously. An alternative view of this is that  [2.x.515]  is not really a time variable, but only a time-like parameter that governs the evolution of the problem. 

While these equations are sufficient to describe small deformations, computing large deformations is a little more complicated and, in general, leads to nonlinear equations such as those treated in step-44. In the following, let us consider some of the tools one would employ when simulating problems in which the deformation becomes [1.x.192]. 

 [2.x.516]  The model we will consider below is not founded on anything that would be mathematically sound: we will consider a model in which we produce a small deformation, deform the physical coordinates of the body by this deformation, and then consider the next loading step again as a linear problem. This isn't consistent, since the assumption of linearity implies that deformations are infinitesimal and so moving around the vertices of our mesh by a finite amount before solving the next linear problem is an inconsistent approach. We should therefore note that it is not surprising that the equations discussed below can't be found in the literature: [1.x.193] On the other hand, the implementation techniques we consider are very much what one would need to use when implementing a [1.x.194] model, as we will see in step-44. 


To come back to defining our "artificial" model, let us first introduce a tensorial stress variable  [2.x.517] , and write the differential equations in terms of the stress: 

[1.x.195] 

Note that these equations are posed on a domain  [2.x.518]  that changes with time, with the boundary moving according to the displacements  [2.x.519]  of the points on the boundary. To complete this system, we have to specify the incremental relationship between the stress and the strain, as follows: [1.x.196] 

[1.x.197] 

where a dot indicates a time derivative. Both the stress  [2.x.520]  and the strain  [2.x.521]  are symmetric tensors of rank 2. 




[1.x.198] 

Numerically, this system is solved as follows: first, we discretize the time component using a backward Euler scheme. This leads to a discrete equilibrium of force at time step  [2.x.522] : 

[1.x.199] 

where 

[1.x.200] 

and  [2.x.523]  the incremental displacement for time step  [2.x.524] . In addition, we have to specify initial data  [2.x.525] . This way, if we want to solve for the displacement increment, we have to solve the following system: 

[1.x.201] 

The weak form of this set of equations, which as usual is the basis for the finite element formulation, reads as follows: find  [2.x.526]  such that [1.x.202] 

[1.x.203] 

Using that  [2.x.527] , these equations can be simplified to 

[1.x.204] 



We note that, for simplicity, in the program we will always assume that there are no boundary forces, i.e.  [2.x.528] , and that the deformation of the body is driven by body forces  [2.x.529]  and prescribed boundary displacements  [2.x.530]  alone. It is also worth noting that when integrating by parts, we would get terms of the form  [2.x.531] , but that we replace them with the term involving the symmetric gradient  [2.x.532]  instead of  [2.x.533] . Due to the symmetry of  [2.x.534] , the two terms are mathematically equivalent, but the symmetric version avoids the potential for round-off errors making the resulting matrix slightly non-symmetric. 

The system at time step  [2.x.535] , to be solved on the old domain  [2.x.536] , has exactly the form of a stationary elastic problem, and is therefore similar to what we have already implemented in previous example programs. We will therefore not comment on the space discretization beyond saying that we again use lowest order continuous finite elements. 

There are differences, however: <ol>    [2.x.537]  We have to move (update) the mesh after each time step, in order to be   able to solve the next time step on a new domain; 

   [2.x.538]  We need to know  [2.x.539]  to compute the next incremental   displacement, i.e. we need to compute it at the end of the time step   to make sure it is available for the next time step. Essentially,   the stress variable is our window to the history of deformation of   the body.  [2.x.540]  These two operations are done in the functions  [2.x.541]  and  [2.x.542]  in the program. While moving the mesh is only a technicality, updating the stress is a little more complicated and will be discussed in the next section. 




[1.x.205] 

As indicated above, we need to have the stress variable  [2.x.543]  available when computing time step  [2.x.544] , and we can compute it using [1.x.206] 

[1.x.207] 

There are, despite the apparent simplicity of this equation, two questions that we need to discuss. The first concerns the way we store  [2.x.545] : even if we compute the incremental updates  [2.x.546]  using lowest-order finite elements, then its symmetric gradient  [2.x.547]  is in general still a function that is not easy to describe. In particular, it is not a piecewise constant function, and on general meshes (with cells that are not rectangles %parallel to the coordinate axes) or with non-constant stress-strain tensors  [2.x.548]  it is not even a bi- or trilinear function. Thus, it is a priori not clear how to store  [2.x.549]  in a computer program. 

To decide this, we have to see where it is used. The only place where we require the stress is in the term  [2.x.550] . In practice, we of course replace this term by numerical quadrature: 

[1.x.208] 

where  [2.x.551]  are the quadrature weights and  [2.x.552]  the quadrature points on cell  [2.x.553] . This should make clear that what we really need is not the stress  [2.x.554]  in itself, but only the values of the stress in the quadrature points on all cells. This, however, is a simpler task: we only have to provide a data structure that is able to hold one symmetric tensor of rank 2 for each quadrature point on all cells (or, since we compute in parallel, all quadrature points of all cells that the present MPI process &ldquo;owns&rdquo;). At the end of each time step we then only have to evaluate  [2.x.555] , multiply it by the stress-strain tensor  [2.x.556] , and use the result to update the stress  [2.x.557]  at quadrature point  [2.x.558] . 

The second complication is not visible in our notation as chosen above. It is due to the fact that we compute  [2.x.559]  on the domain  [2.x.560] , and then use this displacement increment to both update the stress as well as move the mesh nodes around to get to  [2.x.561]  on which the next increment is computed. What we have to make sure, in this context, is that moving the mesh does not only involve moving around the nodes, but also making corresponding changes to the stress variable: the updated stress is a variable that is defined with respect to the coordinate system of the material in the old domain, and has to be transferred to the new domain. The reason for this can be understood as follows: locally, the incremental deformation  [2.x.562]  can be decomposed into three parts, a linear translation (the constant part of the displacement increment field in the neighborhood of a point), a dilational component (that part of the gradient of the displacement field that has a nonzero divergence), and a rotation. A linear translation of the material does not affect the stresses that are frozen into it -- the stress values are simply translated along. The dilational or compressional change produces a corresponding stress update. However, the rotational component does not necessarily induce a nonzero stress update (think, in 2d, for example of the situation where  [2.x.563] , with which  [2.x.564] ). Nevertheless, if the material was prestressed in a certain direction, then this direction will be rotated along with the material.  To this end, we have to define a rotation matrix  [2.x.565]  that describes, in each point the rotation due to the displacement increments. It is not hard to see that the actual dependence of  [2.x.566]  on  [2.x.567]  can only be through the curl of the displacement, rather than the displacement itself or its full gradient (as mentioned above, the constant components of the increment describe translations, its divergence the dilational modes, and the curl the rotational modes). Since the exact form of  [2.x.568]  is cumbersome, we only state it in the program code, and note that the correct updating formula for the stress variable is then [1.x.209] 

[1.x.210] 



Both stress update and rotation are implemented in the function  [2.x.569]  of the example program. 




[1.x.211] 

In step-17, the main bottleneck for %parallel computations as far as run time is concerned was that only the first processor generated output for the entire domain. Since generating graphical output is expensive, this did not scale well when larger numbers of processors were involved. We will address this here. (For a definition of what it means for a program to "scale", see  [2.x.570]  "this glossary entry".) 

Basically, what we need to do is let every process generate graphical output for that subset of cells that it owns, write them into separate files and have a way to display all files for a certain timestep at the same time. This way the code produces one  [2.x.571]  file per process per time step. The two common VTK file viewers ParaView and VisIt both support opening more than one  [2.x.572]  file at once. To simplify the process of picking the correct files and allow moving around in time, both support record files that reference all files for a given timestep. Sadly, the record files have a different format between VisIt and Paraview, so we write out both formats. 

The code will generate the files  [2.x.573] , where  [2.x.574]  is the timestep number (starting from 1) and  [2.x.575]  is the process rank (starting from 0). These files contain the locally owned cells for the timestep and processor. The files  [2.x.576]  is the visit record for timestep  [2.x.577]  is the same for ParaView. (More recent versions of VisIt can actually read  [2.x.578]  files as well, but it doesn't hurt to output both kinds of record files.) Finally, the file  [2.x.579]  is a special record only supported by ParaView that references all time steps. So in ParaView, only solution.pvd needs to be opened, while one needs to select the group of all .visit files in VisIt for the same effect. 




[1.x.212] 

In step-17, we used a regular triangulation that was simply replicated on every processor, and a corresponding DoFHandler. Both had no idea that they were used in a %parallel context -- they just existed in their entirety on every processor, and we argued that this was eventually going to be a major memory bottleneck. 

We do not address this issue here (we will do so in step-40) but make the situation slightly more automated. In step-17, we created the triangulation and then manually "partitioned" it, i.e., we assigned  [2.x.580]  "subdomain ids" to every cell that indicated which  [2.x.581]  "MPI process" "owned" the cell. Here, we use a class  [2.x.582]  that at least does this part automatically: whenever you create or refine such a triangulation, it automatically partitions itself among all involved processes (which it knows about because you have to tell it about the  [2.x.583]  "MPI communicator" that connects these processes upon construction of the triangulation). Otherwise, the  [2.x.584]  looks, for all practical purposes, like a regular Triangulation object. 

The convenience of using this class does not only result from being able to avoid the manual call to  [2.x.585]  Rather, the DoFHandler class now also knows that you want to use it in a parallel context, and by default automatically enumerates degrees of freedom in such a way that all DoFs owned by process zero come before all DoFs owned by process 1, etc. In other words, you can also avoid the call to  [2.x.586]  

There are other benefits. For example, because the triangulation knows that it lives in a %parallel universe, it also knows that it "owns" certain cells (namely, those whose subdomain id equals its MPI rank; previously, the triangulation only stored these subdomain ids, but had no way to make sense of them). Consequently, in the assembly function, you can test whether a cell is "locally owned" (i.e., owned by the current process, see  [2.x.587] ) when you loop over all cells using the syntax 

[1.x.213] 

This knowledge extends to the DoFHandler object built on such triangulations, which can then identify which degrees of freedom are locally owned (see  [2.x.588] ) via calls such as  [2.x.589]  and  [2.x.590]  Finally, the DataOut class also knows how to deal with such triangulations and will simply skip generating graphical output on cells not locally owned. 

Of course, as has been noted numerous times in the discussion in step-17, keeping the entire triangulation on every process will not scale: large problems may simply not fit into each process's memory any more, even if we have sufficiently many processes around to solve them in a reasonable time. In such cases, the  [2.x.591]  is no longer a reasonable basis for computations and we will show in step-40 how the  [2.x.592]  class can be used to work around this, namely by letting each process store only a [1.x.214] of the triangulation. 




[1.x.215] 

The overall structure of the program can be inferred from the  [2.x.593]  function that first calls  [2.x.594]  for the first time step, and then  [2.x.595]  on all subsequent time steps. The difference between these functions is only that in the first time step we start on a coarse mesh, solve on it, refine the mesh adaptively, and then start again with a clean state on that new mesh. This procedure gives us a better starting mesh, although we should of course keep adapting the mesh as iterations proceed -- this isn't done in this program, but commented on below. 

The common part of the two functions treating time steps is the following sequence of operations on the present mesh:  [2.x.596]   [2.x.597]   [2.x.598] ]:   This first function is also the most interesting one. It assembles the   linear system corresponding to the discretized version of equation   [1.x.216]. This leads to a system matrix  [2.x.599]  built up of local contributions on each cell  [2.x.600]  with entries   [1.x.217] 

  In practice,  [2.x.601]  is computed using numerical quadrature according to the   formula   [1.x.218] 

  with quadrature points  [2.x.602]  and weights  [2.x.603] . We have built these   contributions before, in step-8 and step-17, but in both of these cases we   have done so rather clumsily by using knowledge of how the rank-4 tensor  [2.x.604]    is composed, and considering individual elements of the strain tensors    [2.x.605] . This is not really   convenient, in particular if we want to consider more complicated elasticity   models than the isotropic case for which  [2.x.606]  had the convenient form    [2.x.607] . While we in fact do not use a more complicated   form than this in the present program, we nevertheless want to write it in a   way that would easily allow for this. It is then natural to introduce   classes that represent symmetric tensors of rank 2 (for the strains and   stresses) and 4 (for the stress-strain tensor  [2.x.608] ). Fortunately, deal.II   provides these: the  [2.x.609]  class template   provides a full-fledged implementation of such tensors of rank  [2.x.610]    (which needs to be an even number) and dimension  [2.x.611] . 

  What we then need is two things: a way to create the stress-strain rank-4   tensor  [2.x.612]  as well as to create a symmetric tensor of rank 2 (the strain   tensor) from the gradients of a shape function  [2.x.613]  at a quadrature   point  [2.x.614]  on a given cell. At the top of the implementation of this   example program, you will find such functions. The first one,    [2.x.615] , takes two arguments corresponding to   the Lam&eacute; constants  [2.x.616]  and  [2.x.617]  and returns the stress-strain tensor   for the isotropic case corresponding to these constants (in the program, we   will choose constants corresponding to steel); it would be simple to replace   this function by one that computes this tensor for the anisotropic case, or   taking into account crystal symmetries, for example. The second one,    [2.x.618]  and indices    [2.x.619]  and  [2.x.620]  and returns the symmetric gradient, i.e. the strain,   corresponding to shape function  [2.x.621] , evaluated on the cell   on which the  [2.x.622]  object was last reinitialized. 

  Given this, the innermost loop of  [2.x.623]  computes the   local contributions to the matrix in the following elegant way (the variable    [2.x.624] , corresponding to the tensor  [2.x.625] , has   previously been initialized with the result of the first function above):   [1.x.219] 

  It is worth noting the expressive power of this piece of code, and to   compare it with the complications we had to go through in previous examples   for the elasticity problem. (To be fair, the SymmetricTensor class   template did not exist when these previous examples were written.) For   simplicity,  [2.x.626]  provides for the (double summation) product   between symmetric tensors of even rank here. 

  Assembling the local contributions   [1.x.220] 

  to the right hand side of [1.x.221] is equally   straightforward (note that we do not consider any boundary tractions  [2.x.627]  here). Remember that we only had to store the old stress in the   quadrature points of cells. In the program, we will provide a variable    [2.x.628]  that allows to access the stress    [2.x.629]  in each quadrature point. With this the code for the right   hand side looks as this, again rather elegant:   [1.x.222] 

  Note that in the multiplication  [2.x.630] , we have made use of the fact that for the chosen finite element, only   one vector component (namely  [2.x.631] ) of  [2.x.632]  is   nonzero, and that we therefore also have to consider only one component of    [2.x.633] . 

  This essentially concludes the new material we present in this function. It   later has to deal with boundary conditions as well as hanging node   constraints, but this parallels what we had to do previously in other   programs already. 

 [2.x.634]   [2.x.635] ]:   Unlike the previous one, this function is not really interesting, since it   does what similar functions have done in all previous tutorial programs --   solving the linear system using the CG method, using an incomplete LU   decomposition as a preconditioner (in the %parallel case, it uses an ILU of   each processor's block separately). It is virtually unchanged   from step-17. 

 [2.x.636]   [2.x.637]  [via    [2.x.638] ]: Based on the displacement field  [2.x.639]  computed before, we update the stress values in all quadrature points   according to [1.x.223] and [1.x.224],   including the rotation of the coordinate system. 

 [2.x.640]   [2.x.641] : Given the solution computed before, in this   function we deform the mesh by moving each vertex by the displacement vector   field evaluated at this particular vertex. 

 [2.x.642]   [2.x.643] : This function simply outputs the solution   based on what we have said above, i.e. every processor computes output only   for its own portion of the domain. In addition to the solution, we also compute the norm of   the stress averaged over all the quadrature points on each cell.  [2.x.644]  

With this general structure of the code, we only have to define what case we want to solve. For the present program, we have chosen to simulate the quasistatic deformation of a vertical cylinder for which the bottom boundary is fixed and the top boundary is pushed down at a prescribed vertical velocity. However, the horizontal velocity of the top boundary is left unspecified -- one can imagine this situation as a well-greased plate pushing from the top onto the cylinder, the points on the top boundary of the cylinder being allowed to slide horizontally along the surface of the plate, but forced to move downward by the plate. The inner and outer boundaries of the cylinder are free and not subject to any prescribed deflection or traction. In addition, gravity acts on the body. 

The program text will reveal more about how to implement this situation, and the results section will show what displacement pattern comes out of this simulation. 


examples/step-18/doc/results.dox 



[1.x.225] 


Running the program takes a good while if one uses debug mode; it takes about eleven minutes on my i7 desktop. Fortunately, the version compiled with optimizations is much faster; the program only takes about a minute and a half after recompiling with the command <tt>make release</tt> on the same machine, a much more reasonable time. 


If run, the program prints the following output, explaining what it is doing during all that time: 

[1.x.226] 

In other words, it is computing on 12,000 cells and with some 52,000 unknowns. Not a whole lot, but enough for a coupled three-dimensional problem to keep a computer busy for a while. At the end of the day, this is what we have for output: 

[1.x.227] 




If we visualize these files with VisIt or Paraview, we get to see the full picture of the disaster our forced compression wreaks on the cylinder (colors in the images encode the norm of the stress in the material): 


<div class="threecolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0002.0000.png"            alt="Time = 2"            width="400">     </div>     <div class="text" align="center">       Time = 2     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0005.0000.png"            alt="Time = 5"            width="400">     </div>     <div class="text" align="center">       Time = 5     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0007.0000.png"            alt="Time = 7"            width="400">     </div>     <div class="text" align="center">       Time = 7     </div>   </div> </div> 


<div class="threecolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0008.0000.png"            alt="Time = 8"            width="400">     </div>     <div class="text" align="center">       Time = 8     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0009.0000.png"            alt="Time = 9"            width="400">     </div>     <div class="text" align="center">       Time = 9     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.sequential-0010.0000.png"            alt="Time = 10"            width="400">     </div>     <div class="text" align="center">       Time = 10     </div>   </div> </div> 


As is clearly visible, as we keep compressing the cylinder, it starts to bow out near the fully constrained bottom surface and, after about eight time units, buckle in an azimuthally symmetric manner. 


Although the result appears plausible for the symmetric geometry and loading, it is yet to be established whether or not the computation is fully converged. In order to see whether it is, we ran the program again with one more global refinement at the beginning and with the time step halved. This would have taken a very long time on a single machine, so we used a proper workstation and ran it on 16 processors in parallel. The beginning of the output now looks like this: 

[1.x.228] 

That's quite a good number of unknowns, given that we are in 3d. The output of this program are 16 files for each time step: 

[1.x.229] 




Here are first the mesh on which we compute as well as the partitioning for the 16 processors: 


<div class="twocolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-000mesh.png"            alt="Discretization"            width="400">     </div>     <div class="text" align="center">       Discretization     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0002.p.png"            alt="Parallel partitioning"            width="400">     </div>     <div class="text" align="center">       Parallel partitioning     </div>   </div> </div> 


Finally, here is the same output as we have shown before for the much smaller sequential case: 

<div class="threecolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0002.s.png"            alt="Time = 2"            width="400">     </div>     <div class="text" align="center">       Time = 2     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0005.s.png"            alt="Time = 5"            width="400">     </div>     <div class="text" align="center">       Time = 5     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0007.s.png"            alt="Time = 7"            width="400">     </div>     <div class="text" align="center">       Time = 7     </div>   </div> </div> 


<div class="threecolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0008.s.png"            alt="Time = 8"            width="400">     </div>     <div class="text" align="center">       Time = 8     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0009.s.png"            alt="Time = 9"            width="400">     </div>     <div class="text" align="center">       Time = 9     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-18.parallel-0010.s.png"            alt="Time = 10"            width="400">     </div>     <div class="text" align="center">       Time = 10     </div>   </div> </div> 


As before, we observe that at high axial compression the cylinder begins to buckle, but this time ultimately collapses on itself. In contrast to our first run, towards the end of the simulation the deflection pattern becomes nonsymmetric (the central bulge deflects laterally). The model clearly does not provide for this (all our forces and boundary deflections are symmetric) but the effect is probably physically correct anyway: in reality, small inhomogeneities in the body's material properties would lead it to buckle to one side to evade the forcing; in numerical simulations, small perturbations such as numerical round-off or an inexact solution of a linear system by an iterative solver could have the same effect. Another typical source for asymmetries in adaptive computations is that only a certain fraction of cells is refined in each step, which may lead to asymmetric meshes even if the original coarse mesh was symmetric. 


If one compares this with the previous run, the results both qualitatively and quantitatively different. The previous computation was therefore certainly not converged, though we can't say for sure anything about the present one. One would need an even finer computation to find out. However, the point may be moot: looking at the last picture in detail, it is pretty obvious that not only is the linear small deformation model we chose completely inadequate, but for a realistic simulation we would also need to make sure that the body does not intersect itself during deformation (if we continued compressing the cylinder we would observe some self-intersection). Without such a formulation we cannot expect anything to make physical sense, even if it produces nice pictures! 




[1.x.230] 

The program as is does not really solve an equation that has many applications in practice: quasi-static material deformation based on a purely elastic law is almost boring. However, the program may serve as the starting point for more interesting experiments, and that indeed was the initial motivation for writing it. Here are some suggestions of what the program is missing and in what direction it may be extended: 

[1.x.231] 

 The most obvious extension is to use a more realistic material model for large-scale quasistatic deformation. The natural choice for this would be plasticity, in which a nonlinear relationship between stress and strain replaces equation [1.x.232]. Plasticity models are usually rather complicated to program since the stress-strain dependence is generally non-smooth. The material can be thought of being able to withstand only a maximal stress (the yield stress) after which it starts to &ldquo;flow&rdquo;. A mathematical description to this can be given in the form of a variational inequality, which alternatively can be treated as minimizing the elastic energy 

[1.x.233] 

subject to the constraint 

[1.x.234] 

on the stress. This extension makes the problem to be solved in each time step nonlinear, so we need another loop within each time step. 

Without going into further details of this model, we refer to the excellent book by Simo and Hughes on &ldquo;Computational Inelasticity&rdquo; for a comprehensive overview of computational strategies for solving plastic models. Alternatively, a brief but concise description of an algorithm for plasticity is given in an article by S. Commend, A. Truty, and Th. Zimmermann;  [2.x.645] . 




[1.x.235] 

The formulation we have chosen, i.e. using piecewise (bi-, tri-)linear elements for all components of the displacement vector, and treating the stress as a variable dependent on the displacement is appropriate for most materials. However, this so-called displacement-based formulation becomes unstable and exhibits spurious modes for incompressible or nearly-incompressible materials. While fluids are usually not elastic (in most cases, the stress depends on velocity gradients, not displacement gradients, although there are exceptions such as electro-rheologic fluids), there are a few solids that are nearly incompressible, for example rubber. Another case is that many plasticity models ultimately let the material become incompressible, although this is outside the scope of the present program. 

Incompressibility is characterized by Poisson's ratio 

[1.x.236] 

where  [2.x.646]  are the Lam&eacute; constants of the material. Physical constraints indicate that  [2.x.647]  (the condition also follows from mathematical stability considerations). If  [2.x.648]  approaches  [2.x.649] , then the material becomes incompressible. In that case, pure displacement-based formulations are no longer appropriate for the solution of such problems, and stabilization techniques have to be employed for a stable and accurate solution. The book and paper cited above give indications as to how to do this, but there is also a large volume of literature on this subject; a good start to get an overview of the topic can be found in the references of the paper by H.-Y. Duan and Q. Lin;  [2.x.650] . 




[1.x.237] 

In the present form, the program only refines the initial mesh a number of times, but then never again. For any kind of realistic simulation, one would want to extend this so that the mesh is refined and coarsened every few time steps instead. This is not hard to do, in fact, but has been left for future tutorial programs or as an exercise, if you wish. 

The main complication one has to overcome is that one has to transfer the data that is stored in the quadrature points of the cells of the old mesh to the new mesh, preferably by some sort of projection scheme. The general approach to this would go like this: 

- At the beginning, the data is only available in the quadrature points of   individual cells, not as a finite element field that is defined everywhere. 

- So let us find a finite element field that [1.x.238] defined everywhere so   that we can later interpolate it to the quadrature points of the new   mesh. In general, it will be difficult to find a continuous finite element   field that matches the values in the quadrature points exactly because the   number of degrees of freedom of these fields does not match the number of   quadrature points there are, and the nodal values of this global field will   either be over- or underdetermined. But it is usually not very difficult to   find a discontinuous field that matches the values in the quadrature points;   for example, if you have a QGauss(2) quadrature formula (i.e. 4 points per   cell in 2d, 8 points in 3d), then one would use a finite element of kind   FE_DGQ(1), i.e. bi-/tri-linear functions as these have 4 degrees of freedom   per cell in 2d and 8 in 3d. 

- There are functions that can make this conversion from individual points to   a global field simpler. The following piece of pseudo-code should help if   you use a QGauss(2) quadrature formula. Note that the multiplication by the   projection matrix below takes a vector of scalar components, i.e., we can only   convert one set of scalars at a time from the quadrature points to the degrees   of freedom and vice versa. So we need to store each component of stress separately,   which requires  [2.x.651]  vectors. We'll store this set of vectors in a 2D array to   make it easier to read off components in the same way you would the stress tensor.   Thus, we'll loop over the components of stress on each cell and store   these values in the global history field. (The prefix  [2.x.652]    indicates that we work with quantities related to the history variables defined   in the quadrature points.)   [1.x.239] 



- Now that we have a global field, we can refine the mesh and transfer the   history_field vector as usual using the SolutionTransfer class. This will   interpolate everything from the old to the new mesh. 

- In a final step, we have to get the data back from the now interpolated   global field to the quadrature points on the new mesh. The following code   will do that:   [1.x.240] 



It becomes a bit more complicated once we run the program in parallel, since then each process only stores this data for the cells it owned on the old mesh. That said, using a parallel vector for  [2.x.653]  will do the trick if you put a call to  [2.x.654]  after the transfer from quadrature points into the global vector. 




[1.x.241] 

At present, the program makes no attempt to make sure that a cell, after moving its vertices at the end of the time step, still has a valid geometry (i.e. that its Jacobian determinant is positive and bounded away from zero everywhere). It is, in fact, not very hard to set boundary values and forcing terms in such a way that one gets distorted and inverted cells rather quickly. Certainly, in some cases of large deformation, this is unavoidable with a mesh of finite mesh size, but in some other cases this should be preventable by appropriate mesh refinement and/or a reduction of the time step size. The program does not do that, but a more sophisticated version definitely should employ some sort of heuristic defining what amount of deformation of cells is acceptable, and what isn't. 


examples/step-19/doc/intro.dox 



 [2.x.655]  

[1.x.242] 

 [2.x.656]  Support for particles exists in deal.II primarily due to the initial   efforts of Rene Gassmoeller. Please acknowledge this work by citing   the publication  [2.x.657]  if you use particle functionality in your   own work. 

[1.x.243] 

[1.x.244] 

The finite element method in general, and deal.II in particular, were invented to solve partial differential equations -- in other words, to solve [continuum mechanics](https://en.wikipedia.org/wiki/Continuum_mechanics) problems. On the other hand, sometimes one wants to solve problems in which it is useful to track individual objects ("particles") and how their positions evolve. If this simply leads to a set of ordinary differential equations, for example if you want to track the positions of the planets in the solar system over time, then deal.II is clearly not your right tool. On the other hand, if this evolution is due to the interaction with the solution of partial differential equation, or if having a mesh to determine which particles interact with others (such as in the [smoothed particle hydrodynamics (SPH)](https://en.wikipedia.org/wiki/Smoothed-particle_hydrodynamics) method), then deal.II has support for you. 

The case we will consider here is how electrically charged particles move through an electric field. As motivation, we will consider [cathode rays](https://en.wikipedia.org/wiki/Cathode_ray): Electrons emitted by a heated piece of metal that is negatively charged (the "cathode"), and that are then accelerated by an electric field towards the positively charged electrode (the "anode"). The anode is typically ring-shaped so that the majority of electrons can fly through the hole in the form of an electron beam. In the olden times, they might then have illuminated the screen of a TV built from a [cathode ray tube](https://en.wikipedia.org/wiki/Cathode-ray_tube). Today, instead, electron beams are useful in [X-ray machines](https://en.wikipedia.org/wiki/X-ray_tube), [electron beam lithography](https://en.wikipedia.org/wiki/Electron-beam_lithography), [electron beam welding](https://en.wikipedia.org/wiki/Electron-beam_welding), and a number of other areas. 

The equations we will then consider are as follows: First, we need to describe the electric field. This is most easily accomplished by noting that the electric potential  [2.x.658]  satisfied the equation 

[1.x.245] 

where  [2.x.659]  is the dielectric constant of vacuum, and  [2.x.660]  is the charge density. This is augmented by boundary conditions that we will choose as follows: 

[1.x.246] 

In other words, we prescribe voltages  [2.x.661]  and  [2.x.662]  at the two electrodes and insulating (Neumann) boundary conditions elsewhere. Since the dynamics of the particles are purely due to the electric field  [2.x.663] , we could as well have prescribed  [2.x.664]  and  [2.x.665]  at the two electrodes -- all that matters is the voltage difference at the two electrodes. 

Given this electric potential  [2.x.666]  and the electric field  [2.x.667] , we can describe the trajectory of the  [2.x.668] th particle using the differential equation 

[1.x.247] 

where  [2.x.669]  are the mass and electric charge of each particle. In practice, it is convenient to write this as a system of first-order differential equations in the position  [2.x.670]  and velocity  [2.x.671] : 

[1.x.248] 

The deal.II class we will use to deal with particles,  [2.x.672]  stores particles in a way so that the position  [2.x.673]  is part of the  [2.x.674]  data structures. (It stores particles sorted by cell they are in, and consequently needs to know where each particle is.) The velocity  [2.x.675] , on the other hand, is of no concern to  [2.x.676]  and consequently we will store it as a "property" of each particle that we will update in each time step. Properties can also be used to store any other quantity we might care about each particle: its charge, or if they were larger than just an electron, its color, mass, attitude in space, chemical composition, etc. 

There remain two things to discuss to complete the model: Where particles start and what the charge density  [2.x.677]  is. 

First, historically, cathode rays used very large electric fields to pull electrons out of the metal. This produces only a relatively small current. One can do better by heating the cathode: a statistical fraction of electrons in that case have enough thermal energy to leave the metal; the electric field then just has to be strong enough to pull them away from the attraction of their host body. We will model this in the following way: We will create a new particle if (i) the electric field points away from the electrode, i.e., if  [2.x.678]  where  [2.x.679]  is the normal vector at a face pointing out of the domain (into the electrode), and (ii) the electric field exceeds a threshold value  [2.x.680] . This is surely not a sufficiently accurate model for what really happens, but is good enough for our current tutorial program. 

Second, in principle we would have to model the charge density via 

[1.x.249] 



 [2.x.681]  The issue now is that in reality, a cathode ray tube in an old television yields a current of somewhere around a few milli-Amperes. In the much higher energy beams of particle accelerators, the current may only be a few nano-Ampere. But an Ampere is  [2.x.682]  electrons flowing per second. Now, as you will see in the results section, we really only simulate a few microseconds ( [2.x.683]  seconds), but that still results in very very large numbers of electrons -- far more than we can hope to simulate with a program as small as the current one. As a consequence, let us presume that each particle represents  [2.x.684]  electrons. Then the particle mass and charge are also  [2.x.685]  and  [2.x.686]  and the equations we have to solve are 

[1.x.250] 

which is of course exactly the same as above. On the other hand, the charge density for these "clumps" of electrons is given by 

[1.x.251] 

It is this form that we will implement in the program, where  [2.x.687]  is chosen rather large in the program to ensure that the particles actually affect the electric field. (This may not be realistic in practice: In most cases, there are just not enough electrons to actually affect the overall electric field. But realism is not our goal here.) 




 [2.x.688]  One may wonder why the equation for the electric field (or, rather, the electric potential) has no time derivative whereas the equations for the electron positions do. In essence, this is a modeling assumption: We assume that the particles move so slowly that at any given time the electric field is in equilibrium. This is saying, in other words, that the velocity of the electrons is much less than the speed of light. In yet other words, we can rephrase this in terms of the electrode voltage  [2.x.689] : Since every volt of electric potential accelerates electrons by approximately 600 km/s (neglecting relativistic effects), requiring  [2.x.690]  is equivalent to saying that  [2.x.691] . Under this assumption (and the assumption that the total number of electrons is small), one can also neglect the creation of magnetic fields by the moving charges, which would otherwise also affect the movement of the electrons. 




[1.x.252] 

The equations outlined above form a set of coupled differential equations. Let us bring them all together in one place again to make that clear: 

[1.x.253] 

Because of the awkward dependence of the electric potential on the particle locations, we don't want to solve this as a coupled system but instead use a decoupled approach where we first solve for the potential in each time step and then the particle locations. (One could also do it the other way around, of course.) This is very much in the same spirit as we do in step-21, step-31, and step-32, to name just a few, and can all be understood in the context of the operator splitting methods discussed in step-58. 

So, if we denote by an upper index  [2.x.692]  the time step, and if we use a simple time discretization for the ODE, then this means that we have to solve the following set of equations in each time step: 

[1.x.254] 

There are of course many better ways to do a time discretization (for example the simple [leapfrog scheme](https://en.wikipedia.org/wiki/Leapfrog_integration)) but this isn't the point of the tutorial program, and so we will be content with what we have here. (We will comment on a piece of this puzzle in the [1.x.255] section of this program, however.) 

There remains the question of how we should choose the time step size  [2.x.693] . The limitation here is that the  [2.x.694]  class needs to keep track of which cell each particle is in. This is particularly an issue if we are running computations in parallel (say, in step-70) because in that case every process only stores those cells it owns plus one layer of "ghost cells". That's not relevant here, but in general we should make sure that over the course of each time step, a particle moves only from one cell to any of its immediate neighbors (face, edge, or vertex neighbors). If we can ensure that, then  [2.x.695]  is guaranteed to be able to figure out which cell a particle ends up in. To do this, a useful rule of thumb is that we should choose the time step so that for all particles the expected distance the particle moves by is less than one cell diameter: 

[1.x.256] 

or equivalently 

[1.x.257] 

Here,  [2.x.696]  is the length of the shortest edge of the cell on which particle  [2.x.697]  is located -- in essence, a measure of the size of a cell. 

On the other hand, a particle might already be at the boundary of one cell and the neighboring cell might be once further refined. So then the time to cross that *neighboring* cell would actually be half the amount above, suggesting 

[1.x.258] 



But even that is not good enough: The formula above updates the particle positions in each time using the formula 

[1.x.259] 

that is, using the *current* velocity  [2.x.698] . But we don't have the current velocity yet at the time when we need to choose  [2.x.699]  -- which is after we have updated the potential  [2.x.700]  but before we update the velocity from  [2.x.701]  to  [2.x.702] . All we have is  [2.x.703] . So we need an additional safety factor for our final choice: 

[1.x.260] 

How large should  [2.x.704]  be? That depends on how much of underestimate  [2.x.705]  might be compared to  [2.x.706] , and that is actually quite easy to assess: A particle created in one time step with zero velocity will roughly pick up equal velocity increments in each successive time step if the electric field it encounters along the way were roughly constant. So the maximal difference between  [2.x.707]  and  [2.x.708]  would be a factor of two. As a consequence, we will choose  [2.x.709] . 

There is only one other case we ought to consider: What happens in the very first time step? There, any particles to be moved along have just been created, but they have a zero velocity. So we don't know what velocity we should choose for them. Of course, in all other time steps there are also particles that have just been created, but in general, the particles with the highest velocity limit the time step size and so the newly created particles with their zero velocity don't matter. But if we *only* have such particles? 

In that case, we can use the following approximation: If a particle starts at  [2.x.710] , then the update formula tells us that 

[1.x.261] 

and consequently 

[1.x.262] 

which we can write as 

[1.x.263] 

Not wanting to move a particle by more than  [2.x.711]  then implies that we should choose the time step as 

[1.x.264] 

Using the same argument about neighboring cells possibly being smaller by a factor of two then leads to the final formula for time step zero: 

[1.x.265] 



Strictly speaking, we would have to evaluate the electric potential  [2.x.712]  at the location of each particle, but a good enough approximation is to use the maximum of the values at the vertices of the respective cell. (Why the vertices and not the midpoint? Because the gradient of the solution of the Laplace equation, i.e., the electric field, is largest in corner singularities which are located at the vertices of cells.) This has the advantage that we can make good use of the FEValues functionality which can recycle pre-computed material as long as the quadrature points are the same from one cell to the next. 

We could always run this kind of scheme to estimate the difference between  [2.x.713]  and  [2.x.714] , but it relies on evaluating the electric field  [2.x.715]  on each cell, and that is expensive. As a consequence, we will limit this approach to the very first time step. 




[1.x.266] 

Having discussed the time discretization, the discussion of the spatial discretization is going to be short: We use quadratic finite elements, i.e., the space  [2.x.716] , to approximate the electric potential  [2.x.717] . The mesh is adapted a couple of times during the initial time step. All of this is entirely standard if you have read step-6, and the implementation does not provide for any kind of surprise. 




[1.x.267] 

Adding and moving particles is, in practice, not very difficult in deal.II. To add one, the `create_particles()` function of this program simply uses a code snippet of the following form: 

[1.x.268] 

In other words, it is not all that different from inserting an object into a  [2.x.718]  or  [2.x.719]  Create the object, set its properties (here, the current location, its reference cell location, and its id) and call `insert_particle`. The only thing that may be surprising is the reference location: In order to evaluate things such as  [2.x.720] , it is necessary to evaluate finite element fields at locations  [2.x.721] . But this requires evaluating the finite element shape functions at points on the reference cell  [2.x.722] . To make this efficient, every particle doesn't just store its location and the cell it is on, but also what location that point corresponds to in the cell's reference coordinate system. 

Updating a particle's position is then no more difficult: One just has to call 

[1.x.269] 

We do this in the `move_particles()` function. The only difference is that we then have to tell the  [2.x.723]  class to also find what cell that position corresponds to (and, when computing in parallel, which process owns this cell). For efficiency reason, this is most easily done after updating all particles' locations, and is achieved via the  [2.x.724]  function. 

There are, of course, times where a particle may leave the domain in question. In that case,  [2.x.725]  can not find a surrounding cell and simply deletes the particle. But, it is often useful to track the number of particles that have been lost this way, and for this the  [2.x.726]  class offers a "signal" that one can attach to. We show how to do this in the constructor of the main class to count how many particles were lost in each time step. Specifically, the way this works is that the  [2.x.727]  class has a "signal" to which one can attach a function that will be executed whenever the signal is triggered. Here, this looks as follows: 

[1.x.270] 

That's a bit of a mouthful, but what's happening is this: We declare a lambda function that "captures" the `this` pointer (so that we can access member functions of the surrounding object inside the lambda function), and that takes two arguments: 

- A reference to the particle that has been "lost". 

- A reference to the cell it was on last. The lambda function then simply calls the  [2.x.728]  function with these arguments. When we attach this lambda function to the signal, the  [2.x.729]  function will trigger the signal for every particle for which it can't find a new home. This gives us the chance to record where the particle is, and to record statistics on it. 




 [2.x.730]  In this tutorial program, we insert particles by hand and at   locations we specifically choose based on conditions that include   the solution of the electrostatic problem. But there are other cases   where one primarily wants to use particles as passive objects, for   example to trace and visualize the flow field of a fluid flow   problem. In those cases, there are numerous functions in the    [2.x.731]  namespace that can generate particles   automatically. One of the functions of this namespace is also used   in the step-70 tutorial program, for example. 




[1.x.271] 

The test case here is not meant to be a realistic depiction of a cathode ray tube, but it has the right general characteristics and the point is, in any case, only to demonstrate how one would implement deal.II codes that use particles. 

The following picture shows the geometry that we're going to use: 

<p align="center">   <img src="https://www.dealii.org/images/steps/developer/step-19.geometry.png"        alt="The geometry used in this program"        width="600">  [2.x.732]  

In this picture, the parts of the boundary marked in red and blue are the cathode, held at an electric potential  [2.x.733] . The part of the cathode shown in red is the part that is heated, leading to electrons leaving the metal and then being accelerated by the electric field (a few electric field lines are also shown). The green part of the boundary is the anode, held at  [2.x.734] . The rest of the boundary satisfies a Neumann boundary condition. 

This setup mimics real devices. The re-entrant corner results in an electric potential  [2.x.735]  whose derivative (the electric field  [2.x.736] ) has a singularity -- in other words, it becomes very large in the vicinity of the corner, allowing it to rip electrons away from the metal. These electrons are then accelerated towards the (green) anode which has a hole in the middle through which the electrons can escape the device and fly on to hit the screen, where they excite the "phosphor" to then emit the light that we see from these old-style TV screens. The non-heated part of the cathode is not subject to the emission of electrons -- in the code, we will mark this as the "focussing element" of the tube, because its negative electric voltage repels the electrons and makes sure that they do not just fly away from the heated part of the cathode perpendicular to the boundary, but in fact bend their paths towards the anode on the right. 

The electric field lines also shown in the picture illustrate that the electric field connects the negative and positive electrodes, respectively. The accelerating force the electrons experience is along these field lines. Finally, the picture shows the mesh used in the computation, illustrating that there are singularities at the tip of the re-rentrant corner as well as at all places where the boundary conditions change; these singularities are visible because the mesh is refined in these locations. 

Of practical interest is to figure out which fraction of the electrons emitted from the cathode actually make it through the hole in the anode -- electrons that just bounce into the anode itself are not actually doing anything useful other than converting electricity into heat. As a consequence, in the `track_lost_particle()` function (which is called for each particle that leaves the domain, see above), we will estimate where it might have left the domain and report this in the output. 




 [2.x.737]  It is worth repeating that neither the geometry used here, nor in fact any other aspect of this program is intended to represent anything even half-way realistic. Tutorial programs are our tools to teach how deal.II works, and we often use situations for which we have some kind of intuition since this helps us interpret the output of a program, but that's about the extent to which we intend the program to do anything of use besides being a teaching tool. 


examples/step-19/doc/results.dox 



[1.x.272] 

When this program is run, it produces output that looks as follows: ``` Timestep 1   Field degrees of freedom:                                 4989   Total number of particles in simulation:  20   Number of particles lost this time step:  0 

  Now at t=2.12647e-07, dt=2.12647e-07. 

Timestep 2   Field degrees of freedom:                 4989   Total number of particles in simulation:  24   Number of particles lost this time step:  0 

  Now at t=4.14362e-07, dt=2.01715e-07. 

Timestep 3   Field degrees of freedom:                 4989   Total number of particles in simulation:  28   Number of particles lost this time step:  0 

  Now at t=5.96019e-07, dt=1.81657e-07. 

Timestep 4   Field degrees of freedom:                 4989   Total number of particles in simulation:  32   Number of particles lost this time step:  0 

  Now at t=7.42634e-07, dt=1.46614e-07. 


... 


  Timestep 1000   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  6   Fraction of particles lost through anode: 0.0601266 

  Now at t=4.93276e-05, dt=4.87463e-08. 

Timestep 1001   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  0   Fraction of particles lost through anode: 0.0601266 

  Now at t=4.93759e-05, dt=4.82873e-08. 


... 


Timestep 2091   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  0   Fraction of particles lost through anode: 0.0503338 

  Now at t=9.99237e-05, dt=4.26254e-08. 

Timestep 2092   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  0   Fraction of particles lost through anode: 0.0503338 

  Now at t=9.99661e-05, dt=4.24442e-08. 

Timestep 2093   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  2   Fraction of particles lost through anode: 0.050308 

  Now at t=0.0001, dt=3.38577e-08. ``` 

Picking a random few time steps, we can visualize the solution in the form of streamlines for the electric field and dots for the electrons: <div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-19.solution.0000.png"          alt="The solution at time step 0 (t=0 seconds)."          width="500">      [2.x.738]      Solution at time step 0 (t=0 seconds).      [2.x.739]    </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-19.solution.1400.png"          alt="The solution at time step 1400 (t=0.000068 seconds)."          width="500">      [2.x.740]      Solution at time step 1400 (t=0.000068 seconds).      [2.x.741]    </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-19.solution.0700.png"          alt="The solution at time step 700 (t=0.000035 seconds)."          width="500">      [2.x.742]      Solution at time step 700 (t=0.000035 seconds).      [2.x.743]    </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-19.solution.2092.png"          alt="The solution at time step 2092 (t=0.0001 seconds)."          width="500">      [2.x.744]      Solution at time step 2092 (t=0.0001 seconds).      [2.x.745]    </div> </div> 

That said, a more appropriate way to visualize the results of this program are by creating a video that shows how these electrons move, and how the electric field changes in response to their motion: 

[1.x.273] 



What you can see here is how the "focus element" of the boundary with its negative voltage repels the electrons and makes sure that they do not just fly away perpendicular from the cathode (as they do in the initial part of their trajectories). It also shows how the electric field lines move around over time, in response to the charges flying by -- in other words, the feedback the particles have on the electric field that itself drives the motion of the electrons. 

The movie suggests that electrons move in "bunches" or "bursts". One element of this appearance is an artifact of how the movie was created: Every frame of the movie corresponds to one time step, but the time step length varies. More specifically, the fastest particle moving through the smallest cell determines the length of the time step (see the discussion in the introduction), and consequently time steps are small whenever a (fast) particle moves through the small cells at the right edge of the domain; time steps are longer again once the particle has left the domain. This slowing-accelerating effect can easily be visualized by plotting the time step length shown in the screen output. 

The second part of this is real, however: The simulation creates a large group of particles in the beginning, and fewer after about the 300th time step. This is probably because of the negative charge of the particles in the simulation: They reduce the magnitude of the electric field at the (also negatively charged electrode) and consequently reduce the number of points on the cathode at which the magnitude exceeds the threshold necessary to draw an electron out of the electrode. 


[1.x.274] 

[1.x.275] 

[1.x.276] 

The `assemble_system()`, `move_particles()`, and `update_timestep_size()` functions all call  [2.x.746]  and  [2.x.747]  that query information about the particles located on the current cell. While this is convenient, it's also inefficient. To understand why this is so, one needs to know how particles are stored in  [2.x.748]  namely, in a data structure in which particles are ordered in some kind of linear fashion sorted by the cell they are on. Consequently, in order to find the particles associated with a given cell, these functions need to search for the first (and possibly last) particle on a given cell -- an effort that costs  [2.x.749]  operations where  [2.x.750]  is the number of particles. But this is repeated on every cell; assuming that for large computations, the number of cells and particles are roughly proportional, the accumulated cost of these function calls is then  [2.x.751]  and consequently larger than the  [2.x.752]  cost that we should shoot for with all parts of a program. 

We can make this cheaper, though. First, instead of calling  [2.x.753]  we might first call  [2.x.754]  and then compute the number of particles on a cell by just computing the distance of the last to the first particle on the current cell: 

[1.x.277] 

The first of these calls is of course still  [2.x.755] , but at least the second call only takes a compute time proportional to the number of particles on the current cell and so, when accumulated over all cells, has a cost of  [2.x.756] . 

But we can even get rid of the first of these calls with some proper algorithm design. That's because particles are ordered in the same way as cells, and so we can just walk them as we move along on the cells. The following outline of an algorithm does this: 

[1.x.278] 



In this code, we touch every cell exactly once and we never have to search the big data structure for the first or last particle on each cell. As a consequence, the algorithm costs a total of  [2.x.757]  for a complete sweep of all particles and all cells. 

It would not be very difficult to implement this scheme for all three of the functions in this program that have this issue. 




[1.x.279] 

The program already computes the fraction of the electrons that leave the domain through the hole in the anode. But there are other quantities one might be interested in. For example, the average velocity of these particles. It would not be very difficult to obtain each particle's velocity from its properties, in the same way as we do in the `move_particles()` function, and compute statistics from it. 




[1.x.280] 

As discussed above, there is a varying time difference between different frames of the video because we create output for every time step. A better way to create movies would be to generate a new output file in fixed time intervals, regardless of how many time steps lie between each such point. 




[1.x.281] 

The problem we are considering in this program is a coupled, multiphysics problem. But the way we solve it is by first computing the (electric) potential field and then update the particle locations. This is what is called an "operator-splitting method", a concept we will investigate in more detail in step-58. 

While it is awkward to think of a way to solve this problem that does not involve splitting the problem into a PDE piece and a particles piece, one *can* (and probably should!) think of a better way to update the particle locations. Specifically, the equations we use to update the particle location are 

[1.x.282] 

This corresponds to a simple forward-Euler time discretization -- a method of first order accuracy in the time step size  [2.x.758]  that we know we should avoid because we can do better. Rather, one might want to consider a scheme such as the [leapfrog scheme](https://en.wikipedia.org/wiki/Leapfrog_integration) or more generally [symplectic integrators](https://en.wikipedia.org/wiki/Symplectic_integrator) such as the [Verlet scheme](https://en.wikipedia.org/wiki/Verlet_integration). 




[1.x.283] 

In release mode, the program runs in about 3.5 minutes on one of the author's laptops at the time of writing this. That's acceptable. But what if we wanted to make the simulation three-dimensional? If we wanted to not use a maximum of around 100 particles at any given time (as happens with the parameters used here) but 100,000? If we needed a substantially finer mesh? 

In those cases, one would want to run the program not just on a single processor, but in fact on as many as one has available. This requires parallelization both the PDE solution as well as over particles. In practice, while there are substantial challenges to making this efficient and scale well, these challenges are all addressed in deal.II itself. For example, step-40 shows how to parallelize the finite element part, and step-70 shows how one can then also parallelize the particles part. 


examples/step-2/doc/intro.dox 

[1.x.284] 

[1.x.285] 

 [2.x.759]  

After we have created a grid in the previous example, we now show how to define degrees of freedom on this mesh. For this example, we will use the lowest order ( [2.x.760] ) finite elements, for which the degrees of freedom are associated with the vertices of the mesh. Later examples will demonstrate higher order elements where degrees of freedom are not necessarily associated with vertices any more, but can be associated with edges, faces, or cells. 

The term "degree of freedom" is commonly used in the finite element community to indicate two slightly different, but related things. The first is that we'd like to represent the finite element solution as a linear combination of shape functions, in the form  [2.x.761] . Here,  [2.x.762]  is a vector of expansion coefficients. Because we don't know their values yet (we will compute them as the solution of a linear or nonlinear system), they are called "unknowns" or "degrees of freedom". The second meaning of the term can be explained as follows: A mathematical description of finite element problems is often to say that we are looking for a finite dimensional function  [2.x.763]  that satisfies some set of equations (e.g.  [2.x.764]  for all test functions  [2.x.765] ). In other words, all we say here that the solution needs to lie in some space  [2.x.766] . However, to actually solve this problem on a computer we need to choose a basis of this space; this is the set of shape functions  [2.x.767]  we have used above in the expansion of  [2.x.768]  with coefficients  [2.x.769] . There are of course many bases of the space  [2.x.770] , but we will specifically choose the one that is described by the finite element functions that are traditionally defined locally on the cells of the mesh. Describing "degrees of freedom" in this context requires us to simply [1.x.286] the basis functions of the space  [2.x.771] . For  [2.x.772]  elements this means simply enumerating the vertices of the mesh in some way, but for higher order elements, one also has to enumerate the shape functions that are associated with edges, faces, or cell interiors of the mesh. In other words, the enumeration of degrees of freedom is an entirely separate thing from the indices we use for vertices. The class that provides this enumeration of the basis functions of  [2.x.773]  is called DoFHandler. 

Defining degrees of freedom ("DoF"s in short) on a mesh is a rather simple task, since the library does all the work for you. Essentially, all you have to do is create a finite element object (from one of the many finite element classes deal.II already has, see for example the  [2.x.774]  documentation) and give it to a DoFHandler object through the  [2.x.775]  function ("distributing DoFs" is the term we use to describe the process of [1.x.287] the basis functions as discussed above). The DoFHandler is a class that knows which degrees of freedom live where, i.e., it can answer questions like "how many degrees of freedom are there globally" and "on this cell, give me the global indices of the shape functions that live here". This is the sort of information you need when determining how big your system matrix should be, and when copying the contributions of a single cell into the global matrix. 

[1.x.288] 

The next step would then be to compute a matrix and right hand side corresponding to a particular differential equation using this finite element and mesh. We will keep this step for the step-3 program and rather talk about one practical aspect of a finite element program, namely that finite element matrices are always very sparse: almost all entries in these matrices are zero. 

To be more precise, we say that a matrix is sparse if the number of nonzero entries [1.x.289] in the matrix is bounded by a number that is independent of the overall number of degrees of freedom. For example, the simple 5-point stencil of a finite difference approximation of the Laplace equation leads to a sparse matrix since the number of nonzero entries per row is five, and therefore independent of the total size of the matrix. For more complicated problems -- say, the Stokes problem of step-22 -- and in particular in 3d, the number of entries per row may be several hundred. But the important point is that this number is independent of the overall size of the problem: If you refine the mesh, the maximal number of unknowns per row remains the same. 

Sparsity is one of the distinguishing feature of the finite element method compared to, say, approximating the solution of a partial differential equation using a Taylor expansion and matching coefficients, or using a Fourier basis. 

In practical terms, it is the sparsity of matrices that enables us to solve problems with millions or billions of unknowns. To understand this, note that a matrix with  [2.x.776]  rows, each with a fixed upper bound for the number of nonzero entries, requires  [2.x.777]  memory locations for storage, and a matrix-vector multiplication also requires only  [2.x.778]  operations. Consequently, if we had a linear solver that requires only a fixed number of matrix-vector multiplications to come up with the solution of a linear system with this matrix, then we would have a solver that can find the values of all  [2.x.779]  unknowns with optimal complexity, i.e., with a total of  [2.x.780]  operations. It is clear that this wouldn't be possible if the matrix were not sparse (because then the number of entries in the matrix would have to be  [2.x.781]  with some  [2.x.782] , and doing a fixed number of matrix-vector products would take  [2.x.783]  operations), but it also requires very specialized solvers such as multigrid methods to satisfy the requirement that the solution requires only a fixed number of matrix-vector multiplications. We will frequently look at the question of what solver to use in the remaining programs of this tutorial. 

The sparsity is generated by the fact that finite element shape functions are defined [1.x.290] on individual cells, rather than globally, and that the local differential operators in the bilinear form only couple shape functions whose support overlaps. (The "support" of a function is the area where it is nonzero. For the finite element method, the support of a shape function is generally the cells adjacent to the vertex, edge, or face it is defined on.) In other words, degrees of freedom  [2.x.784]  and  [2.x.785]  that are not defined on the same cell do not overlap, and consequently the matrix entry  [2.x.786]  will be zero.  (In some cases such as the Discontinuous Galerkin method, shape functions may also connect to neighboring cells through face integrals. But finite element methods do not generally couple shape functions beyond the immediate neighbors of a cell on which the function is defined.) 




[1.x.291] 

By default, the DoFHandler class enumerates degrees of freedom on a mesh in a rather random way; consequently, the sparsity pattern is also not optimized for any particular purpose. To show this, the code below will demonstrate a simple way to output the "sparsity pattern" that corresponds to a DoFHandler, i.e., an object that represents all of the potentially nonzero elements of a matrix one may build when discretizing a partial differential equation on a mesh and its DoFHandler. This lack of structure in the sparsity pattern will be apparent from the pictures we show below. 

For most applications and algorithms, the exact way in which degrees of freedom are numbered does not matter. For example, the Conjugate Gradient method we use to solve linear systems does not care. On the other hand, some algorithms do care: in particular, some preconditioners such as SSOR will work better if they can walk through degrees of freedom in a particular order, and it would be nice if we could just sort them in such a way that SSOR can iterate through them from zero to  [2.x.787]  in this order. Other examples include computing incomplete LU or Cholesky factorizations, or if we care about the block structure of matrices (see step-20 for an example). deal.II therefore has algorithms that can re-enumerate degrees of freedom in particular ways in namespace DoFRenumbering. Renumbering can be thought of as choosing a different, permuted basis of the finite element space. The sparsity pattern and matrices that result from this renumbering are therefore also simply a permutation of rows and columns compared to the ones we would get without explicit renumbering. 

In the program below, we will use the algorithm of Cuthill and McKee to do so. We will show the sparsity pattern for both the original enumeration of degrees of freedom and of the renumbered version below, in the [1.x.292]. 


examples/step-2/doc/results.dox 



[1.x.293] 

The program has, after having been run, produced two sparsity patterns. We can visualize them by opening the  [2.x.788]  files in a web browser. 

The results then look like this (every point denotes an entry which might be nonzero; of course the fact whether the entry actually is zero or not depends on the equation under consideration, but the indicated positions in the matrix tell us which shape functions can and which can't couple when discretizing a local, i.e. differential, equation):  [2.x.789]  

The different regions in the left picture, indicated by kinks in the lines and single dots on the left and top, represent the degrees of freedom on the different refinement levels of the triangulation.  As can be seen in the right picture, the sparsity pattern is much better clustered around the main diagonal of the matrix after renumbering. Although this might not be apparent, the number of nonzero entries is the same in both pictures, of course. 




[1.x.294] 

Just as with step-1, you may want to play with the program a bit to familiarize yourself with deal.II. For example, in the  [2.x.790]  function, we use linear finite elements (that's what the argument "1" to the FE_Q object is). Explore how the sparsity pattern changes if you use higher order elements, for example cubic or quintic ones (by using 3 and 5 as the respective arguments). 

You could also explore how the sparsity pattern changes by refining the mesh. You will see that not only the size of the matrix changes, but also its bandwidth (the distance from the diagonal of those nonzero elements of the matrix that are farthest away from the diagonal), though the ratio of bandwidth to size typically shrinks, i.e. the matrix clusters more around the diagonal. 

Another idea of experiments would be to try other renumbering strategies than Cuthill-McKee from the DoFRenumbering namespace and see how they affect the sparsity pattern. 

You can also visualize the output using [1.x.295] (one of the simpler visualization programs; maybe not the easiest to use since it is command line driven, but also universally available on all Linux and other Unix-like systems) by changing from  [2.x.791] : 

[1.x.296] 



Another practice based on [1.x.297] is trying to print out the mesh with locations and numbering of the support points. For that, you need to include header files for GridOut and MappingQ1. The code for this is: 

[1.x.298] 

After we run the code, we get a file called gnuplot.gpl. To view this file, we can run the following code in the command line: 

[1.x.299]. With that, you will get a picture similar to  [2.x.792]  depending on the mesh you are looking at. For more information, see  [2.x.793]  


examples/step-20/doc/intro.dox 

[1.x.300] 

[1.x.301] 

 [2.x.794]  

This program is devoted to two aspects: the use of mixed finite elements -- in particular Raviart-Thomas elements -- and using block matrices to define solvers, preconditioners, and nested versions of those that use the substructure of the system matrix. The equation we are going to solve is again the Poisson equation, though with a matrix-valued coefficient: 

[1.x.302] 

 [2.x.795]  is assumed to be uniformly positive definite, i.e., there is  [2.x.796]  such that the eigenvalues  [2.x.797]  of  [2.x.798]  satisfy  [2.x.799] . The use of the symbol  [2.x.800]  instead of the usual  [2.x.801]  for the solution variable will become clear in the next section. 

After discussing the equation and the formulation we are going to use to solve it, this introduction will cover the use of block matrices and vectors, the definition of solvers and preconditioners, and finally the actual test case we are going to solve. 

We are going to extend this tutorial program in step-21 to solve not only the mixed Laplace equation, but add another equation that describes the transport of a mixture of two fluids. 

The equations covered here fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.802]  module. 




[1.x.303] 

In the form above, the Poisson equation (i.e., the Laplace equation with a nonzero right hand side) is generally considered a good model equation for fluid flow in porous media. Of course, one typically models fluid flow through the [1.x.304] or, if fluid velocities are slow or the viscosity is large, the [1.x.305] (which we cover in step-22). In the first of these two models, the forces that act are inertia and viscous friction, whereas in the second it is only viscous friction -- i.e., forces that one fluid particle exerts on a nearby one. This is appropriate if you have free flow in a large domain, say a pipe, a river, or in the air. On the other hand, if the fluid is confined in pores, then friction forces exerted by the pore walls on the fluid become more and more important and internal viscous friction becomes less and less important. Modeling this then first leads to the [1.x.306] if both effects are important, and in the limit of very small pores to the [1.x.307]. The latter is just a different name for the Poisson or Laplace equation, connotating it with the area to which one wants to apply it: slow flow in a porous medium. In essence it says that the velocity is proportional to the negative pressure gradient that drives the fluid through the porous medium. 

The Darcy equation models this pressure that drives the flow. (Because the solution variable is a pressure, we here use the name  [2.x.803]  instead of the name  [2.x.804]  more commonly used for the solution of partial differential equations.) Typical applications of this view of the Laplace equation are then modeling groundwater flow, or the flow of hydrocarbons in oil reservoirs. In these applications,  [2.x.805]  is the permeability tensor, i.e., a measure for how much resistance the soil or rock matrix asserts on the fluid flow. 

In the applications named above, a desirable feature for a numerical scheme is that it should be locally conservative, i.e., that whatever flows into a cell also flows out of it (or the difference is equal to the integral over the source terms over each cell, if the sources are nonzero). However, as it turns out, the usual discretizations of the Laplace equation (such as those used in step-3, step-4, or step-6) do not satisfy this property. But, one can achieve this by choosing a different formulation of the problem and a particular combination of finite element spaces. 




[1.x.308] 

To this end, one first introduces a second variable, called the velocity,  [2.x.806] . By its definition, the velocity is a vector in the negative direction of the pressure gradient, multiplied by the permeability tensor. If the permeability tensor is proportional to the unit matrix, this equation is easy to understand and intuitive: the higher the permeability, the higher the velocity; and the velocity is proportional to the gradient of the pressure, going from areas of high pressure to areas of low pressure (thus the negative sign). 

With this second variable, one then finds an alternative version of the Laplace equation, called the [1.x.309]: 

[1.x.310] 

Here, we have multiplied the equation defining the velocity  [2.x.807]  by  [2.x.808]  because this makes the set of equations symmetric: one of the equations has the gradient, the second the negative divergence, and these two are of course adjoints of each other, resulting in a symmetric bilinear form and a consequently symmetric system matrix under the common assumption that  [2.x.809]  is a symmetric tensor. 

The weak formulation of this problem is found by multiplying the two equations with test functions and integrating some terms by parts: 

[1.x.311] 

where 

[1.x.312] 

Here,  [2.x.810]  is the outward normal vector at the boundary. Note how in this formulation, Dirichlet boundary values of the original problem are incorporated in the weak form. 

To be well-posed, we have to look for solutions and test functions in the space  [2.x.811]  for  [2.x.812] , [2.x.813] , and  [2.x.814]  for  [2.x.815] . It is a well-known fact stated in almost every book on finite element theory that if one chooses discrete finite element spaces for the approximation of  [2.x.816]  inappropriately, then the resulting discrete problem is instable and the discrete solution will not converge to the exact solution. (Some details on the problem considered here -- which falls in the class of "saddle-point problems" 

-- can be found on the Wikipedia page on the [1.x.313].) 

To overcome this, a number of different finite element pairs for  [2.x.817]  have been developed that lead to a stable discrete problem. One such pair is to use the Raviart-Thomas spaces  [2.x.818]  for the velocity  [2.x.819]  and discontinuous elements of class  [2.x.820]  for the pressure  [2.x.821] . For details about these spaces, we refer in particular to the book on mixed finite element methods by Brezzi and Fortin, but many other books on the theory of finite elements, for example the classic book by Brenner and Scott, also state the relevant results. In any case, with appropriate choices of function spaces, the discrete formulation reads as follows: Find  [2.x.822]  so that 

[1.x.314] 




Before continuing, let us briefly pause and show that the choice of function spaces above provides us with the desired local conservation property. In particular, because the pressure space consists of discontinuous piecewise polynomials, we can choose the test function  [2.x.823]  as the function that is equal to one on any given cell  [2.x.824]  and zero everywhere else. If we also choose  [2.x.825]  everywhere (remember that the weak form above has to hold for [1.x.315] discrete test functions  [2.x.826] ), then putting these choices of test functions into the weak formulation above implies in particular that 

[1.x.316] 

which we can of course write in more explicit form as 

[1.x.317] 

Applying the divergence theorem results in the fact that  [2.x.827]  has to satisfy, for every choice of cell  [2.x.828] , the relationship 

[1.x.318] 

If you now recall that  [2.x.829]  was the velocity, then the integral on the left is exactly the (discrete) flux across the boundary of the cell  [2.x.830] . The statement is then that the flux must be equal to the integral over the sources within  [2.x.831] . In particular, if there are no sources (i.e.,  [2.x.832]  in  [2.x.833] ), then the statement is that [1.x.319] flux is zero, i.e., whatever flows into a cell must flow out of it through some other part of the cell boundary. This is what we call [1.x.320] because it holds for every cell. 

On the other hand, the usual continuous  [2.x.834]  elements would not result in this kind of property when used for the pressure (as, for example, we do in step-43) because one can not choose a discrete test function  [2.x.835]  that is one on a cell  [2.x.836]  and zero everywhere else: It would be discontinuous and consequently not in the finite element space. (Strictly speaking, all we can say is that the proof above would not work for continuous elements. Whether these elements might still result in local conservation is a different question as one could think that a different kind of proof might still work; in reality, however, the property really does not hold.) 




[1.x.321] 

The deal.II library (of course) implements Raviart-Thomas elements  [2.x.837]  of arbitrary order  [2.x.838] , as well as discontinuous elements  [2.x.839] . If we forget about their particular properties for a second, we then have to solve a discrete problem 

[1.x.322] 

with the bilinear form and right hand side as stated above, and  [2.x.840] ,  [2.x.841] . Both  [2.x.842]  and  [2.x.843]  are from the space  [2.x.844] , where  [2.x.845]  is itself a space of  [2.x.846] -dimensional functions to accommodate for the fact that the flow velocity is vector-valued. The necessary question then is: how do we do this in a program? 

Vector-valued elements have already been discussed in previous tutorial programs, the first time and in detail in step-8. The main difference there was that the vector-valued space  [2.x.847]  is uniform in all its components: the  [2.x.848]  components of the displacement vector are all equal and from the same function space. What we could therefore do was to build  [2.x.849]  as the outer product of the  [2.x.850]  times the usual  [2.x.851]  finite element space, and by this make sure that all our shape functions have only a single non-zero vector component. Instead of dealing with vector-valued shape functions, all we did in step-8 was therefore to look at the (scalar) only non-zero component and use the  [2.x.852]  call to figure out which component this actually is. 

This doesn't work with Raviart-Thomas elements: following from their construction to satisfy certain regularity properties of the space  [2.x.853] , the shape functions of  [2.x.854]  are usually nonzero in all their vector components at once. For this reason, were  [2.x.855]  applied to determine the only nonzero component of shape function  [2.x.856] , an exception would be generated. What we really need to do is to get at  [2.x.857] all [2.x.858]  vector components of a shape function. In deal.II diction, we call such finite elements  [2.x.859] non-primitive [2.x.860] , whereas finite elements that are either scalar or for which every vector-valued shape function is nonzero only in a single vector component are called  [2.x.861] primitive [2.x.862] . 

So what do we have to do for non-primitive elements? To figure this out, let us go back in the tutorial programs, almost to the very beginnings. There, we learned that we use the  [2.x.863]  class to determine the values and gradients of shape functions at quadrature points. For example, we would call  [2.x.864]  to obtain the value of the  [2.x.865] th shape function on the quadrature point with number  [2.x.866] . Later, in step-8 and other tutorial programs, we learned that this function call also works for vector-valued shape functions (of primitive finite elements), and that it returned the value of the only non-zero component of shape function  [2.x.867]  at quadrature point  [2.x.868] . 

For non-primitive shape functions, this is clearly not going to work: there is no single non-zero vector component of shape function  [2.x.869] , and the call to  [2.x.870]  would consequently not make much sense. However, deal.II offers a second function call,  [2.x.871]  that returns the value of the  [2.x.872]  at quadrature point  [2.x.873]  is an index between zero and the number of vector components of the present finite element; for example, the element we will use to describe velocities and pressures is going to have  [2.x.874]  components. It is worth noting that this function call can also be used for primitive shape functions: it will simply return zero for all components except one; for non-primitive shape functions, it will in general return a non-zero value for more than just one component. 

We could now attempt to rewrite the bilinear form above in terms of vector components. For example, in 2d, the first term could be rewritten like this (note that  [2.x.875] ): 

[1.x.323] 

If we implemented this, we would get code like this: 

[1.x.324] 



This is, at best, tedious, error prone, and not dimension independent. There are obvious ways to make things dimension independent, but in the end, the code is simply not pretty. What would be much nicer is if we could simply extract the  [2.x.876]  and  [2.x.877]  components of a shape function  [2.x.878] . In the program we do that in the following way: 

[1.x.325] 



This is, in fact, not only the first term of the bilinear form, but the whole thing (sans boundary contributions). 

What this piece of code does is, given an  [2.x.879]  object, to extract the values of the first  [2.x.880]  components of shape function  [2.x.881]  at quadrature points  [2.x.882] , that is the velocity components of that shape function. Put differently, if we write shape functions  [2.x.883]  as the tuple  [2.x.884] , then the function returns the velocity part of this tuple. Note that the velocity is of course a  [2.x.885] -dimensional tensor, and that the function returns a corresponding object. Similarly, where we subscript with the pressure extractor, we extract the scalar pressure component. The whole mechanism is described in more detail in the  [2.x.886]  module. 

In practice, it turns out that we can do a bit better if we evaluate the shape functions, their gradients and divergences only once per outermost loop, and store the result, as this saves us a few otherwise repeated computations (it is possible to save even more repeated operations by calculating all relevant quantities in advance and then only inserting the results in the actual loop, see step-22 for a realization of that approach). The final result then looks like this, working in every space dimension: 

[1.x.326] 



This very closely resembles the form in which we have originally written down the bilinear form and right hand side. 

There is one final term that we have to take care of: the right hand side contained the term  [2.x.887] , constituting the weak enforcement of pressure boundary conditions. We have already seen in step-7 how to deal with face integrals: essentially exactly the same as with domain integrals, except that we have to use the FEFaceValues class instead of  [2.x.888] . To compute the boundary term we then simply have to loop over all boundary faces and integrate there. The mechanism works in the same way as above, i.e. the extractor classes also work on FEFaceValues objects: 

[1.x.327] 



You will find the exact same code as above in the sources for the present program. We will therefore not comment much on it below. 




[1.x.328] 

After assembling the linear system we are faced with the task of solving it. The problem here is that the matrix possesses two undesirable properties: 

- It is [1.x.329],   i.e., it has both positive and negative eigenvalues.   We don't want to prove this property here, but note that this is true   for all matrices of the form    [2.x.889]    such as the one here where  [2.x.890]  is positive definite. 

- The matrix has a zero block at the bottom right (there is no term in   the bilinear form that couples the pressure  [2.x.891]  with the   pressure test function  [2.x.892] ). 

At least it is symmetric, but the first issue above still means that the Conjugate Gradient method is not going to work since it is only applicable to problems in which the matrix is symmetric and positive definite. We would have to resort to other iterative solvers instead, such as MinRes, SymmLQ, or GMRES, that can deal with indefinite systems. However, then the next problem immediately surfaces: Due to the zero block, there are zeros on the diagonal and none of the usual, "simple" preconditioners (Jacobi, SSOR) will work as they require division by diagonal elements. 

For the matrix sizes we expect to run with this program, the by far simplest approach would be to just use a direct solver (in particular, the SparseDirectUMFPACK class that is bundled with deal.II). step-29 goes this route and shows that solving [1.x.330] linear system can be done in just 3 or 4 lines of code. 

But then, this is a tutorial: We teach how to do things. Consequently, in the following, we will introduce some techniques that can be used in cases like these. Namely, we will consider the linear system as not consisting of one large matrix and vectors, but we will want to decompose matrices into [1.x.331] that correspond to the individual operators that appear in the system. We note that the resulting solver is not optimal -- there are much better ways to efficiently compute the system, for example those explained in the results section of step-22 or the one we use in step-43 for a problem similar to the current one. Here, our goal is simply to introduce new solution techniques and how they can be implemented in deal.II. 




[1.x.332] 

In view of the difficulties using standard solvers and preconditioners mentioned above, let us take another look at the matrix. If we sort our degrees of freedom so that all velocity come before all pressure variables, then we can subdivide the linear system  [2.x.893]  into the following blocks: 

[1.x.333] 

where  [2.x.894]  are the values of velocity and pressure degrees of freedom, respectively,  [2.x.895]  is the mass matrix on the velocity space,  [2.x.896]  corresponds to the negative divergence operator, and  [2.x.897]  is its transpose and corresponds to the gradient. 

By block elimination, we can then re-order this system in the following way (multiply the first row of the system by  [2.x.898]  and then subtract the second row from it): 

[1.x.334] 

Here, the matrix  [2.x.899]  (called the [1.x.335] of  [2.x.900] ) is obviously symmetric and, owing to the positive definiteness of  [2.x.901]  and the fact that  [2.x.902]  has full column rank,  [2.x.903]  is also positive definite. 

Consequently, if we could compute  [2.x.904] , we could apply the Conjugate Gradient method to it. However, computing  [2.x.905]  is expensive because it requires us to compute the inverse of the (possibly large) matrix  [2.x.906] ; and  [2.x.907]  is in fact also a full matrix because even though  [2.x.908]  is sparse, its inverse  [2.x.909]  will generally be a dense matrix. On the other hand, the CG algorithm doesn't require us to actually have a representation of  [2.x.910] : It is sufficient to form matrix-vector products with it. We can do so in steps, using the fact that matrix products are associative (i.e., we can set parentheses in such a way that the product is more convenient to compute): To compute  [2.x.911] , we <ol>   [2.x.912]  compute  [2.x.913] ;   [2.x.914]  solve  [2.x.915]  for  [2.x.916] , using the CG method applied to the   positive definite and symmetric mass matrix  [2.x.917] ;   [2.x.918]  compute  [2.x.919]  to obtain  [2.x.920] .  [2.x.921]  Note how we evaluate the expression  [2.x.922]  right to left to avoid matrix-matrix products; this way, all we have to do is evaluate matrix-vector products. 

In the following, we will then have to come up with ways to represent the matrix  [2.x.923]  so that it can be used in a Conjugate Gradient solver, as well as to define ways in which we can precondition the solution of the linear system involving  [2.x.924] , and deal with solving linear systems with the matrix  [2.x.925]  (the second step above). 

 [2.x.926]  The key point in this consideration is to recognize that to implement an iterative solver such as CG or GMRES, we never actually need the actual [1.x.336] of a matrix! All that is required is that we can form matrix-vector products. The same is true for preconditioners. In deal.II we encode this requirement by only requiring that matrices and preconditioners given to solver classes have a  [2.x.927]  member function that does the matrix-vector product. How a class chooses to implement this function is not important to the solver. Consequently, classes can implement it by, for example, doing a sequence of products and linear solves as discussed above. 




[1.x.337] 

deal.II includes support for describing such linear operations in a very general way. This is done with the LinearOperator class that, like  [2.x.928]  "the MatrixType concept", defines a minimal interface for [1.x.338] a linear operation to a vector: 

[1.x.339] 

The key difference between a LinearOperator and an ordinary matrix is however that a LinearOperator does not allow any further access to the underlying object. All you can do with a LinearOperator is to apply its "action" to a vector! We take the opportunity to introduce the LinearOperator concept at this point because it is a very useful tool that allows you to construct complex solvers and preconditioners in a very intuitive manner. 

As a first example let us construct a LinearOperator object that represents  [2.x.929] . This means that whenever the  [2.x.930]  function of this operator is called it has to solve a linear system. This requires us to specify a solver (and corresponding) preconditioner. Assuming that  [2.x.931]  is a reference to the upper left block of the system matrix we can write: 

[1.x.340] 

Rather than using a SolverControl we use the ReductionControl class here that stops iterations when either an absolute tolerance is reached (for which we choose  [2.x.932] ) or when the residual is reduced by a certain factor (here,  [2.x.933] ). In contrast the SolverControl class only checks for absolute tolerances. We have to use ReductionControl in our case to work around a minor issue: The right hand sides that we  will feed to  [2.x.934]  are essentially formed by residuals that naturally decrease vastly in norm as the outer iterations progress. This makes control by an absolute tolerance very error prone. 

We now have a LinearOperator  [2.x.935]  that we can use to construct more complicated operators such as the Schur complement  [2.x.936] . Assuming that  [2.x.937]  is a reference to the upper right block constructing a LinearOperator  [2.x.938]  is a matter of two lines: 

[1.x.341] 

Here, the multiplication of three LinearOperator objects yields a composite object  [2.x.939]  function first applies  [2.x.940] , then  [2.x.941]  (i.e. solving an equation with  [2.x.942] ), and finally  [2.x.943]  to any given input vector. In that sense  [2.x.944]  is similar to the following code: 

[1.x.342] 

( [2.x.945]  are two temporary vectors). The key point behind this approach is the fact that we never actually create an inner product of matrices. Instead, whenever we have to perform a matrix vector multiplication with  [2.x.946]  we simply run all individual  [2.x.947]  operations in above sequence. 

 [2.x.948]  We could have achieved the same goal of creating a "matrix like" object by implementing a specialized class  [2.x.949]  that provides a suitable  [2.x.950]  function. Skipping over some details this might have looked like the following: 

[1.x.343] 

Even though both approaches are exactly equivalent, the LinearOperator class has a big advantage over this manual approach. It provides so-called [1.x.344][1.x.345]: Mathematically, we think about  [2.x.951]  as being the composite matrix  [2.x.952]  and the LinearOperator class allows you to write this out more or less verbatim, 

[1.x.346] 

The manual approach on the other hand obscures this fact. 

All that is left for us to do now is to form the right hand sides of the two equations defining  [2.x.953]  and  [2.x.954] , and then solve them with the Schur complement matrix and the mass matrix, respectively. For example the right hand side of the first equation reads  [2.x.955] . This could be implemented as follows: 

[1.x.347] 

Again, this is a perfectly valid approach, but the fact that deal.II requires us to manually resize the final and temporary vector, and that every operation takes up a new line makes this hard to read. This is the point where a second class in the linear operator framework can will help us. Similarly in spirit to LinearOperator, a PackagedOperation stores a "computation": 

[1.x.348] 

The class allows [1.x.349] of expressions involving vectors and linear operators. This is done by storing the computational expression and only performing the computation when either the object is converted to a vector object, or  [2.x.956]  (or  [2.x.957]  is invoked by hand. Assuming that  [2.x.958]  are the two vectors of the right hand side we can simply write: 

[1.x.350] 

Here,  [2.x.959]  is a PackagedOperation that [1.x.351] the computation we specified. It does not create a vector with the actual result immediately. 

With these prerequisites at hand, solving for  [2.x.960]  and  [2.x.961]  is a matter of creating another solver and inverse: 

[1.x.352] 



 [2.x.962]  The functionality that we developed in this example step by hand is already readily available in the library. Have a look at schur_complement(), condense_schur_rhs(), and postprocess_schur_solution(). 




[1.x.353] 

One may ask whether it would help if we had a preconditioner for the Schur complement  [2.x.963] . The general answer, as usual, is: of course. The problem is only, we don't know anything about this Schur complement matrix. We do not know its entries, all we know is its action. On the other hand, we have to realize that our solver is expensive since in each iteration we have to do one matrix-vector product with the Schur complement, which means that we have to do invert the mass matrix once in each iteration. 

There are different approaches to preconditioning such a matrix. On the one extreme is to use something that is cheap to apply and therefore has no real impact on the work done in each iteration. The other extreme is a preconditioner that is itself very expensive, but in return really brings down the number of iterations required to solve with  [2.x.964] . 

We will try something along the second approach, as much to improve the performance of the program as to demonstrate some techniques. To this end, let us recall that the ideal preconditioner is, of course,  [2.x.965] , but that is unattainable. However, how about 

[1.x.354] 

as a preconditioner? That would mean that every time we have to do one preconditioning step, we actually have to solve with  [2.x.966] . At first, this looks almost as expensive as solving with  [2.x.967]  right away. However, note that in the inner iteration, we do not have to calculate  [2.x.968] , but only the inverse of its diagonal, which is cheap. 

Thankfully, the LinearOperator framework makes this very easy to write out. We already used a Jacobi preconditioner ( [2.x.969] ) for the  [2.x.970]  matrix earlier. So all that is left to do is to write out how the approximate Schur complement should look like: 

[1.x.355] 

Note how this operator differs in simply doing one Jacobi sweep (i.e. multiplying with the inverses of the diagonal) instead of multiplying with the full  [2.x.971] . (This is how a single Jacobi preconditioner step with  [2.x.972]  is defined: it is the multiplication with the inverse of the diagonal of  [2.x.973] ; in other words, the operation  [2.x.974]  on a vector  [2.x.975]  is exactly what PreconditionJacobi does.) 

With all this we almost have the preconditioner completed: it should be the inverse of the approximate Schur complement. We implement this again by creating a linear operator with inverse_operator() function. This time however we would like to choose a relatively modest tolerance for the CG solver (that inverts  [2.x.976] ). The reasoning is that  [2.x.977] , so we actually do not need to invert it exactly. This, however creates a subtle problem:  [2.x.978]  will be used in the final outer CG iteration to create an orthogonal basis. But for this to work, it must be precisely the same linear operation for every invocation. We ensure this by using an IterationNumberControl that allows us to fix the number of CG iterations that are performed to a fixed small number (in our case 30): 

[1.x.356] 



That's all! 

Obviously, applying this inverse of the approximate Schur complement is a very expensive preconditioner, almost as expensive as inverting the Schur complement itself. We can expect it to significantly reduce the number of outer iterations required for the Schur complement. In fact it does: in a typical run on 7 times refined meshes using elements of order 0, the number of outer iterations drops from 592 to 39. On the other hand, we now have to apply a very expensive preconditioner 25 times. A better measure is therefore simply the run-time of the program: on a current laptop (as of January 2019), it drops from 3.57 to 2.05 seconds for this test case. That doesn't seem too impressive, but the savings become more pronounced on finer meshes and with elements of higher order. For example, an seven times refined mesh and using elements of order 2 (which amounts to about 0.4 million degrees of freedom) yields an improvement of 1134 to 83 outer iterations, at a runtime of 168 seconds to 40 seconds. Not earth shattering, but significant. 




[1.x.357] 

In this tutorial program, we will solve the Laplace equation in mixed formulation as stated above. Since we want to monitor convergence of the solution inside the program, we choose right hand side, boundary conditions, and the coefficient so that we recover a solution function known to us. In particular, we choose the pressure solution 

[1.x.358] 

and for the coefficient we choose the unit matrix  [2.x.979]  for simplicity. Consequently, the exact velocity satisfies 

[1.x.359] 

This solution was chosen since it is exactly divergence free, making it a realistic test case for incompressible fluid flow. By consequence, the right hand side equals  [2.x.980] , and as boundary values we have to choose  [2.x.981] . 

For the computations in this program, we choose  [2.x.982] . You can find the resulting solution in the [1.x.360], after the commented program. 


examples/step-20/doc/results.dox 



[1.x.361] 

[1.x.362] 


If we run the program as is, we get this output for the  [2.x.983]  mesh we use (for a total of 1024 cells with 1024 pressure degrees of freedom since we use piecewise constants, and 2112 velocities because the Raviart-Thomas element defines one degree per freedom per face and there are  [2.x.984]  faces parallel to the  [2.x.985] -axis and the same number parallel to the  [2.x.986] -axis): 

[1.x.363] 



The fact that the number of iterations is so small, of course, is due to the good (but expensive!) preconditioner we have developed. To get confidence in the solution, let us take a look at it. The following three images show (from left to right) the x-velocity, the y-velocity, and the pressure: 

 [2.x.987]  




Let us start with the pressure: it is highest at the left and lowest at the right, so flow will be from left to right. In addition, though hardly visible in the graph, we have chosen the pressure field such that the flow left-right flow first channels towards the center and then outward again. Consequently, the x-velocity has to increase to get the flow through the narrow part, something that can easily be seen in the left image. The middle image represents inward flow in y-direction at the left end of the domain, and outward flow in y-direction at the right end of the domain. 




As an additional remark, note how the x-velocity in the left image is only continuous in x-direction, whereas the y-velocity is continuous in y-direction. The flow fields are discontinuous in the other directions. This very obviously reflects the continuity properties of the Raviart-Thomas elements, which are, in fact, only in the space H(div) and not in the space  [2.x.988] . Finally, the pressure field is completely discontinuous, but that should not surprise given that we have chosen  [2.x.989]  as the finite element for that solution component. 




[1.x.364] 


The program offers two obvious places where playing and observing convergence is in order: the degree of the finite elements used (passed to the constructor of the  [2.x.990] ), and the refinement level (determined in  [2.x.991] ). What one can do is to change these values and observe the errors computed later on in the course of the program run. 




If one does this, one finds the following pattern for the  [2.x.992]  error in the pressure variable:  [2.x.993]  

The theoretically expected convergence orders are very nicely reflected by the experimentally observed ones indicated in the last row of the table. 




One can make the same experiment with the  [2.x.994]  error in the velocity variables:  [2.x.995]  The result concerning the convergence order is the same here. 




[1.x.365] 

[1.x.366] 

[1.x.367] 

Realistic flow computations for ground water or oil reservoir simulations will not use a constant permeability. Here's a first, rather simple way to change this situation: we use a permeability that decays very rapidly away from a central flowline until it hits a background value of 0.001. This is to mimic the behavior of fluids in sandstone: in most of the domain, the sandstone is homogeneous and, while permeable to fluids, not overly so; on the other stone, the stone has cracked, or faulted, along one line, and the fluids flow much easier along this large crack. Here is how we could implement something like this: 

[1.x.368] 

Remember that the function returns the inverse of the permeability tensor. 




With a significantly higher mesh resolution, we can visualize this, here with x- and y-velocity: 

 [2.x.996]  

It is obvious how fluids flow essentially only along the middle line, and not anywhere else. 




Another possibility would be to use a random permeability field. A simple way to achieve this would be to scatter a number of centers around the domain and then use a permeability field that is the sum of (negative) exponentials for each of these centers. Flow would then try to hop from one center of high permeability to the next one. This is an entirely unscientific attempt at describing a random medium, but one possibility to implement this behavior would look like this: 

[1.x.369] 



A piecewise constant interpolation of the diagonal elements of the inverse of this tensor (i.e., of  [2.x.997] ) looks as follows: 

 [2.x.998]  


With a permeability field like this, we would get x-velocities and pressures as follows: 

 [2.x.999]  

We will use these permeability fields again in step-21 and step-43. 




[1.x.370] 

As mentioned in the introduction, the Schur complement solver used here is not the best one conceivable (nor is it intended to be a particularly good one). Better ones can be found in the literature and can be built using the same block matrix techniques that were introduced here. We pick up on this theme again in step-22, where we first build a Schur complement solver for the Stokes equation as we did here, and then in the [1.x.371] section discuss better ways based on solving the system as a whole but preconditioning based on individual blocks. We will also come back to this in step-43. 


examples/step-21/doc/intro.dox 

[1.x.372] [1.x.373] 

This program grew out of a student project by Yan Li at Texas A&amp;M University. Most of the work for this program is by her. 

In this project, we propose a numerical simulation for two phase flow problems in porous media. This problem includes one elliptic equation and one nonlinear, time dependent transport equation. This is therefore also the first time-dependent tutorial program (besides the somewhat strange time-dependence of  [2.x.1000]  "step-18"). 

The equations covered here are an extension of the material already covered in step-20. In particular, they fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.1001]  module. 




[1.x.374] 

Modeling of two phase flow in porous media is important for both environmental remediation and the management of petroleum and groundwater reservoirs. Practical situations involving two phase flow include the dispersal of a nonaqueous phase liquid in an aquifer, or the joint movement of a mixture of fluids such as oil and water in a reservoir. Simulation models, if they are to provide realistic predictions, must accurately account for these effects. 

To derive the governing equations, consider two phase flow in a reservoir  [2.x.1002]  under the assumption that the movement of fluids is dominated by viscous effects; i.e. we neglect the effects of gravity, compressibility, and capillary pressure. Porosity will be considered to be constant. We will denote variables referring to either of the two phases using subscripts  [2.x.1003]  and  [2.x.1004] , short for water and oil. The derivation of the equations holds for other pairs of fluids as well, however. 

The velocity with which molecules of each of the two phases move is determined by Darcy's law that states that the velocity is proportional to the pressure gradient: 

[1.x.375] 

where  [2.x.1005]  is the velocity of phase  [2.x.1006] ,  [2.x.1007]  is the permeability tensor,  [2.x.1008]  is the relative permeability of phase  [2.x.1009] ,  [2.x.1010]  is the pressure and  [2.x.1011]  is the viscosity of phase  [2.x.1012] . Finally,  [2.x.1013]  is the saturation (volume fraction), i.e. a function with values between 0 and 1 indicating the composition of the mixture of fluids. In general, the coefficients  [2.x.1014]  may be spatially dependent variables, and we will always treat them as non-constant functions in the following. 

We combine Darcy's law with the statement of conservation of mass for each phase, 

[1.x.376] 

with a source term for each phase. By summing over the two phases, we can express the governing equations in terms of the so-called pressure equation: 

[1.x.377] 

Here,  [2.x.1015]  is the sum source term, and 

[1.x.378] 

is the total mobility. 

So far, this looks like an ordinary stationary, Poisson-like equation that we can solve right away with the techniques of the first few tutorial programs (take a look at step-6, for example, for something very similar). However, we have not said anything yet about the saturation, which of course is going to change as the fluids move around. 

The second part of the equations is the description of the dynamics of the saturation, i.e., how the relative concentration of the two fluids changes with time. The saturation equation for the displacing fluid (water) is given by the following conservation law: 

[1.x.379] 

which can be rewritten by using the product rule of the divergence operator in the previous equation: 

[1.x.380] 

Here,  [2.x.1016]  is the total influx introduced above, and  [2.x.1017]  is the flow rate of the displacing fluid (water). These two are related to the fractional flow  [2.x.1018]  in the following way: 

[1.x.381] 

where the fractional flow is often parameterized via the (heuristic) expression 

[1.x.382] 

Putting it all together yields the saturation equation in the following, advected form: 

[1.x.383] 

where  [2.x.1019]  is the total velocity 

[1.x.384] 

Note that the advection equation contains the term  [2.x.1020]  rather than  [2.x.1021]  to indicate that the saturation is not simply transported along; rather, since the two phases move with different velocities, the saturation can actually change even in the advected coordinate system. To see this, rewrite  [2.x.1022]  to observe that the [1.x.385] velocity with which the phase with saturation  [2.x.1023]  is transported is  [2.x.1024]  whereas the other phase is transported at velocity  [2.x.1025] .  [2.x.1026]  is consequently often referred to as the [1.x.386]. 

In summary, what we get are the following two equations: 

[1.x.387] 

Here,  [2.x.1027]  are now time dependent functions: while at every time instant the flow field is in equilibrium with the pressure (i.e. we neglect dynamic accelerations), the saturation is transported along with the flow and therefore changes over time, in turn affected the flow field again through the dependence of the first equation on  [2.x.1028] . 

This set of equations has a peculiar character: one of the two equations has a time derivative, the other one doesn't. This corresponds to the character that the pressure and velocities are coupled through an instantaneous constraint, whereas the saturation evolves over finite time scales. 

Such systems of equations are called Differential Algebraic Equations (DAEs), since one of the equations is a differential equation, the other is not (at least not with respect to the time variable) and is therefore an "algebraic" equation. (The notation comes from the field of ordinary differential equations, where everything that does not have derivatives with respect to the time variable is necessarily an algebraic equation.) This class of equations contains pretty well-known cases: for example, the time dependent Stokes and Navier-Stokes equations (where the algebraic constraint is that the divergence of the flow field,  [2.x.1029] , must be zero) as well as the time dependent Maxwell equations (here, the algebraic constraint is that the divergence of the electric displacement field equals the charge density,  [2.x.1030]  and that the divergence of the magnetic flux density is zero:  [2.x.1031] ); even the quasistatic model of step-18 falls into this category. We will see that the different character of the two equations will inform our discretization strategy for the two equations. 




[1.x.388] 

In the reservoir simulation community, it is common to solve the equations derived above by going back to the first order, mixed formulation. To this end, we re-introduce the total velocity  [2.x.1032]  and write the equations in the following form: 

[1.x.389] 

This formulation has the additional benefit that we do not have to express the total velocity  [2.x.1033]  appearing in the transport equation as a function of the pressure, but can rather take the primary variable for it. Given the saddle point structure of the first two equations and their similarity to the mixed Laplace formulation we have introduced in step-20, it will come as no surprise that we will use a mixed discretization again. 

But let's postpone this for a moment. The first business we have with these equations is to think about the time discretization. In reservoir simulation, there is a rather standard algorithm that we will use here. It first solves the pressure using an implicit equation, then the saturation using an explicit time stepping scheme. The algorithm is called IMPES for IMplicit Pressure Explicit Saturation and was first proposed a long time ago: by Sheldon et al. in 1959 and Stone and Gardner in 1961 (J. W. Sheldon, B. Zondek and W. T. Cardwell: [1.x.390], Trans. SPE AIME, 216 (1959), pp. 290-296; H. L. Stone and A. O. Gardner Jr: [1.x.391], Trans. SPE AIME, 222 (1961), pp. 92-104). In a slightly modified form, this algorithm can be written as follows: for each time step, solve 

[1.x.392] 

where  [2.x.1034]  is the length of a time step. Note how we solve the implicit pressure-velocity system that only depends on the previously computed saturation  [2.x.1035] , and then do an explicit time step for  [2.x.1036]  that only depends on the previously known  [2.x.1037]  and the just computed  [2.x.1038] . This way, we never have to iterate for the nonlinearities of the system as we would have if we used a fully implicit method. (In a more modern perspective, this should be seen as an "operator splitting" method. step-58 has a long description of the idea behind this.) 

We can then state the problem in weak form as follows, by multiplying each equation with test functions  [2.x.1039] ,  [2.x.1040] , and  [2.x.1041]  and integrating terms by parts: 

[1.x.393] 

Note that in the first term, we have to prescribe the pressure  [2.x.1042]  on the boundary  [2.x.1043]  as boundary values for our problem.  [2.x.1044]  denotes the unit outward normal vector to  [2.x.1045] , as usual. 

For the saturation equation, we obtain after integrating by parts 

[1.x.394] 

Using the fact that  [2.x.1046] , we can rewrite the cell term to get an equation as follows: 

[1.x.395] 

We introduce an object of type DiscreteTime in order to keep track of the current value of time and time step in the code. This class encapsulates many complexities regarding adjusting time step size and stopping at a specified final time. 




[1.x.396] 

In each time step, we then apply the mixed finite method of  [2.x.1047]  "step-20" to the velocity and pressure. To be well-posed, we choose Raviart-Thomas spaces  [2.x.1048]  for  [2.x.1049]  and discontinuous elements of class  [2.x.1050]  for  [2.x.1051] . For the saturation, we will also choose  [2.x.1052]  spaces. 

Since we have discontinuous spaces, we have to think about how to evaluate terms on the interfaces between cells, since discontinuous functions are not really defined there. In particular, we have to give a meaning to the last term on the left hand side of the saturation equation. To this end, let us define that we want to evaluate it in the following sense: 

[1.x.397] 

where  [2.x.1053]  denotes the inflow boundary and  [2.x.1054]  is the outflow part of the boundary. The quantities  [2.x.1055]  then correspond to the values of these variables on the present cell, whereas  [2.x.1056]  (needed on the inflow part of the boundary of  [2.x.1057] ) are quantities taken from the neighboring cell. Some more context on discontinuous element techniques and evaluation of fluxes can also be found in step-12 and step-12b. 




[1.x.398] 

The linear solvers used in this program are a straightforward extension of the ones used in step-20 (but without LinearOperator). Essentially, we simply have to extend everything from two to three solution components. If we use the discrete spaces mentioned above and put shape functions into the bilinear forms, we arrive at the following linear system to be solved for time step  [2.x.1058] : 

[1.x.399] 

where the individual matrices and vectors are defined as follows using shape functions  [2.x.1059]  (of type Raviart Thomas  [2.x.1060] ) for velocities and  [2.x.1061]  (of type  [2.x.1062] ) for both pressures and saturations: 

[1.x.400] 



 [2.x.1063]  Due to historical accidents, the role of matrices  [2.x.1064]  and  [2.x.1065]  has been reverted in this program compared to step-20. In other words, here  [2.x.1066]  refers to the divergence and  [2.x.1067]  to the gradient operators when it was the other way around in step-20. 

The system above presents a complication: Since the matrix  [2.x.1068]  depends on  [2.x.1069]  implicitly (the velocities are needed to determine which parts of the boundaries  [2.x.1070]  of cells are influx or outflux parts), we can only assemble this matrix after we have solved for the velocities. 

The solution scheme then involves the following steps: <ol>    [2.x.1071] Solve for the pressure  [2.x.1072]  using the Schur complement   technique introduced in step-20. 

   [2.x.1073] Solve for the velocity  [2.x.1074]  as also discussed in   step-20. 

   [2.x.1075] Compute the term  [2.x.1076] , using   the just computed velocities. 

   [2.x.1077] Solve for the saturation  [2.x.1078] .  [2.x.1079]  

In this scheme, we never actually build the matrix  [2.x.1080] , but rather generate the right hand side of the third equation once we are ready to do so. 

In the program, we use a variable  [2.x.1081]  to store the solution of the present time step. At the end of each step, we copy its content, i.e. all three of its block components, into the variable  [2.x.1082]  for use in the next time step. 




[1.x.401] 

A general rule of thumb in hyperbolic transport equations like the equation we have to solve for the saturation equation is that if we use an explicit time stepping scheme, then we should use a time step such that the distance that a particle can travel within one time step is no larger than the diameter of a single cell. In other words, here, we should choose 

[1.x.402] 

Fortunately, we are in a position where we can do that: we only need the time step when we want to assemble the right hand side of the saturation equation, which is after we have already solved for  [2.x.1083] . All we therefore have to do after solving for the velocity is to loop over all quadrature points in the domain and determine the maximal magnitude of the velocity. We can then set the time step for the saturation equation to 

[1.x.403] 



Why is it important to do this? If we don't, then we will end up with lots of places where our saturation is larger than one or less than zero, as can easily be verified. (Remember that the saturation corresponds to something like the water fraction in the fluid mixture, and therefore must physically be between 0 and 1.) On the other hand, if we choose our time step according to the criterion listed above, this only happens very very infrequently &mdash; in fact only once for the entire run of the program. However, to be on the safe side, however, we run a function  [2.x.1084]  at the end of each time step, that simply projects the saturation back onto the interval  [2.x.1085] , should it have gotten out of the physical range. This is useful since the functions  [2.x.1086]  and  [2.x.1087]  do not represent anything physical outside this range, and we should not expect the program to do anything useful once we have negative saturations or ones larger than one. 

Note that we will have similar restrictions on the time step also in step-23 and step-24 where we solve the time dependent wave equation, another hyperbolic problem. We will also come back to the issue of time step choice below in the section on [1.x.404]. 




[1.x.405] 

For simplicity, this program assumes that there is no source,  [2.x.1088] , and that the heterogeneous porous medium is isotropic  [2.x.1089] . The first one of these is a realistic assumption in oil reservoirs: apart from injection and production wells, there are usually no mechanisms for fluids to appear or disappear out of the blue. The second one is harder to justify: on a microscopic level, most rocks are isotropic, because they consist of a network of interconnected pores. However, this microscopic scale is out of the range of today's computer simulations, and we have to be content with simulating things on the scale of meters. On that scale, however, fluid transport typically happens through a network of cracks in the rock, rather than through pores. However, cracks often result from external stress fields in the rock layer (for example from tectonic faulting) and the cracks are therefore roughly aligned. This leads to a situation where the permeability is often orders of magnitude larger in the direction parallel to the cracks than perpendicular to the cracks. A problem typically faces in reservoir simulation, however, is that the modeler doesn't know the direction of cracks because oil reservoirs are not accessible to easy inspection. The only solution in that case is to assume an effective, isotropic permeability. 

Whatever the matter, both of these restrictions, no sources and isotropy, would be easy to lift with a few lines of code in the program. 

Next, for simplicity, our numerical simulation will be done on the unit cell  [2.x.1090]  for  [2.x.1091] . Our initial conditions are  [2.x.1092] ; in the oil reservoir picture, where  [2.x.1093]  would indicate the water saturation, this means that the reservoir contains pure oil at the beginning. Note that we do not need any initial conditions for pressure or velocity, since the equations do not contain time derivatives of these variables. Finally, we impose the following pressure boundary conditions: 

[1.x.406] 

Since the pressure and velocity solve a mixed form Poisson equation, the imposed pressure leads to a resulting flow field for the velocity. On the other hand, this flow field determines whether a piece of the boundary is of inflow or outflow type, which is of relevance because we have to impose boundary conditions for the saturation on the inflow part of the boundary, 

[1.x.407] 

On this inflow boundary, we impose the following saturation values: 

[1.x.408] 

In other words, we have pure water entering the reservoir at the left, whereas the other parts of the boundary are in contact with undisturbed parts of the reservoir and whenever influx occurs on these boundaries, pure oil will enter. 

In our simulations, we choose the total mobility as 

[1.x.409] 

where we use  [2.x.1094]  for the viscosity. In addition, the fractional flow of water is given by 

[1.x.410] 



 [2.x.1095]  Coming back to this testcase in step-43 several years later revealed an oddity in the setup of this testcase. To this end, consider that we can rewrite the advection equation for the saturation as  [2.x.1096] . Now, at the initial time, we have  [2.x.1097] , and with the given choice of function  [2.x.1098] , we happen to have  [2.x.1099] . In other words, at  [2.x.1100] , the equation reduces to  [2.x.1101]  for all  [2.x.1102] , so the saturation is zero everywhere and it is going to stay zero everywhere! This is despite the fact that  [2.x.1103]  is not necessarily zero: the combined fluid is moving, but we've chosen our partial flux  [2.x.1104]  in such a way that infinitesimal amounts of wetting fluid also only move at infinitesimal speeds (i.e., they stick to the medium more than the non-wetting phase in which they are embedded). That said, how can we square this with the knowledge that wetting fluid is invading from the left, leading to the flow patterns seen in the [1.x.411]? That's where we get into mathematics: Equations like the transport equation we are considering here have infinitely many solutions, but only one of them is physical: the one that results from the so-called viscosity limit, called the [1.x.412]. The thing is that with discontinuous elements we arrive at this viscosity limit because using a numerical flux introduces a finite amount of artificial viscosity into the numerical scheme. On the other hand, in step-43, we use an artificial viscosity that is proportional to  [2.x.1105]  on every cell, which at the initial time is zero. Thus, the saturation there is zero and remains zero; the solution we then get is [1.x.413] solution of the advection equation, but the method does not converge to the viscosity solution without further changes. We will therefore use a different initial condition in that program. 


Finally, to come back to the description of the testcase, we will show results for computations with the two permeability functions introduced at the end of the results section of  [2.x.1106]  "step-20":  [2.x.1107]     [2.x.1108] A function that models a single, winding crack that snakes through the   domain. In analogy to step-20, but taking care of the slightly   different geometry we have here, we describe this by the following function:   [1.x.414] 

  Taking the maximum is necessary to ensure that the ratio between maximal and   minimal permeability remains bounded. If we don't do that, permeabilities   will span many orders of magnitude. On the other hand, the ratio between   maximal and minimal permeability is a factor in the condition number of the   Schur complement matrix, and if too large leads to problems for which our   linear solvers will no longer converge properly. 

   [2.x.1109] A function that models a somewhat random medium. Here, we choose   [1.x.415] 

  where the centers  [2.x.1110]  are  [2.x.1111]  randomly chosen locations inside   the domain. This function models a domain in which there are  [2.x.1112]  centers of   higher permeability (for example where rock has cracked) embedded in a   matrix of more pristine, unperturbed background rock. Note that here we have   cut off the permeability function both above and below to ensure a bounded   condition number.  [2.x.1113]  


examples/step-21/doc/results.dox 



[1.x.416] 

The code as presented here does not actually compute the results found on the web page. The reason is, that even on a decent computer it runs more than a day. If you want to reproduce these results, modify the end time of the DiscreteTime object to `250` within the constructor of TwoPhaseFlowProblem. 

If we run the program, we get the following kind of output: 

[1.x.417] 

As we can see, the time step is pretty much constant right from the start, which indicates that the velocities in the domain are not strongly dependent on changes in saturation, although they certainly are through the factor  [2.x.1114]  in the pressure equation. 

Our second observation is that the number of CG iterations needed to solve the pressure Schur complement equation drops from 22 to 17 between the first and the second time step (in fact, it remains around 17 for the rest of the computations). The reason is actually simple: Before we solve for the pressure during a time step, we don't reset the  [2.x.1115]  variable to zero. The pressure (and the other variables) therefore have the previous time step's values at the time we get into the CG solver. Since the velocities and pressures don't change very much as computations progress, the previous time step's pressure is actually a good initial guess for this time step's pressure. Consequently, the number of iterations we need once we have computed the pressure once is significantly reduced. 

The final observation concerns the number of iterations needed to solve for the saturation, i.e. one. This shouldn't surprise us too much: the matrix we have to solve with is the mass matrix. However, this is the mass matrix for the  [2.x.1116]  element of piecewise constants where no element couples with the degrees of freedom on neighboring cells. The matrix is therefore a diagonal one, and it is clear that we should be able to invert this matrix in a single CG iteration. 


With all this, here are a few movies that show how the saturation progresses over time. First, this is for the single crack model, as implemented in the  [2.x.1117]  class: 

 [2.x.1118]  

As can be seen, the water rich fluid snakes its way mostly along the high-permeability zone in the middle of the domain, whereas the rest of the domain is mostly impermeable. This and the next movie are generated using  [2.x.1119] , leading to a  [2.x.1120]  mesh with some 16,000 cells and about 66,000 unknowns in total. 


The second movie shows the saturation for the random medium model of class  [2.x.1121] , where we have randomly distributed centers of high permeability and fluid hops from one of these zones to the next: 

 [2.x.1122]  


Finally, here is the same situation in three space dimensions, on a mesh with  [2.x.1123] , which produces a mesh of some 32,000 cells and 167,000 degrees of freedom: 

 [2.x.1124]  

To repeat these computations, all you have to do is to change the line 

[1.x.418] 

in the main function to 

[1.x.419] 

The visualization uses a cloud technique, where the saturation is indicated by colored but transparent clouds for each cell. This way, one can also see somewhat what happens deep inside the domain. A different way of visualizing would have been to show isosurfaces of the saturation evolving over time. There are techniques to plot isosurfaces transparently, so that one can see several of them at the same time like the layers of an onion. 

So why don't we show such isosurfaces? The problem lies in the way isosurfaces are computed: they require that the field to be visualized is continuous, so that the isosurfaces can be generated by following contours at least across a single cell. However, our saturation field is piecewise constant and discontinuous. If we wanted to plot an isosurface for a saturation  [2.x.1125] , chances would be that there is no single point in the domain where that saturation is actually attained. If we had to define isosurfaces in that context at all, we would have to take the interfaces between cells, where one of the two adjacent cells has a saturation greater than and the other cell a saturation less than 0.5. However, it appears that most visualization programs are not equipped to do this kind of transformation. 


[1.x.420] 

[1.x.421] 

There are a number of areas where this program can be improved. Three of them are listed below. All of them are, in fact, addressed in a tutorial program that forms the continuation of the current one: step-43. 




[1.x.422] 

At present, the program is not particularly fast: the 2d random medium computation took about a day for the 1,000 or so time steps. The corresponding 3d computation took almost two days for 800 time steps. The reason why it isn't faster than this is twofold. First, we rebuild the entire matrix in every time step, although some parts such as the  [2.x.1126] ,  [2.x.1127] , and  [2.x.1128]  blocks never change. 

Second, we could do a lot better with the solver and preconditioners. Presently, we solve the Schur complement  [2.x.1129]  with a CG method, using  [2.x.1130]  as a preconditioner. Applying this preconditioner is expensive, since it involves solving a linear system each time. This may have been appropriate for  [2.x.1131]  "step-20", where we have to solve the entire problem only once. However, here we have to solve it hundreds of times, and in such cases it is worth considering a preconditioner that is more expensive to set up the first time, but cheaper to apply later on. 

One possibility would be to realize that the matrix we use as preconditioner,  [2.x.1132]  is still sparse, and symmetric on top of that. If one looks at the flow field evolve over time, we also see that while  [2.x.1133]  changes significantly over time, the pressure hardly does and consequently  [2.x.1134] . In other words, the matrix for the first time step should be a good preconditioner also for all later time steps.  With a bit of back-and-forthing, it isn't hard to actually get a representation of it as a SparseMatrix object. We could then hand it off to the SparseMIC class to form a sparse incomplete Cholesky decomposition. To form this decomposition is expensive, but we have to do it only once in the first time step, and can then use it as a cheap preconditioner in the future. We could do better even by using the SparseDirectUMFPACK class that produces not only an incomplete, but a complete decomposition of the matrix, which should yield an even better preconditioner. 

Finally, why use the approximation  [2.x.1135]  to precondition  [2.x.1136] ? The latter matrix, after all, is the mixed form of the Laplace operator on the pressure space, for which we use linear elements. We could therefore build a separate matrix  [2.x.1137]  on the side that directly corresponds to the non-mixed formulation of the Laplacian, for example using the bilinear form  [2.x.1138] . We could then form an incomplete or complete decomposition of this non-mixed matrix and use it as a preconditioner of the mixed form. 

Using such techniques, it can reasonably be expected that the solution process will be faster by at least an order of magnitude. 




[1.x.423] 

In the introduction we have identified the time step restriction 

[1.x.424] 

that has to hold globally, i.e. for all  [2.x.1139] . After discretization, we satisfy it by choosing 

[1.x.425] 



This restriction on the time step is somewhat annoying: the finer we make the mesh the smaller the time step; in other words, we get punished twice: each time step is more expensive to solve and we have to do more time steps. 

This is particularly annoying since the majority of the additional work is spent solving the implicit part of the equations, i.e. the pressure-velocity system, whereas it is the hyperbolic transport equation for the saturation that imposes the time step restriction. 

To avoid this bottleneck, people have invented a number of approaches. For example, they may only re-compute the pressure-velocity field every few time steps (or, if you want, use different time step sizes for the pressure/velocity and saturation equations). This keeps the time step restriction on the cheap explicit part while it makes the solution of the implicit part less frequent. Experiments in this direction are certainly worthwhile; one starting point for such an approach is the paper by Zhangxin Chen, Guanren Huan and Baoyan Li: [1.x.426], Transport in Porous Media, 54 (2004), pp. 361&mdash;376. There are certainly many other papers on this topic as well, but this one happened to land on our desk a while back. 




[1.x.427] 

Adaptivity would also clearly help. Looking at the movies, one clearly sees that most of the action is confined to a relatively small part of the domain (this particularly obvious for the saturation, but also holds for the velocities and pressures). Adaptivity can therefore be expected to keep the necessary number of degrees of freedom low, or alternatively increase the accuracy. 

On the other hand, adaptivity for time dependent problems is not a trivial thing: we would have to change the mesh every few time steps, and we would have to transport our present solution to the next mesh every time we change it (something that the SolutionTransfer class can help with). These are not insurmountable obstacles, but they do require some additional coding and more than we felt comfortable was worth packing into this tutorial program. 


examples/step-22/doc/intro.dox 

 [2.x.1140]  

[1.x.428] 




[1.x.429] 

[1.x.430] 

This program deals with the Stokes system of equations which reads as follows in non-dimensionalized form: 

[1.x.431] 

where  [2.x.1141]  denotes the velocity of a fluid,  [2.x.1142]  is its pressure,  [2.x.1143]  are external forces, and  [2.x.1144]   is the rank-2 tensor of symmetrized gradients; a component-wise definition of it is  [2.x.1145] . 

The Stokes equations describe the steady-state motion of a slow-moving, viscous fluid such as honey, rocks in the earth mantle, or other cases where inertia does not play a significant role. If a fluid is moving fast enough that inertia forces are significant compared to viscous friction, the Stokes equations are no longer valid; taking into account inertia effects then leads to the nonlinear Navier-Stokes equations. However, in this tutorial program, we will focus on the simpler Stokes system. 

Note that when deriving the more general compressible Navier-Stokes equations, the diffusion is modeled as the divergence of the stress tensor 

[1.x.432] 

where  [2.x.1146]  is the viscosity of the fluid. With the assumption of  [2.x.1147]  (assume constant viscosity and non-dimensionalize the equation by dividing out  [2.x.1148] ) and assuming incompressibility ( [2.x.1149] ), we arrive at the formulation from above: 

[1.x.433] 

A different formulation uses the Laplace operator ( [2.x.1150] ) instead of the symmetrized gradient. A big difference here is that the different components of the velocity do not couple. If you assume additional regularity of the solution  [2.x.1151]  (second partial derivatives exist and are continuous), the formulations are equivalent: 

[1.x.434] 

This is because the  [2.x.1152] th entry of   [2.x.1153]  is given by: 

[1.x.435] 

If you can not assume the above mentioned regularity, or if your viscosity is not a constant, the equivalence no longer holds. Therefore, we decided to stick with the more physically accurate symmetric tensor formulation in this tutorial. 


To be well-posed, we will have to add boundary conditions to the equations. What boundary conditions are readily possible here will become clear once we discuss the weak form of the equations. 

The equations covered here fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.1154]  module. 




[1.x.436] 

The weak form of the equations is obtained by writing it in vector form as 

[1.x.437] 

forming the dot product from the left with a vector-valued test function  [2.x.1155]  and integrating over the domain  [2.x.1156] , yielding the following set of equations: 

[1.x.438] 

which has to hold for all test functions  [2.x.1157] . 

A generally good rule of thumb is that if one [1.x.439] reduce how many derivatives are taken on any variable in the formulation, then one [1.x.440] in fact do that using integration by parts. (This is motivated by the theory of [1.x.441], and in particular the difference between strong and [1.x.442].) We have already done that for the Laplace equation, where we have integrated the second derivative by parts to obtain the weak formulation that has only one derivative on both test and trial function. 

In the current context, we integrate by parts the second term: 

[1.x.443] 

Likewise, we integrate by parts the first term to obtain 

[1.x.444] 

where the scalar product between two tensor-valued quantities is here defined as 

[1.x.445] 

Using this, we have now reduced the requirements on our variables to first derivatives for  [2.x.1158]  and no derivatives at all for  [2.x.1159] . 

Because the scalar product between a general tensor like  [2.x.1160]  and a symmetric tensor like  [2.x.1161]  equals the scalar product between the symmetrized forms of the two, we can also write the bilinear form above as follows: 

[1.x.446] 

We will deal with the boundary terms in the next section, but it is already clear from the domain terms 

[1.x.447] 

of the bilinear form that the Stokes equations yield a symmetric bilinear form, and consequently a symmetric (if indefinite) system matrix. 




[1.x.448] 

 [2.x.1162]  ( [2.x.1163]  

The weak form just derived immediately presents us with different possibilities for imposing boundary conditions: <ol>  [2.x.1164] Dirichlet velocity boundary conditions: On a part      [2.x.1165]  we may impose Dirichlet conditions     on the velocity  [2.x.1166] : 

    [1.x.449] 

    Because test functions  [2.x.1167]  come from the tangent space of     the solution variable, we have that  [2.x.1168]  on  [2.x.1169]      and consequently that     [1.x.450] 

    In other words, as usual, strongly imposed boundary values do not     appear in the weak form. 

    It is noteworthy that if we impose Dirichlet boundary values on the entire     boundary, then the pressure is only determined up to a constant. An     algorithmic realization of that would use similar tools as have been seen in     step-11. 

 [2.x.1170] Neumann-type or natural boundary conditions: On the rest of the boundary      [2.x.1171] , let us re-write the     boundary terms as follows:     [1.x.451] 

    In other words, on the Neumann part of the boundary we can     prescribe values for the total stress:     [1.x.452] 

    If the boundary is subdivided into Dirichlet and Neumann parts      [2.x.1172] , this then leads to the following weak form:     [1.x.453] 




 [2.x.1173] Robin-type boundary conditions: Robin boundary conditions are a mixture of     Dirichlet and Neumann boundary conditions. They would read     [1.x.454] 

    with a rank-2 tensor (matrix)  [2.x.1174] . The associated weak form is     [1.x.455] 



 [2.x.1175] Partial boundary conditions: It is possible to combine Dirichlet and     Neumann boundary conditions by only enforcing each of them for certain     components of the velocity. For example, one way to impose artificial     boundary conditions is to require that the flow is perpendicular to the     boundary, i.e. the tangential component  [2.x.1176]  be zero, thereby constraining      [2.x.1177] -1 components of the velocity. The remaining component can     be constrained by requiring that the normal component of the normal     stress be zero, yielding the following set of boundary conditions:     [1.x.456] 



    An alternative to this is when one wants the flow to be [1.x.457]     rather than perpendicular to the boundary (in deal.II, the      [2.x.1178]  function can do this for     you). This is frequently the case for problems with a free boundary     (e.g. at the surface of a river or lake if vertical forces of the flow are     not large enough to actually deform the surface), or if no significant     friction is exerted by the boundary on the fluid (e.g. at the interface     between earth mantle and earth core where two fluids meet that are     stratified by different densities but that both have small enough     viscosities to not introduce much tangential stress on each other).     In formulas, this means that     [1.x.458] 

    the first condition (which needs to be imposed strongly) fixing a single     component of the velocity, with the second (which would be enforced in the     weak form) fixing the remaining two components.  [2.x.1179]  

Despite this wealth of possibilities, we will only use Dirichlet and (homogeneous) Neumann boundary conditions in this tutorial program. 




[1.x.459] 

As developed above, the weak form of the equations with Dirichlet and Neumann boundary conditions on  [2.x.1180]  and  [2.x.1181]  reads like this: find  [2.x.1182]  so that 

[1.x.460] 

for all test functions  [2.x.1183] . 

These equations represent a symmetric [1.x.461]. It is well known that then a solution only exists if the function spaces in which we search for a solution have to satisfy certain conditions, typically referred to as the Babuska-Brezzi or Ladyzhenskaya-Babuska-Brezzi (LBB) conditions. The continuous function spaces above satisfy these. However, when we discretize the equations by replacing the continuous variables and test functions by finite element functions in finite dimensional spaces  [2.x.1184] , we have to make sure that  [2.x.1185]  also satisfy the LBB conditions. This is similar to what we had to do in step-20. 

For the Stokes equations, there are a number of possible choices to ensure that the finite element spaces are compatible with the LBB condition. A simple and accurate choice that we will use here is  [2.x.1186] , i.e. use elements one order higher for the velocities than for the pressures. 

This then leads to the following discrete problem: find  [2.x.1187]  so that 

[1.x.462] 

for all test functions  [2.x.1188] . Assembling the linear system associated with this problem follows the same lines used in  [2.x.1189]  "step-20", step-21, and explained in detail in the  [2.x.1190]  module. 




[1.x.463] 

The weak form of the discrete equations naturally leads to the following linear system for the nodal values of the velocity and pressure fields: 

[1.x.464] 

Like in step-20 and step-21, we will solve this system of equations by forming the Schur complement, i.e. we will first find the solution  [2.x.1191]  of 

[1.x.465] 

and then 

[1.x.466] 

The way we do this is pretty much exactly like we did in these previous tutorial programs, i.e. we use the same classes  [2.x.1192]  and  [2.x.1193]  again. There are two significant differences, however: 

<ol>  [2.x.1194]  First, in the mixed Laplace equation we had to deal with the question of how to precondition the Schur complement  [2.x.1195] , which was spectrally equivalent to the Laplace operator on the pressure space (because  [2.x.1196]  represents the gradient operator,  [2.x.1197]  its adjoint  [2.x.1198] , and  [2.x.1199]  the identity (up to the material parameter  [2.x.1200] ), so  [2.x.1201]  is something like  [2.x.1202] ). Consequently, the matrix is badly conditioned for small mesh sizes and we had to come up with an elaborate preconditioning scheme for the Schur complement. 

 [2.x.1203]  Second, every time we multiplied with  [2.x.1204]  we had to solve with the mass matrix  [2.x.1205] . This wasn't particularly difficult, however, since the mass matrix is always well conditioned and so simple to invert using CG and a little bit of preconditioning.  [2.x.1206]  In other words, preconditioning the inner solver for  [2.x.1207]  was simple whereas preconditioning the outer solver for  [2.x.1208]  was complicated. 

Here, the situation is pretty much exactly the opposite. The difference stems from the fact that the matrix at the heart of the Schur complement does not stem from the identity operator but from a variant of the Laplace operator,  [2.x.1209]  (where  [2.x.1210]  is the symmetric gradient) acting on a vector field. In the investigation of this issue we largely follow the paper D. Silvester and A. Wathen: "Fast iterative solution of stabilised Stokes systems part II. Using general block preconditioners." (SIAM J. Numer. Anal., 31 (1994), pp. 1352-1367), which is available online [1.x.467]. Principally, the difference in the matrix at the heart of the Schur complement has two consequences: 

<ol>  [2.x.1211]  First, it makes the outer preconditioner simple: the Schur complement corresponds to the operator  [2.x.1212]  on the pressure space; forgetting about the fact that we deal with symmetric gradients instead of the regular one, the Schur complement is something like  [2.x.1213] , which, even if not mathematically entirely concise, is spectrally equivalent to the identity operator (a heuristic argument would be to commute the operators into  [2.x.1214] ). It turns out that it isn't easy to solve this Schur complement in a straightforward way with the CG method: using no preconditioner, the condition number of the Schur complement matrix depends on the size ratios of the largest to the smallest cells, and one still needs on the order of 50-100 CG iterations. However, there is a simple cure: precondition with the mass matrix on the pressure space and we get down to a number between 5-15 CG iterations, pretty much independently of the structure of the mesh (take a look at the [1.x.468] of this program to see that indeed the number of CG iterations does not change as we refine the mesh). 

So all we need in addition to what we already have is the mass matrix on the pressure variables and we will store it in a separate object. 




 [2.x.1215]  While the outer preconditioner has become simpler compared to the mixed Laplace case discussed in step-20, the issue of the inner solver has become more complicated. In the mixed Laplace discretization, the Schur complement has the form  [2.x.1216] . Thus, every time we multiplied with the Schur complement, we had to solve a linear system  [2.x.1217] ; this isn't too complicated there, however, since the mass matrix  [2.x.1218]  on the pressure space is well-conditioned. 


On the other hand, for the Stokes equation we consider here, the Schur complement is  [2.x.1219]  where the matrix  [2.x.1220]  is related to the Laplace operator (it is, in fact, the matrix corresponding to the bilinear form  [2.x.1221] ). Thus, solving with  [2.x.1222]  is a lot more complicated: the matrix is badly conditioned and we know that we need many iterations unless we have a very good preconditioner. What is worse, we have to solve with  [2.x.1223]  every time we multiply with the Schur complement, which is 5-15 times using the preconditioner described above. 

Because we have to solve with  [2.x.1224]  several times, it pays off to spend a bit more time once to create a good preconditioner for this matrix. So here's what we're going to do: if in 2d, we use the ultimate preconditioner, namely a direct sparse LU decomposition of the matrix. This is implemented using the SparseDirectUMFPACK class that uses the UMFPACK direct solver to compute the decomposition. To use it, you will have to build deal.II with UMFPACK support (which is the default); see the [1.x.469] for instructions. With this, the inner solver converges in one iteration. 

In 2d, we can do this sort of thing because even reasonably large problems rarely have more than a few 100,000 unknowns with relatively few nonzero entries per row. Furthermore, the bandwidth of matrices in 2d is  [2.x.1225]  and therefore moderate. For such matrices, sparse factors can be computed in a matter of a few seconds. (As a point of reference, computing the sparse factors of a matrix of size  [2.x.1226]  and bandwidth  [2.x.1227]  takes  [2.x.1228]  operations. In 2d, this is  [2.x.1229] ; though this is a higher complexity than, for example, assembling the linear system which takes  [2.x.1230] , the constant for computing the decomposition is so small that it doesn't become the dominating factor in the entire program until we get to very large %numbers of unknowns in the high 100,000s or more.) 

The situation changes in 3d, because there we quickly have many more unknowns and the bandwidth of matrices (which determines the number of nonzero entries in sparse LU factors) is  [2.x.1231] , and there are many more entries per row as well. This makes using a sparse direct solver such as UMFPACK inefficient: only for problem sizes of a few 10,000 to maybe 100,000 unknowns can a sparse decomposition be computed using reasonable time and memory resources. 

What we do in that case is to use an incomplete LU decomposition (ILU) as a preconditioner, rather than actually computing complete LU factors. As it so happens, deal.II has a class that does this: SparseILU. Computing the ILU takes a time that only depends on the number of nonzero entries in the sparse matrix (or that we are willing to fill in the LU factors, if these should be more than the ones in the matrix), but is independent of the bandwidth of the matrix. It is therefore an operation that can efficiently also be computed in 3d. On the other hand, an incomplete LU decomposition, by definition, does not represent an exact inverse of the matrix  [2.x.1232] . Consequently, preconditioning with the ILU will still require more than one iteration, unlike preconditioning with the sparse direct solver. The inner solver will therefore take more time when multiplying with the Schur complement: an unavoidable trade-off.  [2.x.1233]  

In the program below, we will make use of the fact that the SparseILU and SparseDirectUMFPACK classes have a very similar interface and can be used interchangeably. All that we need is a switch class that, depending on the dimension, provides a type that is either of the two classes mentioned above. This is how we do that: 

[1.x.470] 



From here on, we can refer to the type <code>typename  [2.x.1234]  and automatically get the correct preconditioner class. Because of the similarity of the interfaces of the two classes, we will be able to use them interchangeably using the same syntax in all places. 




[1.x.471] 

The discussions above showed *one* way in which the linear system that results from the Stokes equations can be solved, and because the tutorial programs are teaching tools that makes sense. But is this the way this system of equations *should* be solved? 

The answer to this is no. The primary bottleneck with the approach, already identified above, is that we have to repeatedly solve linear systems with  [2.x.1235]  inside the Schur complement, and because we don't have a good preconditioner for the Schur complement, these solves just have to happen too often. A better approach is to use a block decomposition, which is based on an observation of Silvester and Wathen  [2.x.1236]  and explained in much greater detail in  [2.x.1237]  . An implementation of this alternative approach is discussed below, in the section on a [1.x.472] in the results section of this program. 




[1.x.473] 

Above, we have claimed that the linear system has the form 

[1.x.474] 

i.e., in particular that there is a zero block at the bottom right of the matrix. This then allowed us to write the Schur complement as  [2.x.1238] . But this is not quite correct. 

Think of what would happen if there are constraints on some pressure variables (see the  [2.x.1239]  "Constraints on degrees of freedom" documentation module), for example because we use adaptively refined meshes and continuous pressure finite elements so that there are hanging nodes. Another cause for such constraints are Dirichlet boundary conditions on the pressure. Then the AffineConstraints class, upon copying the local contributions to the matrix into the global linear system will zero out rows and columns corresponding to constrained degrees of freedom and put a positive entry on the diagonal. (You can think of this entry as being one for simplicity, though in reality it is a value of the same order of magnitude as the other matrix entries.) In other words, the bottom right block is really not empty at all: It has a few entries on the diagonal, one for each constrained pressure degree of freedom, and a correct description of the linear system we have to solve is that it has the form 

[1.x.475] 

where  [2.x.1240]  is the zero matrix with the exception of the positive diagonal entries for the constrained degrees of freedom. The correct Schur complement would then in fact be the matrix  [2.x.1241]  instead of the one stated above. 

Thinking about this makes us, first, realize that the resulting Schur complement is now indefinite because  [2.x.1242]  is symmetric and positive definite whereas  [2.x.1243]  is a positive semidefinite, and subtracting the latter from the former may no longer be positive definite. This is annoying because we could no longer employ the Conjugate Gradient method on this true Schur complement. That said, we could fix the issue in  [2.x.1244]  by simply putting *negative* values onto the diagonal for the constrained pressure variables -- because we really only put something nonzero to ensure that the resulting matrix is not singular; we really didn't care whether that entry is positive or negative. So if the entries on the diagonal of  [2.x.1245]  were negative, then  [2.x.1246]  would again be a symmetric and positive definite matrix. 

But, secondly, the code below doesn't actually do any of that: It happily solves the linear system with the wrong Schur complement  [2.x.1247]  that just ignores the issue altogether. Why does this even work? To understand why this is so, recall that when writing local contributions into the global matrix,  [2.x.1248]  zeros out the rows and columns that correspond to constrained degrees of freedom. This means that  [2.x.1249]  has some zero rows, and  [2.x.1250]  zero columns. As a consequence, if one were to multiply out what the entries of  [2.x.1251]  are, one would realize that it has zero rows and columns for all constrained pressure degrees of freedom, including a zero on the diagonal. The nonzero entries of  [2.x.1252]  would fit into exactly those zero diagonal locations, and ensure that  [2.x.1253]  is invertible. Not doing so, strictly speaking, means that  [2.x.1254]  remains singular: It is symmetric and positive definite on the subset of non-constrained pressure degrees of freedom, and simply the zero matrix on the constrained pressures. Why does the Conjugate Gradient method work for this matrix? Because  [2.x.1255]  also makes sure that the right hand side entries that correspond to these zero rows of the matrix are *also* zero, i.e., the right hand side is compatible. 

What this means is that whatever the values of the solution vector for these constrained pressure degrees of freedom, these rows will always have a zero residual and, if one were to consider what the CG algorithm does internally, just never produce any updates to the solution vector. In other words, the CG algorithm just *ignores* these rows, despite the fact that the matrix is singular. This only works because these degrees of freedom are entirely decoupled from the rest of the linear system (because the entire row and corresponding column are zero). At the end of the solution process, the constrained pressure values in the solution vector therefore remain exactly as they were when we started the call to the solver; they are finally overwritten with their correct values when we call  [2.x.1256]  after the CG solver is done. 

The upshot of this discussion is that the assumption that the bottom right block of the big matrix is zero is a bit simplified, but that just going with it does not actually lead to any practical problems worth addressing. 




[1.x.476] 

The domain, right hand side and boundary conditions we implement below relate to a problem in geophysics: there, one wants to compute the flow field of magma in the earth's interior under a mid-ocean rift. Rifts are places where two continental plates are very slowly drifting apart (a few centimeters per year at most), leaving a crack in the earth crust that is filled with magma from below. Without trying to be entirely realistic, we model this situation by solving the following set of equations and boundary conditions on the domain  [2.x.1257] : 

[1.x.477] 

and using natural boundary conditions  [2.x.1258]  everywhere else. In other words, at the left part of the top surface we prescribe that the fluid moves with the continental plate to the left at speed  [2.x.1259] , that it moves to the right on the right part of the top surface, and impose natural flow conditions everywhere else. If we are in 2d, the description is essentially the same, with the exception that we omit the second component of all vectors stated above. 

As will become apparent in the [1.x.478], the flow field will pull material from below and move it to the left and right ends of the domain, as expected. The discontinuity of velocity boundary conditions will produce a singularity in the pressure at the center of the top surface that sucks material all the way to the top surface to fill the gap left by the outward motion of material at this location. 




[1.x.479] 

[1.x.480] 

In all the previous tutorial programs, we used the AffineConstraints object merely for handling hanging node constraints (with exception of step-11). However, the class can also be used to implement Dirichlet boundary conditions, as we will show in this program, by fixing some node values  [2.x.1260] . Note that these are inhomogeneous constraints, and we have to pay some special attention to that. The way we are going to implement this is to first read in the boundary values into the AffineConstraints object by using the call 

[1.x.481] 



very similar to how we were making the list of boundary nodes before (note that we set Dirichlet conditions only on boundaries with boundary flag 1). The actual application of the boundary values is then handled by the AffineConstraints object directly, without any additional interference. 

We could then proceed as before, namely by filling the matrix, and then calling a condense function on the constraints object of the form 

[1.x.482] 



Note that we call this on the system matrix and system right hand side simultaneously, since resolving inhomogeneous constraints requires knowledge about both the matrix entries and the right hand side. For efficiency reasons, though, we choose another strategy: all the constraints collected in the AffineConstraints object can be resolved on the fly while writing local data into the global matrix, by using the call 

[1.x.483] 



This technique is further discussed in the step-27 tutorial program. All we need to know here is that this functions does three things at once: it writes the local data into the global matrix and right hand side, it distributes the hanging node constraints and additionally implements (inhomogeneous) Dirichlet boundary conditions. That's nice, isn't it? 

We can conclude that the AffineConstraints class provides an alternative to using  [2.x.1261]  for implementing Dirichlet boundary conditions. 


[1.x.484][1.x.485] 

Frequently, a sparse matrix contains a substantial amount of elements that actually are zero when we are about to start a linear solve. Such elements are introduced when we eliminate constraints or implement Dirichlet conditions, where we usually delete all entries in constrained rows and columns, i.e., we set them to zero. The fraction of elements that are present in the sparsity pattern, but do not really contain any information, can be up to one fourth of the total number of elements in the matrix for the 3D application considered in this tutorial program. Remember that matrix-vector products or preconditioners operate on all the elements of a sparse matrix (even those that are zero), which is an inefficiency we will avoid here. 

An advantage of directly resolving constrained degrees of freedom is that we can avoid having most of the entries that are going to be zero in our sparse matrix &mdash; we do not need constrained entries during matrix construction (as opposed to the traditional algorithms, which first fill the matrix, and only resolve constraints afterwards). This will save both memory and time when forming matrix-vector products. The way we are going to do that is to pass the information about constraints to the function that generates the sparsity pattern, and then set a <tt>false</tt> argument specifying that we do not intend to use constrained entries: 

[1.x.486] 

This functions obviates, by the way, also the call to the <tt>condense()</tt> function on the sparsity pattern. 




[1.x.487] 

The program developed below has seen a lot of TLC. We have run it over and over under profiling tools (mainly [1.x.488]'s cachegrind and callgrind tools, as well as the KDE [1.x.489] program for visualization) to see where the bottlenecks are. This has paid off: through this effort, the program has become about four times as fast when considering the runtime of the refinement cycles zero through three, reducing the overall number of CPU instructions executed from 869,574,060,348 to 199,853,005,625. For higher refinement levels, the gain is probably even larger since some algorithms that are not  [2.x.1262]  have been eliminated. 

Essentially, there are currently two algorithms in the program that do not scale linearly with the number of degrees of freedom: renumbering of degrees of freedom (which is  [2.x.1263] , and the linear solver (which is  [2.x.1264] ). As for the first, while reordering degrees of freedom may not scale linearly, it is an indispensable part of the overall algorithm as it greatly improves the quality of the sparse ILU, easily making up for the time spent on computing the renumbering; graphs and timings to demonstrate this are shown in the documentation of the DoFRenumbering namespace, also underlining the choice of the Cuthill-McKee reordering algorithm chosen below. 

As for the linear solver: as mentioned above, our implementation here uses a Schur complement formulation. This is not necessarily the very best choice but demonstrates various important techniques available in deal.II. The question of which solver is best is again discussed in the [1.x.490] of this program, along with code showing alternative solvers and a comparison of their results. 

Apart from this, many other algorithms have been tested and improved during the creation of this program. For example, in building the sparsity pattern, we originally used a (now no longer existing) BlockCompressedSparsityPattern object that added one element at a time; however, its data structures were poorly adapted for the large numbers of nonzero entries per row created by our discretization in 3d, leading to a quadratic behavior. Replacing the internal algorithms in deal.II to set many elements at a time, and using a BlockCompressedSimpleSparsityPattern (which has, as of early 2015, been in turn replaced by BlockDynamicSparsityPattern) as a better adapted data structure, removed this bottleneck at the price of a slightly higher memory consumption. Likewise, the implementation of the decomposition step in the SparseILU class was very inefficient and has been replaced by one that is about 10 times faster. Even the vmult function of the SparseILU has been improved to save about twenty percent of time. Small improvements were applied here and there. Moreover, the AffineConstraints object has been used to eliminate a lot of entries in the sparse matrix that are eventually going to be zero, see [1.x.491]. 

A profile of how many CPU instructions are spent at the various different places in the program during refinement cycles zero through three in 3d is shown here: 

 [2.x.1265]  

As can be seen, at this refinement level approximately three quarters of the instruction count is spent on the actual solver (the  [2.x.1266]  calls on the left, the  [2.x.1267]  call in the middle for the Schur complement solve, and another box representing the multiplications with SparseILU and SparseMatrix in the solve for [1.x.492]). About one fifth of the instruction count is spent on matrix assembly and sparse ILU computation (box in the lower right corner) and the rest on other things. Since floating point operations such as in the  [2.x.1268]  calls typically take much longer than many of the logical operations and table lookups in matrix assembly, the fraction of the run time taken up by matrix assembly is actually significantly less than the fraction of instructions, as will become apparent in the comparison we make in the results section. 

For higher refinement levels, the boxes representing the solver as well as the blue box at the top right stemming from reordering algorithm are going to grow at the expense of the other parts of the program, since they don't scale linearly. The fact that at this moderate refinement level (3168 cells and 93176 degrees of freedom) the linear solver already makes up about three quarters of the instructions is a good sign that most of the algorithms used in this program are well-tuned and that major improvements in speeding up the program are most likely not to come from hand-optimizing individual aspects but by changing solver algorithms. We will address this point in the discussion of results below as well. 

As a final point, and as a point of reference, the following picture also shows how the profile looked at an early stage of optimizing this program: 

 [2.x.1269]  

As mentioned above, the runtime of this version was about four times as long as for the first profile, with the SparseILU decomposition taking up about 30% of the instruction count, and operations an early, inefficient version of DynamicSparsityPattern about 10%. Both these bottlenecks have since been completely removed. 


examples/step-22/doc/results.dox 

[1.x.493] 

[1.x.494] 

[1.x.495] 

[1.x.496] 

Running the program with the space dimension set to 2 in the  [2.x.1270]  function yields the following output (in "release mode",  [2.x.1271]  

[1.x.497] 



The entire computation above takes about 2 seconds on a reasonably quick (for 2015 standards) machine. 

What we see immediately from this is that the number of (outer) iterations does not increase as we refine the mesh. This confirms the statement in the introduction that preconditioning the Schur complement with the mass matrix indeed yields a matrix spectrally equivalent to the identity matrix (i.e. with eigenvalues bounded above and below independently of the mesh size or the relative sizes of cells). In other words, the mass matrix and the Schur complement are spectrally equivalent. 

In the images below, we show the grids for the first six refinement steps in the program.  Observe how the grid is refined in regions where the solution rapidly changes: On the upper boundary, we have Dirichlet boundary conditions that are -1 in the left half of the line and 1 in the right one, so there is an abrupt change at  [2.x.1272] . Likewise, there are changes from Dirichlet to Neumann data in the two upper corners, so there is need for refinement there as well: 

 [2.x.1273]  

Finally, following is a plot of the flow field. It shows fluid transported along with the moving upper boundary and being replaced by material coming from below: 

 [2.x.1274]  

This plot uses the capability of VTK-based visualization programs (in this case of VisIt) to show vector data; this is the result of us declaring the velocity components of the finite element in use to be a set of vector components, rather than independent scalar components in the  [2.x.1275]  function of this tutorial program. 




[1.x.498] 

In 3d, the screen output of the program looks like this: 

[1.x.499] 



Again, we see that the number of outer iterations does not increase as we refine the mesh. Nevertheless, the compute time increases significantly: for each of the iterations above separately, it takes about 0.14 seconds, 0.63 seconds, 4.8 seconds, 35 seconds, 2 minutes and 33 seconds, and 13 minutes and 12 seconds. This overall superlinear (in the number of unknowns) increase in runtime is due to the fact that our inner solver is not  [2.x.1276] : a simple experiment shows that as we keep refining the mesh, the average number of ILU-preconditioned CG iterations to invert the velocity-velocity block  [2.x.1277]  increases. 

We will address the question of how possibly to improve our solver [1.x.500]. 

As for the graphical output, the grids generated during the solution look as follow: 

 [2.x.1278]  

Again, they show essentially the location of singularities introduced by boundary conditions. The vector field computed makes for an interesting graph: 

 [2.x.1279]  

The isocontours shown here as well are those of the pressure variable, showing the singularity at the point of discontinuous velocity boundary conditions. 




[1.x.501] 

As explained during the generation of the sparsity pattern, it is important to have the numbering of degrees of freedom in mind when using preconditioners like incomplete LU decompositions. This is most conveniently visualized using the distribution of nonzero elements in the stiffness matrix. 

If we don't do anything special to renumber degrees of freedom (i.e., without using  [2.x.1280]  but with using  [2.x.1281]  to ensure that degrees of freedom are appropriately sorted into their corresponding blocks of the matrix and vector), then we get the following image after the first adaptive refinement in two dimensions: 

 [2.x.1282]  

In order to generate such a graph, you have to insert a piece of code like the following to the end of the setup step. 

[1.x.502] 



It is clearly visible that the nonzero entries are spread over almost the whole matrix.  This makes preconditioning by ILU inefficient: ILU generates a Gaussian elimination (LU decomposition) without fill-in elements, which means that more tentative fill-ins left out will result in a worse approximation of the complete decomposition. 

In this program, we have thus chosen a more advanced renumbering of components.  The renumbering with  [2.x.1283]  and grouping the components into velocity and pressure yields the following output: 

 [2.x.1284]  

It is apparent that the situation has improved a lot. Most of the elements are now concentrated around the diagonal in the (0,0) block in the matrix. Similar effects are also visible for the other blocks. In this case, the ILU decomposition will be much closer to the full LU decomposition, which improves the quality of the preconditioner. (It may be interesting to note that the sparse direct solver UMFPACK does some %internal renumbering of the equations before actually generating a sparse LU decomposition; that procedure leads to a very similar pattern to the one we got from the Cuthill-McKee algorithm.) 

Finally, we want to have a closer look at a sparsity pattern in 3D. We show only the (0,0) block of the matrix, again after one adaptive refinement. Apart from the fact that the matrix size has increased, it is also visible that there are many more entries in the matrix. Moreover, even for the optimized renumbering, there will be a considerable amount of tentative fill-in elements. This illustrates why UMFPACK is not a good choice in 3D - a full decomposition needs many new entries that  eventually won't fit into the physical memory (RAM): 

 [2.x.1285]  




[1.x.503] 

[1.x.504][1.x.505] 

We have seen in the section of computational results that the number of outer iterations does not depend on the mesh size, which is optimal in a sense of scalability. This does, however, not apply to the solver as a whole, as mentioned above: We did not look at the number of inner iterations when generating the inverse of the matrix  [2.x.1286]  and the mass matrix  [2.x.1287] . Of course, this is unproblematic in the 2D case where we precondition  [2.x.1288]  with a direct solver and the  [2.x.1289]  operation of the inverse matrix structure will converge in one single CG step, but this changes in 3D where we only use an ILU preconditioner.  There, the number of required preconditioned CG steps to invert  [2.x.1290]  increases as the mesh is refined, and each  [2.x.1291]  operation involves on average approximately 14, 23, 36, 59, 75 and 101 inner CG iterations in the refinement steps shown above. (On the other hand, the number of iterations for applying the inverse pressure mass matrix is always around five, both in two and three dimensions.)  To summarize, most work is spent on solving linear systems with the same matrix  [2.x.1292]  over and over again. What makes this look even worse is the fact that we actually invert a matrix that is about 95 percent the size of the total system matrix and stands for 85 percent of the non-zero entries in the sparsity pattern. Hence, the natural question is whether it is reasonable to solve a linear system with matrix  [2.x.1293]  for about 15 times when calculating the solution to the block system. 

The answer is, of course, that we can do that in a few other (most of the time better) ways. Nevertheless, it has to be remarked that an indefinite system as the one at hand puts indeed much higher demands on the linear algebra than standard elliptic problems as we have seen in the early tutorial programs. The improvements are still rather unsatisfactory, if one compares with an elliptic problem of similar size. Either way, we will introduce below a number of improvements to the linear solver, a discussion that we will re-consider again with additional options in the step-31 program. 

[1.x.506][1.x.507] A first attempt to improve the speed of the linear solution process is to choose a dof reordering that makes the ILU being closer to a full LU decomposition, as already mentioned in the in-code comments. The DoFRenumbering namespace compares several choices for the renumbering of dofs for the Stokes equations. The best result regarding the computing time was found for the King ordering, which is accessed through the call  [2.x.1294]  With that program, the inner solver needs considerably less operations, e.g. about 62 inner CG iterations for the inversion of  [2.x.1295]  at cycle 4 compared to about 75 iterations with the standard Cuthill-McKee-algorithm. Also, the computing time at cycle 4 decreased from about 17 to 11 minutes for the  [2.x.1296]  call. However, the King ordering (and the orderings provided by the  [2.x.1297]  namespace in general) has a serious drawback - it uses much more memory than the in-build deal versions, since it acts on abstract graphs rather than the geometry provided by the triangulation. In the present case, the renumbering takes about 5 times as much memory, which yields an infeasible algorithm for the last cycle in 3D with 1.2 million unknowns. 

[1.x.508] Another idea to improve the situation even more would be to choose a preconditioner that makes CG for the (0,0) matrix  [2.x.1298]  converge in a mesh-independent number of iterations, say 10 to 30. We have seen such a candidate in step-16: multigrid. 

[1.x.509] [1.x.510] Even with a good preconditioner for  [2.x.1299] , we still need to solve of the same linear system repeatedly (with different right hand sides, though) in order to make the Schur complement solve converge. The approach we are going to discuss here is how inner iteration and outer iteration can be combined. If we persist in calculating the Schur complement, there is no other possibility. 

The alternative is to attack the block system at once and use an approximate Schur complement as efficient preconditioner. The idea is as follows: If we find a block preconditioner  [2.x.1300]  such that the matrix 

[1.x.511] 

is simple, then an iterative solver with that preconditioner will converge in a few iterations. Using the Schur complement  [2.x.1301] , one finds that 

[1.x.512] 

would appear to be a good choice since 

[1.x.513] 

This is the approach taken by the paper by Silvester and Wathen referenced to in the introduction (with the exception that Silvester and Wathen use right preconditioning). In this case, a Krylov-based iterative method would converge in one step only if exact inverses of  [2.x.1302]  and  [2.x.1303]  were applied, since all the eigenvalues are one (and the number of iterations in such a method is bounded by the number of distinct eigenvalues). Below, we will discuss the choice of an adequate solver for this problem. First, we are going to have a closer look at the implementation of the preconditioner. 

Since  [2.x.1304]  is aimed to be a preconditioner only, we shall use approximations to the inverse of the Schur complement  [2.x.1305]  and the matrix  [2.x.1306] . Hence, the Schur complement will be approximated by the pressure mass matrix  [2.x.1307] , and we use a preconditioner to  [2.x.1308]  (without an InverseMatrix class around it) for approximating  [2.x.1309] . 

Here comes the class that implements the block Schur complement preconditioner. The  [2.x.1310]  operation for block vectors according to the derivation above can be specified by three successive operations: 

[1.x.514] 



Since we act on the whole block system now, we have to live with one disadvantage: we need to perform the solver iterations on the full block system instead of the smaller pressure space. 

Now we turn to the question which solver we should use for the block system. The first observation is that the resulting preconditioned matrix cannot be solved with CG since it is neither positive definite nor symmetric. 

The deal.II libraries implement several solvers that are appropriate for the problem at hand. One choice is the solver  [2.x.1311]  "BiCGStab", which was used for the solution of the unsymmetric advection problem in step-9. The second option, the one we are going to choose, is  [2.x.1312]  "GMRES" (generalized minimum residual). Both methods have their pros and cons - there are problems where one of the two candidates clearly outperforms the other, and vice versa. [1.x.515]'s article on the GMRES method gives a comparative presentation. A more comprehensive and well-founded comparison can be read e.g. in the book by J.W. Demmel (Applied Numerical Linear Algebra, SIAM, 1997, section 6.6.6). 

For our specific problem with the ILU preconditioner for  [2.x.1313] , we certainly need to perform hundreds of iterations on the block system for large problem sizes (we won't beat CG!). Actually, this disfavors GMRES: During the GMRES iterations, a basis of Krylov vectors is successively built up and some operations are performed on these vectors. The more vectors are in this basis, the more operations and memory will be needed. The number of operations scales as  [2.x.1314]  and memory as  [2.x.1315] , where  [2.x.1316]  is the number of vectors in the Krylov basis and  [2.x.1317]  the size of the (block) matrix. To not let these demands grow excessively, deal.II limits the size  [2.x.1318]  of the basis to 30 vectors by default. Then, the basis is rebuilt. This implementation of the GMRES method is called GMRES(k), with default  [2.x.1319] . What we have gained by this restriction, namely a bound on operations and memory requirements, will be compensated by the fact that we use an incomplete basis - this will increase the number of required iterations. 

BiCGStab, on the other hand, won't get slower when many iterations are needed (one iteration uses only results from one preceding step and not all the steps as GMRES). Besides the fact the BiCGStab is more expensive per step since two matrix-vector products are needed (compared to one for CG or GMRES), there is one main reason which makes BiCGStab not appropriate for this problem: The preconditioner applies the inverse of the pressure mass matrix by using the InverseMatrix class. Since the application of the inverse matrix to a vector is done only in approximative way (an exact inverse is too expensive), this will also affect the solver. In the case of BiCGStab, the Krylov vectors will not be orthogonal due to that perturbation. While this is uncritical for a small number of steps (up to about 50), it ruins the performance of the solver when these perturbations have grown to a significant magnitude in the coarse of iterations. 

We did some experiments with BiCGStab and found it to be faster than GMRES up to refinement cycle 3 (in 3D), but it became very slow for cycles 4 and 5 (even slower than the original Schur complement), so the solver is useless in this situation. Choosing a sharper tolerance for the inverse matrix class ( [2.x.1320]  instead of  [2.x.1321] ) made BiCGStab perform well also for cycle 4, but did not change the failure on the very large problems. 

GMRES is of course also effected by the approximate inverses, but it is not as sensitive to orthogonality and retains a relatively good performance also for large sizes, see the results below. 

With this said, we turn to the realization of the solver call with GMRES with  [2.x.1322]  temporary vectors: 

[1.x.516] 



Obviously, one needs to add the include file  [2.x.1323]  "<lac/solver_gmres.h>" in order to make this run. We call the solver with a BlockVector template in order to enable GMRES to operate on block vectors and matrices. Note also that we need to set the (1,1) block in the system matrix to zero (we saved the pressure mass matrix there which is not part of the problem) after we copied the information to another matrix. 

Using the Timer class, we collect some statistics that compare the runtime of the block solver with the one from the problem implementation above. Besides the solution with the two options we also check if the solutions of the two variants are close to each other (i.e. this solver gives indeed the same solution as we had before) and calculate the infinity norm of the vector difference. 

Let's first see the results in 2D: 

[1.x.517] 



We see that there is no huge difference in the solution time between the block Schur complement preconditioner solver and the Schur complement itself. The reason is simple: we used a direct solve as preconditioner for  [2.x.1324]  - so we cannot expect any gain by avoiding the inner iterations. We see that the number of iterations has slightly increased for GMRES, but all in all the two choices are fairly similar. 

The picture of course changes in 3D: 

[1.x.518] 



Here, the block preconditioned solver is clearly superior to the Schur complement, but the advantage gets less for more mesh points. This is because GMRES(k) scales worse with the problem size than CG, as we discussed above.  Nonetheless, the improvement by a factor of 3-6 for moderate problem sizes is quite impressive. 




[1.x.519] An ultimate linear solver for this problem could be imagined as a combination of an optimal preconditioner for  [2.x.1325]  (e.g. multigrid) and the block preconditioner described above, which is the approach taken in the step-31 and step-32 tutorial programs (where we use an algebraic multigrid method) and step-56 (where we use a geometric multigrid method). 




[1.x.520] Another possibility that can be taken into account is to not set up a block system, but rather solve the system of velocity and pressure all at once. The options are direct solve with UMFPACK (2D) or GMRES with ILU preconditioning (3D). It should be straightforward to try that. 




[1.x.521] 

The program can of course also serve as a basis to compute the flow in more interesting cases. The original motivation to write this program was for it to be a starting point for some geophysical flow problems, such as the movement of magma under places where continental plates drift apart (for example mid-ocean ridges). Of course, in such places, the geometry is more complicated than the examples shown above, but it is not hard to accommodate for that. 

For example, by using the following modification of the boundary values function 

[1.x.522] 

and the following way to generate the mesh as the domain  [2.x.1326]  

[1.x.523] 

then we get images where the fault line is curved:  [2.x.1327]  


examples/step-23/doc/intro.dox 

[1.x.524] 

[1.x.525] 

 [2.x.1328]  

This is the first of a number of tutorial programs that will finally cover "real" time-dependent problems, not the slightly odd form of time dependence found in step-18 or the DAE model of step-21. In particular, this program introduces the wave equation in a bounded domain. Later, step-24 will consider an example of absorbing boundary conditions, and  [2.x.1329]  "step-25" a kind of nonlinear wave equation producing solutions called solitons. 

The wave equation in its prototypical form reads as follows: find  [2.x.1330]  that satisfies 

[1.x.526] 

Note that since this is an equation with second-order time derivatives, we need to pose two initial conditions, one for the value and one for the time derivative of the solution. 

Physically, the equation describes the motion of an elastic medium. In 2-d, one can think of how a membrane moves if subjected to a force. The Dirichlet boundary conditions above indicate that the membrane is clamped at the boundary at a height  [2.x.1331]  (this height might be moving as well &mdash; think of people holding a blanket and shaking it up and down). The first initial condition equals the initial deflection of the membrane, whereas the second one gives its velocity. For example, one could think of pushing the membrane down with a finger and then letting it go at  [2.x.1332]  (nonzero deflection but zero initial velocity), or hitting it with a hammer at  [2.x.1333]  (zero deflection but nonzero velocity). Both cases would induce motion in the membrane. 




[1.x.527] 

[1.x.528] There is a long-standing debate in the numerical analysis community over whether a discretization of time dependent equations should involve first discretizing the time variable leading to a stationary PDE at each time step that is then solved using standard finite element techniques (this is called the Rothe method), or whether one should first discretize the spatial variables, leading to a large system of ordinary differential equations that can then be handled by one of the usual ODE solvers (this is called the method of lines). 

Both of these methods have advantages and disadvantages. Traditionally, people have preferred the method of lines, since it allows to use the very well developed machinery of high-order ODE solvers available for the rather stiff ODEs resulting from this approach, including step length control and estimation of the temporal error. 

On the other hand, Rothe's method becomes awkward when using higher-order time stepping method, since one then has to write down a PDE that couples the solution of the present time step not only with that at the previous time step, but possibly also even earlier solutions, leading to a significant number of terms. 

For these reasons, the method of lines was the method of choice for a long time. However, it has one big drawback: if we discretize the spatial variable first, leading to a large ODE system, we have to choose a mesh once and for all. If we are willing to do this, then this is a legitimate and probably superior approach. 

If, on the other hand, we are looking at the wave equation and many other time dependent problems, we find that the character of a solution changes as time progresses. For example, for the wave equation, we may have a single wave travelling through the domain, where the solution is smooth or even constant in front of and behind the wave &mdash; adaptivity would be really useful for such cases, but the key is that the area where we need to refine the mesh changes from time step to time step! 

If we intend to go that way, i.e. choose a different mesh for each time step (or set of time steps), then the method of lines is not appropriate any more: instead of getting one ODE system with a number of variables equal to the number of unknowns in the finite element mesh, our number of unknowns now changes all the time, a fact that standard ODE solvers are certainly not prepared to deal with at all. On the other hand, for the Rothe method, we just get a PDE for each time step that we may choose to discretize independently of the mesh used for the previous time step; this approach is not without perils and difficulties, but at least is a sensible and well-defined procedure. 

For all these reasons, for the present program, we choose to use the Rothe method for discretization, i.e. we first discretize in time and then in space. We will not actually use adaptive meshes at all, since this involves a large amount of additional code, but we will comment on this some more in the [1.x.529]. 




[1.x.530] 

Given these considerations, here is how we will proceed: let us first define a simple time stepping method for this second order problem, and then in a second step do the spatial discretization, i.e. we will follow Rothe's approach. 

For the first step, let us take a little detour first: in order to discretize a second time derivative, we can either discretize it directly, or we can introduce an additional variable and transform the system into a first order system. In many cases, this turns out to be equivalent, but dealing with first order systems is often simpler. To this end, let us introduce 

[1.x.531] 

and call this variable the [1.x.532] for obvious reasons. We can then reformulate the original wave equation as follows: 

[1.x.533] 

The advantage of this formulation is that it now only contains first time derivatives for both variables, for which it is simple to write down time stepping schemes. Note that we do not have boundary conditions for  [2.x.1334]  at first. However, we could enforce  [2.x.1335]  on the boundary. It turns out in numerical examples that this is actually necessary: without doing so the solution doesn't look particularly wrong, but the Crank-Nicolson scheme does not conserve energy if one doesn't enforce these boundary conditions. 

With this formulation, let us introduce the following time discretization where a superscript  [2.x.1336]  indicates the number of a time step and  [2.x.1337]  is the length of the present time step: 

[1.x.534] Note how we introduced a parameter  [2.x.1338]  here. If we chose  [2.x.1339] , for example, the first equation would reduce to  [2.x.1340] , which is well-known as the forward or explicit Euler method. On the other hand, if we set  [2.x.1341] , then we would get  [2.x.1342] , which corresponds to the backward or implicit Euler method. Both these methods are first order accurate methods. They are simple to implement, but they are not really very accurate. 

The third case would be to choose  [2.x.1343] . The first of the equations above would then read  [2.x.1344] . This method is known as the Crank-Nicolson method and has the advantage that it is second order accurate. In addition, it has the nice property that it preserves the energy in the solution (physically, the energy is the sum of the kinetic energy of the particles in the membrane plus the potential energy present due to the fact that it is locally stretched; this quantity is a conserved one in the continuous equation, but most time stepping schemes do not conserve it after time discretization). Since  [2.x.1345]  also appears in the equation for  [2.x.1346] , the Crank-Nicolson scheme is also implicit. 

In the program, we will leave  [2.x.1347]  as a parameter, so that it will be easy to play with it. The results section will show some numerical evidence comparing the different schemes. 

The equations above (called the [1.x.535] equations because we have only discretized the time, but not space), can be simplified a bit by eliminating  [2.x.1348]  from the first equation and rearranging terms. We then get 

[1.x.536] In this form, we see that if we are given the solution  [2.x.1349]  of the previous timestep, that we can then solve for the variables  [2.x.1350]  separately, i.e. one at a time. This is convenient. In addition, we recognize that the operator in the first equation is positive definite, and the second equation looks particularly simple. 




[1.x.537] 

We have now derived equations that relate the approximate (semi-discrete) solution  [2.x.1351]  and its time derivative  [2.x.1352]  at time  [2.x.1353]  with the solutions  [2.x.1354]  of the previous time step at  [2.x.1355] . The next step is to also discretize the spatial variable using the usual finite element methodology. To this end, we multiply each equation with a test function, integrate over the entire domain, and integrate by parts where necessary. This leads to 

[1.x.538] 

It is then customary to approximate  [2.x.1356] , where  [2.x.1357]  are the shape functions used for the discretization of the  [2.x.1358] -th time step and  [2.x.1359]  are the unknown nodal values of the solution. Similarly,  [2.x.1360] . Finally, we have the solutions of the previous time step,  [2.x.1361]  and  [2.x.1362] . Note that since the solution of the previous time step has already been computed by the time we get to time step  [2.x.1363] ,  [2.x.1364]  are known. Furthermore, note that the solutions of the previous step may have been computed on a different mesh, so we have to use shape functions  [2.x.1365] . 

If we plug these expansions into above equations and test with the test functions from the present mesh, we get the following linear system: 

[1.x.539] where 

[1.x.540] 



If we solve these two equations, we can move the solution one step forward and go on to the next time step. 

It is worth noting that if we choose the same mesh on each time step (as we will in fact do in the program below), then we have the same shape functions on time step  [2.x.1366]  and  [2.x.1367] , i.e.  [2.x.1368] . Consequently, we get  [2.x.1369]  and  [2.x.1370] . On the other hand, if we had used different shape functions, then we would have to compute integrals that contain shape functions defined on two meshes. This is a somewhat messy process that we omit here, but that is treated in some detail in step-28. 

Under these conditions (i.e. a mesh that doesn't change), one can optimize the solution procedure a bit by basically eliminating the solution of the second linear system. We will discuss this in the introduction of the  [2.x.1371]  "step-25" program. 

[1.x.541] 

One way to compare the quality of a time stepping scheme is to see whether the numerical approximation preserves conservation properties of the continuous equation. For the wave equation, the natural quantity to look at is the energy. By multiplying the wave equation by  [2.x.1372] , integrating over  [2.x.1373] , and integrating by parts where necessary, we find that 

[1.x.542] 

By consequence, in absence of body forces and constant boundary values, we get that 

[1.x.543] 

is a conserved quantity, i.e. one that doesn't change with time. We will compute this quantity after each time step. It is straightforward to see that if we replace  [2.x.1374]  by its finite element approximation, and  [2.x.1375]  by the finite element approximation of the velocity  [2.x.1376] , then 

[1.x.544] 

As we will see in the results section, the Crank-Nicolson scheme does indeed conserve the energy, whereas neither the forward nor the backward Euler scheme do. 




[1.x.545] 

One of the reasons why the wave equation is nasty to solve numerically is that explicit time discretizations are only stable if the time step is small enough. In particular, it is coupled to the spatial mesh width  [2.x.1377] . For the lowest order discretization we use here, the relationship reads 

[1.x.546] 

where  [2.x.1378]  is the wave speed, which in our formulation of the wave equation has been normalized to one. Consequently, unless we use the implicit schemes with  [2.x.1379] , our solutions will not be numerically stable if we violate this restriction. Implicit schemes do not have this restriction for stability, but they become inaccurate if the time step is too large. 

This condition was first recognized by Courant, Friedrichs, and Lewy &mdash; in 1928, long before computers became available for numerical computations! (This result appeared in the German language article R. Courant, K. Friedrichs and H. Lewy: [1.x.547], Mathematische Annalen, vol. 100, no. 1, pages 32-74, 1928.) This condition on the time step is most frequently just referred to as the [1.x.548] condition. Intuitively, the CFL condition says that the time step must not be larger than the time it takes a wave to cross a single cell. 

In the program, we will refine the square  [2.x.1380]  seven times uniformly, giving a mesh size of  [2.x.1381] , which is what we set the time step to. The fact that we set the time step and mesh size individually in two different places is error prone: it is too easy to refine the mesh once more but forget to also adjust the time step.  [2.x.1382]  "step-24" shows a better way how to keep these things in sync. 




[1.x.549] 

Although the program has all the hooks to deal with nonzero initial and boundary conditions and body forces, we take a simple case where the domain is a square  [2.x.1383]  and 

[1.x.550] 

This corresponds to a membrane initially at rest and clamped all around, where someone is waving a part of the clamped boundary once up and down, thereby shooting a wave into the domain. 


examples/step-23/doc/results.dox 



[1.x.551] 

When the program is run, it produces the following output: 

[1.x.552] 



What we see immediately is that the energy is a constant at least after  [2.x.1384]  (until which the boundary source term  [2.x.1385]  is nonzero, injecting energy into the system). 

In addition to the screen output, the program writes the solution of each time step to an output file. If we process them adequately and paste them into a movie, we get the following: 

 [2.x.1386]  

The movie shows the generated wave nice traveling through the domain and back, being reflected at the clamped boundary. Some numerical noise is trailing the wave, an artifact of a too-large mesh size that can be reduced by reducing the mesh width and the time step. 


[1.x.553] 

[1.x.554] 

If you want to explore a bit, try out some of the following things:  [2.x.1387]     [2.x.1388] Varying  [2.x.1389] . This gives different time stepping schemes, some of   which are stable while others are not. Take a look at how the energy   evolves. 

   [2.x.1390] Different initial and boundary conditions, right hand sides. 

   [2.x.1391] More complicated domains or more refined meshes. Remember that the time   step needs to be bounded by the mesh width, so changing the mesh should   always involve also changing the time step. We will come back to this issue   in step-24. 

   [2.x.1392] Variable coefficients: In real media, the wave speed is often   variable. In particular, the "real" wave equation in realistic media would   read   [1.x.555] 

  where  [2.x.1393]  is the density of the material, and  [2.x.1394]  is related to the   stiffness coefficient. The wave speed is then  [2.x.1395] . 

  To make such a change, we would have to compute the mass and Laplace   matrices with a variable coefficient. Fortunately, this isn't too hard: the   functions  [2.x.1396]  and    [2.x.1397]  have additional default parameters that can   be used to pass non-constant coefficient functions to them. The required   changes are therefore relatively small. On the other hand, care must be   taken again to make sure the time step is within the allowed range. 

   [2.x.1398] In the in-code comments, we discussed the fact that the matrices for   solving for  [2.x.1399]  and  [2.x.1400]  need to be reset in every time because of   boundary conditions, even though the actual content does not change. It is   possible to avoid copying by not eliminating columns in the linear systems,   which is implemented by appending a  [2.x.1401]  argument to the call:   [1.x.556] 



   [2.x.1402] deal.II being a library that supports adaptive meshes it would of course be   nice if this program supported change the mesh every few time steps. Given the   structure of the solution &mdash; a wave that travels through the domain &mdash;   it would seem appropriate if we only refined the mesh where the wave currently is,   and not simply everywhere. It is intuitively clear that we should be able to   save a significant amount of cells this way. (Though upon further thought one   realizes that this is really only the case in the initial stages of the simulation.   After some time, for wave phenomena, the domain is filled with reflections of   the initial wave going in every direction and filling every corner of the domain.   At this point, there is in general little one can gain using local mesh   refinement.) 

  To make adaptively changing meshes possible, there are basically two routes.   The "correct" way would be to go back to the weak form we get using Rothe's   method. For example, the first of the two equations to be solved in each time   step looked like this:   [1.x.557]   Now, note that we solve for  [2.x.1403]  on mesh  [2.x.1404] , and   consequently the test functions  [2.x.1405]  have to be from the space    [2.x.1406]  as well. As discussed in the introduction, terms like    [2.x.1407]  then require us to integrate the solution of the   previous step (which may have been computed on a different mesh    [2.x.1408] ) against the test functions of the current mesh,   leading to a matrix  [2.x.1409] . This process of integrating shape   functions from different meshes is, at best, awkward. It can be done   but because it is difficult to ensure that  [2.x.1410]  and    [2.x.1411]  differ by at most one level of refinement, one   has to recursively match cells from both meshes. It is feasible to   do this, but it leads to lengthy and not entirely obvious code. 

  The second approach is the following: whenever we change the mesh,   we simply interpolate the solution from the last time step on the old   mesh to the new mesh, using the SolutionTransfer class. In other words,   instead of the equation above, we would solve   [1.x.558]   where  [2.x.1412]  interpolates a given function onto mesh  [2.x.1413] .   This is a much simpler approach because, in each time step, we no   longer have to worry whether  [2.x.1414]  were computed on the   same mesh as we are using now or on a different mesh. Consequently,   the only changes to the code necessary are the addition of a function   that computes the error, marks cells for refinement, sets up a   SolutionTransfer object, transfers the solution to the new mesh, and   rebuilds matrices and right hand side vectors on the new mesh. Neither   the functions building the matrices and right hand sides, nor the   solvers need to be changed. 

  While this second approach is, strictly speaking,   not quite correct in the Rothe framework (it introduces an addition source   of error, namely the interpolation), it is nevertheless what   almost everyone solving time dependent equations does. We will use this   method in step-31, for example.  [2.x.1415]  


examples/step-24/doc/intro.dox 

[1.x.559] 

[1.x.560] 

This program grew out of a student project by Xing Jin at Texas A&amp;M University. Most of the work for this program is by her. Some of the work on this tutorial program has been funded by NSF under grant DMS-0604778. 

The program is part of a project that aims to simulate thermoacoustic tomography imaging. In thermoacoustic tomography, pulsed electromagnetic energy is delivered into biological issues. Tissues absorb some of this energy and those parts of the tissue that absorb the most energy generate thermoacoustic waves through thermoelastic expansion. For imaging, one uses that different kinds of tissue, most importantly healthy and diseased tissue, absorb different amounts of energy and therefore expand at different rates. The experimental setup is to measure the amplitude of the pressure waves generated by these sources on the surface of the tissue and try to reconstruct the source distributions, which is indicative for the distribution of absorbers and therefore of different kinds of tissue. Part of this project is to compare simulated data with actual measurements, so one has to solve the "forward problem", i.e. the wave equation that describes the propagation of pressure waves in tissue. This program is therefore a continuation of  [2.x.1416]  "step-23", where the wave equation was first introduced. 




[1.x.561] 

The temperature at a given location, neglecting thermal diffusion, can be stated as 

[1.x.562] 



Here  [2.x.1417]  is the density;  [2.x.1418]  is the specific heat;  [2.x.1419]  is the temperature rise due to the delivered microwave energy; and  [2.x.1420]  is the heating function defined as the thermal energy per time and volume transformed from deposited microwave energy. 

Let us assume that tissues have heterogeneous dielectric properties but homogeneous acoustic properties. The basic acoustic generation equation in an acoustically homogeneous medium can be described as follows: if  [2.x.1421]  is the vector-valued displacement, then tissue certainly reacts to changes in pressure by acceleration: 

[1.x.563] 

Furthermore, it contracts due to excess pressure and expands based on changes in temperature: 

[1.x.564] 

Here,  [2.x.1422]  is a thermoexpansion coefficient. 

Let us now make the assumption that heating only happens on a time scale much shorter than wave propagation through tissue (i.e. the temporal length of the microwave pulse that heats the tissue is much shorter than the time it takes a wave to cross the domain). In that case, the heating rate  [2.x.1423]  can be written as  [2.x.1424]  (where  [2.x.1425]  is a map of absorption strengths for microwave energy and  [2.x.1426]  is the Dirac delta function), which together with the first equation above will yield an instantaneous jump in the temperature  [2.x.1427]  at time  [2.x.1428] . Using this assumption, and taking all equations together, we can rewrite and combine the above as follows: 

[1.x.565] 

where  [2.x.1429] . 

This somewhat strange equation with the derivative of a Dirac delta function on the right hand side can be rewritten as an initial value problem as follows: 

[1.x.566] 

(A derivation of this transformation into an initial value problem is given at the end of this introduction as an appendix.) 

In the inverse problem, it is the initial condition  [2.x.1430]  that one would like to recover, since it is a map of absorption strengths for microwave energy, and therefore presumably an indicator to discern healthy from diseased tissue. 

In real application, the thermoacoustic source is very small as compared to the medium.  The propagation path of the thermoacoustic waves can then be approximated as from the source to the infinity. Furthermore, detectors are only a limited distance from the source. One only needs to evaluate the values when the thermoacoustic waves pass through the detectors, although they do continue beyond. This is therefore a problem where we are only interested in a small part of an infinite medium, and we do not want waves generated somewhere to be reflected at the boundary of the domain which we consider interesting. Rather, we would like to simulate only that part of the wave field that is contained inside the domain of interest, and waves that hit the boundary of that domain to simply pass undisturbed through the boundary. In other words, we would like the boundary to absorb any waves that hit it. 

In general, this is a hard problem: Good absorbing boundary conditions are nonlinear and/or numerically very expensive. We therefore opt for a simple first order approximation to absorbing boundary conditions that reads 

[1.x.567] 

Here,  [2.x.1431]  is the normal derivative at the boundary. It should be noted that this is not a particularly good boundary condition, but it is one of the very few that are reasonably simple to implement. 




[1.x.568] 

As in step-23, one first introduces a second variable, which is defined as the derivative of the pressure potential: 

[1.x.569] 



With the second variable, one then transforms the forward problem into two separate equations: 

[1.x.570] 

with initial conditions: 

[1.x.571] 

Note that we have introduced a right hand side  [2.x.1432]  here to show how to derive these formulas in the general case, although in the application to the thermoacoustic problem  [2.x.1433] . 

The semi-discretized, weak version of this model, using the general  [2.x.1434]  scheme introduced in step-23 is then: 

[1.x.572] 

where  [2.x.1435]  is an arbitrary test function, and where we have used the absorbing boundary condition to integrate by parts: absorbing boundary conditions are incorporated into the weak form by using 

[1.x.573] 



From this we obtain the discrete model by introducing a finite number of shape functions, and get 

[1.x.574] 

The matrices  [2.x.1436]  and  [2.x.1437]  are here as in step-23, and the boundary mass matrix 

[1.x.575] 

results from the use of absorbing boundary conditions. 

Above two equations can be rewritten in a matrix form with the pressure and its derivative as an unknown vector: 

[1.x.576] 



where 

[1.x.577] 



By simple transformations, one then obtains two equations for the pressure potential and its derivative, just as in the previous tutorial program: 

[1.x.578] 






[1.x.579] 

Compared to step-23, this programs adds the treatment of a simple absorbing boundary conditions. In addition, it deals with data obtained from actual experimental measurements. To this end, we need to evaluate the solution at points at which the experiment also evaluates a real pressure field. We will see how to do that using the  [2.x.1438]  function further down below. 




[1.x.580] 

In the derivation of the initial value problem for the wave equation, we initially found that the equation had the derivative of a Dirac delta function as a right hand side: 

[1.x.581] 

In order to see how to transform this single equation into the usual statement of a PDE with initial conditions, let us make the assumption that the physically quite reasonable medium is at rest initially, i.e.  [2.x.1439]  for  [2.x.1440] . Next, let us form the indefinite integral with respect to time of both sides: 

[1.x.582] 

This immediately leads to the statement 

[1.x.583] 

where  [2.x.1441]  is such that  [2.x.1442] . Next, we form the (definite) integral over time from  [2.x.1443]  to  [2.x.1444]  to find 

[1.x.584] 

If we use the property of the delta function that  [2.x.1445] , and assume that  [2.x.1446]  is a continuous function in time, we find as we let  [2.x.1447]  go to zero that 

[1.x.585] 

In other words, using that  [2.x.1448] , we retrieve the initial condition 

[1.x.586] 

At the same time, we know that for every  [2.x.1449]  the delta function is zero, so for  [2.x.1450]  we get the equation 

[1.x.587] 

Consequently, we have obtained a representation of the wave equation and one initial condition from the original somewhat strange equation. 

Finally, because we here have an equation with two time derivatives, we still need a second initial condition. To this end, let us go back to the equation 

[1.x.588] 

and integrate it in time from  [2.x.1451]  to  [2.x.1452] . This leads to 

[1.x.589] 

Using integration by parts of the form 

[1.x.590] 

where we use that  [2.x.1453]  and inserting  [2.x.1454] , we see that in fact 

[1.x.591] 



Now, let  [2.x.1455] . Assuming that  [2.x.1456]  is a continuous function in time, we see that 

[1.x.592] 

and consequently 

[1.x.593] 

However, we have assumed that  [2.x.1457] . Consequently, we obtain as the second initial condition that 

[1.x.594] 

completing the system of equations. 


examples/step-24/doc/results.dox 



[1.x.595] 

The program writes both graphical data for each time step as well as the values evaluated at each detector location to disk. We then draw them in plots. Experimental data were also collected for comparison. Currently our experiments have only been done in two dimensions by circularly scanning a single detector. The tissue sample here is a thin slice in the  [2.x.1458]  plane ( [2.x.1459] ), and we assume that signals from other  [2.x.1460]  directions won't contribute to the data. Consequently, we only have to compare our experimental data with two dimensional simulated data. 

[1.x.596] 

This movie shows the thermoacoustic waves generated by a single small absorber propagating in the medium (in our simulation, we assume the medium is mineral oil, which has a acoustic speed of 1.437  [2.x.1461] ): 

 [2.x.1462]  

For a single absorber, we of course have to change the  [2.x.1463]  class accordingly. 

Next, let us compare experimental and computational results. The visualization uses a technique long used in seismology, where the data of each detector is plotted all in one graph. The way this is done is by offsetting each detector's signal a bit compared to the previous one. For example, here is a plot of the first four detectors (from bottom to top, with time in microseconds running from left to right) using the source setup used in the program, to make things a bit more interesting compared to the present case of only a single source: 

 [2.x.1464]  

One thing that can be seen, for example, is that the arrival of the second and fourth signals shifts to earlier times for greater detector numbers (i.e. the topmost ones), but not the first and the third; this can be interpreted to mean that the origin of these signals must be closer to the latter detectors than to the former ones. 

If we stack not only 4, but all 160 detectors in one graph, the individual lines blur, but where they run together they create a pattern of darker or lighter grayscales.  The following two figures show the results obtained at the detector locations stacked in that way. The left figure is obtained from experiments, and the right is the simulated data. In the experiment, a single small strong absorber was embedded in weaker absorbing tissue: 

 [2.x.1465]  

It is obvious that the source location is closer to the detectors at angle  [2.x.1466] . All the other signals that can be seen in the experimental data result from the fact that there are weak absorbers also in the rest of the tissue, which surrounds the signals generated by the small strong absorber in the center. On the other hand, in the simulated data, we only simulate the small strong absorber. 

In reality, detectors have limited bandwidth. The thermoacoustic waves passing through the detector will therefore be filtered. By using a high-pass filter (implemented in MATLAB and run against the data file produced by this program), the simulated results can be made to look closer to the experimental data: 

 [2.x.1467]  

In our simulations, we see spurious signals behind the main wave that result from numerical artifacts. This problem can be alleviated by using finer mesh, resulting in the following plot: 

 [2.x.1468]  




[1.x.597] 

To further verify the program, we will also show simulation results for multiple absorbers. This corresponds to the case that is actually implemented in the program. The following movie shows the propagation of the generated thermoacoustic waves in the medium by multiple absorbers: 

 [2.x.1469]  

Experimental data and our simulated data are compared in the following two figures:  [2.x.1470]  

Note that in the experimental data, the first signal (i.e. the left-most dark line) results from absorption at the tissue boundary, and therefore reaches the detectors first and before any of the signals from the interior. This signal is also faintly visible at the end of the traces, around 30  [2.x.1471] , which indicates that the signal traveled through the entire tissue to reach detectors at the other side, after all the signals originating from the interior have reached them. 

As before, the numerical result better matches experimental ones by applying a bandwidth filter that matches the actual behavior of detectors (left) and by choosing a finer mesh (right): 

 [2.x.1472]  

One of the important differences between the left and the right figure is that the curves look much less "angular" at the right. The angularity comes from the fact that while waves in the continuous equation travel equally fast in all directions, this isn't the case after discretization: there, waves that travel diagonal to cells move at slightly different speeds to those that move parallel to mesh lines. This anisotropy leads to wave fronts that aren't perfectly circular (and would produce sinusoidal signals in the stacked plots), but are bulged out in certain directions. To make things worse, the circular mesh we use (see for example step-6 for a view of the coarse mesh) is not isotropic either. The net result is that the signal fronts are not sinusoidal unless the mesh is sufficiently fine. The right image is a lot better in this respect, though artifacts in the form of trailing spurious waves can still be seen. 


examples/step-25/doc/intro.dox 

[1.x.598] [1.x.599] 

This program grew out of a student project by Ivan Christov at Texas A&amp;M University. Most of the work for this program is by him. 

The goal of this program is to solve the sine-Gordon soliton equation in 1, 2 or 3 spatial dimensions. The motivation for solving this equation is that very little is known about the nature of the solutions in 2D and 3D, even though the 1D case has been studied extensively. 

Rather facetiously, the sine-Gordon equation's moniker is a pun on the so-called Klein-Gordon equation, which is a relativistic version of the Schrödinger equation for particles with non-zero mass. The resemblance is not just superficial, the sine-Gordon equation has been shown to model some unified-field phenomena such as interaction of subatomic particles (see, e.g., Perring &amp; Skyrme in Nuclear %Physics [1.x.600]) and the Josephson (quantum) effect in superconductor junctions (see, e.g., [1.x.601]). Furthermore, from the mathematical standpoint, since the sine-Gordon equation is "completely integrable," it is a candidate for study using the usual methods such as the inverse scattering transform. Consequently, over the years, many interesting solitary-wave, and even stationary, solutions to the sine-Gordon equation have been found. In these solutions, particles correspond to localized features. For more on the sine-Gordon equation, the inverse scattering transform and other methods for finding analytical soliton equations, the reader should consult the following "classical" references on the subject: G. L. Lamb's [1.x.602] (Chapter 5, Section 2) and G. B. Whitham's [1.x.603] (Chapter 17, Sections 10-13). 

 [2.x.1473]  We will cover a separate nonlinear equation from quantum   mechanics, the Nonlinear Schr&ouml;dinger Equation, in step-58. 

[1.x.604] The sine-Gordon initial-boundary-value problem (IBVP) we wish to solve consists of the following equations: 

[1.x.605] It is a nonlinear equation similar to the wave equation we discussed in step-23 and step-24. We have chosen to enforce zero Neumann boundary conditions in order for waves to reflect off the boundaries of our domain. It should be noted, however, that Dirichlet boundary conditions are not appropriate for this problem. Even though the solutions to the sine-Gordon equation are localized, it only makes sense to specify (Dirichlet) boundary conditions at  [2.x.1474] , otherwise either a solution does not exist or only the trivial solution  [2.x.1475]  exists. 

However, the form of the equation above is not ideal for numerical discretization. If we were to discretize the second-order time derivative directly and accurately, then  we would need a large stencil (i.e., several time steps would need to be kept in the memory), which could become expensive. Therefore, in complete analogy to what we did in step-23 and step-24, we split the second-order (in time) sine-Gordon equation into a system of two first-order (in time) equations, which we call the split, or velocity, formulation. To this end, by setting  [2.x.1476] , it is easy to see that the sine-Gordon equation is equivalent to 

[1.x.606] 

[1.x.607] Now, we can discretize the split formulation in time using the  [2.x.1477] -method, which has a stencil of only two time steps. By choosing a  [2.x.1478] , the latter discretization allows us to choose from a continuum of schemes. In particular, if we pick  [2.x.1479]  or  [2.x.1480] , we obtain the first-order accurate explicit or implicit Euler method, respectively. Another important choice is  [2.x.1481] , which gives the second-order accurate Crank-Nicolson scheme. Henceforth, a superscript  [2.x.1482]  denotes the values of the variables at the  [2.x.1483]  time step, i.e. at  [2.x.1484] , where  [2.x.1485]  is the (fixed) time step size. Thus, the split formulation of the time-discretized sine-Gordon equation becomes 

[1.x.608] 

We can simplify the latter via a bit of algebra. Eliminating  [2.x.1486]  from the first equation and rearranging, we obtain 

[1.x.609] 

It may seem as though we can just proceed to discretize the equations in space at this point. While this is true for the second equation (which is linear in  [2.x.1487] ), this would not work for all  [2.x.1488]  since the first equation above is nonlinear. Therefore, a nonlinear solver must be implemented, then the equations can be discretized in space and solved. 

To this end, we can use Newton's method. Given the nonlinear equation  [2.x.1489] , we produce successive approximations to  [2.x.1490]  as follows: 

[1.x.610] The iteration can be initialized with the old time step, i.e.  [2.x.1491] , and eventually it will produce a solution to the first equation of the split formulation (see above). For the time discretization of the sine-Gordon equation under consideration here, we have that 

[1.x.611] Notice that while  [2.x.1492]  is a function,  [2.x.1493]  is an operator. 

[1.x.612] With hindsight, we choose both the solution and the test space to be  [2.x.1494] . Hence, multiplying by a test function  [2.x.1495]  and integrating, we obtain the following variational (or weak) formulation of the split formulation (including the nonlinear solver for the first equation) at each time step: 

[1.x.613] Note that the we have used integration by parts and the zero Neumann boundary conditions on all terms involving the Laplacian operator. Moreover,  [2.x.1496]  and  [2.x.1497]  are as defined above, and  [2.x.1498]  denotes the usual  [2.x.1499]  inner product over the domain  [2.x.1500] , i.e.  [2.x.1501] . Finally, notice that the first equation is, in fact, the definition of an iterative procedure, so it is solved multiple times during each time step until a stopping criterion is met. 

[1.x.614] Using the Finite Element Method, we discretize the variational formulation in space. To this end, let  [2.x.1502]  be a finite-dimensional  [2.x.1503] -conforming finite element space ( [2.x.1504] ) with nodal basis  [2.x.1505] . Now, we can expand all functions in the weak formulation (see above) in terms of the nodal basis. Henceforth, we shall denote by a capital letter the vector of coefficients (in the nodal basis) of a function denoted by the same letter in lower case; e.g.,  [2.x.1506]  where  [2.x.1507]  and  [2.x.1508] . Thus, the finite-dimensional version of the variational formulation requires that we solve the following matrix equations at each time step: 

[1.x.615] 

Above, the matrix  [2.x.1509]  and the vector  [2.x.1510]  denote the discrete versions of the gadgets discussed above, i.e., 

[1.x.616] Again, note that the first matrix equation above is, in fact, the definition of an iterative procedure, so it is solved multiple times until a stopping criterion is met. Moreover,  [2.x.1511]  is the mass matrix, i.e.  [2.x.1512] ,  [2.x.1513]  is the Laplace matrix, i.e.  [2.x.1514] ,  [2.x.1515]  is the nonlinear term in the equation that defines our auxiliary velocity variable, i.e.  [2.x.1516] , and  [2.x.1517]  is the nonlinear term in the Jacobian matrix of  [2.x.1518] , i.e.  [2.x.1519] . 

What solvers can we use for the first equation? Let's look at the matrix we have to invert: 

[1.x.617] 

for some  [2.x.1520]  that depends on the present and previous solution. First, note that the matrix is symmetric. In addition, if the time step  [2.x.1521]  is small enough, i.e. if  [2.x.1522] , then the matrix is also going to be positive definite. In the program below, this will always be the case, so we will use the Conjugate Gradient method together with the SSOR method as preconditioner. We should keep in mind, however, that this will fail if we happen to use a bigger time step. Fortunately, in that case the solver will just throw an exception indicating a failure to converge, rather than silently producing a wrong result. If that happens, then we can simply replace the CG method by something that can handle indefinite symmetric systems. The GMRES solver is typically the standard method for all "bad" linear systems, but it is also a slow one. Possibly better would be a solver that utilizes the symmetry, such as, for example, SymmLQ, which is also implemented in deal.II. 

This program uses a clever optimization over step-23 and  [2.x.1523]  "step-24": If you read the above formulas closely, it becomes clear that the velocity  [2.x.1524]  only ever appears in products with the mass matrix. In step-23 and step-24, we were, therefore, a bit wasteful: in each time step, we would solve a linear system with the mass matrix, only to multiply the solution of that system by  [2.x.1525]  again in the next time step. This can, of course, be avoided, and we do so in this program. 




[1.x.618] 

There are a few analytical solutions for the sine-Gordon equation, both in 1D and 2D. In particular, the program as is computes the solution to a problem with a single kink-like solitary wave initial condition.  This solution is given by Leibbrandt in \e Phys. \e Rev. \e Lett. \b 41(7), and is implemented in the  [2.x.1526]  class. 

It should be noted that this closed-form solution, strictly speaking, only holds for the infinite-space initial-value problem (not the Neumann initial-boundary-value problem under consideration here). However, given that we impose \e zero Neumann boundary conditions, we expect that the solution to our initial-boundary-value problem would be close to the solution of the infinite-space initial-value problem, if reflections of waves off the boundaries of our domain do \e not occur. In practice, this is of course not the case, but we can at least assume that this were so. 

The constants  [2.x.1527]  and  [2.x.1528]  in the 2D solution and  [2.x.1529] ,  [2.x.1530]  and  [2.x.1531]  in the 3D solution are called the B&auml;cklund transformation parameters. They control such things as the orientation and steepness of the kink. For the purposes of testing the code against the exact solution, one should choose the parameters so that the kink is aligned with the grid. 

The solutions that we implement in the  [2.x.1532]  class are these:  [2.x.1533]     [2.x.1534] In 1D:   [1.x.619] 

  where we choose  [2.x.1535] . 

  In 1D, more interesting analytical solutions are known. Many of them are   listed on http://mathworld.wolfram.com/Sine-GordonEquation.html . 

   [2.x.1536] In 2D:   [1.x.620] 

  where  [2.x.1537]  is defined as   [1.x.621] 

  and where we choose  [2.x.1538] . 

   [2.x.1539] In 3D:   [1.x.622] 

  where  [2.x.1540]  is defined as   [1.x.623] 

  and where we choose  [2.x.1541] .  [2.x.1542]  


Since it makes it easier to play around, the  [2.x.1543]  class that is used to set &mdash; surprise! &mdash; the initial values of our simulation simply queries the class that describes the exact solution for the value at the initial time, rather than duplicating the effort to implement a solution function. 


examples/step-25/doc/results.dox 



[1.x.624] The explicit Euler time stepping scheme  ( [2.x.1544] ) performs adequately for the problems we wish to solve. Unfortunately, a rather small time step has to be chosen due to stability issues ---  [2.x.1545]  appears to work for most the simulations we performed. On the other hand, the Crank-Nicolson scheme ( [2.x.1546] ) is unconditionally stable, and (at least for the case of the 1D breather) we can pick the time step to be as large as  [2.x.1547]  without any ill effects on the solution. The implicit Euler scheme ( [2.x.1548] ) is "exponentially damped," so it is not a good choice for solving the sine-Gordon equation, which is conservative. However, some of the damped schemes in the continuum that is offered by the  [2.x.1549] -method were useful for eliminating spurious oscillations due to boundary effects. 

In the simulations below, we solve the sine-Gordon equation on the interval  [2.x.1550]  in 1D and on the square  [2.x.1551]  in 2D. In each case, the respective grid is refined uniformly 6 times, i.e.  [2.x.1552] . 

[1.x.625] The first example we discuss is the so-called 1D (stationary) breather solution of the sine-Gordon equation. The breather has the following closed-form expression, as mentioned in the Introduction: 

[1.x.626] where  [2.x.1553] ,  [2.x.1554]  and  [2.x.1555]  are constants. In the simulation below, we have chosen  [2.x.1556] ,  [2.x.1557] ,  [2.x.1558] . Moreover, it is know that the period of oscillation of the breather is  [2.x.1559] , hence we have chosen  [2.x.1560]  and  [2.x.1561]  so that we can observe three oscillations of the solution. Then, taking  [2.x.1562] ,  [2.x.1563]  and  [2.x.1564] , the program computed the following solution. 

 [2.x.1565]  

Though not shown how to do this in the program, another way to visualize the (1+1)-d solution is to use output generated by the DataOutStack class; it allows to "stack" the solutions of individual time steps, so that we get 2D space-time graphs from 1D time-dependent solutions. This produces the space-time plot below instead of the animation above. 

 [2.x.1566]  

Furthermore, since the breather is an analytical solution of the sine-Gordon equation, we can use it to validate our code, although we have to assume that the error introduced by our choice of Neumann boundary conditions is small compared to the numerical error. Under this assumption, one could use the  [2.x.1567]  function to compute the difference between the numerical solution and the function described by the  [2.x.1568]  class of this program. For the simulation shown in the two images above, the  [2.x.1569]  norm of the error in the finite element solution at each time step remained on the order of  [2.x.1570] . Hence, we can conclude that the numerical method has been implemented correctly in the program. 




[1.x.627] 

The only analytical solution to the sine-Gordon equation in (2+1)D that can be found in the literature is the so-called kink solitary wave. It has the following closed-form expression:   [1.x.628] 

with   [1.x.629] 

where  [2.x.1571] ,  [2.x.1572]  and  [2.x.1573]  are constants. In the simulation below we have chosen  [2.x.1574] . Notice that if  [2.x.1575]  the kink is stationary, hence it would make a good solution against which we can validate the program in 2D because no reflections off the boundary of the domain occur. 

The simulation shown below was performed with  [2.x.1576] ,  [2.x.1577] ,  [2.x.1578] ,  [2.x.1579]  and  [2.x.1580] . The  [2.x.1581]  norm of the error of the finite element solution at each time step remained on the order of  [2.x.1582] , showing that the program is working correctly in 2D, as well as 1D. Unfortunately, the solution is not very interesting, nonetheless we have included a snapshot of it below for completeness. 

 [2.x.1583]  

Now that we have validated the code in 1D and 2D, we move to a problem where the analytical solution is unknown. 

To this end, we rotate the kink solution discussed above about the  [2.x.1584]  axis: we let   [2.x.1585] . The latter results in a solitary wave that is not aligned with the grid, so reflections occur at the boundaries of the domain immediately. For the simulation shown below, we have taken  [2.x.1586] ,  [2.x.1587] ,  [2.x.1588] ,  [2.x.1589]  and  [2.x.1590] . Moreover, we had to pick  [2.x.1591]  because for any  [2.x.1592]  oscillations arose at the boundary, which are likely due to the scheme and not the equation, thus picking a value of  [2.x.1593]  a good bit into the "exponentially damped" spectrum of the time stepping schemes assures these oscillations are not created. 

 [2.x.1594]  

Another interesting solution to the sine-Gordon equation (which cannot be obtained analytically) can be produced by using two 1D breathers to construct the following separable 2D initial condition: 

[1.x.630] where  [2.x.1595] ,  [2.x.1596]  as in the 1D case we discussed above. For the simulation shown below, we have chosen  [2.x.1597] ,  [2.x.1598] ,  [2.x.1599]  and  [2.x.1600] . The solution is pretty interesting 

--- it acts like a breather (as far as the pictures are concerned); however, it appears to break up and reassemble, rather than just oscillate. 

 [2.x.1601]  


[1.x.631] 

[1.x.632] 

It is instructive to change the initial conditions. Most choices will not lead to solutions that stay localized (in the soliton community, such solutions are called "stationary", though the solution does change with time), but lead to solutions where the wave-like character of the equation dominates and a wave travels away from the location of a localized initial condition. For example, it is worth playing around with the  [2.x.1602]  class, by replacing the call to the  [2.x.1603]  class by something like this function: 

[1.x.633] 

if  [2.x.1604] , and  [2.x.1605]  outside this region. 

A second area would be to investigate whether the scheme is energy-preserving. For the pure wave equation, discussed in  [2.x.1606]  "step-23", this is the case if we choose the time stepping parameter such that we get the Crank-Nicolson scheme. One could do a similar thing here, noting that the energy in the sine-Gordon solution is defined as 

[1.x.634] 

(We use  [2.x.1607]  instead of  [2.x.1608]  in the formula to ensure that all contributions to the energy are positive, and so that decaying solutions have finite energy on unbounded domains.) 

Beyond this, there are two obvious areas: 

- Clearly, adaptivity (i.e. time-adaptive grids) would be of interest   to problems like these. Their complexity leads us to leave this out   of this program again, though the general comments in the   introduction of  [2.x.1609]  "step-23" remain true. 

- Faster schemes to solve this problem. While computers today are   plenty fast enough to solve 2d and, frequently, even 3d stationary   problems within not too much time, time dependent problems present   an entirely different class of problems. We address this topic in   step-48 where we show how to solve this problem in parallel and   without assembling or inverting any matrix at all. 


examples/step-26/doc/intro.dox 

[1.x.635] 

[1.x.636] 

 [2.x.1610]  ( [2.x.1611]  


This program implements the heat equation 

[1.x.637] 

In some sense, this equation is simpler than the ones we have discussed in the preceding programs step-23, step-24, step-25, namely the wave equation. This is due to the fact that the heat equation smoothes out the solution over time, and is consequently more forgiving in many regards. For example, when using implicit time stepping methods, we can actually take large time steps, we have less trouble with the small disturbances we introduce through adapting the mesh every few time steps, etc. 

Our goal here will be to solve the equations above using the theta-scheme that discretizes the equation in time using the following approach, where we would like  [2.x.1612]  to approximate  [2.x.1613]  at some time  [2.x.1614] : 

[1.x.638] 

Here,  [2.x.1615]  is the time step size. The theta-scheme generalizes the explicit Euler ( [2.x.1616] ), implicit Euler ( [2.x.1617] ) and Crank-Nicolson ( [2.x.1618] ) time discretizations. Since the latter has the highest convergence order, we will choose  [2.x.1619]  in the program below, but make it so that playing with this parameter remains simple. (If you are interested in playing with higher order methods, take a look at step-52.) 

Given this time discretization, space discretization happens as it always does, by multiplying with test functions, integrating by parts, and then restricting everything to a finite dimensional subspace. This yields the following set of fully discrete equations after multiplying through with  [2.x.1620] : 

[1.x.639] 

where  [2.x.1621]  is the mass matrix and  [2.x.1622]  is the stiffness matrix that results from discretizing the Laplacian. Bringing all known quantities to the right hand side yields the linear system we have to solve in every step: 

[1.x.640] 

The linear system on the left hand side is symmetric and positive definite, so we should have no trouble solving it with the Conjugate Gradient method. 

We can start the iteration above if we have the set of nodal coefficients  [2.x.1623]  at the initial time. Here, we take the ones we get by interpolating the initial values  [2.x.1624]  onto the mesh used for the first time step. We will also need to choose a time step; we will here just choose it as fixed, but clearly advanced simulators will want to choose it adaptively. We will briefly come back to this in the [1.x.641]. 




[1.x.642] 

When solving the wave equation and its variants in the previous few programs, we kept the mesh fixed. Just as for stationary equations, one can make a good case that this is not the smartest approach and that significant savings can be had by adapting the mesh. There are, however, significant difficulties compared to the stationary case. Let us go through them in turn: 

 [2.x.1625]     [2.x.1626] [1.x.643]: For stationary problems, the   general approach is "make the mesh as fine as it is necessary". For problems   with singularities, this often leads to situations where we get many levels   of refinement into corners or along interfaces. The very first tutorial to   use adaptive meshes, step-6, is a point in case already. 

  However, for time dependent problems, we typically need to choose the time   step related to the mesh size. For explicit time discretizations, this is   obvious, since we need to respect a CFL condition that ties the time step   size to the smallest mesh size. For implicit time discretizations, no such   hard restriction exists, but in practice we still want to make the time step   smaller if we make the mesh size smaller since we typically have error   estimates of the form  [2.x.1627]  where  [2.x.1628]  are the   convergence orders of the time and space discretization, respectively. We   can only make the error small if we decrease both terms. Ideally, an   estimate like this would suggest to choose  [2.x.1629] . Because, at   least for problems with non-smooth solutions, the error is typically   localized in the cells with the smallest mesh size, we have to indeed choose    [2.x.1630] , using the [1.x.644] mesh size. 

  The consequence is that refining the mesh further in one place implies not   only the moderate additional effort of increasing the number of degrees of   freedom slightly, but also the much larger effort of having the solve the   [1.x.645] linear system more often because of the smaller time step. 

  In practice, one typically deals with this by acknowledging that we can not   make the time step arbitrarily small, and consequently can not make the   local mesh size arbitrarily small. Rather, we set a maximal level of   refinement and when we flag cells for refinement, we simply do not refine   those cells whose children would exceed this maximal level of refinement. 

  There is a similar problem in that we will choose a right hand side that   will switch on in different parts of the domain at different times. To avoid   being caught flat footed with too coarse a mesh in areas where we suddenly   need a finer mesh, we will also enforce in our program a [1.x.646] mesh   refinement level. 

   [2.x.1631] [1.x.647]: Let us consider again the   semi-discrete equations we have written down above:   [1.x.648] 

  We can here consider  [2.x.1632]  as data since it has presumably been computed   before. Now, let us replace   [1.x.649] 

  multiply with test functions  [2.x.1633]  and integrate by parts   where necessary. In a process as outlined above, this would yield   [1.x.650] 

  Now imagine that we have changed the mesh between time steps  [2.x.1634]  and    [2.x.1635] . Then the problem is that the basis functions we use for  [2.x.1636]  and    [2.x.1637]  are different! This pertains to the terms on the right hand side,   the first of which we could more clearly write as (the second follows the   same pattern)   [1.x.651] 

  If the meshes used in these two time steps are the same, then    [2.x.1638]  forms a square mass matrix    [2.x.1639] . However, if the meshes are not the same, then in general the matrix   is rectangular. Worse, it is difficult to even compute these integrals   because if we loop over the cells of the mesh at time step  [2.x.1640] , then we need   to evaluate  [2.x.1641]  at the quadrature points of these cells, but   they do not necessarily correspond to the cells of the mesh at time step    [2.x.1642]  and  [2.x.1643]  is not defined via these cells; the same of   course applies if we wanted to compute the integrals via integration on the   cells of mesh  [2.x.1644] . 

  In any case, what we have to face is a situation where we need to integrate   shape functions defined on two different meshes. This can be done, and is in   fact demonstrated in step-28, but the process is at best described by the   word "awkward". 

  In practice, one does not typically want to do this. Rather, we avoid the   whole situation by interpolating the solution from the old to the new mesh   every time we adapt the mesh. In other words, rather than solving the   equations above, we instead solve the problem   [1.x.652] 

  where  [2.x.1645]  is the interpolation operator onto the finite element space   used in time step  [2.x.1646] . This is not the optimal approach since it introduces   an additional error besides time and space discretization, but it is a   pragmatic one that makes it feasible to do time adapting meshes.  [2.x.1647]  




[1.x.653] 

There are a number of things one can typically get wrong when implementing a finite element code. In particular, for time dependent problems, the following are common sources of bugs: 

- The time integration, for example by getting the coefficients in front of   the terms involving the current and previous time steps wrong (e.g., mixing   up a factor  [2.x.1648]  for  [2.x.1649] ). 

- Handling the right hand side, for example forgetting a factor of  [2.x.1650]  or    [2.x.1651] . 

- Mishandling the boundary values, again for example forgetting a factor of    [2.x.1652]  or  [2.x.1653] , or forgetting to apply nonzero boundary values not only   to the right hand side but also to the system matrix. 

A less common problem is getting the initial conditions wrong because one can typically see that it is wrong by just outputting the first time step. In any case, in order to verify the correctness of the code, it is helpful to have a testing protocol that allows us to verify each of these components separately. This means: 

- Testing the code with nonzero initial conditions but zero right hand side   and boundary values and verifying that the time evolution is correct. 

- Then testing with zero initial conditions and boundary values but nonzero   right hand side and again ensuring correctness. 

- Finally, testing with zero initial conditions and right hand side but   nonzero boundary values. 

This sounds complicated, but fortunately, for linear partial differential equations without coefficients (or constant coefficients) like the one here, there is a fairly standard protocol that rests on the following observation: if you choose as your domain a square  [2.x.1654]  (or, with slight modifications, a rectangle), then the exact solution can be written as 

[1.x.654] 

(with integer constants  [2.x.1655] ) if only the initial condition, right hand side and boundary values are all of the form  [2.x.1656]  as well. This is due to the fact that the function  [2.x.1657]  is an eigenfunction of the Laplace operator and allows us to compute things like the time factor  [2.x.1658]  analytically and, consequently, compare with what we get numerically. 

As an example, let us consider the situation where we have  [2.x.1659]  and  [2.x.1660] . With the claim (ansatz) of the form for  [2.x.1661]  above, we get that 

[1.x.655] 

For this to be equal to  [2.x.1662] , we need that 

[1.x.656] 

and due to the initial conditions,  [2.x.1663] . This differential equation can be integrated to yield 

[1.x.657] 

In other words, if the initial condition is a product of sines, then the solution has exactly the same shape of a product of sines that decays to zero with a known time dependence. This is something that is easy to test if you have a sufficiently fine mesh and sufficiently small time step. 

What is typically going to happen if you get the time integration scheme wrong (e.g., by having the wrong factors of  [2.x.1664]  or  [2.x.1665]  in front of the various terms) is that you don't get the right temporal behavior of the solution. Double check the various factors until you get the right behavior. You may also want to verify that the temporal decay rate (as determined, for example, by plotting the value of the solution at a fixed point) does not double or halve each time you double or halve the time step or mesh size. You know that it's not the handling of the boundary conditions or right hand side because these were both zero. 

If you have so verified that the time integrator is correct, take the situation where the right hand side is nonzero but the initial conditions are zero:  [2.x.1666]  and  [2.x.1667] . Again, 

[1.x.658] 

and for this to be equal to  [2.x.1668] , we need that 

[1.x.659] 

and due to the initial conditions,  [2.x.1669] . Integrating this equation in time yields 

[1.x.660] 



Again, if you have the wrong factors of  [2.x.1670]  or  [2.x.1671]  in front of the right hand side terms you will either not get the right temporal behavior of the solution, or it will converge to a maximum value other than  [2.x.1672] . 

Once we have verified that the time integration and right hand side handling are correct using this scheme, we can go on to verifying that we have the boundary values correct, using a very similar approach. 




[1.x.661] 

Solving the heat equation on a simple domain with a simple right hand side almost always leads to solutions that are exceedingly boring, since they become very smooth very quickly and then do not move very much any more. Rather, we here solve the equation on the L-shaped domain with zero Dirichlet boundary values and zero initial conditions, but as right hand side we choose 

[1.x.662] 

Here, 

[1.x.663] 

In other words, in every period of length  [2.x.1673] , the right hand side first flashes on in domain 1, then off completely, then on in domain 2, then off completely again. This pattern is probably best observed via the little animation of the solution shown in the [1.x.664]. 

If you interpret the heat equation as finding the spatially and temporally variable temperature distribution of a conducting solid, then the test case above corresponds to an L-shaped body where we keep the boundary at zero temperature, and heat alternatingly in two parts of the domain. While heating is in effect, the temperature rises in these places, after which it diffuses and diminishes again. The point of these initial conditions is that they provide us with a solution that has singularities both in time (when sources switch on and off) as well as time (at the reentrant corner as well as at the edges and corners of the regions where the source acts). 


examples/step-26/doc/results.dox 



[1.x.665] 

As in many of the tutorials, the actual output of the program matters less than how we arrived there. Nonetheless, here it is: 

[1.x.666] 



Maybe of more interest is a visualization of the solution and the mesh on which it was computed: 

 [2.x.1674]  

The movie shows how the two sources switch on and off and how the mesh reacts to this. It is quite obvious that the mesh as is is probably not the best we could come up with. We'll get back to this in the next section. 


[1.x.667] 

[1.x.668] 

There are at least two areas where one can improve this program significantly: adaptive time stepping and a better choice of the mesh. 

[1.x.669] 

Having chosen an implicit time stepping scheme, we are not bound by any CFL-like condition on the time step. Furthermore, because the time scales on which change happens on a given cell in the heat equation are not bound to the cells diameter (unlike the case with the wave equation, where we had a fixed speed of information transport that couples the temporal and spatial scales), we can choose the time step as we please. Or, better, choose it as we deem necessary for accuracy. 

Looking at the solution, it is clear that the action does not happen uniformly over time: a lot is changing around the time we switch on a source, things become less dramatic once a source is on for a little while, and we enter a long phase of decline when both sources are off. During these times, we could surely get away with a larger time step than before without sacrificing too much accuracy. 

The literature has many suggestions on how to choose the time step size adaptively. Much can be learned, for example, from the way ODE solvers choose their time steps. One can also be inspired by a posteriori error estimators that can, ideally, be written in a way that the consist of a temporal and a spatial contribution to the overall error. If the temporal one is too large, we should choose a smaller time step. Ideas in this direction can be found, for example, in the PhD thesis of a former principal developer of deal.II, Ralf Hartmann, published by the University of Heidelberg, Germany, in 2002. 




[1.x.670] 

We here use one of the simpler time stepping methods, namely the second order in time Crank-Nicolson method. However, more accurate methods such as Runge-Kutta methods are available and should be used as they do not represent much additional effort. It is not difficult to implement this for the current program, but a more systematic treatment is also given in step-52. 




[1.x.671] 

If you look at the meshes in the movie above, it is clear that they are not particularly well suited to the task at hand. In fact, they look rather random. 

There are two factors at play. First, there are some islands where cells have been refined but that are surrounded by non-refined cells (and there are probably also a few occasional coarsened islands). These are not terrible, as they most of the time do not affect the approximation quality of the mesh, but they also don't help because so many of their additional degrees of freedom are in fact constrained by hanging node constraints. That said, this is easy to fix: the Triangulation class takes an argument to its constructor indicating a level of "mesh smoothing". Passing one of many possible flags, this instructs the triangulation to refine some additional cells, or not to refine some cells, so that the resulting mesh does not have these artifacts. 

The second problem is more severe: the mesh appears to lag the solution. The underlying reason is that we only adapt the mesh once every fifth time step, and only allow for a single refinement in these cases. Whenever a source switches on, the solution had been very smooth in this area before and the mesh was consequently rather coarse. This implies that the next time step when we refine the mesh, we will get one refinement level more in this area, and five time steps later another level, etc. But this is not enough: first, we should refine immediately when a source switches on (after all, in the current context we at least know what the right hand side is), and we should allow for more than one refinement level. Of course, all of this can be done using deal.II, it just requires a bit of algorithmic thinking in how to make this work! 




[1.x.672] 

To increase the accuracy and resolution of your simulation in time, one typically decreases the time step size  [2.x.1675] . If you start playing around with the time step in this particular example, you will notice that the solution becomes partly negative, if  [2.x.1676]  is below a certain threshold. This is not what we would expect to happen (in nature). 

To get an idea of this behavior mathematically, let us consider a general, fully discrete problem: 

[1.x.673] 

The general form of the  [2.x.1677] th equation then reads: 

[1.x.674] 

where  [2.x.1678]  is the set of degrees of freedom that DoF  [2.x.1679]  couples with (i.e., for which either the matrix  [2.x.1680]  or matrix  [2.x.1681]  has a nonzero entry at position  [2.x.1682] ). If all coefficients fulfill the following conditions: 

[1.x.675] 

all solutions  [2.x.1683]  keep their sign from the previous ones  [2.x.1684] , and consequently from the initial values  [2.x.1685] . See e.g. [1.x.676] for more information on positivity preservation. 

Depending on the PDE to solve and the time integration scheme used, one is able to deduce conditions for the time step  [2.x.1686] . For the heat equation with the Crank-Nicolson scheme, [1.x.677] have translated it to the following ones: 

[1.x.678] 

where  [2.x.1687]  denotes the mass matrix and  [2.x.1688]  the stiffness matrix with  [2.x.1689]  for  [2.x.1690] , respectively. With  [2.x.1691] , we can formulate bounds for the global time step  [2.x.1692]  as follows: 

[1.x.679] 

In other words, the time step is constrained by [1.x.680] in case of a Crank-Nicolson scheme. These bounds should be considered along with the CFL condition to ensure significance of the performed simulations. 

Being unable to make the time step as small as we want to get more accuracy without losing the positivity property is annoying. It raises the question of whether we can at least [1.x.681] the minimal time step we can choose  to ensure positivity preservation in this particular tutorial. Indeed, we can use the SparseMatrix objects for both mass and stiffness that are created via the MatrixCreator functions. Iterating through each entry via SparseMatrixIterators lets us check for diagonal and off-diagonal entries to set a proper time step dynamically. For quadratic matrices, the diagonal element is stored as the first member of a row (see SparseMatrix documentation). An exemplary code snippet on how to grab the entries of interest from the  [2.x.1693]  is shown below. 

[1.x.682] 



Using the information so computed, we can bound the time step via the formulas above. 


examples/step-27/doc/intro.dox 

[1.x.683] 

[1.x.684] 

This tutorial program attempts to show how to use  [2.x.1694] -finite element methods with deal.II. It solves the Laplace equation and so builds only on the first few tutorial programs, in particular on step-4 for dimension independent programming and step-6 for adaptive mesh refinement. 

The  [2.x.1695] -finite element method was proposed in the early 1980s by Babu&scaron;ka and Guo as an alternative to either (i) mesh refinement (i.e., decreasing the mesh parameter  [2.x.1696]  in a finite element computation) or (ii) increasing the polynomial degree  [2.x.1697]  used for shape functions. It is based on the observation that increasing the polynomial degree of the shape functions reduces the approximation error if the solution is sufficiently smooth. On the other hand, it is well known that even for the generally well-behaved class of elliptic problems, higher degrees of regularity can not be guaranteed in the vicinity of boundaries, corners, or where coefficients are discontinuous; consequently, the approximation can not be improved in these areas by increasing the polynomial degree  [2.x.1698]  but only by refining the mesh, i.e., by reducing the mesh size  [2.x.1699] . These differing means to reduce the error have led to the notion of  [2.x.1700] -finite elements, where the approximating finite element spaces are adapted to have a high polynomial degree  [2.x.1701]  wherever the solution is sufficiently smooth, while the mesh width  [2.x.1702]  is reduced at places wherever the solution lacks regularity. It was already realized in the first papers on this method that  [2.x.1703] -finite elements can be a powerful tool that can guarantee that the error is reduced not only with some negative power of the number of degrees of freedom, but in fact exponentially. 

In order to implement this method, we need several things above and beyond what a usual finite element program needs, and in particular above what we have introduced in the tutorial programs leading up to step-6. In particular, we will have to discuss the following aspects:  [2.x.1704]     [2.x.1705] Instead of using the same finite element on all cells, we now will want   a collection of finite element objects, and associate each cell with one   of these objects in this collection. [2.x.1706]  

   [2.x.1707] Degrees of freedom will then have to be allocated on each cell depending   on what finite element is associated with this particular cell. Constraints   will have to be generated in the same way as for hanging nodes, but we now   also have to deal with the case where two neighboring cells have different   finite elements assigned. [2.x.1708]  

   [2.x.1709] We will need to be able to assemble cell and face contributions   to global matrices and right hand side vectors. [2.x.1710]  

   [2.x.1711] After solving the resulting linear system, we will want to   analyze the solution. In particular, we will want to compute error   indicators that tell us whether a given cell should be refined   and/or whether the polynomial degree of the shape functions used on   it should be increased. [2.x.1712]   [2.x.1713]  

We will discuss all these aspects in the following subsections of this introduction. It will not come as a big surprise that most of these tasks are already well supported by functionality provided by the deal.II, and that we will only have to provide the logic of what the program should do, not exactly how all this is going to happen. 

In deal.II, the  [2.x.1714] -functionality is largely packaged into the hp-namespace. This namespace provides classes that handle  [2.x.1715] -discretizations, assembling matrices and vectors, and other tasks. We will get to know many of them further down below. In addition, most of the functions in the DoFTools, and VectorTools namespaces accept  [2.x.1716] -objects in addition to the non- [2.x.1717] -ones. Much of the  [2.x.1718] -implementation is also discussed in the  [2.x.1719]  documentation module and the links found there. 

It may be worth giving a slightly larger perspective at the end of this first part of the introduction.  [2.x.1720] -functionality has been implemented in a number of different finite element packages (see, for example, the list of references cited in the  [2.x.1721]  "hp-paper"). However, by and large, most of these packages have implemented it only for the (i) the 2d case, and/or (ii) the discontinuous Galerkin method. The latter is a significant simplification because discontinuous finite elements by definition do not require continuity across faces between cells and therefore do not require the special treatment otherwise necessary whenever finite elements of different polynomial degree meet at a common face. In contrast, deal.II implements the most general case, i.e., it allows for continuous and discontinuous elements in 1d, 2d, and 3d, and automatically handles the resulting complexity. In particular, it handles computing the constraints (similar to hanging node constraints) of elements of different degree meeting at a face or edge. The many algorithmic and data structure techniques necessary for this are described in the  [2.x.1722]  "hp-paper" for those interested in such detail. 

We hope that providing such a general implementation will help explore the potential of  [2.x.1723] -methods further. 




[1.x.685] 

Now on again to the details of how to use the  [2.x.1724] -functionality in deal.II. The first aspect we have to deal with is that now we do not have only a single finite element any more that is used on all cells, but a number of different elements that cells can choose to use. For this, deal.II introduces the concept of a [1.x.686], implemented in the class  [2.x.1725]  In essence, such a collection acts like an object of type  [2.x.1726] , but with a few more bells and whistles and a memory management better suited to the task at hand. As we will later see, we will also use similar quadrature collections, and &mdash; although we don't use them here &mdash; there is also the concept of mapping collections. All of these classes are described in the  [2.x.1727]  overview. 

In this tutorial program, we will use continuous Lagrange elements of orders 2 through 7 (in 2d) or 2 through 5 (in 3d). The collection of used elements can then be created as follows: 

[1.x.687] 






[1.x.688][1.x.689] 

The next task we have to consider is what to do with the list of finite element objects we want to use. In previous tutorial programs, starting with step-2, we have seen that the DoFHandler class is responsible for making the connection between a mesh (described by a Triangulation object) and a finite element, by allocating the correct number of degrees of freedom for each vertex, face, edge, and cell of the mesh. 

The situation here is a bit more complicated since we do not just have a single finite element object, but rather may want to use different elements on different cells. We therefore need two things: (i) a version of the DoFHandler class that can deal with this situation, and (ii) a way to tell the DoFHandler which element to use on which cell. 

The first of these two things is implemented in the [1.x.690]-mode of the DoFHandler class: rather than associating it with a triangulation and a single finite element object, it is associated with a triangulation and a finite element collection. The second part is achieved by a loop over all cells of this DoFHandler and for each cell setting the index of the finite element within the collection that shall be used on this cell. We call the index of the finite element object within the collection that shall be used on a cell the cell's [1.x.691] to indicate that this is the finite element that is active on this cell, whereas all the other elements of the collection are inactive on it. The general outline of this reads like this: 

[1.x.692] 



Dots in the call to  [2.x.1728]  indicate that we will have to have some sort of strategy later on to decide which element to use on which cell; we will come back to this later. The main point here is that the first and last line of this code snippet is pretty much exactly the same as for the non- [2.x.1729] -case. 

Another complication arises from the fact that this time we do not simply have hanging nodes from local mesh refinement, but we also have to deal with the case that if there are two cells with different active finite element indices meeting at a face (for example a Q2 and a Q3 element) then we have to compute additional constraints on the finite element field to ensure that it is continuous. This is conceptually very similar to how we compute hanging node constraints, and in fact the code looks exactly the same: 

[1.x.693] 

In other words, the  [2.x.1730]  deals not only with hanging node constraints, but also with  [2.x.1731] -constraints at the same time. 




[1.x.694] 

Following this, we have to set up matrices and vectors for the linear system of the correct size and assemble them. Setting them up works in exactly the same way as for the non- [2.x.1732] -case. Assembling requires a bit more thought. 

The main idea is of course unchanged: we have to loop over all cells, assemble local contributions, and then copy them into the global objects. As discussed in some detail first in step-3, deal.II has the FEValues class that pulls the finite element description, mapping, and quadrature formula together and aids in evaluating values and gradients of shape functions as well as other information on each of the quadrature points mapped to the real location of a cell. Every time we move on to a new cell we re-initialize this FEValues object, thereby asking it to re-compute that part of the information that changes from cell to cell. It can then be used to sum up local contributions to bilinear form and right hand side. 

In the context of  [2.x.1733] -finite element methods, we have to deal with the fact that we do not use the same finite element object on each cell. In fact, we should not even use the same quadrature object for all cells, but rather higher order quadrature formulas for cells where we use higher order finite elements. Similarly, we may want to use higher order mappings on such cells as well. 

To facilitate these considerations, deal.II has a class  [2.x.1734]  that does what we need in the current context. The difference is that instead of a single finite element, quadrature formula, and mapping, it takes collections of these objects. It's use is very much like the regular FEValues class, i.e., the interesting part of the loop over all cells would look like this: 

[1.x.695] 



In this tutorial program, we will always use a Q1 mapping, so the mapping collection argument to the  [2.x.1735]  construction will be omitted. Inside the loop, we first initialize the  [2.x.1736]  object for the current cell. The second, third and fourth arguments denote the index within their respective collections of the quadrature, mapping, and finite element objects we wish to use on this cell. These arguments can be omitted (and are in the program below), in which case  [2.x.1737]  is used for this index. The order of these arguments is chosen in this way because one may sometimes want to pick a different quadrature or mapping object from their respective collections, but hardly ever a different finite element than the one in use on this cell, i.e., one with an index different from  [2.x.1738] . The finite element collection index is therefore the last default argument so that it can be conveniently omitted. 

What this  [2.x.1739]  call does is the following: the  [2.x.1740]  class checks whether it has previously already allocated a non- [2.x.1741] -FEValues object for this combination of finite element, quadrature, and mapping objects. If not, it allocates one. It then re-initializes this object for the current cell, after which there is now a FEValues object for the selected finite element, quadrature and mapping usable on the current cell. A reference to this object is then obtained using the call  [2.x.1742] , and will be used in the usual fashion to assemble local contributions. 




[1.x.696] 

One of the central pieces of the adaptive finite element method is that we inspect the computed solution (a posteriori) with an indicator that tells us which are the cells where the error is largest, and then refine them. In many of the other tutorial programs, we use the KellyErrorEstimator class to get an indication of the size of the error on a cell, although we also discuss more complicated strategies in some programs, most importantly in step-14. 

In any case, as long as the decision is only "refine this cell" or "do not refine this cell", the actual refinement step is not particularly challenging. However, here we have a code that is capable of hp-refinement, i.e., we suddenly have two choices whenever we detect that the error on a certain cell is too large for our liking: we can refine the cell by splitting it into several smaller ones, or we can increase the polynomial degree of the shape functions used on it. How do we know which is the more promising strategy? Answering this question is the central problem in  [2.x.1743] -finite element research at the time of this writing. 

In short, the question does not appear to be settled in the literature at this time. There are a number of more or less complicated schemes that address it, but there is nothing like the KellyErrorEstimator that is universally accepted as a good, even if not optimal, indicator of the error. Most proposals use the fact that it is beneficial to increase the polynomial degree whenever the solution is locally smooth whereas it is better to refine the mesh wherever it is rough. However, the questions of how to determine the local smoothness of the solution as well as the decision when a solution is smooth enough to allow for an increase in  [2.x.1744]  are certainly big and important ones. 

In the following, we propose a simple estimator of the local smoothness of a solution. As we will see in the results section, this estimator has flaws, in particular as far as cells with local hanging nodes are concerned. We therefore do not intend to present the following ideas as a complete solution to the problem. Rather, it is intended as an idea to approach it that merits further research and investigation. In other words, we do not intend to enter a sophisticated proposal into the fray about answers to the general question. However, to demonstrate our approach to  [2.x.1745] -finite elements, we need a simple indicator that does generate some useful information that is able to drive the simple calculations this tutorial program will perform. 




[1.x.697] 

Our approach here is simple: for a function  [2.x.1746]  to be in the Sobolev space  [2.x.1747]  on a cell  [2.x.1748] , it has to satisfy the condition 

[1.x.698] 

Assuming that the cell  [2.x.1749]  is not degenerate, i.e., that the mapping from the unit cell to cell  [2.x.1750]  is sufficiently regular, above condition is of course equivalent to 

[1.x.699] 

where  [2.x.1751]  is the function  [2.x.1752]  mapped back onto the unit cell  [2.x.1753] . From here, we can do the following: first, let us define the Fourier series of  [2.x.1754]  as 

[1.x.700] 

with Fourier vectors  [2.x.1755]  in 2d,  [2.x.1756]  in 3d, etc, and  [2.x.1757] . The coefficients of expansion  [2.x.1758]  can be obtained using  [2.x.1759] -orthogonality of the exponential basis 

[1.x.701] 

that leads to the following expression 

[1.x.702] 

It becomes clear that we can then write the  [2.x.1760]  norm of  [2.x.1761]  as 

[1.x.703] 

In other words, if this norm is to be finite (i.e., for  [2.x.1762]  to be in  [2.x.1763] ), we need that 

[1.x.704] 

Put differently: the higher regularity  [2.x.1764]  we want, the faster the Fourier coefficients have to go to zero. If you wonder where the additional exponent  [2.x.1765]  comes from: we would like to make use of the fact that  [2.x.1766]  if the sequence  [2.x.1767]  for any  [2.x.1768] . The problem is that we here have a summation not only over a single variable, but over all the integer multiples of  [2.x.1769]  that are located inside the  [2.x.1770] -dimensional sphere, because we have vector components  [2.x.1771] . In the same way as we prove that the sequence  [2.x.1772]  above converges by replacing the sum by an integral over the entire line, we can replace our  [2.x.1773] -dimensional sum by an integral over  [2.x.1774] -dimensional space. Now we have to note that between distance  [2.x.1775]  and  [2.x.1776] , there are, up to a constant,  [2.x.1777]  modes, in much the same way as we can transform the volume element  [2.x.1778]  into  [2.x.1779] . Consequently, it is no longer  [2.x.1780]  that has to decay as  [2.x.1781] , but it is in fact  [2.x.1782] . A comparison of exponents yields the result. 

We can turn this around: Assume we are given a function  [2.x.1783]  of unknown smoothness. Let us compute its Fourier coefficients  [2.x.1784]  and see how fast they decay. If they decay as 

[1.x.705] 

then consequently the function we had here was in  [2.x.1785] . 




[1.x.706] 

So what do we have to do to estimate the local smoothness of  [2.x.1786]  on a cell  [2.x.1787] ? Clearly, the first step is to compute the Fourier coefficients of our solution. Fourier series being infinite series, we simplify our task by only computing the first few terms of the series, such that  [2.x.1788]  with a cut-off  [2.x.1789] . Let us parenthetically remark that we want to choose  [2.x.1790]  large enough so that we capture at least the variation of those shape functions that vary the most. On the other hand, we should not choose  [2.x.1791]  too large: clearly, a finite element function, being a polynomial, is in  [2.x.1792]  on any given cell, so the coefficients will have to decay exponentially at one point; since we want to estimate the smoothness of the function this polynomial approximates, not of the polynomial itself, we need to choose a reasonable cutoff for  [2.x.1793] . Either way, computing this series is not particularly hard: from the definition 

[1.x.707] 

we see that we can compute the coefficient  [2.x.1794]  as 

[1.x.708] 

where  [2.x.1795]  is the value of the  [2.x.1796] th degree of freedom on this cell. In other words, we can write it as a matrix-vector product 

[1.x.709] 

with the matrix 

[1.x.710] 

This matrix is easily computed for a given number of shape functions  [2.x.1797]  and Fourier modes  [2.x.1798] . Consequently, finding the coefficients  [2.x.1799]  is a rather trivial job. To simplify our life even further, we will use  [2.x.1800]  class which does exactly this. 

The next task is that we have to estimate how fast these coefficients decay with  [2.x.1801] . The problem is that, of course, we have only finitely many of these coefficients in the first place. In other words, the best we can do is to fit a function  [2.x.1802]  to our data points  [2.x.1803] , for example by determining  [2.x.1804]  via a least-squares procedure: 

[1.x.711] 

However, the problem with this is that it leads to a nonlinear problem, a fact that we would like to avoid. On the other hand, we can transform the problem into a simpler one if we try to fit the logarithm of our coefficients to the logarithm of  [2.x.1805] , like this: 

[1.x.712] 

Using the usual facts about logarithms, we see that this yields the problem 

[1.x.713] 

where  [2.x.1806] . This is now a problem for which the optimality conditions  [2.x.1807] , are linear in  [2.x.1808] . We can write these conditions as follows: 

[1.x.714] 

This linear system is readily inverted to yield 

[1.x.715] 

and 

[1.x.716] 



This is nothing else but linear regression fit and to do that we will use  [2.x.1809]  While we are not particularly interested in the actual value of  [2.x.1810] , the formula above gives us a mean to calculate the value of the exponent  [2.x.1811]  that we can then use to determine that  [2.x.1812]  is in  [2.x.1813]  with  [2.x.1814] . 

These steps outlined above are applicable to many different scenarios, which motivated the introduction of a generic function  [2.x.1815]  in deal.II, that combines all the tasks described in this section in one simple function call. We will use it in the implementation of this program. 




[1.x.717] 

In the formulas above, we have derived the Fourier coefficients  [2.x.1816] . Because  [2.x.1817]  is a vector, we will get a number of Fourier coefficients  [2.x.1818]  for the same absolute value  [2.x.1819] , corresponding to the Fourier transform in different directions. If we now consider a function like  [2.x.1820]  then we will find lots of large Fourier coefficients in  [2.x.1821] -direction because the function is non-smooth in this direction, but fast-decaying Fourier coefficients in  [2.x.1822] -direction because the function is smooth there. The question that arises is this: if we simply fit our polynomial decay  [2.x.1823]  to [1.x.718] Fourier coefficients, we will fit it to a smoothness [1.x.719]. Is this what we want? Or would it be better to only consider the largest coefficient  [2.x.1824]  for all  [2.x.1825]  with the same magnitude, essentially trying to determine the smoothness of the solution in that spatial direction in which the solution appears to be roughest? 

One can probably argue for either case. The issue would be of more interest if deal.II had the ability to use anisotropic finite elements, i.e., ones that use different polynomial degrees in different spatial directions, as they would be able to exploit the directionally variable smoothness much better. Alas, this capability does not exist at the time of writing this tutorial program. 

Either way, because we only have isotopic finite element classes, we adopt the viewpoint that we should tailor the polynomial degree to the lowest amount of regularity, in order to keep numerical efforts low. Consequently, instead of using the formula 

[1.x.720] 

To calculate  [2.x.1826]  as shown above, we have to slightly modify all sums: instead of summing over all Fourier modes, we only sum over those for which the Fourier coefficient is the largest one among all  [2.x.1827]  with the same magnitude  [2.x.1828] , i.e., all sums above have to replaced by the following sums: 

[1.x.721] 

This is the form we will implement in the program. 




[1.x.722] 

One may ask whether it is a problem that we only compute the Fourier transform on the [1.x.723] (rather than the real cell) of the solution. After all, we stretch the solution by a factor  [2.x.1829]  during the transformation, thereby shifting the Fourier frequencies by a factor of  [2.x.1830] . This is of particular concern since we may have neighboring cells with mesh sizes  [2.x.1831]  that differ by a factor of 2 if one of them is more refined than the other. The concern is also motivated by the fact that, as we will see in the results section below, the estimated smoothness of the solution should be a more or less continuous function, but exhibits jumps at locations where the mesh size jumps. It therefore seems natural to ask whether we have to compensate for the transformation. 

The short answer is "no". In the process outlined above, we attempt to find coefficients  [2.x.1832]  that minimize the sum of squares of the terms 

[1.x.724] 

To compensate for the transformation means not attempting to fit a decay  [2.x.1833]  with respect to the Fourier frequencies  [2.x.1834]  [1.x.725], but to fit the coefficients  [2.x.1835]  computed on the reference cell [1.x.726], where  [2.x.1836]  is the norm of the transformation operator (i.e., something like the diameter of the cell). In other words, we would have to minimize the sum of squares of the terms 

[1.x.727] 

instead. However, using fundamental properties of the logarithm, this is simply equivalent to minimizing 

[1.x.728] 

In other words, this and the original least squares problem will produce the same best-fit exponent  [2.x.1837] , though the offset will in one case be  [2.x.1838]  and in the other  [2.x.1839] . However, since we are not interested in the offset at all but only in the exponent, it doesn't matter whether we scale Fourier frequencies in order to account for mesh size effects or not, the estimated smoothness exponent will be the same in either case. 




[1.x.729] 

[1.x.730] 

One of the problems with  [2.x.1840] -methods is that the high polynomial degree of shape functions together with the large number of constrained degrees of freedom leads to matrices with large numbers of nonzero entries in some rows. At the same time, because there are areas where we use low polynomial degree and consequently matrix rows with relatively few nonzero entries. Consequently, allocating the sparsity pattern for these matrices is a challenge: we cannot simply assemble a SparsityPattern by starting with an estimate of the bandwidth without using a lot of extra memory. 

The way in which we create a SparsityPattern for the underlying linear system is tightly coupled to the strategy we use to enforce constraints. deal.II supports handling constraints in linear systems in two ways: <ol>    [2.x.1841] Assembling the matrix without regard to the constraints and applying them   afterwards with  [2.x.1842]  or [2.x.1843]     [2.x.1844] Applying constraints as we assemble the system with    [2.x.1845]   [2.x.1846]  Most programs built on deal.II use the  [2.x.1847]  function to allocate a DynamicSparsityPattern that takes constraints into account. The system matrix then uses a SparsityPattern copied over from the DynamicSparsityPattern. This method is explained in step-2 and used in most tutorial programs. 

The early tutorial programs use first or second degree finite elements, so removing entries in the sparsity pattern corresponding to constrained degrees of freedom does not have a large impact on the overall number of zeros explicitly stored by the matrix. However, since as many as a third of the degrees of freedom may be constrained in an hp-discretization (and, with higher degree elements, these constraints can couple one DoF to as many as ten or twenty other DoFs), it is worthwhile to take these constraints into consideration since the resulting matrix will be much sparser (and, therefore, matrix-vector products or factorizations will be substantially faster too). 




[1.x.731] 

A second problem particular to  [2.x.1848] -methods arises because we have so many constrained degrees of freedom: typically up to about one third of all degrees of freedom (in 3d) are constrained because they either belong to cells with hanging nodes or because they are on cells adjacent to cells with a higher or lower polynomial degree. This is, in fact, not much more than the fraction of constrained degrees of freedom in non- [2.x.1849] -mode, but the difference is that each constrained hanging node is constrained not only against the two adjacent degrees of freedom, but is constrained against many more degrees of freedom. 

It turns out that the strategy presented first in step-6 to eliminate the constraints while computing the element matrices and vectors with  [2.x.1850]  is the most efficient approach also for this case. The alternative strategy to first build the matrix without constraints and then "condensing" away constrained degrees of freedom is considerably more expensive. It turns out that building the sparsity pattern by this inefficient algorithm requires at least  [2.x.1851]  in the number of unknowns, whereas an ideal finite element program would of course only have algorithms that are linear in the number of unknowns. Timing the sparsity pattern creation as well as the matrix assembly shows that the algorithm presented in step-6 (and used in the code below) is indeed faster. 

In our program, we will also treat the boundary conditions as (possibly inhomogeneous) constraints and eliminate the matrix rows and columns to those as well. All we have to do for this is to call the function that interpolates the Dirichlet boundary conditions already in the setup phase in order to tell the AffineConstraints object about them, and then do the transfer from local to global data on matrix and vector simultaneously. This is exactly what we've shown in step-6. 




[1.x.732] 

The test case we will solve with this program is a re-take of the one we already look at in step-14: we solve the Laplace equation 

[1.x.733] 

in 2d, with  [2.x.1852] , and with zero Dirichlet boundary values for  [2.x.1853] . We do so on the domain  [2.x.1854] , i.e., a square with a square hole in the middle. 

The difference to step-14 is of course that we use  [2.x.1855] -finite elements for the solution. The test case is of interest because it has re-entrant corners in the corners of the hole, at which the solution has singularities. We therefore expect that the solution will be smooth in the interior of the domain, and rough in the vicinity of the singularities. The hope is that our refinement and smoothness indicators will be able to see this behavior and refine the mesh close to the singularities, while the polynomial degree is increased away from it. As we will see in the results section, this is indeed the case. 


examples/step-27/doc/results.dox 



[1.x.734] 

In this section, we discuss a few results produced from running the current tutorial program. More results, in particular the extension to 3d calculations and determining how much compute time the individual components of the program take, are given in the  [2.x.1856]  "hp-paper". 

When run, this is what the program produces: 

[1.x.735] 



The first thing we learn from this is that the number of constrained degrees of freedom is on the order of 20-25% of the total number of degrees of freedom, at least on the later grids when we have elements of relatively high order (in 3d, the fraction of constrained degrees of freedom can be up to 30%). This is, in fact, on the same order of magnitude as for non- [2.x.1857] -discretizations. For example, in the last step of the step-6 program, we have 18353 degrees of freedom, 4432 of which are constrained. The difference is that in the latter program, each constrained hanging node is constrained against only the two adjacent degrees of freedom, whereas in the  [2.x.1858] -case, constrained nodes are constrained against many more degrees of freedom. Note also that the current program also includes nodes subject to Dirichlet boundary conditions in the list of constraints. In cycle 0, all the constraints are actually because of boundary conditions. 

Of maybe more interest is to look at the graphical output. First, here is the solution of the problem: 

<img src="https://www.dealii.org/images/steps/developer/step-27-solution.png"      alt="Elevation plot of the solution, showing the lack of regularity near           the interior (reentrant) corners."      width="200" height="200"> 

Secondly, let us look at the sequence of meshes generated: 

<div class="threecolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-00.svg"          alt="Triangulation containing reentrant corners without adaptive refinement."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-01.svg"          alt="Triangulation containing reentrant corners with one level of          refinement. New cells are placed near the corners."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-02.svg"          alt="Triangulation containing reentrant corners with two levels of          refinement. New cells are placed near the corners."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-03.svg"          alt="Triangulation containing reentrant corners with three levels of          refinement. New cells are placed near the corners."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-04.svg"          alt="Triangulation containing reentrant corners with four levels of          refinement. New cells are placed near the corners."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.mesh-05.svg"          alt="Triangulation containing reentrant corners with five levels of          refinement. New cells are placed near the corners."          width="200" height="200">   </div> </div> 

It is clearly visible how the mesh is refined near the corner singularities, as one would expect it. More interestingly, we should be curious to see the distribution of finite element polynomial degrees to these mesh cells, where the lightest color corresponds to degree two and the darkest one corresponds to degree seven: 

<div class="threecolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-00.svg"          alt="Initial grid where all cells contain just biquadratic functions."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-01.svg"          alt="Depiction of local approximation degrees after one refinement."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-02.svg"          alt="Depiction of local approximation degrees after two refinements."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-03.svg"          alt="Depiction of local approximation degrees after three refinements."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-04.svg"          alt="Depiction of local approximation degrees after four refinements."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.fe_degree-05.svg"          alt="Depiction of local approximation degrees after five refinements."          width="200" height="200">   </div> </div> 

While this is certainly not a perfect arrangement, it does make some sense: we use low order elements close to boundaries and corners where regularity is low. On the other hand, higher order elements are used where (i) the error was at one point fairly large, i.e., mainly in the general area around the corner singularities and in the top right corner where the solution is large, and (ii) where the solution is smooth, i.e., far away from the boundary. 

This arrangement of polynomial degrees of course follows from our smoothness estimator. Here is the estimated smoothness of the solution, with darker colors indicating least smoothness and lighter indicating the smoothest areas: 

<div class="threecolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-00.svg"          alt="Estimated regularity per cell on the initial grid."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-01.svg"          alt="Depiction of the estimated regularity per cell after one refinement."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-02.svg"          alt="Depiction of the estimated regularity per cell after two refinements."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-03.svg"          alt="Depiction of the estimated regularity per cell after three refinements."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-04.svg"          alt="Depiction of the estimated regularity per cell after four refinements."          width="200" height="200">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-27.smoothness-05.svg"          alt="Depiction of the estimated regularity per cell after five refinements."          width="200" height="200">   </div> </div> 

The primary conclusion one can draw from this is that the loss of regularity at the internal corners is a highly localized phenomenon; it only seems to impact the cells adjacent to the corner itself, so when we refine the mesh the black coloring is no longer visible. Besides the corners, this sequence of plots implies that the smoothness estimates are somewhat independent of the mesh refinement, particularly when we are far away from boundaries. It is also obvious that the smoothness estimates are independent of the actual size of the solution (see the picture of the solution above), as it should be. A point of larger concern, however, is that one realizes on closer inspection that the estimator we have overestimates the smoothness of the solution on cells with hanging nodes. This in turn leads to higher polynomial degrees in these areas, skewing the allocation of finite elements onto cells. 

We have no good explanation for this effect at the moment. One theory is that the numerical solution on cells with hanging nodes is, of course, constrained and therefore not entirely free to explore the function space to get close to the exact solution. This lack of degrees of freedom may manifest itself by yielding numerical solutions on these cells with suppressed oscillation, meaning a higher degree of smoothness. The estimator picks this signal up and the estimated smoothness overestimates the actual value. However, a definite answer to what is going on currently eludes the authors of this program. 

The bigger question is, of course, how to avoid this problem. Possibilities include estimating the smoothness not on single cells, but cell assemblies or patches surrounding each cell. It may also be possible to find simple correction factors for each cell depending on the number of constrained degrees of freedom it has. In either case, there are ample opportunities for further research on finding good  [2.x.1859] -refinement criteria. On the other hand, the main point of the current program was to demonstrate using the  [2.x.1860] -technology in deal.II, which is unaffected by our use of a possible sub-optimal refinement criterion. 




[1.x.736] 

[1.x.737] 

[1.x.738] 

This tutorial demonstrates only one particular strategy to decide between  [2.x.1861] - and  [2.x.1862] -adaptation. In fact, there are many more ways to automatically decide on the adaptation type, of which a few are already implemented in deal.II:  [2.x.1863]     [2.x.1864] [1.x.739] This is the strategy currently   implemented in this tutorial. For more information on this strategy, see   the general documentation of the  [2.x.1865]  namespace. [2.x.1866]  

   [2.x.1867] [1.x.740] This strategy is quite similar   to the current one, but uses Legendre series expansion rather than the   Fourier one: instead of sinusoids as basis functions, this strategy uses   Legendre polynomials. Of course, since we approximate the solution using a   finite-dimensional polynomial on each cell, the expansion of the solution in   Legendre polynomials is also finite and, consequently, when we talk about the   "decay" of this expansion, we can only consider the finitely many nonzero   coefficients of this expansion, rather than think about it in asymptotic terms.   But, if we have enough of these coefficients, we can certainly think of the   decay of these coefficients as characteristic of the decay of the coefficients   of the exact solution (which is, in general, not polynomial and so will have an   infinite Legendre expansion), and considering the coefficients we have should   reveal something about the properties of the exact solution. 

  The transition from the Fourier strategy to the Legendre one is quite simple:   You just need to change the series expansion class and the corresponding   smoothness estimation function to be part of the proper namespaces    [2.x.1868]  and  [2.x.1869]  This strategy is used   in step-75. For the theoretical background of this strategy, consult the   general documentation of the  [2.x.1870]  namespace, as well   as  [2.x.1871]  ,  [2.x.1872]  and  [2.x.1873]  . [2.x.1874]  

   [2.x.1875] [1.x.741] The last strategy is quite different   from the other two. In theory, we know how the error will converge   after changing the discretization of the function space. With    [2.x.1876] -refinement the solution converges algebraically as already pointed   out in step-7. If the solution is sufficiently smooth, though, we   expect that the solution will converge exponentially with increasing   polynomial degree of the finite element. We can compare a proper   prediction of the error with the actual error in the following step to   see if our choice of adaptation type was justified. 

  The transition to this strategy is a bit more complicated. For this, we need   an initialization step with pure  [2.x.1877] - or  [2.x.1878] -refinement and we need to   transfer the predicted errors over adapted meshes. The extensive   documentation of the  [2.x.1879]  function describes not   only the theoretical details of this approach, but also presents a blueprint   on how to implement this strategy in your code. For more information, see    [2.x.1880]  . 

  Note that with this particular function you cannot predict the error for   the next time step in time-dependent problems. Therefore, this strategy   cannot be applied to this type of problem without further ado. Alternatively,   the following approach could be used, which works for all the other   strategies as well: start each time step with a coarse mesh, keep refining   until happy with the result, and only then move on to the next time step. [2.x.1881]   [2.x.1882]  

Try implementing one of these strategies into this tutorial and observe the subtle changes to the results. You will notice that all strategies are capable of identifying the singularities near the reentrant corners and will perform  [2.x.1883] -refinement in these regions, while preferring  [2.x.1884] -refinement in the bulk domain. A detailed comparison of these strategies is presented in  [2.x.1885]  . 




[1.x.742] 

All functionality presented in this tutorial already works for both sequential and parallel applications. It is possible without too much effort to change to either the  [2.x.1886]  or the  [2.x.1887]  classes. If you feel eager to try it, we recommend reading step-18 for the former and step-40 for the latter case first for further background information on the topic, and then come back to this tutorial to try out your newly acquired skills. 

We go one step further in step-75: Here, we combine hp-adapative and MatrixFree methods in combination with  [2.x.1888]  objects. 


examples/step-28/doc/intro.dox 

 [2.x.1889]  

[1.x.743][1.x.744][1.x.745] 

 [2.x.1890]  


[1.x.746] [1.x.747] In this example, we intend to solve the multigroup diffusion approximation of the neutron transport equation. Essentially, the way to view this is as follows: In a nuclear reactor, neutrons are speeding around at different energies, get absorbed or scattered, or start a new fission event. If viewed at long enough length scales, the movement of neutrons can be considered a diffusion process. 

A mathematical description of this would group neutrons into energy bins, and consider the balance equations for the neutron fluxes in each of these bins, or energy groups. The scattering, absorption, and fission events would then be operators within the diffusion equation describing the neutron fluxes. Assume we have energy groups  [2.x.1891] , where by convention we assume that the neutrons with the highest energy are in group 1 and those with the lowest energy in group  [2.x.1892] . Then the neutron flux of each group satisfies the following equations: 

[1.x.748] 

augmented by appropriate boundary conditions. Here,  [2.x.1893]  is the velocity of neutrons within group  [2.x.1894] . In other words, the change in time in flux of neutrons in group  [2.x.1895]  is governed by the following processes:  [2.x.1896]   [2.x.1897]  Diffusion  [2.x.1898] . Here,  [2.x.1899]  is the   (spatially variable) diffusion coefficient.  [2.x.1900]  Absorption  [2.x.1901]  (note the   negative sign). The coefficient  [2.x.1902]  is called the [1.x.749].  [2.x.1903]  Nuclear fission  [2.x.1904] .   The production of neutrons of energy  [2.x.1905]  is   proportional to the flux of neutrons of energy  [2.x.1906]  times the   probability  [2.x.1907]  that neutrons of energy  [2.x.1908]  cause a fission   event times the number  [2.x.1909]  of neutrons produced in each fission event   times the probability that a neutron produced in this event has energy    [2.x.1910] .  [2.x.1911]  is called the [1.x.750] and    [2.x.1912]  the [1.x.751]. We will denote the term    [2.x.1913]  as the [1.x.752] in the program.  [2.x.1914]  Scattering  [2.x.1915]    of neutrons of energy  [2.x.1916]  producing neutrons   of energy  [2.x.1917] .  [2.x.1918]  is called the [1.x.753]. The case of elastic, in-group scattering  [2.x.1919]  exists, too, but   we subsume this into the removal cross section. The case  [2.x.1920]  is called   down-scattering, since a neutron loses energy in such an event. On the   other hand,  [2.x.1921]  corresponds to up-scattering: a neutron gains energy in   a scattering event from the thermal motion of the atoms surrounding it;   up-scattering is therefore only an important process for neutrons with   kinetic energies that are already on the same order as the thermal kinetic   energy (i.e. in the sub  [2.x.1922]  range).  [2.x.1923]  An extraneous source  [2.x.1924] .  [2.x.1925]  

For realistic simulations in reactor analysis, one may want to split the continuous spectrum of neutron energies into many energy groups, often up to 100. However, if neutron energy spectra are known well enough for some type of reactor (for example Pressurized Water Reactors, PWR), it is possible to obtain satisfactory results with only 2 energy groups. 

In the program shown in this tutorial program, we provide the structure to compute with as many energy groups as desired. However, to keep computing times moderate and in order to avoid tabulating hundreds of coefficients, we only provide the coefficients for above equations for a two-group simulation, i.e.  [2.x.1926] . We do, however, consider a realistic situation by assuming that the coefficients are not constant, but rather depend on the materials that are assembled into reactor fuel assemblies in rather complicated ways (see below). 




[1.x.754] 

If we consider all energy groups at once, we may write above equations in the following operator form: 

[1.x.755] 

where  [2.x.1927]  are sinking, fission, and scattering operators, respectively.  [2.x.1928]  here includes both the diffusion and removal terms. Note that  [2.x.1929]  is symmetric, whereas  [2.x.1930]  and  [2.x.1931]  are not. 

It is well known that this equation admits a stable solution if all eigenvalues of the operator  [2.x.1932]  are negative. This can be readily seen by multiplying the equation by  [2.x.1933]  and integrating over the domain, leading to 

[1.x.756] 

Stability means that the solution does not grow, i.e. we want the left hand side to be less than zero, which is the case if the eigenvalues of the operator on the right are all negative. For obvious reasons, it is not very desirable if a nuclear reactor produces neutron fluxes that grow exponentially, so eigenvalue analyses are the bread-and-butter of nuclear engineers. The main point of the program is therefore to consider the eigenvalue problem 

[1.x.757] 

where we want to make sure that all eigenvalues are positive. Note that  [2.x.1934] , being the diffusion operator plus the absorption (removal), is positive definite; the condition that all eigenvalues are positive therefore means that we want to make sure that fission and inter-group scattering are weak enough to not shift the spectrum into the negative. 

In nuclear engineering, one typically looks at a slightly different formulation of the eigenvalue problem. To this end, we do not just multiply with  [2.x.1935]  and integrate, but rather multiply with  [2.x.1936] . We then get the following evolution equation: 

[1.x.758] 

Stability is then guaranteed if the eigenvalues of the following problem are all negative: 

[1.x.759] 

which is equivalent to the eigenvalue problem 

[1.x.760] 

The typical formulation in nuclear engineering is to write this as 

[1.x.761] 

where  [2.x.1937] . Intuitively,  [2.x.1938]  is something like the multiplication factor for neutrons per typical time scale and should be less than or equal to one for stable operation of a reactor: if it is less than one, the chain reaction will die down, whereas nuclear bombs for example have a  [2.x.1939] -eigenvalue larger than one. A stable reactor should have  [2.x.1940] . 

For those who wonder how this can be achieved in practice without inadvertently getting slightly larger than one and triggering a nuclear bomb: first, fission processes happen on different time scales. While most neutrons are released very quickly after a fission event, a small number of neutrons are only released by daughter nuclei after several further decays, up to 10-60 seconds after the fission was initiated. If one is therefore slightly beyond  [2.x.1941] , one therefore has many seconds to react until all the neutrons created in fission re-enter the fission cycle. Nevertheless, control rods in nuclear reactors absorbing neutrons -- and therefore reducing  [2.x.1942]  -- are designed in such a way that they are all the way in the reactor in at most 2 seconds. 

One therefore has on the order of 10-60 seconds to regulate the nuclear reaction if  [2.x.1943]  should be larger than one for some time, as indicated by a growing neutron flux. Regulation can be achieved by continuously monitoring the neutron flux, and if necessary increase or reduce neutron flux by moving neutron-absorbing control rods a few millimeters into or out of the reactor. On a longer scale, the water cooling the reactor contains boron, a good neutron absorber. Every few hours, boron concentrations are adjusted by adding boron or diluting the coolant. 

Finally, some of the absorption and scattering reactions have some stability built in; for example, higher neutron fluxes result in locally higher temperatures, which lowers the density of water and therefore reduces the number of scatterers that are necessary to moderate neutrons from high to low energies before they can start fission events themselves. 

In this tutorial program, we solve above  [2.x.1944] -eigenvalue problem for two energy groups, and we are looking for the largest multiplication factor  [2.x.1945] , which is proportional to the inverse of the minimum eigenvalue plus one. To solve the eigenvalue problem, we generally use a modified version of the [1.x.762]. The algorithm looks like this: 

<ol>  [2.x.1946]  Initialize  [2.x.1947]  and  [2.x.1948]  with  [2.x.1949]    and  [2.x.1950]  and let  [2.x.1951] . 

 [2.x.1952]  Define the so-called [1.x.763] by   [1.x.764] 



 [2.x.1953]  Solve for all group fluxes  [2.x.1954]  using   [1.x.765] 



 [2.x.1955]  Update   [1.x.766] 



 [2.x.1956]  Compare  [2.x.1957]  with  [2.x.1958] .   If the change greater than a prescribed tolerance then set  [2.x.1959]  repeat   the iteration starting at step 2, otherwise end the iteration.  [2.x.1960]  

Note that in this scheme, we do not solve group fluxes exactly in each power iteration, but rather consider previously compute  [2.x.1961]  only for down-scattering events  [2.x.1962] . Up-scattering is only treated by using old iterators  [2.x.1963] , in essence assuming that the scattering operator is triangular. This is physically motivated since up-scattering does not play a too important role in neutron scattering. In addition, practices shows that the inverse power iteration is stable even using this simplification. 

Note also that one can use lots of extrapolation techniques to accelerate the power iteration laid out above. However, none of these are implemented in this example. 




[1.x.767] 

One may wonder whether it is appropriate to solve for the solutions of the individual energy group equations on the same meshes. The question boils down to this: will  [2.x.1964]  and  [2.x.1965]  have similar smoothness properties? If this is the case, then it is appropriate to use the same mesh for the two; a typical application could be chemical combustion, where typically the concentrations of all or most chemical species change rapidly within the flame front. As it turns out, and as will be apparent by looking at the graphs shown in the results section of this tutorial program, this isn't the case here, however: since the diffusion coefficient is different for different energy groups, fast neutrons (in bins with a small group number  [2.x.1966] ) have a very smooth flux function, whereas slow neutrons (in bins with a large group number) are much more affected by the local material properties and have a correspondingly rough solution if the coefficient are rough as in the case we compute here. Consequently, we will want to use different meshes to compute each energy group. 

This has two implications that we will have to consider: First, we need to find a way to refine the meshes individually. Second, assembling the source terms for the inverse power iteration, where we have to integrate solution  [2.x.1967]  defined on mesh  [2.x.1968]  against the shape functions defined on mesh  [2.x.1969] , becomes a much more complicated task. 




[1.x.768] 

We use the usual paradigm: solve on a given mesh, then evaluate an error indicator for each cell of each mesh we have. Because it is so convenient, we again use the a posteriori error estimator by Kelly, Gago, Zienkiewicz and Babuska which approximates the error per cell by integrating the jump of the gradient of the solution along the faces of each cell. Using this, we obtain indicators 

[1.x.769] 

where  [2.x.1970]  is the triangulation used in the solution of  [2.x.1971] . The question is what to do with this. For one, it is clear that refining only those cells with the highest error indicators might lead to bad results. To understand this, it is important to realize that  [2.x.1972]  scales with the second derivative of  [2.x.1973] . In other words, if we have two energy groups  [2.x.1974]  whose solutions are equally smooth but where one is larger by a factor of 10,000, for example, then only the cells of that mesh will be refined, whereas the mesh for the solution of small magnitude will remain coarse. This is probably not what one wants, since we can consider both components of the solution equally important. 

In essence, we would therefore have to scale  [2.x.1975]  by an importance factor  [2.x.1976]  that says how important it is to resolve  [2.x.1977]  to any given accuracy. Such important factors can be computed using duality techniques (see, for example, the step-14 tutorial program, and the reference to the book by Bangerth and Rannacher cited there). We won't go there, however, and simply assume that all energy groups are equally important, and will therefore normalize the error indicators  [2.x.1978]  for group  [2.x.1979]  by the maximum of the solution  [2.x.1980] . We then refine the cells whose errors satisfy 

[1.x.770] 

and coarsen the cells where 

[1.x.771] 

We chose  [2.x.1981]  and  [2.x.1982]  in the code. Note that this will, of course, lead to different meshes for the different energy groups. 

The strategy above essentially means the following: If for energy group  [2.x.1983]  there are many cells  [2.x.1984]  on which the error is large, for example because the solution is globally very rough, then many cells will be above the threshold. On the other hand, if there are a few cells with large and many with small errors, for example because the solution is overall rather smooth except at a few places, then only the few cells with large errors will be refined. Consequently, the strategy allows for meshes that track the global smoothness properties of the corresponding solutions rather well. 




[1.x.772] 

As pointed out above, the multigroup refinement strategy results in different meshes for the different solutions  [2.x.1985] . So what's the problem? In essence it goes like this: in step 3 of the eigenvalue iteration, we have form the weak form for the equation to compute  [2.x.1986]  as usual by multiplication with test functions  [2.x.1987]  defined on the mesh for energy group  [2.x.1988] ; in the process, we have to compute the right hand side vector that contains terms of the following form: 

[1.x.773] 

where  [2.x.1989]  is one of the coefficient functions  [2.x.1990]  or  [2.x.1991]  used in the right hand side of eigenvalue equation. The difficulty now is that  [2.x.1992]  is defined on the mesh for energy group  [2.x.1993] , i.e. it can be expanded as  [2.x.1994] , with basis functions  [2.x.1995]  defined on mesh  [2.x.1996] . The contribution to the right hand side can therefore be written as 

[1.x.774] 

On the other hand, the test functions  [2.x.1997]  are defined on mesh  [2.x.1998] . This means that we can't just split the integral  [2.x.1999]  into integrals over the cells of either mesh  [2.x.2000]  or  [2.x.2001] , since the respectively other basis functions may not be defined on these cells. 

The solution to this problem lies in the fact that both the meshes for  [2.x.2002]  and  [2.x.2003]  are derived by adaptive refinement from a common coarse mesh. We can therefore always find a set of cells, which we denote by  [2.x.2004] , that satisfy the following conditions:  [2.x.2005]   [2.x.2006]  the union of the cells covers the entire domain, and  [2.x.2007]  a cell  [2.x.2008]  is active on at least   one of the two meshes.  [2.x.2009]  A way to construct this set is to take each cell of coarse mesh and do the following steps: (i) if the cell is active on either  [2.x.2010]  or  [2.x.2011] , then add this cell to the set; (ii) otherwise, i.e. if this cell has children on both meshes, then do step (i) for each of the children of this cell. In fact, deal.II has a function  [2.x.2012]  that computes exactly this set of cells that are active on at least one of two meshes. 

With this, we can write above integral as follows: 

[1.x.775] 

 In the code, we compute the right hand side in the function  [2.x.2013] , where (among other things) we loop over the set of common most refined cells, calling the function  [2.x.2014]  on each pair of these cells. 

By construction, there are now three cases to be considered: <ol>  [2.x.2015]  The cell  [2.x.2016]  is active on both meshes, i.e. both the basis   functions  [2.x.2017]  as well as  [2.x.2018]  are defined on  [2.x.2019] .  [2.x.2020]  The cell  [2.x.2021]  is active on mesh  [2.x.2022] , but not  [2.x.2023] , i.e. the    [2.x.2024]   are defined on  [2.x.2025] , whereas the  [2.x.2026]  are defined   on children of  [2.x.2027] .  [2.x.2028]  The cell  [2.x.2029]  is active on mesh  [2.x.2030] , but not  [2.x.2031] , with opposite   conclusions than in (ii).  [2.x.2032]  

To compute the right hand side above, we then need to have different code for these three cases, as follows: <ol>  [2.x.2033]  If the cell  [2.x.2034]  is active on both meshes, then we can directly   evaluate the integral. In fact, we don't even have to bother with the basis   functions  [2.x.2035] , since all we need is the values of  [2.x.2036]  at   the quadrature points. We can do this using the    [2.x.2037]  function. This is done directly in   the  [2.x.2038]  function. 

 [2.x.2039]  If the cell  [2.x.2040]  is active on mesh  [2.x.2041] , but not  [2.x.2042] , then the   basis functions  [2.x.2043]  are only defined either on the children    [2.x.2044] , or on children of these children if cell  [2.x.2045]    is refined more than once on mesh  [2.x.2046] . 

  Let us assume for a second that  [2.x.2047]  is only once more refined on mesh  [2.x.2048]    than on mesh  [2.x.2049] . Using the fact that we use embedded finite element spaces   where each basis function on one mesh can be written as a linear combination   of basis functions on the next refined mesh, we can expand the restriction   of  [2.x.2050]  to child cell  [2.x.2051]  into the basis functions defined on that   child cell (i.e. on cells on which the basis functions  [2.x.2052]  are   defined):   [1.x.776] 

  Here, and in the following, summation over indices appearing twice is   implied. The matrix  [2.x.2053]  is the matrix that interpolated data from a cell   to its  [2.x.2054] -th child. 

  Then we can write the contribution of cell  [2.x.2055]  to the right hand side   component  [2.x.2056]  as   [1.x.777] 

  In matrix notation, this can be written as   [1.x.778] 

  where  [2.x.2057]  is   the weighted mass matrix on child  [2.x.2058]  of cell  [2.x.2059] . 

  The next question is what happens if a child  [2.x.2060]  of  [2.x.2061]  is not   active. Then, we have to apply the process recursively, i.e. we have to   interpolate the basis functions  [2.x.2062]  onto child  [2.x.2063]  of  [2.x.2064] , then   onto child  [2.x.2065]  of that cell, onto child  [2.x.2066]  of that one, etc,   until we find an active cell. We then have to sum up all the contributions   from all the children, grandchildren, etc, of cell  [2.x.2067] , with contributions   of the form   [1.x.779] 

  or   [1.x.780] 

  etc. We do this process recursively, i.e. if we sit on cell  [2.x.2068]  and see that   it has children on grid  [2.x.2069] , then we call a function    [2.x.2070]  with an identity matrix; the function will   multiply it's argument from the left with the prolongation matrix; if the   cell has further children, it will call itself with this new matrix,   otherwise it will perform the integration. 

 [2.x.2071]  The last case is where  [2.x.2072]  is active on mesh  [2.x.2073]  but not mesh    [2.x.2074] . In that case, we have to express basis function  [2.x.2075]  in   terms of the basis functions defined on the children of cell  [2.x.2076] , rather   than  [2.x.2077]  as before. This of course works in exactly the same   way. If the children of  [2.x.2078]  are active on mesh  [2.x.2079] , then   leading to the expression   [1.x.781] 

  In matrix notation, this expression now reads as   [1.x.782] 

  and correspondingly for cases where cell  [2.x.2080]  is refined more than once on   mesh  [2.x.2081] :   [1.x.783] 

  or   [1.x.784] 

  etc. In other words, the process works in exactly the same way as before,   except that we have to take the transpose of the prolongation matrices and   need to multiply it to the mass matrix from the other side.  [2.x.2082]  


The expressions for cases (ii) and (iii) can be understood as repeatedly interpolating either the left or right basis functions in the scalar product  [2.x.2083]  onto child cells, and then finally forming the inner product (the mass matrix) on the final cell. To make the symmetry in these cases more obvious, we can write them like this: for case (ii), we have 

[1.x.785] 

whereas for case (iii) we get 

[1.x.786] 






[1.x.787] 

A nuclear reactor core is composed of different types of assemblies. An assembly is essentially the smallest unit that can be moved in and out of a reactor, and is usually rectangular or square. However, assemblies are not fixed units, as they are assembled from a complex lattice of different fuel rods, control rods, and instrumentation elements that are held in place relative to each other by spacers that are permanently attached to the rods. To make things more complicated, there are different kinds of assemblies that are used at the same time in a reactor, where assemblies differ in the type and arrangement of rods they are made up of. 

Obviously, the arrangement of assemblies as well as the arrangement of rods inside them affect the distribution of neutron fluxes in the reactor (a fact that will be obvious by looking at the solution shown below in the results sections of this program). Fuel rods, for example, differ from each other in the enrichment of U-235 or Pu-239. Control rods, on the other hand, have zero fission, but nonzero scattering and absorption cross sections. 

This whole arrangement would make the description or spatially dependent material parameters very complicated. It will not become much simpler, but we will make one approximation: we merge the volume inhabited by each cylindrical rod and the surrounding water into volumes of quadratic cross section into so-called `pin cells' for which homogenized material data are obtained with nuclear database and knowledge of neutron spectrum. The homogenization makes all material data piecewise constant on the solution domain for a reactor with fresh fuel. Spatially dependent material parameters are then looked up for the quadratic assembly in which a point is located, and then for the quadratic pin cell within this assembly. 

In this tutorial program, we simulate a quarter of a reactor consisting of  [2.x.2084]  assemblies. We use symmetry (Neumann) boundary conditions to reduce the problem to one quarter of the domain, and consequently only simulate a  [2.x.2085]  set of assemblies. Two of them will be UO [2.x.2086]  fuel, the other two of them MOX fuel. Each of these assemblies consists of  [2.x.2087]  rods of different compositions. In total, we therefore create a  [2.x.2088]  lattice of rods. To make things simpler later on, we reflect this fact by creating a coarse mesh of  [2.x.2089]  cells (even though the domain is a square, for which we would usually use a single cell). In deal.II, each cell has a  [2.x.2090]  which one may use to associated each cell with a particular number identifying the material from which this cell's volume is made of; we will use this material ID to identify which of the 8 different kinds of rods that are used in this testcase make up a particular cell. Note that upon mesh refinement, the children of a cell inherit the material ID, making it simple to track the material even after mesh refinement. 

The arrangement of the rods will be clearly visible in the images shown in the results section. The cross sections for materials and for both energy groups are taken from a OECD/NEA benchmark problem. The detailed configuration and material data is given in the code. 




[1.x.788] 

As a coarse overview of what exactly the program does, here is the basic layout: starting on a coarse mesh that is the same for each energy group, we compute inverse eigenvalue iterations to compute the  [2.x.2091] -eigenvalue on a given set of meshes. We stop these iterations when the change in the eigenvalue drops below a certain tolerance, and then write out the meshes and solutions for each energy group for inspection by a graphics program. Because the meshes for the solutions are different, we have to generate a separate output file for each energy group, rather than being able to add all energy group solutions into the same file. 

After this, we evaluate the error indicators as explained in one of the sections above for each of the meshes, and refine and coarsen the cells of each mesh independently. Since the eigenvalue iterations are fairly expensive, we don't want to start all over on the new mesh; rather, we use the SolutionTransfer class to interpolate the solution on the previous mesh to the next one upon mesh refinement. A simple experiment will convince you that this is a lot cheaper than if we omitted this step. After doing so, we resume our eigenvalue iterations on the next set of meshes. 

The program is controlled by a parameter file, using the ParameterHandler class. We will show a parameter file in the results section of this tutorial. For the moment suffice it to say that it controls the polynomial degree of the finite elements used, the number of energy groups (even though all that is presently implemented are the coefficients for a 2-group problem), the tolerance where to stop the inverse eigenvalue iteration, and the number of refinement cycles we will do. 


examples/step-28/doc/results.dox 



[1.x.789] 

We can run the program with the following input file: 

[1.x.790] 

The output of this program then consists of the console output, a file named `convergence_table' to record main results of mesh iteration, and the graphical output in vtu format. 

The console output looks like this: 

[1.x.791] 



We see that power iteration does converge faster after cycle 0 due to the initialization with solution from last mesh iteration. The contents of `convergence_table' are, 

[1.x.792] 

The meanings of columns are: number of mesh iteration, numbers of degrees of  freedom of fast energy group, numbers of DoFs of thermal group, converged k-effective and the ratio between maximum of fast flux and maximum of thermal one. 

The grids of fast and thermal energy groups at mesh iteration #9 look as follows: 

 [2.x.2092]  &nbsp;  [2.x.2093]  

We see that the grid of thermal group is much finer than the one of fast group. The solutions on these grids are, (Note: flux are normalized with total fission source equal to 1) 

 [2.x.2094]  &nbsp;  [2.x.2095]  

Then we plot the convergence data with polynomial order being equal to 1,2 and 3. 

 [2.x.2096]  

The estimated `exact' k-effective = 0.906834721253 which is simply from last mesh iteration of polynomial order 3 minus 2e-10. We see that h-adaptive calculations deliver an algebraic convergence. And the higher polynomial order is, the faster mesh iteration converges. In our problem, we need smaller number of DoFs to achieve same accuracy with higher polynomial order. 


examples/step-29/doc/intro.dox 

 [2.x.2097]  

[1.x.793] 

[1.x.794] In order to run this program, deal.II must be configured to use the UMFPACK sparse direct solver. Refer to the [1.x.795] for instructions how to do this. 


[1.x.796] 

[1.x.797] 


A question that comes up frequently is how to solve problems involving complex valued functions with deal.II. For many problems, instead of working with complex valued finite elements directly, it is often more convenient to split complex valued functions into their real and imaginary parts and use separate scalar finite element fields for discretizing each one of them. Basically this amounts to viewing a single complex valued equation as a system of two real valued equations. This short example demonstrates how this can be implemented in deal.II by using an  [2.x.2098]  object to stack two finite element fields representing real and imaginary parts. (The opposite approach, keeping everything complex-valued, is demonstrated in a different tutorial program: see step-58 for this.) When split into real and imaginary parts, the equations covered here fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.2099]  module. 

In addition to this discussion, we also discuss the ParameterHandler class, which provides a convenient way for reading parameters from a configuration file at runtime without the need to recompile the program code. 




[1.x.798] 

The original purpose of this program is to simulate the focusing properties of an ultrasound wave generated by a transducer lens with variable geometry. Recent applications in medical imaging use ultrasound waves not only for imaging purposes, but also to excite certain local effects in a material, like changes in optical properties, that can then be measured by other imaging techniques. A vital ingredient for these methods is the ability to focus the intensity of the ultrasound wave in a particular part of the material, ideally in a point, to be able to examine the properties of the material at that particular location. 

To derive a model for this problem, we think of ultrasound as a pressure wave governed by the wave equation: 

[1.x.799] 

where  [2.x.2100]  is the wave speed (that for simplicity we assume to be constant),  [2.x.2101] . The boundary  [2.x.2102]  is divided into two parts  [2.x.2103]  and  [2.x.2104] , with  [2.x.2105]  representing the transducer lens and  [2.x.2106]  an absorbing boundary (that is, we want to choose boundary conditions on  [2.x.2107]  in such a way that they imitate a larger domain). On  [2.x.2108] , the transducer generates a wave of constant frequency  [2.x.2109]  and constant amplitude (that we chose to be 1 here): 

[1.x.800] 



If there are no other (interior or boundary) sources, and since the only source has frequency  [2.x.2110] , then the solution admits a separation of variables of the form  [2.x.2111] . The complex-valued function  [2.x.2112]  describes the spatial dependency of amplitude and phase (relative to the source) of the waves of frequency  [2.x.2113] , with the amplitude being the quantity that we are interested in. By plugging this form of the solution into the wave equation, we see that for  [2.x.2114]  we have 

[1.x.801] 



For finding suitable conditions on  [2.x.2115]  that model an absorbing boundary, consider a wave of the form  [2.x.2116]  with frequency  [2.x.2117]  traveling in direction  [2.x.2118] . In order for  [2.x.2119]  to solve the wave equation,  [2.x.2120]  must hold. Suppose that this wave hits the boundary in  [2.x.2121]  at a right angle, i.e.  [2.x.2122]  with  [2.x.2123]  denoting the outer unit normal of  [2.x.2124]  in  [2.x.2125] . Then at  [2.x.2126] , this wave satisfies the equation 

[1.x.802] 

Hence, by enforcing the boundary condition 

[1.x.803] 

waves that hit the boundary  [2.x.2127]  at a right angle will be perfectly absorbed. On the other hand, those parts of the wave field that do not hit a boundary at a right angle do not satisfy this condition and enforcing it as a boundary condition will yield partial reflections, i.e. only parts of the wave will pass through the boundary as if it wasn't here whereas the remaining fraction of the wave will be reflected back into the domain. 

If we are willing to accept this as a sufficient approximation to an absorbing boundary we finally arrive at the following problem for  [2.x.2128] : 

[1.x.804] 

This is a Helmholtz equation (similar to the one in step-7, but this time with ''the bad sign'') with Dirichlet data on  [2.x.2129]  and mixed boundary conditions on  [2.x.2130] . Because of the condition on  [2.x.2131] , we cannot just treat the equations for real and imaginary parts of  [2.x.2132]  separately. What we can do however is to view the PDE for  [2.x.2133]  as a system of two PDEs for the real and imaginary parts of  [2.x.2134] , with the boundary condition on  [2.x.2135]  representing the coupling terms between the two components of the system. This works along the following lines: Let  [2.x.2136] , then in terms of  [2.x.2137]  and  [2.x.2138]  we have the following system: 

[1.x.805] 



For test functions  [2.x.2139]  with  [2.x.2140] , after the usual multiplication, integration over  [2.x.2141]  and applying integration by parts, we get the weak formulation 

[1.x.806] 



We choose finite element spaces  [2.x.2142]  and  [2.x.2143]  with bases  [2.x.2144]  and look for approximate solutions 

[1.x.807] 

Plugging into the variational form yields the equation system 

[1.x.808] 

In matrix notation: 

[1.x.809] 

(One should not be fooled by the right hand side being zero here, that is because we haven't included the Dirichlet boundary data yet.) Because of the alternating sign in the off-diagonal blocks, we can already see that this system is non-symmetric, in fact it is even indefinite. Of course, there is no necessity to choose the spaces  [2.x.2145]  and  [2.x.2146]  to be the same. However, we expect real and imaginary part of the solution to have similar properties and will therefore indeed take  [2.x.2147]  in the implementation, and also use the same basis functions  [2.x.2148]  for both spaces. The reason for the notation using different symbols is just that it allows us to distinguish between shape functions for  [2.x.2149]  and  [2.x.2150] , as this distinction plays an important role in the implementation. 




[1.x.810] 

For the computations, we will consider wave propagation in the unit square, with ultrasound generated by a transducer lens that is shaped like a segment of the circle with center at  [2.x.2151]  and a radius slightly greater than  [2.x.2152] ; this shape should lead to a focusing of the sound wave at the center of the circle. Varying  [2.x.2153]  changes the "focus" of the lens and affects the spatial distribution of the intensity of  [2.x.2154] , where our main concern is how well  [2.x.2155]  is focused. 

In the program below, we will implement the complex-valued Helmholtz equations using the formulation with split real and imaginary parts. We will also discuss how to generate a domain that looks like a square with a slight bulge simulating the transducer (in the  [2.x.2156]  function), and how to generate graphical output that not only contains the solution components  [2.x.2157]  and  [2.x.2158] , but also the magnitude  [2.x.2159]  directly in the output file (in  [2.x.2160] ). Finally, we use the ParameterHandler class to easily read parameters like the focal distance  [2.x.2161] , wave speed  [2.x.2162] , frequency  [2.x.2163] , and a number of other parameters from an input file at run-time, rather than fixing those parameters in the source code where we would have to re-compile every time we want to change parameters. 


examples/step-29/doc/results.dox 

[1.x.811] 

[1.x.812] 

The current program reads its run-time parameters from an input file called  [2.x.2164]  that looks like this: 

[1.x.813] 



As can be seen, we set  [2.x.2165] , which amounts to a focus of the transducer lens at  [2.x.2166] ,  [2.x.2167] . The coarse mesh is refined 5 times, resulting in 160x160 cells, and the output is written in vtu format. The parameter reader understands many more parameters pertaining in particular to the generation of output, but we need none of these parameters here and therefore stick with their default values. 

Here's the console output of the program in debug mode: 

[1.x.814] 



(Of course, execution times will differ if you run the program locally.) The fact that most of the time is spent on assembling the system matrix and generating output is due to the many assertions that need to be checked in debug mode. In release mode these parts of the program run much faster whereas solving the linear system is hardly sped up at all: 

[1.x.815] 



The graphical output of the program looks as follows: 


 [2.x.2168]  

The first two pictures show the real and imaginary parts of  [2.x.2169] , whereas the last shows the intensity  [2.x.2170] . One can clearly see that the intensity is focused around the focal point of the lens (0.5, 0.3), and that the focus is rather sharp in  [2.x.2171] -direction but more blurred in  [2.x.2172] -direction, which is a consequence of the geometry of the focusing lens, its finite aperture, and the wave nature of the problem. 

Because colorful graphics are always fun, and to stress the focusing effects some more, here is another set of images highlighting how well the intensity is actually focused in  [2.x.2173] -direction: 

 [2.x.2174]  


As a final note, the structure of the program makes it easy to determine which parts of the program scale nicely as the mesh is refined and which parts don't. Here are the run times for 5, 6, and 7 global refinements: 

[1.x.816] 



Each time we refine the mesh once, so the number of cells and degrees of freedom roughly quadruples from each step to the next. As can be seen, generating the grid, setting up degrees of freedom, assembling the linear system, and generating output scale pretty closely to linear, whereas solving the linear system is an operation that requires 8 times more time each time the number of degrees of freedom is increased by a factor of 4, i.e. it is  [2.x.2175] . This can be explained by the fact that (using optimal ordering) the bandwidth of a finite element matrix is  [2.x.2176] , and the effort to solve a banded linear system using LU decomposition is  [2.x.2177] . This also explains why the program does run in 3d as well (after changing the dimension on the  [2.x.2178]  object), but scales very badly and takes extraordinary patience before it finishes solving the linear system on a mesh with appreciable resolution, even though all the other parts of the program scale very nicely. 




[1.x.817] 

[1.x.818] 

An obvious possible extension for this program is to run it in 3d &mdash; after all, the world around us is three-dimensional, and ultrasound beams propagate in three-dimensional media. You can try this by simply changing the template parameter of the principal class in  [2.x.2179]  and running it. This won't get you very far, though: certainly not if you do 5 global refinement steps as set in the parameter file. You'll simply run out of memory as both the mesh (with its  [2.x.2180]  cells) and in particular the sparse direct solver take too much memory. You can solve with 3 global refinement steps, however, if you have a bit of time: in early 2011, the direct solve takes about half an hour. What you'll notice, however, is that the solution is completely wrong: the mesh size is simply not small enough to resolve the solution's waves accurately, and you can see this in plots of the solution. Consequently, this is one of the cases where adaptivity is indispensable if you don't just want to throw a bigger (presumably %parallel) machine at the problem. 


examples/step-3/doc/intro.dox 

[1.x.819] 

[1.x.820] 

 [2.x.2181]  

[1.x.821] 

This is the first example where we actually use finite elements to compute something. We will solve a simple version of Poisson's equation with zero boundary values, but a nonzero right hand side: 

[1.x.822] 

We will solve this equation on the square,  [2.x.2182] , for which you've already learned how to generate a mesh in step-1 and step-2. In this program, we will also only consider the particular case  [2.x.2183]  and come back to how to implement the more general case in the next tutorial program, step-4. 

If you've learned about the basics of the finite element method, you will remember the steps we need to take to approximate the solution  [2.x.2184]  by a finite dimensional approximation. Specifically, we first need to derive the weak form of the equation above, which we obtain by multiplying the equation by a test function  [2.x.2185]  [1.x.823] (we will come back to the reason for multiplying from the left and not from the right below) and integrating over the domain  [2.x.2186] : 

[1.x.824] 

This can be integrated by parts: 

[1.x.825] 

The test function  [2.x.2187]  has to satisfy the same kind of boundary conditions (in mathematical terms: it needs to come from the tangent space of the set in which we seek the solution), so on the boundary  [2.x.2188]  and consequently the weak form we are looking for reads 

[1.x.826] 

where we have used the common notation  [2.x.2189] . The problem then asks for a function  [2.x.2190]  for which this statement is true for all test functions  [2.x.2191]  from the appropriate space (which here is the space  [2.x.2192] ). 

Of course we can't find such a function on a computer in the general case, and instead we seek an approximation  [2.x.2193] , where the  [2.x.2194]  are unknown expansion coefficients we need to determine (the "degrees of freedom" of this problem), and  [2.x.2195]  are the finite element shape functions we will use. To define these shape functions, we need the following: 

- A mesh on which to define shape functions. You have already seen how to   generate and manipulate the objects that describe meshes in step-1 and   step-2. 

- A finite element that describes the shape functions we want to use on the   reference cell (which in deal.II is always the unit interval  [2.x.2196] , the   unit square  [2.x.2197]  or the unit cube  [2.x.2198] , depending on which space   dimension you work in). In step-2, we had already used an object of type   FE_Q<2>, which denotes the usual Lagrange elements that define shape   functions by interpolation on support points. The simplest one is   FE_Q<2>(1), which uses polynomial degree 1. In 2d, these are often referred   to as [1.x.827], since they are linear in each of the two coordinates   of the reference cell. (In 1d, they would be [1.x.828] and in 3d   [1.x.829]; however, in the deal.II documentation, we will frequently   not make this distinction and simply always call these functions "linear".) 

- A DoFHandler object that enumerates all the degrees of freedom on the mesh,   taking the reference cell description the finite element object provides as   the basis. You've also already seen how to do this in step-2. 

- A mapping that tells how the shape functions on the real cell are obtained   from the shape functions defined by the finite element class on the   reference cell. By default, unless you explicitly say otherwise, deal.II   will use a (bi-, tri-)linear mapping for this, so in most cases you don't   have to worry about this step. 

Through these steps, we now have a set of functions  [2.x.2199] , and we can define the weak form of the discrete problem: Find a function  [2.x.2200] , i.e., find the expansion coefficients  [2.x.2201]  mentioned above, so that 

[1.x.830] 

Note that we here follow the convention that everything is counted starting at zero, as common in C and C++. This equation can be rewritten as a linear system if you insert the representation  [2.x.2202]  and then observe that 

[1.x.831] 

With this, the problem reads: Find a vector  [2.x.2203]  so that 

[1.x.832] 

where the matrix  [2.x.2204]  and the right hand side  [2.x.2205]  are defined as 

[1.x.833] 






[1.x.834] 

Before we move on with describing how these quantities can be computed, note that if we had multiplied the original equation from the [1.x.835] by a test function rather than from the left, then we would have obtained a linear system of the form 

[1.x.836] 

with a row vector  [2.x.2206] . By transposing this system, this is of course equivalent to solving 

[1.x.837] 

which here is the same as above since  [2.x.2207] . But in general is not, and in order to avoid any sort of confusion, experience has shown that simply getting into the habit of multiplying the equation from the left rather than from the right (as is often done in the mathematical literature) avoids a common class of errors as the matrix is automatically correct and does not need to be transposed when comparing theory and implementation. See step-9 for the first example in this tutorial where we have a non-symmetric bilinear form for which it makes a difference whether we multiply from the right or from the left. 




[1.x.838] 

Now we know what we need (namely: objects that hold the matrix and vectors, as well as ways to compute  [2.x.2208] ), and we can look at what it takes to make that happen: 

- The object for  [2.x.2209]  is of type SparseMatrix while those for  [2.x.2210]  and  [2.x.2211]  are of   type Vector. We will see in the program below what classes are used to solve   linear systems. 

- We need a way to form the integrals. In the finite element method, this is   most commonly done using quadrature, i.e. the integrals are replaced by a   weighted sum over a set of points on each cell. That is, we first split the   integral over  [2.x.2212]  into integrals over all cells,   [1.x.839] 

  and then approximate each cell's contribution by quadrature:   [1.x.840] 

  where  [2.x.2213]  is the  [2.x.2214] th quadrature point on cell  [2.x.2215] , and  [2.x.2216]    the  [2.x.2217] th quadrature weight. There are different parts to what is needed in   doing this, and we will discuss them in turn next. 

- First, we need a way to describe the location  [2.x.2218]  of quadrature   points and their weights  [2.x.2219] . They are usually mapped from the reference   cell in the same way as shape functions, i.e., implicitly using the   MappingQ1 class or, if you explicitly say so, through one of the other   classes derived from Mapping. The locations and weights on the reference   cell are described by objects derived from the Quadrature base   class. Typically, one chooses a quadrature formula (i.e. a set of points and   weights) so that the quadrature exactly equals the integral in the matrix;   this can be achieved because all factors in the integral are polynomial, and   is done by Gaussian quadrature formulas, implemented in the QGauss class. 

- We then need something that can help us evaluate  [2.x.2220]    on cell  [2.x.2221] . This is what the FEValues class does: it takes a finite element   objects to describe  [2.x.2222]  on the reference cell, a quadrature object to   describe the quadrature points and weights, and a mapping object (or   implicitly takes the MappingQ1 class) and provides values and derivatives of   the shape functions on the real cell  [2.x.2223]  as well as all sorts of other   information needed for integration, at the quadrature points located on  [2.x.2224] . 

FEValues really is the central class in the assembly process. One way you can view it is as follows: The FiniteElement and derived classes describe shape [1.x.841], i.e., infinite dimensional objects: functions have values at every point. We need this for theoretical reasons because we want to perform our analysis with integrals over functions. However, for a computer, this is a very difficult concept, since they can in general only deal with a finite amount of information, and so we replace integrals by sums over quadrature points that we obtain by mapping (the Mapping object) using  points defined on a reference cell (the Quadrature object) onto points on the real cell. In essence, we reduce the problem to one where we only need a finite amount of information, namely shape function values and derivatives, quadrature weights, normal vectors, etc, exclusively at a finite set of points. The FEValues class is the one that brings the three components together and provides this finite set of information on a particular cell  [2.x.2225] . You will see it in action when we assemble the linear system below. 

It is noteworthy that all of this could also be achieved if you simply created these three objects yourself in an application program, and juggled the information yourself. However, this would neither be simpler (the FEValues class provides exactly the kind of information you actually need) nor faster: the FEValues class is highly optimized to only compute on each cell the particular information you need; if anything can be re-used from the previous cell, then it will do so, and there is a lot of code in that class to make sure things are cached wherever this is advantageous. 

The final piece of this introduction is to mention that after a linear system is obtained, it is solved using an iterative solver and then postprocessed: we create an output file using the DataOut class that can then be visualized using one of the common visualization programs. 

 [2.x.2226]  The preceding overview of all the important steps of any finite element implementation has its counterpart in deal.II: The library can naturally be grouped into a number of "modules" that cover the basic concepts just outlined. You can access these modules through the tab at the top of this page. An overview of the most fundamental groups of concepts is also available on the [1.x.842]. 




[1.x.843] 

Although this is the simplest possible equation you can solve using the finite element method, this program shows the basic structure of most finite element programs and also serves as the template that almost all of the following programs will essentially follow. Specifically, the main class of this program looks like this: 

[1.x.844] 



This follows the object oriented programming mantra of [1.x.845], i.e. we do our best to hide almost all internal details of this class in private members that are not accessible to the outside. 

Let's start with the member variables: These follow the building blocks we have outlined above in the bullet points, namely we need a Triangulation and a DoFHandler object, and a finite element object that describes the kinds of shape functions we want to use. The second group of objects relate to the linear algebra: the system matrix and right hand side as well as the solution vector, and an object that describes the sparsity pattern of the matrix. This is all this class needs (and the essentials that any solver for a stationary PDE requires) and that needs to survive throughout the entire program. In contrast to this, the FEValues object we need for assembly is only required throughout assembly, and so we create it as a local object in the function that does that and destroy it again at its end. 

Secondly, let's look at the member functions. These, as well, already form the common structure that almost all following tutorial programs will use:  [2.x.2227]     [2.x.2228]   [2.x.2229] : This is what one could call a        [1.x.846]. As its name suggests, it sets up the        object that stores the triangulation. In later examples, it could also        deal with boundary conditions, geometries, etc.    [2.x.2230]   [2.x.2231] : This then is the function in which all the        other data structures are set up that are needed to solve the        problem. In particular, it will initialize the DoFHandler object and        correctly size the various objects that have to do with the linear        algebra. This function is often separated from the preprocessing        function above because, in a time dependent program, it may be called        at least every few time steps whenever the mesh        is adaptively refined (something we will see how to do in step-6). On        the other hand, setting up the mesh itself in the preprocessing        function above is done only once at the beginning of the program and        is, therefore, separated into its own function.    [2.x.2232]   [2.x.2233] : This, then is where the contents of the        matrix and right hand side are computed, as discussed at length in the        introduction above. Since doing something with this linear system is        conceptually very different from computing its entries, we separate it        from the following function.    [2.x.2234]   [2.x.2235] : This then is the function in which we compute the        solution  [2.x.2236]  of the linear system  [2.x.2237] . In the current program, this        is a simple task since the matrix is so simple, but it will become a        significant part of a program's size whenever the problem is not so        trivial any more (see, for example, step-20, step-22, or step-31 once        you've learned a bit more about the library).    [2.x.2238]   [2.x.2239] : Finally, when you have computed a        solution, you probably want to do something with it. For example, you        may want to output it in a format that can be visualized, or you may        want to compute quantities you are interested in: say, heat fluxes in a        heat exchanger, air friction coefficients of a wing, maximum bridge        loads, or simply the value of the numerical solution at a point. This        function is therefore the place for postprocessing your solution.  [2.x.2240]  All of this is held together by the single public function (other than the constructor), namely the  [2.x.2241]  function. It is the one that is called from the place where an object of this type is created, and it is the one that calls all the other functions in their proper order. Encapsulating this operation into the  [2.x.2242]  function, rather than calling all the other functions from  [2.x.2243]  makes sure that you can change how the separation of concerns within this class is implemented. For example, if one of the functions becomes too big, you can split it up into two, and the only places you have to be concerned about changing as a consequence are within this very same class, and not anywhere else. 

As mentioned above, you will see this general structure &mdash; sometimes with variants in spelling of the functions' names, but in essentially this order of separation of functionality &mdash; again in many of the following tutorial programs. 




[1.x.847] 

deal.II defines a number of integral %types via alias in namespace  [2.x.2244]  (In the previous sentence, the word "integral" is used as the [1.x.848] that corresponds to the noun "integer". It shouldn't be confused with the [1.x.849] "integral" that represents the area or volume under a curve or surface. The adjective "integral" is widely used in the C++ world in contexts such as "integral type", "integral constant", etc.) In particular, in this program you will see  [2.x.2245]  in a couple of places: an integer type that is used to denote the [1.x.850] index of a degree of freedom, i.e., the index of a particular degree of freedom within the DoFHandler object that is defined on top of a triangulation (as opposed to the index of a particular degree of freedom within a particular cell). For the current program (as well as almost all of the tutorial programs), you will have a few thousand to maybe a few million unknowns globally (and, for  [2.x.2246]  elements, you will have 4 [1.x.851] in 2d and 8 in 3d). Consequently, a data type that allows to store sufficiently large numbers for global DoF indices is  [2.x.2247]  given that it allows to store numbers between 0 and slightly more than 4 billion (on most systems, where integers are 32-bit). In fact, this is what  [2.x.2248]  is. 

So, why not just use  [2.x.2249]  right away? deal.II used to do this until version 7.3. However, deal.II supports very large computations (via the framework discussed in step-40) that may have more than 4 billion unknowns when spread across a few thousand processors. Consequently, there are situations where  [2.x.2250]  is not sufficiently large and we need a 64-bit unsigned integral type. To make this possible, we introduced  [2.x.2251]  which by default is defined as simply <code>unsigned int</code> whereas it is possible to define it as <code>unsigned long long int</code> if necessary, by passing a particular flag during configuration (see the ReadMe file). 

This covers the technical aspect. But there is also a documentation purpose: everywhere in the library and codes that are built on it, if you see a place using the data type  [2.x.2252]  you immediately know that the quantity that is being referenced is, in fact, a global dof index. No such meaning would be apparent if we had just used  [2.x.2253]  (which may also be a local index, a boundary indicator, a material id, etc.). Immediately knowing what a variable refers to also helps avoid errors: it's quite clear that there must be a bug if you see an object of type  [2.x.2254]  being assigned to variable of type  [2.x.2255]  even though they are both represented by unsigned integers and the compiler will, consequently, not complain. 

In more practical terms what the presence of this type means is that during assembly, we create a  [2.x.2256]  matrix (in 2d, using a  [2.x.2257]  element) of the contributions of the cell we are currently sitting on, and then we need to add the elements of this matrix to the appropriate elements of the global (system) matrix. For this, we need to get at the global indices of the degrees of freedom that are local to the current cell, for which we will always use the following piece of the code: 

[1.x.852] 

where  [2.x.2258]  is declared as 

[1.x.853] 

The name of this variable might be a bit of a misnomer -- it stands for "the global indices of those degrees of freedom locally defined on the current cell" -- but variables that hold this information are universally named this way throughout the library. 

 [2.x.2259]   [2.x.2260]  is not the only type defined in this namespace. Rather, there is a whole family, including  [2.x.2261]   [2.x.2262]  and  [2.x.2263]  All of these are alias for integer data types but, as explained above, they are used throughout the library so that (i) the intent of a variable becomes more easily discerned, and (ii) so that it becomes possible to change the actual type to a larger one if necessary without having to go through the entire library and figure out whether a particular use of  [2.x.2264]  corresponds to, say, a material indicator. 


examples/step-3/doc/results.dox 



[1.x.854] 

The output of the program looks as follows: 

[1.x.855] 



The first two lines is what we wrote to  [2.x.2265] . The last two lines were generated without our intervention by the CG solver. The first two lines state the residual at the start of the iteration, while the last line tells us that the solver needed 47 iterations to bring the norm of the residual to 5.3e-13, i.e. below the threshold 1e-12 which we have set in the `solve' function. We will show in the next program how to suppress this output, which is sometimes useful for debugging purposes, but often clutters up the screen display. 

Apart from the output shown above, the program generated the file  [2.x.2266] , which is in the VTK format that is widely used by many visualization programs today -- including the two heavy-weights [1.x.856] and [1.x.857] that are the most commonly used programs for this purpose today. 

Using VisIt, it is not very difficult to generate a picture of the solution like this:  [2.x.2267]  It shows both the solution and the mesh, elevated above the  [2.x.2268] - [2.x.2269]  plane based on the value of the solution at each point. Of course the solution here is not particularly exciting, but that is a result of both what the Laplace equation represents and the right hand side  [2.x.2270]  we have chosen for this program: The Laplace equation describes (among many other uses) the vertical deformation of a membrane subject to an external (also vertical) force. In the current example, the membrane's borders are clamped to a square frame with no vertical variation; a constant force density will therefore intuitively lead to a membrane that simply bulges upward -- like the one shown above. 

VisIt and Paraview both allow playing with various kinds of visualizations of the solution. Several video lectures show how to use these programs.  [2.x.2271]  




[1.x.858] 

[1.x.859] 

If you want to play around a little bit with this program, here are a few suggestions:  [2.x.2272]  

 [2.x.2273]     [2.x.2274]    Change the geometry and mesh: In the program, we have generated a square   domain and mesh by using the  [2.x.2275]    function. However, the  [2.x.2276]  has a good number of other   functions as well. Try an L-shaped domain, a ring, or other domains you find   there.    [2.x.2277]  

   [2.x.2278]    Change the boundary condition: The code uses the  [2.x.2279]    function to generate zero boundary conditions. However, you may want to try   non-zero constant boundary values using    [2.x.2280]  instead of    [2.x.2281]  to have unit Dirichlet boundary   values. More exotic functions are described in the documentation of the   Functions namespace, and you may pick one to describe your particular boundary   values.    [2.x.2282]  

   [2.x.2283]  Modify the type of boundary condition: Presently, what happens   is that we use Dirichlet boundary values all around, since the   default is that all boundary parts have boundary indicator zero, and   then we tell the    [2.x.2284]  function to   interpolate boundary values to zero on all boundary components with   indicator zero.   [2.x.2285]  We can change this behavior if we assign parts   of the boundary different indicators. For example, try this   immediately after calling  [2.x.2286]    [1.x.860] 



  What this does is it first asks the triangulation to   return an iterator that points to the first active cell. Of course,   this being the coarse mesh for the triangulation of a square, the   triangulation has only a single cell at this moment, and it is   active. Next, we ask the cell to return an iterator to its first   face, and then we ask the face to reset the boundary indicator of   that face to 1. What then follows is this: When the mesh is refined,   faces of child cells inherit the boundary indicator of their   parents, i.e. even on the finest mesh, the faces on one side of the   square have boundary indicator 1. Later, when we get to   interpolating boundary conditions, the    [2.x.2287]  call will only produce boundary   values for those faces that have zero boundary indicator, and leave   those faces alone that have a different boundary indicator. What   this then does is to impose Dirichlet boundary conditions on the   former, and homogeneous Neumann conditions on the latter (i.e. zero   normal derivative of the solution, unless one adds additional terms   to the right hand side of the variational equality that deal with   potentially non-zero Neumann conditions). You will see this if you   run the program. 

  An alternative way to change the boundary indicator is to label   the boundaries based on the Cartesian coordinates of the face centers.   For example, we can label all of the cells along the top and   bottom boundaries with a boundary indicator 1 by checking to   see if the cell centers' y-coordinates are within a tolerance   (here 1e-12) of -1 and 1. Try this immediately after calling    [2.x.2288]  as before:   [1.x.861] 

  Although this code is a bit longer than before, it is useful for   complex geometries, as it does not require knowledge of face labels. 

   [2.x.2289]    A slight variation of the last point would be to set different boundary   values as above, but then use a different boundary value function for   boundary indicator one. In practice, what you have to do is to add a second   call to  [2.x.2290]  for boundary indicator one:   [1.x.862] 

  If you have this call immediately after the first one to this function, then   it will interpolate boundary values on faces with boundary indicator 1 to the   unit value, and merge these interpolated values with those previously   computed for boundary indicator 0. The result will be that we will get   discontinuous boundary values, zero on three sides of the square, and one on   the fourth. 

   [2.x.2291]    Observe convergence: We will only discuss computing errors in norms in   step-7, but it is easy to check that computations converge   already here. For example, we could evaluate the value of the solution in a   single point and compare the value for different %numbers of global   refinement (the number of global refinement steps is set in    [2.x.2292]  above). To evaluate the   solution at a point, say at  [2.x.2293] , we could add the   following code to the  [2.x.2294]  function:   [1.x.863] 

  For 1 through 9 global refinement steps, we then get the following sequence   of point values:    [2.x.2295]    By noticing that the difference between each two consecutive values reduces   by about a factor of 4, we can conjecture that the "correct" value may be    [2.x.2296] . In fact, if we assumed this to be   the correct value, we could show that the sequence above indeed shows  [2.x.2297]  convergence &mdash; theoretically, the convergence order should be    [2.x.2298]  but the symmetry of the domain and the mesh may lead   to the better convergence order observed. 

  A slight variant of this would be to repeat the test with quadratic   elements. All you need to do is to set the polynomial degree of the finite   element to two in the constructor    [2.x.2299] . 

   [2.x.2300] Convergence of the mean: A different way to see that the solution   actually converges (to something &mdash; we can't tell whether it's really   the correct value!) is to compute the mean of the solution. To this end, add   the following code to  [2.x.2301] :   [1.x.864] 

  The documentation of the function explains what the second and fourth   parameters mean, while the first and third should be obvious. Doing the same   study again where we change the number of global refinement steps, we get   the following result:    [2.x.2302]    Again, the difference between two adjacent values goes down by about a   factor of four, indicating convergence as  [2.x.2303] .  [2.x.2304]  




[1.x.865] 

%HDF5 is a commonly used format that can be read by many scripting languages (e.g. R or Python). It is not difficult to get deal.II to produce some %HDF5 files that can then be used in external scripts to postprocess some of the data generated by this program. Here are some ideas on what is possible. 




[1.x.866] 

To fully make use of the automation we first need to introduce a private variable for the number of global refinement steps  [2.x.2305] , which will be used for the output filename. In  [2.x.2306]  with 

[1.x.867] 

The deal.II library has two different %HDF5 bindings, one in the HDF5 namespace (for interfacing to general-purpose data files) and another one in DataOut (specifically for writing files for the visualization of solutions). Although the HDF5 deal.II binding supports both serial and MPI, the %HDF5 DataOut binding only supports parallel output. For this reason we need to initialize an MPI communicator with only one processor. This is done by adding the following code. 

[1.x.868] 

Next we change the  [2.x.2307]  output routine as described in the DataOutBase namespace documentation: 

[1.x.869] 

The resulting file can then be visualized just like the VTK file that the original version of the tutorial produces; but, since %HDF5 is a more general file format, it can also easily be processed in scripting languages for other purposes. 




[1.x.870] 

After outputting the solution, the file can be opened again to include more datasets.  This allows us to keep all the necessary information of our experiment in a single result file, which can then be read and processed by some postprocessing script. (Have a look at  [2.x.2308]  for further information on the possible output options.) 

To make this happen, we first include the necessary header into our file: 

[1.x.871] 

Adding the following lines to the end of our output routine adds the information about the value of the solution at a particular point, as well as the mean value of the solution, to our %HDF5 file: 

[1.x.872] 






[1.x.873] 

The data put into %HDF5 files above can then be used from scripting languages for further postprocessing. In the following, let us show how this can, in particular, be done with the [1.x.874], a widely used language in statistical data analysis. (Similar things can also be done in Python, for example.) If you are unfamiliar with R and ggplot2 you could check out the data carpentry course on R [1.x.875]. Furthermore, since most search engines struggle with searches of the form "R + topic", we recommend using the specializes service [1.x.876] instead. 

The most prominent difference between R and other languages is that the assignment operator (`a = 5`) is typically written as `a <- 5`. As the latter is considered standard we will use it in our examples as well. To open the `.h5` file in R you have to install the [1.x.877] package, which is a part of the Bioconductor package. 

First we will include all necessary packages and have a look at how the data is structured in our file. 

[1.x.878] 

This gives the following output 

[1.x.879] 

The datasets can be accessed by  [2.x.2309] . The function  [2.x.2310]  gives us the dimensions of the matrix that is used to store our cells. We can see the following three matrices, as well as the two additional data points we added.  [2.x.2311]   [2.x.2312]   [2.x.2313] : a 4x1024 matrix that stores the  (C++) vertex indices for each cell  [2.x.2314]   [2.x.2315] : a 2x1089 matrix storing the position values (x,y) for our cell vertices  [2.x.2316]   [2.x.2317] : a 1x1089 matrix storing the values of our solution at each vertex  [2.x.2318]  Now we can use this data to generate various plots. Plotting with ggplot2 usually splits into two steps. At first the data needs to be manipulated and added to a  [2.x.2319] . After that, a  [2.x.2320]  object is constructed and manipulated by adding plot elements to it. 

 [2.x.2321]  contain all the information we need to plot our grid. The following code wraps all the data into one dataframe for plotting our grid: 

[1.x.880] 



With the finished dataframe we have everything we need to plot our grid: 

[1.x.881] 



The contents of this file then look as follows (not very exciting, but you get the idea):  [2.x.2322]  

We can also visualize the solution itself, and this is going to look more interesting. To make a 2D pseudocolor plot of our solution we will use  [2.x.2323] . This function needs a structured grid, i.e. uniform in x and y directions. Luckily our data at this point is structured in the right way. The following code plots a pseudocolor representation of our surface into a new PDF: 

[1.x.882] 

This is now going to look as follows:  [2.x.2324]  

For plotting the converge curves we need to re-run the C++ code multiple times with different values for  [2.x.2325]  starting from 1. Since every file only contains a single data point we need to loop over them and concatenate the results into a single vector. 

[1.x.883] 

As we are not interested in the values themselves but rather in the error compared to a "exact" solution we will assume our highest refinement level to be that solution and omit it from the data. 

[1.x.884] 

Now we have all the data available to generate our plots. It is often useful to plot errors on a log-log scale, which is accomplished in the following code: 

[1.x.885] 

This results in the following plot that shows how the errors in the mean value and the solution value at the chosen point nicely converge to zero:  [2.x.2326]  


examples/step-30/doc/intro.dox 

[1.x.886] 

[1.x.887] 




[1.x.888] 

This example is devoted to  [2.x.2327] anisotropic refinement [2.x.2328] , which extends to possibilities of local refinement. In most parts, this is a modification of the step-12 tutorial program, we use the same DG method for a linear transport equation. This program will cover the following topics: <ol>    [2.x.2329]   [2.x.2330] Anisotropic refinement [2.x.2331] : What is the meaning of anisotropic refinement?    [2.x.2332]   [2.x.2333] Implementation [2.x.2334] : Necessary modifications of code to work with anisotropically refined meshes.    [2.x.2335]   [2.x.2336] Jump indicator [2.x.2337] : A simple indicator for anisotropic refinement in   the context of DG methods.  [2.x.2338]  The discretization itself will not be discussed, and neither will implementation techniques not specific to anisotropic refinement used here. Please refer to step-12 for this. 

Please note, at the moment of writing this tutorial program, anisotropic refinement is only fully implemented for discontinuous Galerkin Finite Elements. This may later change (or may already have). 




 [2.x.2339]  While this program is a modification of step-12, it is an adaptation of a version of step-12 written early on in the history of deal.II when the MeshWorker framework wasn't available yet. Consequently, it bears little resemblance to the step-12 as it exists now, apart from the fact that it solves the same equation with the same discretization. 




[1.x.889] 

All the adaptive processes in the preceding tutorial programs were based on  [2.x.2340] isotropic [2.x.2341]  refinement of cells, which cuts all edges in half and forms new cells of these split edges (plus some additional edges, faces and vertices, of course). In deal.II,  [2.x.2342] anisotropic refinement [2.x.2343]  refers to the process of splitting only part of the edges while leaving the others unchanged. Consider a simple square cell, for example: 

[1.x.890] 

After the usual refinement it will consist of four children and look like this: 

[1.x.891] 

The new anisotropic refinement may take two forms: either we can split the edges which are parallel to the horizontal x-axis, resulting in these two child cells: 

[1.x.892] 

or we can split the two edges which run along the y-axis, resulting again in two children, which look that way, however: 

[1.x.893] 

All refinement cases of cells are described by an enumeration  [2.x.2344]  and the above anisotropic cases are called  [2.x.2345]  and  [2.x.2346]  for obvious reasons. The isotropic refinement case is called  [2.x.2347]  in 2D and can be requested from the RefinementCase class via  [2.x.2348]  

In 3D, there is a third axis which can be split, the z-axis, and thus we have an additional refinement case  [2.x.2349]  here. Isotropic refinement will now refine a cell along the x-, y- and z-axes and thus be referred to as  [2.x.2350]  cut_xyz. Additional cases  [2.x.2351]   [2.x.2352]  and  [2.x.2353]  exist, which refine a cell along two of the axes, but not along the third one. Given a hex cell with x-axis running to the right, y-axis 'into the page' and z-axis to the top, 

[1.x.894] 

we have the isotropic refinement case, 

[1.x.895] 

three anisotropic cases which refine only one axis: 

[1.x.896] 

and three cases which refine two of the three axes: 

[1.x.897] 

For 1D problems, anisotropic refinement can make no difference, as there is only one coordinate direction for a cell, so it is not possible to split it in any other way than isotropically. 

[1.x.898] Adaptive local refinement is used to obtain fine meshes which are well adapted to solving the problem at hand efficiently. In short, the size of cells which produce a large error is reduced to obtain a better approximation of the solution to the problem at hand. However, a lot of problems contain anisotropic features. Prominent examples are shocks or boundary layers in compressible viscous flows. An efficient mesh approximates these features with cells of higher aspect ratio which are oriented according to the mentioned features. Using only isotropic refinement, the aspect ratios of the original mesh cells are preserved, as they are inherited by the children of a cell. Thus, starting from an isotropic mesh, a boundary layer will be refined in order to catch the rapid variation of the flow field in the wall normal direction, thus leading to cells with very small edge lengths both in normal and tangential direction. Usually, much higher edge lengths in tangential direction and thus significantly less cells could be used without a significant loss in approximation accuracy. An anisotropic refinement process can modify the aspect ratio from mother to child cells by a factor of two for each refinement step. In the course of several refinements, the aspect ratio of the fine cells can be optimized, saving a considerable number of cells and correspondingly degrees of freedom and thus computational resources, memory as well as CPU time. 

[1.x.899] 

Most of the time, when we do finite element computations, we only consider one cell at a time, for example to calculate cell contributions to the global matrix, or to interpolate boundary values. However, sometimes we have to look at how cells are related in our algorithms. Relationships between cells come in two forms: neighborship and mother-child relationship. For the case of isotropic refinement, deal.II uses certain conventions (invariants) for cell relationships that are always maintained. For example, a refined cell always has exactly  [2.x.2354]  children. And (except for the 1d case), two neighboring cells may differ by at most one refinement level: they are equally often refined or one of them is exactly once more refined, leaving exactly one hanging node on the common face. Almost all of the time these invariants are only of concern in the internal implementation of the library. However, there are cases where knowledge of them is also relevant to an application program. 

In the current context, it is worth noting that the kind of mesh refinement affects some of the most fundamental assumptions. Consequently, some of the usual code found in application programs will need modifications to exploit the features of meshes which were created using anisotropic refinement. For those interested in how deal.II evolved, it may be of interest that the loosening of such invariants required some incompatible changes. For example, the library used to have a member  [2.x.2355]  that specified how many children a cell has once it is refined. For isotropic refinement, this number is equal to  [2.x.2356] , as mentioned above. However, for anisotropic refinement, this number does not exist, as is can be either two or four in 2D and two, four or eight in 3D, and the member  [2.x.2357]  has consequently been removed. It has now been replaced by  [2.x.2358]  which specifies the [1.x.900] number of children a cell can have. How many children a refined cell has was previously available as static information, but now it depends on the actual refinement state of a cell and can be retrieved using  [2.x.2359]  a call that works equally well for both isotropic and anisotropic refinement. A very similar situation can be found for faces and their subfaces: the pertinent information can be queried using  [2.x.2360]  or  [2.x.2361] , depending on the context. 

Another important aspect, and the most important one in this tutorial, is the treatment of neighbor-relations when assembling jump terms on the faces between cells. Looking at the documentation of the assemble_system functions in step-12 we notice, that we need to decide if a neighboring cell is coarser, finer or on the same (refinement) level as our current cell. These decisions do not work in the same way for anisotropic refinement as the information given by the  [2.x.2362] level [2.x.2363]  of a cell is not enough to completely characterize anisotropic cells; for example, are the terminal children of a two-dimensional cell that is first cut in  [2.x.2364] -direction and whose children are then cut in  [2.x.2365] -direction on level 2, or are they on level 1 as they would be if the cell would have been refined once isotropically, resulting in the same set of finest cells? 

After anisotropic refinement, a coarser neighbor is not necessarily exactly one level below ours, but can pretty much have any level relative to the current one; in fact, it can even be on a higher level even though it is coarser. Thus the decisions have to be made on a different basis, whereas the intention of the decisions stays the same. 

In the following, we will discuss the cases that can happen when we want to compute contributions to the matrix (or right hand side) of the form 

[1.x.901] 

or similar; remember that we integrate terms like this using the FEFaceValues and FESubfaceValues classes. We will also show how to write code that works for both isotropic and anisotropic refinement: 

 [2.x.2366]  

   [2.x.2367]   [2.x.2368] Finer neighbor [2.x.2369] : If we are on an active cell and want   to integrate over a face  [2.x.2370] , the first   possibility is that the neighbor behind this face is more refined,   i.e. has children occupying only part of the   common face. In this case, the face   under consideration has to be a refined one, which can determine by   asking  [2.x.2371] . If this is true, we need to   loop over   all subfaces and get the neighbors' child behind this subface, so that we can   reinit an FEFaceValues object with the neighbor and an FESubfaceValues object   with our cell and the respective subface. 

  For isotropic refinement, this kind is reasonably simple because we   know that an invariant of the isotropically refined adaptive meshes   in deal.II is that neighbors can only differ by exactly one   refinement level. However, this isn't quite true any more for   anisotropically refined meshes, in particular in 3d; there,   the active cell we are interested on the other side of  [2.x.2372]  might not   actually be a child of our   neighbor, but perhaps a grandchild or even a farther offspring. Fortunately,   this complexity is hidden in the internals of the library. All we need to do   is call the  [2.x.2373]    function. Still, in 3D there are two cases which need special consideration:    [2.x.2374]       [2.x.2375]  If the neighbor is refined more than once anisotropically, it might be   that here are not two or four but actually three subfaces to   consider. Imagine   the following refinement process of the (two-dimensional) face of   the (three-dimensional) neighbor cell we are considering: first the   face is refined along x, later on only the left subface is refined along y. 

[1.x.902] 

     Here the number of subfaces is three. It is important to note the subtle   differences between, for a face,  [2.x.2376]  and    [2.x.2377]  The first function returns the number of   immediate children, which would be two for the above example, whereas the   second returns the number of active offspring (i.e., including children,   grandchildren, and further descendants), which is the correct three in   the example above. Using  [2.x.2378]  works for   isotropic and anisotropic as well as 2D and 3D cases, so it should always be   used. It should be noted that if any of the cells behind the two   small subfaces on the left side of the rightmost image is further   refined, then the current cell (i.e. the side from which we are   viewing this common face) is going to be refined as well: this is so   because otherwise the invariant of having only one hanging node per   edge would be violated. 

     [2.x.2379]  It might be, that the neighbor is coarser, but still has children which   are finer than our current cell. This situation can occur if two equally   coarse cells are refined, where one of the cells has two children at the face   under consideration and the other one four. The cells in the next graphic are   only separated from each other to show the individual refinement cases. 

[1.x.903] 



  Here, the left two cells resulted from an anisotropic bisection of   the mother cell in  [2.x.2380] -direction, whereas the right four cells   resulted from a simultaneous anisotropic refinement in both the  [2.x.2381] -   and  [2.x.2382] -directions.   The left cell marked with # has two finer neighbors marked with +, but the   actual neighbor of the left cell is the complete right mother cell, as the   two cells marked with + are finer and their direct mother is the one   large cell.    [2.x.2383]  

  However, fortunately,  [2.x.2384]  takes care of   these situations by itself, if you loop over the correct number of subfaces,   in the above example this is two. The  [2.x.2385]  function   takes care of this too, so that the resulting state is always correct. There   is one little caveat, however: For reiniting the neighbors FEFaceValues object   you need to know the index of the face that points toward the current   cell. Usually you assume that the neighbor you get directly is as coarse or as   fine as you, if it has children, thus this information can be obtained with    [2.x.2386]  If the neighbor is coarser, however, you   would have to use the first value in  [2.x.2387]    instead. In order to make this easy for you, there is    [2.x.2388]  which does the correct thing for you and   returns the desired result. 

   [2.x.2389]   [2.x.2390] Neighbor is as fine as our cell [2.x.2391] : After we ruled out all cases in   which there are finer children, we only need to decide, whether the neighbor   is coarser here. For this, there is the    [2.x.2392]  function which returns a boolean. In   order to get the relevant case of a neighbor of the same coarseness we would   use  [2.x.2393] . The code inside this   block can be left untouched. However, there is one thing to mention here: If   we want to use a rule, which cell should assemble certain terms on a given   face we might think of the rule presented in step-12. We know that we have to   leave out the part about comparing our cell's level with that of the neighbor   and replace it with the test for a coarser neighbor presented above. However,   we also have to consider the possibility that neighboring cells of same   coarseness have the same index (on different levels). Thus we have to include   the case where the cells have the same index, and give an additional   condition, which of the cells should assemble the terms, e.g. we can choose   the cell with lower level. The details of this concept can be seen in the   implementation below. 

   [2.x.2394]   [2.x.2395] Coarser neighbor [2.x.2396] : The remaining case is obvious: If there are no   refined neighbors and the neighbor is not as fine as the current cell, then it must   be coarser. Thus we can leave the old condition phrase, simply using    [2.x.2397] . The  [2.x.2398]    function takes care of all the complexity of anisotropic refinement combined   with possible non standard face orientation, flip and rotation on general 3D meshes. 

 [2.x.2399]  

[1.x.904] When a triangulation is refined, cells which were not flagged for refinement may be refined nonetheless. This is due to additional smoothing algorithms which are either necessary or requested explicitly. In particular, the restriction that there be at most one hanging node on each edge frequently forces the refinement of additional cells neighboring ones that are already finer and are flagged for further refinement. 

However, deal.II also implements a number of algorithms that make sure that resulting meshes are smoother than just the bare minimum, for example ensuring that there are no isolated refined cells surrounded by non-refined ones, since the additional degrees of freedom on these islands would almost all be constrained by hanging node constraints. (See the documentation of the Triangulation class and its  [2.x.2400]  member for more information on mesh smoothing.) 

Most of the smoothing algorithms that were originally developed for the isotropic case have been adapted to work in a very similar way for both anisotropic and isotropic refinement. There are two algorithms worth mentioning, however: <ol>    [2.x.2401]   [2.x.2402] : In an isotropic environment,   this algorithm tries to ensure a good approximation quality by reducing the   difference in refinement level of cells meeting at a common vertex. However,   there is no clear corresponding concept for anisotropic refinement, thus this   algorithm may not be used in combination with anisotropic refinement. This   restriction is enforced by an assertion which throws an error as soon as the   algorithm is called on a triangulation which has been refined anisotropically. 

   [2.x.2403]   [2.x.2404] : If refinement is introduced to   limit the number of hanging nodes, the additional cells are often not needed   to improve the approximation quality. This is especially true for DG   methods. If you set the flag  [2.x.2405]  the   smoothing algorithm tries to minimize the number of probably unneeded   additional cells by using anisotropic refinement for the smoothing. If you set   this smoothing flag you might get anisotropically refined cells, even if you   never set a single refinement flag to anisotropic refinement. Be aware that   you should only use this flag, if your code respects the possibility of   anisotropic meshes. Combined with a suitable anisotropic indicator this flag   can help save additional cells and thus effort.  [2.x.2406]  




[1.x.905] 

Using the benefits of anisotropic refinement requires an indicator to catch anisotropic features of the solution and exploit them for the refinement process. Generally the anisotropic refinement process will consist of several steps: <ol>    [2.x.2407]  Calculate an error indicator.    [2.x.2408]  Use the error indicator to flag cells for refinement, e.g. using a fixed   number or fraction of cells. Those cells will be flagged for isotropic   refinement automatically.    [2.x.2409]  Evaluate a distinct anisotropic indicator only on the flagged cells.    [2.x.2410]  Use the anisotropic indicator to set a new, anisotropic refinement flag   for cells where this is appropriate, leave the flags unchanged otherwise.    [2.x.2411]  Call  [2.x.2412]  to perform the   requested refinement, using the requested isotropic and anisotropic flags.  [2.x.2413]  This approach is similar to the one we have used in step-27 for hp-refinement and has the great advantage of flexibility: Any error indicator can be used in the anisotropic process, i.e. if you have quite involved a posteriori goal-oriented error indicators available you can use them as easily as a simple Kelly error estimator. The anisotropic part of the refinement process is not influenced by this choice. Furthermore, simply leaving out the third and forth steps leads to the same isotropic refinement you used to get before any anisotropic changes in deal.II or your application program. As a last advantage, working only on cells flagged for refinement results in a faster evaluation of the anisotropic indicator, which can become noticeable on finer meshes with a lot of cells if the indicator is quite involved. 

Here, we use a very simple approach which is only applicable to DG methods. The general idea is quite simple: DG methods allow the discrete solution to jump over the faces of a cell, whereas it is smooth within each cell. Of course, in the limit we expect that the jumps tend to zero as we refine the mesh and approximate the true solution better and better. Thus, a large jump across a given face indicates that the cell should be refined (at least) orthogonally to that face, whereas a small jump does not lead to this conclusion. It is possible, of course, that the exact solution is not smooth and that it also features a jump. In that case, however, a large jump over one face indicates, that this face is more or less parallel to the jump and in the vicinity of it, thus again we would expect a refinement orthogonal to the face under consideration to be effective. 

The proposed indicator calculates the average jump  [2.x.2414] , i.e. the mean value of the absolute jump  [2.x.2415]  of the discrete solution  [2.x.2416]  over the two faces  [2.x.2417] ,  [2.x.2418] ,  [2.x.2419]  orthogonal to coordinate direction  [2.x.2420]  on the unit cell. 

[1.x.906] 

If the average jump in one direction is larger than the average of the jumps in the other directions by a certain factor  [2.x.2421] , i.e. if  [2.x.2422] , the cell is refined only along that particular direction  [2.x.2423] , otherwise the cell is refined isotropically. 

Such a criterion is easily generalized to systems of equations: the absolute value of the jump would be replaced by an appropriate norm of the vector-valued jump. 




[1.x.907] 

We solve the linear transport equation presented in step-12. The domain is extended to cover  [2.x.2424]  in 2D, where the flow field  [2.x.2425]  describes a counterclockwise quarter circle around the origin in the right half of the domain and is parallel to the x-axis in the left part of the domain. The inflow boundary is again located at  [2.x.2426]  and along the positive part of the x-axis, and the boundary conditions are chosen as in step-12. 


examples/step-30/doc/results.dox 



[1.x.908] 


The output of this program consist of the console output, the SVG files containing the grids, and the solutions given in VTU format. 

[1.x.909] 



This text output shows the reduction in the number of cells which results from the successive application of anisotropic refinement. After the last refinement step the savings have accumulated so much that almost four times as many cells and thus degrees of freedom are needed in the isotropic case. The time needed for assembly scales with a similar factor. 

The first interesting part is of course to see how the meshes look like. On the left are the isotropically refined ones, on the right the anisotropic ones (colors indicate the refinement level of cells): 

 [2.x.2427]  


The other interesting thing is, of course, to see the solution on these two sequences of meshes. Here they are, on the refinement cycles 1 and 4, clearly showing that the solution is indeed composed of [1.x.910] piecewise polynomials: 

 [2.x.2428]  

We see, that the solution on the anisotropically refined mesh is very similar to the solution obtained on the isotropically refined mesh. Thus the anisotropic indicator seems to effectively select the appropriate cells for anisotropic refinement. 

The pictures also explain why the mesh is refined as it is. In the whole left part of the domain refinement is only performed along the  [2.x.2429] -axis of cells. In the right part of the domain the refinement is dominated by isotropic refinement, as the anisotropic feature of the solution - the jump from one to zero - is not well aligned with the mesh where the advection direction takes a turn. However, at the bottom and closest (to the observer) parts of the quarter circle this jumps again becomes more and more aligned with the mesh and the refinement algorithm reacts by creating anisotropic cells of increasing aspect ratio. 

It might seem that the necessary alignment of anisotropic features and the coarse mesh can decrease performance significantly for real world problems. That is not wrong in general: If one were, for example, to apply anisotropic refinement to problems in which shocks appear (e.g., the equations solved in step-69), then it many cases the shock is not aligned with the mesh and anisotropic refinement will help little unless one also introduces techniques to move the mesh in alignment with the shocks. On the other hand, many steep features of solutions are due to boundary layers. In those cases, the mesh is already aligned with the anisotropic features because it is of course aligned with the boundary itself, and anisotropic refinement will almost always increase the efficiency of computations on adapted grids for these cases. 


examples/step-31/doc/intro.dox 

 [2.x.2430]  

[1.x.911] 


[1.x.912] 

[1.x.913] 

[1.x.914] 

This program deals with an interesting physical problem: how does a fluid (i.e., a liquid or gas) behave if it experiences differences in buoyancy caused by temperature differences? It is clear that those parts of the fluid that are hotter (and therefore lighter) are going to rise up and those that are cooler (and denser) are going to sink down with gravity. 

In cases where the fluid moves slowly enough such that inertial effects can be neglected, the equations that describe such behavior are the Boussinesq equations that read as follows: 

[1.x.915] 

These equations fall into the class of vector-valued problems (a toplevel overview of this topic can be found in the  [2.x.2431]  module). Here,  [2.x.2432]  is the velocity field,  [2.x.2433]  the pressure, and  [2.x.2434]  the temperature of the fluid.  [2.x.2435]  is the symmetric gradient of the velocity. As can be seen, velocity and pressure solve a Stokes equation describing the motion of an incompressible fluid, an equation we have previously considered in step-22; we will draw extensively on the experience we have gained in that program, in particular with regard to efficient linear Stokes solvers. 

The forcing term of the fluid motion is the buoyancy of the fluid, expressed as the product of the density  [2.x.2436] , the thermal expansion coefficient  [2.x.2437] , the temperature  [2.x.2438]  and the gravity vector  [2.x.2439]  pointing downward. (A derivation of why the right hand side looks like it looks is given in the introduction of step-32.) While the first two equations describe how the fluid reacts to temperature differences by moving around, the third equation states how the fluid motion affects the temperature field: it is an advection diffusion equation, i.e., the temperature is attached to the fluid particles and advected along in the flow field, with an additional diffusion (heat conduction) term. In many applications, the diffusion coefficient is fairly small, and the temperature equation is in fact transport, not diffusion dominated and therefore in character more hyperbolic than elliptic; we will have to take this into account when developing a stable discretization. 

In the equations above, the term  [2.x.2440]  on the right hand side denotes the heat sources and may be a spatially and temporally varying function.  [2.x.2441]  and  [2.x.2442]  denote the viscosity and diffusivity coefficients, which we assume constant for this tutorial program. The more general case when  [2.x.2443]  depends on the temperature is an important factor in physical applications: Most materials become more fluid as they get hotter (i.e.,  [2.x.2444]  decreases with  [2.x.2445] ); sometimes, as in the case of rock minerals at temperatures close to their melting point,  [2.x.2446]  may change by orders of magnitude over the typical range of temperatures. 

We note that the Stokes equation above could be nondimensionalized by introducing the [1.x.916]  [2.x.2447]  using a typical length scale  [2.x.2448] , typical temperature difference  [2.x.2449] , density  [2.x.2450] , thermal diffusivity  [2.x.2451] , and thermal conductivity  [2.x.2452] .  [2.x.2453]  is a dimensionless number that describes the ratio of heat transport due to convection induced by buoyancy changes from temperature differences, and of heat transport due to thermal diffusion. A small Rayleigh number implies that buoyancy is not strong relative to viscosity and fluid motion  [2.x.2454]  is slow enough so that heat diffusion  [2.x.2455]  is the dominant heat transport term. On the other hand, a fluid with a high Rayleigh number will show vigorous convection that dominates heat conduction. 

For most fluids for which we are interested in computing thermal convection, the Rayleigh number is very large, often  [2.x.2456]  or larger. From the structure of the equations, we see that this will lead to large pressure differences and large velocities. Consequently, the convection term in the convection-diffusion equation for  [2.x.2457]  will also be very large and an accurate solution of this equation will require us to choose small time steps. Problems with large Rayleigh numbers are therefore hard to solve numerically for similar reasons that make solving the [1.x.917] hard to solve when the [1.x.918] is large. 

Note that a large Rayleigh number does not necessarily involve large velocities in absolute terms. For example, the Rayleigh number in the earth mantle is larger than  [2.x.2458] . Yet the velocities are small: the material is in fact solid rock but it is so hot and under pressure that it can flow very slowly, on the order of at most a few centimeters per year. Nevertheless, this can lead to mixing over time scales of many million years, a time scale much shorter than for the same amount of heat to be distributed by thermal conductivity and a time scale of relevance to affect the evolution of the earth's interior and surface structure. 

 [2.x.2459]  If you are interested in using the program as the basis for your own experiments, you will also want to take a look at its continuation in step-32. Furthermore, step-32 later was developed into the much larger open source code ASPECT (see https://aspect.geodynamics.org/ ) that can solve realistic problems and that you may want to investigate before trying to morph step-31 into something that can solve whatever you want to solve. 




[1.x.919] 

Since the Boussinesq equations are derived under the assumption that inertia of the fluid's motion does not play a role, the flow field is at each time entirely determined by buoyancy difference at that time, not by the flow field at previous times. This is reflected by the fact that the first two equations above are the steady state Stokes equation that do not contain a time derivative. Consequently, we do not need initial conditions for either velocities or pressure. On the other hand, the temperature field does satisfy an equation with a time derivative, so we need initial conditions for  [2.x.2460] . 

As for boundary conditions: if  [2.x.2461]  then the temperature satisfies a second order differential equation that requires boundary data all around the boundary for all times. These can either be a prescribed boundary temperature  [2.x.2462]  (Dirichlet boundary conditions), or a prescribed thermal flux  [2.x.2463] ; in this program, we will use an insulated boundary condition, i.e., prescribe no thermal flux:  [2.x.2464] . 

Similarly, the velocity field requires us to pose boundary conditions. These may be no-slip no-flux conditions  [2.x.2465]  on  [2.x.2466]  if the fluid sticks to the boundary, or no normal flux conditions  [2.x.2467]  if the fluid can flow along but not across the boundary, or any number of other conditions that are physically reasonable. In this program, we will use no normal flux conditions. 




[1.x.920] 

Like the equations solved in step-21, we here have a system of differential-algebraic equations (DAE): with respect to the time variable, only the temperature equation is a differential equation whereas the Stokes system for  [2.x.2468]  and  [2.x.2469]  has no time-derivatives and is therefore of the sort of an algebraic constraint that has to hold at each time instant. The main difference to step-21 is that the algebraic constraint there was a mixed Laplace system of the form 

[1.x.921] 

where now we have a Stokes system 

[1.x.922] 

where  [2.x.2470]  is an operator similar to the Laplacian  [2.x.2471]  applied to a vector field. 

Given the similarity to what we have done in step-21, it may not come as a surprise that we choose a similar approach, although we will have to make adjustments for the change in operator in the top-left corner of the differential operator. 




[1.x.923] 

The structure of the problem as a DAE allows us to use the same strategy as we have already used in step-21, i.e., we use a time lag scheme: we first solve the temperature equation (using an extrapolated velocity field), and then insert the new temperature solution into the right hand side of the velocity equation. The way we implement this in our code looks at things from a slightly different perspective, though. We first solve the Stokes equations for velocity and pressure using the temperature field from the previous time step, which means that we get the velocity for the previous time step. In other words, we first solve the Stokes system for time step  [2.x.2472]  as 

[1.x.924] 

and then the temperature equation with an extrapolated velocity field to time  [2.x.2473] . 

In contrast to step-21, we'll use a higher order time stepping scheme here, namely the [1.x.925] that replaces the time derivative  [2.x.2474]  by the (one-sided) difference quotient  [2.x.2475]  with  [2.x.2476]  the time step size. This gives the discretized-in-time temperature equation 

[1.x.926] 

Note how the temperature equation is solved semi-explicitly: diffusion is treated implicitly whereas advection is treated explicitly using an extrapolation (or forward-projection) of temperature and velocity, including the just-computed velocity  [2.x.2477] . The forward-projection to the current time level  [2.x.2478]  is derived from a Taylor expansion,  [2.x.2479] . We need this projection for maintaining the order of accuracy of the BDF-2 scheme. In other words, the temperature fields we use in the explicit right hand side are second order approximations of the current temperature field &mdash; not quite an explicit time stepping scheme, but by character not too far away either. 

The introduction of the temperature extrapolation limits the time step by a [1.x.927] just like it was in  [2.x.2480]  "step-21". (We wouldn't have had that stability condition if we treated the advection term implicitly since the BDF-2 scheme is A-stable, at the price that we needed to build a new temperature matrix at each time step.) We will discuss the exact choice of time step in the [1.x.928], but for the moment of importance is that this CFL condition means that the time step size  [2.x.2481]  may change from time step to time step, and that we have to modify the above formula slightly. If  [2.x.2482]  are the time steps sizes of the current and previous time step, then we use the approximations 

[1.x.929] 

and 

[1.x.930] 

and above equation is generalized as follows: 

[1.x.931] 



where  [2.x.2483]  denotes the extrapolation of velocity  [2.x.2484]  and temperature  [2.x.2485]  to time level  [2.x.2486] , using the values at the two previous time steps. That's not an easy to read equation, but will provide us with the desired higher order accuracy. As a consistency check, it is easy to verify that it reduces to the same equation as above if  [2.x.2487] . 

As a final remark we note that the choice of a higher order time stepping scheme of course forces us to keep more time steps in memory; in particular, we here will need to have  [2.x.2488]  around, a vector that we could previously discard. This seems like a nuisance that we were able to avoid previously by using only a first order time stepping scheme, but as we will see below when discussing the topic of stabilization, we will need this vector anyway and so keeping it around for time discretization is essentially for free and gives us the opportunity to use a higher order scheme. 




[1.x.932] 

Like solving the mixed Laplace equations, solving the Stokes equations requires us to choose particular pairs of finite elements for velocities and pressure variables. Because this has already been discussed in step-22, we only cover this topic briefly: Here, we use the stable pair  [2.x.2489] . These are continuous elements, so we can form the weak form of the Stokes equation without problem by integrating by parts and substituting continuous functions by their discrete counterparts: 

[1.x.933] 

for all test functions  [2.x.2490] . The first term of the first equation is considered as the inner product between tensors, i.e.  [2.x.2491] . Because the second tensor in this product is symmetric, the anti-symmetric component of  [2.x.2492]  plays no role and it leads to the entirely same form if we use the symmetric gradient of  [2.x.2493]  instead. Consequently, the formulation we consider and that we implement is 

[1.x.934] 



This is exactly the same as what we already discussed in step-22 and there is not much more to say about this here. 




[1.x.935] 

The more interesting question is what to do with the temperature advection-diffusion equation. By default, not all discretizations of this equation are equally stable unless we either do something like upwinding, stabilization, or all of this. One way to achieve this is to use discontinuous elements (i.e., the FE_DGQ class that we used, for example, in the discretization of the transport equation in step-12, or in discretizing the pressure in step-20 and step-21) and to define a flux at the interface between cells that takes into account upwinding. If we had a pure advection problem this would probably be the simplest way to go. However, here we have some diffusion as well, and the discretization of the Laplace operator with discontinuous elements is cumbersome because of the significant number of additional terms that need to be integrated on each face between cells. Discontinuous elements also have the drawback that the use of numerical fluxes introduces an additional numerical diffusion that acts everywhere, whereas we would really like to minimize the effect of numerical diffusion to a minimum and only apply it where it is necessary to stabilize the scheme. 

A better alternative is therefore to add some nonlinear viscosity to the model. Essentially, what this does is to transform the temperature equation from the form 

[1.x.936] 

to something like 

[1.x.937] 

where  [2.x.2494]  is an addition viscosity (diffusion) term that only acts in the vicinity of shocks and other discontinuities.  [2.x.2495]  is chosen in such a way that if  [2.x.2496]  satisfies the original equations, the additional viscosity is zero. 

To achieve this, the literature contains a number of approaches. We will here follow one developed by Guermond and Popov that builds on a suitably defined residual and a limiting procedure for the additional viscosity. To this end, let us define a residual  [2.x.2497]  as follows: 

[1.x.938] 

where we will later choose the stabilization exponent  [2.x.2498]  from within the range  [2.x.2499] . Note that  [2.x.2500]  will be zero if  [2.x.2501]  satisfies the temperature equation, since then the term in parentheses will be zero. Multiplying terms out, we get the following, entirely equivalent form: 

[1.x.939] 



With this residual, we can now define the artificial viscosity as a piecewise constant function defined on each cell  [2.x.2502]  with diameter  [2.x.2503]  separately as follows: 

[1.x.940] 



Here,  [2.x.2504]  is a stabilization constant (a dimensional analysis reveals that it is unitless and therefore independent of scaling; we will discuss its choice in the [1.x.941]) and  [2.x.2505]  is a normalization constant that must have units  [2.x.2506] . We will choose it as  [2.x.2507] , where  [2.x.2508]  is the range of present temperature values (remember that buoyancy is driven by temperature variations, not the absolute temperature) and  [2.x.2509]  is a dimensionless constant. To understand why this method works consider this: If on a particular cell  [2.x.2510]  the temperature field is smooth, then we expect the residual to be small there (in fact to be on the order of  [2.x.2511] ) and the stabilization term that injects artificial diffusion will there be of size  [2.x.2512]  &mdash; i.e., rather small, just as we hope it to be when no additional diffusion is necessary. On the other hand, if we are on or close to a discontinuity of the temperature field, then the residual will be large; the minimum operation in the definition of  [2.x.2513]  will then ensure that the stabilization has size  [2.x.2514]  &mdash; the optimal amount of artificial viscosity to ensure stability of the scheme. 

Whether or not this scheme really works is a good question. Computations by Guermond and Popov have shown that this form of stabilization actually performs much better than most of the other stabilization schemes that are around (for example streamline diffusion, to name only the simplest one). Furthermore, for  [2.x.2515]  they can even prove that it produces better convergence orders for the linear transport equation than for example streamline diffusion. For  [2.x.2516] , no theoretical results are currently available, but numerical tests indicate that the results are considerably better than for  [2.x.2517] . 

A more practical question is how to introduce this artificial diffusion into the equations we would like to solve. Note that the numerical viscosity  [2.x.2518]  is temperature-dependent, so the equation we want to solve is nonlinear in  [2.x.2519]  &mdash; not what one desires from a simple method to stabilize an equation, and even less so if we realize that  [2.x.2520]  is nondifferentiable in  [2.x.2521] . However, there is no reason to despair: we still have to discretize in time and we can treat the term explicitly. 

In the definition of the stabilization parameter, we approximate the time derivative by  [2.x.2522] . This approximation makes only use of available time data and this is the reason why we need to store data of two previous time steps (which enabled us to use the BDF-2 scheme without additional storage cost). We could now simply evaluate the rest of the terms at  [2.x.2523] , but then the discrete residual would be nothing else than a backward Euler approximation, which is only first order accurate. So, in case of smooth solutions, the residual would be still of the order  [2.x.2524] , despite the second order time accuracy in the outer BDF-2 scheme and the spatial FE discretization. This is certainly not what we want to have (in fact, we desired to have small residuals in regions where the solution behaves nicely), so a bit more care is needed. The key to this problem is to observe that the first derivative as we constructed it is actually centered at  [2.x.2525] . We get the desired second order accurate residual calculation if we evaluate all spatial terms at  [2.x.2526]  by using the approximation  [2.x.2527] , which means that we calculate the nonlinear viscosity as a function of this intermediate temperature,  [2.x.2528] . Note that this evaluation of the residual is nothing else than a Crank-Nicholson scheme, so we can be sure that now everything is alright. One might wonder whether it is a problem that the numerical viscosity now is not evaluated at time  [2.x.2529]  (as opposed to the rest of the equation). However, this offset is uncritical: For smooth solutions,  [2.x.2530]  will vary continuously, so the error in time offset is  [2.x.2531]  times smaller than the nonlinear viscosity itself, i.e., it is a small higher order contribution that is left out. That's fine because the term itself is already at the level of discretization error in smooth regions. 

Using the BDF-2 scheme introduced above, this yields for the simpler case of uniform time steps of size  [2.x.2532] : 

[1.x.942] 

On the left side of this equation remains the term from the time derivative and the original (physical) diffusion which we treat implicitly (this is actually a nice term: the matrices that result from the left hand side are the mass matrix and a multiple of the Laplace matrix &mdash; both are positive definite and if the time step size  [2.x.2533]  is small, the sum is simple to invert). On the right hand side, the terms in the first line result from the time derivative; in the second line is the artificial diffusion at time  [2.x.2534] ; the third line contains the advection term, and the fourth the sources. Note that the artificial diffusion operates on the extrapolated temperature at the current time in the same way as we have discussed the advection works in the section on time stepping. 

The form for nonuniform time steps that we will have to use in reality is a bit more complicated (which is why we showed the simpler form above first) and reads: 

[1.x.943] 



After settling all these issues, the weak form follows naturally from the strong form shown in the last equation, and we immediately arrive at the weak form of the discretized equations: 

[1.x.944] 

for all discrete test functions  [2.x.2535] . Here, the diffusion term has been integrated by parts, and we have used that we will impose no thermal flux,  [2.x.2536] . 

This then results in a matrix equation of form 

[1.x.945] 

which given the structure of matrix on the left (the sum of two positive definite matrices) is easily solved using the Conjugate Gradient method. 




[1.x.946] 

As explained above, our approach to solving the joint system for velocities/pressure on the one hand and temperature on the other is to use an operator splitting where we first solve the Stokes system for the velocities and pressures using the old temperature field, and then solve for the new temperature field using the just computed velocity field. (A more extensive discussion of operator splitting methods can be found in step-58.) 




[1.x.947] 

Solving the linear equations coming from the Stokes system has been discussed in great detail in step-22. In particular, in the results section of that program, we have discussed a number of alternative linear solver strategies that turned out to be more efficient than the original approach. The best alternative identified there we to use a GMRES solver preconditioned by a block matrix involving the Schur complement. Specifically, the Stokes operator leads to a block structured matrix 

[1.x.948] 

and as discussed there a good preconditioner is 

[1.x.949] 

where  [2.x.2537]  is the Schur complement of the Stokes operator  [2.x.2538] . Of course, this preconditioner is not useful because we can't form the various inverses of matrices, but we can use the following as a preconditioner: 

[1.x.950] 

where  [2.x.2539]  are approximations to the inverse matrices. In particular, it turned out that  [2.x.2540]  is spectrally equivalent to the mass matrix and consequently replacing  [2.x.2541]  by a CG solver applied to the mass matrix on the pressure space was a good choice. In a small deviation from step-22, we here have a coefficient  [2.x.2542]  in the momentum equation, and by the same derivation as there we should arrive at the conclusion that it is the weighted mass matrix with entries  [2.x.2543]  that we should be using. 

It was more complicated to come up with a good replacement  [2.x.2544] , which corresponds to the discretized symmetric Laplacian of the vector-valued velocity field, i.e.  [2.x.2545] . In step-22 we used a sparse LU decomposition (using the SparseDirectUMFPACK class) of  [2.x.2546]  for  [2.x.2547]  &mdash; the perfect preconditioner &mdash; in 2d, but for 3d memory and compute time is not usually sufficient to actually compute this decomposition; consequently, we only use an incomplete LU decomposition (ILU, using the SparseILU class) in 3d. 

For this program, we would like to go a bit further. To this end, note that the symmetrized bilinear form on vector fields,  [2.x.2548]  is not too far away from the nonsymmetrized version,  [2.x.2549]  (note that the factor 2 has disappeared in this form). The latter, however, has the advantage that the  [2.x.2550]  vector components of the test functions are not coupled (well, almost, see below), i.e., the resulting matrix is block-diagonal: one block for each vector component, and each of these blocks is equal to the Laplace matrix for this vector component. So assuming we order degrees of freedom in such a way that first all  [2.x.2551] -components of the velocity are numbered, then the  [2.x.2552] -components, and then the  [2.x.2553] -components, then the matrix  [2.x.2554]  that is associated with this slightly different bilinear form has the form 

[1.x.951] 

where  [2.x.2555]  is a Laplace matrix of size equal to the number of shape functions associated with each component of the vector-valued velocity. With this matrix, one could be tempted to define our preconditioner for the velocity matrix  [2.x.2556]  as follows: 

[1.x.952] 

where  [2.x.2557]  is a preconditioner for the Laplace matrix &mdash; something where we know very well how to build good preconditioners! 

In reality, the story is not quite as simple: To make the matrix  [2.x.2558]  definite, we need to make the individual blocks  [2.x.2559]  definite by applying boundary conditions. One can try to do so by applying Dirichlet boundary conditions all around the boundary, and then the so-defined preconditioner  [2.x.2560]  turns out to be a good preconditioner for  [2.x.2561]  if the latter matrix results from a Stokes problem where we also have Dirichlet boundary conditions on the velocity components all around the domain, i.e., if we enforce  [2.x.2562] . 

Unfortunately, this "if" is an "if and only if": in the program below we will want to use no-flux boundary conditions of the form  [2.x.2563]  (i.e., flow %parallel to the boundary is allowed, but no flux through the boundary). In this case, it turns out that the block diagonal matrix defined above is not a good preconditioner because it neglects the coupling of components at the boundary. A better way to do things is therefore if we build the matrix  [2.x.2564]  as the vector Laplace matrix  [2.x.2565]  and then apply the same boundary condition as we applied to  [2.x.2566] . If this is a Dirichlet boundary condition all around the domain, the  [2.x.2567]  will decouple to three diagonal blocks as above, and if the boundary conditions are of the form  [2.x.2568]  then this will introduce a coupling of degrees of freedom at the boundary but only there. This, in fact, turns out to be a much better preconditioner than the one introduced above, and has almost all the benefits of what we hoped to get. 


To sum this whole story up, we can observe:  [2.x.2569]     [2.x.2570]  Compared to building a preconditioner from the original matrix  [2.x.2571]    resulting from the symmetric gradient as we did in step-22,   we have to expect that the preconditioner based on the Laplace bilinear form   performs worse since it does not take into account the coupling between   vector components. 

   [2.x.2572] On the other hand, preconditioners for the Laplace matrix are typically   more mature and perform better than ones for vector problems. For example,   at the time of this writing, Algebraic %Multigrid (AMG) algorithms are very   well developed for scalar problems, but not so for vector problems. 

   [2.x.2573] In building this preconditioner, we will have to build up the   matrix  [2.x.2574]  and its preconditioner. While this means that we   have to store an additional matrix we didn't need before, the   preconditioner  [2.x.2575]  is likely going to need much less   memory than storing a preconditioner for the coupled matrix    [2.x.2576] . This is because the matrix  [2.x.2577]  has only a third of the   entries per row for all rows corresponding to interior degrees of   freedom, and contains coupling between vector components only on   those parts of the boundary where the boundary conditions introduce   such a coupling. Storing the matrix is therefore comparatively   cheap, and we can expect that computing and storing the   preconditioner  [2.x.2578]  will also be much cheaper compared to   doing so for the fully coupled matrix.  [2.x.2579]  




[1.x.953] 

This is the easy part: The matrix for the temperature equation has the form  [2.x.2580] , where  [2.x.2581]  are mass and stiffness matrices on the temperature space, and  [2.x.2582]  are constants related the time stepping scheme and the current and previous time step. This being the sum of a symmetric positive definite and a symmetric positive semidefinite matrix, the result is also symmetric positive definite. Furthermore,  [2.x.2583]  is a number proportional to the time step, and so becomes small whenever the mesh is fine, damping the effect of the then ill-conditioned stiffness matrix. 

As a consequence, inverting this matrix with the Conjugate Gradient algorithm, using a simple preconditioner, is trivial and very cheap compared to inverting the Stokes matrix. 




[1.x.954] 

[1.x.955] 

One of the things worth explaining up front about the program below is the use of two different DoFHandler objects. If one looks at the structure of the equations above and the scheme for their solution, one realizes that there is little commonality that keeps the Stokes part and the temperature part together. In all previous tutorial programs in which we have discussed  [2.x.2584]  "vector-valued problems" we have always only used a single finite element with several vector components, and a single DoFHandler object. Sometimes, we have substructured the resulting matrix into blocks to facilitate particular solver schemes; this was, for example, the case in the step-22 program for the Stokes equations upon which the current program is based. 

We could of course do the same here. The linear system that we would get would look like this: 

[1.x.956] 

The problem with this is: We never use the whole matrix at the same time. In fact, it never really exists at the same time: As explained above,  [2.x.2585]  and  [2.x.2586]  depend on the already computed solution  [2.x.2587] , in the first case through the time step (that depends on  [2.x.2588]  because it has to satisfy a CFL condition). So we can only assemble it once we've already solved the top left  [2.x.2589]  block Stokes system, and once we've moved on to the temperature equation we don't need the Stokes part any more; the fact that we build an object for a matrix that never exists as a whole in memory at any given time led us to jumping through some hoops in step-21, so let's not repeat this sort of error. Furthermore, we don't actually build the matrix  [2.x.2590] : Because by the time we get to the temperature equation we already know  [2.x.2591] , and because we have to assemble the right hand side  [2.x.2592]  at this time anyway, we simply move the term  [2.x.2593]  to the right hand side and assemble it along with all the other terms there. What this means is that there does not remain a part of the matrix where temperature variables and Stokes variables couple, and so a global enumeration of all degrees of freedom is no longer important: It is enough if we have an enumeration of all Stokes degrees of freedom, and of all temperature degrees of freedom independently. 

In essence, there is consequently not much use in putting [1.x.957] into a block matrix (though there are of course the same good reasons to do so for the  [2.x.2594]  Stokes part), or, for that matter, in putting everything into the same DoFHandler object. 

But are there [1.x.958] to doing so? These exist, though they may not be obvious at first. The main problem is that if we need to create one global finite element that contains velocity, pressure, and temperature shape functions, and use this to initialize the DoFHandler. But we also use this finite element object to initialize all FEValues or FEFaceValues objects that we use. This may not appear to be that big a deal, but imagine what happens when, for example, we evaluate the residual  [2.x.2595]  that we need to compute the artificial viscosity  [2.x.2596] .  For this, we need the Laplacian of the temperature, which we compute using the tensor of second derivatives (Hessians) of the shape functions (we have to give the  [2.x.2597]  flag to the FEValues object for this). Now, if we have a finite that contains the shape functions for velocities, pressures, and temperatures, that means that we have to compute the Hessians of [1.x.959] shape functions, including the many higher order shape functions for the velocities. That's a lot of computations that we don't need, and indeed if one were to do that (as we had in an early version of the program), assembling the right hand side took about a quarter of the overall compute time. 

So what we will do is to use two different finite element objects, one for the Stokes components and one for the temperatures. With this come two different DoFHandlers, two sparsity patterns and two matrices for the Stokes and temperature parts, etc. And whenever we have to assemble something that contains both temperature and Stokes shape functions (in particular the right hand sides of Stokes and temperature equations), then we use two FEValues objects initialized with two cell iterators that we walk in %parallel through the two DoFHandler objects associated with the same Triangulation object; for these two FEValues objects, we use of course the same quadrature objects so that we can iterate over the same set of quadrature points, but each FEValues object will get update flags only according to what it actually needs to compute. In particular, when we compute the residual as above, we only ask for the values of the Stokes shape functions, but also the Hessians of the temperature shape functions &mdash; much cheaper indeed, and as it turns out: assembling the right hand side of the temperature equation is now a component of the program that is hardly measurable. 

With these changes, timing the program yields that only the following operations are relevant for the overall run time:  [2.x.2598]     [2.x.2599] Solving the Stokes system: 72% of the run time.    [2.x.2600] Assembling the Stokes preconditioner and computing the algebraic       multigrid hierarchy using the Trilinos ML package: 11% of the       run time.    [2.x.2601] The function  [2.x.2602] : 7%       of overall run time.    [2.x.2603] Assembling the Stokes and temperature right hand side vectors as       well as assembling the matrices: 7%.  [2.x.2604]  In essence this means that all bottlenecks apart from the algebraic multigrid have been removed. 




[1.x.960] 

In much the same way as we used PETSc to support our linear algebra needs in step-17 and step-18, we use interfaces to the [1.x.961] library (see the deal.II README file for installation instructions) in this program. Trilinos is a very large collection of everything that has to do with linear and nonlinear algebra, as well as all sorts of tools around that (and looks like it will grow in many other directions in the future as well). 

The main reason for using Trilinos, similar to our exploring PETSc, is that it is a very powerful library that provides a lot more tools than deal.II's own linear algebra library. That includes, in particular, the ability to work in %parallel on a cluster, using MPI, and a wider variety of preconditioners. In the latter class, one of the most interesting capabilities is the existence of the Trilinos ML package that implements an Algebraic Multigrid (AMG) method. We will use this preconditioner to precondition the second order operator part of the momentum equation. The ability to solve problems in %parallel will be explored in step-32, using the same problem as discussed here. 

PETSc, which we have used in step-17 and step-18, is certainly a powerful library, providing a large number of functions that deal with matrices, vectors, and iterative solvers and preconditioners, along with lots of other stuff, most of which runs quite well in %parallel. It is, however, a few years old already than Trilinos, written in C, and generally not quite as easy to use as some other libraries. As a consequence, deal.II has also acquired interfaces to Trilinos, which shares a lot of the same functionality with PETSc. It is, however, a project that is several years younger, is written in C++ and by people who generally have put a significant emphasis on software design. 




[1.x.962] 

The case we want to solve here is as follows: we solve the Boussinesq equations described above with  [2.x.2605] , i.e., a relatively slow moving fluid that has virtually no thermal diffusive conductivity and transports heat mainly through convection. On the boundary, we will require no-normal flux for the velocity ( [2.x.2606] ) and for the temperature ( [2.x.2607] ). This is one of the cases discussed in the introduction of step-22 and fixes one component of the velocity while allowing flow to be %parallel to the boundary. There remain  [2.x.2608]  components to be fixed, namely the tangential components of the normal stress; for these, we choose homogeneous conditions which means that we do not have to anything special. Initial conditions are only necessary for the temperature field, and we choose it to be constant zero. 

The evolution of the problem is then entirely driven by the right hand side  [2.x.2609]  of the temperature equation, i.e., by heat sources and sinks. Here, we choose a setup invented in advance of a Christmas lecture: real candles are of course prohibited in U.S. class rooms, but virtual ones are allowed. We therefore choose three spherical heat sources unequally spaced close to the bottom of the domain, imitating three candles. The fluid located at these sources, initially at rest, is then heated up and as the temperature rises gains buoyancy, rising up; more fluid is dragged up and through the sources, leading to three hot plumes that rise up until they are captured by the recirculation of fluid that sinks down on the outside, replacing the air that rises due to heating. 


examples/step-31/doc/results.dox 



[1.x.963] 

[1.x.964] 

When you run the program in 2d, the output will look something like this: <code> <pre> Number of active cells: 256 (on 5 levels) Number of degrees of freedom: 3556 (2178+289+1089) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.919118    9 CG iterations for temperature.    Temperature range: -0.16687 1.30011 

Number of active cells: 280 (on 6 levels) Number of degrees of freedom: 4062 (2490+327+1245) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.459559    9 CG iterations for temperature.    Temperature range: -0.0982971 0.598503 

Number of active cells: 520 (on 7 levels) Number of degrees of freedom: 7432 (4562+589+2281) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.229779    9 CG iterations for temperature.    Temperature range: -0.0551098 0.294493 

Number of active cells: 1072 (on 8 levels) Number of degrees of freedom: 15294 (9398+1197+4699) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.11489    9 CG iterations for temperature.    Temperature range: -0.0273524 0.156861 

Number of active cells: 2116 (on 9 levels) Number of degrees of freedom: 30114 (18518+2337+9259) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.0574449    9 CG iterations for temperature.    Temperature range: -0.014993 0.0738328 

Timestep 1:  t=0.0574449    Assembling...    Solving...    56 GMRES iterations for Stokes subsystem.    Time step: 0.0574449    9 CG iterations for temperature.    Temperature range: -0.0273934 0.14488 

... </pre> </code> 

In the beginning we refine the mesh several times adaptively and always return to time step zero to restart on the newly refined mesh. Only then do we start the actual time iteration. 

The program runs for a while. The temperature field for time steps 0, 500, 1000, 1500, 2000, 3000, 4000, and 5000 looks like this (note that the color scale used for the temperature is not always the same): 

 [2.x.2610]  

The visualizations shown here were generated using a version of the example which did not enforce the constraints after transferring the mesh. 

As can be seen, we have three heat sources that heat fluid and therefore produce a buoyancy effect that lets hots pockets of fluid rise up and swirl around. By a chimney effect, the three streams are pressed together by fluid that comes from the outside and wants to join the updraft party. Note that because the fluid is initially at rest, those parts of the fluid that were initially over the sources receive a longer heating time than that fluid that is later dragged over the source by the fully developed flow field. It is therefore hotter, a fact that can be seen in the red tips of the three plumes. Note also the relatively fine features of the flow field, a result of the sophisticated transport stabilization of the temperature equation we have chosen. 

In addition to the pictures above, the following ones show the adaptive mesh and the flow field at the same time steps: 

 [2.x.2611]  




[1.x.965] 

The same thing can of course be done in 3d by changing the template parameter to the BoussinesqFlowProblem object in  [2.x.2612]  from 2 to 3, so that the output now looks like follows: 

<code> <pre> Number of active cells: 64 (on 3 levels) Number of degrees of freedom: 3041 (2187+125+729) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 2.45098    9 CG iterations for temperature.    Temperature range: -0.675683 4.94725 

Number of active cells: 288 (on 4 levels) Number of degrees of freedom: 12379 (8943+455+2981) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 1.22549    9 CG iterations for temperature.    Temperature range: -0.527701 2.25764 

Number of active cells: 1296 (on 5 levels) Number of degrees of freedom: 51497 (37305+1757+12435) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.612745    10 CG iterations for temperature.    Temperature range: -0.496942 0.847395 

Number of active cells: 5048 (on 6 levels) Number of degrees of freedom: 192425 (139569+6333+46523) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.306373    10 CG iterations for temperature.    Temperature range: -0.267683 0.497739 

Timestep 1:  t=0.306373    Assembling...    Solving...    27 GMRES iterations for Stokes subsystem.    Time step: 0.306373    10 CG iterations for temperature.    Temperature range: -0.461787 0.958679 

... </pre> </code> 

Visualizing the temperature isocontours at time steps 0, 50, 100, 150, 200, 300, 400, 500, 600, 700, and 800 yields the following plots: 

 [2.x.2613]  

That the first picture looks like three hedgehogs stems from the fact that our scheme essentially projects the source times the first time step size onto the mesh to obtain the temperature field in the first time step. Since the source function is discontinuous, we need to expect over- and undershoots from this project. This is in fact what happens (it's easier to check this in 2d) and leads to the crumpled appearance of the isosurfaces.  The visualizations shown here were generated using a version of the example which did not enforce the constraints after transferring the mesh. 




[1.x.966] 

The program as is has three parameters that we don't have much of a theoretical handle on how to choose in an optimal way. These are:  [2.x.2614]     [2.x.2615] The time step must satisfy a CFL condition        [2.x.2616] . Here,  [2.x.2617]  is       dimensionless, but what is the right value?    [2.x.2618] In the computation of the artificial viscosity, 

[1.x.967] 

      with  [2.x.2619] .       Here, the choice of the dimensionless %numbers  [2.x.2620]  is of       interest.  [2.x.2621]  In all of these cases, we will have to expect that the correct choice of each value depends on that of the others, and most likely also on the space dimension and polynomial degree of the finite element used for the temperature. Below we'll discuss a few numerical experiments to choose constants  [2.x.2622]  and  [2.x.2623] . 

Below, we will not discuss the choice of  [2.x.2624] . In the program, we set it to  [2.x.2625] . The reason for this value is a bit complicated and has more to do with the history of the program than reasoning: while the correct formula for the global scaling parameter  [2.x.2626]  is shown above, the program (including the version shipped with deal.II 6.2) initially had a bug in that we computed  [2.x.2627]  instead, where we had set the scaling parameter to one. Since we only computed on the unit square/cube where  [2.x.2628] , this was entirely equivalent to using the correct formula with  [2.x.2629] . Since this value for  [2.x.2630]  appears to work just fine for the current program, we corrected the formula in the program and set  [2.x.2631]  to a value that reproduces exactly the results we had before. We will, however, revisit this issue again in step-32. 

Now, however, back to the discussion of what values of  [2.x.2632]  and  [2.x.2633]  to choose: 




[1.x.968][1.x.969] 

These two constants are definitely linked in some way. The reason is easy to see: In the case of a pure advection problem,  [2.x.2634] , any explicit scheme has to satisfy a CFL condition of the form  [2.x.2635] . On the other hand, for a pure diffusion problem,  [2.x.2636] , explicit schemes need to satisfy a condition  [2.x.2637] . So given the form of  [2.x.2638]  above, an advection diffusion problem like the one we have to solve here will result in a condition of the form  [2.x.2639] . It follows that we have to face the fact that we might want to choose  [2.x.2640]  larger to improve the stability of the numerical scheme (by increasing the amount of artificial diffusion), but we have to pay a price in the form of smaller, and consequently more time steps. In practice, one would therefore like to choose  [2.x.2641]  as small as possible to keep the transport problem sufficiently stabilized while at the same time trying to choose the time step as large as possible to reduce the overall amount of work. 

The find the right balance, the only way is to do a few computational experiments. Here's what we did: We modified the program slightly to allow less mesh refinement (so we don't always have to wait that long) and to choose  [2.x.2642]  to eliminate the effect of the constant  [2.x.2643]  (we know that solutions are stable by using this version of  [2.x.2644]  as an artificial viscosity, but that we can improve things -- i.e. make the solution sharper -- by using the more complicated formula for this artificial viscosity). We then run the program for different values  [2.x.2645]  and observe maximal and minimal temperatures in the domain. What we expect to see is this: If we choose the time step too big (i.e. choose a  [2.x.2646]  bigger than theoretically allowed) then we will get exponential growth of the temperature. If we choose  [2.x.2647]  too small, then the transport stabilization becomes insufficient and the solution will show significant oscillations but not exponential growth. 




[1.x.970] 

Here is what we get for  [2.x.2648] , and  [2.x.2649] , different choices of  [2.x.2650] , and bilinear elements ( [2.x.2651] ) in 2d: 

 [2.x.2652]  

The way to interpret these graphs goes like this: for  [2.x.2653]  and  [2.x.2654] , we see exponential growth or at least large variations, but if we choose  [2.x.2655]  or smaller, then the scheme is stable though a bit wobbly. For more artificial diffusion, we can choose  [2.x.2656]  or smaller for  [2.x.2657] ,  [2.x.2658]  or smaller for  [2.x.2659] , and again need  [2.x.2660]  for  [2.x.2661]  (this time because much diffusion requires a small time step). 

So how to choose? If we were simply interested in a large time step, then we would go with  [2.x.2662]  and  [2.x.2663] . On the other hand, we're also interested in accuracy and here it may be of interest to actually investigate what these curves show. To this end note that we start with a zero temperature and that our sources are positive &mdash; so we would intuitively expect that the temperature can never drop below zero. But it does, a consequence of Gibb's phenomenon when using continuous elements to approximate a discontinuous solution. We can therefore see that choosing  [2.x.2664]  too small is bad: too little artificial diffusion leads to over- and undershoots that aren't diffused away. On the other hand, for large  [2.x.2665] , the minimum temperature drops below zero at the beginning but then quickly diffuses back to zero. 

On the other hand, let's also look at the maximum temperature. Watching the movie of the solution, we see that initially the fluid is at rest. The source keeps heating the same volume of fluid whose temperature increases linearly at the beginning until its buoyancy is able to move it upwards. The hottest part of the fluid is therefore transported away from the solution and fluid taking its place is heated for only a short time before being moved out of the source region, therefore remaining cooler than the initial bubble. If  [2.x.2666]  (in the program it is nonzero but very small) then the hottest part of the fluid should be advected along with the flow with its temperature constant. That's what we can see in the graphs with the smallest  [2.x.2667] : Once the maximum temperature is reached, it hardly changes any more. On the other hand, the larger the artificial diffusion, the more the hot spot is diffused. Note that for this criterion, the time step size does not play a significant role. 

So to sum up, likely the best choice would appear to be  [2.x.2668]  and  [2.x.2669] . The curve is a bit wobbly, but overall pictures looks pretty reasonable with the exception of some over and undershoots close to the start time due to Gibb's phenomenon. 




[1.x.971] 

One can repeat the same sequence of experiments for higher order elements as well. Here are the graphs for bi-quadratic shape functions ( [2.x.2670] ) for the temperature, while we retain the  [2.x.2671]  stable Taylor-Hood element for the Stokes system: 

 [2.x.2672]  

Again, small values of  [2.x.2673]  lead to less diffusion but we have to choose the time step very small to keep things under control. Too large values of  [2.x.2674]  make for more diffusion, but again require small time steps. The best value would appear to be  [2.x.2675] , as for the  [2.x.2676]  element, and then we have to choose  [2.x.2677]  &mdash; exactly half the size for the  [2.x.2678]  element, a fact that may not be surprising if we state the CFL condition as the requirement that the time step be small enough so that the distance transport advects in each time step is no longer than one [1.x.972] away (which for  [2.x.2679]  elements is  [2.x.2680] , but for  [2.x.2681]  elements is  [2.x.2682] ). It turns out that  [2.x.2683]  needs to be slightly larger for obtaining stable results also late in the simulation at times larger than 60, so we actually choose it as  [2.x.2684]  in the code. 




[1.x.973] 

One can repeat these experiments in 3d and find the optimal time step for each value of  [2.x.2685]  and find the best value of  [2.x.2686] . What one finds is that for the same  [2.x.2687]  already used in 2d, the time steps needs to be a bit smaller, by around a factor of 1.2 or so. This is easily explained: the time step restriction is  [2.x.2688]  where  [2.x.2689]  is the [1.x.974] of the cell. However, what is really needed is the distance between mesh points, which is  [2.x.2690] . So a more appropriate form would be  [2.x.2691] . 

The second find is that one needs to choose  [2.x.2692]  slightly bigger (about  [2.x.2693]  or so). This then again reduces the time step we can take. 







[1.x.975] 

Concluding, from the simple computations above,  [2.x.2694]  appears to be a good choice for the stabilization parameter in 2d, and  [2.x.2695]  in 3d. In a dimension independent way, we can model this as  [2.x.2696] . If one does longer computations (several thousand time steps) on finer meshes, one realizes that the time step size is not quite small enough and that for stability one will have to reduce the above values a bit more (by about a factor of  [2.x.2697] ). 

As a consequence, a formula that reconciles 2d, 3d, and variable polynomial degree and takes all factors in account reads as follows: 

[1.x.976] 

In the first form (in the center of the equation),  [2.x.2698]  is a universal constant,  [2.x.2699]  is the factor that accounts for the difference between cell diameter and grid point separation,  [2.x.2700]  accounts for the increase in  [2.x.2701]  with space dimension,  [2.x.2702]  accounts for the distance between grid points for higher order elements, and  [2.x.2703]  for the local speed of transport relative to the cell size. This is the formula that we use in the program. 

As for the question of whether to use  [2.x.2704]  or  [2.x.2705]  elements for the temperature, the following considerations may be useful: First, solving the temperature equation is hardly a factor in the overall scheme since almost the entire compute time goes into solving the Stokes system in each time step. Higher order elements for the temperature equation are therefore not a significant drawback. On the other hand, if one compares the size of the over- and undershoots the solution produces due to the discontinuous source description, one notices that for the choice of  [2.x.2706]  and  [2.x.2707]  as above, the  [2.x.2708]  solution dips down to around  [2.x.2709] , whereas the  [2.x.2710]  solution only goes to  [2.x.2711]  (remember that the exact solution should never become negative at all. This means that the  [2.x.2712]  solution is significantly more accurate; the program therefore uses these higher order elements, despite the penalty we pay in terms of smaller time steps. 




[1.x.977] 

There are various ways to extend the current program. Of particular interest is, of course, to make it faster and/or increase the resolution of the program, in particular in 3d. This is the topic of the step-32 tutorial program which will implement strategies to solve this problem in %parallel on a cluster. It is also the basis of the much larger open source code ASPECT (see https://aspect.geodynamics.org/ ) that can solve realistic problems and that constitutes the further development of step-32. 

Another direction would be to make the fluid flow more realistic. The program was initially written to simulate various cases simulating the convection of material in the earth's mantle, i.e. the zone between the outer earth core and the solid earth crust: there, material is heated from below and cooled from above, leading to thermal convection. The physics of this fluid are much more complicated than shown in this program, however: The viscosity of mantle material is strongly dependent on the temperature, i.e.  [2.x.2713] , with the dependency frequently modeled as a viscosity that is reduced exponentially with rising temperature. Secondly, much of the dynamics of the mantle is determined by chemical reactions, primarily phase changes of the various crystals that make up the mantle; the buoyancy term on the right hand side of the Stokes equations then depends not only on the temperature, but also on the chemical composition at a given location which is advected by the flow field but also changes as a function of pressure and temperature. We will investigate some of these effects in later tutorial programs as well. 


examples/step-32/doc/intro.dox 

 [2.x.2714]  

[1.x.978][1.x.979][1.x.980][1.x.981][1.x.982] 


[1.x.983] 

[1.x.984] 

This program does pretty much exactly what step-31 already does: it solves the Boussinesq equations that describe the motion of a fluid whose temperature is not in equilibrium. As such, all the equations we have described in step-31 still hold: we solve the same general partial differential equation (with only minor modifications to adjust for more realism in the problem setting), using the same finite element scheme, the same time stepping algorithm, and more or less the same stabilization method for the temperature advection-diffusion equation. As a consequence, you may first want to understand that program &mdash; and its implementation &mdash; before you work on the current one. 

The difference between step-31 and the current program is that here we want to do things in %parallel, using both the availability of many machines in a cluster (with parallelization based on MPI) as well as many processor cores within a single machine (with parallelization based on threads). This program's main job is therefore to introduce the changes that are necessary to utilize the availability of these %parallel compute resources. In this regard, it builds on the step-40 program that first introduces the necessary classes for much of the %parallel functionality, and on step-55 that shows how this is done for a vector-valued problem. 

In addition to these changes, we also use a slightly different preconditioner, and we will have to make a number of changes that have to do with the fact that we want to solve a [1.x.985] problem here, not a model problem. The latter, in particular, will require that we think about scaling issues as well as what all those parameters and coefficients in the equations under consideration actually mean. We will discuss first the issues that affect changes in the mathematical formulation and solver structure, then how to parallelize things, and finally the actual testcase we will consider. 




[1.x.986] 

In step-31, we used the following Stokes model for the velocity and pressure field: 

[1.x.987] 

The right hand side of the first equation appears a wee bit unmotivated. Here's how things should really be. We need the external forces that act on the fluid, which we assume are given by gravity only. In the current case, we assume that the fluid does expand slightly for the purposes of this gravity force, but not enough that we need to modify the incompressibility condition (the second equation). What this means is that for the purpose of the right hand side, we can assume that  [2.x.2715] . An assumption that may not be entirely justified is that we can assume that the changes of density as a function of temperature are small, leading to an expression of the form  [2.x.2716] , i.e., the density equals  [2.x.2717]  at reference temperature and decreases linearly as the temperature increases (as the material expands). The force balance equation then looks properly written like this: 

[1.x.988] 

Now note that the gravity force results from a gravity potential as  [2.x.2718] , so that we can re-write this as follows: 

[1.x.989] 

The second term on the right is time independent, and so we could introduce a new "dynamic" pressure  [2.x.2719]  with which the Stokes equations would read: 

[1.x.990] 

This is exactly the form we used in step-31, and it was appropriate to do so because all changes in the fluid flow are only driven by the dynamic pressure that results from temperature differences. (In other words: Any contribution to the right hand side that results from taking the gradient of a scalar field have no effect on the velocity field.) 

On the other hand, we will here use the form of the Stokes equations that considers the total pressure instead: 

[1.x.991] 

There are several advantages to this: 

- This way we can plot the pressure in our program in such a way that it   actually shows the total pressure that includes the effects of   temperature differences as well as the static pressure of the   overlying rocks. Since the pressure does not appear any further in any   of the other equations, whether to use one or the other is more a   matter of taste than of correctness. The flow field is exactly the   same, but we get a pressure that we can now compare with values that   are given in geophysical books as those that hold at the bottom of the   earth mantle, for example. 

- If we wanted to make the model even more realistic, we would have to take   into account that many of the material parameters (e.g. the viscosity, the   density, etc) not only depend on the temperature but also the   [1.x.992] pressure. 

- The model above assumed a linear dependence  [2.x.2720]  and assumed that  [2.x.2721]  is small. In   practice, this may not be so. In fact, realistic models are   certainly not linear, and  [2.x.2722]  may also not be small for at least   part of the temperature range because the density's behavior is   substantially dependent not only on thermal expansion but by phase   changes. 

- A final reason to do this is discussed in the results section and   concerns possible extensions to the model we use here. It has to do   with the fact that the temperature equation (see below) we use here does not   include a term that contains the pressure. It should, however:   rock, like gas, heats up as you compress it. Consequently,   material that rises up cools adiabatically, and cold material that   sinks down heats adiabatically. We discuss this further below. 

 [2.x.2723]  There is, however, a downside to this procedure. In the earth, the dynamic pressure is several orders of magnitude smaller than the total pressure. If we use the equations above and solve all variables to, say, 4 digits of accuracy, then we may be able to get the velocity and the total pressure right, but we will have no accuracy at all if we compute the dynamic pressure by subtracting from the total pressure the static part  [2.x.2724] . If, for example, the dynamic pressure is six orders of magnitude smaller than the static pressure, then we need to solve the overall pressure to at least seven digits of accuracy to get anything remotely accurate. That said, in practice this turns out not to be a limiting factor. 




[1.x.993] 

Remember that we want to solve the following set of equations: 

[1.x.994] 

augmented by appropriate boundary and initial conditions. As discussed in step-31, we will solve this set of equations by solving for a Stokes problem first in each time step, and then moving the temperature equation forward by one time interval. 

The problem under consideration in this current section is with the Stokes problem: if we discretize it as usual, we get a linear system 

[1.x.995] 

which in this program we will solve with a FGMRES solver. This solver iterates until the residual of these linear equations is below a certain tolerance, i.e., until 

[1.x.996] 

This does not make any sense from the viewpoint of physical units: the quantities involved here have physical units so that the first part of the residual has units  [2.x.2725]  (most easily established by considering the term  [2.x.2726]  and considering that the pressure has units  [2.x.2727]  and the integration yields a factor of  [2.x.2728] ), whereas the second part of the residual has units  [2.x.2729] . Taking the norm of this residual vector would yield a quantity with units  [2.x.2730] . This, quite obviously, does not make sense, and we should not be surprised that doing so is eventually going to come back hurting us. 

So why is this an issue here, but not in step-31? The reason back there is that everything was nicely balanced: velocities were on the order of one, the pressure likewise, the viscosity was one, and the domain had a diameter of  [2.x.2731] . As a result, while nonsensical, nothing bad happened. On the other hand, as we will explain below, things here will not be that simply scaled:  [2.x.2732]  will be around  [2.x.2733] , velocities on the order of  [2.x.2734] , pressure around  [2.x.2735] , and the diameter of the domain is  [2.x.2736] . In other words, the order of magnitude for the first equation is going to be  [2.x.2737] , whereas the second equation will be around  [2.x.2738] . Well, so what this will lead to is this: if the solver wants to make the residual small, it will almost entirely focus on the first set of equations because they are so much bigger, and ignore the divergence equation that describes mass conservation. That's exactly what happens: unless we set the tolerance to extremely small values, the resulting flow field is definitely not divergence free. As an auxiliary problem, it turns out that it is difficult to find a tolerance that always works; in practice, one often ends up with a tolerance that requires 30 or 40 iterations for most time steps, and 10,000 for some others. 

So what's a numerical analyst to do in a case like this? The answer is to start at the root and first make sure that everything is mathematically consistent first. In our case, this means that if we want to solve the system of Stokes equations jointly, we have to scale them so that they all have the same physical dimensions. In our case, this means multiplying the second equation by something that has units  [2.x.2739] ; one choice is to multiply with  [2.x.2740]  where  [2.x.2741]  is a typical lengthscale in our domain (which experiments show is best chosen to be the diameter of plumes &mdash; around 10 km &mdash; rather than the diameter of the domain). Using these %numbers for  [2.x.2742]  and  [2.x.2743] , this factor is around  [2.x.2744] . So, we now get this for the Stokes system: 

[1.x.997] 

The trouble with this is that the result is not symmetric any more (we have  [2.x.2745]  at the bottom left, but not its transpose operator at the top right). This, however, can be cured by introducing a scaled pressure  [2.x.2746] , and we get the scaled equations 

[1.x.998] 

This is now symmetric. Obviously, we can easily recover the original pressure  [2.x.2747]  from the scaled pressure  [2.x.2748]  that we compute as a result of this procedure. 

In the program below, we will introduce a factor  [2.x.2749]  that corresponds to  [2.x.2750] , and we will use this factor in the assembly of the system matrix and preconditioner. Because it is annoying and error prone, we will recover the unscaled pressure immediately following the solution of the linear system, i.e., the solution vector's pressure component will immediately be unscaled to retrieve the physical pressure. Since the solver uses the fact that we can use a good initial guess by extrapolating the previous solutions, we also have to scale the pressure immediately [1.x.999] solving. 




[1.x.1000] 

In this tutorial program, we apply a variant of the preconditioner used in step-31. That preconditioner was built to operate on the system matrix  [2.x.2751]  in block form such that the product matrix 

[1.x.1001] 

is of a form that Krylov-based iterative solvers like GMRES can solve in a few iterations. We then replaced the exact inverse of  [2.x.2752]  by the action of an AMG preconditioner  [2.x.2753]  based on a vector Laplace matrix, approximated the Schur complement  [2.x.2754]  by a mass matrix  [2.x.2755]  on the pressure space and wrote an <tt>InverseMatrix</tt> class for implementing the action of  [2.x.2756]  on vectors. In the InverseMatrix class, we used a CG solve with an incomplete Cholesky (IC) preconditioner for performing the inner solves. 

An observation one can make is that we use just the action of a preconditioner for approximating the velocity inverse  [2.x.2757]  (and the outer GMRES iteration takes care of the approximate character of the inverse), whereas we use a more or less [1.x.1002] inverse for  [2.x.2758] , realized by a fully converged CG solve. This appears unbalanced, but there's system to this madness: almost all the effort goes into the upper left block to which we apply the AMG preconditioner, whereas even an exact inversion of the pressure mass matrix costs basically nothing. Consequently, if it helps us reduce the overall number of iterations somewhat, then this effort is well spent. 

That said, even though the solver worked well for step-31, we have a problem here that is a bit more complicated (cells are deformed, the pressure varies by orders of magnitude, and we want to plan ahead for more complicated physics), and so we'll change a few things slightly: 

- For more complex problems, it turns out that using just a single AMG V-cycle   as preconditioner is not always sufficient. The outer solver converges just   fine most of the time in a reasonable number of iterations (say, less than   50) but there are the occasional time step where it suddenly takes 700 or   so. What exactly is going on there is hard to determine, but the problem can   be avoided by using a more accurate solver for the top left   block. Consequently, we'll want to use a CG iteration to invert the top left   block of the preconditioner matrix, and use the AMG as a preconditioner for   the CG solver. 

- The downside of this is that, of course, the Stokes preconditioner becomes   much more expensive (approximately 10 times more expensive than when we just   use a single V-cycle). Our strategy then is this: let's do up to 30 GMRES   iterations with just the V-cycle as a preconditioner and if that doesn't   yield convergence, then take the best approximation of the Stokes solution   obtained after this first round of iterations and use that as the starting   guess for iterations where we use the full inner solver with a rather   lenient tolerance as preconditioner. In all our experiments this leads to   convergence in only a few additional iterations. 

- One thing we need to pay attention to is that when using a CG with a lenient   tolerance in the preconditioner, then  [2.x.2759]  is no longer a   linear function of  [2.x.2760]  (it is, of course, if we have a very stringent   tolerance in our solver, or if we only apply a single V-cycle). This is a   problem since now our preconditioner is no longer a linear operator; in   other words, every time GMRES uses it the preconditioner looks   different. The standard GMRES solver can't deal with this, leading to slow   convergence or even breakdown, but the F-GMRES variant is designed to deal   with exactly this kind of situation and we consequently use it. 

- On the other hand, once we have settled on using F-GMRES we can relax the   tolerance used in inverting the preconditioner for  [2.x.2761] . In step-31, we ran a   preconditioned CG method on  [2.x.2762]  until the residual had been reduced   by 7 orders of magnitude. Here, we can again be more lenient because we know   that the outer preconditioner doesn't suffer. 

- In step-31, we used a left preconditioner in which we first invert the top   left block of the preconditioner matrix, then apply the bottom left   (divergence) one, and then invert the bottom right. In other words, the   application of the preconditioner acts as a lower left block triangular   matrix. Another option is to use a right preconditioner that here would be   upper right block triangulation, i.e., we first invert the bottom right   Schur complement, apply the top right (gradient) operator and then invert   the elliptic top left block. To a degree, which one to choose is a matter of   taste. That said, there is one significant advantage to a right   preconditioner in GMRES-type solvers: the residual with which we determine   whether we should stop the iteration is the true residual, not the norm of   the preconditioned equations. Consequently, it is much simpler to compare it   to the stopping criterion we typically use, namely the norm of the right   hand side vector. In writing this code we found that the scaling issues we   discussed above also made it difficult to determine suitable stopping   criteria for left-preconditioned linear systems, and consequently this   program uses a right preconditioner. 

- In step-31, we used an IC (incomplete Cholesky) preconditioner for the   pressure mass matrix in the Schur complement preconditioner and for the   solution of the temperature system. Here, we could in principle do the same,   but we do choose an even simpler preconditioner, namely a Jacobi   preconditioner for both systems. This is because here we target at massively   %parallel computations, where the decompositions for IC/ILU would have to be   performed block-wise for the locally owned degrees of freedom on each   processor. This means, that the preconditioner gets more like a Jacobi   preconditioner anyway, so we rather start from that variant straight   away. Note that we only use the Jacobi preconditioners for CG solvers with   mass matrices, where they give optimal ([1.x.1003]-independent) convergence   anyway, even though they usually require about twice as many iterations as   an IC preconditioner. 

As a final note, let us remark that in step-31 we computed the Schur complement  [2.x.2763]  by approximating  [2.x.2764] . Now, however, we have re-scaled the  [2.x.2765]  and  [2.x.2766]  operators. So  [2.x.2767]  should now approximate  [2.x.2768] . We use the discrete form of the right hand side of this as our approximation  [2.x.2769]  to  [2.x.2770] . 




[1.x.1004] 

Similarly to step-31, we will use an artificial viscosity for stabilization based on a residual of the equation.  As a difference to step-31, we will provide two slightly different definitions of the stabilization parameter. For  [2.x.2771] , we use the same definition as in step-31: 

[1.x.1005] 

where we compute the viscosity from a residual  [2.x.2772]  of the equation, limited by a diffusion proportional to the mesh size  [2.x.2773]  in regions where the residual is large (around steep gradients). This definition has been shown to work well for the given case,  [2.x.2774]  in step-31, but it is usually less effective as the diffusion for  [2.x.2775] . For that case, we choose a slightly more readable definition of the viscosity, 

[1.x.1006] 

where the first term gives again the maximum dissipation (similarly to a first order upwind scheme), 

[1.x.1007] 

and the entropy viscosity is defined as 

[1.x.1008] 



This formula is described in the article [1.x.1009] Compared to the case  [2.x.2776] , the residual is computed from the temperature entropy,  [2.x.2777]  with  [2.x.2778]  an average temperature (we choose the mean between the maximum and minimum temperature in the computation), which gives the following formula 

[1.x.1010] 

The denominator in the formula for  [2.x.2779]  is computed as the global deviation of the entropy from the space-averaged entropy  [2.x.2780] . As in step-31, we evaluate the artificial viscosity from the temperature and velocity at two previous time levels, in order to avoid a nonlinearity in its definition. 

The above definitions of the viscosity are simple, but depend on two parameters, namely  [2.x.2781]  and  [2.x.2782] .  For the current program, we want to go about this issue a bit more systematically for both parameters in the case  [2.x.2783] , using the same line of reasoning with which we chose two other parameters in our discretization,  [2.x.2784]  and  [2.x.2785] , in the results section of step-31. In particular, remember that we would like to make the artificial viscosity as small as possible while keeping it as large as necessary. In the following, let us describe the general strategy one may follow. The computations shown here were done with an earlier version of the program and so the actual numerical values you get when running the program may no longer match those shown here; that said, the general approach remains valid and has been used to find the values of the parameters actually used in the program. 

To see what is happening, note that below we will impose boundary conditions for the temperature between 973 and 4273 Kelvin, and initial conditions are also chosen in this range; for these considerations, we run the program without %internal heat sources or sinks, and consequently the temperature should always be in this range, barring any %internal oscillations. If the minimal temperature drops below 973 Kelvin, then we need to add stabilization by either increasing  [2.x.2786]  or decreasing  [2.x.2787] . 

As we did in step-31, we first determine an optimal value of  [2.x.2788]  by using the "traditional" formula 

[1.x.1011] 

which we know to be stable if only  [2.x.2789]  is large enough. Doing a couple hundred time steps (on a coarser mesh than the one shown in the program, and with a different viscosity that affects transport velocities and therefore time step sizes) in 2d will produce the following graph: 

 [2.x.2790]  

As can be seen, values  [2.x.2791]  are too small whereas  [2.x.2792]  appears to work, at least to the time horizon shown here. As a remark on the side, there are at least two questions one may wonder here: First, what happens at the time when the solution becomes unstable? Looking at the graphical output, we can see that with the unreasonably coarse mesh chosen for these experiments, around time  [2.x.2793]  seconds the plumes of hot material that have been rising towards the cold outer boundary and have then spread sideways are starting to get close to each other, squeezing out the cold material in-between. This creates a layer of cells into which fluids flows from two opposite sides and flows out toward a third, apparently a scenario that then produce these instabilities without sufficient stabilization. Second: In step-31, we used  [2.x.2794] ; why does this not work here? The answer to this is not entirely clear -- stabilization parameters are certainly known to depend on things like the shape of cells, for which we had squares in step-31 but have trapezoids in the current program. Whatever the exact cause, we at least have a value of  [2.x.2795] , namely 0.052 for 2d, that works for the current program. A similar set of experiments can be made in 3d where we find that  [2.x.2796]  is a good choice &mdash; neatly leading to the formula  [2.x.2797] . 

With this value fixed, we can go back to the original formula for the viscosity  [2.x.2798]  and play with the constant  [2.x.2799] , making it as large as possible in order to make  [2.x.2800]  as small as possible. This gives us a picture like this: 

 [2.x.2801]  

Consequently,  [2.x.2802]  would appear to be the right value here. While this graph has been obtained for an exponent  [2.x.2803] , in the program we use  [2.x.2804]  instead, and in that case one has to re-tune the parameter (and observe that  [2.x.2805]  appears in the numerator and not in the denominator). It turns out that  [2.x.2806]  works with  [2.x.2807] . 




[1.x.1012] 

The standard Taylor-Hood discretization for Stokes, using the  [2.x.2808]  element, is globally conservative, i.e.  [2.x.2809] . This can easily be seen: the weak form of the divergence equation reads  [2.x.2810] . Because the pressure space does contain the function  [2.x.2811] , we get 

[1.x.1013] 

by the divergence theorem. This property is important: if we want to use the velocity field  [2.x.2812]  to transport along other quantities (such as the temperature in the current equations, but it could also be concentrations of chemical substances or entirely artificial tracer quantities) then the conservation property guarantees that the amount of the quantity advected remains constant. 

That said, there are applications where this [1.x.1014] property is not enough. Rather, we would like that it holds [1.x.1015], on every cell. This can be achieved by using the space  [2.x.2813]  for discretization, where we have replaced the [1.x.1016] space of tensor product polynomials of degree  [2.x.2814]  for the pressure by the [1.x.1017] space of the complete polynomials of the same degree. (Note that tensor product polynomials in 2d contain the functions  [2.x.2815] , whereas the complete polynomials only have the functions  [2.x.2816] .) This space turns out to be stable for the Stokes equation. 

Because the space is discontinuous, we can now in particular choose the test function  [2.x.2817] , i.e. the characteristic function of cell  [2.x.2818] . We then get in a similar fashion as above 

[1.x.1018] 

showing the conservation property for cell  [2.x.2819] . This clearly holds for each cell individually. 

There are good reasons to use this discretization. As mentioned above, this element guarantees conservation of advected quantities on each cell individually. A second advantage is that the pressure mass matrix we use as a preconditioner in place of the Schur complement becomes block diagonal and consequently very easy to invert. However, there are also downsides. For one, there are now more pressure variables, increasing the overall size of the problem, although this doesn't seem to cause much harm in practice. More importantly, though, the fact that now the divergence integrated over each cell is zero when it wasn't before does not guarantee that the divergence is pointwise smaller. In fact, as one can easily verify, the  [2.x.2820]  norm of the divergence is [1.x.1019] for this than for the standard Taylor-Hood discretization. (However, both converge at the same rate to zero, since it is easy to see that  [2.x.2821] .) It is therefore not a priori clear that the error is indeed smaller just because we now have more degrees of freedom. 

Given these considerations, it remains unclear which discretization one should prefer. Consequently, we leave that up to the user and make it a parameter in the input file which one to use. 




[1.x.1020] 

In the program, we will use a spherical shell as domain. This means that the inner and outer boundary of the domain are no longer "straight" (by which we usually mean that they are bilinear surfaces that can be represented by the FlatManifold class). Rather, they are curved and it seems prudent to use a curved approximation in the program if we are already using higher order finite elements for the velocity. Consequently, we will introduce a member variable of type MappingQ that denotes such a mapping (step-10 and step-11 introduce such mappings for the first time) and that we will use in all computations on cells that are adjacent to the boundary. Since this only affects a relatively small fraction of cells, the additional effort is not very large and we will take the luxury of using a quartic mapping for these cells. 




[1.x.1021] 

Running convection codes in 3d with significant Rayleigh numbers requires a lot of computations &mdash; in the case of whole earth simulations on the order of one or several hundred million unknowns. This can obviously not be done with a single machine any more (at least not in 2010 when we started writing this code). Consequently, we need to parallelize it. Parallelization of scientific codes across multiple machines in a cluster of computers is almost always done using the Message Passing Interface (MPI). This program is no exception to that, and it follows the general spirit of the step-17 and step-18 programs in this though in practice it borrows more from step-40 in which we first introduced the classes and strategies we use when we want to [1.x.1022] distribute all computations, and step-55 that shows how to do that for  [2.x.2822]  "vector-valued problems": including, for example, splitting the mesh up into a number of parts so that each processor only stores its own share plus some ghost cells, and using strategies where no processor potentially has enough memory to hold the entries of the combined solution vector locally. The goal is to run this code on hundreds or maybe even thousands of processors, at reasonable scalability. 

 [2.x.2823]  Even though it has a larger number, step-40 comes logically before the current program. The same is true for step-55. You will probably want to look at these programs before you try to understand what we do here. 

MPI is a rather awkward interface to program with. It is a semi-object oriented set of functions, and while one uses it to send data around a network, one needs to explicitly describe the data types because the MPI functions insist on getting the address of the data as  [2.x.2824]  objects rather than deducing the data type automatically through overloading or templates. We've already seen in step-17 and step-18 how to avoid almost all of MPI by putting all the communication necessary into either the deal.II library or, in those programs, into PETSc. We'll do something similar here: like in step-40 and step-55, deal.II and the underlying p4est library are responsible for all the communication necessary for distributing the mesh, and we will let the Trilinos library (along with the wrappers in namespace TrilinosWrappers) deal with parallelizing the linear algebra components. We have already used Trilinos in step-31, and will do so again here, with the difference that we will use its %parallel capabilities. 

Trilinos consists of a significant number of packages, implementing basic %parallel linear algebra operations (the Epetra package), different solver and preconditioner packages, and on to things that are of less importance to deal.II (e.g., optimization, uncertainty quantification, etc). deal.II's Trilinos interfaces encapsulate many of the things Trilinos offers that are of relevance to PDE solvers, and provides wrapper classes (in namespace TrilinosWrappers) that make the Trilinos matrix, vector, solver and preconditioner classes look very much the same as deal.II's own implementations of this functionality. However, as opposed to deal.II's classes, they can be used in %parallel if we give them the necessary information. As a consequence, there are two Trilinos classes that we have to deal with directly (rather than through wrappers), both of which are part of Trilinos' Epetra library of basic linear algebra and tool classes:  [2.x.2825]   [2.x.2826]  The Epetra_Comm class is an abstraction of an MPI "communicator", i.e.,   it describes how many and which machines can communicate with each other.   Each distributed object, such as a sparse matrix or a vector for which we   may want to store parts on different machines, needs to have a communicator   object to know how many parts there are, where they can be found, and how   they can be accessed. 

  In this program, we only really use one communicator object -- based on the   MPI variable  [2.x.2827]  -- that encompasses [1.x.1023]   processes that work together. It would be perfectly legitimate to start a   process on  [2.x.2828]  machines but only store vectors on a subset of these by   producing a communicator object that only encompasses this subset of   machines; there is really no compelling reason to do so here, however. 

 [2.x.2829]  The IndexSet class is used to describe which elements of a vector or which   rows of a matrix should reside on the current machine that is part of a   communicator. To create such an object, you need to know (i) the total   number of elements or rows, (ii) the indices of the elements you want to   store locally. We will set up these  [2.x.2830]  in the    [2.x.2831]  function below and then hand   it to every %parallel object we create. 

  Unlike PETSc, Trilinos makes no assumption that the elements of a vector   need to be partitioned into contiguous chunks. At least in principle, we   could store all elements with even indices on one processor and all odd ones   on another. That's not very efficient, of course, but it's   possible. Furthermore, the elements of these partitionings do not   necessarily be mutually exclusive. This is important because when   postprocessing solutions, we need access to all locally relevant or at least   the locally active degrees of freedom (see the module on  [2.x.2832]    for a definition, as well as the discussion in step-40). Which elements the   Trilinos vector considers as locally owned is not important to us then. All   we care about is that it stores those elements locally that we need.  [2.x.2833]  

There are a number of other concepts relevant to distributing the mesh to a number of processors; you may want to take a look at the  [2.x.2834]  module and step-40 or step-55 before trying to understand this program.  The rest of the program is almost completely agnostic about the fact that we don't store all objects completely locally. There will be a few points where we have to limit loops over all cells to those that are locally owned, or where we need to distinguish between vectors that store only locally owned elements and those that store everything that is locally relevant (see  [2.x.2835]  "this glossary entry"), but by and large the amount of heavy lifting necessary to make the program run in %parallel is well hidden in the libraries upon which this program builds. In any case, we will comment on these locations as we get to them in the program code. 




[1.x.1024] 

The second strategy to parallelize a program is to make use of the fact that most computers today have more than one processor that all have access to the same memory. In other words, in this model, we don't explicitly have to say which pieces of data reside where -- all of the data we need is directly accessible and all we have to do is split [1.x.1025] this data between the available processors. We will then couple this with the MPI parallelization outlined above, i.e., we will have all the processors on a machine work together to, for example, assemble the local contributions to the global matrix for the cells that this machine actually "owns" but not for those cells that are owned by other machines. We will use this strategy for four kinds of operations we frequently do in this program: assembly of the Stokes and temperature matrices, assembly of the matrix that forms the Stokes preconditioner, and assembly of the right hand side of the temperature system. 

All of these operations essentially look as follows: we need to loop over all cells for which  [2.x.2836]  equals the index our machine has within the communicator object used for all communication (i.e.,  [2.x.2837] , as explained above). The test we are actually going to use for this, and which describes in a concise way why we test this condition, is  [2.x.2838] . On each such cell we need to assemble the local contributions to the global matrix or vector, and then we have to copy each cell's contribution into the global matrix or vector. Note that the first part of this (the loop) defines a range of iterators on which something has to happen. The second part, assembly of local contributions is something that takes the majority of CPU time in this sequence of steps, and is a typical example of things that can be done in %parallel: each cell's contribution is entirely independent of all other cells' contributions. The third part, copying into the global matrix, must not happen in %parallel since we are modifying one object and so several threads can not at the same time read an existing matrix element, add their contribution, and write the sum back into memory without danger of producing a [1.x.1026]. 

deal.II has a class that is made for exactly this workflow: WorkStream, first discussed in step-9 and step-13. Its use is also extensively documented in the module on  [2.x.2839]  (in the section on  [2.x.2840]  "the WorkStream class") and we won't repeat here the rationale and detailed instructions laid out there, though you will want to read through this module to understand the distinction between scratch space and per-cell data. Suffice it to mention that we need the following: 

- An iterator range for those cells on which we are supposed to work. This is   provided by the FilteredIterator class which acts just like every other cell   iterator in deal.II with the exception that it skips all cells that do not   satisfy a particular predicate (i.e., a criterion that evaluates to true or   false). In our case, the predicate is whether a cell is locally owned. 

- A function that does the work on each cell for each of the tasks identified   above, i.e., functions that assemble the local contributions to Stokes matrix   and preconditioner, temperature matrix, and temperature right hand   side. These are the    [2.x.2841] ,    [2.x.2842] ,    [2.x.2843] , and    [2.x.2844]  functions in   the code below. These four functions can all have several instances   running in %parallel at the same time. 

- %Functions that copy the result of the previous ones into the global object   and that run sequentially to avoid race conditions. These are the    [2.x.2845] ,    [2.x.2846] ,    [2.x.2847] , and    [2.x.2848]    functions. 

We will comment on a few more points in the actual code, but in general their structure should be clear from the discussion in  [2.x.2849] . 

The underlying technology for WorkStream identifies "tasks" that need to be worked on (e.g. assembling local contributions on a cell) and schedules these tasks automatically to available processors. WorkStream creates these tasks automatically, by splitting the iterator range into suitable chunks. 

 [2.x.2850]  Using multiple threads within each MPI process only makes sense if you have fewer MPI processes running on each node of your cluster than there are processor cores on this machine. Otherwise, MPI will already keep your processors busy and you won't get any additional speedup from using threads. For example, if your cluster nodes have 8 cores as they often have at the time of writing this, and if your batch scheduler puts 8 MPI processes on each node, then using threads doesn't make the program any faster. Consequently, you probably want to either configure your deal.II without threads, or set the number of threads in  [2.x.2851]  to 1 (third argument), or "export DEAL_II_NUM_THREADS=1" before running. That said, at the time of writing this, we only use the WorkStream class for assembling (parts of) linear systems, while 75% or more of the run time of the program is spent in the linear solvers that are not parallelized &mdash; in other words, the best we could hope is to parallelize the remaining 25%. 




[1.x.1027] 

The setup for this program is mildly reminiscent of the problem we wanted to solve in the first place (see the introduction of step-31): convection in the earth mantle. As a consequence, we choose the following data, all of which appears in the program in units of meters and seconds (the SI system) even if we list them here in other units. We do note, however, that these choices are essentially still only exemplary, and not meant to result in a completely realistic description of convection in the earth mantle: for that, more and more difficult physics would have to be implemented, and several other aspects are currently missing from this program as well. We will come back to this issue in the results section again, but state for now that providing a realistic description is a goal of the [1.x.1028] code in development at the time of writing this. 

As a reminder, let us again state the equations we want to solve are these: 

[1.x.1029] 

augmented by boundary and initial conditions. We then have to choose data for the following quantities:  [2.x.2852]     [2.x.2853] The domain is an annulus (in 2d) or a spherical shell (in 3d) with inner   and outer radii that match that of the earth: the total radius of the earth   is 6371km, with the mantle starting at a depth of around 35km (just under   the solid earth [1.x.1030] composed of   [1.x.1031] and [1.x.1032]) to a depth of 2890km (where the   [1.x.1033] starts). The radii are therefore  [2.x.2854] . This domain is conveniently generated using the    [2.x.2855]  function. 

   [2.x.2856] At the interface between crust and mantle, the temperature is between   500 and 900 degrees Celsius, whereas at its bottom it is around 4000 degrees   Celsius (see, for example, [1.x.1034]). In Kelvin, we therefore choose  [2.x.2857] ,    [2.x.2858]  as boundary conditions at the inner and outer edge. 

  In addition to this, we also have to specify some initial conditions for   the temperature field. The real temperature field of the earth is quite   complicated as a consequence of the convection that has been going on for   more than four billion years -- in fact, it is the properties of this   temperature distribution that we want to explore with programs like   this. As a consequence, we   don't really have anything useful to offer here, but we can hope that if we   start with something and let things run for a while that the exact initial   conditions don't matter that much any more &mdash; as is in fact suggested   by looking at the pictures shown in the [1.x.1035]. The initial temperature field we use here is given in terms of   the radius by   [1.x.1036] 

  where   [1.x.1037] 

  This complicated function is essentially a perturbation of a linear profile   between the inner and outer temperatures. In 2d, the function    [2.x.2859]  looks like this (I got the picture from   [1.x.1038]): 

   [2.x.2860]  

  The point of this profile is that if we had used  [2.x.2861]  instead of  [2.x.2862]  in   the definition of  [2.x.2863]  then it would simply be a linear   interpolation.  [2.x.2864]  has the same function values as  [2.x.2865]  on the inner and   outer boundaries (zero and one, respectively), but it stretches the   temperature profile a bit depending on the angle and the  [2.x.2866]  value in 3d,   producing an angle-dependent perturbation of the linearly interpolating   field. We will see in the results section that this is an   entirely unphysical temperature field (though it will make for   interesting images) as the equilibrium state for the temperature   will be an almost constant temperature with boundary layers at the   inner and outer boundary. 

   [2.x.2867] The right hand side of the temperature equation contains the rate of   %internal heating  [2.x.2868] . The earth does heat naturally through several mechanisms:   radioactive decay, chemical separation (heavier elements sink to the bottom,   lighter ones rise to the top; the countercurrents dissipate energy equal to   the loss of potential energy by this separation process); heat release   by crystallization of liquid metal as the solid inner core of the earth   grows; and heat dissipation from viscous friction as the fluid moves. 

  Chemical separation is difficult to model since it requires modeling mantle   material as multiple phases; it is also a relatively small   effect. Crystallization heat is even more difficult since it is confined to   areas where temperature and pressure allow for phase changes, i.e., a   discontinuous process. Given the difficulties in modeling these two   phenomena, we will neglect them. 

  The other two are readily handled and, given the way we scaled the   temperature equation, lead to the equation   [1.x.1039] 

  where  [2.x.2869]  is the radiogenic heating in  [2.x.2870] , and the second   term in the enumerator is viscous friction heating.  [2.x.2871]  is the density   and  [2.x.2872]  is the specific heat. The literature provides the following   approximate values:  [2.x.2873] .   The other parameters are discussed elsewhere in this section. 

  We neglect one internal heat source, namely adiabatic heating here,   which will lead to a surprising temperature field. This point is   commented on in detail in the results section below. 

   [2.x.2874] For the velocity we choose as boundary conditions  [2.x.2875]  at the   inner radius (i.e., the fluid sticks to the earth core) and    [2.x.2876]  at the outer radius (i.e., the fluid flows   tangentially along the bottom of the earth crust). Neither of these is   physically overly correct: certainly, on both boundaries, fluids can flow   tangentially, but they will incur a shear stress through friction against   the medium at the other side of the interface (the metallic core and the   crust, respectively). Such a situation could be modeled by a Robin-type   boundary condition for the tangential velocity; in either case, the normal (vertical)   velocity would be zero, although even that is not entirely correct since   continental plates also have vertical motion (see, for example, the   phenomenon of [1.x.1040]). But to already make things worse for the tangential velocity,   the medium on the other side is in motion as well, so the shear stress   would, in the simplest case, be proportional to the [1.x.1041], leading to a boundary condition of the form   [1.x.1042] 

  with a proportionality constant  [2.x.2877] . Rather than going down this route,   however, we go with the choice of zero (stick) and tangential   flow boundary conditions. 

  As a side note of interest, we may also have chosen tangential flow   conditions on both inner and outer boundary. That has a significant   drawback, however: it leaves the velocity not uniquely defined. The reason   is that all velocity fields  [2.x.2878]  that correspond to a solid   body rotation around the center of the domain satisfy  [2.x.2879] , and    [2.x.2880] . As a consequence, if  [2.x.2881]    satisfies equations and boundary conditions, then so does  [2.x.2882] . That's certainly not a good situation that we would like   to avoid. The traditional way to work around this is to pick an arbitrary   point on the boundary and call this your fixed point by choosing the   velocity to be zero in all components there. (In 3d one has to choose two   points.) Since this program isn't meant to be too realistic to begin with,   we avoid this complication by simply fixing the velocity along the entire   interior boundary. 

   [2.x.2883] To first order, the gravity vector always points downward. The question for   a body as big as the earth is just: where is "up". The naive answer of course is   "radially inward, towards the center of the earth". So at the surface of the   earth, we have   [1.x.1043] 

  where  [2.x.2884]  happens to be the average gravity   acceleration at the earth surface. But in the earth interior, the question   becomes a bit more complicated: at the (bary-)center of the earth, for   example, you have matter pulling equally hard in all directions, and so    [2.x.2885] . In between, the net force is described as follows: let us   define the [1.x.1044] by   [1.x.1045] 

  then  [2.x.2886] . If we assume that   the density  [2.x.2887]  is constant throughout the earth, we can produce an   analytical expression for the gravity vector (don't try to integrate above   equation somehow -- it leads to elliptic integrals; a simpler way is to   notice that  [2.x.2888]  and solving this   partial differential equation in all of  [2.x.2889]  exploiting the   radial symmetry):   [1.x.1046] 

  The factor  [2.x.2890]  is the unit vector pointing   radially inward. Of course, within this problem, we are only interested in   the branch that pertains to within the earth, i.e.,  [2.x.2891] . We would therefore only consider the expression   [1.x.1047] 

  where we can infer the last expression because we know Earth's gravity at   the surface (where  [2.x.2892] ). 

  One can derive a more general expression by integrating the   differential equation for  [2.x.2893]  in the case that the density   distribution is radially symmetric, i.e.,  [2.x.2894] . In that case, one would get   [1.x.1048] 




  There are two problems with this, however: (i) The Earth is not homogeneous,   i.e., the density  [2.x.2895]  depends on  [2.x.2896] ; in fact it is not even a   function that only depends on the radius  [2.x.2897] . In reality, gravity therefore   does not always decrease as we get deeper: because the earth core is so much   denser than the mantle, gravity actually peaks at around  [2.x.2898]  at the core mantle boundary (see [1.x.1049]). (ii) The density, and by   consequence the gravity vector, is not even constant in time: after all, the   problem we want to solve is the time dependent upwelling of hot, less dense   material and the downwelling of cold dense material. This leads to a gravity   vector that varies with space and time, and does not always point straight   down. 

  In order to not make the situation more complicated than necessary, we could   use the approximation that at the inner boundary of the mantle,   gravity is  [2.x.2899]  and at the outer   boundary it is  [2.x.2900] , in each case   pointing radially inward, and that in between gravity varies   linearly with the radial distance from the earth center. That said, it isn't   that hard to actually be slightly more realistic and assume (as we do below)   that the earth mantle has constant density. In that case, the equation above   can be integrated and we get an expression for  [2.x.2901]  where we   can fit constants to match the gravity at the top and bottom of the earth   mantle to obtain   [1.x.1050] 



   [2.x.2902] The density of the earth mantle varies spatially, but not by very   much.  [2.x.2903]  is a relatively good average   value for the density at reference temperature  [2.x.2904]  Kelvin. 

   [2.x.2905] The thermal expansion coefficient  [2.x.2906]  also varies with depth   (through its dependence on temperature and pressure). Close to the surface,   it appears to be on the order of  [2.x.2907] ,   whereas at the core mantle boundary, it may be closer to  [2.x.2908] . As a reasonable value, let us choose    [2.x.2909] . The density as a function   of temperature is then    [2.x.2910] . 

   [2.x.2911] The second to last parameter we need to specify is the viscosity    [2.x.2912] . This is a tough one, because rocks at the temperatures and pressure   typical for the earth mantle flow so slowly that the viscosity can not be   determined accurately in the laboratory. So how do we know about the   viscosity of the mantle? The most commonly used route is to consider that   during and after ice ages, ice shields form and disappear on time scales   that are shorter than the time scale of flow in the mantle. As a   consequence, continents slowly sink into the earth mantle under the added   weight of an ice shield, and they rise up again slowly after the ice shield   has disappeared again (this is called [1.x.1051][1.x.1052]). By measuring the speed of this rebound, we can infer the   viscosity of the material that flows into the area vacated under the   rebounding continental plates. 

  Using this technique, values around  [2.x.2913]  have been found as the most   likely, though the error bar on this is at least one order of magnitude. 

  While we will use this value, we again have to caution that there are many   physical reasons to assume that this is not the correct value. First, it   should really be made dependent on temperature: hotter material is most   likely to be less viscous than colder material. In reality, however, the   situation is even more complex. Most rocks in the mantle undergo phase   changes as temperature and pressure change: depending on temperature and   pressure, different crystal configurations are thermodynamically favored   over others, even if the chemical composition of the mantle were   homogeneous. For example, the common mantle material MgSiO<sub>3</sub> exists   in its [1.x.1053] throughout most of the mantle, but in the lower mantle the   same substance is stable only as [1.x.1054]. Clearly,   to compute realistic viscosities, we would not only need to know the exact   chemical composition of the mantle and the viscosities of all materials, but   we would also have to compute the thermodynamically most stable   configurations for all materials at each quadrature point. This is at the   time of writing this program not a feasible suggestion. 

   [2.x.2914] Our last material parameter is the thermal diffusivity  [2.x.2915] , which   is defined as  [2.x.2916]  where  [2.x.2917]  is the thermal   conductivity,  [2.x.2918]  the density, and  [2.x.2919]  the specific heat. For   this, the literature indicates that it increases from around  [2.x.2920]  in the   upper mantle to around  [2.x.2921]  in the lower   mantle, though the exact value   is not really all that important: heat transport through convection is   several orders of magnitude more important than through thermal   conduction. It may be of interest to know that perovskite, the most abundant   material in the earth mantle, appears to become transparent at pressures   above around 120 GPa (see, for example, J. Badro et al., Science 305,   383-386 (2004)); in the lower mantle, it may therefore be that heat   transport through radiative transfer is more efficient than through thermal   conduction. 

  In view of these considerations, let us choose    [2.x.2922]    for the purpose of this program.  [2.x.2923]  

All of these pieces of equation data are defined in the program in the  [2.x.2924]  namespace. When run, the program produces long-term maximal velocities around 10-40 centimeters per year (see the results section below), approximately the physically correct order of magnitude. We will set the end time to 1 billion years. 

 [2.x.2925]  The choice of the constants and material parameters above follows in large part the comprehensive book "Mantle Convection in the Earth and Planets, Part 1" by G. Schubert and D. L. Turcotte and P. Olson (Cambridge, 2001). It contains extensive discussion of ways to make the program more realistic. 




[1.x.1055] 

Compared to step-31, this program has a number of noteworthy differences: 

- The  [2.x.2926]  namespace is significantly larger, reflecting   the fact that we now have much more physics to deal with. That said, most of   this additional physical detail is rather self-contained in functions in   this one namespace, and does not proliferate throughout the rest of the   program. 

- Of more obvious visibility is the fact that we have put a good number of   parameters into an input file handled by the ParameterHandler class (see,   for example, step-29, for ways to set up run-time parameter files with this   class). This often makes sense when one wants to avoid re-compiling the   program just because one wants to play with a single parameter (think, for   example, of parameter studies determining the best values of the   stabilization constants discussed above), in particular given that it takes   a nontrivial amount of time to re-compile programs of the current size. To   just give an overview of the kinds of parameters we have moved from fixed   values into the input file, here is a listing of a typical    [2.x.2927]  file:   [1.x.1056] 



- There are, obviously, a good number of changes that have to do with the fact   that we want to run our program on a possibly very large number of   machines. Although one may suspect that this requires us to completely   re-structure our code, that isn't in fact the case (although the classes   that implement much of this functionality in deal.II certainly look very   different from an implementation viewpoint, but this doesn't reflect in   their public interface). Rather, the changes are mostly subtle, and the   overall structure of the main class is pretty much unchanged. That said, the   devil is in the detail: getting %parallel computing right, without   deadlocks, ensuring that the right data is available at the right place   (see, for example, the discussion on fully distributed vectors vs. vectors   with ghost elements), and avoiding bottlenecks is difficult and discussions   on this topic will appear in a good number of places in this program. 




[1.x.1057] 

This is a tutorial program. That means that at least most of its focus needs to lie on demonstrating ways of using deal.II and associated libraries, and not diluting this teaching lesson by focusing overly much on physical details. Despite the lengthy section above on the choice of physical parameters, the part of the program devoted to this is actually quite short and self contained. 

That said, both step-31 and the current step-32 have not come about by chance but are certainly meant as wayposts along the path to a more comprehensive program that will simulate convection in the earth mantle. We call this code [1.x.1058] (short for [1.x.1059]); its development is funded by the [1.x.1060] initiative with support from the National Science Foundation. More information on [1.x.1061] is available at its [1.x.1062]. 


examples/step-32/doc/results.dox 



[1.x.1063] 

When run, the program simulates convection in 3d in much the same way as step-31 did, though with an entirely different testcase. 




[1.x.1064] 

Before we go to this testcase, however, let us show a few results from a slightly earlier version of this program that was solving exactly the testcase we used in step-31, just that we now solve it in parallel and with much higher resolution. We show these results mainly for comparison. 

Here are two images that show this higher resolution if we choose a 3d computation in  [2.x.2928]  and if we set  [2.x.2929]  and  [2.x.2930] . At the time steps shown, the meshes had around 72,000 and 236,000 cells, for a total of 2,680,000 and 8,250,000 degrees of freedom, respectively, more than an order of magnitude more than we had available in step-31: 

 [2.x.2931]  

The computation was done on a subset of 50 processors of the Brazos cluster at Texas A&amp;M University. 




[1.x.1065] 

Next, we will run step-32 with the parameter file in the directory with one change: we increase the final time to 1e9. Here we are using 16 processors. The command to launch is (note that step-32.prm is the default): 

<code> <pre> \ [2.x.2932]  mpirun -np 16 ./step-32 Number of active cells: 12,288 (on 6 levels) Number of degrees of freedom: 186,624 (99,840+36,864+49,920) 

Timestep 0:  t=0 years 

   Rebuilding Stokes preconditioner...    Solving Stokes system... 41 iterations.    Maximal velocity: 60.4935 cm/year    Time step: 18166.9 years    17 CG iterations for temperature    Temperature range: 973 4273.16 

Number of active cells: 15,921 (on 7 levels) Number of degrees of freedom: 252,723 (136,640+47,763+68,320) 

Timestep 0:  t=0 years 

   Rebuilding Stokes preconditioner...    Solving Stokes system... 50 iterations.    Maximal velocity: 60.3223 cm/year    Time step: 10557.6 years    19 CG iterations for temperature    Temperature range: 973 4273.16 

Number of active cells: 19,926 (on 8 levels) Number of degrees of freedom: 321,246 (174,312+59,778+87,156) 

Timestep 0:  t=0 years 

   Rebuilding Stokes preconditioner...    Solving Stokes system... 50 iterations.    Maximal velocity: 57.8396 cm/year    Time step: 5453.78 years    18 CG iterations for temperature    Temperature range: 973 4273.16 

Timestep 1:  t=5453.78 years 

   Solving Stokes system... 49 iterations.    Maximal velocity: 59.0231 cm/year    Time step: 5345.86 years    18 CG iterations for temperature    Temperature range: 973 4273.16 

Timestep 2:  t=10799.6 years 

   Solving Stokes system... 24 iterations.    Maximal velocity: 60.2139 cm/year    Time step: 5241.51 years    17 CG iterations for temperature    Temperature range: 973 4273.16 

[...] 

Timestep 100:  t=272151 years 

   Solving Stokes system... 21 iterations.    Maximal velocity: 161.546 cm/year    Time step: 1672.96 years    17 CG iterations for temperature    Temperature range: 973 4282.57 

Number of active cells: 56,085 (on 8 levels) Number of degrees of freedom: 903,408 (490,102+168,255+245,051) 




+---------------------------------------------+------------+------------+ | Total wallclock time elapsed since start    |       115s |            | |                                             |            |            | | Section                         | no. calls |  wall time | % of total | +---------------------------------+-----------+------------+------------+ | Assemble Stokes system          |       103 |      2.82s |       2.5% | | Assemble temperature matrices   |        12 |     0.452s |      0.39% | | Assemble temperature rhs        |       103 |      11.5s |        10% | | Build Stokes preconditioner     |        12 |      2.09s |       1.8% | | Solve Stokes system             |       103 |      90.4s |        79% | | Solve temperature system        |       103 |      1.53s |       1.3% | | Postprocessing                  |         3 |     0.532s |      0.46% | | Refine mesh structure, part 1   |        12 |      0.93s |      0.81% | | Refine mesh structure, part 2   |        12 |     0.384s |      0.33% | | Setup dof systems               |        13 |      2.96s |       2.6% | +---------------------------------+-----------+------------+------------+ 

[...] 

+---------------------------------------------+------------+------------+ | Total wallclock time elapsed since start    |  9.14e+04s |            | |                                             |            |            | | Section                         | no. calls |  wall time | % of total | +---------------------------------+-----------+------------+------------+ | Assemble Stokes system          |     47045 |  2.05e+03s |       2.2% | | Assemble temperature matrices   |      4707 |       310s |      0.34% | | Assemble temperature rhs        |     47045 |   8.7e+03s |       9.5% | | Build Stokes preconditioner     |      4707 |  1.48e+03s |       1.6% | | Solve Stokes system             |     47045 |  7.34e+04s |        80% | | Solve temperature system        |     47045 |  1.46e+03s |       1.6% | | Postprocessing                  |      1883 |       222s |      0.24% | | Refine mesh structure, part 1   |      4706 |       641s |       0.7% | | Refine mesh structure, part 2   |      4706 |       259s |      0.28% | | Setup dof systems               |      4707 |  1.86e+03s |         2% | +---------------------------------+-----------+------------+------------+ </pre> </code> 

The simulation terminates when the time reaches the 1 billion years selected in the input file.  You can extrapolate from this how long a simulation would take for a different final time (the time step size ultimately settles on somewhere around 20,000 years, so computing for two billion years will take 100,000 time steps, give or take 20%).  As can be seen here, we spend most of the compute time in assembling linear systems and &mdash; above all &mdash; in solving Stokes systems. 


To demonstrate the output we show the output from every 1250th time step here:  [2.x.2933]  

The last two images show the grid as well as the partitioning of the mesh for the same computation with 16 subdomains and 16 processors. The full dynamics of this simulation are really only visible by looking at an animation, for example the one [1.x.1066]. This image is well worth watching due to its artistic quality and entrancing depiction of the evolution of the magma plumes. 

If you watch the movie, you'll see that the convection pattern goes through several stages: First, it gets rid of the instable temperature layering with the hot material overlain by the dense cold material. After this great driver is removed and we have a sort of stable situation, a few blobs start to separate from the hot boundary layer at the inner ring and rise up, with a few cold fingers also dropping down from the outer boundary layer. During this phase, the solution remains mostly symmetric, reflecting the 12-fold symmetry of the original mesh. In a final phase, the fluid enters vigorous chaotic stirring in which all symmetries are lost. This is a pattern that then continues to dominate flow. 

These different phases can also be identified if we look at the maximal velocity as a function of time in the simulation: 

 [2.x.2934]  

Here, the velocity (shown in centimeters per year) becomes very large, to the order of several meters per year) at the beginning when the temperature layering is instable. It then calms down to relatively small values before picking up again in the chaotic stirring regime. There, it remains in the range of 10-40 centimeters per year, quite within the physically expected region. 




[1.x.1067] 

3d computations are very expensive computationally. Furthermore, as seen above, interesting behavior only starts after quite a long time requiring more CPU hours than is available on a typical cluster. Consequently, rather than showing a complete simulation here, let us simply show a couple of pictures we have obtained using the successor to this program, called [1.x.1068] (short for [1.x.1069]), that is being developed independently of deal.II and that already incorporates some of the extensions discussed below. The following two pictures show isocontours of the temperature and the partition of the domain (along with the mesh) onto 512 processors: 

<p align="center">  [2.x.2935]  

 [2.x.2936]   [2.x.2937]  


[1.x.1070] 

[1.x.1071] 

There are many directions in which this program could be extended. As mentioned at the end of the introduction, most of these are under active development in the [1.x.1072] (short for [1.x.1073]) code at the time this tutorial program is being finished. Specifically, the following are certainly topics that one should address to make the program more useful: 

 [2.x.2938]     [2.x.2939]  [1.x.1074]   The temperature field we get in our simulations after a while   is mostly constant with boundary layers at the inner and outer   boundary, and streamers of cold and hot material mixing   everything. Yet, this doesn't match our expectation that things   closer to the earth core should be hotter than closer to the   surface. The reason is that the energy equation we have used does   not include a term that describes adiabatic cooling and heating:   rock, like gas, heats up as you compress it. Consequently, material   that rises up cools adiabatically, and cold material that sinks down   heats adiabatically. The correct temperature equation would   therefore look somewhat like this:   [1.x.1075] 

  or, expanding the advected derivative  [2.x.2940] :   [1.x.1076] 

  In other words, as pressure increases in a rock volume   ( [2.x.2941] ) we get an additional heat source, and vice   versa. 

  The time derivative of the pressure is a bit awkward to   implement. If necessary, one could approximate using the fact   outlined in the introduction that the pressure can be decomposed   into a dynamic component due to temperature differences and the   resulting flow, and a static component that results solely from the   static pressure of the overlying rock. Since the latter is much   bigger, one may approximate  [2.x.2942] , and consequently    [2.x.2943] .   In other words, if the fluid is moving in the direction of gravity   (downward) it will be compressed and because in that case  [2.x.2944]  we get a positive heat source. Conversely, the   fluid will cool down if it moves against the direction of gravity. 

 [2.x.2945]  [1.x.1077]   As already hinted at in the temperature model above,   mantle rocks are not incompressible. Rather, given the enormous pressures in   the earth mantle (at the core-mantle boundary, the pressure is approximately   140 GPa, equivalent to 1,400,000 times atmospheric pressure), rock actually   does compress to something around 1.5 times the density it would have   at surface pressure. Modeling this presents any number of   difficulties. Primarily, the mass conservation equation is no longer    [2.x.2946]  but should read    [2.x.2947]  where the density  [2.x.2948]  is now no longer   spatially constant but depends on temperature and pressure. A consequence is   that the model is now no longer linear; a linearized version of the Stokes   equation is also no longer symmetric requiring us to rethink preconditioners   and, possibly, even the discretization. We won't go into detail here as to   how this can be resolved. 

 [2.x.2949]  [1.x.1078] As already hinted at in various places,   material parameters such as the density, the viscosity, and the various   thermal parameters are not constant throughout the earth mantle. Rather,   they nonlinearly depend on the pressure and temperature, and in the case of   the viscosity on the strain rate  [2.x.2950] . For complicated   models, the only way to solve such models accurately may be to actually   iterate this dependence out in each time step, rather than simply freezing   coefficients at values extrapolated from the previous time step(s). 

 [2.x.2951]  [1.x.1079] Running this program in 2d on a number of   processors allows solving realistic models in a day or two. However, in 3d,   compute times are so large that one runs into two typical problems: (i) On   most compute clusters, the queuing system limits run times for individual   jobs are to 2 or 3 days; (ii) losing the results of a computation due to   hardware failures, misconfigurations, or power outages is a shame when   running on hundreds of processors for a couple of days. Both of these   problems can be addressed by periodically saving the state of the program   and, if necessary, restarting the program at this point. This technique is   commonly called [1.x.1080] and it requires that the entire   state of the program is written to a permanent storage location (e.g. a hard   drive). Given the complexity of the data structures of this program, this is   not entirely trivial (it may also involve writing gigabytes or more of   data), but it can be made easier by realizing that one can save the state   between two time steps where it essentially only consists of the mesh and   solution vectors; during restart one would then first re-enumerate degrees   of freedom in the same way as done before and then re-assemble   matrices. Nevertheless, given the distributed nature of the data structures   involved here, saving and restoring the state of a program is not   trivial. An additional complexity is introduced by the fact that one may   want to change the number of processors between runs, for example because   one may wish to continue computing on a mesh that is finer than the one used   to precompute a starting temperature field at an intermediate time. 

 [2.x.2952]  [1.x.1081] The point of computations like this is   not simply to solve the equations. Rather, it is typically the exploration   of different physical models and their comparison with things that we can   measure at the earth surface, in order to find which models are realistic   and which are contradicted by reality. To this end, we need to compute   quantities from our solution vectors that are related to what we can   observe. Among these are, for example, heatfluxes at the surface of the   earth, as well as seismic velocities throughout the mantle as these affect   earthquake waves that are recorded by seismographs. 

 [2.x.2953]  [1.x.1082] As can be seen above for the 3d case, the mesh in 3d is primarily refined along the inner boundary. This is because the boundary layer there is stronger than any other transition in the domain, leading us to refine there almost exclusively and basically not at all following the plumes. One certainly needs better refinement criteria to track the parts of the solution we are really interested in better than the criterion used here, namely the KellyErrorEstimator applied to the temperature, is able to.  [2.x.2954]  


There are many other ways to extend the current program. However, rather than discussing them here, let us point to the much larger open source code ASPECT (see https://aspect.geodynamics.org/ ) that constitutes the further development of step-32 and that already includes many such possible extensions. 


examples/step-33/doc/intro.dox 

 [2.x.2955]  

[1.x.1083] 

 [2.x.2956]  The program uses the [1.x.1084] linear solvers (these can be found in Trilinos in the Aztec/Amesos packages) and an automatic differentiation package, Sacado, also part of Trilinos. deal.II must be configured to use Trilinos. Refer to the [1.x.1085] file for instructions how to do this. 

 [2.x.2957]  While this program demonstrates the use of automatic differentiation well, it does not express the state of the art in Euler equation solvers. There are much faster and more accurate method for this equation, and you should take a look at step-67 and step-69 to see how this equation can be solved more efficiently. 




[1.x.1086] [1.x.1087] 

[1.x.1088] 

The equations that describe the movement of a compressible, inviscid gas (the so-called Euler equations of gas dynamics) are a basic system of conservation laws. In spatial dimension  [2.x.2958]  they read 

[1.x.1089] 

with the solution  [2.x.2959]  consisting of  [2.x.2960]  the fluid density,  [2.x.2961]  the flow velocity (and thus  [2.x.2962]  being the linear momentum density), and  [2.x.2963]  the energy density of the gas. We interpret the equations above as  [2.x.2964] ,  [2.x.2965] . 

For the Euler equations, the flux matrix  [2.x.2966]  (or system of flux functions) is defined as (shown here for the case  [2.x.2967] ) 

[1.x.1090] 

and we will choose as particular right hand side forcing only the effects of gravity, described by 

[1.x.1091] 

where  [2.x.2968]  denotes the gravity vector. With this, the entire system of equations reads: 

[1.x.1092] 

These equations describe, respectively, the conservation of momentum, mass, and energy. The system is closed by a relation that defines the pressure:  [2.x.2969] . For the constituents of air (mainly nitrogen and oxygen) and other diatomic gases, the ratio of specific heats is  [2.x.2970] . 

This problem obviously falls into the class of vector-valued problems. A general overview of how to deal with these problems in deal.II can be found in the  [2.x.2971]  module. 

[1.x.1093] 

Discretization happens in the usual way, taking into account that this is a hyperbolic problem in the same style as the simple one discussed in step-12: We choose a finite element space  [2.x.2972] , and integrate our conservation law against our (vector-valued) test function  [2.x.2973] .  We then integrate by parts and approximate the boundary flux with a [1.x.1094] flux  [2.x.2974] , 

[1.x.1095] 

where a superscript  [2.x.2975]  denotes the interior trace of a function, and  [2.x.2976]  represents the outer trace. The diffusion term  [2.x.2977]  is introduced strictly for stability,  where  [2.x.2978]  is the mesh size and  [2.x.2979]  is a parameter prescribing how  much diffusion to add. 

On the boundary, we have to say what the outer trace  [2.x.2980]  is. Depending on the boundary condition, we prescribe either of the following:  [2.x.2981]   [2.x.2982]  Inflow boundary:  [2.x.2983]  is prescribed to be the desired value.  [2.x.2984]  Supersonic outflow boundary:  [2.x.2985]   [2.x.2986]  Subsonic outflow boundary:  [2.x.2987]  except that the energy variable is modified to support a prescribed pressure  [2.x.2988] , i.e.  [2.x.2989]   [2.x.2990]  Reflective boundary: we set  [2.x.2991]  so that  [2.x.2992]  and  [2.x.2993] .  [2.x.2994]  

More information on these issues can be found, for example, in Ralf Hartmann's PhD thesis ("Adaptive Finite Element Methods for the Compressible Euler Equations", PhD thesis, University of Heidelberg, 2002). 

We use a time stepping scheme to substitute the time derivative in the above equations. For simplicity, we define  [2.x.2995]  as the spatial residual at time step  [2.x.2996]  : 

[1.x.1096] 



At each time step, our full discretization is thus that the residual applied to any test function  [2.x.2997]  equals zero: 

[1.x.1097] 

where  [2.x.2998]  and  [2.x.2999] . Choosing  [2.x.3000]  results in the explicit (forward) Euler scheme,  [2.x.3001]  in the stable implicit (backward) Euler scheme, and  [2.x.3002]  in the Crank-Nicolson scheme. 

In the implementation below, we choose the Lax-Friedrichs flux for the function  [2.x.3003] , i.e.   [2.x.3004] , where  [2.x.3005]  is either a fixed number specified in the input file, or where  [2.x.3006]  is a mesh dependent value. In the latter case, it is chosen as  [2.x.3007]  with  [2.x.3008]  the diameter of the face to which the flux is applied, and  [2.x.3009]  the current time step. 

With these choices, equating the residual to zero results in a nonlinear system of equations  [2.x.3010] . We solve this nonlinear system by a Newton iteration (in the same way as explained in step-15), i.e. by iterating 

[1.x.1098] 

until  [2.x.3011]  (the residual) is sufficiently small. By testing with the nodal basis of a finite element space instead of all  [2.x.3012] , we arrive at a linear system for  [2.x.3013] : 

[1.x.1099] 

This linear system is, in general, neither symmetric nor has any particular definiteness properties. We will either use a direct solver or Trilinos' GMRES implementation to solve it. As will become apparent from the [1.x.1100], this fully implicit iteration converges very rapidly (typically in 3 steps) and with the quadratic convergence order expected from a Newton method. 




[1.x.1101] 

Since computing the Jacobian matrix  [2.x.3014]  is a terrible beast, we use an automatic differentiation package, Sacado, to do this.  Sacado is a package within the [1.x.1102] framework and offers a C++ template class  [2.x.3015]  ( [2.x.3016]  standing for "forward automatic differentiation") that supports basic arithmetic operators and functions such as  [2.x.3017]  etc. In order to use this feature, one declares a collection of variables of this type and then denotes some of this collection as degrees of freedom, the rest of the variables being functions of the independent variables.  These variables are used in an algorithm, and as the variables are used, their sensitivities with respect to the degrees of freedom are continuously updated. 

One can imagine that for the full Jacobian matrix as a whole, this could be prohibitively expensive: the number of independent variables are the  [2.x.3018] , the dependent variables the elements of the vector  [2.x.3019] . Both of these vectors can easily have tens of thousands of elements or more.  However, it is important to note that not all elements of  [2.x.3020]  depend on all elements of  [2.x.3021] : in fact, an entry in  [2.x.3022]  only depends on an element of  [2.x.3023]  if the two corresponding shape functions overlap and couple in the weak form. 

Specifically, it is wise to define a minimum set of independent AD variables that the residual on the current cell may possibly depend on: on every element, we define those variables as independent that correspond to the degrees of freedom defined on this cell (or, if we have to compute jump terms between cells, that correspond to degrees of freedom defined on either of the two adjacent cells), and the dependent variables are the elements of the local residual vector. Not doing this, i.e. defining [1.x.1103] elements of  [2.x.3024]  as independent, will result a very expensive computation of a lot of zeros: the elements of the local residual vector are independent of almost all elements of the solution vector, and consequently their derivatives are zero; however, trying to compute these zeros can easily take 90% or more of the compute time of the entire program, as shown in an experiment inadvertently made by a student a few years after this program was first written. 


Coming back to the question of computing the Jacobian automatically: The author has used this approach side by side with a hand coded Jacobian for the incompressible Navier-Stokes problem and found the Sacado approach to be just as fast as using a hand coded Jacobian, but infinitely simpler and less error prone: Since using the auto-differentiation requires only that one code the residual  [2.x.3025] , ensuring code correctness and maintaining code becomes tremendously more simple -- the Jacobian matrix  [2.x.3026]  is computed by essentially the same code that also computes the residual  [2.x.3027] . 

All this said, here's a very simple example showing how Sacado can be used: 

[1.x.1104] 



The output are the derivatives  [2.x.3028]  of  [2.x.3029]  at  [2.x.3030] . 

It should be noted that Sacado provides more auto-differentiation capabilities than the small subset used in this program.  However, understanding the example above is enough to understand the use of Sacado in this Euler flow program. 

[1.x.1105] The program uses either the Aztec iterative solvers, or the Amesos sparse direct solver, both provided by the Trilinos package.  This package is inherently designed to be used in a parallel program, however, it may be used in serial just as easily, as is done here.  The Epetra package is the basic vector/matrix library upon which the solvers are built.  This very powerful package can be used to describe the parallel distribution of a vector, and to define sparse matrices that operate on these vectors.  Please view the commented code for more details on how these solvers are used within the example. 

[1.x.1106] The example uses an ad hoc refinement indicator that shows some usefulness in shock-type problems, and in the downhill flow example included.  We refine according to the squared gradient of the density. Hanging nodes are handled by computing the numerical flux across cells that are of differing refinement levels, rather than using the AffineConstraints class as in all other tutorial programs so far.  In this way, the example combines the continuous and DG methodologies. It also simplifies the generation of the Jacobian because we do not have to track constrained degrees of freedom through the automatic differentiation used to compute it. 

 [2.x.3031]  Whereas this program was written in 2008, we were unaware of any publication that would actually have used this approach. However, a more recent paper by A. Dedner, R. Kl&ouml;fkorn, and M. Kr&auml;nkel ("Continuous Finite-Elements on Non-Conforming Grids Using Discontinuous Galerkin Stabilization", Proceedings of Finite Volumes for Complex Applications VII - Methods and Theoretical Aspects, Springer, 2014) comes close. 

Further, we enforce a maximum number of refinement levels to keep refinement under check.  It is the author's experience that for adaptivity for a time dependent problem, refinement can easily lead the simulation to a screeching halt, because of time step restrictions if the mesh becomes too fine in any part of the domain, if care is not taken.  The amount of refinement is limited in the example by letting the user specify the maximum level of refinement that will be present anywhere in the mesh.  In this way, refinement tends not to slow the simulation to a halt.  This, of course, is purely a heuristic strategy, and if the author's advisor heard about it, the author would likely be exiled forever from the finite  element error estimation community. 

[1.x.1107] 

We use an input file deck to drive the simulation.  In this way, we can alter the boundary conditions and other important properties of the simulation without having to recompile.  For more information on the format, look at the [1.x.1108], where we describe an example input file in more detail. 

In previous example programs, we have usually hard-coded the initial and boundary conditions. In this program, we instead use the expression parser class FunctionParser so that we can specify a generic expression in the input file and have it parsed at run time &mdash; this way, we can change initial conditions without the need to recompile the program. Consequently, no classes named InitialConditions or BoundaryConditions will be declared in the program below. 




[1.x.1109] 

The implementation of this program is split into three essential parts:  [2.x.3032]     [2.x.3033] The  [2.x.3034]  class that encapsulates everything that   completely describes the specifics of the Euler equations. This includes the   flux matrix  [2.x.3035] , the numerical flux  [2.x.3036] , the right hand side  [2.x.3037] ,   boundary conditions, refinement indicators, postprocessing the output, and   similar things that require knowledge of the meaning of the individual   components of the solution vectors and the equations. 

   [2.x.3038] A namespace that deals with everything that has to do with run-time   parameters. 

   [2.x.3039] The  [2.x.3040]  class that deals with time stepping,   outer nonlinear and inner linear solves, assembling the linear systems, and   the top-level logic that drives all this.  [2.x.3041]  

The reason for this approach is that it separates the various concerns in a program: the  [2.x.3042]  is written in such a way that it would be relatively straightforward to adapt it to a different set of equations: One would simply re-implement the members of the  [2.x.3043]  class for some other hyperbolic equation, or augment the existing equations by additional ones (for example by advecting additional variables, or by adding chemistry, etc). Such modifications, however, would not affect the time stepping, or the nonlinear solvers if correctly done, and consequently nothing in the  [2.x.3044]  would have to be modified. 

Similarly, if we wanted to improve on the linear or nonlinear solvers, or on the time stepping scheme (as hinted at the end of the [1.x.1110]), then this would not require changes in the  [2.x.3045]  at all. 


examples/step-33/doc/results.dox 

[1.x.1111] 

[1.x.1112] 

We run the problem with the mesh  [2.x.3046]  (this file is in the same directory as the source code for this program) and the following input deck (available as  [2.x.3047]  in the same directory): 

[1.x.1113] 



When we run the program, we get the following kind of output: 

[1.x.1114] 



This output reports the progress of the Newton iterations and the time stepping. Note that our implementation of the Newton iteration indeed shows the expected quadratic convergence order: the norm of the nonlinear residual in each step is roughly the norm of the previous step squared. This leads to the very rapid convergence we can see here. This holds until times up to  [2.x.3048]  at which time the nonlinear iteration reports a lack of convergence: 

[1.x.1115] 



We may find out the cause and possible remedies by looking at the animation of the solution. 

The result of running these computations is a bunch of output files that we can pass to our visualization program of choice. When we collate them into a movie, the results of last several time steps looks like this: 

 [2.x.3049]  

As we see, when the heavy mass of fluid hits the left bottom corner, some oscillation occurs and lead to the divergence of the iteration. A lazy solution to this issue is add more viscosity. If we set the diffusion power  [2.x.3050]  instead of  [2.x.3051] , the simulation would be able to survive this crisis. Then, the result looks like this: 


 [2.x.3052]  

The heavy mass of fluid is drawn down the slope by gravity, where it collides with the ski lodge and is flung into the air!  Hopefully everyone escapes! And also, we can see the boundary between heavy mass and light mass blur quickly due to the artificial viscosity. 

We can also visualize the evolution of the adaptively refined grid: 

 [2.x.3053]  

The adaptivity follows and precedes the flow pattern, based on the heuristic refinement scheme discussed above. 





[1.x.1116] 

[1.x.1117] 

[1.x.1118] 

The numerical scheme we have chosen is not particularly stable when the artificial viscosity is small while is too diffusive when the artificial viscosity is large. Furthermore, it is known there are more advanced techniques to stabilize the solution, for example streamline diffusion, least-squares stabilization terms, entropy viscosity. 




[1.x.1119] 

While the Newton method as a nonlinear solver appears to work very well if the time step is small enough, the linear solver can be improved. For example, in the current scheme whenever we use an iterative solver, an ILU is computed anew for each Newton step; likewise, for the direct solver, an LU decomposition of the Newton matrix is computed in each step. This is obviously wasteful: from one Newton step to another, and probably also between time steps, the Newton matrix does not radically change: an ILU or a sparse LU decomposition for one Newton step is probably still a very good preconditioner for the next Newton or time step. Avoiding the recomputation would therefore be a good way to reduce the amount of compute time. 

One could drive this a step further: since close to convergence the Newton matrix changes only a little bit, one may be able to define a quasi-Newton scheme where we only re-compute the residual (i.e. the right hand side vector) in each Newton iteration, and re-use the Newton matrix. The resulting scheme will likely not be of quadratic convergence order, and we have to expect to do a few more nonlinear iterations; however, given that we don't have to spend the time to build the Newton matrix each time, the resulting scheme may well be faster. 




[1.x.1120] 

The residual calculated in  [2.x.3054]  function reads     [2.x.3055]  This means that we calculate the spatial residual twice at one Newton iteration step: once respect to the current solution  [2.x.3056]  and once more respect to the last time step solution  [2.x.3057]  which remains the same during all Newton iterations through one timestep. Cache up the explicit part of residual   [2.x.3058]  during Newton iteration will save lots of labor. 




[1.x.1121] 

Finally, as a direction beyond the immediate solution of the Euler equations, this program tries very hard to separate the implementation of everything that is specific to the Euler equations into one class (the  [2.x.3059]  class), and everything that is specific to assembling the matrices and vectors, nonlinear and linear solvers, and the general top-level logic into another (the  [2.x.3060]  class). 

By replacing the definitions of flux matrices and numerical fluxes in this class, as well as the various other parts defined there, it should be possible to apply the  [2.x.3061]  class to other hyperbolic conservation laws as well. 


examples/step-34/doc/intro.dox 

 [2.x.3062]  

[1.x.1122] 

 [2.x.3063]  

[1.x.1123] 

[1.x.1124] 

[1.x.1125] The incompressible motion of an inviscid fluid past a body (for example air past an airplane wing, or air or water past a propeller) is usually modeled by the Euler equations of fluid dynamics: 

[1.x.1126] where the fluid density  [2.x.3064]  and the acceleration  [2.x.3065]  due to external forces are given and the velocity  [2.x.3066]  and the pressure  [2.x.3067]  are the unknowns. Here  [2.x.3068]  is a closed bounded region representing the body around which the fluid moves. 

The above equations can be derived from Navier-Stokes equations assuming that the effects due to viscosity are negligible compared to those due to the pressure gradient, inertial forces and the external forces. This is the opposite case of the Stokes equations discussed in step-22 which are the limit case of dominant viscosity, i.e. where the velocity is so small that inertia forces can be neglected. On the other hand, owing to the assumed incompressibility, the equations are not suited for very high speed gas flows where compressibility and the equation of state of the gas have to be taken into account, leading to the Euler equations of gas dynamics, a hyperbolic system. 

For the purpose of this tutorial program, we will consider only stationary flow without external forces: 

[1.x.1127] 


Uniqueness of the solution of the Euler equations is ensured by adding the boundary conditions 

[1.x.1128] 

which is to say that the body is at rest in our coordinate systems and is not permeable, and that the fluid has (constant) velocity  [2.x.3069]  at infinity. An alternative viewpoint is that our coordinate system moves along with the body whereas the background fluid is at rest at infinity. Notice that we define the normal  [2.x.3070]  as the [1.x.1129] normal to the domain  [2.x.3071] , which is the opposite of the outer normal to the integration domain. 

For both stationary and non stationary flow, the solution process starts by solving for the velocity in the second equation and substituting in the first equation in order to find the pressure. The solution of the stationary Euler equations is typically performed in order to understand the behavior of the given (possibly complex) geometry when a prescribed motion is enforced on the system. 

The first step in this process is to change the frame of reference from a coordinate system moving along with the body to one in which the body moves through a fluid that is at rest at infinity. This can be expressed by introducing a new velocity  [2.x.3072]  for which we find that the same equations hold (because  [2.x.3073] ) and we have boundary conditions 

[1.x.1130] 

If we assume that the fluid is irrotational, i.e.,  [2.x.3074]  in  [2.x.3075] , we can represent the velocity, and consequently also the perturbation velocity, as the gradient of a scalar function: 

[1.x.1131] and so the second part of Euler equations above can be rewritten as the homogeneous Laplace equation for the unknown  [2.x.3076] : 

[1.x.1132] while the momentum equation reduces to Bernoulli's equation that expresses the pressure  [2.x.3077]  as a function of the potential  [2.x.3078] : 

[1.x.1133] 

So we can solve the problem by solving the Laplace equation for the potential.  We recall that the following functions, called fundamental solutions of the Laplace equation, 

[1.x.1134] 

satisfy in a distributional sense the equation: 

[1.x.1135] 

where the derivative is done in the variable  [2.x.3079] . By using the usual Green identities, our problem can be written on the boundary  [2.x.3080]  only. We recall the general definition of the second Green %identity: 

[1.x.1136] 

where  [2.x.3081]  is the normal to the surface of  [2.x.3082]  pointing outwards from the domain of integration  [2.x.3083] . 

In our case the domain of integration is the domain  [2.x.3084] , whose boundary is  [2.x.3085] , where the "boundary" at infinity is defined as 

[1.x.1137] 

In our program the normals are defined as [1.x.1138] to the domain  [2.x.3086] , that is, they are in fact [1.x.1139] to the integration domain, and some care is required in defining the various integrals with the correct signs for the normals, i.e. replacing  [2.x.3087]  by  [2.x.3088] . 

If we substitute  [2.x.3089]  and  [2.x.3090]  in the Green %identity with the solution  [2.x.3091]  and with the fundamental solution of the Laplace equation respectively, as long as  [2.x.3092]  is chosen in the region  [2.x.3093] , we obtain: 

[1.x.1140] 

where the normals are now pointing [1.x.1141] the domain of integration. 

Notice that in the above equation, we also have the integrals on the portion of the boundary at  [2.x.3094] . Using the boundary conditions of our problem, we have that  [2.x.3095]  is zero at infinity (which simplifies the integral on  [2.x.3096]  on the right hand side). 

The integral on  [2.x.3097]  that appears on the left hand side can be treated by observing that  [2.x.3098]  implies that  [2.x.3099]  at infinity is necessarily constant. We define its value to be  [2.x.3100] .  It is an easy exercise to prove that 

[1.x.1142] 

Using this result, we can reduce the above equation only on the boundary  [2.x.3101]  using the so-called Single and Double Layer Potential operators: 

[1.x.1143] 

(The name of these operators comes from the fact that they describe the electric potential in  [2.x.3102]  due to a single thin sheet of charges along a surface, and due to a double sheet of charges and anti-charges along the surface, respectively.) 

In our case, we know the Neumann values of  [2.x.3103]  on the boundary:  [2.x.3104] . Consequently, 

[1.x.1144] If we take the limit for  [2.x.3105]  tending to  [2.x.3106]  of the above equation, using well known properties of the single and double layer operators, we obtain an equation for  [2.x.3107]  just on the boundary  [2.x.3108]  of  [2.x.3109] : 

[1.x.1145] 

which is the Boundary Integral Equation (BIE) we were looking for, where the quantity  [2.x.3110]  is the fraction of angle or solid angle by which the point  [2.x.3111]  sees the domain of integration  [2.x.3112] . 

In particular, at points  [2.x.3113]  where the boundary  [2.x.3114]  is differentiable (i.e. smooth) we have  [2.x.3115] , but the value may be smaller or larger at points where the boundary has a corner or an edge. 

Substituting the single and double layer operators we get: 

[1.x.1146] for two dimensional flows and 

[1.x.1147] for three dimensional flows, where the normal derivatives of the fundamental solutions have been written in a form that makes computation easier. In either case,  [2.x.3116]  is the solution of an integral equation posed entirely on the boundary since both  [2.x.3117] . 

Notice that the fraction of angle (in 2d) or solid angle (in 3d)  [2.x.3118]  by which the point  [2.x.3119]  sees the domain  [2.x.3120]  can be defined using the double layer potential itself: 

[1.x.1148] 

The reason why this is possible can be understood if we consider the fact that the solution of a pure Neumann problem is known up to an arbitrary constant  [2.x.3121] , which means that, if we set the Neumann data to be zero, then any constant  [2.x.3122]  will be a solution. Inserting the constant solution and the Neumann boundary condition in the boundary integral equation, we have 

[1.x.1149] 

The integral on  [2.x.3123]  is unity, see above, so division by the constant  [2.x.3124]  gives us the explicit expression above for  [2.x.3125] . 

While this example program is really only focused on the solution of the boundary integral equation, in a realistic setup one would still need to solve for the velocities. To this end, note that we have just computed  [2.x.3126]  for all  [2.x.3127] . In the next step, we can compute (analytically, if we want) the solution  [2.x.3128]  in all of  [2.x.3129] . To this end, recall that we had 

[1.x.1150] where now we have everything that is on the right hand side ( [2.x.3130]  and  [2.x.3131]  are integrals we can evaluate, the normal velocity on the boundary is given, and  [2.x.3132]  on the boundary we have just computed). Finally, we can then recover the velocity as  [2.x.3133] . 

Notice that the evaluation of the above formula for  [2.x.3134]  should yield zero as a result, since the integration of the Dirac delta  [2.x.3135]  in the domain  [2.x.3136]  is always zero by definition. 

As a final test, let us verify that this velocity indeed satisfies the momentum balance equation for a stationary flow field, i.e., whether  [2.x.3137]  where  [2.x.3138]  for some (unknown) pressure  [2.x.3139]  and a given constant  [2.x.3140] . In other words, we would like to verify that Bernoulli's law as stated above indeed holds. To show this, we use that the left hand side of this equation equates to 

[1.x.1151] 

where we have used that  [2.x.3141]  is constant. We would like to write this expression as the gradient of something (remember that  [2.x.3142]  is a constant). The next step is more convenient if we consider the components of the equation individually (summation over indices that appear twice is implied): 

[1.x.1152] 

because  [2.x.3143]  and  [2.x.3144] . Next, 

[1.x.1153] 

Again, the last term disappears because  [2.x.3145]  is constant and we can merge the first and third term into one: 

[1.x.1154] 



We now only need to massage that last term a bit more. Using the product rule, we get 

[1.x.1155] 

The first of these terms is zero (because, again, the summation over  [2.x.3146]  gives  [2.x.3147] , which is zero). The last term can be written as  [2.x.3148]  which is in the desired gradient form. As a consequence, we can now finally state that 

[1.x.1156] 

or in vector form: 

[1.x.1157] 

or in other words: 

[1.x.1158] 

Because the pressure is only determined up to a constant (it appears only with a gradient in the equations), an equally valid definition is 

[1.x.1159] 

This is exactly Bernoulli's law mentioned above. 




[1.x.1160] 

Numerical approximations of Boundary Integral Equations (BIE) are commonly referred to as the boundary element method or panel method (the latter expression being used mostly in the computational fluid dynamics community). The goal of the following test problem is to solve the integral formulation of the Laplace equation with Neumann boundary conditions, using a circle and a sphere respectively in two and three space dimensions, illustrating along the way the features that allow one to treat boundary element problems almost as easily as finite element problems using the deal.II library. 

To this end, let  [2.x.3149]  be a subdivision of the manifold  [2.x.3150]  into  [2.x.3151]  line segments if  [2.x.3152] , or  [2.x.3153]  quadrilaterals if  [2.x.3154] . We will call each individual segment or quadrilateral an [1.x.1161] or [1.x.1162], independently of the dimension  [2.x.3155]  of the surrounding space  [2.x.3156] . We define the finite dimensional space  [2.x.3157]  as 

[1.x.1163] with basis functions  [2.x.3158]  for which we will use the usual FE_Q finite element, with the catch that this time it is defined on a manifold of codimension one (which we do by using the second template argument that is usually defaulted to equal the first; here, we will create objects  [2.x.3159]  dimensional cells in a  [2.x.3160]  dimensional space). An element  [2.x.3161]  of  [2.x.3162]  is uniquely identified by the vector  [2.x.3163]  of its coefficients  [2.x.3164] , that is: 

[1.x.1164] where summation  is implied over repeated indexes. Note that we could use discontinuous elements here &mdash; in fact, there is no real reason to use continuous ones since the integral formulation does not imply any derivatives on our trial functions so continuity is unnecessary, and often in the literature only piecewise constant elements are used. 

[1.x.1165] 

By far, the most common approximation of boundary integral equations is by use of the collocation based boundary element method. 

This method requires the evaluation of the boundary integral equation at a number of collocation points which is equal to the number of unknowns of the system. The choice of these points is a delicate matter, that requires a careful study. Assume that these points are known for the moment, and call them  [2.x.3165]  with  [2.x.3166] . 

The problem then becomes: Given the datum  [2.x.3167] , find a function  [2.x.3168]  in  [2.x.3169]  such that the following  [2.x.3170]  equations are satisfied: 

[1.x.1166] 

where the quantity  [2.x.3171]  is the fraction of (solid) angle by which the point  [2.x.3172]  sees the domain  [2.x.3173] , as explained above, and we set  [2.x.3174]  to be zero.  If the support points  [2.x.3175]  are chosen appropriately, then the problem can be written as the following linear system: 

[1.x.1167] 

where 

[1.x.1168] 

From a linear algebra point of view, the best possible choice of the collocation points is the one that renders the matrix  [2.x.3176]  the most diagonally dominant. A natural choice is then to select the  [2.x.3177]  collocation points to be the support points of the nodal basis functions  [2.x.3178] . In that case,  [2.x.3179] , and as a consequence the matrix  [2.x.3180]  is diagonal with entries 

[1.x.1169] where we have used that  [2.x.3181]  for the usual Lagrange elements. With this choice of collocation points, the computation of the entries of the matrices  [2.x.3182] ,  [2.x.3183]  and of the right hand side  [2.x.3184]  requires the evaluation of singular integrals on the elements  [2.x.3185]  of the triangulation  [2.x.3186] . As usual in these cases, all integrations are performed on a reference simple domain, i.e., we assume that each element  [2.x.3187]  of  [2.x.3188]  can be expressed as a linear (in two dimensions) or bi-linear (in three dimensions) transformation of the reference boundary element  [2.x.3189] , and we perform the integrations after a change of variables from the real element  [2.x.3190]  to the reference element  [2.x.3191] . 

[1.x.1170] 

In two dimensions it is not necessary to compute the diagonal elements  [2.x.3192]  of the system matrix, since, even if the denominator goes to zero when  [2.x.3193] , the numerator is always zero because  [2.x.3194]  and  [2.x.3195]  are orthogonal (on our polygonal approximation of the boundary of  [2.x.3196] ), and the only singular integral arises in the computation of  [2.x.3197]  on the i-th element of  [2.x.3198] : 

[1.x.1171] 

This can be easily treated by the QGaussLogR quadrature formula. 

Similarly, it is possible to use the QGaussOneOverR quadrature formula to perform the singular integrations in three dimensions. The interested reader will find detailed explanations on how these quadrature rules work in their documentation. 

The resulting matrix  [2.x.3199]  is full. Depending on its size, it might be convenient to use a direct solver or an iterative one. For the purpose of this example code, we chose to use only an iterative solver, without providing any preconditioner. 

If this were a production code rather than a demonstration of principles, there are techniques that are available to not store full matrices but instead store only those entries that are large and/or relevant. In the literature on boundary element methods, a plethora of methods is available that allows to determine which elements are important and which are not, leading to a significantly sparser representation of these matrices that also facilitates rapid evaluations of the scalar product between vectors and matrices. This not being the goal of this program, we leave this for more sophisticated implementations. 




[1.x.1172] 

The implementation is rather straight forward. The main point that hasn't been used in any of the previous tutorial programs is that most classes in deal.II are not only templated on the dimension, but in fact on the dimension of the manifold on which we pose the differential equation as well as the dimension of the space into which this manifold is embedded. By default, the second template argument equals the first, meaning for example that we want to solve on a two-dimensional region of two-dimensional space. The triangulation class to use in this case would be  [2.x.3200] , which is an equivalent way of writing  [2.x.3201] . 

However, this doesn't have to be so: in the current example, we will for example want to solve on the surface of a sphere, which is a two-dimensional manifold embedded in a three-dimensional space. Consequently, the right class will be  [2.x.3202] , and correspondingly we will use  [2.x.3203]  as the DoF handler class and  [2.x.3204]  for finite elements. 

Some further details on what one can do with things that live on curved manifolds can be found in the report [1.x.1173][1.x.1174]. In addition, the step-38 tutorial program extends what we show here to cases where the equation posed on the manifold is not an integral operator but in fact involves derivatives. 




[1.x.1175] 

The testcase we will be solving is for a circular (in 2d) or spherical (in 3d) obstacle. Meshes for these geometries will be read in from files in the current directory and an object of type SphericalManifold will then be attached to the triangulation to allow mesh refinement that respects the continuous geometry behind the discrete initial mesh. 

For a sphere of radius  [2.x.3205]  translating at a velocity of  [2.x.3206]  in the  [2.x.3207]  direction, the potential reads 

[1.x.1176] 

see, e.g. J. N. Newman, [1.x.1177], 1977, pp. 127. For unit speed and radius, and restricting  [2.x.3208]  to lie on the surface of the sphere,  [2.x.3209] . In the test problem, the flow is  [2.x.3210] , so the appropriate exact solution on the surface of the sphere is the superposition of the above solution with the analogous solution along the  [2.x.3211]  and  [2.x.3212]  axes, or  [2.x.3213] . 


examples/step-34/doc/results.dox 



[1.x.1178] 

We ran the program using the following  [2.x.3214]  file (which can also be found in the directory in which all the other source files are): 

[1.x.1179] 



When we run the program, the following is printed on screen: 

[1.x.1180] 



As we can see from the convergence table in 2d, if we choose quadrature formulas which are accurate enough, then the error we obtain for  [2.x.3215]  should be exactly the inverse of the number of elements. The approximation of the circle with N segments of equal size generates a regular polygon with N faces, whose angles are exactly  [2.x.3216] , therefore the error we commit should be exactly  [2.x.3217] . In fact this is a very good indicator that we are performing the singular integrals in an appropriate manner. 

The error in the approximation of the potential  [2.x.3218]  is largely due to approximation of the domain. A much better approximation could be obtained by using higher order mappings. 

If we modify the main() function, setting fe_degree and mapping_degree to two, and raise the order of the quadrature formulas  in the parameter file, we obtain the following convergence table for the two dimensional simulation 

[1.x.1181] 



and 

[1.x.1182] 



for the three dimensional case. As we can see, convergence results are much better with higher order mapping, mainly due to a better resolution of the curved geometry. Notice that, given the same number of degrees of freedom, for example in step 3 of the Q1 case and step 2 of Q2 case in the three dimensional simulation, the error is roughly three orders of magnitude lower. 

The result of running these computations is a bunch of output files that we can pass to our visualization program of choice. The output files are of two kind: the potential on the boundary element surface, and the potential extended to the outer and inner domain. The combination of the two for the two dimensional case looks like 

 [2.x.3219]  

while in three dimensions we show first the potential on the surface, together with a contour plot, 

 [2.x.3220]  

and then the external contour plot of the potential, with opacity set to 25%: 

 [2.x.3221]  


[1.x.1183] 

[1.x.1184] 

This is the first tutorial program that considers solving equations defined on surfaces embedded in higher dimensional spaces. But the equation discussed here was relatively simple because it only involved an integral operator, not derivatives which are more difficult to define on the surface. The step-38 tutorial program considers such problems and provides the necessary tools. 

From a practical perspective, the Boundary Element Method (BEM) used here suffers from two bottlenecks. The first is that assembling the matrix has a cost that is *quadratic* in the number of unknowns, that is  [2.x.3222]  where  [2.x.3223]  is the total number of unknowns. This can be seen by looking at the `assemble_system()` function, which has this structure: 

[1.x.1185] 

Here, the first loop walks over all cells (one factor of  [2.x.3224] ) whereas the inner loop contributes another factor of  [2.x.3225] . 

This has to be contrasted with the finite element method for *local* differential operators: There, we loop over all cells (one factor of  [2.x.3226] ) and on each cell do an amount of work that is independent of how many cells or unknowns there are. This clearly presents a bottleneck. 

The second bottleneck is that the system matrix is dense (i.e., is of type FullMatrix) because every degree of freedom couples with every other degree of freedom. As pointed out above, just *computing* this matrix with its  [2.x.3227]  nonzero entries necessarily requires at least  [2.x.3228]  operations, but it's worth pointing out that it also costs this many operations to just do one matrix-vector product. If the GMRES method used to solve the linear system requires a number of iterations that grows with the size of the problem, as is typically the case, then solving the linear system will require a number of operations that grows even faster than just  [2.x.3229] . 

"Real" boundary element methods address these issues by strategies that determine which entries of the matrix will be small and can consequently be neglected (at the cost of introducing an additional error, of course). This is possible by recognizing that the matrix entries decay with the (physical) distance between the locations where degrees of freedom  [2.x.3230]  and  [2.x.3231]  are defined. This can be exploited in methods such as the Fast Multipole Method (FMM) that control which matrix entries must be stored and computed to achieve a certain accuracy, and -- if done right -- result in methods in which both assembly and solution of the linear system requires less than  [2.x.3232]  operations. 

Implementing these methods clearly presents opportunities to extend the current program. 


examples/step-35/doc/intro.dox 

 [2.x.3233]  

[1.x.1186] 

[1.x.1187] 

[1.x.1188] 

[1.x.1189] 

[1.x.1190] The purpose of this program is to show how to effectively solve the incompressible time-dependent Navier-Stokes equations. These equations describe the flow of a viscous incompressible fluid and read 

[1.x.1191] 

where  [2.x.3234]  represents the velocity of the flow and  [2.x.3235]  the pressure. This system of equations is supplemented by the initial condition 

[1.x.1192] 

with  [2.x.3236]  sufficiently smooth and solenoidal, and suitable boundary conditions. For instance, an admissible boundary condition, is 

[1.x.1193] 

It is possible to prescribe other boundary conditions as well. In the test case that we solve here the boundary is partitioned into two disjoint subsets  [2.x.3237]  and we have 

[1.x.1194] 

and 

[1.x.1195] 

where  [2.x.3238]  is the outer unit normal. The boundary conditions on  [2.x.3239]  are often used to model outflow conditions. 

In previous tutorial programs (see for instance step-20 and step-22) we have seen how to solve the time-independent Stokes equations using a Schur complement approach. For the time-dependent case, after time discretization, we would arrive at a system like 

[1.x.1196] 

where  [2.x.3240]  is the time-step. Although the structure of this system is similar to the Stokes system and thus it could be solved using a Schur complement approach, it turns out that the condition number of the Schur complement is proportional to  [2.x.3241] . This makes the system very difficult to solve, and means that for the Navier-Stokes equations, this is not a useful avenue to the solution. 

[1.x.1197] 

[1.x.1198] 

Rather, we need to come up with a different approach to solve the time-dependent Navier-Stokes equations. The difficulty in their solution comes from the fact that the velocity and the pressure are coupled through the constraint 

[1.x.1199] 

for which the pressure is the Lagrange multiplier. Projection methods aim at decoupling this constraint from the diffusion (Laplace) operator. 

Let us shortly describe how the projection methods look like in a semi-discrete setting. The objective is to obtain a sequence of velocities  [2.x.3242]  and pressures  [2.x.3243] . We will also obtain a sequence  [2.x.3244]  of auxiliary variables. Suppose that from the initial conditions, and an application of a first order method we have found  [2.x.3245]  and  [2.x.3246] . Then the projection method consists of the following steps:  [2.x.3247]     [2.x.3248]  [1.x.1200]: Extrapolation. Define:   [1.x.1201] 

   [2.x.3249]  [1.x.1202]: Diffusion step. We find  [2.x.3250]  that solves the single   linear equation   [1.x.1203] 



   [2.x.3251]  [1.x.1204]: Projection. Find  [2.x.3252]  that solves   [1.x.1205] 

   [2.x.3253]  [1.x.1206]: Pressure correction. Here we have two options:      [2.x.3254]         [2.x.3255]  [1.x.1207]. The pressure is updated by:       [1.x.1208] 

       [2.x.3256]  [1.x.1209]. In this case       [1.x.1210] 

     [2.x.3257]   [2.x.3258]  

Without going into details, let us remark a few things about the projection methods that we have just described:  [2.x.3259]     [2.x.3260]  The advection term  [2.x.3261]  is replaced by its [1.x.1211]   [1.x.1212] 

  This is consistent with the continuous equation (because  [2.x.3262] ,   though this is not true pointwise for the discrete solution) and it is needed to   guarantee unconditional stability of the   time-stepping scheme. Moreover, to linearize the term we use the second order extrapolation  [2.x.3263]  of    [2.x.3264] .    [2.x.3265]  The projection step is a realization of the Helmholtz decomposition   [1.x.1213] 

  where   [1.x.1214] 

  and   [1.x.1215] 

  Indeed, if we use this decomposition on  [2.x.3266]  we obtain   [1.x.1216] 

  with  [2.x.3267] . Taking the divergence of this equation we arrive at the projection equation.    [2.x.3268]  The more accurate of the two variants outlined above is the rotational   one. However, the program below implements both variants. Moreover, in the author's experience,   the standard form is the one that should be used if, for instance, the viscosity  [2.x.3269]  is variable.  [2.x.3270]  


 [2.x.3271]  The standard incremental scheme and the rotational incremental scheme were first considered by van Kan in  [2.x.3272]     [2.x.3273]  J. van Kan, "A second-order accurate pressure-correction scheme for viscous incompressible flow",        SIAM Journal on Scientific and Statistical Computing, vol. 7, no. 3, pp. 870–891, 1986  [2.x.3274]  and is analyzed by Guermond in  [2.x.3275]     [2.x.3276]  J.-L. Guermond, "Un résultat de convergence d’ordre deux en temps pour                         l’approximation des équations de Navier–Stokes par une technique de projection incrémentale",        ESAIM: Mathematical Modelling and Numerical Analysis, vol. 33, no. 1, pp. 169–189, 1999  [2.x.3277]  for the case  [2.x.3278] . It turns out that this technique suffers from unphysical boundary conditions for the kinematic pressure that lead to reduced rates of convergence. To prevent this, Timmermans et al. proposed in  [2.x.3279]     [2.x.3280]  L. Timmermans, P. Minev, and F. Van De Vosse,        "An approximate projection scheme for incompressible flow using spectral elements",        International Journal for Numerical Methods in Fluids, vol. 22, no. 7, pp. 673–688, 1996  [2.x.3281]  the rotational pressure-correction projection method that uses a divergence correction for the kinematic pressure. A thorough analysis for scheme has first been performed in  [2.x.3282]     [2.x.3283]  J.-L. Guermond and J. Shen, "On the error estimates for the rotational pressure-correction projection methods",        Mathematics of Computation, vol. 73, no. 248, pp. 1719–1737, 2004  [2.x.3284]  for the Stokes problem.  [2.x.3285]  

[1.x.1217] 

[1.x.1218] To obtain a fully discrete setting of the method we, as always, need a variational formulation. There is one subtle issue here given the nature of the boundary conditions. When we multiply the equation by a suitable test function one of the term that arises is 

[1.x.1219] 

If we, say, had Dirichlet boundary conditions on the whole boundary then after integration by parts we would obtain 

[1.x.1220] 

One of the advantages of this formulation is that it fully decouples the components of the velocity. Moreover, they all share the same system matrix. This can be exploited in the program. 

However, given the nonstandard boundary conditions, to be able to take them into account we need to use the following %identity 

[1.x.1221] 

so that when we integrate by parts and take into account the boundary conditions we obtain 

[1.x.1222] 

which is the form that we would have to use. Notice that this couples the components of the velocity. Moreover, to enforce the boundary condition on the pressure, we need to rewrite 

[1.x.1223] 

where the boundary integral in  [2.x.3286]  equals zero given the boundary conditions for the velocity, and the one in  [2.x.3287]  given the boundary conditions for the pressure. 

In the simplified case where the boundary  [2.x.3288]  is %parallel to a coordinate axis, which holds for the testcase that we carry out below, it can actually be shown that 

[1.x.1224] 

This issue is not very often addressed in the literature. For more information the reader can consult, for instance,  [2.x.3289]     [2.x.3290]  J.-L. GUERMOND, L. QUARTAPELLE, On the approximation of the unsteady Navier-Stokes equations by   finite element projection methods, Numer. Math., 80  (1998) 207-238    [2.x.3291]  J.-L. GUERMOND, P. MINEV, J. SHEN, Error analysis of pressure-correction schemes for the   Navier-Stokes equations with open boundary conditions, SIAM J. Numer. Anal., 43  1 (2005) 239--258.  [2.x.3292]  




[1.x.1225] 

[1.x.1226] 

Our implementation of the projection methods follows [1.x.1227] the description given above. We must note, however, that as opposed to most other problems that have several solution components, we do not use vector-valued finite elements. Instead, we use separate finite elements the components of the velocity and the pressure, respectively, and use different  [2.x.3293] 's for those as well. The main reason for doing this is that, as we see from the description of the scheme, the  [2.x.3294]  components of the velocity and the pressure are decoupled. As a consequence, the equations for all the velocity components look all the same, have the same system matrix, and can be solved in %parallel. Obviously, this approach has also its disadvantages. For instance, we need to keep several  [2.x.3295] s and iterators synchronized when assembling matrices and right hand sides; obtaining quantities that are inherent to vector-valued functions (e.g. divergences) becomes a little awkward, and others. 

[1.x.1228] 

[1.x.1229] 

The testcase that we use for this program consists of the flow around a square obstacle. The geometry is as follows: 

 [2.x.3296]  

with  [2.x.3297] , making the geometry slightly non-symmetric. 

We impose no-slip boundary conditions on both the top and bottom walls and the obstacle. On the left side we have the inflow boundary condition 

[1.x.1230] 

with  [2.x.3298] , i.e. the inflow boundary conditions correspond to Poiseuille flow for this configuration. Finally, on the right vertical wall we impose the condition that the vertical component of the velocity and the pressure should both be zero. The final time  [2.x.3299] . 


examples/step-35/doc/results.dox 

[1.x.1231] 

[1.x.1232] 

[1.x.1233] 

[1.x.1234] 

We run the code with the following  [2.x.3300] , which can be found in the same directory as the source: 

[1.x.1235] 



Since the  [2.x.3301] , we do not get any kind of output besides the number of the time step the program is currently working on. If we were to set it to  [2.x.3302]  we would get information on what the program is doing and how many steps each iterative process had to make to converge, etc. 

Let us plot the obtained results for  [2.x.3303]  (i.e. time steps 200, 1000, 2400, 4000, and 5000), where in the left column we show the vorticity and in the right the velocity field: 

 [2.x.3304]  

The images show nicely the development and extension of a vortex chain behind the obstacles, with the sign of the vorticity indicating whether this is a left or right turning vortex. 


[1.x.1236] 

[1.x.1237] 

We can change the Reynolds number,  [2.x.3305] , in the parameter file to a value of  [2.x.3306] . Doing so, and reducing the time step somewhat as well, yields the following images at times  [2.x.3307] : 

 [2.x.3308]  

For this larger Reynolds number, we observe unphysical oscillations, especially for the vorticity. The discretization scheme has now difficulties in correctly resolving the flow, which should still be laminar and well-organized. These phenomena are typical of discretization schemes that lack robustness in under-resolved scenarios, where under-resolved means that the Reynolds number computed with the mesh size instead of the physical dimensions of the geometry is large. We look at a zoom at the region behind the obstacle, and the mesh size we have there: 


 [2.x.3309]  

We can easily test our hypothesis by re-running the simulation with one more mesh refinement set in the parameter file: 

 [2.x.3310]  

Indeed, the vorticity field now looks much smoother. While we can expect that further refining the mesh will suppress the remaining oscillations as well, one should take measures to obtain a robust scheme in the limit of coarse resolutions, as described below. 


[1.x.1238] 

[1.x.1239] 

This program can be extended in the following directions:  [2.x.3311]     [2.x.3312]  Adaptive mesh refinement: As we have seen, we computed everything on a single fixed mesh.   Using adaptive mesh refinement can lead to increased accuracy while not significantly increasing the   computational time. 

   [2.x.3313]  Adaptive time-stepping: Although there apparently is currently no theory about   projection methods with variable time step,   practice shows that they perform very well. 

   [2.x.3314]  High Reynolds %numbers: As we can see from the results, increasing the Reynolds number changes significantly   the behavior of the discretization scheme. Using well-known stabilization techniques we could be able to   compute the flow in this, or many other problems, when the Reynolds number is very large and where computational   costs demand spatial resolutions for which the flow is only marginally resolved, especially for 3D turbulent   flows. 

   [2.x.3315]  Variable density incompressible flows: There are projection-like methods for the case of incompressible   flows with variable density. Such flows play a role if fluids of different   density mix, for example fresh water and salt water, or alcohol and water. 

   [2.x.3316]  Compressible Navier-Stokes equations: These equations are relevant for   cases where   velocities are high enough so that the fluid becomes compressible, but not   fast enough that we get into a regime where viscosity becomes negligible   and the Navier-Stokes equations need to be replaced by the hyperbolic Euler   equations of gas dynamics. Compressibility starts to become a factor if the   velocity becomes greater than about one third of the speed of sound, so it   is not a factor for almost all terrestrial vehicles. On the other hand,   commercial jetliners fly at about 85 per cent of the speed of sound, and   flow over the wings becomes significantly supersonic, a regime in which the   compressible Navier-Stokes equations are not applicable any more   either. There are significant applications for the range in between,   however, such as for small aircraft or the fast trains in many European and   East Asian countries.  [2.x.3317]  


examples/step-36/doc/intro.dox 

 [2.x.3318]  

[1.x.1240] 

[1.x.1241] 

[1.x.1242] 

The problem we want to solve in this example is an eigenspectrum problem. Eigenvalue problems appear in a wide context of problems, for example in the computation of electromagnetic standing waves in cavities, vibration modes of drum membranes, or oscillations of lakes and estuaries. One of the most enigmatic applications is probably the computation of stationary or quasi-static wave functions in quantum mechanics. The latter application is what we would like to investigate here, though the general techniques outlined in this program are of course equally applicable to the other applications above. 

Eigenspectrum problems have the general form 

[1.x.1243] 

where the Dirichlet boundary condition on  [2.x.3319]  could also be replaced by Neumann or Robin conditions;  [2.x.3320]  is an operator that generally also contains differential operators. 

Under suitable conditions, the above equations have a set of solutions  [2.x.3321] ,  [2.x.3322] , where  [2.x.3323]  can be a finite or infinite set (and in the latter case it may be a discrete or sometimes at least in part a continuous set). In either case, let us note that there is no longer just a single solution, but a set of solutions (the various eigenfunctions and corresponding eigenvalues) that we want to compute. The problem of numerically finding all eigenvalues (eigenfunctions) of such eigenvalue problems is a formidable challenge. In fact, if the set  [2.x.3324]  is infinite, the challenge is of course intractable.  Most of the time however we are really only interested in a small subset of these values (functions); and fortunately, the interface to the SLEPc library that we will use for this tutorial program allows us to select which portion of the eigenspectrum and how many solutions we want to solve for. 

In this program, the eigenspectrum solvers we use are classes provided by deal.II that wrap around the linear algebra implementation of the [1.x.1244] library; SLEPc itself builds on the [1.x.1245] library for linear algebra contents. 

[1.x.1246] 

[1.x.1247] 

The basic equation of stationary quantum mechanics is the Schrödinger equation which models the motion of particles in an external potential  [2.x.3325] . The particle is described by a wave function  [2.x.3326]  that satisfies a relation of the (nondimensionalized) form 

[1.x.1248] 

As a consequence, this particle can only exist in a certain number of eigenstates that correspond to the energy eigenvalues  [2.x.3327]  admitted as solutions of this equation. The orthodox (Copenhagen) interpretation of quantum mechanics posits that, if a particle has energy  [2.x.3328]  then the probability of finding it at location  [2.x.3329]  is proportional to  [2.x.3330]  where  [2.x.3331]  is the eigenfunction that corresponds to this eigenvalue. 

In order to numerically find solutions to this equation, i.e. a set of pairs of eigenvalues/eigenfunctions, we use the usual finite element approach of multiplying the equation from the left with test functions, integrating by parts, and searching for solutions in finite dimensional spaces by approximating  [2.x.3332] , where  [2.x.3333]  is a vector of expansion coefficients. We then immediately arrive at the following equation that discretizes the continuous eigenvalue problem: [1.x.1249] In matrix and vector notation, this equation then reads: [1.x.1250] where  [2.x.3334]  is the stiffness matrix arising from the differential operator  [2.x.3335] , and  [2.x.3336]  is the mass matrix. The solution to the eigenvalue problem is an eigenspectrum  [2.x.3337] , with associated eigenfunctions  [2.x.3338] . 




[1.x.1251] 

In this program, we use Dirichlet boundary conditions for the wave function  [2.x.3339] . What this means, from the perspective of a finite element code, is that only the interior degrees of freedom are real degrees of [1.x.1252]: the ones on the boundary are not free but are forced to have a zero value, after all. On the other hand, the finite element method gains much of its power and simplicity from the fact that we just do the same thing on every cell, without having to think too much about where a cell is, whether it bounds on a less refined cell and consequently has a hanging node, or is adjacent to the boundary. All such checks would make the assembly of finite element linear systems unbearably difficult to write and even more so to read. 

Consequently, of course, when you distribute degrees of freedom with your DoFHandler object, you don't care whether some of the degrees of freedom you enumerate are at a Dirichlet boundary. They all get numbers. We just have to take care of these degrees of freedom at a later time when we apply boundary values. There are two basic ways of doing this (either using  [2.x.3340]  [1.x.1253] assembling the linear system, or using  [2.x.3341]  [1.x.1254] assembly; see the  [2.x.3342]  "constraints module" for more information), but both result in the same: a linear system that has a total number of rows equal to the number of [1.x.1255] degrees of freedom, including those that lie on the boundary. However, degrees of freedom that are constrained by Dirichlet conditions are separated from the rest of the linear system by zeroing out the corresponding row and column, putting a single positive entry on the diagonal, and the corresponding Dirichlet value on the right hand side. 

If you assume for a moment that we had renumbered degrees of freedom in such a way that all of those on the Dirichlet boundary come last, then the linear system we would get when solving a regular PDE with a right hand side would look like this: 

[1.x.1256] 

Here, subscripts  [2.x.3343]  and  [2.x.3344]  correspond to interior and boundary degrees of freedom, respectively. The interior degrees of freedom satisfy the linear system  [2.x.3345]  which yields the correct solution in the interior, and boundary values are determined by  [2.x.3346]  where  [2.x.3347]  is a diagonal matrix that results from the process of eliminating boundary degrees of freedom, and  [2.x.3348]  is chosen in such a way that  [2.x.3349]  has the correct boundary values for every boundary degree of freedom  [2.x.3350] . (For the curious, the entries of the matrix  [2.x.3351]  result from adding modified local contributions to the global matrix where for the local matrices the diagonal elements, if non-zero, are set to their absolute value; otherwise, they are set to the average of absolute values of the diagonal. This process guarantees that the entries of  [2.x.3352]  are positive and of a size comparable to the rest of the diagonal entries, ensuring that the resulting matrix does not incur unreasonable losses of accuracy due to roundoff involving matrix entries of drastically different size. The actual values that end up on the diagonal are difficult to predict and you should treat them as arbitrary and unpredictable, but positive.) 

For "regular" linear systems, this all leads to the correct solution. On the other hand, for eigenvalue problems, this is not so trivial. There, eliminating boundary values affects both matrices  [2.x.3353]  and  [2.x.3354]  that we will solve with in the current tutorial program. After elimination of boundary values, we then receive an eigenvalue problem that can be partitioned like this: 

[1.x.1257] 

This form makes it clear that there are two sets of eigenvalues: the ones we care about, and spurious eigenvalues from the separated problem 

[1.x.1258] 

These eigenvalues are spurious since they result from an eigenvalue system that operates only on boundary nodes -- nodes that are not real degrees of [1.x.1259]. Of course, since the two matrices  [2.x.3355]  are diagonal, we can exactly quantify these spurious eigenvalues: they are  [2.x.3356]  (where the indices  [2.x.3357]  corresponds exactly to the degrees of freedom that are constrained by Dirichlet boundary values). 

So how does one deal with them? The fist part is to recognize when our eigenvalue solver finds one of them. To this end, the program computes and prints an interval within which these eigenvalues lie, by computing the minimum and maximum of the expression  [2.x.3358]  over all constrained degrees of freedom. In the program below, this already suffices: we find that this interval lies outside the set of smallest eigenvalues and corresponding eigenfunctions we are interested in and compute, so there is nothing we need to do here. 

On the other hand, it may happen that we find that one of the eigenvalues we compute in this program happens to be in this interval, and in that case we would not know immediately whether it is a spurious or a true eigenvalue. In that case, one could simply scale the diagonal elements of either matrix after computing the two matrices, thus shifting them away from the frequency of interest in the eigen-spectrum. This can be done by using the following code, making sure that all spurious eigenvalues are exactly equal to  [2.x.3359] : 

[1.x.1260] 

However, this strategy is not pursued here as the spurious eigenvalues we get from our program as-is happen to be greater than the lowest five that we will calculate and are interested in. 




[1.x.1261] 

The program below is essentially just a slightly modified version of step-4. The things that are different are the following: 

 [2.x.3360]  

 [2.x.3361] The main class (named  [2.x.3362] ) now no longer has a single solution vector, but a whole set of vectors for the various eigenfunctions we want to compute. Moreover, the  [2.x.3363]  function, which has the top-level control over everything here, initializes and finalizes the interface to SLEPc and PETSc simultaneously via  [2.x.3364]  and  [2.x.3365] . [2.x.3366]  

 [2.x.3367] We use PETSc matrices and vectors as in step-17 and step-18 since that is what the SLEPc eigenvalue solvers require. [2.x.3368]  

 [2.x.3369] The function  [2.x.3370]  is entirely different from anything seen so far in the tutorial, as it does not just solve a linear system but actually solves the eigenvalue problem. It is built on the SLEPc library, and more immediately on the deal.II SLEPc wrappers in the class  [2.x.3371]  

 [2.x.3372] We use the ParameterHandler class to describe a few input parameters, such as the exact form of the potential  [2.x.3373] , the number of global refinement steps of the mesh, or the number of eigenvalues we want to solve for. We could go much further with this but stop at making only a few of the things that one could select at run time actual input file parameters. In order to see what could be done in this regard, take a look at  [2.x.3374]  "step-29" and step-33. [2.x.3375]  

 [2.x.3376] We use the FunctionParser class to make the potential  [2.x.3377]  a run-time parameter that can be specified in the input file as a formula. [2.x.3378]  

 [2.x.3379]  

The rest of the program follows in a pretty straightforward way from step-4. 


examples/step-36/doc/results.dox 



[1.x.1262] 

[1.x.1263] 

The problem's input is parameterized by an input file  [2.x.3380]  which could, for example, contain the following text: 

[1.x.1264] 



Here, the potential is zero inside the domain, and we know that the eigenvalues are given by  [2.x.3381]  where  [2.x.3382] . Eigenfunctions are sines and cosines with  [2.x.3383]  and  [2.x.3384]  periods in  [2.x.3385]  and  [2.x.3386]  directions. This matches the output our program generates: 

[1.x.1265] These eigenvalues are exactly the ones that correspond to pairs  [2.x.3387] ,  [2.x.3388]  and  [2.x.3389] ,  [2.x.3390] , and  [2.x.3391] . A visualization of the corresponding eigenfunctions would look like this: 

 [2.x.3392]  

[1.x.1266] 

It is always worth playing a few games in the playground! So here goes with a few suggestions: 

 [2.x.3393]  

 [2.x.3394]  The potential used above (called the [1.x.1267] because it is a flat potential surrounded by infinitely high walls) is interesting because it allows for analytically known solutions. Apart from that, it is rather boring, however. That said, it is trivial to play around with the potential by just setting it to something different in the input file. For example, let us assume that we wanted to work with the following potential in 2d: 

[1.x.1268] 

In other words, the potential is -100 in two sectors of a circle of radius 0.75, -5 in the other two sectors, and zero outside the circle. We can achieve this by using the following in the input file: 

[1.x.1269] 

If in addition we also increase the mesh refinement by one level, we get the following results: 

[1.x.1270] 



The output file also contains an interpolated version of the potential, which looks like this (note that as expected the lowest few eigenmodes have probability densities  [2.x.3395]  that are significant only where the potential is the lowest, i.e. in the top right and bottom left sector of inner circle of the potential): 

 [2.x.3396]  

The first five eigenfunctions are now like this: 

 [2.x.3397]  

 [2.x.3398]  In our derivation of the problem we have assumed that the particle is confined to a domain  [2.x.3399]  and that at the boundary of this domain its probability  [2.x.3400]  of being is zero. This is equivalent to solving the eigenvalue problem on all of  [2.x.3401]  and assuming that the energy potential is finite only inside a region  [2.x.3402]  and infinite outside. It is relatively easy to show that  [2.x.3403]  at all locations  [2.x.3404]  where  [2.x.3405] . So the question is what happens if our potential is not of this form, i.e. there is no bounded domain outside of which the potential is infinite? In that case, it may be worth to just consider a very large domain at the boundary of which  [2.x.3406]  is at least very large, if not infinite. Play around with a few cases like this and explore how the spectrum and eigenfunctions change as we make the computational region larger and larger. 

 [2.x.3407]  What happens if we investigate the simple harmonic oscillator problem  [2.x.3408] ? This potential is exactly of the form discussed in the previous paragraph and has hyper spherical symmetry. One may want to use a large spherical domain with a large outer radius, to approximate the whole-space problem (say, by invoking  [2.x.3409]  

 [2.x.3410]  The plots above show the wave function  [2.x.3411] , but the physical quantity of interest is actually the probability density  [2.x.3412]  for the particle to be at location  [2.x.3413] . Some visualization programs can compute derived quantities from the data in an input file, but we can also do so right away when creating the output file. The facility to do that is the DataPostprocessor class that can be used in conjunction with the DataOut class. Examples of how this can be done can be found in step-29 and step-33. 

 [2.x.3414]  What happens if the particle in the box has %internal degrees of freedom? For example, if the particle were a spin- [2.x.3415]  particle? In that case, we may want to start solving a vector-valued problem instead. 

 [2.x.3416]  Our implementation of the deal.II library here uses the PETScWrappers and SLEPcWrappers and is suitable for running on serial machine architecture. However, for larger grids and with a larger number of degrees-of-freedom, we may want to run our application on parallel architectures. A parallel implementation of the above code can be particularly useful here since the generalized eigenspectrum problem is somewhat more expensive to solve than the standard problems considered in most of the earlier tutorials. Fortunately, modifying the above program to be MPI compliant is a relatively straightforward procedure. A sketch of how this can be done can be found in  [2.x.3417]  "step-17". 

 [2.x.3418]  Finally, there are alternatives to using the SLEPc eigenvalue solvers. deal.II has interfaces to one of them, ARPACK (see [1.x.1271] for setup instructions), implemented in the ArpackSolver class. Here is a short and quick overview of what one would need to change to use it, provided you have a working installation of ARPACK and deal.II has been configured properly for it (see the deal.II [1.x.1272] file): 

First, in order to use the ARPACK interfaces, we can go back to using standard deal.II matrices and vectors, so we start by replacing the PETSc and SLEPc headers 

[1.x.1273] 

with these: 

[1.x.1274] 

ARPACK allows complex eigenvalues, so we will also need 

[1.x.1275] 



Secondly, we switch back to the deal.II matrix and vector definitions in the main class: 

[1.x.1276] 

and initialize them as usual in  [2.x.3419] : 

[1.x.1277] 



For solving the eigenvalue problem with ARPACK, we finally need to modify  [2.x.3420] : 

[1.x.1278] 

Note how we have used an exact decomposition (using SparseDirectUMFPACK) as a preconditioner to ARPACK.  [2.x.3421]  


examples/step-37/doc/intro.dox 

 [2.x.3422]  

[1.x.1279][1.x.1280] 

[1.x.1281] 

[1.x.1282] 

This example shows how to implement a matrix-free method, that is, a method that does not explicitly store the matrix elements, for a second-order Poisson equation with variable coefficients on a hypercube. The linear system will be solved with a multigrid method and uses large-scale parallelism with MPI. 

The major motivation for matrix-free methods is the fact that on today's processors access to main memory (i.e., for objects that do not fit in the caches) has become the bottleneck in many solvers for partial differential equations: To perform a matrix-vector product based on matrices, modern CPUs spend far more time waiting for data to arrive from memory than on actually doing the floating point multiplications and additions. Thus, if we could substitute looking up matrix elements in memory by re-computing them &mdash; or rather, the operator represented by these entries &mdash;, we may win in terms of overall run-time even if this requires a significant number of additional floating point operations. That said, to realize this with a trivial implementation is not enough and one needs to really look at the details to gain in performance. This tutorial program and the papers referenced above show how one can implement such a scheme and demonstrates the speedup that can be obtained. 




[1.x.1283] 

In this example, we consider the Poisson problem [1.x.1284] where  [2.x.3423]  is a variable coefficient. Below, we explain how to implement a matrix-vector product for this problem without explicitly forming the matrix. The construction can, of course, be done in a similar way for other equations as well. 

We choose as domain  [2.x.3424]  and  [2.x.3425] . Since the coefficient is symmetric around the origin but the domain is not, we will end up with a non-symmetric solution. 




[1.x.1285] 

In order to find out how we can write a code that performs a matrix-vector product, but does not need to store the matrix elements, let us start at looking how a finite element matrix [1.x.1286] is assembled: 

[1.x.1287] 

In this formula, the matrix [1.x.1288]<sub>cell,loc-glob</sub> is a rectangular matrix that defines the index mapping from local degrees of freedom in the current cell to the global degrees of freedom. The information from which this operator can be built is usually encoded in the  [2.x.3426]  variable and is used in the assembly calls filling matrices in deal.II. Here, [1.x.1289]<sub>cell</sub> denotes the cell matrix associated with [1.x.1290]. 

If we are to perform a matrix-vector product, we can hence use that 

[1.x.1291] 

where [1.x.1292]<sub>cell</sub> are the values of [1.x.1293] at the degrees of freedom of the respective cell, and [1.x.1294]<sub>cell</sub>=[1.x.1295]<sub>cell</sub>[1.x.1296]<sub>cell</sub> correspondingly for the result. A naive attempt to implement the local action of the Laplacian would hence be to use the following code: 

[1.x.1297] 



Here we neglected boundary conditions as well as any hanging nodes we may have, though neither would be very difficult to include using the AffineConstraints class. Note how we first generate the local matrix in the usual way as a sum over all quadrature points for each local matrix entry. To form the actual product as expressed in the above formula, we extract the values of  [2.x.3427]  of the cell-related degrees of freedom (the action of [1.x.1298]<sub>cell,loc-glob</sub>), multiply by the local matrix (the action of [1.x.1299]<sub>cell</sub>), and finally add the result to the destination vector  [2.x.3428]  (the action of [1.x.1300]<sub>cell,loc-glob</sub><sup>T</sup>, added over all the elements). It is not more difficult than that, in principle. 

While this code is completely correct, it is very slow. For every cell, we generate a local matrix, which takes three nested loops with loop length equal to the number of local degrees of freedom to compute. The multiplication itself is then done by two nested loops, which means that it is much cheaper. 

One way to improve this is to realize that conceptually the local matrix can be thought of as the product of three matrices, 

[1.x.1301] 

where for the example of the Laplace operator the ([1.x.1302]*dim+[1.x.1303])-th element of [1.x.1304]<sub>cell</sub> is given by  [2.x.3429] . This matrix consists of  [2.x.3430]  rows and  [2.x.3431]  columns. The matrix [1.x.1305]<sub>cell</sub> is diagonal and contains the values  [2.x.3432]  (or, rather,  [2.x.3433]  dim copies of each of these values). This kind of representation of finite element matrices can often be found in the engineering literature. 

When the cell matrix is applied to a vector, 

[1.x.1306] 

one would then not form the matrix-matrix products, but rather multiply one matrix at a time with a vector from right to left so that only three successive matrix-vector products are formed. This approach removes the three nested loops in the calculation of the local matrix, which reduces the complexity of the work on one cell from something like  [2.x.3434]  to  [2.x.3435] . An interpretation of this algorithm is that we first transform the vector of values on the local DoFs to a vector of gradients on the quadrature points. In the second loop, we multiply these gradients by the integration weight and the coefficient. The third loop applies the second gradient (in transposed form), so that we get back to a vector of (Laplacian) values on the cell dofs. 

The bottleneck in the above code is the operations done by the call to  [2.x.3436]  for every  [2.x.3437] , which take about as much time as the other steps together (at least if the mesh is unstructured; deal.II can recognize that the gradients are often unchanged on structured meshes). That is certainly not ideal and we would like to do better than this. What the reinit function does is to calculate the gradient in real space by transforming the gradient on the reference cell using the Jacobian of the transformation from real to reference cell. This is done for each basis function on the cell, for each quadrature point. The Jacobian does not depend on the basis function, but it is different on different quadrature points in general. If you only build the matrix once as we've done in all previous tutorial programs, there is nothing to be optimized since  [2.x.3438]  needs to be called on every cell. In this process, the transformation is applied while computing the local matrix elements. 

In a matrix-free implementation, however, we will compute those integrals very often because iterative solvers will apply the matrix many times during the solution process. Therefore, we need to think about whether we may be able to cache some data that gets reused in the operator applications, i.e., integral computations. On the other hand, we realize that we must not cache too much data since otherwise we get back to the situation where memory access becomes the dominating factor. Therefore, we will not store the transformed gradients in the matrix [1.x.1307], as they would in general be different for each basis function and each quadrature point on every element for curved meshes. 

The trick is to factor out the Jacobian transformation and first apply the gradient on the reference cell only. This operation interpolates the vector of values on the local dofs to a vector of (unit-coordinate) gradients on the quadrature points. There, we first apply the Jacobian that we factored out from the gradient, then apply the weights of the quadrature, and finally apply the transposed Jacobian for preparing the third loop which tests by the gradients on the unit cell and sums over quadrature points. 

Let us again write this in terms of matrices. Let the matrix [1.x.1308]<sub>cell</sub> denote the cell-related gradient matrix, with each row containing the values on the quadrature points. It is constructed by a matrix-matrix product as [1.x.1309] where [1.x.1310]<sub>ref_cell</sub> denotes the gradient on the reference cell and [1.x.1311]<sup>-T</sup><sub>cell</sub> denotes the inverse transpose Jacobian of the transformation from unit to real cell (in the language of transformations, the operation represented by [1.x.1312]<sup>-T</sup><sub>cell</sub> represents a covariant transformation). [1.x.1313]<sup>-T</sup><sub>cell</sub> is block-diagonal, and the blocks size is equal to the dimension of the problem. Each diagonal block is the Jacobian transformation that goes from the reference cell to the real cell. 

Putting things together, we find that 

[1.x.1314] 

so we calculate the product (starting the local product from the right) 

[1.x.1315] 



[1.x.1316] 



Note how we create an additional FEValues object for the reference cell gradients and how we initialize it to the reference cell. The actual derivative data is then applied by the inverse, transposed Jacobians (deal.II calls the Jacobian matrix from real to unit cell inverse_jacobian, as the forward transformation is from unit to real cell). The factor  [2.x.3439]  is block-diagonal over quadrature. In this form, one realizes that variable coefficients (possibly expressed through a tensor) and general grid topologies with Jacobian transformations have a similar effect on the coefficient transforming the unit-cell derivatives. 

At this point, one might wonder why we store the matrix  [2.x.3440]  and the coefficient separately, rather than only the complete factor  [2.x.3441] . The latter would use less memory because the tensor is symmetric with six independent values in 3D, whereas for the former we would need nine entries for the inverse transposed Jacobian, one for the quadrature weight and Jacobian determinant, and one for the coefficient, totaling to 11 doubles. The reason is that the former approach allows for implementing generic differential operators through a common framework of cached data, whereas the latter specifically stores the coefficient for the Laplacian. In case applications demand for it, this specialization could pay off and would be worthwhile to consider. Note that the implementation in deal.II is smart enough to detect Cartesian or affine geometries where the Jacobian is constant throughout the cell and needs not be stored for every cell (and indeed often is the same over different cells as well). 

The final optimization that is most crucial from an operation count point of view is to make use of the tensor product structure in the basis functions. This is possible because we have factored out the gradient from the reference cell operation described by [1.x.1317]<sub>ref_cell</sub>, i.e., an interpolation operation over the completely regular data fields of the reference cell. We illustrate the process of complexity reduction in two space dimensions, but the same technique can be used in higher dimensions. On the reference cell, the basis functions are of the tensor product form  [2.x.3442] . The part of the matrix [1.x.1318]<sub>ref_cell</sub> that computes the first component has the form  [2.x.3443] , where [1.x.1319]<sub>grad,x</sub> and [1.x.1320]<sub>val,y</sub> contain the evaluation of all the 1D basis functions on all the 1D quadrature points. Forming a matrix [1.x.1321] with [1.x.1322] containing the coefficient belonging to basis function  [2.x.3444] , we get  [2.x.3445] . This reduces the complexity for computing this product from  [2.x.3446]  to  [2.x.3447] , where [1.x.1323]-1 is the degree of the finite element (i.e., equivalently, [1.x.1324] is the number of shape functions in each coordinate direction), or  [2.x.3448]  to  [2.x.3449]  in general. The reason why we look at the complexity in terms of the polynomial degree is since we want to be able to go to high degrees and possibly increase the polynomial degree [1.x.1325] instead of the grid resolution. Good algorithms for moderate degrees like the ones used here are linear in the polynomial degree independent on the dimension, as opposed to matrix-based schemes or naive evaluation through FEValues. The techniques used in the implementations of deal.II have been established in the spectral element community since the 1980s. 

Implementing a matrix-free and cell-based finite element operator requires a somewhat different program design as compared to the usual matrix assembly codes shown in previous tutorial programs. The data structures for doing this are the MatrixFree class that collects all data and issues a (parallel) loop over all cells and the FEEvaluation class that evaluates finite element basis functions by making use of the tensor product structure. 

The implementation of the matrix-free matrix-vector product shown in this tutorial is slower than a matrix-vector product using a sparse matrix for linear elements, but faster for all higher order elements thanks to the reduced complexity due to the tensor product structure and due to less memory transfer during computations. The impact of reduced memory transfer is particularly beneficial when working on a multicore processor where several processing units share access to memory. In that case, an algorithm which is computation bound will show almost perfect parallel speedup (apart from possible changes of the processor's clock frequency through turbo modes depending on how many cores are active), whereas an algorithm that is bound by memory transfer might not achieve similar speedup (even when the work is perfectly parallel and one could expect perfect scaling like in sparse matrix-vector products). An additional gain with this implementation is that we do not have to build the sparse matrix itself, which can also be quite expensive depending on the underlying differential equation. Moreover, the above framework is simple to generalize to nonlinear operations, as we demonstrate in step-48. 




[1.x.1326] 

Above, we have gone to significant lengths to implement a matrix-vector product that does not actually store the matrix elements. In many user codes, however, one wants more than just doing a few matrix-vector products &mdash; one wants to do as few of these operations as possible when solving linear systems. In theory, we could use the CG method without preconditioning; however, that would not be very efficient for the Laplacian. Rather, preconditioners are used for increasing the speed of convergence. Unfortunately, most of the more frequently used preconditioners such as SSOR, ILU or algebraic multigrid (AMG) cannot be used here because their implementation requires knowledge of the elements of the system matrix. 

One solution is to use geometric multigrid methods as shown in step-16. They are known to be very fast, and they are suitable for our purpose since all ingredients, including the transfer between different grid levels, can be expressed in terms of matrix-vector products related to a collection of cells. All one needs to do is to find a smoother that is based on matrix-vector products rather than all the matrix entries. One such candidate would be a damped Jacobi iteration that requires access to the matrix diagonal, but it is often not sufficiently good in damping all high-frequency errors. The properties of the Jacobi method can be improved by iterating it a few times with the so-called Chebyshev iteration. The Chebyshev iteration is described by a polynomial expression of the matrix-vector product where the coefficients can be chosen to achieve certain properties, in this case to smooth the high-frequency components of the error which are associated to the eigenvalues of the Jacobi-preconditioned matrix. At degree zero, the Jacobi method with optimal damping parameter is retrieved, whereas higher order corrections are used to improve the smoothing properties. The effectiveness of Chebyshev smoothing in multigrid has been demonstrated, e.g., in the article [1.x.1327][1.x.1328]. This publication also identifies one more advantage of Chebyshev smoothers that we exploit here, namely that they are easy to parallelize, whereas SOR/Gauss&ndash;Seidel smoothing relies on substitutions, for which a naive parallelization works on diagonal sub-blocks of the matrix, thereby decreases efficiency (for more detail see e.g. Y. Saad, Iterative Methods for Sparse Linear Systems, SIAM, 2nd edition, 2003, chapters 11 & 12). 

The implementation into the multigrid framework is then straightforward. The multigrid implementation in this program is similar to step-16 and includes adaptivity. 




[1.x.1329] 

The computational kernels for evaluation in FEEvaluation are written in a way to optimally use computational resources. To achieve this, they do not operate on double data types, but something we call VectorizedArray (check e.g. the return type of  [2.x.3450]  which is VectorizedArray for a scalar element and a Tensor of VectorizedArray for a vector finite element). VectorizedArray is a short array of doubles or float whose length depends on the particular computer system in use. For example, systems based on x86-64 support the streaming SIMD extensions (SSE), where the processor's vector units can process two doubles (or four single-precision floats) by one CPU instruction. Newer processors (from about 2012 and onwards) support the so-called advanced vector extensions (AVX) with 256 bit operands, which can use four doubles and eight floats, respectively. Vectorization is a single-instruction/multiple-data (SIMD) concept, that is, one CPU instruction is used to process multiple data values at once. Often, finite element programs do not use vectorization explicitly as the benefits of this concept are only in arithmetic intensive operations. The bulk of typical finite element workloads are memory bandwidth limited (operations on sparse matrices and vectors) where the additional computational power is useless. 

Behind the scenes, optimized BLAS packages might heavily rely on vectorization, though. Also, optimizing compilers might automatically transform loops involving standard code into more efficient vectorized form (deal.II uses OpenMP SIMD pragmas inside the regular loops of vector updates). However, the data flow must be very regular in order for compilers to produce efficient code. For example, already the automatic vectorization of the prototype operation that benefits from vectorization, matrix-matrix products, fails on most compilers (as of writing this tutorial in early 2012 and updating in late 2016, neither gcc nor the Intel compiler manage to produce useful vectorized code for the  [2.x.3451]  function, and not even on the simpler case where the matrix bounds are compile-time constants instead of run-time constants as in  [2.x.3452]  The main reason for this is that the information to be processed at the innermost loop (that is where vectorization is applied) is not necessarily a multiple of the vector length, leaving parts of the resources unused. Moreover, the data that can potentially be processed together might not be laid out in a contiguous way in memory or not with the necessary alignment to address boundaries that are needed by the processor. Or the compiler might not be able to prove that data arrays do not overlap when loading several elements at once. 

In the matrix-free implementation in deal.II, we have therefore chosen to apply vectorization at the level which is most appropriate for finite element computations: The cell-wise computations are typically exactly the same for all cells (except for indices in the indirect addressing used while reading from and writing to vectors), and hence SIMD can be used to process several cells at once. In all what follows, you can think of a VectorizedArray to hold data from several cells. Remember that it is not related to the spatial dimension and the number of elements e.g. in a Tensor or Point. 

Note that vectorization depends on the CPU the code is running on and for which the code is compiled. In order to generate the fastest kernels of FEEvaluation for your computer, you should compile deal.II with the so-called [1.x.1330] processor variant. When using the gcc compiler, it can be enabled by setting the variable <tt>CMAKE_CXX_FLAGS</tt> to <tt>"-march=native"</tt> in the cmake build settings (on the command line, specify <tt>-DCMAKE_CXX_FLAGS="-march=native"</tt>, see the deal.II README for more information). Similar options exist for other compilers. We output the current vectorization length in the run() function of this example. 




[1.x.1331] 

As mentioned above, all components in the matrix-free framework can easily be parallelized with MPI using domain decomposition. Thanks to the easy access to large-scale parallel meshes through p4est (see step-40 for details) in deal.II, and the fact that cell-based loops with matrix-free evaluation [1.x.1332] need a decomposition of the mesh into chunks of roughly equal size on each processor, there is relatively little to do to write a parallel program working with distributed memory. While other tutorial programs using MPI have relied on either PETSc or Trilinos, this program uses deal.II's own parallel vector facilities. 

The deal.II parallel vector class,  [2.x.3453]  holds the processor-local part of the solution as well as data fields for ghosted DoFs, i.e. DoFs that are owned by a remote processor but accessed by cells that are owned by the present processor. In the  [2.x.3454]  "glossary" these degrees of freedom are referred to as locally active degrees of freedom. The function  [2.x.3455]  provides a method that sets this design. Note that hanging nodes can relate to additional ghosted degrees of freedom that must be included in the distributed vector but are not part of the locally active DoFs in the sense of the  [2.x.3456]  "glossary". Moreover, the distributed vector holds the MPI metadata for DoFs that are owned locally but needed by other processors. A benefit of the design of this vector class is the way ghosted entries are accessed. In the storage scheme of the vector, the data array extends beyond the processor-local part of the solution with further vector entries available for the ghosted degrees of freedom. This gives a contiguous index range for all locally active degrees of freedom. (Note that the index range depends on the exact configuration of the mesh.) Since matrix-free operations can be thought of doing linear algebra that is performance critical, and performance-critical code cannot waste time on doing MPI-global to MPI-local index translations, the availability of an index spaces local to one MPI rank is fundamental. The way things are accessed here is a direct array access. This is provided through  [2.x.3457]  but it is actually rarely needed because all of this happens internally in FEEvaluation. 

The design of  [2.x.3458]  is similar to the  [2.x.3459]  and  [2.x.3460]  data types we have used in step-40 and step-32 before, but since we do not need any other parallel functionality of these libraries, we use the  [2.x.3461]  class of deal.II instead of linking in another large library in this tutorial program. Also note that the PETSc and Trilinos vectors do not provide the fine-grained control over ghost entries with direct array access because they abstract away the necessary implementation details. 


examples/step-37/doc/results.dox 



[1.x.1333] 

[1.x.1334] 

Since this example solves the same problem as step-5 (except for a different coefficient), there is little to say about the solution. We show a picture anyway, illustrating the size of the solution through both isocontours and volume rendering: 

 [2.x.3462]  

Of more interest is to evaluate some aspects of the multigrid solver. When we run this program in 2D for quadratic ( [2.x.3463] ) elements, we get the following output (when run on one core in release mode): 

[1.x.1335] 



As in step-16, we see that the number of CG iterations remains constant with increasing number of degrees of freedom. A constant number of iterations (together with optimal computational properties) means that the computing time approximately quadruples as the problem size quadruples from one cycle to the next. The code is also very efficient in terms of storage. Around 2-4 million degrees of freedom fit into 1 GB of memory, see also the MPI results below. An interesting fact is that solving one linear system is cheaper than the setup, despite not building a matrix (approximately half of which is spent in the  [2.x.3464]  and  [2.x.3465]  calls). This shows the high efficiency of this approach, but also that the deal.II data structures are quite expensive to set up and the setup cost must be amortized over several system solves. 

Not much changes if we run the program in three spatial dimensions. Since we use uniform mesh refinement, we get eight times as many elements and approximately eight times as many degrees of freedom with each cycle: 

[1.x.1336] 



Since it is so easy, we look at what happens if we increase the polynomial degree. When selecting the degree as four in 3D, i.e., on  [2.x.3466]  elements, by changing the line <code>const unsigned int degree_finite_element=4;</code> at the top of the program, we get the following program output: 

[1.x.1337] 



Since  [2.x.3467]  elements on a certain mesh correspond to  [2.x.3468]  elements on half the mesh size, we can compare the run time at cycle 4 with fourth degree polynomials with cycle 5 using quadratic polynomials, both at 2.1 million degrees of freedom. The surprising effect is that the solver for  [2.x.3469]  element is actually slightly faster than for the quadratic case, despite using one more linear iteration. The effect that higher-degree polynomials are similarly fast or even faster than lower degree ones is one of the main strengths of matrix-free operator evaluation through sum factorization, see the [1.x.1338]. This is fundamentally different to matrix-based methods that get more expensive per unknown as the polynomial degree increases and the coupling gets denser. 

In addition, also the setup gets a bit cheaper for higher order, which is because fewer elements need to be set up. 

Finally, let us look at the timings with degree 8, which corresponds to another round of mesh refinement in the lower order methods: 

[1.x.1339] 



Here, the initialization seems considerably slower than before, which is mainly due to the computation of the diagonal of the matrix, which actually computes a 729 x 729 matrix on each cell and throws away everything but the diagonal. The solver times, however, are again very close to the quartic case, showing that the linear increase with the polynomial degree that is theoretically expected is almost completely offset by better computational characteristics and the fact that higher order methods have a smaller share of degrees of freedom living on several cells that add to the evaluation complexity. 

[1.x.1340] 

In order to understand the capabilities of the matrix-free implementation, we compare the performance of the 3d example above with a sparse matrix implementation based on  [2.x.3470]  by measuring both the computation times for the initialization of the problem (distribute DoFs, setup and assemble matrices, setup multigrid structures) and the actual solution for the matrix-free variant and the variant based on sparse matrices. We base the preconditioner on float numbers and the actual matrix and vectors on double numbers, as shown above. Tests are run on an Intel Core i7-5500U notebook processor (two cores and [1.x.1341] support, i.e., four operations on doubles can be done with one CPU instruction, which is heavily used in FEEvaluation), optimized mode, and two MPI ranks. 

 [2.x.3471]  

The table clearly shows that the matrix-free implementation is more than twice as fast for the solver, and more than six times as fast when it comes to initialization costs. As the problem size is made a factor 8 larger, we note that the times usually go up by a factor eight, too (as the solver iterations are constant at six). The main deviation is in the sparse matrix between 5k and 36k degrees of freedom, where the time increases by a factor 12. This is the threshold where the (L3) cache in the processor can no longer hold all data necessary for the matrix-vector products and all matrix elements must be fetched from main memory. 

Of course, this picture does not necessarily translate to all cases, as there are problems where knowledge of matrix entries enables much better solvers (as happens when the coefficient is varying more strongly than in the above example). Moreover, it also depends on the computer system. The present system has good memory performance, so sparse matrices perform comparably well. Nonetheless, the matrix-free implementation gives a nice speedup already for the [1.x.1342]<sub>2</sub> elements used in this example. This becomes particularly apparent for time-dependent or nonlinear problems where sparse matrices would need to be reassembled over and over again, which becomes much easier with this class. And of course, thanks to the better complexity of the products, the method gains increasingly larger advantages when the order of the elements increases (the matrix-free implementation has costs 4[1.x.1343]<sup>2</sup>[1.x.1344] per degree of freedom, compared to 2[1.x.1345] for the sparse matrix, so it will win anyway for order 4 and higher in 3d). 

[1.x.1346] 

As explained in the introduction and the in-code comments, this program can be run in parallel with MPI. It turns out that geometric multigrid schemes work really well and can scale to very large machines. To the authors' knowledge, the geometric multigrid results shown here are the largest computations done with deal.II as of late 2016, run on up to 147,456 cores of the [1.x.1347]. The ingredients for scalability beyond 1000 cores are that no data structure that depends on the global problem size is held in its entirety on a single processor and that the communication is not too frequent in order not to run into latency issues of the network.  For PDEs solved with iterative solvers, the communication latency is often the limiting factor, rather than the throughput of the network. For the example of the SuperMUC system, the point-to-point latency between two processors is between 1e-6 and 1e-5 seconds, depending on the proximity in the MPI network. The matrix-vector products with  [2.x.3472]  from this class involves several point-to-point communication steps, interleaved with computations on each core. The resulting latency of a matrix-vector product is around 1e-4 seconds. Global communication, for example an  [2.x.3473]  operation that accumulates the sum of a single number per rank over all ranks in the MPI network, has a latency of 1e-4 seconds. The multigrid V-cycle used in this program is also a form of global communication. Think about the coarse grid solve that happens on a single processor: It accumulates the contributions from all processors before it starts. When completed, the coarse grid solution is transferred to finer levels, where more and more processors help in smoothing until the fine grid. Essentially, this is a tree-like pattern over the processors in the network and controlled by the mesh. As opposed to the  [2.x.3474]  operations where the tree in the reduction is optimized to the actual links in the MPI network, the multigrid V-cycle does this according to the partitioning of the mesh. Thus, we cannot expect the same optimality. Furthermore, the multigrid cycle is not simply a walk up and down the refinement tree, but also communication on each level when doing the smoothing. In other words, the global communication in multigrid is more challenging and related to the mesh that provides less optimization opportunities. The measured latency of the V-cycle is between 6e-3 and 2e-2 seconds, i.e., the same as 60 to 200 MPI_Allreduce operations. 

The following figure shows a scaling experiments on  [2.x.3475]  elements. Along the lines, the problem size is held constant as the number of cores is increasing. When doubling the number of cores, one expects a halving of the computational time, indicated by the dotted gray lines. The results show that the implementation shows almost ideal behavior until an absolute time of around 0.1 seconds is reached. The solver tolerances have been set such that the solver performs five iterations. This way of plotting data is the [1.x.1348] of the algorithm. As we go to very large core counts, the curves flatten out a bit earlier, which is because of the communication network in SuperMUC where communication between processors farther away is slightly slower. 

 [2.x.3476]  

In addition, the plot also contains results for [1.x.1349] that lists how the algorithm behaves as both the number of processor cores and elements is increased at the same pace. In this situation, we expect that the compute time remains constant. Algorithmically, the number of CG iterations is constant at 5, so we are good from that end. The lines in the plot are arranged such that the top left point in each data series represents the same size per processor, namely 131,072 elements (or approximately 3.5 million degrees of freedom per core). The gray lines indicating ideal strong scaling are by the same factor of 8 apart. The results show again that the scaling is almost ideal. The parallel efficiency when going from 288 cores to 147,456 cores is at around 75% for a local problem size of 750,000 degrees of freedom per core which takes 1.0s on 288 cores, 1.03s on 2304 cores, 1.19s on 18k cores, and 1.35s on 147k cores. The algorithms also reach a very high utilization of the processor. The largest computation on 147k cores reaches around 1.7 PFLOPs/s on SuperMUC out of an arithmetic peak of 3.2 PFLOPs/s. For an iterative PDE solver, this is a very high number and significantly more is often only reached for dense linear algebra. Sparse linear algebra is limited to a tenth of this value. 

As mentioned in the introduction, the matrix-free method reduces the memory consumption of the data structures. Besides the higher performance due to less memory transfer, the algorithms also allow for very large problems to fit into memory. The figure below shows the computational time as we increase the problem size until an upper limit where the computation exhausts memory. We do this for 1k cores, 8k cores, and 65k cores and see that the problem size can be varied over almost two orders of magnitude with ideal scaling. The largest computation shown in this picture involves 292 billion ( [2.x.3477] ) degrees of freedom. On a DG computation of 147k cores, the above algorithms were also run involving up to 549 billion (2^39) DoFs. 

 [2.x.3478]  

Finally, we note that while performing the tests on the large-scale system shown above, improvements of the multigrid algorithms in deal.II have been developed. The original version contained the sub-optimal code based on MGSmootherPrecondition where some MPI_Allreduce commands (checking whether all vector entries are zero) were done on each smoothing operation on each level, which only became apparent on 65k cores and more. However, the following picture shows that the improvement already pay off on a smaller scale, here shown on computations on up to 14,336 cores for  [2.x.3479]  elements: 

 [2.x.3480]  




[1.x.1350] 

As explained in the code, the algorithm presented here is prepared to run on adaptively refined meshes. If only part of the mesh is refined, the multigrid cycle will run with local smoothing and impose Dirichlet conditions along the interfaces which differ in refinement level for smoothing through the  [2.x.3481]  class. Due to the way the degrees of freedom are distributed over levels, relating the owner of the level cells to the owner of the first descendant active cell, there can be an imbalance between different processors in MPI, which limits scalability by a factor of around two to five. 

[1.x.1351] 

[1.x.1352] 

As mentioned above the code is ready for locally adaptive h-refinement. For the Poisson equation one can employ the Kelly error indicator, implemented in the KellyErrorEstimator class. However one needs to be careful with the ghost indices of parallel vectors. In order to evaluate the jump terms in the error indicator, each MPI process needs to know locally relevant DoFs. However  [2.x.3482]  function initializes the vector only with some locally relevant DoFs. The ghost indices made available in the vector are a tight set of only those indices that are touched in the cell integrals (including constraint resolution). This choice has performance reasons, because sending all locally relevant degrees of freedom would be too expensive compared to the matrix-vector product. Consequently the solution vector as-is is not suitable for the KellyErrorEstimator class. The trick is to change the ghost part of the partition, for example using a temporary vector and  [2.x.3483]  as shown below. 

[1.x.1353] 



[1.x.1354] 

This program is parallelized with MPI only. As an alternative, the MatrixFree loop can also be issued in hybrid mode, for example by using MPI parallelizing over the nodes of a cluster and with threads through Intel TBB within the shared memory region of one node. To use this, one would need to both set the number of threads in the MPI_InitFinalize data structure in the main function, and set the  [2.x.3484]  to partition_color to actually do the loop in parallel. This use case is discussed in step-48. 

[1.x.1355] 

The presented program assumes homogeneous Dirichlet boundary conditions. When going to non-homogeneous conditions, the situation is a bit more intricate. To understand how to implement such a setting, let us first recall how these arise in the mathematical formulation and how they are implemented in a matrix-based variant. In essence, an inhomogeneous Dirichlet condition sets some of the nodal values in the solution to given values rather than determining them through the variational principles, 

[1.x.1356] 

where  [2.x.3485]  denotes the nodal values of the solution and  [2.x.3486]  denotes the set of all nodes. The set  [2.x.3487]  is the subset of the nodes that are subject to Dirichlet boundary conditions where the solution is forced to equal  [2.x.3488]  as the interpolation of boundary values on the Dirichlet-constrained node points  [2.x.3489] . We then insert this solution representation into the weak form, e.g. the Laplacian shown above, and move the known quantities to the right hand side: 

[1.x.1357] 

In this formula, the equations are tested for all basis functions  [2.x.3490]  with  [2.x.3491]  that are not related to the nodes constrained by Dirichlet conditions. 

In the implementation in deal.II, the integrals  [2.x.3492]  on the right hand side are already contained in the local matrix contributions we assemble on each cell. When using  [2.x.3493]  as first described in the step-6 and step-7 tutorial programs, we can account for the contribution of inhomogeneous constraints [1.x.1358] by multiplying the columns [1.x.1359] and rows [1.x.1360] of the local matrix according to the integrals  [2.x.3494]  by the inhomogeneities and subtracting the resulting from the position [1.x.1361] in the global right-hand-side vector, see also the  [2.x.3495]  module. In essence, we use some of the integrals that get eliminated from the left hand side of the equation to finalize the right hand side contribution. Similar mathematics are also involved when first writing all entries into a left hand side matrix and then eliminating matrix rows and columns by  [2.x.3496]  

In principle, the components that belong to the constrained degrees of freedom could be eliminated from the linear system because they do not carry any information. In practice, in deal.II we always keep the size of the linear system the same to avoid handling two different numbering systems and avoid confusion about the two different index sets. In order to ensure that the linear system does not get singular when not adding anything to constrained rows, we then add dummy entries to the matrix diagonal that are otherwise unrelated to the real entries. 

In a matrix-free method, we need to take a different approach, since the  [2.x.3497]  LaplaceOperator class represents the matrix-vector product of a [1.x.1362] operator (the left-hand side of the last formula).  It does not matter whether the AffineConstraints object passed to the  [2.x.3498]  contains inhomogeneous constraints or not, the  [2.x.3499]  call will only resolve the homogeneous part of the constraints as long as it represents a [1.x.1363] operator. 

In our matrix-free code, the right hand side computation where the contribution of inhomogeneous conditions ends up is completely decoupled from the matrix operator and handled by a different function above. Thus, we need to explicitly generate the data that enters the right hand side rather than using a byproduct of the matrix assembly. Since we already know how to apply the operator on a vector, we could try to use those facilities for a vector where we only set the Dirichlet values: 

[1.x.1364] 

or, equivalently, if we already had filled the inhomogeneous constraints into an AffineConstraints object, 

[1.x.1365] 



We could then pass the vector  [2.x.3500]  to the  [2.x.3501]   [2.x.3502]  function and add the new contribution to the  [2.x.3503]  system_rhs vector that gets filled in the  [2.x.3504]  function. However, this idea does not work because the  [2.x.3505]  call used inside the vmult() functions assumes homogeneous values on all constraints (otherwise the operator would not be a linear operator but an affine one). To also retrieve the values of the inhomogeneities, we could select one of two following strategies. 

[1.x.1366] 

The class FEEvaluation has a facility that addresses precisely this requirement: For non-homogeneous Dirichlet values, we do want to skip the implicit imposition of homogeneous (Dirichlet) constraints upon reading the data from the vector  [2.x.3506]  For example, we could extend the  [2.x.3507]   [2.x.3508]  function to deal with inhomogeneous Dirichlet values as follows, assuming the Dirichlet values have been interpolated into the object  [2.x.3509]  

[1.x.1367] 



In this code, we replaced the  [2.x.3510]  function for the tentative solution vector by  [2.x.3511]  that ignores all constraints. Due to this setup, we must make sure that other constraints, e.g. by hanging nodes, are correctly distributed to the input vector already as they are not resolved as in  [2.x.3512]  Inside the loop, we then evaluate the Laplacian and repeat the second derivative call with  [2.x.3513]  from the  [2.x.3514]  class, but with the sign switched since we wanted to subtract the contribution of Dirichlet conditions on the right hand side vector according to the formula above. When we invoke the  [2.x.3515]  call, we then set both arguments regarding the value slot and first derivative slot to true to account for both terms added in the loop over quadrature points. Once the right hand side is assembled, we then go on to solving the linear system for the homogeneous problem, say involving a variable  [2.x.3516]  After solving, we can add  [2.x.3517]  to the  [2.x.3518]  vector that contains the final (inhomogeneous) solution. 

Note that the negative sign for the Laplacian alongside with a positive sign for the forcing that we needed to build the right hand side is a more general concept: We have implemented nothing else than Newton's method for nonlinear equations, but applied to a linear system. We have used an initial guess for the variable  [2.x.3519]  in terms of the Dirichlet boundary conditions and computed a residual  [2.x.3520] . The linear system was then solved as  [2.x.3521]  and we finally computed  [2.x.3522] . For a linear system, we obviously reach the exact solution after a single iteration. If we wanted to extend the code to a nonlinear problem, we would rename the  [2.x.3523]  function into a more descriptive name like  [2.x.3524]  assemble_residual() that computes the (weak) form of the residual, whereas the  [2.x.3525]  function would get the linearization of the residual with respect to the solution variable. 

[1.x.1368] 

A second alternative to get the right hand side that re-uses the  [2.x.3526]   [2.x.3527]  function is to instead add a second LaplaceOperator that skips Dirichlet constraints. To do this, we initialize a second MatrixFree object which does not have any boundary value constraints. This  [2.x.3528]  object is then passed to a  [2.x.3529]  class instance  [2.x.3530]  inhomogeneous_operator that is only used to create the right hand side: 

[1.x.1369] 



A more sophisticated implementation of this technique could reuse the original MatrixFree object. This can be done by initializing the MatrixFree object with multiple blocks, where each block corresponds to a different AffineConstraints object. Doing this would require making substantial modifications to the LaplaceOperator class, but the  [2.x.3531]  class that comes with the library can do this. See the discussion on blocks in  [2.x.3532]  for more information on how to set up blocks. 


examples/step-38/doc/intro.dox 

 [2.x.3533]  

[1.x.1370] 

[1.x.1371] 

[1.x.1372] 

In this example, we show how to solve a partial differential equation (PDE) on a codimension one surface  [2.x.3534]  made of quadrilaterals, i.e. on a surface in 3d or a line in 2d. We focus on the following elliptic second order PDE 

[1.x.1373] 

which generalized the Laplace equation we have previously solved in several of the early tutorial programs. Our implementation is based on step-4. step-34 also solves problems on lower dimensional surfaces; however, there we only consider integral equations that do not involve derivatives on the solution variable, while here we actually have to investigate what it means to take derivatives of a function only defined on a (possibly curved) surface. 

In order to define the above operator, we start by introducing some notations. Let  [2.x.3535]  be a parameterization of a surface  [2.x.3536]  from a reference element  [2.x.3537] , i.e. each point  [2.x.3538]  induces a point  [2.x.3539] . Then let 

[1.x.1374] 

denotes the corresponding first fundamental form, where  [2.x.3540]  is the derivative (Jacobian) of the mapping. In the following,  [2.x.3541]  will be either the entire surface  [2.x.3542]  or, more convenient for the finite element method, any face  [2.x.3543] , where  [2.x.3544]  is a partition (triangulation) of  [2.x.3545]  constituted of quadrilaterals. We are now in position to define the tangential gradient of a function  [2.x.3546]  by 

[1.x.1375] 

The surface Laplacian (also called the Laplace-Beltrami operator) is then defined as   [2.x.3547] . Note that an alternate way to compute the surface gradient on smooth surfaces  [2.x.3548]  is 

[1.x.1376] 

where  [2.x.3549]  is a "smooth" extension of  [2.x.3550]  in a tubular neighborhood of  [2.x.3551]  and  [2.x.3552]  is the normal of  [2.x.3553] . Since  [2.x.3554] , we deduce 

[1.x.1377] 

Worth mentioning, the term  [2.x.3555]  appearing in the above expression is the total curvature of the surface (sum of principal curvatures). 

As usual, we are only interested in weak solutions for which we can use  [2.x.3556]  finite elements (rather than requiring  [2.x.3557]  continuity as for strong solutions). We therefore resort to the weak formulation 

[1.x.1378] 

and take advantage of the partition  [2.x.3558]  to further write 

[1.x.1379] 

Moreover, each integral in the above expression is computed in the reference element  [2.x.3559]  so that 

[1.x.1380] 

and 

[1.x.1381] 

Finally, we use a quadrature formula defined by points  [2.x.3560]  and weights  [2.x.3561]  to evaluate the above integrals and obtain 

[1.x.1382] 

and 

[1.x.1383] 




Fortunately, deal.II has already all the tools to compute the above expressions. In fact, they barely differ from the ways in which we solve the usual Laplacian, only requiring the surface coordinate mapping to be provided in the constructor of the FEValues class. This surface description given, in the codimension one surface case, the two routines  [2.x.3562]  and  [2.x.3563]  return 

[1.x.1384] 

This provides exactly the terms we need for our computations. 

On a more general note, details for the finite element approximation on surfaces can be found for instance in [Dziuk, in Partial differential equations and calculus of variations 1357, Lecture Notes in Math., 1988], [Demlow, SIAM J. Numer. Anal.  47(2), 2009] and [Bonito, Nochetto, and Pauletti, SIAM J. Numer. Anal. 48(5), 2010]. 




[1.x.1385] 

In general when you want to test numerically the accuracy and/or order of convergence of an algorithm you need to provide an exact solution. The usual trick is to pick a function that we want to be the solution, then apply the differential operator to it that defines a forcing term for the right hand side. This is what we do in this example. In the current case, the form of the domain is obviously also essential. 

We produce one test case for a 2d problem and another one for 3d: 

 [2.x.3564]   [2.x.3565]    In 2d, let's choose as domain a half circle. On this domain, we choose the   function  [2.x.3566]  as the solution. To compute the right hand   side, we have to compute the surface Laplacian of the   solution function. There are (at least) two ways to do that. The first one   is to project away the normal derivative as described above using the natural extension of  [2.x.3567]  (still denoted by  [2.x.3568] ) over  [2.x.3569] , i.e. to compute   [1.x.1386] 

  where  [2.x.3570]  is the total curvature of  [2.x.3571] .   Since we are on the unit circle,  [2.x.3572]  and  [2.x.3573]  so that   [1.x.1387] 



  A somewhat simpler way, at least for the current case of a curve in   two-dimensional space, is to note that we can map the interval  [2.x.3574]  onto the domain  [2.x.3575]  using the transformation    [2.x.3576] .   At position  [2.x.3577] , the value of the solution is then    [2.x.3578] .   Taking into account that the transformation is length preserving, i.e. a   segment of length  [2.x.3579]  is mapped onto a piece of curve of exactly the same   length, the tangential Laplacian then satisfies   [1.x.1388] 

  which is of course the same result as we had above.  [2.x.3580]   [2.x.3581]    In 3d, the domain is again half of the surface of the unit ball, i.e. a half   sphere or dome. We choose  [2.x.3582]  as   the solution. We can compute the right hand side of the   equation,  [2.x.3583] , in the same way as the method above (with  [2.x.3584] ), yielding an   awkward and lengthy expression. You can find the full expression in the   source code.  [2.x.3585]   [2.x.3586]  

In the program, we will also compute the  [2.x.3587]  seminorm error of the solution. Since the solution function and its numerical approximation are only defined on the manifold, the obvious definition of this error functional is  [2.x.3588] . This requires us to provide the [1.x.1389] gradient  [2.x.3589]  to the function  [2.x.3590]  (first introduced in step-7), which we will do by implementing the function  [2.x.3591]  in the program below. 




[1.x.1390] 

If you've read through step-4 and understand the discussion above of how solution and right hand side correspond to each other, you will be immediately familiar with this program as well. In fact, there are only two things that are of significance: 

- The way we generate the mesh that triangulates the computational domain. 

- The way we use Mapping objects to describe that the domain on which we solve   the partial differential equation is not planar but in fact curved. 

Mapping objects were already introduced in step-10 and step-11 and as explained there, there is usually not a whole lot you have to know about how they work as long as you have a working description of how the boundary looks. In essence, we will simply declare an appropriate object of type MappingQ that will automatically obtain the boundary description from the Triangulation. The mapping object will then be passed to the appropriate functions, and we will get a boundary description for half circles or half spheres that is predefined in the library. 

The rest of the program follows closely step-4 and, as far as computing the error, step-7. Some aspects of this program, in particular the use of two template arguments on the classes Triangulation, DoFHandler, and similar, are already described in detail in step-34; you may wish to read through this tutorial program as well. 


examples/step-38/doc/results.dox 



[1.x.1391] 

When you run the program, the following output should be printed on screen: 

[1.x.1392] 




By playing around with the number of global refinements in the  [2.x.3592]  function you increase or decrease mesh refinement. For example, doing one more refinement and only running the 3d surface problem yields the following output: 

[1.x.1393] 



This is what we expect: make the mesh size smaller by a factor of two and the error goes down by a factor of four (remember that we use bi-quadratic elements). The full sequence of errors from one to five refinements looks like this, neatly following the theoretically predicted pattern: 

[1.x.1394] 



Finally, the program produces graphical output that we can visualize. Here is a plot of the results: 

 [2.x.3593]  

The program also works for 1d curves in 2d, not just 2d surfaces in 3d. You can test this by changing the template argument in  [2.x.3594]  like so: 

[1.x.1395] 

The domain is a curve in 2d, and we can visualize the solution by using the third dimension (and color) to denote the value of the function  [2.x.3595] . This then looks like so (the white curve is the domain, the colored curve is the solution extruded into the third dimension, clearly showing the change in sign as the curve moves from one quadrant of the domain into the adjacent one): 

 [2.x.3596]  


[1.x.1396] 

[1.x.1397] 

Computing on surfaces only becomes interesting if the surface is more interesting than just a half sphere. To achieve this, deal.II can read meshes that describe surfaces through the usual GridIn class. Or, in case you have an analytic description, a simple mesh can sometimes be stretched and bent into a shape we are interested in. 

Let us consider a relatively simple example: we take the half sphere we used before, we stretch it by a factor of 10 in the z-direction, and then we jumble the x- and y-coordinates a bit. Let's show the computational domain and the solution first before we go into details of the implementation below: 

 [2.x.3597]  

 [2.x.3598]  

The way to produce such a mesh is by using the  [2.x.3599]  function. It needs a way to transform each individual mesh point to a different position. Let us here use the following, rather simple function (remember: stretch in one direction, jumble in the other two): 

[1.x.1398] 



If we followed the  [2.x.3600]  function, we would extract the half spherical surface mesh as before, warp it into the shape we want, and refine as often as necessary. This is not quite as simple as we'd like here, though: refining requires that we have an appropriate manifold object attached to the triangulation that describes where new vertices of the mesh should be located upon refinement. I'm sure it's possible to describe this manifold in a not-too-complicated way by simply undoing the transformation above (yielding the spherical surface again), finding the location of a new point on the sphere, and then re-warping the result. But I'm a lazy person, and since doing this is not really the point here, let's just make our lives a bit easier: we'll extract the half sphere, refine it as often as necessary, get rid of the object that describes the manifold since we now no longer need it, and then finally warp the mesh. With the function above, this would look as follows: 

[1.x.1399] 



Note that the only essential addition is the line marked with asterisks. It is worth pointing out one other thing here, though: because we detach the manifold description from the surface mesh, whenever we use a mapping object in the rest of the program, it has no curves boundary description to go on any more. Rather, it will have to use the implicit, FlatManifold class that is used on all parts of the domain not explicitly assigned a different manifold object. Consequently, whether we use MappingQ(2), MappingQ(15) or MappingQ1, each cell of our mesh will be mapped using a bilinear approximation. 

All these drawbacks aside, the resulting pictures are still pretty. The only other differences to what's in step-38 is that we changed the right hand side to  [2.x.3601]  and the boundary values (through the  [2.x.3602]  class) to  [2.x.3603] . Of course, we now no longer know the exact solution, so the computation of the error at the end of  [2.x.3604]  will yield a meaningless number. 


examples/step-39/doc/intro.dox 

[1.x.1400] 

In this program, we use the interior penalty method and Nitsche's weak boundary conditions to solve Poisson's equation. We use multigrid methods on locally refined meshes, which are generated using a bulk criterion and a standard error estimator based on cell and face residuals. All operators are implemented using the MeshWorker interface. 

Like in step-12, the discretization relies on finite element spaces, which are polynomial inside the mesh cells  [2.x.3605] , but have no continuity between cells. Since such functions have two values on each interior face  [2.x.3606] , one from each side, we define mean value and jump operators as follows: let [1.x.1401]<sub>1</sub> and [1.x.1402]<sub>2</sub> be the two cells sharing a face, and let the traces of functions [1.x.1403] and the outer normal vectors [1.x.1404][1.x.1405] be labeled accordingly. Then, on the face, we let 

[1.x.1406] 



Note, that if such an expression contains a normal vector, the averaging operator turns into a jump. The interior penalty method for the problem 

[1.x.1407] 

becomes 

[1.x.1408] 



Here,  [2.x.3607]  is the penalty parameter, which is chosen as follows: for a face [1.x.1409] of a cell [1.x.1410], compute the value 

[1.x.1411] 

where [1.x.1412] is the polynomial degree of the finite element functions and  [2.x.3608]  and  [2.x.3609]  denote the  [2.x.3610]  and  [2.x.3611]  dimensional Hausdorff measure of the corresponding object. If the face is at the boundary, choose  [2.x.3612] . For an interior face, we take the average of the two values at this face. 

In our finite element program, we distinguish three different integrals, corresponding to the sums over cells, interior faces and boundary faces above. Since the  [2.x.3613]  organizes the sums for us, we only need to implement the integrals over each mesh element. The class MatrixIntegrator below has these three functions for the left hand side of the formula, the class RHSIntegrator for the right. 

As we will see below, even the error estimate is of the same structure, since it can be written as 

[1.x.1413] 



Thus, the functions for assembling matrices, right hand side and error estimates below exhibit that these loops are all generic and can be programmed in the same way. 

This program is related to step-12b, in that it uses MeshWorker and discontinuous Galerkin methods. While there, we solved an advection problem, here it is a diffusion problem. Here, we also use multigrid preconditioning and a theoretically justified error estimator, see Karakashian and Pascal (2003). The multilevel scheme was discussed in detail in Kanschat (2004). The adaptive iteration and its convergence have been discussed (for triangular meshes) in Hoppe, Kanschat, and Warburton (2009). 


examples/step-39/doc/results.dox 



[1.x.1414] 

[1.x.1415] First, the program produces the usual logfile here stored in <tt>deallog</tt>. It reads (with omission of intermediate steps) 

[1.x.1416] 



This log for instance shows that the number of conjugate gradient iteration steps is constant at approximately 15. 

[1.x.1417] 

 [2.x.3614]  Using the perl script <tt>postprocess.pl</tt>, we extract relevant data into <tt>output.dat</tt>, which can be used to plot graphs with <tt>gnuplot</tt>. The graph above for instance was produced using the gnuplot script <tt>plot_errors.gpl</tt> via 

[1.x.1418] 



Reference data can be found in <tt>output.reference.dat</tt>. 


examples/step-4/doc/intro.dox 

[1.x.1419] 

[1.x.1420] 

 [2.x.3615]  

deal.II has a unique feature which we call ``dimension independent programming''. You may have noticed in the previous examples that many classes had a number in angle brackets suffixed to them. This is to indicate that for example the triangulation in two and three space dimensions are different, but related data %types. We could as well have called them  [2.x.3616]  instead of  [2.x.3617]  and  [2.x.3618]  to name the two classes, but this has an important drawback: assume you have a function which does exactly the same functionality, but on 2d or 3d triangulations, depending on which dimension we would like to solve the equation in presently (if you don't believe that it is the common case that a function does something that is the same in all dimensions, just take a look at the code below - there are almost no distinctions between 2d and 3d!). We would have to write the same function twice, once working on  [2.x.3619]  and once working with a  [2.x.3620] . This is an unnecessary obstacle in programming and leads to a nuisance to keep the two function in sync (at best) or difficult to find errors if the two versions get out of sync (at worst; this would probably the more common case). 





Such obstacles can be circumvented by using some template magic as provided by the C++ language: templatized classes and functions are not really classes or functions but only a pattern depending on an as-yet undefined data type parameter or on a numerical value which is also unknown at the point of definition. However, the compiler can build proper classes or functions from these templates if you provide it with the information that is needed for that. Of course, parts of the template can depend on the template parameters, and they will be resolved at the time of compilation for a specific template parameter. For example, consider the following piece of code: 

[1.x.1421] 






At the point where the compiler sees this function, it does not know anything about the actual value of  [2.x.3621] . The only thing the compiler has is a template, i.e. a blueprint, to generate functions  [2.x.3622]  if given a particular value of  [2.x.3623]  has an unknown value, there is no code the compiler can generate for the moment. 




However, if later down the compiler would encounter code that looks, for example, like this, 

[1.x.1422] 

then the compiler will deduce that the function  [2.x.3624]  for  [2.x.3625]  was requested and will compile the template above into a function with dim replaced by 2 everywhere, i.e. it will compile the function as if it were defined as 

[1.x.1423] 






However, it is worth to note that the function  [2.x.3626]  depends on the dimension as well, so in this case, the compiler will call the function  [2.x.3627]  while if dim were 3, it would call  [2.x.3628]  which might be (and actually is) a totally unrelated  function. 




The same can be done with member variables. Consider the following function, which might in turn call the above one: 

[1.x.1424] 

This function has a member variable of type  [2.x.3629] . Again, the compiler can't compile this function until it knows for which dimension. If you call this function for a specific dimension as above, the compiler will take the template, replace all occurrences of dim by the dimension for which it was called, and compile it. If you call the function several times for different dimensions, it will compile it several times, each time calling the right  [2.x.3630]  function and reserving the right amount of memory for the member variable; note that the size of a  [2.x.3631]  might, and indeed does, depend on the space dimension. 




The deal.II library is built around this concept of dimension-independent programming, and therefore allows you to program in a way that will not need to distinguish between the space dimensions. It should be noted that in only a very few places is it necessary to actually compare the dimension using  [2.x.3632] es. However, since the compiler has to compile each function for each dimension separately, even there it knows the value of  [2.x.3633]  at the time of compilation and will therefore be able to optimize away the  [2.x.3634]  statement along with the unused branch. 




In this example program, we will show how to program dimension independently (which in fact is even simpler than if you had to take care about the dimension) and we will extend the Laplace problem of the last example to a program that runs in two and three space dimensions at the same time. Other extensions are the use of a non-constant right hand side function and of non-zero boundary values. 




 [2.x.3635]  When using templates, C++ imposes all sorts of syntax constraints that make it sometimes a bit difficult to understand why exactly something has to be written this way. A typical example is the need to use the keyword  [2.x.3636]  in so many places. If you are not entirely familiar with this already, then several of these difficulties are explained in the deal.II Frequently Asked Questions (FAQ) linked to from the [1.x.1425]. 

<!--We need a blank line to end the above block properly.--> 


examples/step-4/doc/results.dox 



[1.x.1426] 


The output of the program looks as follows (the number of iterations may vary by one or two, depending on your computer, since this is often dependent on the round-off accuracy of floating point operations, which differs between processors): 

[1.x.1427] 

It is obvious that in three spatial dimensions the number of cells and therefore also the number of degrees of freedom is much higher. What cannot be seen here, is that besides this higher number of rows and columns in the matrix, there are also significantly more entries per row of the matrix in three space dimensions. Together, this leads to a much higher numerical effort for solving the system of equation, which you can feel in the run time of the two solution steps when you actually run the program. 




The program produces two files:  [2.x.3637]  and  [2.x.3638] , which can be viewed using the programs VisIt or Paraview (in case you do not have these programs, you can easily change the output format in the program to something which you can view more easily). Visualizing solutions is a bit of an art, but it can also be fun, so you should play around with your favorite visualization tool to get familiar with its functionality. Here's what I have come up with for the 2d solution: 

<p align="center">    [2.x.3639]   [2.x.3640]  

( [2.x.3641]  The picture shows the solution of the problem under consideration as a 3D plot. As can be seen, the solution is almost flat in the interior of the domain and has a higher curvature near the boundary. This, of course, is due to the fact that for Laplace's equation the curvature of the solution is equal to the right hand side and that was chosen as a quartic polynomial which is nearly zero in the interior and is only rising sharply when approaching the boundaries of the domain; the maximal values of the right hand side function are at the corners of the domain, where also the solution is moving most rapidly. It is also nice to see that the solution follows the desired quadratic boundary values along the boundaries of the domain. It can also be useful to verify a computed solution against an analytical solution. For an explanation of this technique, see step-7. 

On the other hand, even though the picture does not show the mesh lines explicitly, you can see them as little kinks in the solution. This clearly indicates that the solution hasn't been computed to very high accuracy and that to get a better solution, we may have to compute on a finer mesh. 

In three spatial dimensions, visualization is a bit more difficult. The left picture shows the solution and the mesh it was computed on on the surface of the domain. This is nice, but it has the drawback that it completely hides what is happening on the inside. The picture on the right is an attempt at visualizing the interior as well, by showing surfaces where the solution has constant values (as indicated by the legend at the top left). Isosurface pictures look best if one makes the individual surfaces slightly transparent so that it is possible to see through them and see what's behind. 

 [2.x.3642]  

 [2.x.3643]  A final remark on visualization: the idea of visualization is to give insight, which is not the same as displaying information. In particular, it is easy to overload a picture with information, but while it shows more information it makes it also more difficult to glean insight. As an example, the program I used to generate these pictures, VisIt, by default puts tick marks on every axis, puts a big fat label "X Axis" on the  [2.x.3644]  axis and similar for the other axes, shows the file name from which the data was taken in the top left and the name of the user doing so and the time and date on the bottom right. None of this is important here: the axes are equally easy to make out because the tripod at the bottom left is still visible, and we know from the program that the domain is  [2.x.3645] , so there is no need for tick marks. As a consequence, I have switched off all the extraneous stuff in the picture: the art of visualization is to reduce the picture to those parts that are important to see what one wants to see, but no more. 




[1.x.1428] 

[1.x.1429] 


Essentially the possibilities for playing around with the program are the same as for the previous one, except that they will now also apply to the 3d case. For inspiration read up on [1.x.1430]. 


examples/step-40/doc/intro.dox 

 [2.x.3646]  

[1.x.1431] 




 [2.x.3647]  As a prerequisite of this program, you need to have both PETSc and the p4est library installed. The installation of deal.II together with these two additional libraries is described in the [1.x.1432] file. Note also that to work properly, this program needs access to the Hypre preconditioner package implementing algebraic multigrid; it can be installed as part of PETSc but has to be explicitly enabled during PETSc configuration; see the page linked to from the installation instructions for PETSc. 


[1.x.1433] 

[1.x.1434] 

 [2.x.3648]  

Given today's computers, most finite element computations can be done on a single machine. The majority of previous tutorial programs therefore shows only this, possibly splitting up work among a number of processors that, however, can all access the same, shared memory space. That said, there are problems that are simply too big for a single machine and in that case the problem has to be split up in a suitable way among multiple machines each of which contributes its part to the whole. A simple way to do that was shown in step-17 and step-18, where we show how a program can use [1.x.1435] to parallelize assembling the linear system, storing it, solving it, and computing error estimators. All of these operations scale relatively trivially (for a definition of what it means for an operation to "scale", see  [2.x.3649]  "this glossary entry"), but there was one significant drawback: for this to be moderately simple to implement, each MPI processor had to keep its own copy of the entire Triangulation and DoFHandler objects. Consequently, while we can suspect (with good reasons) that the operations listed above can scale to thousands of computers and problem sizes of billions of cells and billions of degrees of freedom, building the one big mesh for the entire problem these thousands of computers are solving on every last processor is clearly not going to scale: it is going to take forever, and maybe more importantly no single machine will have enough memory to store a mesh that has a billion cells (at least not at the time of writing this). In reality, programs like step-17 and step-18 can therefore not be run on more than maybe 100 or 200 processors and even there storing the Triangulation and DoFHandler objects consumes the vast majority of memory on each machine. 

Consequently, we need to approach the problem differently: to scale to very large problems each processor can only store its own little piece of the Triangulation and DoFHandler objects. deal.II implements such a scheme in the  [2.x.3650]  namespace and the classes therein. It builds on an external library, [1.x.1436] (a play on the expression [1.x.1437] that describes the parallel storage of a hierarchically constructed mesh as a forest of quad- or oct-trees). You need to [1.x.1438] but apart from that all of its workings are hidden under the surface of deal.II. 

In essence, what the  [2.x.3651]  class and code inside the DoFHandler class do is to split the global mesh so that every processor only stores a small bit it "owns" along with one layer of "ghost" cells that surround the ones it owns. What happens in the rest of the domain on which we want to solve the partial differential equation is unknown to each processor and can only be inferred through communication with other machines if such information is needed. This implies that we also have to think about problems in a different way than we did in, for example, step-17 and step-18: no processor can have the entire solution vector for postprocessing, for example, and every part of a program has to be parallelized because no processor has all the information necessary for sequential operations. 

A general overview of how this parallelization happens is described in the  [2.x.3652]  documentation module. You should read it for a top-level overview before reading through the source code of this program. A concise discussion of many terms we will use in the program is also provided in the  [2.x.3653]  "Distributed Computing paper". It is probably worthwhile reading it for background information on how things work internally in this program. 




[1.x.1439] 

This program essentially re-solves what we already do in step-6, i.e. it solves the Laplace equation 

[1.x.1440] 

The difference of course is now that we want to do so on a mesh that may have a billion cells, with a billion or so degrees of freedom. There is no doubt that doing so is completely silly for such a simple problem, but the point of a tutorial program is, after all, not to do something useful but to show how useful programs can be implemented using deal.II. Be that as it may, to make things at least a tiny bit interesting, we choose the right hand side as a discontinuous function, 

[1.x.1441] 

so that the solution has a singularity along the sinusoidal line snaking its way through the domain. As a consequence, mesh refinement will be concentrated along this line. You can see this in the mesh picture shown below in the results section. 

Rather than continuing here and giving a long introduction, let us go straight to the program code. If you have read through step-6 and the  [2.x.3654]  documentation module, most of things that are going to happen should be familiar to you already. In fact, comparing the two programs you will notice that the additional effort necessary to make things work in %parallel is almost insignificant: the two programs have about the same number of lines of code (though step-6 spends more space on dealing with coefficients and output). In either case, the comments below will only be on the things that set step-40 apart from step-6 and that aren't already covered in the  [2.x.3655]  documentation module. 




 [2.x.3656]  This program will be able to compute on as many processors as you want to throw at it, and for as large a problem as you have the memory and patience to solve. However, there [1.x.1442] a limit: the number of unknowns can not exceed the largest number that can be stored with an object of type  [2.x.3657]  By default, this is an alias for <code>unsigned int</code>, which on most machines today is a 32-bit integer, limiting you to some 4 billion (in reality, since this program uses PETSc, you will be limited to half that as PETSc uses signed integers). However, this can be changed during configuration to use 64-bit integers, see the ReadMe file. This will give problem sizes you are unlikely to exceed anytime soon. 


examples/step-40/doc/results.dox 



[1.x.1443] 

When you run the program, on a single processor or with your local MPI installation on a few, you should get output like this: 

[1.x.1444] 



The exact numbers differ, depending on how many processors we use; this is due to the fact that the preconditioner depends on the partitioning of the problem, the solution then differs in the last few digits, and consequently the mesh refinement differs slightly. The primary thing to notice here, though, is that the number of iterations does not increase with the size of the problem. This guarantees that we can efficiently solve even the largest problems. 

When run on a sufficiently large number of machines (say a few thousand), this program can relatively easily solve problems with well over one billion unknowns in less than a minute. On the other hand, such big problems can no longer be visualized, so we also ran the program on only 16 processors. Here are a mesh, along with its partitioning onto the 16 processors, and the corresponding solution: 

 [2.x.3658]  

The mesh on the left has a mere 7,069 cells. This is of course a problem we would easily have been able to solve already on a single processor using step-6, but the point of the program was to show how to write a program that scales to many more machines. For example, here are two graphs that show how the run time of a large number of parts of the program scales on problems with around 52 and 375 million degrees of freedom if we take more and more processors (these and the next couple of graphs are taken from an earlier version of the  [2.x.3659]  "Distributed Computing paper"; updated graphs showing data of runs on even larger numbers of processors, and a lot more interpretation can be found in the final version of the paper): 

 [2.x.3660]  

As can clearly be seen, the program scales nicely to very large numbers of processors. (For a discussion of what we consider "scalable" programs, see  [2.x.3661]  "this glossary entry".) The curves, in particular the linear solver, become a bit wobbly at the right end of the graphs since each processor has too little to do to offset the cost of communication (the part of the whole problem each processor has to solve in the above two examples is only 13,000 and 90,000 degrees of freedom when 4,096 processors are used; a good rule of thumb is that parallel programs work well if each processor has at least 100,000 unknowns). 

While the strong scaling graphs above show that we can solve a problem of fixed size faster and faster if we take more and more processors, the more interesting question may be how big problems can become so that they can still be solved within a reasonable time on a machine of a particular size. We show this in the following two graphs for 256 and 4096 processors: 

 [2.x.3662]  

What these graphs show is that all parts of the program scale linearly with the number of degrees of freedom. This time, lines are wobbly at the left as the size of local problems is too small. For more discussions of these results we refer to the  [2.x.3663]  "Distributed Computing paper". 

So how large are the largest problems one can solve? At the time of writing this problem, the limiting factor is that the program uses the BoomerAMG algebraic multigrid method from the [1.x.1445] as a preconditioner, which unfortunately uses signed 32-bit integers to index the elements of a %distributed matrix. This limits the size of problems to  [2.x.3664]  degrees of freedom. From the graphs above it is obvious that the scalability would extend beyond this number, and one could expect that given more than the 4,096 machines shown above would also further reduce the compute time. That said, one can certainly expect that this limit will eventually be lifted by the hypre developers. 

On the other hand, this does not mean that deal.II cannot solve bigger problems. Indeed, step-37 shows how one can solve problems that are not just a little, but very substantially larger than anything we have shown here. 




[1.x.1446] 

[1.x.1447] 

In a sense, this program is the ultimate solver for the Laplace equation: it can essentially solve the equation to whatever accuracy you want, if only you have enough processors available. Since the Laplace equation by itself is not terribly interesting at this level of accuracy, the more interesting possibilities for extension therefore concern not so much this program but what comes beyond it. For example, several of the other programs in this tutorial have significant run times, especially in 3d. It would therefore be interesting to use the techniques explained here to extend other programs to support parallel distributed computations. We have done this for step-31 in the step-32 tutorial program, but the same would apply to, for example, step-23 and step-25 for hyperbolic time dependent problems, step-33 for gas dynamics, or step-35 for the Navier-Stokes equations. 

Maybe equally interesting is the problem of postprocessing. As mentioned above, we only show pictures of the solution and the mesh for 16 processors because 4,096 processors solving 1 billion unknowns would produce graphical output on the order of several 10 gigabyte. Currently, no program is able to visualize this amount of data in any reasonable way unless it also runs on at least several hundred processors. There are, however, approaches where visualization programs directly communicate with solvers on each processor with each visualization process rendering the part of the scene computed by the solver on this processor. Implementing such an interface would allow to quickly visualize things that are otherwise not amenable to graphical display. 


examples/step-41/doc/intro.dox 

 [2.x.3665]  

[1.x.1448] 


[1.x.1449] 

[1.x.1450] 

This example is based on the Laplace equation in 2d and deals with the question what happens if a membrane is deflected by some external force but is also constrained by an obstacle. In other words, think of a elastic membrane clamped at the boundary to a rectangular frame (we choose  [2.x.3666] ) and that sags through due to gravity acting on it. What happens now if there is an obstacle under the membrane that prevents it from reaching its equilibrium position if gravity was the only existing force? In the current example program, we will consider that under the membrane is a stair step obstacle against which gravity pushes the membrane. 

This problem is typically called the "obstacle problem" (see also [1.x.1451]), and it results in a variational inequality, rather than a variational equation when put into the weak form. We will below derive it from the classical formulation, but before we go on to discuss the mathematics let us show how the solution of the problem we will consider in this tutorial program looks to gain some intuition of what we should expect: 

 [2.x.3667]  

Here, at the left, we see the displacement of the membrane. The shape of the obstacle underneath is clearly visible. On the right, we overlay which parts of the membrane are in contact with the obstacle. We will later call this set of points the "active set" to indicate that an inequality constraint is active there. 




[1.x.1452] 

The classical formulation of the problem possesses the following form: 

[1.x.1453] 

with  [2.x.3668] .   [2.x.3669]  is a scalar valued function that denotes the vertical displacement of the membrane. The first equation is called equilibrium condition with a force of areal density  [2.x.3670] . Here, we will consider this force to be gravity. The second one is known as Hooke's Law that says that the stresses  [2.x.3671]  are proportional to the gradient of the displacements  [2.x.3672]  (the proportionality constant, often denoted by  [2.x.3673] , has been set to one here, without loss of generality; if it is constant, it can be put into the right hand side function). At the boundary we have zero Dirichlet conditions. Obviously, the first two equations can be combined to yield  [2.x.3674] . 

Intuitively, gravity acts downward and so  [2.x.3675]  is a negative function (we choose  [2.x.3676]  in this program). The first condition then means that the total force acting on the membrane is gravity plus something positive: namely the upward force that the obstacle exerts on the membrane at those places where the two of them are in contact. How big is this additional force? We don't know yet (and neither do we know "where" it actually acts) but it must be so that the membrane doesn't penetrate the obstacle. 

The fourth equality above together with the last inequality forms the obstacle condition which has to hold at every point of the whole domain. The latter of these two means that the membrane must be above the obstacle  [2.x.3677]  everywhere. The second to last equation, often called the "complementarity condition" says that where the membrane is not in contact with the obstacle (i.e., those  [2.x.3678]  where  [2.x.3679] ), then  [2.x.3680]  at these locations; in other words, no additional forces act there, as expected. On the other hand, where  [2.x.3681]  we can have  [2.x.3682] , i.e., there can be additional forces (though there don't have to be: it is possible for the membrane to just touch, not press against, the obstacle). 




[1.x.1454] 

An obvious way to obtain the variational formulation of the obstacle problem is to consider the total potential energy: 

[1.x.1455] 

We have to find a solution  [2.x.3683]  of the following minimization problem: 

[1.x.1456] 

with the convex set of admissible displacements: 

[1.x.1457] 

This set takes care of the third and fifth conditions above (the boundary values and the complementarity condition). 

Consider now the minimizer  [2.x.3684]  of  [2.x.3685]  and any other function  [2.x.3686] . Then the function 

[1.x.1458] 

takes its minimum at  [2.x.3687]  (because  [2.x.3688]  is a minimizer of the energy functional  [2.x.3689] ), so that  [2.x.3690]  for any choice of  [2.x.3691] . Note that  [2.x.3692]  because of the convexity of  [2.x.3693] . If we compute  [2.x.3694]  it yields the variational formulation we are searching for: 

[1.x.1459] 

[1.x.1460] 



This is the typical form of variational inequalities, where not just  [2.x.3695]  appears in the bilinear form but in fact  [2.x.3696] . The reason is this: if  [2.x.3697]  is not constrained, then we can find test functions  [2.x.3698]  in  [2.x.3699]  so that  [2.x.3700]  can have any sign. By choosing test functions  [2.x.3701]  so that  [2.x.3702]  it follows that the inequality can only hold for both  [2.x.3703]  and  [2.x.3704]  if the two sides are in fact equal, i.e., we obtain a variational equality. 

On the other hand, if  [2.x.3705]  then  [2.x.3706]  only allows test functions  [2.x.3707]  so that in fact  [2.x.3708] . This means that we can't test the equation with both  [2.x.3709]  and  [2.x.3710]  as above, and so we can no longer conclude that the two sides are in fact equal. Thus, this mimics the way we have discussed the complementarity condition above. 




[1.x.1461] 

The variational inequality above is awkward to work with. We would therefore like to reformulate it as an equivalent saddle point problem. We introduce a Lagrange multiplier  [2.x.3711]  and the convex cone  [2.x.3712] ,  [2.x.3713]  dual space of  [2.x.3714] ,  [2.x.3715]  of Lagrange multipliers, where  [2.x.3716]  denotes the duality pairing between  [2.x.3717]  and  [2.x.3718] . Intuitively,  [2.x.3719]  is the cone of all "non-positive functions", except that  [2.x.3720]  and so contains other objects besides regular functions as well. This yields: 

[1.x.1462] 

[1.x.1463] 

[1.x.1464] 

[1.x.1465] 

In other words, we can consider  [2.x.3721]  as the negative of the additional, positive force that the obstacle exerts on the membrane. The inequality in the second line of the statement above only appears to have the wrong sign because we have  [2.x.3722]  at points where  [2.x.3723] , given the definition of  [2.x.3724] . 

The existence and uniqueness of  [2.x.3725]  of this saddle point problem has been stated in Glowinski, Lions and Tr&eacute;moli&egrave;res: Numerical Analysis of Variational Inequalities, North-Holland, 1981. 




[1.x.1466] 

There are different methods to solve the variational inequality. As one possibility you can understand the saddle point problem as a convex quadratic program (QP) with inequality constraints. 

To get there, let us assume that we discretize both  [2.x.3726]  and  [2.x.3727]  with the same finite element space, for example the usual  [2.x.3728]  spaces. We would then get the equations 

[1.x.1467] 

where  [2.x.3729]  is the mass matrix on the chosen finite element space and the indices  [2.x.3730]  above are for all degrees of freedom in the set  [2.x.3731]  of degrees of freedom located in the interior of the domain (we have Dirichlet conditions on the perimeter). However, we can make our life simpler if we use a particular quadrature rule when assembling all terms that yield this mass matrix, namely a quadrature formula where quadrature points are only located at the interpolation points at which shape functions are defined; since all but one shape function are zero at these locations, we get a diagonal mass matrix with 

[1.x.1468] 

To define  [2.x.3732]  we use the same technique as for  [2.x.3733] . In other words, we define 

[1.x.1469] 

where  [2.x.3734]  is a suitable approximation of  [2.x.3735] . The integral in the definition of  [2.x.3736]  and  [2.x.3737]  are then approximated by the trapezoidal rule. With this, the equations above can be restated as 

[1.x.1470] 



Now we define for each degree of freedom  [2.x.3738]  the function 

[1.x.1471] 

with some  [2.x.3739] . (In this program we choose  [2.x.3740] . It is a kind of a penalty parameter which depends on the problem itself and needs to be chosen large enough; for example there is no convergence for  [2.x.3741]  using the current program if we use 7 global refinements.) 

After some head-scratching one can then convince oneself that the inequalities above can equivalently be rewritten as 

[1.x.1472] 

The primal-dual active set strategy we will use here is an iterative scheme which is based on this condition to predict the next active and inactive sets  [2.x.3742]  and  [2.x.3743]  (that is, those complementary sets of indices  [2.x.3744]  for which  [2.x.3745]  is either equal to or not equal to the value of the obstacle  [2.x.3746] ). For a more in depth treatment of this approach, see Hintermueller, Ito, Kunisch: The primal-dual active set strategy as a semismooth newton method, SIAM J. OPTIM., 2003, Vol. 13, No. 3, pp. 865-888. 

[1.x.1473] 

The algorithm for the primal-dual active set method works as follows (NOTE:  [2.x.3747] ): 

1. Initialize  [2.x.3748]  and  [2.x.3749] , such that   [2.x.3750]  and   [2.x.3751]  and set  [2.x.3752] . 2. Find the primal-dual pair  [2.x.3753]  that satisfies  [1.x.1474] 

 Note that the second and third conditions imply that exactly  [2.x.3754]  unknowns  are fixed, with the first condition yielding the remaining  [2.x.3755]  equations  necessary to determine both  [2.x.3756]  and  [2.x.3757] . 3. Define the new active and inactive sets by  [1.x.1475] 

4. If  [2.x.3758]  (and then, obviously, also   [2.x.3759] ) then stop, else set  [2.x.3760]  and go to step  (2). 

The method is called "primal-dual" because it uses both primal (the displacement  [2.x.3761] ) as well as dual variables (the Lagrange multiplier  [2.x.3762] ) to determine the next active set. 

At the end of this section, let us add two observations. First, for any primal-dual pair  [2.x.3763]  that satisfies these condition, we can distinguish the following cases: 

1.  [2.x.3764]  (i active):    [2.x.3765]    Then either  [2.x.3766]  and  [2.x.3767]  (penetration) or  [2.x.3768]  and  [2.x.3769]  (pressing load). 2.  [2.x.3770]  (i inactive):    [2.x.3771]    Then either  [2.x.3772]  and  [2.x.3773]  (no contact) or  [2.x.3774]  and  [2.x.3775]  (unpressing load). 

Second, the method above appears intuitively correct and useful but a bit ad hoc. However, it can be derived in a concisely in the following way. To this end, note that we'd like to solve the nonlinear system 

[1.x.1476] 

We can iteratively solve this by always linearizing around the previous iterate (i.e., applying a Newton method), but for this we need to linearize the function  [2.x.3776]  that is not differentiable. That said, it is slantly differentiable, and in fact we have 

[1.x.1477] 



[1.x.1478] 

This suggest a semismooth Newton step of the form 

[1.x.1479] 

where we have split matrices  [2.x.3777]  as well as vectors in the natural way into rows and columns whose indices belong to either the active set  [2.x.3778]  or the inactive set  [2.x.3779] . 

Rather than solving for updates  [2.x.3780] , we can also solve for the variables we are interested in right away by setting  [2.x.3781]  and  [2.x.3782]  and bringing all known terms to the right hand side. This yields 

[1.x.1480] 

These are the equations outlined above in the description of the basic algorithm. 

We could even drive this a bit further. It's easy to see that we can eliminate the third row and the third column because it implies  [2.x.3783] : 

[1.x.1481] 

This shows that one in fact only needs to solve for the Lagrange multipliers located on the active set. By considering the second row one would then recover the full Lagrange multiplier vector through 

[1.x.1482] 

Because of the third row and the fact that  [2.x.3784]  is a diagonal matrix we are able to calculate  [2.x.3785]  directly. We can therefore also write the linear system as follows: 

[1.x.1483] 

Fortunately, this form is easy to arrive at: we simply build the usual Laplace linear system 

[1.x.1484] 

and then let the AffineConstraints class eliminate all constrained degrees of freedom, namely  [2.x.3786] , in the same way as if the dofs in  [2.x.3787]  were Dirichlet data. The result linear system (the second to last one above) is symmetric and positive definite and we solve it with a CG-method and the AMG preconditioner from Trilinos. 




[1.x.1485] 

This tutorial is quite similar to step-4. The general structure of the program follows step-4 with minor differences: 

- We need two new methods,  [2.x.3788]  and    [2.x.3789] . 

- We need new member variables that denote the constraints we have here. 

- We change the preconditioner for the solver. 


You may want to read up on step-4 if you want to understand the current program. 


examples/step-41/doc/results.dox 



[1.x.1486] 

Running the program produces output like this: 

[1.x.1487] 



The iterations end once the active set doesn't change any more (it has 5,399 constrained degrees of freedom at that point). The algebraic precondition is apparently working nicely since we only need 4-6 CG iterations to solve the linear system (although this also has a lot to do with the fact that we are not asking for very high accuracy of the linear solver). 

More revealing is to look at a sequence of graphical output files (every third step is shown, with the number of the iteration in the leftmost column): 

 [2.x.3790]  

The pictures show that in the first step, the solution (which has been computed without any of the constraints active) bends through so much that pretty much every interior point has to be bounced back to the stairstep function, producing a discontinuous solution. Over the course of the active set iterations, this unphysical membrane shape is smoothed out, the contact with the lower-most stair step disappears, and the solution stabilizes. 

In addition to this, the program also outputs the values of the Lagrange multipliers. Remember that these are the contact forces and so should only be positive on the contact set, and zero outside. If, on the other hand, a Lagrange multiplier is negative in the active set, then this degree of freedom must be removed from the active set. The following pictures show the multipliers in iterations 1, 9 and 18, where we use red and browns to indicate positive values, and blue for negative values. 

 [2.x.3791]  

It is easy to see that the positive values converge nicely to moderate values in the interior of the contact set and large upward forces at the edges of the steps, as one would expect (to support the large curvature of the membrane there); at the fringes of the active set, multipliers are initially negative, causing the set to shrink until, in iteration 18, there are no more negative multipliers and the algorithm has converged. 




[1.x.1488] 

[1.x.1489] 

As with any of the programs of this tutorial, there are a number of obvious possibilities for extensions and experiments. The first one is clear: introduce adaptivity. Contact problems are prime candidates for adaptive meshes because the solution has lines along which it is less regular (the places where contact is established between membrane and obstacle) and other areas where the solution is very smooth (or, in the present context, constant wherever it is in contact with the obstacle). Adding this to the current program should not pose too many difficulties, but it is not trivial to find a good error estimator for that purpose. 

A more challenging task would be an extension to 3d. The problem here is not so much to simply make everything run in 3d. Rather, it is that when a 3d body is deformed and gets into contact with an obstacle, then the obstacle does not act as a constraining body force within the domain as is the case here. Rather, the contact force only acts on the boundary of the object. The inequality then is not in the differential equation but in fact in the (Neumann-type) boundary conditions, though this leads to a similar kind of variational inequality. Mathematically, this means that the Lagrange multiplier only lives on the surface, though it can of course be extended by zero into the domain if that is convenient. As in the current program, one does not need to form and store this Lagrange multiplier explicitly. 

A further interesting problem for the 3d case is to consider contact problems with friction. In almost every mechanical process friction has a big influence. For the modelling we have to take into account tangential stresses at the contact surface. Also we have to observe that friction adds another nonlinearity to our problem. 

Another nontrivial modification is to implement a more complex constitutive law like nonlinear elasticity or elasto-plastic  material behavior. The difficulty here is to handle the additional nonlinearity arising through the nonlinear constitutive law. 


examples/step-42/doc/intro.dox 

 [2.x.3792]  

[1.x.1490][1.x.1491] 




[1.x.1492] 

[1.x.1493] 

This example is an extension of step-41, considering a 3d contact problem with an elasto-plastic material behavior with isotropic hardening in three dimensions. In other words, it considers how a three-dimensional body deforms if one pushes into it a rigid obstacle (the contact problem) where deformation is governed by an elasto-plastic material law (a material that can only accommodate a certain maximal stress) that hardens as deformation accumulates. To show what we intend to do before going into too many details, let us just show a picture of what the solution will look like (the deformable body is a cube - only half of which is actually shown -, the obstacle corresponds to a Chinese character that is discussed below): 

 [2.x.3793]  


This problem description implies that we have to take care of an additional nonlinearity compared to step-41: the material behavior. Since we consider a three dimensional problem here, we also have to account for the fact that the contact area is at the boundary of the deformable body now, rather than in the interior. Finally, compared to step-41, we also have to deal with hanging nodes in both the handling of the linear system as well as of the inequality constraints as we would like to use an adaptive mesh; in the latter case, we will have to deal with prioritizing whether the constraints from the hanging nodes or from the inequalities are more important. 

Since you can very easily reach a few million degrees of freedom in three dimensions, even with adaptive mesh refinement, we decided to use Trilinos and p4est to run our code in parallel, building on the framework of step-40 for the parallelization. Additional pointers for parallelization can be found in step-32. 




[1.x.1494] 

The classical formulation of the problem possesses the following form: 

[1.x.1495] 

Here, the first of these equations defines the relationship between strain  [2.x.3794]  and stress  [2.x.3795]  via the fourth-order compliance tensor  [2.x.3796] ;  [2.x.3797]  provides the plastic component of the strain to ensure that the stress does not exceed the yield stress. We will only consider isotropic materials for which  [2.x.3798]  can be expressed in terms of the Lam&eacute; moduli  [2.x.3799]  and  [2.x.3800]  or alternatively in terms of the bulk modulus  [2.x.3801]  and  [2.x.3802] . The second equation is the force balance; we will here not consider any body forces and henceforth assume that  [2.x.3803] . The complementarity condition in the third line implies that  [2.x.3804]  if  [2.x.3805]  but that  [2.x.3806]  may be a nonzero tensor if and only if  [2.x.3807] , and in particular that in this case  [2.x.3808]  must point in the direction  [2.x.3809] . The inequality  [2.x.3810]  is a statement of the fact that plastic materials can only support a finite amount of stress; in other words, they react with plastic deformations  [2.x.3811]  if external forces would result in a stress  [2.x.3812]  for which  [2.x.3813]  would result. A typical form for this [1.x.1496] is  [2.x.3814]  where  [2.x.3815]  is the deviatoric part of a tensor and  [2.x.3816]  denotes the Frobenius norm. 

Further equations describe a fixed, zero displacement on  [2.x.3817]  and that on the surface  [2.x.3818]  where contact may appear, the normal force  [2.x.3819]  exerted by the obstacle is inward (no "pull" by the obstacle on our body) and with zero tangential component  [2.x.3820] . The last condition is again a complementarity condition that implies that on  [2.x.3821] , the normal force can only be nonzero if the body is in contact with the obstacle; the second part describes the impenetrability of the obstacle and the body. The last two equations are commonly referred to as the Signorini contact conditions. 

Most materials - especially metals - have the property that they show some hardening as a result of deformation. In other words,  [2.x.3822]  increases with deformation. In practice, it is not the elastic deformation that results in hardening, but the plastic component. There are different constitutive laws to describe those material behaviors. The simplest one is called linear isotropic hardening described by the flow function  [2.x.3823] . 




[1.x.1497] 

It is generally rather awkward to deal with inequalities. Here, we have to deal with two: plasticity and the contact problem. As described in more detail in the paper mentioned at the top of this page, one can at least reformulate the plasticity in a way that makes it look like a nonlinearity that we can then treat with Newton's method. This is slightly tricky mathematically since the nonlinearity is not just some smooth function but instead has kinks where the stress reaches the yield stress; however, it can be shown for such [1.x.1498] functions that Newton's method still converges. 

Without going into details, we will also get rid of the stress as an independent variable and instead work exclusively with the displacements  [2.x.3824] . Ultimately, the goal of this reformulation is that we will want to end up with a symmetric, positive definite problem - such as a linearized elasticity problem with spatially variable coefficients resulting from the plastic behavior - that needs to be solved in each Newton step. We want this because there are efficient and scalable methods for the solution of such linear systems, such as CG preconditioned with an algebraic multigrid. This is opposed to the saddle point problem akin to the mixed Laplace (see step-20) we would get were we to continue with the mixed formulation containing both displacements and stresses, and for which step-20 already gives a hint at how difficult it is to construct good solvers and preconditioners. 

With this said, let us simply state the problem we obtain after reformulation (again, details can be found in the paper): Find a displacement  [2.x.3825]  so that 

[1.x.1499] 

where the projector  [2.x.3826]  is defined as 

[1.x.1500] 

and the space  [2.x.3827]  is the space of all displacements that satisfy the contact condition: 

[1.x.1501] 



In the actual code, we will use the abbreviation  [2.x.3828] . 

Given this formulation, we will apply two techniques: 

- Run a Newton method to iterate out the nonlinearity in the projector. 

- Run an active set method for the contact condition, in much the same   way as we did in step-41. 

A strict approach would keep the active set fixed while we iterate the Newton method to convergence (or maybe the other way around: find the final active set before moving on to the next Newton iteration). In practice, it turns out that it is sufficient to do only a single Newton step per active set iteration, and so we will iterate over them concurrently. We will also, every once in a while, refine the mesh. 




[1.x.1502] 

As mentioned, we will treat the nonlinearity of the operator  [2.x.3829]  by applying a Newton method, despite the fact that the operator is not differentiable in the strict sense. However, it satisfies the conditions of [1.x.1503] differentiability and this turns out to be enough for Newton's method to work. The resulting method then goes by the name [1.x.1504], which sounds impressive but is, in reality, just a Newton method applied to a semi-smooth function with an appropriately chosen "derivative". 

In the current case, we will run our iteration by solving in each iteration  [2.x.3830]  the following equation (still an inequality, but linearized): 

[1.x.1505] 

where the rank-4 tensor  [2.x.3831]  given by 

[1.x.1506] 

This tensor is the (formal) linearization of  [2.x.3832]  around  [2.x.3833] . For the linear isotropic material we consider here, the bulk and shear components of the projector are given by 

[1.x.1507] 

where  [2.x.3834]  and  [2.x.3835]  are the identity tensors of rank 2 and 4, respectively. 

Note that this problem corresponds to a linear elastic contact problem where  [2.x.3836]  plays the role of the elasticity tensor  [2.x.3837] . Indeed, if the material is not plastic at a point, then  [2.x.3838] . However, at places where the material is plastic,  [2.x.3839]  is a spatially varying function. In any case, the system we have to solve for the Newton iterate  [2.x.3840]  gets us closer to the goal of rewriting our problem in a way that allows us to use well-known solvers and preconditioners for elliptic systems. 

As a final note about the Newton method let us mention that as is common with Newton methods we need to globalize it by controlling the step length. In other words, while the system above solves for  [2.x.3841] , the final iterate will rather be 

[1.x.1508] 

where the difference in parentheses on the right takes the role of the traditional Newton direction,  [2.x.3842] . We will determine  [2.x.3843]  using a standard line search. 




[1.x.1509] 

This linearized problem to be solved in each Newton step is essentially like in step-41. The only difference consists in the fact that the contact area is at the boundary instead of in the domain. But this has no further consequence so that we refer to the documentation of step-41 with the only hint that  [2.x.3844]  contains all the vertices at the contact boundary  [2.x.3845]  this time. As there, what we need to do is keep a subset of degrees of freedom fixed, leading to additional constraints that one can write as a saddle point problem. However, as discussed in the paper, by writing these constraints in an appropriate way that removes the coupling between degrees of freedom, we end up with a set of nodes that essentially just have Dirichlet values attached to them. 




[1.x.1510] 

The algorithm outlined above combines the damped semismooth Newton-method, which we use for the nonlinear constitutive law, with the semismooth Newton method for the contact. It works as follows: <ol>   [2.x.3846]  Initialize the active and inactive sets  [2.x.3847]  and  [2.x.3848]   such that  [2.x.3849]  and  [2.x.3850]  and set  [2.x.3851] . Here,  [2.x.3852]  is the set of  all degrees of freedom located at the surface of the domain where contact  may happen.  The start value  [2.x.3853]  fulfills our obstacle condition, i.e., we project an  initial zero displacement onto the set of feasible displacements. 

  [2.x.3854]  Assemble the Newton matrix  [2.x.3855]  and the right-hand-side  [2.x.3856] .  These correspond to the linearized Newton step, ignoring for the moment  the contact inequality. 

  [2.x.3857]  Find the primal-dual pair  [2.x.3858]  that satisfies  [1.x.1511] 

 As in step-41, we can obtain the solution to this problem by eliminating  those degrees of freedom in  [2.x.3859]  from the first equation and  obtain a linear system  [2.x.3860] . 




  [2.x.3861]  Damp the Newton iteration for  [2.x.3862]  by applying a line search and  calculating a linear combination of  [2.x.3863]  and  [2.x.3864] . This  requires finding an   [2.x.3865]  so that  [1.x.1512] 

 satisfies  [1.x.1513]  with  [2.x.3866]  with  the exceptions of (i) elements  [2.x.3867]  where we set  [2.x.3868] ,  and (ii) elements that correspond to hanging nodes, which we eliminate in the usual manner. 

  [2.x.3869]  Define the new active and inactive sets by  [1.x.1514] 

 [1.x.1515] 



  [2.x.3870] Project  [2.x.3871]  so that it satisfies the contact inequality,  [1.x.1516] 

 Here,   [2.x.3872]  is the projection of the active  components in  [2.x.3873]  to the gap  [1.x.1517] 

 where  [2.x.3874]  is the [1.x.1518] denoting the distance of the obstacle  from the undisplaced configuration of the body. 

  [2.x.3875]  If  [2.x.3876]  and  [2.x.3877]  then stop, else set  [2.x.3878]  and go to  step (1). This step ensures that we only stop iterations if both the correct  active set has been found and the plasticity has been iterated to sufficient  accuracy.  [2.x.3879]  

In step 3 of this algorithm, the matrix  [2.x.3880] ,  [2.x.3881]  describes the coupling of the bases for the displacements and Lagrange multiplier (contact forces) and it is not quadratic in our situation since  [2.x.3882]  is only defined on  [2.x.3883] , i.e., the surface where contact may happen. As shown in the paper, we can choose  [2.x.3884]  to be a matrix that has only one entry per row, (see also H&uuml;eber, Wohlmuth: A primal-dual active set strategy for non-linear multibody contact problems, Comput. Methods Appl. Mech. Engrg. 194, 2005, pp. 3147-3166). The vector  [2.x.3885]  is defined by a suitable approximation  [2.x.3886]  of the gap  [2.x.3887]  

[1.x.1519] 






[1.x.1520] 

Since we run our program in 3d, the computations the program performs are expensive. Consequently using adaptive mesh refinement is an important step towards staying within acceptable run-times. To make our lives easier we simply choose the KellyErrorEstimator that is already implemented in deal.II. We hand the solution vector to it which contains the displacement  [2.x.3888] . As we will see in the results it yields a quite reasonable adaptive mesh for the contact zone as well as for plasticity. 




[1.x.1521] 

This tutorial is essentially a mixture of step-40 and step-41 but instead of PETSc we let the Trilinos library deal with parallelizing the linear algebra (like in step-32). Since we are trying to solve a similar problem like in step-41 we will use the same methods but now in parallel. 

A difficulty is handling of the constraints from the Dirichlet conditions, hanging nodes and the inequality condition that arises from the contact. For this purpose we create three objects of type AffineConstraints that describe the various constraints and that we will combine as appropriate in each iteration. 

Compared to step-41, the programs has a few new classes: 

 [2.x.3889]   [2.x.3890]   [2.x.3891]  describes the plastic behavior of the   material 

 [2.x.3892]   [2.x.3893]  describes a sphere that serves as the   obstacle that is pushed into the deformable, elastoplastic body.   Whether this or the next class is used to describe the obstacle is   determined from the input parameter file. 

 [2.x.3894]   [2.x.3895]  (and a helper class) is a class that   allows us to read in an obstacle from a file. In the example we   will show in the results section, this file will be    [2.x.3896]  and will correspond to data that shows the   Chinese, Japanese or   Korean symbol for force or power (see http://www.orientaloutpost.com/ :   "This word can be used for motivation - it   can also mean power/motion/propulsion/force. It can be anything   internal or external that keeps you going. This is the safest way to express   motivation in Chinese. If your audience is Japanese, please see the other entry   for motivation. This is a word in Japanese and Korean, but it means "motive   power" or "kinetic energy" (without the motivation meaning that you are   probably looking for)"). In essence, we will pretend that we have a stamp   (i.e., a mask that corresponds to a flat bottomed obstacle with no pieces   of intermediate height) that we press into the body. The symbol in question   looks as follows (see also the picture at   the top of this section on how the end result looks like): 

   [2.x.3897]   [2.x.3898]  

Other than that, let us comment only on the following aspects:  [2.x.3899]   [2.x.3900]  The program allows you to select from two different coarse meshes   through the parameter file. These are either a cube  [2.x.3901]  or   a half sphere with the open side facing the positive  [2.x.3902]  direction. 

 [2.x.3903] In either case, we will assume the convention that the part of the   boundary that may be in contact with the obstacle has boundary   indicator one. For both kinds of meshes, we assume that this is a free   surface, i.e., the body is either in contact there or there is no force   acting on it. For the half sphere, the curved part has boundary   indicator zero and we impose zero displacement there. For the box,   we impose zero displacement along the bottom but allow vertical   displacement along the sides (though no horizontal displacement).  [2.x.3904]  


examples/step-42/doc/results.dox 



[1.x.1522] 

The directory that contains this program also contains a number of input parameter files that can be used to create various different simulations. For example, running the program with the  [2.x.3905]  parameter file (using a ball as obstacle and the box as domain) on 16 cores produces output like this: 

[1.x.1523] 



The tables at the end of each cycle show information about computing time (these numbers are of course specific to the machine on which this output was produced) and the number of calls of different parts of the program like assembly or calculating the residual, for the most recent mesh refinement cycle. Some of the numbers above can be improved by transferring the solution from one mesh to the next, an option we have not exercised here. Of course, you can also make the program run faster, especially on the later refinement cycles, by just using more processors: the accompanying paper shows good scaling to at least 1000 cores. 

In a typical run, you can observe that for every refinement step, the active set - the contact points - are iterated out at first. After that the Newton method has only to resolve the plasticity. For the finer meshes, quadratic convergence can be observed for the last 4 or 5 Newton iterations. 

We will not discuss here in all detail what happens with each of the input files. Rather, let us just show pictures of the solution (the left half of the domain is omitted if cells have zero quadrature points at which the plastic inequality is active): 

 [2.x.3906]  

The picture shows the adaptive refinement and as well how much a cell is plastified during the contact with the ball. Remember that we consider the norm of the deviator part of the stress in each quadrature point to see if there is elastic or plastic behavior. The blue color means that this cell contains only elastic quadrature points in contrast to the red cells in which all quadrature points are plastified. In the middle of the top surface - where the mesh is finest - a very close look shows the dimple caused by the obstacle. This is the result of the  [2.x.3907]  function. However, because the indentation of the obstacles we consider here is so small, it is hard to discern this effect; one could play with displacing vertices of the mesh by a multiple of the computed displacement. 

Further discussion of results that can be obtained using this program is provided in the publication mentioned at the very top of this page. 


[1.x.1524] 

[1.x.1525] 

There are, as always, multiple possibilities for extending this program. From an algorithmic perspective, this program goes about as far as one can at the time of writing, using the best available algorithms for the contact inequality, the plastic nonlinearity, and the linear solvers. However, there are things one would like to do with this program as far as more realistic situations are concerned:  [2.x.3908]   [2.x.3909]  Extend the program from a static to a quasi-static situation, perhaps by choosing a backward-Euler-scheme for the time discretization. Some theoretical results can be found in the PhD thesis by Jörg Frohne, [1.x.1526], University of Siegen, Germany, 2011. 

 [2.x.3910]  It would also be an interesting advance to consider a contact problem with friction. In almost every mechanical process friction has a big influence.  To model this situation, we have to take into account tangential stresses at the contact surface. Friction also adds another inequality to our problem since body and obstacle will typically stick together as long as the tangential stress does not exceed a certain limit, beyond which the two bodies slide past each other. 

 [2.x.3911]  If we already simulate a frictional contact, the next step to consider is heat generation over the contact zone. The heat that is caused by friction between two bodies raises the temperature in the deformable body and entails an change of some material parameters. 

 [2.x.3912]  It might be of interest to implement more accurate, problem-adapted error estimators for contact as well as for the plasticity.  [2.x.3913]  


examples/step-43/doc/intro.dox 

 [2.x.3914]  

[1.x.1527] 


[1.x.1528] [1.x.1529] 

The simulation of multiphase flow in porous media is a ubiquitous problem, and we have previously addressed it already in some form in step-20 and step-21. However, as was easy to see there, it faces two major difficulties: numerical accuracy and efficiency. The first is easy to see in the stationary solver step-20: using lowest order Raviart-Thomas elements can not be expected to yield highly accurate solutions. We need more accurate methods. The second reason is apparent from the time dependent step-21: that program is excruciatingly slow, and there is no hope to get highly accurate solutions in 3d within reasonable time frames. 

In this program, in order to overcome these two problems, there are five areas which we are trying to improve for a high performance simulator: 

 [2.x.3915]   [2.x.3916]  Higher order spatial discretizations  [2.x.3917]  Adaptive mesh refinement  [2.x.3918]  Adaptive time stepping  [2.x.3919]  Operator splitting  [2.x.3920]  Efficient solver and preconditioning  [2.x.3921]  

Much inspiration for this program comes from step-31 but several of the techniques discussed here are original. 




[1.x.1530] 

We consider the flow of a two-phase immiscible, incompressible fluid. Capillary and gravity effects are neglected, and viscous effects are assumed dominant. The governing equations for such a flow that are identical to those used in step-21 and are 

[1.x.1531] 

where  [2.x.3922]  is the saturation (volume fraction between zero and one) of the second (wetting) phase,  [2.x.3923]  is the pressure,  [2.x.3924]  is the permeability tensor,  [2.x.3925]  is the total mobility,  [2.x.3926]  is the porosity,  [2.x.3927]  is the fractional flow of the wetting phase,  [2.x.3928]  is the source term and  [2.x.3929]  is the total velocity. The total mobility, fractional flow of the wetting phase and total velocity are respectively given by 

[1.x.1532] 

where subscripts  [2.x.3930]  represent the wetting and non-wetting phases, respectively. 

For convenience, the porosity  [2.x.3931]  in the saturation equation, which can be considered a scaling factor for the time variable, is set to one. Following a commonly used prescription for the dependence of the relative permeabilities  [2.x.3932]  and  [2.x.3933]  on saturation, we use 

[1.x.1533] 



The porous media equations above are augmented by initial conditions for the saturation and boundary conditions for the pressure. Since saturation and the gradient of the pressure uniquely determine the velocity, no boundary conditions are necessary for the velocity. Since the flow equations do not contain time derivatives, initial conditions for the velocity and pressure variables are not required. The flow field separates the boundary into inflow or outflow parts. Specifically, 

[1.x.1534] 

and we arrive at a complete model by also imposing boundary values for the saturation variable on the inflow boundary  [2.x.3934] . 




[1.x.1535] 

As seen in step-21, solving the flow equations for velocity and pressure are the parts of the program that take far longer than the (explicit) updating step for the saturation variable once we know the flow variables. On the other hand,  the pressure and velocity depend only weakly on saturation, so one may think about only solving for pressure and velocity every few time steps while updating the saturation in every step. If we can find a criterion for when the flow variables need to be updated, we call this splitting an "adaptive operator splitting" scheme. 

Here, we use the following a posteriori criterion to decide when to re-compute pressure and velocity variables (detailed derivations and descriptions can be found in [Chueh, Djilali and Bangerth 2011]): 

[1.x.1536] 

where superscripts in parentheses denote the number of the saturation time step at which any quantity is defined and  [2.x.3935]  represents the last step where we actually computed the pressure and velocity. If  [2.x.3936]  exceeds a certain threshold we re-compute the flow variables; otherwise, we skip this computation in time step  [2.x.3937]  and only move the saturation variable one time step forward. 

In short, the algorithm allows us to perform a number of saturation time steps of length  [2.x.3938]  until the criterion above tells us to re-compute velocity and pressure variables, leading to a macro time step of length 

[1.x.1537] 

We choose the length of (micro) steps subject to the Courant-Friedrichs-Lewy (CFL) restriction according to the criterion 

[1.x.1538] 

which we have confirmed to be stable for the choice of finite element and time stepping scheme for the saturation equation discussed below ( [2.x.3939]  denotes the diameter of cell  [2.x.3940] ). The result is a scheme where neither micro nor macro time steps are of uniform length, and both are chosen adaptively. 

[1.x.1539] Using this time discretization, we obtain the following set of equations for each time step from the IMPES approach (see step-21): 

[1.x.1540] 




Using the fact that  [2.x.3941] , the time discrete saturation equation becomes 

[1.x.1541] 



[1.x.1542] 

By multiplying the equations defining the total velocity  [2.x.3942]  and the equation that expresses its divergence in terms of source terms, with test functions  [2.x.3943]  and  [2.x.3944]  respectively and then integrating terms by parts as necessary, the weak form of the problem reads: Find  [2.x.3945]  so that for all test functions  [2.x.3946]  there holds 

[1.x.1543] 

Here,  [2.x.3947]  represents the unit outward normal vector to  [2.x.3948]  and the pressure  [2.x.3949]  can be prescribed weakly on the open part of the boundary  [2.x.3950]  whereas on those parts where a velocity is prescribed (for example impermeable boundaries with  [2.x.3951]  the term disappears altogether because  [2.x.3952] . 

We use continuous finite elements to discretize the velocity and pressure equations. Specifically, we use mixed finite elements to ensure high order approximation for both vector (e.g. a fluid velocity) and scalar variables (e.g. pressure) simultaneously. For saddle point problems, it is well established that the so-called Babuska-Brezzi or Ladyzhenskaya-Babuska-Brezzi (LBB) conditions [Brezzi 1991, Chen 2005] need to be satisfied to ensure stability of the pressure-velocity system. These stability conditions are satisfied in the present work by using elements for velocity that are one order higher than for the pressure, i.e.  [2.x.3953]  and  [2.x.3954] , where  [2.x.3955] ,  [2.x.3956]  is the space dimension, and  [2.x.3957]  denotes the space of tensor product Lagrange polynomials of degree  [2.x.3958]  in each variable. 

[1.x.1544] The chosen  [2.x.3959]  elements for the saturation equation do not lead to a stable discretization without upwinding or other kinds of stabilization, and spurious oscillations will appear in the numerical solution. Adding an artificial diffusion term is one approach to eliminating these oscillations [Chen 2005]. On the other hand, adding too much diffusion smears sharp fronts in the solution and suffers from grid-orientation difficulties [Chen 2005]. To avoid these effects, we use the artificial diffusion term proposed by [Guermond and Pasquetti 2008] and validated in [Chueh, Djilali, Bangerth 2011] and [Kronbichler, Heister and Bangerth, 2011], as well as in step-31. 

This method modifies the (discrete) weak form of the saturation equation to read 

[1.x.1545] 

where  [2.x.3960]  is the artificial diffusion parameter and  [2.x.3961]  is an appropriately chosen numerical flux on the boundary of the domain (we choose the obvious full upwind flux for this). 

Following [Guermond and Pasquetti 2008] (and as detailed in [Chueh, Djilali and Bangerth 2011]), we use the parameter as a piecewise constant function set on each cell  [2.x.3962]  with the diameter  [2.x.3963]  as 

[1.x.1546] 

where  [2.x.3964]  is a stabilization exponent and  [2.x.3965]  is a dimensionless user-defined stabilization constant. Following [Guermond and Pasquetti 2008] as well as the implementation in step-31, the velocity and saturation global normalization constant,  [2.x.3966] , and the residual  [2.x.3967]  are respectively given by 

[1.x.1547] 

and 

[1.x.1548] 

where  [2.x.3968]  is a second dimensionless user-defined constant,  [2.x.3969]  is the diameter of the domain and  [2.x.3970]  is the range of the present saturation values in the entire computational domain  [2.x.3971] . 

This stabilization scheme has a number of advantages over simpler schemes such as finite volume (or discontinuous Galerkin) methods or streamline upwind Petrov Galerkin (SUPG) discretizations. In particular, the artificial diffusion term acts primarily in the vicinity of discontinuities since the residual is small in areas where the saturation is smooth. It therefore provides for a higher degree of accuracy. On the other hand, it is nonlinear since  [2.x.3972]  depends on the saturation  [2.x.3973] . We avoid this difficulty by treating all nonlinear terms explicitly, which leads to the following fully discrete problem at time step  [2.x.3974] : 

[1.x.1549] 

where  [2.x.3975]  is the velocity linearly extrapolated from  [2.x.3976]  and  [2.x.3977]  to the current time  [2.x.3978]  if  [2.x.3979]  while  [2.x.3980]  is  [2.x.3981]  if  [2.x.3982] . Consequently, the equation is linear in  [2.x.3983]  and all that is required is to solve with a mass matrix on the saturation space. 

Since the Dirichlet boundary conditions for saturation are only imposed on the inflow boundaries, the third term on the left hand side of the equation above needs to be split further into two parts: 

[1.x.1550] 

where  [2.x.3984]  and  [2.x.3985]  represent inflow and outflow boundaries, respectively. We choose values using an upwind formulation, i.e.  [2.x.3986]  and  [2.x.3987]  correspond to the values taken from the present cell, while the values of  [2.x.3988]  and  [2.x.3989]  are those taken from the neighboring boundary  [2.x.3990] . 




[1.x.1551] 

Choosing meshes adaptively to resolve sharp saturation fronts is an essential ingredient to achieve efficiency in our algorithm. Here, we use the same shock-type refinement approach used in [Chueh, Djilali and Bangerth 2011] to select those cells that should be refined or coarsened. The refinement indicator for each cell  [2.x.3991]  of the triangulation is computed by 

[1.x.1552] 

where  [2.x.3992]  is the gradient of the discrete saturation variable evaluated at the center  [2.x.3993]  of cell  [2.x.3994] . This approach is analogous to ones frequently used in compressible flow problems, where density gradients are used to indicate refinement. That said, as we will discuss at the end of the [1.x.1553], this turns out to not be a very useful criterion since it leads to refinement basically everywhere. We only show it here for illustrative purposes. 




[1.x.1554] 

Following the discretization of the governing equations discussed above, we obtain a linear system of equations in time step  [2.x.3995]  of the following form: 

[1.x.1555] 

where the individual matrices and vectors are defined as follows using shape functions  [2.x.3996]  for velocity, and  [2.x.3997]  for both pressure and saturation: 

[1.x.1556] 

and  [2.x.3998]  as given in the definition of the stabilized transport equation. 

The linear system above is of block triangular form if we consider the top left  [2.x.3999]  panel of matrices as one block. We can therefore first solve for the velocity and pressure (unless we decide to use  [2.x.4000]  in place of the velocity) followed by a solve for the saturation variable. The first of these steps requires us to solve 

[1.x.1557] 

We apply the Generalized Minimal Residual (GMRES) method [Saad and Schultz 1986] to this linear system. The ideal preconditioner for the velocity-pressure system is 

[1.x.1558] 

where  [2.x.4001]  is the Schur complement [Zhang 2005] of the system. This preconditioner is optimal since 

[1.x.1559] 

for which it can be shown that GMRES converges in two iterations. 

However, we cannot of course expect to use exact inverses of the velocity mass matrix and the Schur complement. We therefore follow the approach by [Silvester and Wathen 1994] originally proposed for the Stokes system. Adapting it to the current set of equations yield the preconditioner 

[1.x.1560] 

where a tilde indicates an approximation of the exact inverse matrix. In particular, since  [2.x.4002]  is a sparse symmetric and positive definite matrix, we choose for  [2.x.4003]  a single application of a sparse incomplete Cholesky decomposition of this matrix [Golub and Van Loan 1996]. We note that the Schur complement that corresponds to the porous media flow operator in non-mixed form,  [2.x.4004]  and  [2.x.4005]  should be a good approximation of the actual Schur complement matrix  [2.x.4006] . Since both of these matrices are again symmetric and positive definite, we use an incomplete Cholesky decomposition of  [2.x.4007]  for  [2.x.4008] . It is important to note that  [2.x.4009]  needs to be built with Dirichlet boundary conditions to ensure its invertibility. 

Once the velocity  [2.x.4010]   is available, we can assemble  [2.x.4011]  and  [2.x.4012]  and solve for the saturations using 

[1.x.1561] 

where the mass matrix  [2.x.4013]  is solved by the conjugate gradient method, using an incomplete Cholesky decomposition as preconditioner once more. 

[1.x.1562] 

 [2.x.4014]  The implementation discussed here uses and extends parts of the step-21, step-31 and step-33 tutorial programs of this library. In particular, if you want to understand how it works, please consult step-21 for a discussion of the mathematical problem, and step-31 from which most of the implementation is derived. We will not discuss aspects of the implementation that have already been discussed in step-31. 

We show numerical results for some two-phase flow equations augmented by appropriate initial and boundary conditions in conjunction with two different choices of the permeability model. In the problems considered, there is no internal source term ( [2.x.4015] ). As mentioned above, quantitative numerical results are presented in [Chueh, Djilali and Bangerth 2011]. 

For simplicity, we choose  [2.x.4016] , though all methods (as well as our implementation) should work equally well on general unstructured meshes. 

Initial conditions are only required for the saturation variable, and we choose  [2.x.4017] , i.e. the porous medium is initially filled by a mixture of the non-wetting (80%) and wetting (20%) phases. This differs from the initial condition in step-21 where we had taken  [2.x.4018] , but for complicated mathematical reasons that are mentioned there in a longish remark, the current method using an entropy-based artificial diffusion term does not converge to the viscosity solution with this initial condition without additional modifications to the method. We therefore choose this modified version for the current program. 

Furthermore, we prescribe a linear pressure on the boundaries: 

[1.x.1563] 

Pressure and saturation uniquely determine a velocity, and the velocity determines whether a boundary segment is an inflow or outflow boundary. On the inflow part of the boundary,  [2.x.4019] , we impose 

[1.x.1564] 

In other words, the domain is flooded by the wetting phase from the left. No boundary conditions for the saturation are required for the outflow parts of the boundary. 

All the numerical and physical parameters used for the 2D/3D cases are listed in the following table: 

 [2.x.4020]  




[1.x.1565] 


<ol>  [2.x.4021]  CC Chueh, N Djilali and W Bangerth.  [2.x.4022]  An h-adaptive operator splitting method for two-phase flow in 3D   heterogeneous porous media.  [2.x.4023]  SIAM Journal on Scientific Computing, vol. 35 (2013), pp. B149-B175 

 [2.x.4024]  M. Kronbichler, T. Heister, and W. Bangerth  [2.x.4025]  High Accuracy Mantle Convection Simulation through Modern Numerical Methods.  [2.x.4026]  Geophysics Journal International, vol. 191 (2012), pp. 12-29 

 [2.x.4027]  F Brezzi and M Fortin.  [2.x.4028]  [1.x.1566].  [2.x.4029]  Springer-Verlag, 1991. 

 [2.x.4030]  Z Chen.  [2.x.4031]  [1.x.1567].  [2.x.4032]  Springer, 2005. 

 [2.x.4033]  JL Guermond and R Pasquetti.  [2.x.4034]  Entropy-based nonlinear viscosity for Fourier approximations of   conservation laws.  [2.x.4035]  [1.x.1568], 346(13-14):801-806, 2008. 

 [2.x.4036]  CC Chueh, M Secanell, W Bangerth, and N Djilali.  [2.x.4037]  Multi-level adaptive simulation of transient two-phase flow in   heterogeneous porous media.  [2.x.4038]  [1.x.1569], 39:1585-1596, 2010. 

 [2.x.4039]  Y Saad and MH Schultz.  [2.x.4040]  Gmres: A generalized minimal residual algorithm for solving   nonsymmetric linear systems.  [2.x.4041]  [1.x.1570],   7(3):856-869, 1986. 

 [2.x.4042]  F Zhang.  [2.x.4043]  [1.x.1571].  [2.x.4044]  Springer, 2005. 

 [2.x.4045]  D Silvester and A Wathen.  [2.x.4046]  Fast iterative solution of stabilised Stokes systems part ii: Using   general block preconditioners.  [2.x.4047]  [1.x.1572], 31(5):1352-1367, 1994. 

 [2.x.4048]  GH Golub and CF van Loan.  [2.x.4049]  [1.x.1573].  [2.x.4050]  3rd Edition, Johns Hopkins, 1996. 

 [2.x.4051]  SE Buckley and MC Leverett.  [2.x.4052]  Mechanism of fluid displacements in sands.  [2.x.4053]  [1.x.1574], 146:107-116, 1942. 

 [2.x.4054]  


examples/step-43/doc/results.dox 



[1.x.1575] 


The output of this program is not really much different from that of step-21: it solves the same problem, after all. Of more importance are quantitative metrics such as the accuracy of the solution as well as the time needed to compute it. These are documented in detail in the two publications listed at the top of this page and we won't repeat them here. 

That said, no tutorial program is complete without a couple of good pictures, so here is some output of a run in 3d: 

 [2.x.4055]  


[1.x.1576] 

[1.x.1577] 

The primary objection one may have to this program is that it is still too slow: 3d computations on reasonably fine meshes are simply too expensive to be done routinely and with reasonably quick turn-around. This is similar to the situation we were in when we wrote step-31, from which this program has taken much inspiration. The solution is similar as it was there as well: We need to parallelize the program in a way similar to how we derived step-32 out of step-31. In fact, all of the techniques used in step-32 would be transferable to this program as well, making the program run on dozens or hundreds of processors immediately. 

A different direction is to make the program more relevant to many other porous media applications. Specifically, one avenue is to go to the primary user of porous media flow simulators, namely the oil industry. There, applications in this area are dominated by multiphase flow (i.e., more than the two phases we have here), and the reactions they may have with each other (or any other way phases may exchange mass, such as through dissolution in and bubbling out of gas from the oil phase). Furthermore, the presence of gas often leads to compressibility effects of the fluid. Jointly, these effects are typically formulated in the widely-used "black oil model". True reactions between multiple phases also play a role in oil reservoir modeling when considering controlled burns of oil in the reservoir to raise pressure and temperature. These are much more complex problems, though, and left for future projects. 

Finally, from a mathematical perspective, we have derived the criterion for re-computing the velocity/pressure solution at a given time step under the assumption that we want to compare the solution we would get at the current time step with that computed the last time we actually solved this system. However, in the program, whenever we did not re-compute the solution, we didn't just use the previously computed solution but instead extrapolated from the previous two times we solved the system. Consequently, the criterion was pessimistically stated: what we should really compare is the solution we would get at the current time step with the extrapolated one. Re-stating the theorem in this regard is left as an exercise. 

There are also other ways to extend the mathematical foundation of this program; for example, one may say that it isn't the velocity we care about, but in fact the saturation. Thus, one may ask whether the criterion we use here to decide whether  [2.x.4056]  needs to be recomputed is appropriate; one may, for example, suggest that it is also important to decide whether (and by how much) a wrong velocity field in fact affects the solution of the saturation equation. This would then naturally lead to a sensitivity analysis. 

From an algorithmic viewpoint, we have here used a criterion for refinement that is often used in engineering, namely by looking at the gradient of the solution. However, if you inspect the solution, you will find that it quickly leads to refinement almost everywhere, even in regions where it is clearly not necessary: frequently used therefore does not need to imply that it is a useful criterion to begin with. On the other hand, replacing this criterion by a different and better one should not be very difficult. For example, the KellyErrorEstimator class used in many other programs should certainly be applicable to the current problem as well. 


examples/step-44/doc/intro.dox 

 [2.x.4057]  

[1.x.1578] 

 [2.x.4058]  

[1.x.1579] 

[1.x.1580] 

The subject of this tutorial is nonlinear solid mechanics. Classical single-field approaches (see e.g. step-18) can not correctly describe the response of quasi-incompressible materials. The response is overly stiff; a phenomenon known as locking. Locking problems can be circumvented using a variety of alternative strategies. One such strategy is the  three-field formulation. It is used here  to model the three-dimensional, fully-nonlinear (geometrical and material) response of an isotropic continuum body. The material response is approximated as hyperelastic. Additionally, the three-field formulation employed is valid for quasi-incompressible as well as compressible materials. 

The objective of this presentation is to provide a basis for using deal.II for problems in nonlinear solid mechanics. The linear problem was addressed in step-8. A non-standard, hypoelastic-type form of the geometrically nonlinear problem was partially considered in step-18: a rate form of the linearised constitutive relations is used and the problem domain evolves with the motion. Important concepts surrounding the nonlinear kinematics are absent in the theory and implementation. Step-18 does, however, describe many of the key concepts to implement elasticity within the framework of deal.II. 

We begin with a crash-course in nonlinear kinematics. For the sake of simplicity, we restrict our attention to the quasi-static problem. Thereafter, various key stress measures are introduced and the constitutive model described. We then describe the three-field formulation in detail prior to explaining the structure of the class used to manage the material. The setup of the example problem is then presented. 

 [2.x.4059]  This tutorial has been developed (and is described in the introduction) for the problem of elasticity in three dimensions.  While the space dimension could be changed in the main() routine, care needs to be taken.  Two-dimensional elasticity problems, in general, exist only as idealizations of three-dimensional ones.  That is, they are either plane strain or plane stress.  The assumptions that follow either of these choices needs to be consistently imposed.  For more information see the note in step-8. 

[1.x.1581] 

The three-field formulation implemented here was pioneered by Simo et al. (1985) and is known as the mixed Jacobian-pressure formulation. Important related contributions include those by Simo and Taylor (1991), and Miehe (1994). The notation adopted here draws heavily on the excellent overview of the theoretical aspects of nonlinear solid mechanics by Holzapfel (2001). A nice overview of issues pertaining to incompressible elasticity (at small strains) is given in Hughes (2000). 

<ol> 	 [2.x.4060]  J.C. Simo, R.L. Taylor and K.S. Pister (1985), 		Variational and projection methods for the volume constraint in finite deformation elasto-plasticity, 		 [2.x.4061]  Computer Methods in Applied Mechanics and Engineering  [2.x.4062] , 		<strong> 51 </strong>, 1-3, 		177-208. 		DOI: [1.x.1582]; 	 [2.x.4063]  J.C. Simo and R.L. Taylor (1991),   		Quasi-incompressible finite elasticity in principal stretches. Continuum 			basis and numerical algorithms, 		 [2.x.4064]  Computer Methods in Applied Mechanics and Engineering  [2.x.4065] , 		<strong> 85 </strong>, 3, 		273-310. 		DOI: [1.x.1583]; 	 [2.x.4066]  C. Miehe (1994), 		Aspects of the formulation and finite element implementation of large strain isotropic elasticity 		 [2.x.4067]  International Journal for Numerical Methods in Engineering  [2.x.4068]  		<strong> 37 </strong>, 12, 		1981-2004. 		DOI: [1.x.1584]; 	 [2.x.4069]  G.A. Holzapfel (2001), 		Nonlinear Solid Mechanics. A Continuum Approach for Engineering, 		John Wiley & Sons. 		ISBN: 0-471-82304-X; 	 [2.x.4070]  T.J.R. Hughes (2000), 		The Finite Element Method: Linear Static and Dynamic Finite Element Analysis, 		Dover. 		ISBN: 978-0486411811  [2.x.4071]  

An example where this three-field formulation is used in a coupled problem is documented in <ol> 	 [2.x.4072]  J-P. V. Pelteret, D. Davydov, A. McBride, D. K. Vu, and P. Steinmann (2016), 		Computational electro- and magneto-elasticity for quasi-incompressible media immersed in free space, 		 [2.x.4073]  International Journal for Numerical Methods in Engineering  [2.x.4074] . 		DOI: [1.x.1585]  [2.x.4075]  

[1.x.1586] 

One can think of fourth-order tensors as linear operators mapping second-order tensors (matrices) onto themselves in much the same way as matrices map vectors onto vectors. There are various fourth-order unit tensors that will be required in the forthcoming presentation. The fourth-order unit tensors  [2.x.4076]  and  [2.x.4077]  are defined by 

[1.x.1587] 

Note  [2.x.4078] . Furthermore, we define the symmetric and skew-symmetric fourth-order unit tensors by 

[1.x.1588] 

such that 

[1.x.1589] 

The fourth-order  [2.x.4079]  returned by identity_tensor() is  [2.x.4080] . 




[1.x.1590] 

Let the time domain be denoted  [2.x.4081] , where  [2.x.4082]  and  [2.x.4083]  is the total problem duration. Consider a continuum body that occupies the reference configuration  [2.x.4084]  at time  [2.x.4085] . %Particles in the reference configuration are identified by the position vector  [2.x.4086] . The configuration of the body at a later time  [2.x.4087]  is termed the current configuration, denoted  [2.x.4088] , with particles identified by the vector  [2.x.4089] . The nonlinear map between the reference and current configurations, denoted  [2.x.4090] , acts as follows: 

[1.x.1591] 

The material description of the displacement of a particle is defined by 

[1.x.1592] 



The deformation gradient  [2.x.4091]  is defined as the material gradient of the motion: 

[1.x.1593] 

The determinant of the of the deformation gradient  [2.x.4092]  maps corresponding volume elements in the reference and current configurations, denoted  [2.x.4093]  and  [2.x.4094] , respectively, as 

[1.x.1594] 



Two important measures of the deformation in terms of the spatial and material coordinates are the left and right Cauchy-Green tensors, respectively, and denoted  [2.x.4095]  and  [2.x.4096] . They are both symmetric and positive definite. 

The Green-Lagrange strain tensor is defined by 

[1.x.1595] 

If the assumption of infinitesimal deformations is made, then the second term on the right can be neglected, and  [2.x.4097]  (the linearised strain tensor) is the only component of the strain tensor. This assumption is, looking at the setup of the problem, not valid in step-18, making the use of the linearized  [2.x.4098]  as the strain measure in that tutorial program questionable. 

In order to handle the different response that materials exhibit when subjected to bulk and shear type deformations we consider the following decomposition of the deformation gradient  [2.x.4099]   and the left Cauchy-Green tensor  [2.x.4100]  into volume-changing (volumetric) and volume-preserving (isochoric) parts: 

[1.x.1596] 

Clearly,  [2.x.4101] . 

The spatial velocity field is denoted  [2.x.4102] . The derivative of the spatial velocity field with respect to the spatial coordinates gives the spatial velocity gradient  [2.x.4103] , that is 

[1.x.1597] 

where  [2.x.4104] . 




[1.x.1598] 

Cauchy's stress theorem equates the Cauchy traction  [2.x.4105]  acting on an infinitesimal surface element in the current configuration  [2.x.4106]  to the product of the Cauchy stress tensor  [2.x.4107]  (a spatial quantity)  and the outward unit normal to the surface  [2.x.4108]  as 

[1.x.1599] 

The Cauchy stress is symmetric. Similarly,  the first Piola-Kirchhoff traction  [2.x.4109]  which acts on an infinitesimal surface element in the reference configuration  [2.x.4110]  is the product of the first Piola-Kirchhoff stress tensor  [2.x.4111]  (a two-point tensor)  and the outward unit normal to the surface  [2.x.4112]  as 

[1.x.1600] 

The Cauchy traction  [2.x.4113]  and the first Piola-Kirchhoff traction  [2.x.4114]  are related as 

[1.x.1601] 

This can be demonstrated using [1.x.1602]. 

The first Piola-Kirchhoff stress tensor is related to the Cauchy stress as 

[1.x.1603] 

Further important stress measures are the (spatial) Kirchhoff stress   [2.x.4115]  and the (referential) second Piola-Kirchhoff stress  [2.x.4116] . 




[1.x.1604] 

Push-forward and pull-back operators allow one to transform various measures between the material and spatial settings. The stress measures used here are contravariant, while the strain measures are covariant. 

The push-forward and-pull back operations for second-order covariant tensors  [2.x.4117]  are respectively given by: 

[1.x.1605] 



The push-forward and pull back operations for second-order contravariant tensors  [2.x.4118]  are respectively given by: 

[1.x.1606] 

For example  [2.x.4119] . 




[1.x.1607] 

A hyperelastic material response is governed by a Helmholtz free energy function  [2.x.4120]  which serves as a potential for the stress. For example, if the Helmholtz free energy depends on the right Cauchy-Green tensor  [2.x.4121]  then the isotropic hyperelastic response is 

[1.x.1608] 

If the Helmholtz free energy depends on the left Cauchy-Green tensor  [2.x.4122]  then the isotropic hyperelastic response is 

[1.x.1609] 



Following the multiplicative decomposition of the deformation gradient, the Helmholtz free energy can be decomposed as 

[1.x.1610] 

Similarly, the Kirchhoff stress can be decomposed into volumetric and isochoric parts as  [2.x.4123]  where: 

[1.x.1611] 

where  [2.x.4124]  is the pressure response.  [2.x.4125]  is the projection tensor which provides the deviatoric operator in the Eulerian setting. The fictitious Kirchhoff stress tensor  [2.x.4126]  is defined by 

[1.x.1612] 






 [2.x.4127]  The pressure response as defined above differs from the widely-used definition of the pressure in solid mechanics as  [2.x.4128] . Here  [2.x.4129]  is the hydrostatic pressure. We make use of the pressure response throughout this tutorial (although we refer to it as the pressure). 

[1.x.1613] 

The Helmholtz free energy corresponding to a compressible [1.x.1614] is given by 

[1.x.1615] 

where  [2.x.4130]  is the bulk modulus ( [2.x.4131]  and  [2.x.4132]  are the Lam&eacute; parameters) and  [2.x.4133] . The function  [2.x.4134]  is required to be strictly convex and satisfy the condition  [2.x.4135] , among others, see Holzapfel (2001) for further details. In this work  [2.x.4136] . 

Incompressibility imposes the isochoric constraint that  [2.x.4137]  for all motions  [2.x.4138] . The Helmholtz free energy corresponding to an incompressible neo-Hookean material is given by 

[1.x.1616] 

where  [2.x.4139] . Thus, the incompressible response is obtained by removing the volumetric component from the compressible free energy and enforcing  [2.x.4140] . 




[1.x.1617] 

We will use a Newton-Raphson strategy to solve the nonlinear boundary value problem. Thus, we will need to linearise the constitutive relations. 

The fourth-order elasticity tensor in the material description is defined by 

[1.x.1618] 

The fourth-order elasticity tensor in the spatial description  [2.x.4141]  is obtained from the push-forward of  [2.x.4142]  as 

[1.x.1619] 

The fourth-order elasticity tensors (for hyperelastic materials) possess both major and minor symmetries. 

The fourth-order spatial elasticity tensor can be written in the following decoupled form: 

[1.x.1620] 

where 

[1.x.1621] 

where the fictitious elasticity tensor  [2.x.4143]  in the spatial description is defined by 

[1.x.1622] 



[1.x.1623] 

The total potential energy of the system  [2.x.4144]  is the sum of the internal and external potential energies, denoted  [2.x.4145]  and  [2.x.4146] , respectively. We wish to find the equilibrium configuration by minimising the potential energy. 

As mentioned above, we adopt a three-field formulation. We denote the set of primary unknowns by  [2.x.4147] . The independent kinematic variable  [2.x.4148]  enters the formulation as a constraint on  [2.x.4149]  enforced by the Lagrange multiplier  [2.x.4150]  (the pressure, as we shall see). 

The three-field variational principle used here is given by 

[1.x.1624] 

where the external potential is defined by 

[1.x.1625] 

The boundary of the current configuration   [2.x.4151]  is composed into two parts as  [2.x.4152] , where  [2.x.4153] . The prescribed Cauchy traction, denoted  [2.x.4154] , is applied to  [2.x.4155]  while the motion is prescribed on the remaining portion of the boundary  [2.x.4156] . The body force per unit current volume is denoted  [2.x.4157] . 




The stationarity of the potential follows as 

[1.x.1626] 

for all virtual displacements  [2.x.4158]  subject to the constraint that  [2.x.4159]  on  [2.x.4160] , and all virtual pressures  [2.x.4161]  and virtual dilatations  [2.x.4162] . 

One should note that the definitions of the volumetric Kirchhoff stress in the three field formulation  [2.x.4163]   and the subsequent volumetric tangent differs slightly from the general form given in the section on hyperelastic materials where  [2.x.4164] . This is because the pressure  [2.x.4165]  is now a primary field as opposed to a constitutively derived quantity. One needs to carefully distinguish between the primary fields and those obtained from the constitutive relations. 

 [2.x.4166]  Although the variables are all expressed in terms of spatial quantities, the domain of integration is the initial configuration. This approach is called a  [2.x.4167]  total-Lagrangian formulation  [2.x.4168] . The approach given in step-18, where the domain of integration is the current configuration, could be called an  [2.x.4169]  updated Lagrangian formulation  [2.x.4170] . The various merits of these two approaches are discussed widely in the literature. It should be noted however that they are equivalent. 


The Euler-Lagrange equations corresponding to the residual are: 

[1.x.1627] 

The first equation is the (quasi-static) equilibrium equation in the spatial setting. The second is the constraint that  [2.x.4171] . The third is the definition of the pressure  [2.x.4172] . 

 [2.x.4173]  The simplified single-field derivation ( [2.x.4174]  is the only primary variable) below makes it clear how we transform the limits of integration to the reference domain: 

[1.x.1628] 

where  [2.x.4175] . 

We will use an iterative Newton-Raphson method to solve the nonlinear residual equation  [2.x.4176] . For the sake of simplicity we assume dead loading, i.e. the loading does not change due to the deformation. 

The change in a quantity between the known state at  [2.x.4177]  and the currently unknown state at  [2.x.4178]  is denoted  [2.x.4179] . The value of a quantity at the current iteration  [2.x.4180]  is denoted  [2.x.4181] . The incremental change between iterations  [2.x.4182]  and  [2.x.4183]  is denoted  [2.x.4184] . 

Assume that the state of the system is known for some iteration  [2.x.4185] . The linearised approximation to nonlinear governing equations to be solved using the  Newton-Raphson method is: Find  [2.x.4186]  such that 

[1.x.1629] 

then set  [2.x.4187] . The tangent is given by 

[1.x.1630] 

Thus, 

[1.x.1631] 

where 

[1.x.1632] 



Note that the following terms are termed the geometrical stress and  the material contributions to the tangent matrix: 

[1.x.1633] 






[1.x.1634] 

The three-field formulation used here is effective for quasi-incompressible materials, that is where  [2.x.4188]  (where  [2.x.4189]  is [1.x.1635]), subject to a good choice of the interpolation fields for  [2.x.4190]  and  [2.x.4191] . Typically a choice of  [2.x.4192]  is made. Here  [2.x.4193]  is the FE_DGPMonomial class. A popular choice is  [2.x.4194]  which is known as the mean dilatation method (see Hughes (2000) for an intuitive discussion). This code can accommodate a  [2.x.4195]  formulation. The discontinuous approximation allows  [2.x.4196]  and  [2.x.4197]  to be condensed out and a classical displacement based method is recovered. 

For fully-incompressible materials  [2.x.4198]  and the three-field formulation will still exhibit locking behavior. This can be overcome by introducing an additional constraint into the free energy of the form  [2.x.4199] . Here  [2.x.4200]  is a Lagrange multiplier to enforce the isochoric constraint. For further details see Miehe (1994). 

The linearised problem can be written as 

[1.x.1636] 

where 

[1.x.1637] 



There are no derivatives of the pressure and dilatation (primary) variables present in the formulation. Thus the discontinuous finite element interpolation of the pressure and dilatation yields a block diagonal matrix for  [2.x.4201] ,  [2.x.4202]  and  [2.x.4203] . Therefore we can easily express the fields  [2.x.4204]  and  [2.x.4205]  on each cell simply by inverting a local matrix and multiplying it by the local right hand side. We can then insert the result into the remaining equations and recover a classical displacement-based method. In order to condense out the pressure and dilatation contributions at the element level we need the following results: 

[1.x.1638] 

and thus 

[1.x.1639] 

where 

[1.x.1640] 

Note that due to the choice of  [2.x.4206]  and  [2.x.4207]  as discontinuous at the element level, all matrices that need to be inverted are defined at the element level. 

The procedure to construct the various contributions is as follows: 

- Construct  [2.x.4208] . 

- Form  [2.x.4209]  for element and store where  [2.x.4210]  was stored in  [2.x.4211] . 

- Form  [2.x.4212]  and add to  [2.x.4213]  to get  [2.x.4214]  

- The modified system matrix is called  [2.x.4215] .   That is   [1.x.1641] 






[1.x.1642] 

A good object-oriented design of a Material class would facilitate the extension of this tutorial to a wide range of material types. In this tutorial we simply have one Material class named Material_Compressible_Neo_Hook_Three_Field. Ideally this class would derive from a class HyperelasticMaterial which would derive from the base class Material. The three-field nature of the formulation used here also complicates the matter. 

The Helmholtz free energy function for the three field formulation is  [2.x.4216] . The isochoric part of the Kirchhoff stress  [2.x.4217]  is identical to that obtained using a one-field formulation for a hyperelastic material. However, the volumetric part of the free energy is now a function of the primary variable  [2.x.4218] . Thus, for a three field formulation the constitutive response for the volumetric part of the Kirchhoff stress  [2.x.4219]  (and the tangent) is not given by the hyperelastic constitutive law as in a one-field formulation. One can label the term  [2.x.4220]  as the volumetric Kirchhoff stress, but the pressure  [2.x.4221]  is not derived from the free energy; it is a primary field. 

In order to have a flexible approach, it was decided that the Material_Compressible_Neo_Hook_Three_Field would still be able to calculate and return a volumetric Kirchhoff stress and tangent. In order to do this, we choose to store the interpolated primary fields  [2.x.4222]  and  [2.x.4223]  in the Material_Compressible_Neo_Hook_Three_Field class associated with the quadrature point. This decision should be revisited at a later stage when the tutorial is extended to account for other materials. 




[1.x.1643] 

The numerical example considered here is a nearly-incompressible block under compression. This benchmark problem is taken from 

- S. Reese, P. Wriggers, B.D. Reddy (2000),   A new locking-free brick element technique for large deformation problems in elasticity,    [2.x.4224]  Computers and Structures  [2.x.4225] ,   <strong> 75 </strong>,   291-304.   DOI: [1.x.1644] 

  [2.x.4226]  

The material is quasi-incompressible neo-Hookean with [1.x.1645]  [2.x.4227]  and  [2.x.4228] . For such a choice of material properties a conventional single-field  [2.x.4229]  approach would lock. That is, the response would be overly stiff. The initial and final configurations are shown in the image above. Using symmetry, we solve for only one quarter of the geometry (i.e. a cube with dimension  [2.x.4230] ). The inner-quarter of the upper surface of the domain is subject to a load of  [2.x.4231] . 


examples/step-44/doc/results.dox 



[1.x.1646] 

Firstly, we present a comparison of a series of 3-d results with those in the literature (see Reese et al (2000)) to demonstrate that the program works as expected. 

We begin with a comparison of the convergence with mesh refinement for the  [2.x.4232]  and  [2.x.4233]  formulations, as summarised in the figure below. The vertical displacement of the midpoint of the upper surface of the block is used to assess convergence. Both schemes demonstrate good convergence properties for varying values of the load parameter  [2.x.4234] . The results agree with those in the literature. The lower-order formulation typically overestimates the displacement for low levels of refinement, while the higher-order interpolation scheme underestimates it, but be a lesser degree. This benchmark, and a series of others not shown here, give us confidence that the code is working as it should. 

 [2.x.4235]  


A typical screen output generated by running the problem is shown below. The particular case demonstrated is that of the  [2.x.4236]  formulation. It is clear that, using the Newton-Raphson method, quadratic convergence of the solution is obtained. Solution convergence is achieved within 5 Newton increments for all time-steps. The converged displacement's  [2.x.4237] -norm is several orders of magnitude less than the geometry scale. 

[1.x.1647] 






Using the Timer class, we can discern which parts of the code require the highest computational expense. For a case with a large number of degrees-of-freedom (i.e. a high level of refinement), a typical output of the Timer is given below. Much of the code in the tutorial has been developed based on the optimizations described, discussed and demonstrated in Step-18 and others. With over 93% of the time being spent in the linear solver, it is obvious that it may be necessary to invest in a better solver for large three-dimensional problems. The SSOR preconditioner is not multithreaded but is effective for this class of solid problems. It may be beneficial to investigate the use of another solver such as those available through the Trilinos library. 




[1.x.1648] 




We then used ParaView to visualize the results for two cases. The first was for the coarsest grid and the lowest-order interpolation method:  [2.x.4238] . The second was on a refined grid using a  [2.x.4239]  formulation. The vertical component of the displacement, the pressure  [2.x.4240]  and the dilatation  [2.x.4241]  fields are shown below. 


For the first case it is clear that the coarse spatial discretization coupled with large displacements leads to a low quality solution (the loading ratio is   [2.x.4242] ). Additionally, the pressure difference between elements is very large. The constant pressure field on the element means that the large pressure gradient is not captured. However, it should be noted that locking, which would be present in a standard  [2.x.4243]  displacement formulation does not arise even in this poorly discretised case. The final vertical displacement of the tracked node on the top surface of the block is still within 12.5% of the converged solution. The pressure solution is very coarse and has large jumps between adjacent cells. It is clear that the volume nearest to the applied traction undergoes compression while the outer extents of the domain are in a state of expansion. The dilatation solution field and pressure field are clearly linked, with positive dilatation indicating regions of positive pressure and negative showing regions placed in compression. As discussed in the Introduction, a compressive pressure has a negative sign while an expansive pressure takes a positive sign. This stems from the definition of the volumetric strain energy function and is opposite to the physically realistic interpretation of pressure. 


 [2.x.4244]  

Combining spatial refinement and a higher-order interpolation scheme results in a high-quality solution. Three grid refinements coupled with a  [2.x.4245]  formulation produces a result that clearly captures the mechanics of the problem. The deformation of the traction surface is well resolved. We can now observe the actual extent of the applied traction, with the maximum force being applied at the central point of the surface causing the largest compression. Even though very high strains are experienced in the domain, especially at the boundary of the region of applied traction, the solution remains accurate. The pressure field is captured in far greater detail than before. There is a clear distinction and transition between regions of compression and expansion, and the linear approximation of the pressure field allows a refined visualization of the pressure at the sub-element scale. It should however be noted that the pressure field remains discontinuous and could be smoothed on a continuous grid for the post-processing purposes. 




 [2.x.4246]  

This brief analysis of the results demonstrates that the three-field formulation is effective in circumventing volumetric locking for highly-incompressible media. The mixed formulation is able to accurately simulate the displacement of a near-incompressible block under compression. The command-line output indicates that the volumetric change under extreme compression resulted in less than 0.01% volume change for a Poisson's ratio of 0.4999. 

In terms of run-time, the  [2.x.4247]  formulation tends to be more computationally expensive than the  [2.x.4248]  for a similar number of degrees-of-freedom (produced by adding an extra grid refinement level for the lower-order interpolation). This is shown in the graph below for a batch of tests run consecutively on a single 4-core (8-thread) machine. The increase in computational time for the higher-order method is likely due to the increased band-width required for the higher-order elements. As previously mentioned, the use of a better solver and preconditioner may mitigate the expense of using a higher-order formulation. It was observed that for the given problem using the multithreaded Jacobi preconditioner can reduce the computational runtime by up to 72% (for the worst case being a higher-order formulation with a large number of degrees-of-freedom) in comparison to the single-thread SSOR preconditioner. However, it is the author's experience that the Jacobi method of preconditioning may not be suitable for some finite-strain problems involving alternative constitutive models. 


 [2.x.4249]  


Lastly, results for the displacement solution for the 2-d problem are showcased below for two different levels of grid refinement. It is clear that due to the extra constraints imposed by simulating in 2-d that the resulting displacement field, although qualitatively similar, is different to that of the 3-d case. 


 [2.x.4250]  

[1.x.1649] 

[1.x.1650] 

There are a number of obvious extensions for this work: 

- Firstly, an additional constraint could be added to the free-energy   function in order to enforce a high degree of incompressibility in   materials. An additional Lagrange multiplier would be introduced,   but this could most easily be dealt with using the principle of   augmented Lagrange multipliers. This is demonstrated in  [2.x.4251] Simo and   Taylor (1991)  [2.x.4252] . 

- The constitutive relationship used in this   model is relatively basic. It may be beneficial to split the material   class into two separate classes, one dealing with the volumetric   response and the other the isochoric response, and produce a generic   materials class (i.e. having abstract virtual functions that derived   classes have to implement) that would allow for the addition of more complex   material models. Such models could include other hyperelastic   materials, plasticity and viscoelastic materials and others. 

- The program has been developed for solving problems on single-node   multicore machines. With a little effort, the program could be   extended to a large-scale computing environment through the use of   Petsc or Trilinos, using a similar technique to that demonstrated in   step-40. This would mostly involve changes to the setup, assembly,    [2.x.4253]  and linear solver routines. 

- As this program assumes quasi-static equilibrium, extensions to   include dynamic effects would be necessary to study problems where   inertial effects are important, e.g. problems involving impact. 

- Load and solution limiting procedures may be necessary for highly   nonlinear problems. It is possible to add a linesearch algorithm to   limit the step size within a Newton increment to ensure optimum   convergence. It may also be necessary to use a load limiting method,   such as the Riks method, to solve unstable problems involving   geometric instability such as buckling and snap-through. 

- Many physical problems involve contact. It is possible to include   the effect of frictional or frictionless contact between objects   into this program. This would involve the addition of an extra term   in the free-energy functional and therefore an addition to the   assembly routine. One would also need to manage the contact problem   (detection and stress calculations) itself. An alternative to   additional penalty terms in the free-energy functional would be to   use active set methods such as the one used in step-41. 

- The complete condensation procedure using LinearOperators has been   coded into the linear solver routine. This could also have been   achieved through the application of the schur_complement()   operator to condense out one or more of the fields in a more   automated manner. 

- Finally, adaptive mesh refinement, as demonstrated in step-6 and   step-18, could provide additional solution accuracy. 


examples/step-45/doc/intro.dox 

 [2.x.4254]  

[1.x.1651] [1.x.1652] 

[1.x.1653] 

In this example we present how to use periodic boundary conditions in deal.II. Periodic boundary conditions are algebraic constraints that typically occur in computations on representative regions of a larger domain that repeat in one or more directions. 

An example is the simulation of the electronic structure of photonic crystals, because they have a lattice-like structure and, thus, it often suffices to do the actual computation on only one box of the lattice. To be able to proceed this way one has to assume that the model can be periodically extended to the other boxes; this requires the solution to have a periodic structure. 

[1.x.1654] 

[1.x.1655] 

deal.II provides a number of high level entry points to impose periodic boundary conditions. The general approach to apply periodic boundary conditions consists of three steps (see also the  [2.x.4255]  "Glossary entry on periodic boundary conditions"): 

-# Create a mesh 

-# Identify those pairs of faces on different parts of the boundary across which    the solution should be symmetric, using  [2.x.4256]  

-# Add the periodicity information to the mesh    using  [2.x.4257]  

-# Add periodicity constraints using  [2.x.4258]  

The second and third step are necessary for parallel meshes using the  [2.x.4259]  class to ensure that cells on opposite sides of the domain but connected by periodic faces are part of the ghost layer if one of them is stored on the local processor. If the Triangulation is not a  [2.x.4260]  these steps are not necessary. 

The first step consists of collecting matching periodic faces and storing them in a  [2.x.4261]  of  [2.x.4262]  This is done with the function  [2.x.4263]  that can be invoked for example like this: 

[1.x.1656] 



This call loops over all faces of the container dof_handler on the periodic boundaries with boundary indicator  [2.x.4264]  and  [2.x.4265]  respectively. (You can assign these boundary indicators by hand after creating the coarse mesh, see  [2.x.4266]  "Boundary indicator". Alternatively, you can also let many of the functions in namespace GridGenerator do this for if you specify the "colorize" flag; in that case, these functions will assign different boundary indicators to different parts of the boundary, with the details typically spelled out in the documentation of these functions.) 

Concretely, if  [2.x.4267]  are the vertices of two faces  [2.x.4268] , then the function call above will match pairs of faces (and dofs) such that the difference between  [2.x.4269]  and  [2.x.4270]  vanishes in every component apart from direction and stores the resulting pairs with associated data in  [2.x.4271]  (See  [2.x.4272]  for detailed information about the matching process.) 

Consider, for example, the colored unit square  [2.x.4273]  with boundary indicator 0 on the left, 1 on the right, 2 on the bottom and 3 on the top faces. (See the documentation of  [2.x.4274]  for this convention on how boundary indicators are assigned.) Then, 

[1.x.1657] 

would yield periodicity constraints such that  [2.x.4275]  for all  [2.x.4276] . 

If we instead consider the parallelogram given by the convex hull of  [2.x.4277] ,  [2.x.4278] ,  [2.x.4279] ,  [2.x.4280]  we can achieve the constraints  [2.x.4281]  by specifying an  [2.x.4282]  

[1.x.1658] 

or 

[1.x.1659] 

Here, again, the assignment of boundary indicators 0 and 1 stems from what  [2.x.4283]  documents. 

The resulting  [2.x.4284]  can be used in  [2.x.4285]  for populating an AffineConstraints object with periodicity constraints: 

[1.x.1660] 



Apart from this high level interface there are also variants of  [2.x.4286]  available that combine those two steps (see the variants of  [2.x.4287]  

There is also a low level interface to  [2.x.4288]  if more flexibility is needed. The low level variant allows to directly specify two faces that shall be constrained: 

[1.x.1661] 

Here, we need to specify the orientation of the two faces using  [2.x.4289]   [2.x.4290]  and  [2.x.4291]  For a closer description have a look at the documentation of  [2.x.4292]  The remaining parameters are the same as for the high level interface apart from the self-explaining  [2.x.4293]  and  [2.x.4294]  


[1.x.1662] 

[1.x.1663] 

In the following, we show how to use the above functions in a more involved example. The task is to enforce rotated periodicity constraints for the velocity component of a Stokes flow. 

On a quarter-circle defined by  [2.x.4295]  we are going to solve the Stokes problem 

[1.x.1664] 

where the boundary  [2.x.4296]  is defined as  [2.x.4297] . For the remaining parts of the boundary we are going to use periodic boundary conditions, i.e. 

[1.x.1665] 



The mesh will be generated by  [2.x.4298]  which also documents how it assigns boundary indicators to its various boundaries if its `colorize` argument is set to `true`. 


examples/step-45/doc/results.dox 



[1.x.1666] 

The created output is not very surprising. We simply see that the solution is periodic with respect to the left and lower boundary: 

 [2.x.4299]  

Without the periodicity constraints we would have ended up with the following solution: 

 [2.x.4300]  


examples/step-46/doc/intro.dox 

 [2.x.4301]  

[1.x.1667] 


[1.x.1668] 

[1.x.1669] 

This program deals with the problem of coupling different physics in different parts of the domain. Specifically, let us consider the following situation that couples a Stokes fluid with an elastic solid (these two problems were previously discussed separately in step-22 and step-8, where you may want to read up on the individual equations): 

- In a part  [2.x.4302]  of  [2.x.4303] , we have a fluid flowing that satisfies the   time independent Stokes equations (in the form that involves the strain   tensor):   [1.x.1670] 

  Here,  [2.x.4304]  are the fluid velocity and pressure, respectively.   We prescribe the velocity on part of the external boundary,   [1.x.1671] 

  while we assume free-flow conditions on the remainder of the external   boundary,   [1.x.1672] 



- The remainder of the domain,  [2.x.4305]  is   occupied by a solid whose deformation field  [2.x.4306]  satisfies the   elasticity equation,   [1.x.1673] 

  where  [2.x.4307]  is the rank-4 elasticity tensor (for which we will use a   particularly simple form by assuming that the solid is isotropic).   It deforms in reaction to the forces exerted by the   fluid flowing along the boundary of the solid. We assume this deformation to   be so small that it has no feedback effect on the fluid, i.e. the coupling   is only in one direction. For simplicity, we will assume that the   solid's external boundary is clamped, i.e.   [1.x.1674] 



- As a consequence of the small displacement assumption, we will pose the   following boundary conditions on the interface between the fluid and solid:   first, we have no slip boundary conditions for the fluid,   [1.x.1675] 

  Secondly, the forces (traction) on the solid equal the normal stress from the fluid,   [1.x.1676] 

  where  [2.x.4308]  is the normal vector on  [2.x.4309]  pointing from   the solid to the fluid. 

We get a weak formulation of this problem by following our usual rule of multiplying from the left by a test function and integrating over the domain. It then looks like this: Find  [2.x.4310]  such that 

[1.x.1677] 

for all test functions  [2.x.4311] ; the first, second, and third lines correspond to the fluid, solid, and interface contributions, respectively. Note that  [2.x.4312]  is only a subspace of the spaces listed above to accommodate for the various Dirichlet boundary conditions. 

This sort of coupling is of course possible by simply having two Triangulation and two DoFHandler objects, one each for each of the two subdomains. On the other hand, deal.II is much simpler to use if there is a single DoFHandler object that knows about the discretization of the entire problem. 

This program is about how this can be achieved. Note that the goal is not to present a particularly useful physical model (a realistic fluid-structure interaction model would have to take into account the finite deformation of the solid and the effect this has on the fluid): this is, after all, just a tutorial program intended to demonstrate techniques, not to solve actual problems. Furthermore, we will make the assumption that the interface between the subdomains is aligned with coarse mesh cell faces. 




[1.x.1678] 

Before going into more details let us state the obvious: this is a problem with multiple solution variables; for this, you will probably want to read the  [2.x.4313]  documentation module first, which presents the basic philosophical framework in which we address problems with more than one solution variable. But back to the problem at hand: 

The fundamental idea to implement these sort of problems in deal.II goes as follows: in the problem formulation, the velocity and pressure variables  [2.x.4314]  only live in the fluid subdomain  [2.x.4315] . But let's assume that we extend them by zero to the entire domain  [2.x.4316]  (in the general case this means that they will be discontinuous along  [2.x.4317] ). So what is the appropriate function space for these variables? We know that on  [2.x.4318]  we should require  [2.x.4319] , so for the extensions  [2.x.4320]  to the whole domain the following appears a useful set of function spaces: 

[1.x.1679] 

(Since this is not important for the current discussion, we have omitted the question of boundary values from the choice of function spaces; this question also affects whether we can choose  [2.x.4321]  for the pressure or whether we have to choose the space  [2.x.4322]  for the pressure. None of these questions are relevant to the following discussion, however.) 

Note that these are indeed a linear function spaces with obvious norm. Since no confusion is possible in practice, we will henceforth omit the tilde again to denote the extension of a function to the whole domain and simply refer by  [2.x.4323]  to both the original and the extended function. 

For discretization, we need finite dimensional subspaces  [2.x.4324]  of  [2.x.4325] . For Stokes, we know from step-22 that an appropriate choice is  [2.x.4326]  but this only holds for that part of the domain occupied by the fluid. For the extended field, let's use the following subspaces defined on the triangulation  [2.x.4327] : 

[1.x.1680] 

In other words, on  [2.x.4328]  we choose the usual discrete spaces but we keep the (discontinuous) extension by zero. The point to make is that we now need a description of a finite element space for functions that are zero on a cell &mdash; and this is where the FE_Nothing class comes in: it describes a finite dimensional function space of functions that are constant zero. A particular property of this peculiar linear vector space is that it has no degrees of freedom: it isn't just finite dimensional, it is in fact zero dimensional, and consequently for objects of this type,  [2.x.4329]  will return zero. For discussion below, let us give this space a proper symbol: 

[1.x.1681] 

The symbol  [2.x.4330]  reminds of the fact that functions in this space are zero. Obviously, we choose  [2.x.4331] . 

This entire discussion above can be repeated for the variables we use to describe the elasticity equation. Here, for the extended variables, we have 

[1.x.1682] 

and we will typically use a finite element space of the kind 

[1.x.1683] 

of polynomial degree  [2.x.4332] . 

So to sum up, we are going to look for a discrete vector-valued solution  [2.x.4333]  in the following space: 

[1.x.1684] 






[1.x.1685] 

So how do we implement this sort of thing? First, we realize that the discrete space  [2.x.4334]  essentially calls for two different finite elements: First, on the fluid subdomain, we need the element  [2.x.4335]  which in deal.II is readily implemented by 

[1.x.1686] 

where  [2.x.4336]  implements the space of functions that are always zero. Second, on the solid subdomain, we need the element  [2.x.4337] , which we get using 

[1.x.1687] 



The next step is that we associate each of these two elements with the cells that occupy each of the two subdomains. For this we realize that in a sense the two elements are just variations of each other in that they have the same number of vector components but have different polynomial degrees &mdash; this smells very much like what one would do in  [2.x.4338]  finite element methods, and it is exactly what we are going to do here: we are going to (ab)use the classes and facilities of the hp-namespace to assign different elements to different cells. In other words, we will use collect the two finite elements in an  [2.x.4339]  will integrate with an appropriate  [2.x.4340]  using an  [2.x.4341]  object, and our DoFHandler will be in [1.x.1688]-mode. You may wish to take a look at step-27 for an overview of all of these concepts. 

Before going on describing the testcase, let us clarify a bit [1.x.1689] this approach of extending the functions by zero to the entire domain and then mapping the problem on to the hp-framework makes sense: 

- It makes things uniform: On all cells, the number of vector components is   the same (here,  [2.x.4342] ). This makes all sorts of   things possible since a uniform description allows for code   re-use. For example, counting degrees of freedom per vector   component  [2.x.4343]  sorting degrees of   freedom by component  [2.x.4344]  subsequent   partitioning of matrices and vectors into blocks and many other   functions work as they always did without the need to add special   logic to them that describes cases where some of the variables only   live on parts of the domain. Consequently, you have all sorts of   tools already available to you in programs like the current one that   weren't originally written for the multiphysics case but work just   fine in the current context. 

- It allows for easy graphical output: All graphical output formats we support   require that each field in the output is defined on all nodes of the   mesh. But given that now all solution components live everywhere,   our existing DataOut routines work as they always did, and produce   graphical output suitable for visualization -- the fields will   simply be extended by zero, a value that can easily be filtered out   by visualization programs if not desired. 

- There is essentially no cost: The trick with the FE_Nothing does not add any   degrees of freedom to the overall problem, nor do we ever have to handle a   shape function that belongs to these components &mdash; the FE_Nothing has   no degrees of freedom, not does it have shape functions, all it does is take   up vector components. 




[1.x.1690] 

More specifically, in the program we have to address the following points: 

- Implementing the bilinear form, and in particular dealing with the   interface term, both in the matrix and the sparsity pattern. 

- Implementing Dirichlet boundary conditions on the external and   internal parts of the boundaries    [2.x.4345] . 




[1.x.1691] 

Let us first discuss implementing the bilinear form, which at the discrete level we recall to be 

[1.x.1692] 

Given that we have extended the fields by zero, we could in principle write the integrals over subdomains to the entire domain  [2.x.4346] , though it is little additional effort to first ask whether a cell is part of the elastic or fluid region before deciding which terms to integrate. Actually integrating these terms is not very difficult; for the Stokes equations, the relevant steps have been shown in step-22, whereas for the elasticity equation we take essentially the form shown in the  [2.x.4347]  module (rather than the one from step-8). 

The term that is of more interest is the interface term, 

[1.x.1693] 

Based on our assumption that the interface  [2.x.4348]  coincides with cell boundaries, this can in fact be written as a set of face integrals. If we denote the velocity, pressure and displacement components of shape function  [2.x.4349]  using the extractor notation  [2.x.4350] , then the term above yields the following contribution to the global matrix entry  [2.x.4351] : 

[1.x.1694] 

Although it isn't immediately obvious, this term presents a slight complication: while  [2.x.4352]  and  [2.x.4353]  are evaluated on the solid side of the interface (they are test functions for the displacement and the normal vector to  [2.x.4354] , respectively, we need to evaluate  [2.x.4355]  on the fluid side of the interface since they correspond to the stress/force exerted by the fluid. In other words, in our implementation, we will need FEFaceValue objects for both sides of the interface. To make things slightly worse, we may also have to deal with the fact that one side or the other may be refined, leaving us with the need to integrate over parts of a face. Take a look at the implementation below on how to deal with this. 

As an additional complication, the matrix entries that result from this term need to be added to the sparsity pattern of the matrix somehow. This is the realm of various functions in the DoFTools namespace like  [2.x.4356]  and  [2.x.4357]  Essentially, what these functions do is simulate what happens during assembly of the system matrix: whenever assembly would write a nonzero entry into the global matrix, the functions in DoFTools would add an entry to the sparsity pattern. We could therefore do the following: let  [2.x.4358]  add all those entries to the sparsity pattern that arise from the regular cell-by-cell integration, and then do the same by hand that arise from the interface terms. If you look at the implementation of the interface integrals in the program below, it should be obvious how to do that and would require no more than maybe 100 lines of code at most. 

But we're lazy people: the interface term couples degrees of freedom from two adjacent cells along a face, which is exactly the kind of thing one would do in discontinuous Galerkin schemes for which the function  [2.x.4359]  was written. This is a superset of matrix entries compared to the usual  [2.x.4360]  it will also add all entries that result from computing terms coupling the degrees of freedom from both sides of all faces. Unfortunately, for the simplest version of this function, this is a pretty big superset. Consider for example the following mesh with two cells and a  [2.x.4361]  finite element: 

[1.x.1695] 

Here, the sparsity pattern produced by  [2.x.4362]  will only have entries for degrees of freedom that couple on a cell. However, it will not have sparsity pattern entries  [2.x.4363] . The sparsity pattern generated by  [2.x.4364]  will have these entries, however: it assumes that you want to build a sparsity pattern for a bilinear form that couples [1.x.1696] degrees of freedom from adjacent cells. This is not what we want: our interface term acts only on a small subset of cells, and we certainly don't need all the extra couplings between two adjacent fluid cells, or two adjacent solid cells. Furthermore, the fact that we use higher order elements means that we would really generate many many more entries than we actually need: on the coarsest mesh, in 2d, 44,207 nonzero entries instead of 16,635 for  [2.x.4365]  leading to plenty of zeros in the matrix we later build (of course, the 16,635 are not enough since they don't include the interface entries). This ratio would be even worse in 3d. 

So being extremely lazy comes with a cost: too many entries in the matrix. But we can get away with being moderately lazy: there is a variant of  [2.x.4366]  that allows us to specify which vector components of the finite element couple with which other components, both in cell terms as well as in face terms. For cells that are in the solid subdomain, we couple all displacements with each other; for fluid cells, all velocities with all velocities and the pressure, but not the pressure with itself. Since no cell has both sets of variables, there is no need to distinguish between the two kinds of cells, so we can write the mask like this: 

[1.x.1697] 

Here, we have used the fact that the first  [2.x.4367]  components of the finite element are the velocities, then the pressure, and then the  [2.x.4368]  displacements. (We could as well have stated that the velocities/pressure also couple with the displacements since no cell ever has both sets of variables.) On the other hand, the interface terms require a mask like this: 

[1.x.1698] 

In other words, all displacement test functions (components  [2.x.4369] ) couple with all velocity and pressure shape functions on the other side of an interface. This is not entirely true, though close: in fact, the exact form of the interface term only those pressure displacement shape functions that are indeed nonzero on the common interface, which is not true for all shape functions; on the other hand, it really couples all velocities (since the integral involves gradients of the velocity shape functions, which are all nonzero on all faces of the cell). However, the mask we build above, is not capable of these subtleties. Nevertheless, through these masks we manage to get the number of sparsity pattern entries down to 21,028 &mdash; good enough for now. 




[1.x.1699] 

The second difficulty is that while we know how to enforce a zero velocity or stress on the external boundary (using  [2.x.4370]  called with an appropriate component mask and setting different boundary indicators for solid and fluid external boundaries), we now also needed the velocity to be zero on the interior interface, i.e.  [2.x.4371] . At the time of writing this, there is no function in deal.II that handles this part, but it isn't particularly difficult to implement by hand: essentially, we just have to loop over all cells, and if it is a fluid cell and its neighbor is a solid cell, then add constraints that ensure that the velocity degrees of freedom on this face are zero. Some care is necessary to deal with the case that the adjacent solid cell is refined, yielding the following code: 

[1.x.1700] 



The call  [2.x.4372]  tells the AffineConstraints to start a new constraint for degree of freedom  [2.x.4373]  of the form  [2.x.4374] . Typically, one would then proceed to set individual coefficients  [2.x.4375]  to nonzero values (using  [2.x.4376]  or set  [2.x.4377]  to something nonzero (using  [2.x.4378]  doing nothing as above, funny as it looks, simply leaves the constraint to be  [2.x.4379] , which is exactly what we need in the current context. The call to  [2.x.4380]  makes sure that we only set boundary values to zero for velocity but not pressure components. 

Note that there are cases where this may yield incorrect results: notably, once we find a solid neighbor child to a current fluid cell, we assume that all neighbor children on the common face are in the solid subdomain. But that need not be so; consider, for example, the following mesh: 

[1.x.1701] 



In this case, we would set all velocity degrees of freedom on the right face of the left cell to zero, which is incorrect for the top degree of freedom on that face. That said, that can only happen if the fluid and solid subdomains do not coincide with a set of complete coarse mesh cells &mdash; but this is a contradiction to the assumption stated at the end of the first section of this introduction. 




[1.x.1702] 

We will consider the following situation as a testcase: 

 [2.x.4381]  

As discussed at the top of this document, we need to assume in a few places that a cell is either entirely in the fluid or solid part of the domain and, furthermore, that all children of an inactive cell also belong to the same subdomain. This can definitely be ensured if the coarse mesh already subdivides the mesh into solid and fluid coarse mesh cells; given the geometry outlined above, we can do that by using an  [2.x.4382]  coarse mesh, conveniently provided by the  [2.x.4383]  function. 

The fixed boundary at the bottom implies  [2.x.4384] , and we also prescribe Dirichlet conditions for the flow at the top so that we get inflow at the left and outflow at the right. At the left and right boundaries, no boundary conditions are imposed explicitly for the flow, yielding the implicit no-stress condition  [2.x.4385] . The conditions on the interface between the two domains has already been discussed above. 

For simplicity, we choose the material parameters to be  [2.x.4386] . In the results section below, we will also show a 3d simulation that can be obtained from the same program. The boundary conditions and geometry are defined nearly analogously to the 2d situation above. 




[1.x.1703] 

In the program, we need a way to identify which part of the domain a cell is in. There are many different ways of doing this. A typical way would be to use the  [2.x.4387]  "subdomain_id" tag available with each cell, though this field has a special meaning in %parallel computations. An alternative is the  [2.x.4388]  "material_id" field also available with every cell. It has the additional advantage that it is inherited from the mother to the child cell upon mesh refinement; in other words, we would set the material id once upon creating the mesh and it will be correct for all active cells even after several refinement cycles. We therefore go with this alternative: we define an  [2.x.4389]  with symbolic names for material_id numbers and will use them to identify which part of the domain a cell is on. 

Secondly, we use an object of type DoFHandler operating in [1.x.1704]-mode. This class needs to know which cells will use the Stokes and which the elasticity finite element. At the beginning of each refinement cycle we will therefore have to walk over all cells and set the (in hp-parlance) active FE index to whatever is appropriate in the current situation. While we can use symbolic names for the material id, the active FE index is in fact a number that will frequently be used to index into collections of objects (e.g. of type  [2.x.4390]  and  [2.x.4391]  that means that the active FE index actually has to have value zero for the fluid and one for the elastic part of the domain. 




[1.x.1705] 

This program is primarily intended to show how to deal with different physics in different parts of the domain, and how to implement such models in deal.II. As a consequence, we won't bother coming up with a good solver: we'll just use the SparseDirectUMFPACK class which always works, even if not with optimal complexity. We will, however, comment on possible other solvers in the [1.x.1706] section. 




[1.x.1707] 

One of the trickier aspects of this program is how to estimate the error. Because it works on almost any program, we'd like to use the KellyErrorEstimator, and we can relatively easily do that here as well using code like the following: 

[1.x.1708] 

This gives us two sets of error indicators for each cell. We would then somehow combine them into one for mesh refinement, for example using something like the following (note that we normalize the squared error indicator in the two vectors because error quantities have physical units that do not match in the current situation, leading to error indicators that may differ by orders of magnitude between the two subdomains): 

[1.x.1709] 

(In the code, we actually weigh the error indicators 4:1 in favor of the ones computed on the Stokes subdomain since refinement is otherwise heavily biased towards the elastic subdomain, but this is just a technicality. The factor 4 has been determined heuristically to work reasonably well.) 

While this principle is sound, it doesn't quite work as expected. The reason is that the KellyErrorEstimator class computes error indicators by integrating the jump in the solution's gradient around the faces of each cell. This jump is likely to be very large at the locations where the solution is discontinuous and extended by zero; it also doesn't become smaller as the mesh is refined. The KellyErrorEstimator class can't just ignore the interface because it essentially only sees a DoFHandler in [1.x.1710]-mode where the element type changes from one cell to another &mdash; precisely the thing that the [1.x.1711]-mode was designed for, the interface in the current program looks no different than the interfaces in step-27, for example, and certainly no less legitimate. Be that as it may, the end results is that there is a layer of cells on both sides of the interface between the two subdomains where error indicators are irrationally large. Consequently, most of the mesh refinement is focused on the interface. 

This clearly wouldn't happen if we had a refinement indicator that actually understood something about the problem and simply ignore the interface between subdomains when integrating jump terms. On the other hand, this program is about showing how to represent problems where we have different physics in different subdomains, not about the peculiarities of the KellyErrorEstimator, and so we resort to the big hammer called "heuristics": we simply set the error indicators of cells at the interface to zero. This cuts off the spikes in the error indicators. At first sight one would also think that it prevents the mesh from being refined at the interface, but the requirement that neighboring cells may only differ by one level of refinement will still lead to a reasonably refined mesh. 

While this is clearly a suboptimal solution, it works for now and leaves room for future improvement. 


examples/step-46/doc/results.dox 

[1.x.1712] 

[1.x.1713] 

[1.x.1714] 


When running the program, you should get output like the following: 

[1.x.1715] 



The results are easily visualized: 

 [2.x.4392]  

The plots are easily interpreted: as the flow drives down on the left side and up on the right side of the upright part of the solid, it produces a pressure that is high on the left and low on the right, and these forces bend the vertical part of the solid to the right. 




[1.x.1716] 

By changing the dimension of the  [2.x.4393]  class in  [2.x.4394]  to 3, we can also run the same problem 3d. You'd get output along the following lines: 

[1.x.1717] 

You'll notice that the big bottleneck is the solver: SparseDirectUmfpack needs nearly 5 hours and some 80 GB of memory to solve the last iteration of this problem on a 2016 workstation (the second to last iteration took only 16 minutes). Clearly a better solver is needed here, a topic discussed below. 

The results can also be visualized and yield good pictures as well. Here is one, showing both a vector plot for the velocity (in oranges), the solid displacement (in blues), and shading the solid region: 

<p align="center">    [2.x.4395]   [2.x.4396]  

In addition to the lack of a good solver, the mesh is a bit unbalanced: mesh refinement heavily favors the fluid subdomain (in 2d, it was the other way around, prompting us to weigh the fluid error indicators higher). Clearly, some tweaking of the relative importance of error indicators in the two subdomains is important if one wanted to go on doing more 3d computations. 


[1.x.1718] 

[1.x.1719] 

[1.x.1720] 

An obvious place to improve the program would be to use a more sophisticated solver &mdash; in particular one that scales well and will also work for realistic 3d problems. This shouldn't actually be too hard to achieve here, because of the one-way coupling from fluid into solid. To this end, assume we had re-ordered degrees of freedom in such a way that we first have all velocity and pressure degrees of freedom, and then all displacements (this is easily possible using  [2.x.4397]  Then the system matrix could be split into the following block form: 

[1.x.1721] 

where  [2.x.4398]  is the Stokes matrix for velocity and pressure (it could be further subdivided into a  [2.x.4399]  matrix as in step-22, though this is immaterial for the current purpose),  [2.x.4400]  results from the elasticity equations for the displacements, and  [2.x.4401]  is the matrix that comes from the interface conditions. Now notice that the matrix 

[1.x.1722] 

is the inverse of  [2.x.4402] . Applying this matrix requires only one solve with  [2.x.4403]  and  [2.x.4404]  each since 

[1.x.1723] 

can be computed as  [2.x.4405]  followed by  [2.x.4406] . 

One can therefore expect that 

[1.x.1724] 

would be a good preconditioner if  [2.x.4407] . 

That means, we only need good preconditioners for Stokes and the elasticity equations separately. These are well known: for Stokes, we can use the preconditioner discussed in the results section of step-22; for elasticity, a good preconditioner would be a single V-cycle of a geometric or algebraic multigrid. There are more open questions, however: For an "optimized" solver block-triangular preconditioner built from two sub-preconditioners, one point that often comes up is that, when choosing parameters for the sub-preconditioners, values that work well when solving the two problems separately may not be optimal when combined into a multiphysics preconditioner.  In particular, when solving just a solid or fluid mechanics problem separately, the balancing act between the number of iterations to convergence and the cost of applying the preconditioner on a per iteration basis may lead one to choose an expensive preconditioner for the Stokes problem and a cheap preconditioner for the elasticity problem (or vice versa).  When combined, however, there is the additional constraint that you want the two sub-preconditioners to converge at roughly the same rate, or else the cheap one may drive up the global number of iterations while the expensive one drives up the cost-per-iteration. For example, while a single AMG V-cycle is a good approach for elasticity by itself, when combined into a multiphysics problem there may be an incentive to using a full W-cycle or multiple cycles to help drive down the total solve time. 




[1.x.1725] 

As mentioned in the introduction, the refinement indicator we use for this program is rather ad hoc. A better one would understand that the jump in the gradient of the solution across the interface is not indicative of the error but to be expected and ignore the interface when integrating the jump terms. Nevertheless, this is not what the KellyErrorEstimator class does. Another, bigger question, is whether this kind of estimator is a good strategy in the first place: for example, if we want to have maximal accuracy in one particular aspect of the displacement (e.g. the displacement at the top right corner of the solid), then is it appropriate to scale the error indicators for fluid and solid to the same magnitude? Maybe it is necessary to solve the fluid problem with more accuracy than the solid because the fluid solution directly affects the solids solution? Maybe the other way around? 

Consequently, an obvious possibility for improving the program would be to implement a better refinement criterion. There is some literature on this topic; one of a variety of possible starting points would be the paper by Thomas Wick on "Adaptive finite elements for monolithic fluid-structure interaction on a prolongated domain: Applied to an heart valve simulation", Proceedings of the Computer Methods in Mechanics Conference 2011 (CMM-2011), 9-12 May 2011, Warszaw, Poland. 




[1.x.1726] 

The results above are purely qualitative as there is no evidence that our scheme in fact converges. An obvious thing to do would therefore be to add some quantitative measures to check that the scheme at least converges to [1.x.1727]. For example, we could output for each refinement cycle the deflection of the top right corner of the part of the solid that protrudes into the fluid subdomain. Or we could compute the net force vector or torque the fluid exerts on the solid. 




[1.x.1728] 

In reality, most fluid structure interaction problems are so that the movement of the solid does affect the flow of the fluid. For example, the forces of the air around an air foil cause it to flex and to change its shape. Likewise, a flag flaps in the wind, completely changing its shape. 

Such problems where the coupling goes both ways are typically handled in an Arbitrary Lagrangian Eulerian (ALE) framework, in which the displacement of the solid is extended into the fluid domain in some smooth way, rather than by zero as we do here. The extended displacement field is then used to deform the mesh on which we compute the fluid flow. Furthermore, the boundary conditions for the fluid on the interface are no longer that the velocity is zero; rather, in a time dependent program, the fluid velocity must be equal to the time derivative of the displacement along the interface. 


examples/step-47/doc/intro.dox 

 [2.x.4408]  

[1.x.1729] 

[1.x.1730] 

[1.x.1731] 

This program deals with the [1.x.1732], 

[1.x.1733] 

This equation appears in the modeling of thin structures such as roofs of stadiums. These objects are of course in reality three-dimensional with a large aspect ratio of lateral extent to perpendicular thickness, but one can often very accurately model these structures as two dimensional by making assumptions about how internal forces vary in the perpendicular direction. These assumptions lead to the equation above. 

The model typically comes in two different kinds, depending on what kinds of boundary conditions are imposed. The first case, 

[1.x.1734] 

corresponds to the edges of the thin structure attached to the top of a wall of height  [2.x.4409]  in such a way that the bending forces that act on the structure are  [2.x.4410] ; in most physical situations, one will have  [2.x.4411] , corresponding to the structure simply sitting atop the wall. 

In the second possible case of boundary values, one would have 

[1.x.1735] 

This corresponds to a "clamped" structure for which a nonzero  [2.x.4412]  implies a certain angle against the horizontal. 

As with Dirichlet and Neumann boundary conditions for the Laplace equation, it is of course possible to have one kind of boundary conditions on one part of the boundary, and the other on the remainder. 




[1.x.1736] 

The fundamental issue with the equation is that it takes four derivatives of the solution. In the case of the Laplace equation we treated in step-3, step-4, and several other tutorial programs, one multiplies by a test function, integrates, integrates by parts, and ends up with only one derivative on both the test function and trial function -- something one can do with functions that are continuous globally, but may have kinks at the interfaces between cells: The derivative may not be defined at the interfaces, but that is on a lower-dimensional manifold (and so doesn't show up in the integrated value). 

But for the biharmonic equation, if one followed the same procedure using integrals over the entire domain (i.e., the union of all cells), one would end up with two derivatives on the test functions and trial functions each. If one were to use the usual piecewise polynomial functions with their kinks on cell interfaces, the first derivative would yield a discontinuous gradient, and the second derivative with delta functions on the interfaces -- but because both the second derivatives of the test functions and of the trial functions yield a delta function, we would try to integrate the product of two delta functions. For example, in 1d, where  [2.x.4413]  are the usual piecewise linear "hat functions", we would get integrals of the sort 

[1.x.1737] 

where  [2.x.4414]  is the node location at which the shape function  [2.x.4415]  is defined, and  [2.x.4416]  is the mesh size (assumed uniform). The problem is that delta functions in integrals are defined using the relationship 

[1.x.1738] 

But that only works if (i)  [2.x.4417]  is actually well defined at  [2.x.4418] , and (ii) if it is finite. On the other hand, an integral of the form 

[1.x.1739] 

does not make sense. Similar reasoning can be applied for 2d and 3d situations. 

In other words: This approach of trying to integrate over the entire domain and then integrating by parts can't work. 

Historically, numerical analysts have tried to address this by inventing finite elements that are "C<sup>1</sup> continuous", i.e., that use shape functions that are not just continuous but also have continuous first derivatives. This is the realm of elements such as the Argyris element, the Clough-Tocher element and others, all developed in the late 1960s. From a twenty-first century perspective, they can only be described as bizarre in their construction. They are also exceedingly cumbersome to implement if one wants to use general meshes. As a consequence, they have largely fallen out of favor and deal.II currently does not contain implementations of these shape functions. 




[1.x.1740] 

So how does one approach solving such problems then? That depends a bit on the boundary conditions. If one has the first set of boundary conditions, i.e., if the equation is 

[1.x.1741] 

then the following trick works (at least if the domain is convex, see below): In the same way as we obtained the mixed Laplace equation of step-20 from the regular Laplace equation by introducing a second variable, we can here introduce a variable  [2.x.4419]  and can then replace the equations above by the following, "mixed" system: 

[1.x.1742] 

In other words, we end up with what is in essence a system of two coupled Laplace equations for  [2.x.4420] , each with Dirichlet-type boundary conditions. We know how to solve such problems, and it should not be very difficult to construct good solvers and preconditioners for this system either using the techniques of step-20 or step-22. So this case is pretty simple to deal with. 

 [2.x.4421]  It is worth pointing out that this only works for domains whose   boundary has corners if the domain is also convex -- in other words,   if there are no re-entrant corners.   This sounds like a rather random condition, but it makes   sense in view of the following two facts: The solution of the   original biharmonic equation must satisfy  [2.x.4422] . On the   other hand, the mixed system reformulation above suggests that both    [2.x.4423]  and  [2.x.4424]  satisfy  [2.x.4425]  because both variables only   solve a Poisson equation. In other words, if we want to ensure that   the solution  [2.x.4426]  of the mixed problem is also a solution of the   original biharmonic equation, then we need to be able to somehow   guarantee that the solution of  [2.x.4427]  is in fact more smooth   than just  [2.x.4428] . This can be argued as follows: For convex   domains,   [1.x.1743] implies that if the right hand side  [2.x.4429] , then    [2.x.4430]  if the domain is convex and the boundary is smooth   enough. (This could also be guaranteed if the domain boundary is   sufficiently smooth -- but domains whose boundaries have no corners   are not very practical in real life.)   We know that  [2.x.4431]  because it solves the equation    [2.x.4432] , but we are still left with the condition on convexity   of the boundary; one can show that polygonal, convex domains are   good enough to guarantee that  [2.x.4433]  in this case (smoothly   bounded, convex domains would result in  [2.x.4434] , but we don't   need this much regularity). On the other hand, if the domain is not   convex, we can not guarantee that the solution of the mixed system   is in  [2.x.4435] , and consequently may obtain a solution that can't be   equal to the solution of the original biharmonic equation. 

The more complicated situation is if we have the "clamped" boundary conditions, i.e., if the equation looks like this: 

[1.x.1744] 

The same trick with the mixed system does not work here, because we would end up with [1.x.1745] Dirichlet and Neumann boundary conditions for  [2.x.4436] , but none for  [2.x.4437] . 


The solution to this conundrum arrived with the Discontinuous Galerkin method wave in the 1990s and early 2000s: In much the same way as one can use [1.x.1746] shape functions for the Laplace equation by penalizing the size of the discontinuity to obtain a scheme for an equation that has one derivative on each shape function, we can use a scheme that uses [1.x.1747] (but not  [2.x.4438]  continuous) shape functions and penalize the jump in the derivative to obtain a scheme for an equation that has two derivatives on each shape function. In analogy to the Interior Penalty (IP) method for the Laplace equation, this scheme for the biharmonic equation is typically called the  [2.x.4439]  IP (or C0IP) method, since it uses  [2.x.4440]  (continuous but not continuously differentiable) shape functions with an interior penalty formulation. 




[1.x.1748] 

We base this program on the  [2.x.4441]  IP method presented by Susanne Brenner and Li-Yeng Sung in the paper "C [2.x.4442]  Interior Penalty Method for Linear Fourth Order Boundary Value Problems on polygonal domains''  [2.x.4443]  , where the method is derived for the biharmonic equation with "clamped" boundary conditions. 

As mentioned, this method relies on the use of  [2.x.4444]  Lagrange finite elements where the  [2.x.4445]  continuity requirement is relaxed and has been replaced with interior penalty techniques. To derive this method, we consider a  [2.x.4446]  shape function  [2.x.4447]  which vanishes on  [2.x.4448] . We introduce notation  [2.x.4449]  as the set of all faces of  [2.x.4450] ,  [2.x.4451]  as the set of boundary faces, and  [2.x.4452]  as the set of interior faces for use further down below. Since the higher order derivatives of  [2.x.4453]  have two values on each interface  [2.x.4454]  (shared by the two cells  [2.x.4455] ), we cope with this discontinuity by defining the following single-valued functions on  [2.x.4456] : 

[1.x.1749] 

for  [2.x.4457]  (i.e., for the gradient and the matrix of second derivatives), and where  [2.x.4458]  denotes a unit vector normal to  [2.x.4459]  pointing from  [2.x.4460]  to  [2.x.4461] . In the literature, these functions are referred to as the "jump" and "average" operations, respectively. 

To obtain the  [2.x.4462]  IP approximation  [2.x.4463] , we left multiply the biharmonic equation by  [2.x.4464] , and then integrate over  [2.x.4465] . As explained above, we can't do the integration by parts on all of  [2.x.4466]  with these shape functions, but we can do it on each cell individually since the shape functions are just polynomials on each cell. Consequently, we start by using the following integration-by-parts formula on each mesh cell  [2.x.4467] : 

[1.x.1750] 

At this point, we have two options: We can integrate the domain term's  [2.x.4468]  one more time to obtain 

[1.x.1751] 

For a variety of reasons, this turns out to be a variation that is not useful for our purposes. 

Instead, what we do is recognize that  [2.x.4469] , and we can re-sort these operations as  [2.x.4470]  where we typically write  [2.x.4471]  to indicate that this is the "Hessian" matrix of second derivatives. With this re-ordering, we can now integrate the divergence, rather than the gradient operator, and we get the following instead: 

[1.x.1752] 

Here, the colon indicates a double-contraction over the indices of the matrices to its left and right, i.e., the scalar product between two tensors. The outer product of two vectors  [2.x.4472]  yields the matrix  [2.x.4473] . 

Then, we sum over all cells  [2.x.4474] , and take into account that this means that every interior face appears twice in the sum. If we therefore split everything into a sum of integrals over cell interiors and a separate sum over cell interfaces, we can use the jump and average operators defined above. There are two steps left: First, because our shape functions are continuous, the gradients of the shape functions may be discontinuous, but the continuity guarantees that really only the normal component of the gradient is discontinuous across faces whereas the tangential component(s) are continuous. Second, the discrete formulation that results is not stable as the mesh size goes to zero, and to obtain a stable formulation that converges to the correct solution, we need to add the following terms: 

[1.x.1753] 

Then, after making cancellations that arise, we arrive at the following C0IP formulation of the biharmonic equation: find  [2.x.4475]  such that  [2.x.4476]  on  [2.x.4477]  and 

[1.x.1754] 

where 

[1.x.1755] 

and 

[1.x.1756] 

Here,  [2.x.4478]  is the penalty parameter which both weakly enforces the boundary condition 

[1.x.1757] 

on the boundary interfaces  [2.x.4479] , and also ensures that in the limit  [2.x.4480] ,  [2.x.4481]  converges to a  [2.x.4482]  continuous function.  [2.x.4483]  is chosen to be large enough to guarantee the stability of the method. We will discuss our choice in the program below. 




[1.x.1758] On polygonal domains, the weak solution  [2.x.4484]  to the biharmonic equation lives in  [2.x.4485]  where  [2.x.4486]  is determined by the interior angles at the corners of  [2.x.4487] . For instance, whenever  [2.x.4488]  is convex,  [2.x.4489] ;  [2.x.4490]  may be less than one if the domain has re-entrant corners but  [2.x.4491]  is close to  [2.x.4492]  if one of all interior angles is close to  [2.x.4493] . 

Now suppose that the  [2.x.4494]  IP solution  [2.x.4495]  is approximated by  [2.x.4496]  shape functions with polynomial degree  [2.x.4497] . Then the discretization outlined above yields the convergence rates as discussed below. 


[1.x.1759] 

Ideally, we would like to measure convergence in the "energy norm"  [2.x.4498] . However, this does not work because, again, the discrete solution  [2.x.4499]  does not have two (weak) derivatives. Instead, one can define a discrete ( [2.x.4500]  IP) seminorm that is "equivalent" to the energy norm, as follows: 

[1.x.1760] 



In this seminorm, the theory in the paper mentioned above yields that we can expect 

[1.x.1761] 

much as one would expect given the convergence rates we know are true for the usual discretizations of the Laplace equation. 

Of course, this is true only if the exact solution is sufficiently smooth. Indeed, if  [2.x.4501]  with  [2.x.4502] ,  [2.x.4503]  where  [2.x.4504] , then the convergence rate of the  [2.x.4505]  IP method is  [2.x.4506] . In other words, the optimal convergence rate can only be expected if the solution is so smooth that  [2.x.4507] ; this can only happen if (i) the domain is convex with a sufficiently smooth boundary, and (ii)  [2.x.4508] . In practice, of course, the solution is what it is (independent of the polynomial degree we choose), and the last condition can then equivalently be read as saying that there is definitely no point in choosing  [2.x.4509]  large if  [2.x.4510]  is not also large. In other words, the only reasonably choices for  [2.x.4511]  are  [2.x.4512]  because larger polynomial degrees do not result in higher convergence orders. 

For the purposes of this program, we're a bit too lazy to actually implement this equivalent seminorm -- though it's not very difficult and would make for a good exercise. Instead, we'll simply check in the program what the "broken"  [2.x.4513]  seminorm 

[1.x.1762] 

yields. The convergence rate in this norm can, from a theoretical perspective, of course not be [1.x.1763] than the one for  [2.x.4514]  because it contains only a subset of the necessary terms, but it could at least conceivably be better. It could also be the case that we get the optimal convergence rate even though there is a bug in the program, and that that bug would only show up in sub-optimal rates for the additional terms present in  [2.x.4515] . But, one might hope that if we get the optimal rate in the broken norm and the norms discussed below, then the program is indeed correct. The results section will demonstrate that we obtain optimal rates in all norms shown. 


[1.x.1764] 

The optimal convergence rate in the  [2.x.4516] -norm is  [2.x.4517]  provided  [2.x.4518] . More details can be found in Theorem 4.6 of  [2.x.4519]  . 

The default in the program below is to choose  [2.x.4520] . In that case, the theorem does not apply, and indeed one only gets  [2.x.4521]  instead of  [2.x.4522]  as we will show in the results section. 


[1.x.1765] 

Given that we expect  [2.x.4523]  in the best of cases for a norm equivalent to the  [2.x.4524]  seminorm, and  [2.x.4525]  for the  [2.x.4526]  norm, one may ask about what happens in the  [2.x.4527]  seminorm that is intermediate to the two others. A reasonable guess is that one should expect  [2.x.4528] . There is probably a paper somewhere that proves this, but we also verify that this conjecture is experimentally true below. 




[1.x.1766] 

We remark that the derivation of the  [2.x.4529]  IP method for the biharmonic equation with other boundary conditions -- for instance, for the first set of boundary conditions namely  [2.x.4530]  and  [2.x.4531]  on  [2.x.4532]  -- can be obtained with suitable modifications to  [2.x.4533]  and  [2.x.4534]  described in the book chapter  [2.x.4535]  . 




[1.x.1767] 

The last step that remains to describe is what this program solves for. As always, a trigonometric function is both a good and a bad choice because it does not lie in any polynomial space in which we may seek the solution while at the same time being smoother than real solutions typically are (here, it is in  [2.x.4536]  while real solutions are typically only in  [2.x.4537]  or so on convex polygonal domains, or somewhere between  [2.x.4538]  and  [2.x.4539]  if the domain is not convex). But, since we don't have the means to describe solutions of realistic problems in terms of relatively simple formulas, we just go with the following, on the unit square for the domain  [2.x.4540] : 

[1.x.1768] 

As a consequence, we then need choose as boundary conditions the following: 

[1.x.1769] 

The right hand side is easily computed as 

[1.x.1770] 

The program has classes  [2.x.4541]  and  [2.x.4542]  that encode this information. 


examples/step-47/doc/results.dox 



[1.x.1771] 

We run the program with right hand side and boundary values as discussed in the introduction. These will produce the solution  [2.x.4543]  on the domain  [2.x.4544] . We test this setup using  [2.x.4545] ,  [2.x.4546] , and  [2.x.4547]  elements, which one can change via the `fe_degree` variable in the `main()` function. With mesh refinement, the  [2.x.4548]  convergence rates,  [2.x.4549] -seminorm rate, and  [2.x.4550] -seminorm convergence of  [2.x.4551]  should then be around 2, 2, 1 for  [2.x.4552]  (with the  [2.x.4553]  norm sub-optimal as discussed in the introduction); 4, 3, 2 for  [2.x.4554] ; and 5, 4, 3 for  [2.x.4555] , respectively. 

From the literature, it is not immediately clear what the penalty parameter  [2.x.4556]  should be. For example,  [2.x.4557]  state that it needs to be larger than one, and choose  [2.x.4558] . The FEniCS/Dolphin tutorial chooses it as  [2.x.4559] , see https://fenicsproject.org/docs/dolfin/1.6.0/python/demo/documented/biharmonic/python/documentation.html .  [2.x.4560]  uses a value for  [2.x.4561]  larger than the number of edges belonging to an element for Kirchhoff plates (see their Section 4.2). This suggests that maybe  [2.x.4562] ,  [2.x.4563] , are too small; on the other hand, a value  [2.x.4564]  would be reasonable, where  [2.x.4565]  is the degree of polynomials. The last of these choices is the one one would expect to work by comparing to the discontinuous Galerkin formulations for the Laplace equation (see, for example, the discussions in step-39 and step-74), and it will turn out to also work here. But we should check what value of  [2.x.4566]  is right, and we will do so below; changing  [2.x.4567]  is easy in the two `face_worker` and `boundary_worker` functions defined in `assemble_system()`. 




[1.x.1772][1.x.1773][1.x.1774] 

We run the code with differently refined meshes and get the following convergence rates. 

 [2.x.4568]  We can see that the  [2.x.4569]  convergence rates are around 2,  [2.x.4570] -seminorm convergence rates are around 2, and  [2.x.4571] -seminorm convergence rates are around 1. The latter two match the theoretically expected rates; for the former, we have no theorem but are not surprised that it is sub-optimal given the remark in the introduction. 




[1.x.1775][1.x.1776][1.x.1777] 


 [2.x.4572]  We can see that the  [2.x.4573]  convergence rates are around 4,  [2.x.4574] -seminorm convergence rates are around 3, and  [2.x.4575] -seminorm convergence rates are around 2. This, of course, matches our theoretical expectations. 




[1.x.1778][1.x.1779][1.x.1780] 

 [2.x.4576]  We can see that the  [2.x.4577]  norm convergence rates are around 5,  [2.x.4578] -seminorm convergence rates are around 4, and  [2.x.4579] -seminorm convergence rates are around 3. On the finest mesh, the  [2.x.4580]  norm convergence rate is much smaller than our theoretical expectations because the linear solver becomes the limiting factor due to round-off. Of course the  [2.x.4581]  error is also very small already in that case. 




[1.x.1781][1.x.1782][1.x.1783] 

For comparison with the results above, let us now also consider the case where we simply choose  [2.x.4582] : 

 [2.x.4583]  Although  [2.x.4584]  norm convergence rates of  [2.x.4585]  more or less follows the theoretical expectations, the  [2.x.4586] -seminorm and  [2.x.4587] -seminorm do not seem to converge as expected. Comparing results from  [2.x.4588]  and  [2.x.4589] , it is clear that  [2.x.4590]  is a better penalty. Given that  [2.x.4591]  is already too small for  [2.x.4592]  elements, it may not be surprising that if one repeated the experiment with a  [2.x.4593]  element, the results are even more disappointing: One again only obtains convergence rates of 2, 1, zero -- i.e., no better than for the  [2.x.4594]  element (although the errors are smaller in magnitude). Maybe surprisingly, however, one obtains more or less the expected convergence orders when using  [2.x.4595]  elements. Regardless, this uncertainty suggests that  [2.x.4596]  is at best a risky choice, and at worst an unreliable one and that we should choose  [2.x.4597]  larger. 




[1.x.1784][1.x.1785][1.x.1786] 

Since  [2.x.4598]  is clearly too small, one might conjecture that  [2.x.4599]  might actually work better. Here is what one obtains in that case: 

 [2.x.4600]  In this case, the convergence rates more or less follow the theoretical expectations, but, compared to the results from  [2.x.4601] , are more variable. Again, we could repeat this kind of experiment for  [2.x.4602]  and  [2.x.4603]  elements. In both cases, we will find that we obtain roughly the expected convergence rates. Of more interest may then be to compare the absolute size of the errors. While in the table above, for the  [2.x.4604]  case, the errors on the finest grid are comparable between the  [2.x.4605]  and  [2.x.4606]  case, for  [2.x.4607]  the errors are substantially larger for  [2.x.4608]  than for  [2.x.4609] . The same is true for the  [2.x.4610]  case. 




[1.x.1787] 

The conclusions for which of the "reasonable" choices one should use for the penalty parameter is that  [2.x.4611]  yields the expected results. It is, consequently, what the code uses as currently written. 




[1.x.1788] 

There are a number of obvious extensions to this program that would make sense: 

- The program uses a square domain and a uniform mesh. Real problems   don't come this way, and one should verify convergence also on   domains with other shapes and, in particular, curved boundaries. One   may also be interested in resolving areas of less regularity by   using adaptive mesh refinement. 

- From a more theoretical perspective, the convergence results above   only used the "broken"  [2.x.4612]  seminorm  [2.x.4613]  instead   of the "equivalent" norm  [2.x.4614] . This is good enough to   convince ourselves that the program isn't fundamentally   broken. However, it might be interesting to measure the error in the   actual norm for which we have theoretical results. Implementing this   addition should not be overly difficult using, for example, the   FEInterfaceValues class combined with  [2.x.4615]  in the   same spirit as we used for the assembly of the linear system. 


  [1.x.1789] 

  Similar to the "clamped" boundary condition addressed in the implementation,   we will derive the  [2.x.4616]  IP finite element scheme for simply supported plates:   [1.x.1790] 

  We multiply the biharmonic equation by the test function  [2.x.4617]  and integrate over  [2.x.4618]  and get:   [1.x.1791] 



  Summing up over all cells  [2.x.4619] ,since normal directions of  [2.x.4620]  are pointing at   opposite directions on each interior edge shared by two cells and  [2.x.4621]  on  [2.x.4622] ,   [1.x.1792] 

  and by the definition of jump over cell interfaces,   [1.x.1793] 

  We separate interior faces and boundary faces of the domain,   [1.x.1794] 

  where  [2.x.4623]  is the set of interior faces.   This leads us to   [1.x.1795] 



  In order to symmetrize and stabilize the discrete problem,   we add symmetrization and stabilization term.   We finally get the  [2.x.4624]  IP finite element scheme for the biharmonic equation:   find  [2.x.4625]  such that  [2.x.4626]  on  [2.x.4627]  and   [1.x.1796] 

  where   [1.x.1797] 

  and   [1.x.1798] 

  The implementation of this boundary case is similar to the "clamped" version   except that `boundary_worker` is no longer needed for system assembling   and the right hand side is changed according to the formulation. 


examples/step-48/doc/intro.dox 



[1.x.1799][1.x.1800] 

[1.x.1801] 

[1.x.1802] 

This program demonstrates how to use the cell-based implementation of finite element operators with the MatrixFree class, first introduced in step-37, to solve nonlinear partial differential equations. Moreover, we have another look at the handling of constraints within the matrix-free framework. Finally, we will use an explicit time-stepping method to solve the problem and introduce Gauss-Lobatto finite elements that are very convenient in this case since their mass matrix can be accurately approximated by a diagonal, and thus trivially invertible, matrix. The two ingredients to this property are firstly a distribution of the nodal points of Lagrange polynomials according to the point distribution of the Gauss-Lobatto quadrature rule. Secondly, the quadrature is done with the same Gauss-Lobatto quadrature rule. In this formula, the integrals  [2.x.4628]  become zero whenever  [2.x.4629] , because exactly one function  [2.x.4630]  is one and all others zero in the points defining the Lagrange polynomials. Moreover, the Gauss-Lobatto distribution of nodes of Lagrange polynomials clusters the nodes towards the element boundaries. This results in a well-conditioned polynomial basis for high-order discretization methods. Indeed, the condition number of an FE_Q elements with equidistant nodes grows exponentially with the degree, which destroys any benefit for orders of about five and higher. For this reason, Gauss-Lobatto points are the default distribution for the FE_Q element (but at degrees one and two, those are equivalent to the equidistant points). 

[1.x.1803] 

As an example, we choose to solve the sine-Gordon soliton equation 

[1.x.1804] 

that was already introduced in step-25. As a simple explicit time integration method, we choose leap frog scheme using the second-order formulation of the equation. With this time stepping, the scheme reads in weak form 

[1.x.1805] where [1.x.1806] denotes a test function and the index [1.x.1807] stands for the time step number. 

For the spatial discretization, we choose FE_Q elements with basis functions defined to interpolate the support points of the Gauss-Lobatto quadrature rule. Moreover, when we compute the integrals over the basis functions to form the mass matrix and the operator on the right hand side of the equation above, we use the Gauss-Lobatto quadrature rule with the same support points as the node points of the finite element to evaluate the integrals. Since the finite element is Lagrangian, this will yield a diagonal mass matrix on the left hand side of the equation, making the solution of the linear system in each time step trivial. 

Using this quadrature rule, for a [1.x.1808]th order finite element, we use a [1.x.1809]th order accurate formula to evaluate the integrals. Since the product of two [1.x.1810]th order basis functions when computing a mass matrix gives a function with polynomial degree [1.x.1811] in each direction, the integrals are not computed exactly.  However, the overall convergence properties are not disturbed by the quadrature error on meshes with affine element shapes with L2 errors proportional to [1.x.1812]. Note however that order reduction with sub-optimal convergence rates of the L2 error of [1.x.1813] or even [1.x.1814] for some 3D setups has been reported [1.x.1815] on deformed (non-affine) element shapes for wave equations when the integrand is not a polynomial any more. 

Apart from the fact that we avoid solving linear systems with this type of elements when using explicit time-stepping, they come with two other advantages. When we are using the sum-factorization approach to evaluate the finite element operator (cf. step-37), we have to evaluate the function at the quadrature points. In the case of Gauss-Lobatto elements, where quadrature points and node points of the finite element coincide, this operation is trivial since the value of the function at the quadrature points is given by its one-dimensional coefficients. In this way, the arithmetic work for the finite element operator evaluation is reduced by approximately a factor of two compared to the generic Gaussian quadrature. 

To sum up the discussion, by using the right finite element and quadrature rule combination, we end up with a scheme where we only need to compute the right hand side vector corresponding to the formulation above and then multiply it by the inverse of the diagonal mass matrix in each time step. In practice, of course, we extract the diagonal elements and invert them only once at the beginning of the program. 

[1.x.1816] 

The usual way to handle constraints in  [2.x.4631]  is to use the AffineConstraints class that builds a sparse matrix storing information about which degrees of freedom (DoF) are constrained and how they are constrained. This format uses an unnecessarily large amount of memory since there are not so many different types of constraints: for example, in the case of hanging nodes when using linear finite element on every cell, most constraints have the form  [2.x.4632]  where the coefficients  [2.x.4633]  are always the same and only  [2.x.4634]  are different. While storing this redundant information is not a problem in general because it is only needed once during matrix and right hand side assembly, it becomes a bottleneck in the matrix-free approach since there this information has to be accessed every time we apply the operator, and the remaining components of the operator evaluation are so fast. Thus, instead of an AffineConstraints object, MatrixFree uses a variable that we call  [2.x.4635]  that collects the weights of the different constraints. Then, only an identifier of each constraint in the mesh instead of all the weights have to be stored. Moreover, the constraints are not applied in a pre- and postprocessing step but rather as we evaluate the finite element operator. Therefore, the constraint information is embedded into the variable  [2.x.4636]  that is used to extract the cell information from the global vector. If a DoF is constrained, the  [2.x.4637]  variable contains the global indices of the DoFs that it is constrained to. Then, we have another variable  [2.x.4638]  at hand that holds, for each cell, the local indices of DoFs that are constrained as well as the identifier of the type of constraint. Fortunately, you will not see these data structures in the example program since the class  [2.x.4639]  takes care of the constraints without user interaction. 

In the presence of hanging nodes, the diagonal mass matrix obtained on the element level via the Gauss-Lobatto quadrature/node point procedure does not directly translate to a diagonal global mass matrix, as following the constraints on rows and columns would also add off-diagonal entries. As explained in [1.x.1817], interpolating constraints on a vector, which maintains the diagonal shape of the mass matrix, is consistent with the equations up to an error of the same magnitude as the quadrature error. In the program below, we will simply assemble the diagonal of the mass matrix as if it were a vector to enable this approximation. 




[1.x.1818] 

The MatrixFree class comes with the option to be parallelized on three levels: MPI parallelization on clusters of distributed nodes, thread parallelization scheduled by the Threading Building Blocks library, and finally with a vectorization by working on a batch of two (or more) cells via SIMD data type (sometimes called cross-element or external vectorization). As we have already discussed in step-37, you will get best performance by using an instruction set specific to your system, e.g. with the cmake variable <tt>-DCMAKE_CXX_FLAGS="-march=native"</tt>. The MPI parallelization was already exploited in step-37. Here, we additionally consider thread parallelization with TBB. This is fairly simple, as all we need to do is to tell the initialization of the MatrixFree object about the fact that we want to use a thread parallel scheme through the variable  [2.x.4640]  During setup, a dependency graph is set up similar to the one described in the  [2.x.4641]  , which allows to schedule the work of the  [2.x.4642]  function on chunks of cells without several threads accessing the same vector indices. As opposed to the WorkStream loops, some additional clever tricks to avoid global synchronizations as described in [1.x.1819] are also applied. 

Note that this program is designed to be run with a distributed triangulation  [2.x.4643]  which requires deal.II to be configured with [1.x.1820] as described in the [1.x.1821] file. However, a non-distributed triangulation is also supported, in which case the computation will be run in serial. 

[1.x.1822] 

In our example, we choose the initial value to be [1.x.1823] and solve the equation over the time interval [-10,10]. The constants are chosen to be  [2.x.4644]  and [1.x.1824]. As mentioned in step-25, in one dimension [1.x.1825] as a function of [1.x.1826] is the exact solution of the sine-Gordon equation. For higher dimension, this is however not the case. 


examples/step-48/doc/results.dox 



[1.x.1827] 

[1.x.1828] 

In order to demonstrate the gain in using the MatrixFree class instead of the standard  [2.x.4645]  assembly routines for evaluating the information from old time steps, we study a simple serial run of the code on a nonadaptive mesh. Since much time is spent on evaluating the sine function, we do not only show the numbers of the full sine-Gordon equation but also for the wave equation (the sine-term skipped from the sine-Gordon equation). We use both second and fourth order elements. The results are summarized in the following table. 

 [2.x.4646]  

It is apparent that the matrix-free code outperforms the standard assembly routines in deal.II by far. In 3D and for fourth order elements, one operator evaluation is also almost ten times as fast as a sparse matrix-vector product. 

[1.x.1829] 

We start with the program output obtained on a workstation with 12 cores / 24 threads (one Intel Xeon E5-2687W v4 CPU running at 3.2 GHz, hyperthreading enabled), running the program in release mode: 

[1.x.1830] 



In 3D, the respective output looks like 

[1.x.1831] 



It takes 0.008 seconds for one time step with more than a million degrees of freedom (note that we would need many processors to reach such numbers when solving linear systems). 

If we replace the thread-parallelization by a pure MPI parallelization, the timings change into: 

[1.x.1832] 



We observe a dramatic speedup for the output (which makes sense, given that most code of the output is not parallelized via threads, whereas it is for MPI), but less than the theoretical factor of 12 we would expect from the parallelism. More interestingly, the computations also get faster when switching from the threads-only variant to the MPI-only variant. This is a general observation for the MatrixFree framework (as of updating this data in 2019). The main reason is that the decisions regarding work on conflicting cell batches made to enable execution in parallel are overly pessimistic: While they ensure that no work on neighboring cells is done on different threads at the same time, this conservative setting implies that data from neighboring cells is also evicted from caches by the time neighbors get touched. Furthermore, the current scheme is not able to provide a constant load for all 24 threads for the given mesh with 17,592 cells. 

The current program allows to also mix MPI parallelization with thread parallelization. This is most beneficial when running programs on clusters with multiple nodes, using MPI for the inter-node parallelization and threads for the intra-node parallelization. On the workstation used above, we can run threads in the hyperthreading region (i.e., using 2 threads for each of the 12 MPI ranks). An important setting for mixing MPI with threads is to ensure proper binning of tasks to CPUs. On many clusters the placing is either automatically via the `mpirun/mpiexec` environment, or there can be manual settings. Here, we simply report the run times the plain version of the program (noting that things could be improved towards the timings of the MPI-only program when proper pinning is done): 

[1.x.1833] 






[1.x.1834] 

There are several things in this program that could be improved to make it even more efficient (besides improved boundary conditions and physical stuff as discussed in step-25): 

 [2.x.4647]   [2.x.4648]  [1.x.1835] As becomes obvious   from the comparison of the plain wave equation and the sine-Gordon   equation above, the evaluation of the sine terms dominates the total   time for the finite element operator application. There are a few   reasons for this: Firstly, the deal.II sine computation of a   VectorizedArray field is not vectorized (as opposed to the rest of   the operator application). This could be cured by handing the sine   computation to a library with vectorized sine computations like   Intel's math kernel library (MKL). By using the function    [2.x.4649]  in MKL, the program uses half the computing time   in 2D and 40 percent less time in 3D. On the other hand, the sine   computation is structurally much more complicated than the simple   arithmetic operations like additions and multiplications in the rest   of the local operation. 

   [2.x.4650]  [1.x.1836] While the implementation allows for   arbitrary order in the spatial part (by adjusting the degree of the finite   element), the time stepping scheme is a standard second-order leap-frog   scheme. Since solutions in wave propagation problems are usually very   smooth, the error is likely dominated by the time stepping part. Of course,   this could be cured by using smaller time steps (at a fixed spatial   resolution), but it would be more efficient to use higher order time   stepping as well. While it would be straight-forward to do so for a   first-order system (use some Runge&ndash;Kutta scheme of higher order,   probably combined with adaptive time step selection like the [1.x.1837]), it is more challenging for the second-order formulation. At   least in the finite difference community, people usually use the PDE to find   spatial correction terms that improve the temporal error. 

 [2.x.4651]  


examples/step-49/doc/intro.dox 

[1.x.1838] 

[1.x.1839] 

[1.x.1840] This tutorial is an extension to step-1 and demonstrates several ways to obtain more involved meshes than the ones shown there. 

 [2.x.4652]  This tutorial is also available as a Jupyter Python notebook that   uses the deal.II python interface. The notebook is available in the   same directory as the original C++ program. 

Generating complex geometries is a challenging task, especially in three space dimensions. We will discuss several ways to do this, but this list is not exhaustive. Additionally, there is not one approach that fits all problems. 

This example program shows some of ways to create and modify meshes for computations and outputs them as  [2.x.4653]  files in much the same way as we do in step-1. No other computations or adaptive refinements are done; the idea is that you can use the techniques used here as building blocks in other, more involved simulators. Please note that the example program does not show all the ways to generate meshes that are discussed in this introduction. 




[1.x.1841] 

When you use adaptive mesh refinement, you definitely want the initial mesh to be as coarse as possible. The reason is that you can make it as fine as you want using adaptive refinement as long as you have memory and CPU time available. However, this requires that you don't waste mesh cells in parts of the domain where they don't pay off. As a consequence, you don't want to start with a mesh that is too fine to start with, because that takes up a good part of your cell budget already, and because you can't coarsen away cells that are in the initial mesh. 

That said, your mesh needs to capture the given geometry adequately. 




[1.x.1842] 

There are several ways to create an initial mesh. Meshes can be modified or combined in many ways as discussed later on. 

[1.x.1843] 

The easiest way to generate meshes is to use the functions in namespace GridGenerator, as already discussed in step-1.  There are many different helper functions available, including  [2.x.4654]   [2.x.4655]   [2.x.4656]  and  [2.x.4657]  




[1.x.1844] 

If there is no good fit in the GridGenerator namespace for what you want to do, you can always create a Triangulation in your program "by hand". For that, you need a list of vertices with their coordinates and a list of cells referencing those vertices. You can find an example in the function <tt>create_coarse_grid()</tt> in step-14. All the functions in GridGenerator are implemented in this fashion. 

We are happy to accept more functions to be added to GridGenerator. So, if you end up writing a function that might be useful for a larger audience, please contribute it. 




[1.x.1845] 

The class GridIn can read many different mesh formats from a file from disk. How this is done is explained in step-5 and can be seen in the function  [2.x.4658]  in this example, see the code below. 

Meshes can be generated from different tools like [1.x.1846], [1.x.1847] and [1.x.1848]. See the documentation of GridIn for more information. The problem is that deal.II needs meshes that only consist of quadrilaterals and hexahedra -- tetrahedral meshes won't work (this means tools like tetgen can not be used directly). 

We will describe a possible workflow using %Gmsh. %Gmsh is the smallest and most quickly set up open source tool we are aware of. It can generate unstructured 2d quad meshes. In 3d, it can extrude 2d meshes to get hexahedral meshes; 3D meshing of unstructured geometry into hexahedra is possible, though there are some issues with the quality of these meshes that imply that these meshes only sometimes work in deal.II. 

In %Gmsh, a mesh is fundamentally described in a text-based  [2.x.4659]  file whose format can contain computations, loops, variables, etc. This format is quite flexible in allowing the description of complex geometries. The mesh is then generated from a surface representation, which is built from a list of line loops, which is built from a list of lines, which are in turn built from points. The  [2.x.4660]  script can be written and edited by hand or it can be generated automatically by creating objects graphically inside %Gmsh. In many cases it is best to combine both approaches. The file can be easily reloaded by pressing "reload" under the "Geometry" tab if you want to write it by hand and see the effects in the graphical user interface of gmsh. 

This tutorial contains an example  [2.x.4661]  file that describes a box with two objects cut out in the interior. This is how  [2.x.4662]  looks like in %Gmsh (displaying the boundary indicators as well as the mesh discussed further down below): 

 [2.x.4663]  

You might want to open the  [2.x.4664]  file in a text editor (it is located in the same directory as the <tt>step-49.cc</tt> source file) to see how it is structured. You can see how the boundary of the domain is composed of a number of lines and how later on we combine several lines into "physical lines" (or "physical surfaces") that list the logical lines' numbers. "Physical" object are the ones that carry information about the boundary indicator (see  [2.x.4665]  "this glossary entry"). 

 [2.x.4666]  It is important that this file contain "physical lines" and "physical   surfaces". These give the boundary indicators and material ids for use   in deal.II. Without these physical entities, nothing will be imported into   deal.II. 

deal.II's GridIn class can read the  [2.x.4667]  format written by %Gmsh and that contains a mesh created for the geometry described by the  [2.x.4668]  from the  [2.x.4669]  by running the commands 

[1.x.1849] 



on the command line, or by clicking "Mesh" and then "2D" inside %Gmsh after loading the file.  Now this is the mesh read from the  [2.x.4670]  file and saved again by deal.II as an image (see the  [2.x.4671]  function of the current program): 

 [2.x.4672]  

 [2.x.4673]  %Gmsh has a number of other interfaces by which one can describe   geometries to it. In particular, it has the ability to interface with   scripting languages like Python and Julia, but it can also be scripted   from C++. These interfaces are useful if one doesn't just want to generate   a mesh for a single geometry (in which case the graphical interface or,   in simple cases, a hand-written `.geo` file is probably the simplest   approach), but instead wants to do parametric studies over the geometry   for which it is necessary to generate many meshes for geometries that   differ in certain parameters. Another case where this is useful is if there   is already a CAD geometry for which one only needs a mesh; indeed, this   can be done from within deal.II using the    [2.x.4674]  function. 




[1.x.1850] 

After acquiring one (or several) meshes in the ways described above, there are many ways to manipulate them before using them in a finite element computation. 




[1.x.1851] 

The GridTools namespace contains a collection of small functions to transform a given mesh in various ways. The usage of the functions  [2.x.4675]   [2.x.4676]   [2.x.4677]  is fairly obvious, so we won't discuss those functions here. 

The function  [2.x.4678]  allows you to transform the vertices of a given mesh using a smooth function. An example of its use is also given in the results section of step-38 but let us show a simpler example here: In the function  [2.x.4679]  of the current program, we perturb the y coordinate of a mesh with a sine curve: 

 [2.x.4680]  

Similarly, we can transform a regularly refined unit square to a wall-adapted mesh in y direction using the formula  [2.x.4681] . This is done in  [2.x.4682]  of this tutorial:  [2.x.4683]  

Finally, the function  [2.x.4684]  allows you to move vertices in the mesh (optionally ignoring boundary nodes) by a random amount. This is demonstrated in  [2.x.4685]  and the result is as follows: 

 [2.x.4686]  

This function is primarily intended to negate some of the superconvergence effects one gets when studying convergence on regular meshes, as well as to suppress some optimizations in deal.II that can exploit the fact that cells are similar in shape. (Superconvergence refers to the fact that if a mesh has certain symmetries -- for example, if the edges running into a vertex are symmetric to this vertex, and if this is so for all vertices of a cell 

-- that the solution is then often convergent with a higher order than one would have expected from the usual error analysis. In the end, this is a result of the fact that if one were to make a Taylor expansion of the error, the symmetry leads to the fact that the expected next term of the expansion happens to be zero, and the error order is determined by the *second next* term. A distorted mesh does not have these symmetries and consequently the error reflects what one will see when solving the equation on *any* kind of mesh, rather than showing something that is only reflective of a particular situation.) 




[1.x.1852] 

The function  [2.x.4687]  allows you to merge two given Triangulation objects into a single one.  For this to work, the vertices of the shared edge or face have to match exactly.  Lining up the two meshes can be achieved using  [2.x.4688]  and  [2.x.4689]   In the function  [2.x.4690]  of this tutorial, we merge a square with a round hole (generated with  [2.x.4691]  and a rectangle (generated with  [2.x.4692]  The function  [2.x.4693]  allows you to specify the number of repetitions and the positions of the corners, so there is no need to shift the triangulation manually here. You should inspect the mesh graphically to make sure that cells line up correctly and no unpaired nodes exist in the merged Triangulation. 

These are the input meshes and the output mesh: 

 [2.x.4694]  




[1.x.1853] 

The function  [2.x.4695]  demonstrates the ability to pick individual vertices and move them around in an existing mesh. Note that this has the potential to produce degenerate or inverted cells and you shouldn't expect anything useful to come of using such meshes. Here, we create a box with a cylindrical hole that is not exactly centered by moving the top vertices upwards: 

 [2.x.4696]  

For the exact way how this is done, see the code below. 




[1.x.1854] 

If you need a 3d mesh that can be created by extruding a given 2d mesh (that can be created in any of the ways given above), you can use the function  [2.x.4697]  See the  [2.x.4698]  function in this tutorial for an example. Note that for this particular case, the given result could also be achieved using the 3d version of  [2.x.4699]  The main usage is a 2d mesh, generated for example with %Gmsh, that is read in from a  [2.x.4700]  file as described above. This is the output from grid_4(): 

 [2.x.4701]  




[1.x.1855] 

Creating a coarse mesh using the methods discussed above is only the first step. When you have it, it will typically serve as the basis for further mesh refinement. This is not difficult &mdash; in fact, there is nothing else to do &mdash; if your geometry consists of only straight faces. However, this is often not the case if you have a more complex geometry and more steps than just creating the mesh are necessary. We will go over some of these steps in the [1.x.1856] below. 


examples/step-49/doc/results.dox 



[1.x.1857] 

The program produces a series of  [2.x.4702]  files of the triangulations. The methods are discussed above. 




[1.x.1858] 

As mentioned in the introduction, creating a coarse mesh using the methods discussed here is only the first step. In order to refine a mesh, the Triangulation needs to know where to put new vertices on the mid-points of edges, faces, and cells. By default, these new points will be placed at the arithmetic mean of the surrounding points, but this isn't what you want if you need curved boundaries that aren't already adequately resolved by the coarse mesh. For example, for this mesh the central hole is supposed to be round: 

 [2.x.4703]  

If you simply refine it, the Triangulation class can not know whether you wanted the hole to be round or to be an octagon. The default is to place new points along existing straight lines. After two mesh refinement steps, this would yield the following mesh, which is not what we wanted: 

 [2.x.4704]  

What needs to happen is that you tell the triangulation that you in fact want to use a curved geometry. The way to do this requires three steps: 

- Create an object that describes the desired geometry. This object will be   queried when refining the Triangulation for new point placement. It will also   be used to calculate shape function values if a high degree mapping, like   MappingQ or MappingQGeneric, is used during system assembly.   In deal.II the Manifold class and classes inheriting from it (e.g.,   PolarManifold and FlatManifold) perform these calculations. 

- Notify the Triangulation object which Manifold classes to use. By default, a   Triangulation uses FlatManifold to do all geometric calculations,   which assumes that all cell edges are straight lines and all quadrilaterals   are flat. You can attach Manifold classes to a Triangulation by calling    [2.x.4705]  function, which associates a    [2.x.4706]  with a Manifold object. For more information on this   see the  [2.x.4707]  "glossary entry on this topic". 

- Finally, you must mark cells and cell faces with the correct    [2.x.4708] . For example, you could get an annular sector with   curved cells in Cartesian coordinates (but rectangles in polar coordinates)   by doing the following:   [1.x.1859] 

  Now, when the grid is refined, all cell splitting calculations will be done in   polar coordinates. 

All functions in the GridGenerator namespace which create a mesh where some cells should be curved also attach the correct Manifold object to the provided Triangulation: i.e., for those functions we get the correct behavior by default. For a hand-generated mesh, however, the situation is much more interesting. 

To illustrate this process in more detail, let us consider an example created by Yuhan Zhou as part of a 2013 semester project at Texas A&amp;M University. The goal was to generate (and use) a geometry that describes a microstructured electric device. In a CAD program, the geometry looks like this: 

 [2.x.4709]  

In the following, we will walk you through the entire process of creating a mesh for this geometry, including a number of common pitfalls by showing the things that can go wrong. 

The first step in getting there was to create a coarse mesh, which was done by creating a 2d coarse mesh for each of cross sections, extruding them into the third direction, and gluing them together. The following code does this, using the techniques previously described: 

[1.x.1860] 



This creates the following mesh: 

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.8.png"      alt="" width="400" height="355"> 

This mesh has the right general shape, but the top cells are now polygonal: their edges are no longer along circles and we do not have a very accurate representation of the original geometry. The next step is to teach the top part of the domain that it should be curved. Put another way, all calculations done on the top boundary cells should be done in cylindrical coordinates rather than Cartesian coordinates. We can do this by creating a CylindricalManifold object and associating it with the cells above  [2.x.4710] . This way, when we refine the cells on top, we will place new points along concentric circles instead of straight lines. 

In deal.II we describe all geometries with classes that inherit from Manifold. The default geometry is Cartesian and is implemented in the FlatManifold class. As the name suggests, Manifold and its inheriting classes provide a way to describe curves and curved cells in a general way with ideas and terminology from differential geometry: for example, CylindricalManifold inherits from ChartManifold, which describes a geometry through pull backs and push forwards. In general, one should think that the Triangulation class describes the topology of a domain (in addition, of course, to storing the locations of the vertices) while the Manifold classes describe the geometry of a domain (e.g., whether or not a pair of vertices lie along a circular arc or a straight line). A Triangulation will refine cells by doing computations with the Manifold associated with that cell regardless of whether or not the cell is on the boundary. Put another way: the Manifold classes do not need any information about where the boundary of the Triangulation actually is: it is up to the Triangulation to query the right Manifold for calculations on a cell. Most Manifold functions (e.g.,  [2.x.4711]  know nothing about the domain itself and just assume that the points given to it lie along a geodesic. In this case, with the CylindricalManifold constructed below, the geodesics are arcs along circles orthogonal to the  [2.x.4712] -axis centered along the line  [2.x.4713] . 

Since all three top parts of the domain use the same geodesics, we will mark all cells with centers above the  [2.x.4714]  line as being cylindrical in nature: 

[1.x.1861] 



With this code, we get a mesh that looks like this: 

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.9.png"      alt="" width="400" height="355"> 

This change fixes the boundary but creates a new problem: the cells adjacent to the cylinder's axis are badly distorted. We should use Cartesian coordinates for calculations on these central cells to avoid this issue. The cells along the center line all have a face that touches the line  [2.x.4715]  so, to implement this, we go back and overwrite the  [2.x.4716] s on these cells to be zero (which is the default): 

[1.x.1862] 



This gives us the following grid: 

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.10.png"      alt="" width="400" height="355"> 

This gives us a good mesh, where cells at the center of each circle are still Cartesian and cells around the boundary lie along a circle. We can really see the nice detail of the boundary fitted mesh if we refine two more times: 

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.11.png"      alt="" width="400" height="355"> 




[1.x.1863] 

[1.x.1864] 

It is often useful to assign different boundary ids to a mesh that is generated in one form or another as described in this tutorial to apply different boundary conditions. 

For example, you might want to apply a different boundary condition for the right boundary of the first grid in this program. To do this, iterate over the cells and their faces and identify the correct faces (for example using `cell->center()` to query the coordinates of the center of a cell as we do in step-1, or using `cell->face(f)->get_boundary_id()` to query the current boundary indicator of the  [2.x.4717] th face of the cell). You can then use `cell->face(f)->set_boundary_id()` to set the boundary id to something different. You can take a look back at step-1 how iteration over the meshes is done there. 

[1.x.1865] 

Computations on manifolds, like they are done in step-38, require a surface mesh embedded into a higher dimensional space. While some can be constructed using the GridGenerator namespace or loaded from a file, it is sometimes useful to extract a surface mesh from a volume mesh. 

Use the function  [2.x.4718]  to extract the surface elements of a mesh. Using the function on a 3d mesh (a `Triangulation<3,3>`, for example from `grid_4()`), this will return a `Triangulation<2,3>` that you can use in step-38.  Also try extracting the boundary mesh of a `Triangulation<2,2>`. 


<!-- 

Possible Extensions for this tutorial: 

- Database of unstructured meshes for convergence studies 

- how to remove or disable a cell inside a mesh 

--> 


examples/step-5/doc/intro.dox 

[1.x.1866] 

[1.x.1867] 

 [2.x.4719]  

This example does not show revolutionary new things, but it shows many small improvements over the previous examples, and also many small things that can usually be found in finite element programs. Among them are:  [2.x.4720]     [2.x.4721]  Computations on successively refined grids. At least in the        mathematical sciences, it is common to compute solutions on        a hierarchy of grids, in order to get a feeling for the accuracy        of the solution; if you only have one solution on a single grid, you        usually can't guess the accuracy of the        solution. Furthermore, deal.II is designed to support adaptive        algorithms where iterative solution on successively refined        grids is at the heart of algorithms. Although adaptive grids        are not used in this example, the foundations for them is laid        here.    [2.x.4722]  In practical applications, the domains are often subdivided        into triangulations by automatic mesh generators. In order to        use them, it is important to read coarse grids from a file. In        this example, we will read a coarse grid in UCD (unstructured        cell data) format. When this program was first written around        2000, UCD format was what the AVS Explorer used -- a program        reasonably widely used at the time but now no longer of        importance. (Nonetheless, the file format has survived and is        still understood by a number of programs.)    [2.x.4723]  Finite element programs usually use extensive amounts of        computing time, so some optimizations are sometimes        necessary. We will show some of them.    [2.x.4724]  On the other hand, finite element programs tend to be rather        complex, so debugging is an important aspect. We support safe        programming by using assertions that check the validity of        parameters and %internal states in a debug mode, but are removed        in optimized mode. ( [2.x.4725]     [2.x.4726]  Regarding the mathematical side, we show how to support a        variable coefficient in the elliptic operator and how to use        preconditioned iterative solvers for the linear systems of        equations.  [2.x.4727]  

The equation to solve here is as follows: 

[1.x.1868] 

If  [2.x.4728]  was a constant coefficient, this would simply be the Poisson equation. However, if it is indeed spatially variable, it is a more complex equation (often referred to as the "extended Poisson equation"). Depending on what the variable  [2.x.4729]  refers to it models a variety of situations with wide applicability: 

- If  [2.x.4730]  is the electric potential, then  [2.x.4731]  is the electric current   in a medium and the coefficient  [2.x.4732]  is the conductivity of the medium at any   given point. (In this situation, the right hand side of the equation would   be the electric source density and would usually be zero or consist of   localized, Delta-like, functions.) 

- If  [2.x.4733]  is the vertical deflection of a thin membrane, then  [2.x.4734]  would be a   measure of the local stiffness. This is the interpretation that will allow   us to interpret the images shown in the results section below. 

Since the Laplace/Poisson equation appears in so many contexts, there are many more interpretations than just the two listed above. 

When assembling the linear system for this equation, we need the weak form which here reads as follows: 

[1.x.1869] 

The implementation in the  [2.x.4735]  function follows immediately from this. 


examples/step-5/doc/results.dox 



[1.x.1870] 


Here is the console output: 

[1.x.1871] 






In each cycle, the number of cells quadruples and the number of CG iterations roughly doubles. Also, in each cycle, the program writes one output graphic file in VTU format. They are depicted in the following: 

 [2.x.4736]  




Due to the variable coefficient (the curvature there is reduced by the same factor by which the coefficient is increased), the top region of the solution is flattened. The gradient of the solution is discontinuous along the interface, although this is not very clearly visible in the pictures above. We will look at this in more detail in the next example. 

The pictures also show that the solution computed by this program is actually pretty wrong on a very coarse mesh (its magnitude is wrong). That's because no numerical method guarantees that the solution on a coarse mesh is particularly accurate -- but we know that the solution [1.x.1872] to the exact solution, and indeed you can see how the solutions from one mesh to the next seem to not change very much any more at the end. 


examples/step-50/doc/intro.dox 

 [2.x.4737]  

[1.x.1873] 

 [2.x.4738]  

 [2.x.4739]  As a prerequisite of this program, you need to have both p4est and either the PETSc or Trilinos library installed. The installation of deal.II together with these additional libraries is described in the [1.x.1874] file. 


[1.x.1875] 

[1.x.1876] 


This example shows the usage of the multilevel functions in deal.II on parallel, distributed meshes and gives a comparison between geometric and algebraic multigrid methods. The algebraic multigrid (AMG) preconditioner is the same used in step-40. Two geometric multigrid (GMG) preconditioners are considered: a matrix-based version similar to that in step-16 (but for parallel computations) and a matrix-free version discussed in step-37. The goal is to find out which approach leads to the best solver for large parallel computations. 

This tutorial is based on one of the numerical examples in  [2.x.4740] . Please see that publication for a detailed background on the multigrid implementation in deal.II. We will summarize some of the results in the following text. 

Algebraic multigrid methods are obviously the easiest to implement with deal.II since classes such as  [2.x.4741]  and  [2.x.4742]  are, in essence, black box preconditioners that require only a couple of lines to set up even for parallel computations. On the other hand, geometric multigrid methods require changes throughout a code base -- not very many, but one has to know what one is doing. 

What the results of this program will show is that algebraic and geometric multigrid methods are roughly comparable in performance [1.x.1877], and that matrix-free geometric multigrid methods are vastly better for the problem under consideration here. A secondary conclusion will be that matrix-based geometric multigrid methods really don't scale well strongly when the number of unknowns per processor becomes smaller than 20,000 or so. 




[1.x.1878] 

We consider the variable-coefficient Laplacian weak formulation 

[1.x.1879] 

on the domain  [2.x.4743]  (an L-shaped domain for 2D and a Fichera corner for 3D) with  [2.x.4744]  if  [2.x.4745]  and  [2.x.4746]  otherwise. In other words,  [2.x.4747]  is small along the edges or faces of the domain that run into the reentrant corner, as will be visible in the figure below. 

The boundary conditions are  [2.x.4748]  on the whole boundary and the right-hand side is  [2.x.4749] . We use continuous  [2.x.4750]  elements for the discrete finite element space  [2.x.4751] , and use a residual-based, cell-wise a posteriori error estimator  [2.x.4752]  from  [2.x.4753]  with 

[1.x.1880] 

to adaptively refine the mesh. (This is a generalization of the Kelly error estimator used in the KellyErrorEstimator class that drives mesh refinement in most of the other tutorial programs.) The following figure visualizes the solution and refinement for 2D:  [2.x.4754]  In 3D, the solution looks similar (see below). On the left you can see the solution and on the right we show a slice for  [2.x.4755]  close to the center of the domain showing the adaptively refined mesh.  [2.x.4756]  Both in 2D and 3D you can see the adaptive refinement picking up the corner singularity and the inner singularity where the viscosity jumps, while the interface along the line that separates the two viscosities is (correctly) not refined as it is resolved adequately. This is because the kink in the solution that results from the jump in the coefficient is aligned with cell interfaces. 




[1.x.1881] 

As mentioned above, the purpose of this program is to demonstrate the use of algebraic and geometric multigrid methods for this problem, and to do so for parallel computations. An important component of making algorithms scale to large parallel machines is ensuring that every processor has the same amount of work to do. (More precisely, what matters is that there are no small fraction of processors that have substantially more work than the rest since, if that were so, a large fraction of processors will sit idle waiting for the small fraction to finish. Conversely, a small fraction of processors having substantially [1.x.1882] work is not a problem because the majority of processors continues to be productive and only the small fraction sits idle once finished with their work.) 

For the active mesh, we use the  [2.x.4757]  class as done in step-40 which uses functionality in the external library [1.x.1883] for the distribution of the active cells among processors. For the non-active cells in the multilevel hierarchy, deal.II implements what we will refer to as the "first-child rule" where, for each cell in the hierarchy, we recursively assign the parent of a cell to the owner of the first child cell. The following figures give an example of such a distribution. Here the left image represents the active cells for a sample 2D mesh partitioned using a space-filling curve (which is what p4est uses to partition cells); the center image gives the tree representation of the active mesh; and the right image gives the multilevel hierarchy of cells. The colors and numbers represent the different processors. The circular nodes in the tree are the non-active cells which are distributed using the "first-child rule". 

 [2.x.4758]  

Included among the output to screen in this example is a value "Partition efficiency" given by one over  [2.x.4759]  This value, which will be denoted by  [2.x.4760] ,  quantifies the overhead produced by not having a perfect work balance on each level of the multigrid hierarchy. This imbalance is evident from the example above: while level  [2.x.4761]  is about as well balanced as is possible with four cells among three processors, the coarse level  [2.x.4762]  has work for only one processor, and level  [2.x.4763]  has work for only two processors of which one has three times as much work as the other. 

For defining  [2.x.4764] , it is important to note that, as we are using local smoothing to define the multigrid hierarchy (see the  [2.x.4765]  "multigrid paper" for a description of local smoothing), the refinement level of a cell corresponds to that cell's multigrid level. Now, let  [2.x.4766]  be the number of cells on level  [2.x.4767]  (both active and non-active cells) and  [2.x.4768]  be the subset owned by process  [2.x.4769] . We will also denote by  [2.x.4770]  the total number of processors. Assuming that the workload for any one processor is proportional to the number of cells owned by that processor, the optimal workload per processor is given by 

[1.x.1884] 

Next, assuming a synchronization of work on each level (i.e., on each level of a V-cycle, work must be completed by all processors before moving on to the next level), the limiting effort on each level is given by 

[1.x.1885] 

and the total parallel complexity 

[1.x.1886] 

Then we define  [2.x.4771]  as a ratio of the optimal partition to the parallel complexity of the current partition 

[1.x.1887] 

For the example distribution above, we have 

[1.x.1888] 

The value  [2.x.4772]     [2.x.4773]  then represents the factor increase in timings we expect for GMG methods (vmults, assembly, etc.) due to the imbalance of the mesh partition compared to a perfectly load-balanced workload. We will report on these in the results section below for a sequence of meshes, and compare with the observed slow-downs as we go to larger and larger processor numbers (where, typically, the load imbalance becomes larger as well). 

These sorts of considerations are considered in much greater detail in  [2.x.4774] , which contains a full discussion of the partition efficiency model and the effect the imbalance has on the GMG V-cycle timing. In summary, the value of  [2.x.4775]  is highly dependent on the degree of local mesh refinement used and has an optimal value  [2.x.4776]  for globally refined meshes. Typically for adaptively refined meshes, the number of processors used to distribute a single mesh has a negative impact on  [2.x.4777]  but only up to a leveling off point, where the imbalance remains relatively constant for an increasing number of processors, and further refinement has very little impact on  [2.x.4778] . Finally,  [2.x.4779]  was shown to give an accurate representation of the slowdown in parallel scaling expected for the timing of a V-cycle. 

It should be noted that there is potential for some asynchronous work between multigrid levels, specifically with purely nearest neighbor MPI communication, and an adaptive mesh could be constructed such that the efficiency model would far overestimate the V-cycle slowdown due to the asynchronous work "covering up" the imbalance (which assumes synchronization over levels). However, for most realistic adaptive meshes the expectation is that this asynchronous work will only cover up a very small portion of the imbalance and the efficiency model will describe the slowdown very well. 




[1.x.1889] 

The considerations above show that one has to expect certain limits on the scalability of the geometric multigrid algorithm as it is implemented in deal.II because even in cases where the finest levels of a mesh are perfectly load balanced, the coarser levels may not be. At the same time, the coarser levels are weighted less (the contributions of  [2.x.4780]  to  [2.x.4781]  are small) because coarser levels have fewer cells and, consequently, do not contribute to the overall run time as much as finer levels. In other words, imbalances in the coarser levels may not lead to large effects in the big picture. 

Algebraic multigrid methods are of course based on an entirely different approach to creating a hierarchy of levels. In particular, they create these purely based on analyzing the system matrix, and very sophisticated algorithms for ensuring that the problem is well load-balanced on every level are implemented in both the hypre and ML/MueLu packages that underly the  [2.x.4782]  and  [2.x.4783]  classes. In some sense, these algorithms are simpler than for geometric multigrid methods because they only deal with the matrix itself, rather than all of the connotations of meshes, neighbors, parents, and other geometric entities. At the same time, much work has also been put into making algebraic multigrid methods scale to very large problems, including questions such as reducing the number of processors that work on a given level of the hierarchy to a subset of all processors, if otherwise processors would spend less time on computations than on communication. (One might note that it is of course possible to implement these same kinds of ideas also in geometric multigrid algorithms where one purposefully idles some processors on coarser levels to reduce the amount of communication. deal.II just doesn't do this at this time.) 

These are not considerations we typically have to worry about here, however: For most purposes, we use algebraic multigrid methods as black-box methods. 




[1.x.1890] 

As mentioned above, this program can use three different ways of solving the linear system: matrix-based geometric multigrid ("MB"), matrix-free geometric multigrid ("MF"), and algebraic multigrid ("AMG"). The directory in which this program resides has input files with suffix `.prm` for all three of these options, and for both 2d and 3d. 

You can execute the program as in 

[1.x.1891] 

and this will take the run-time parameters from the given input file (here, `gmg_mb_2d.prm`). 

The program is intended to be run in parallel, and you can achieve this using a command such as 

[1.x.1892] 

if you want to, for example, run on four processors. (That said, the program is also ready to run with, say, `-np 28672` if that's how many processors you have available.) 


examples/step-50/doc/results.dox 



[1.x.1893] 

When you run the program using the following command 

[1.x.1894] 

the screen output should look like the following: 

[1.x.1895] 

Here, the timing of the `solve()` function is split up in 3 parts: setting up the multigrid preconditioner, execution of a single multigrid V-cycle, and the CG solver. The V-cycle that is timed is unnecessary for the overall solve and only meant to give an insight at the different costs for AMG and GMG. Also it should be noted that when using the AMG solver, "Workload imbalance" is not included in the output since the hierarchy of coarse meshes is not required. 

All results in this section are gathered on Intel Xeon Platinum 8280 (Cascade Lake) nodes which have 56 cores and 192GB per node and support AVX-512 instructions, allowing for vectorization over 8 doubles (vectorization used only in the matrix-free computations). The code is compiled using gcc 7.1.0 with intel-mpi 17.0.3. Trilinos 12.10.1 is used for the matrix-based GMG/AMG computations. 

We can then gather a variety of information by calling the program with the input files that are provided in the directory in which step-50 is located. Using these, and adjusting the number of mesh refinement steps, we can produce information about how well the program scales. 

The following table gives weak scaling timings for this program on up to 256M DoFs and 7,168 processors. (Recall that weak scaling keeps the number of degrees of freedom per processor constant while increasing the number of processors; i.e., it considers larger and larger problems.) Here,  [2.x.4784]  is the partition efficiency from the  introduction (also equal to 1.0/workload imbalance), "Setup" is a combination of setup, setup multigrid, assemble, and assemble multigrid from the timing blocks, and "Prec" is the preconditioner setup. Ideally all times would stay constant over each problem size for the individual solvers, but since the partition efficiency decreases from 0.371 to 0.161 from largest to smallest problem size, we expect to see an approximately  [2.x.4785]  times increase in timings for GMG. This is, in fact, pretty close to what we really get: 

 [2.x.4786]  

On the other hand, the algebraic multigrid in the last set of columns is relatively unaffected by the increasing imbalance of the mesh hierarchy (because it doesn't use the mesh hierarchy) and the growth in time is rather driven by other factors that are well documented in the literature (most notably that the algorithmic complexity of some parts of algebraic multigrid methods appears to be  [2.x.4787]  instead of  [2.x.4788]  for geometric multigrid). 

The upshort of the table above is that the matrix-free geometric multigrid method appears to be the fastest approach to solving this equation if not by a huge margin. Matrix-based methods, on the other hand, are consistently the worst. 

The following figure provides strong scaling results for each method, i.e., we solve the same problem on more and more processors. Specifically, we consider the problems after 16 mesh refinement cycles (32M DoFs) and 19 cycles (256M DoFs), on between 56 to 28,672 processors: 

 [2.x.4789]  

While the matrix-based GMG solver and AMG scale similarly and have a similar time to solution (at least as long as there is a substantial number of unknowns per processor -- say, several 10,000), the matrix-free GMG solver scales much better and solves the finer problem in roughly the same time as the AMG solver for the coarser mesh with only an eighth of the number of processors. Conversely, it can solve the same problem on the same number of processors in about one eighth the time. 




[1.x.1896] 

[1.x.1897] 

The finite element degree is currently hard-coded as 2, see the template arguments of the main class. It is easy to change. To test, it would be interesting to switch to a test problem with a reference solution. This way, you can compare error rates. 

[1.x.1898] 

A more interesting example would involve a more complicated coarse mesh (see step-49 for inspiration). The issue in that case is that the coarsest level of the mesh hierarchy is actually quite large, and one would have to think about ways to solve the coarse level problem efficiently. (This is not an issue for algebraic multigrid methods because they would just continue to build coarser and coarser levels of the matrix, regardless of their geometric origin.) 

In the program here, we simply solve the coarse level problem with a Conjugate Gradient method without any preconditioner. That is acceptable if the coarse problem is really small -- for example, if the coarse mesh had a single cell, then the coarse mesh problems has a  [2.x.4790]  matrix in 2d, and a  [2.x.4791]  matrix in 3d; for the coarse mesh we use on the  [2.x.4792] -shaped domain of the current program, these sizes are  [2.x.4793]  in 2d and  [2.x.4794]  in 3d. But if the coarse mesh consists of hundreds or thousands of cells, this approach will no longer work and might start to dominate the overall run-time of each V-cyle. A common approach is then to solve the coarse mesh problem using an algebraic multigrid preconditioner; this would then, however, require assembling the coarse matrix (even for the matrix-free version) as input to the AMG implementation. 


examples/step-51/doc/intro.dox 

 [2.x.4795]  

[1.x.1899] 

[1.x.1900] 

[1.x.1901] 

This tutorial program presents the implementation of a hybridizable discontinuous Galkerin method for the convection-diffusion equation. 

[1.x.1902] 

One common argument against the use of discontinuous Galerkin elements is the large number of globally coupled degrees of freedom that one must solve in an implicit system.  This is because, unlike continuous finite elements, in typical discontinuous elements there is one degree of freedom at each vertex [1.x.1903], rather than just one, and similarly for edges and faces.  As an example of how fast the number of unknowns grows, consider the FE_DGPMonomial basis: each scalar solution component is represented by polynomials of degree  [2.x.4796]  with  [2.x.4797]  degrees of freedom per element. Typically, all degrees of freedom in an element are coupled to all of the degrees of freedom in the adjacent elements.  The resulting discrete equations yield very large linear systems very quickly, especially for systems of equations in 2 or 3 dimensions. 

[1.x.1904] To alleviate the computational cost of solving such large linear systems, the hybridizable discontinuous Galerkin (HDG) methodology was introduced by Cockburn and co-workers (see the references in the recent HDG overview article by Nguyen and Peraire  [2.x.4798] ). 

The HDG method achieves this goal by formulating the mathematical problem using Dirichlet-to-Neumann mappings.  The partial differential equations are first written as a first order system, and each field is then discretized via a DG method.  At this point, the single-valued "trace" values on the skeleton of the mesh, i.e., element faces, are taken to be independent unknown quantities. This yields unknowns in the discrete formulation that fall into two categories: 

- Face unknowns that only couple with the cell unknowns from both sides of the face; 

- Cell unknowns that only couple with the cell and face unknowns   defined within the same cell. Crucially, no cell interior degree of freedom   on one cell ever couples to any interior cell degree of freedom of a   different cell. 

The Dirichlet-to-Neumann map concept then permits the following solution procedure: <ol>    [2.x.4799]   Use local element interior data to enforce a Neumann condition on the skeleton of the triangulation.  The global problem is then to solve for the trace values, which are the only globally coupled unknowns.    [2.x.4800]   Use the known skeleton values as Dirichlet data for solving local element-level solutions.  This is known as the 'local solver', and is an [1.x.1905] element-by-element solution process.  [2.x.4801]  

[1.x.1906] The above procedure also has a linear algebra interpretation---referred to as [1.x.1907]---that was exploited to reduce the size of the global linear system by Guyan in the context of continuous Finite Elements  [2.x.4802] , and by Fraeijs de Veubeke for mixed methods  [2.x.4803] . In the latter case (mixed formulation), the system reduction was achieved through the use of discontinuous fluxes combined with the introduction of an additional auxiliary [1.x.1908] variable that approximates the trace of the unknown at the boundary of every element. This procedure became known as hybridization and---by analogy---is the reason why the local discontinuous Galerkin method introduced by Cockburn, Gopalakrishnan, and Lazarov in 2009  [2.x.4804] , and subsequently developed by their collaborators, eventually came to be known as the [1.x.1909] (HDG) method. 

Let us write the complete linear system associated to the HDG problem as a block system with the discrete DG (cell interior) variables  [2.x.4805]  as first block and the skeleton (face) variables  [2.x.4806]  as the second block: 

[1.x.1910] 

Our aim is now to eliminate the  [2.x.4807]  block with a Schur complement approach similar to step-20, which results in the following two steps: 

[1.x.1911] 

The point is that the presence of  [2.x.4808]  is not a problem because  [2.x.4809]  is a block diagonal matrix where each block corresponds to one cell and is therefore easy enough to invert. The coupling to other cells is introduced by the matrices  [2.x.4810]  and  [2.x.4811]  over the skeleton variable. The block-diagonality of  [2.x.4812]  and the structure in  [2.x.4813]  and  [2.x.4814]  allow us to invert the matrix  [2.x.4815]  element by element (the local solution of the Dirichlet problem) and subtract  [2.x.4816]  from  [2.x.4817] . The steps in the Dirichlet-to-Neumann map concept hence correspond to <ol>    [2.x.4818]  constructing the Schur complement matrix  [2.x.4819]  and right hand     side  [2.x.4820]   [1.x.1912]     and inserting the contribution into the global trace matrix in the usual way,    [2.x.4821]  solving the Schur complement system for  [2.x.4822] , and    [2.x.4823]  solving for  [2.x.4824]  using the second equation, given  [2.x.4825] .  [2.x.4826]  




[1.x.1913] Another criticism of traditional DG methods is that the approximate fluxes converge suboptimally.  The local HDG solutions can be shown to converge as  [2.x.4827] , i.e., at optimal order.  Additionally, a super-convergence property can be used to post-process a new approximate solution that converges at the rate  [2.x.4828] . 




[1.x.1914] 

The hybridizable discontinuous Galerkin method is only one way in which the problems of the discontinuous Galerkin method can be addressed. Another idea is what is called the "weak Galerkin" method. It is explored in step-61. 




[1.x.1915] 

The HDG formulation used for this example is taken from  [2.x.4829]  [1.x.1916][1.x.1917][1.x.1918] 

We consider the convection-diffusion equation over the domain  [2.x.4830]  with Dirichlet boundary  [2.x.4831]  and Neumann boundary  [2.x.4832] : 

[1.x.1919] 



Introduce the auxiliary variable  [2.x.4833]  and rewrite the above equation as the first order system: 

[1.x.1920] 



We multiply these equations by the weight functions  [2.x.4834]  and integrate by parts over every element  [2.x.4835]  to obtain: 

[1.x.1921] 



The terms decorated with a hat denote the numerical traces (also commonly referred to as numerical fluxes).  They are approximations to the interior values on the boundary of the element.  To ensure conservation, these terms must be single-valued on any given element edge  [2.x.4836]  even though, with discontinuous shape functions, there may of course be multiple values coming from the cells adjacent to an interface. We eliminate the numerical trace  [2.x.4837]  by using traces of the form: 

[1.x.1922] 



The variable  [2.x.4838]  is introduced as an additional independent variable and is the one for which we finally set up a globally coupled linear system. As mentioned above, it is defined on the element faces and discontinuous from one face to another wherever faces meet (at vertices in 2d, and at edges and vertices in 3d). Values for  [2.x.4839]  and  [2.x.4840]  appearing in the numerical trace function are taken to be the cell's interior solution restricted to the boundary  [2.x.4841] . 

The local stabilization parameter  [2.x.4842]  has effects on stability and accuracy of HDG solutions; see the literature for a further discussion. A stabilization parameter of unity is reported to be the choice which gives best results. A stabilization parameter  [2.x.4843]  that tends to infinity prohibits jumps in the solution over the element boundaries, making the HDG solution approach the approximation with continuous finite elements. In the program below, we choose the stabilization parameter as 

[1.x.1923] 

where we set the diffusion  [2.x.4844]  and the diffusion length scale to  [2.x.4845] . 

The trace/skeleton variables in HDG methods are single-valued on element faces.  As such, they must strongly represent the Dirichlet data on  [2.x.4846] .  This means that 

[1.x.1924] 

where the equal sign actually means an  [2.x.4847]  projection of the boundary function  [2.x.4848]  onto the space of the face variables (e.g. linear functions on the faces). This constraint is then applied to the skeleton variable  [2.x.4849]  using inhomogeneous constraints by the method  [2.x.4850]  

Summing the elemental contributions across all elements in the triangulation, enforcing the normal component of the numerical flux, and integrating by parts on the equation weighted by  [2.x.4851] , we arrive at the final form of the problem: Find  [2.x.4852]  such that 

[1.x.1925] 



The unknowns  [2.x.4853]  are referred to as local variables; they are represented as standard DG variables.  The unknown  [2.x.4854]  is the skeleton variable which has support on the codimension-1 surfaces (faces) of the mesh. 

We use the notation  [2.x.4855]  to denote the sum of integrals over all cells and  [2.x.4856]  to denote integration over all faces of all cells, i.e., interior faces are visited twice, once from each side and with the corresponding normal vectors. When combining the contribution from both elements sharing a face, the above equation yields terms familiar from the DG method, with jumps of the solution over the cell boundaries. 

In the equation above, the space  [2.x.4857]  for the scalar variable  [2.x.4858]  is defined as the space of functions that are tensor product polynomials of degree  [2.x.4859]  on each cell and discontinuous over the element boundaries  [2.x.4860] , i.e., the space described by  [2.x.4861] . The space for the gradient or flux variable  [2.x.4862]  is a vector element space where each component is a locally polynomial and discontinuous  [2.x.4863] . In the code below, we collect these two local parts together in one FESystem where the first  [2.x.4864]  dim components denote the gradient part and the last scalar component corresponds to the scalar variable. For the skeleton component  [2.x.4865] , we define a space that consists of discontinuous tensor product polynomials that live on the element faces, which in deal.II is implemented by the class FE_FaceQ. This space is otherwise similar to FE_DGQ, i.e., the solution function is not continuous between two neighboring faces, see also the results section below for an illustration. 

In the weak form given above, we can note the following coupling patterns: <ol>    [2.x.4866]  The matrix  [2.x.4867]  consists of local-local coupling terms.  These arise when the   local weighting functions  [2.x.4868]  multiply the local solution terms    [2.x.4869] . Because the elements are discontinuous,  [2.x.4870]    is block diagonal.    [2.x.4871]  The matrix  [2.x.4872]  represents the local-face coupling.  These are the terms   with weighting functions  [2.x.4873]  multiplying the skeleton variable    [2.x.4874] .    [2.x.4875]  The matrix  [2.x.4876]  represents the face-local coupling, which involves the   weighting function  [2.x.4877]  multiplying the local solutions  [2.x.4878] .    [2.x.4879]   The matrix  [2.x.4880]  is the face-face coupling;   terms involve both  [2.x.4881]  and  [2.x.4882] .  [2.x.4883]  

[1.x.1926] 

One special feature of the HDG methods is that they typically allow for constructing an enriched solution that gains accuracy. This post-processing takes the HDG solution in an element-by-element fashion and combines it such that one can get  [2.x.4884]  order of accuracy when using polynomials of degree  [2.x.4885] . For this to happen, there are two necessary ingredients: <ol>    [2.x.4886]  The computed solution gradient  [2.x.4887]  converges at optimal rate,    i.e.,  [2.x.4888] .    [2.x.4889]  The cell-wise average of the scalar part of the solution,     [2.x.4890] , super-converges at rate     [2.x.4891] .  [2.x.4892]  

We now introduce a new variable  [2.x.4893] , which we find by minimizing the expression  [2.x.4894]  over the cell  [2.x.4895]  under the constraint  [2.x.4896] . The constraint is necessary because the minimization functional does not determine the constant part of  [2.x.4897] . This translates to the following system of equations: 

[1.x.1927] 



Since we test by the whole set of basis functions in the space of tensor product polynomials of degree  [2.x.4898]  in the second set of equations, this is an overdetermined system with one more equation than unknowns. We fix this in the code below by omitting one of these equations (since the rows in the Laplacian are linearly dependent when representing a constant function). As we will see below, this form of the post-processing gives the desired super-convergence result with rate  [2.x.4899] .  It should be noted that there is some freedom in constructing  [2.x.4900]  and this minimization approach to extract the information from the gradient is not the only one. In particular, the post-processed solution defined here does not satisfy the convection-diffusion equation in any sense. As an alternative, the paper by Nguyen, Peraire and Cockburn cited above suggests another somewhat more involved formula for convection-diffusion that can also post-process the flux variable into an  [2.x.4901] -conforming variant and better represents the local convection-diffusion operator when the diffusion is small. We leave the implementation of a more sophisticated post-processing as a possible extension to the interested reader. 

Note that for vector-valued problems, the post-processing works similarly. One simply sets the constraint for the mean value of each vector component separately and uses the gradient as the main source of information. 

[1.x.1928] 

For this tutorial program, we consider almost the same test case as in step-7. The computational domain is  [2.x.4902]  and the exact solution corresponds to the one in step-7, except for a scaling. We use the following source centers  [2.x.4903]  for the exponentials  [2.x.4904]     [2.x.4905]  1D:   [2.x.4906] ,    [2.x.4907]  2D:  [2.x.4908] ,    [2.x.4909]  3D:  [2.x.4910] .  [2.x.4911]  

With the exact solution given, we then choose the forcing on the right hand side and the Neumann boundary condition such that we obtain this solution (manufactured solution technique). In this example, we choose the diffusion equal to one and the convection as 

[1.x.1929] Note that the convection is divergence-free,  [2.x.4912] . 

[1.x.1930] 

Besides implementing the above equations, the implementation below provides the following features:  [2.x.4913]     [2.x.4914]  WorkStream to parallelize local solvers. Workstream has been presented   in detail in step-9.    [2.x.4915]  Reconstruct the local DG solution from the trace.    [2.x.4916]  Post-processing the solution for superconvergence.    [2.x.4917]  DataOutFaces for direct output of the global skeleton solution.  [2.x.4918]  


examples/step-51/doc/results.dox 



[1.x.1931] 

[1.x.1932] 

We first have a look at the output generated by the program when run in 2D. In the four images below, we show the solution for polynomial degree  [2.x.4919]  and cycles 2, 3, 4, and 8 of the program. In the plots, we overlay the data generated from the internal data (DG part) with the skeleton part ( [2.x.4920] ) into the same plot. We had to generate two different data sets because cells and faces represent different geometric entities, the combination of which (in the same file) is not supported in the VTK output of deal.II. 

The images show the distinctive features of HDG: The cell solution (colored surfaces) is discontinuous between the cells. The solution on the skeleton variable sits on the faces and ties together the local parts. The skeleton solution is not continuous on the vertices where the faces meet, even though its values are quite close along lines in the same coordinate direction. The skeleton solution can be interpreted as a rubber spring between the two sides that balances the jumps in the solution (or rather, the flux  [2.x.4921] ). From the picture at the top left, it is clear that the bulk solution frequently over- and undershoots and that the skeleton variable in indeed a better approximation to the exact solution; this explains why we can get a better solution using a postprocessing step. 

As the mesh is refined, the jumps between the cells get small (we represent a smooth solution), and the skeleton solution approaches the interior parts. For cycle 8, there is no visible difference in the two variables. We also see how boundary conditions are implemented weakly and that the interior variables do not exactly satisfy boundary conditions. On the lower and left boundaries, we set Neumann boundary conditions, whereas we set Dirichlet conditions on the right and top boundaries. 

 [2.x.4922]  

Next, we have a look at the post-processed solution, again at cycles 2, 3, 4, and 8. This is a discontinuous solution that is locally described by second order polynomials. While the solution does not look very good on the mesh of cycle two, it looks much better for cycles three and four. As shown by the convergence table below, we find that is also converges more quickly to the analytical solution. 

 [2.x.4923]  

Finally, we look at the solution for  [2.x.4924]  at cycle 2. Despite the coarse mesh with only 64 cells, the post-processed solution is similar in quality to the linear solution (not post-processed) at cycle 8 with 4,096 cells. This clearly shows the superiority of high order methods for smooth solutions. 

 [2.x.4925]  

[1.x.1933] 

When the program is run, it also outputs information about the respective steps and convergence tables with errors in the various components in the end. In 2D, the convergence tables look the following: 

[1.x.1934] 




One can see the error reduction upon grid refinement, and for the cases where global refinement was performed, also the convergence rates. The quadratic convergence rates of Q1 elements in the  [2.x.4926]  norm for both the scalar variable and the gradient variable is apparent, as is the cubic rate for the postprocessed scalar variable in the  [2.x.4927]  norm. Note this distinctive feature of an HDG solution. In typical continuous finite elements, the gradient of the solution of order  [2.x.4928]  converges at rate  [2.x.4929]  only, as opposed to  [2.x.4930]  for the actual solution. Even though superconvergence results for finite elements are also available (e.g. superconvergent patch recovery first introduced by Zienkiewicz and Zhu), these are typically limited to structured meshes and other special cases. For Q3 HDG variables, the scalar variable and gradient converge at fourth order and the postprocessed scalar variable at fifth order. 

The same convergence rates are observed in 3d. 

[1.x.1935] 



[1.x.1936] 

[1.x.1937] 

The convergence tables verify the expected convergence rates stated in the introduction. Now, we want to show a quick comparison of the computational efficiency of the HDG method compared to a usual finite element (continuous Galkerin) method on the problem of this tutorial. Of course, stability aspects of the HDG method compared to continuous finite elements for transport-dominated problems are also important in practice, which is an aspect not seen on a problem with smooth analytic solution. In the picture below, we compare the  [2.x.4931]  error as a function of the number of degrees of freedom (left) and of the computing time spent in the linear solver (right) for two space dimensions of continuous finite elements (CG) and the hybridized discontinuous Galerkin method presented in this tutorial. As opposed to the tutorial where we only use unpreconditioned BiCGStab, the times shown in the figures below use the Trilinos algebraic multigrid preconditioner in  [2.x.4932]  For the HDG part, a wrapper around ChunkSparseMatrix for the trace variable has been used in order to utilize the block structure in the matrix on the finest level. 

 [2.x.4933]  

The results in the graphs show that the HDG method is slower than continuous finite elements at  [2.x.4934] , about equally fast for cubic elements and faster for sixth order elements. However, we have seen above that the HDG method actually produces solutions which are more accurate than what is represented in the original variables. Therefore, in the next two plots below we instead display the error of the post-processed solution for HDG (denoted by  [2.x.4935]  for example). We now see a clear advantage of HDG for the same amount of work for both  [2.x.4936]  and  [2.x.4937] , and about the same quality for  [2.x.4938] . 

 [2.x.4939]  

Since the HDG method actually produces results converging as  [2.x.4940] , we should compare it to a continuous Galerkin solution with the same asymptotic convergence behavior, i.e., FE_Q with degree  [2.x.4941] . If we do this, we get the convergence curves below. We see that CG with second order polynomials is again clearly better than HDG with linears. However, the advantage of HDG for higher orders remains. 

 [2.x.4942]  

The results are in line with properties of DG methods in general: Best performance is typically not achieved for linear elements, but rather at somewhat higher order, usually around  [2.x.4943] . This is because of a volume-to-surface effect for discontinuous solutions with too much of the solution living on the surfaces and hence duplicating work when the elements are linear. Put in other words, DG methods are often most efficient when used at relatively high order, despite their focus on a discontinuous (and hence, seemingly low accurate) representation of solutions. 

[1.x.1938] 

We now show the same figures in 3D: The first row shows the number of degrees of freedom and computing time versus the  [2.x.4944]  error in the scalar variable  [2.x.4945]  for CG and HDG at order  [2.x.4946] , the second row shows the post-processed HDG solution instead of the original one, and the third row compares the post-processed HDG solution with CG at order  [2.x.4947] . In 3D, the volume-to-surface effect makes the cost of HDG somewhat higher and the CG solution is clearly better than HDG for linears by any metric. For cubics, HDG and CG are of similar quality, whereas HDG is again more efficient for sixth order polynomials. One can alternatively also use the combination of FE_DGP and FE_FaceP instead of (FE_DGQ, FE_FaceQ), which do not use tensor product polynomials of degree  [2.x.4948]  but Legendre polynomials of [1.x.1939] degree  [2.x.4949] . There are fewer degrees of freedom on the skeleton variable for FE_FaceP for a given mesh size, but the solution quality (error vs. number of DoFs) is very similar to the results for FE_FaceQ. 

 [2.x.4950]  

One final note on the efficiency comparison: We tried to use general-purpose sparse matrix structures and similar solvers (optimal AMG preconditioners for both without particular tuning of the AMG parameters on any of them) to give a fair picture of the cost versus accuracy of two methods, on a toy example. It should be noted however that geometric multigrid (GMG) for continuous finite elements is about a factor four to five faster for  [2.x.4951]  and  [2.x.4952] . As of 2019, optimal-complexity iterative solvers for HDG are still under development in the research community. Also, there are other implementation aspects for CG available such as fast matrix-free approaches as shown in step-37 that make higher order continuous elements more competitive. Again, it is not clear to the authors of the tutorial whether similar improvements could be made for HDG. We refer to [1.x.1940] for a recent efficiency evaluation. 




[1.x.1941] 

As already mentioned in the introduction, one possibility is to implement another post-processing technique as discussed in the literature. 

A second item that is not done optimally relates to the performance of this program, which is of course an issue in practical applications (weighing in also the better solution quality of (H)DG methods for transport-dominated problems). Let us look at the computing time of the tutorial program and the share of the individual components: 

 [2.x.4953]  

As can be seen from the table, the solver and assembly calls dominate the runtime of the program. This also gives a clear indication of where improvements would make the most sense: 

<ol>    [2.x.4954]  Better linear solvers: We use a BiCGStab iterative solver without   preconditioner, where the number of iteration increases with increasing   problem size (the number of iterations for Q1 elements and global   refinements starts at 35 for the small sizes but increase up to 701 for the   largest size). To do better, one could for example use an algebraic   multigrid preconditioner from Trilinos, or some more advanced variants as   the one discussed in [1.x.1942]. For diffusion-dominated problems such as the problem at hand   with finer meshes, such a solver can be designed that uses the matrix-vector   products from the more efficient ChunkSparseMatrix on the finest level, as   long as we are not working in parallel with MPI. For MPI-parallelized   computations, a standard  [2.x.4955]  can be used. 

   [2.x.4956]  Speed up assembly by pre-assembling parts that do not change from one   cell to another (those that do neither contain variable coefficients nor   mapping-dependent terms).  [2.x.4957]  


examples/step-52/doc/intro.dox 

 [2.x.4958]  

[1.x.1943] 

 [2.x.4959]  In order to run this program, deal.II must be configured to use the UMFPACK sparse direct solver. Refer to the [1.x.1944] for instructions how to do this. 

[1.x.1945] 

[1.x.1946] 

This program shows how to use Runge-Kutta methods to solve a time-dependent problem. It solves a small variation of the heat equation discussed first in step-26 but, since the purpose of this program is only to demonstrate using more advanced ways to interface with deal.II's time stepping algorithms, only solves a simple problem on a uniformly refined mesh. 




[1.x.1947] 

In this example, we solve the one-group time-dependent diffusion approximation of the neutron transport equation (see step-28 for the time-independent multigroup diffusion). This is a model for how neutrons move around highly scattering media, and consequently it is a variant of the time-dependent diffusion equation -- which is just a different name for the heat equation discussed in step-26, plus some extra terms. We assume that the medium is not fissible and therefore, the neutron flux satisfies the following equation: 

[1.x.1948] 

augmented by appropriate boundary conditions. Here,  [2.x.4960]  is the velocity of neutrons (for simplicity we assume it is equal to 1 which can be achieved by simply scaling the time variable),  [2.x.4961]  is the diffusion coefficient,  [2.x.4962]  is the absorption cross section, and  [2.x.4963]  is a source. Because we are only interested in the time dependence, we assume that  [2.x.4964]  and  [2.x.4965]  are constant. 

Since this program only intends to demonstrate how to use advanced time stepping algorithms, we will only look for the solutions of relatively simple problems. Specifically, we are looking for a solution on a square domain  [2.x.4966]  of the form 

[1.x.1949] 

By using quadratic finite elements, we can represent this function exactly at any particular time, and all the error will be due to the time discretization. We do this because it is then easy to observe the order of convergence of the various time stepping schemes we will consider, without having to separate spatial and temporal errors. 

We impose the following boundary conditions: homogeneous Dirichlet for  [2.x.4967]  and  [2.x.4968]  and homogeneous Neumann conditions for  [2.x.4969]  and  [2.x.4970] . We choose the source term so that the corresponding solution is in fact of the form stated above: 

[1.x.1950] 

Because the solution is a sine in time, we know that the exact solution satisfies  [2.x.4971] . Therefore, the error at time  [2.x.4972]  is simply the norm of the numerical solution, i.e.,  [2.x.4973] , and is particularly easily evaluated. In the code, we evaluate the  [2.x.4974]  norm of the vector of nodal values of  [2.x.4975]  instead of the  [2.x.4976]  norm of the associated spatial function, since the former is simpler to compute; however, on uniform meshes, the two are just related by a constant and we can consequently observe the temporal convergence order with either. 




[1.x.1951] 

The Runge-Kutta methods implemented in deal.II assume that the equation to be solved can be written as: 

[1.x.1952] 

On the other hand, when using finite elements, discretized time derivatives always result in the presence of a mass matrix on the left hand side. This can easily be seen by considering that if the solution vector  [2.x.4977]  in the equation above is in fact the vector of nodal coefficients  [2.x.4978]  for a variable of the form 

[1.x.1953] 

with spatial shape functions  [2.x.4979] , then multiplying an equation of the form 

[1.x.1954] 

by test functions, integrating over  [2.x.4980] , substituting  [2.x.4981]  and restricting the test functions to the  [2.x.4982]  from above, then this spatially discretized equation has the form 

[1.x.1955] 

where  [2.x.4983]  is the mass matrix and  [2.x.4984]  is the spatially discretized version of  [2.x.4985]  (where  [2.x.4986]  is typically the place where spatial derivatives appear, but this is not of much concern for the moment given that we only consider time derivatives). In other words, this form fits the general scheme above if we write 

[1.x.1956] 



Runke-Kutta methods are time stepping schemes that approximate  [2.x.4987]  through a particular one-step approach. They are typically written in the form 

[1.x.1957] 

where for the form of the right hand side above 

[1.x.1958] 

Here  [2.x.4988] ,  [2.x.4989] , and  [2.x.4990]  are known coefficients that identify which particular Runge-Kutta scheme you want to use, and  [2.x.4991]  is the time step used. Different time stepping methods of the Runge-Kutta class differ in the number of stages  [2.x.4992]  and the values they use for the coefficients  [2.x.4993] ,  [2.x.4994] , and  [2.x.4995]  but are otherwise easy to implement since one can look up tabulated values for these coefficients. (These tables are often called Butcher tableaus.) 

At the time of the writing of this tutorial, the methods implemented in deal.II can be divided in three categories: <ol>  [2.x.4996]  Explicit Runge-Kutta; in order for a method to be explicit, it is necessary that in the formula above defining  [2.x.4997] ,  [2.x.4998]  does not appear on the right hand side. In other words, these methods have to satisfy  [2.x.4999] .  [2.x.5000]  Embedded (or adaptive) Runge-Kutta; we will discuss their properties below.  [2.x.5001]  Implicit Runge-Kutta; this class of methods require the solution of a possibly nonlinear system the stages  [2.x.5002]  above, i.e., they have  [2.x.5003]  for at least one of the stages  [2.x.5004] .  [2.x.5005]  Many well known time stepping schemes that one does not typically associate with the names Runge or Kutta can in fact be written in a way so that they, too, can be expressed in these categories. They oftentimes represent the lowest-order members of these families. 




[1.x.1959] 

These methods, only require a function to evaluate  [2.x.5006]  but not (as implicit methods) to solve an equation that involves  [2.x.5007]  for  [2.x.5008] . As all explicit time stepping methods, they become unstable when the time step chosen is too large. 

Well known methods in this class include forward Euler, third order Runge-Kutta, and fourth order Runge-Kutta (often abbreviated as RK4). 




[1.x.1960] 

These methods use both a lower and a higher order method to estimate the error and decide if the time step needs to be shortened or can be increased. The term "embedded" refers to the fact that the lower-order method does not require additional evaluates of the function  [2.x.5009]  but reuses data that has to be computed for the high order method anyway. It is, in other words, essentially free, and we get the error estimate as a side product of using the higher order method. 

This class of methods include Heun-Euler, Bogacki-Shampine, Dormand-Prince (ode45 in Matlab and often abbreviated as RK45 to indicate that the lower and higher order methods used here are 4th and 5th order Runge-Kutta methods, respectively), Fehlberg, and Cash-Karp. 

At the time of the writing, only embedded explicit methods have been implemented. 




[1.x.1961] 

Implicit methods require the solution of (possibly nonlinear) systems of the form  [2.x.5010]  for  [2.x.5011]  in each (sub-)timestep. Internally, this is done using a Newton-type method and, consequently, they require that the user provide functions that can evaluate  [2.x.5012]  and  [2.x.5013]  or equivalently  [2.x.5014] . 

The particular form of this operator results from the fact that each Newton step requires the solution of an equation of the form 

[1.x.1962] 

for some (given)  [2.x.5015] . Implicit methods are always stable, regardless of the time step size, but too large time steps of course affect the [1.x.1963] of the solution, even if the numerical solution remains stable and bounded. 

Methods in this class include backward Euler, implicit midpoint, Crank-Nicolson, and the two stage SDIRK method (short for "singly diagonally implicit Runge-Kutta", a term coined to indicate that the diagonal elements  [2.x.5016]  defining the time stepping method are all equal; this property allows for the Newton matrix  [2.x.5017]  to be re-used between stages because  [2.x.5018]  is the same every time). 




[1.x.1964] 

By expanding the solution of our model problem as always using shape functions  [2.x.5019]  and writing 

[1.x.1965] 

we immediately get the spatially discretized version of the diffusion equation as 

[1.x.1966] 

where 

[1.x.1967] 

See also step-24 and step-26 to understand how we arrive here. Boundary terms are not necessary due to the chosen boundary conditions for the current problem. To use the Runge-Kutta methods, we recast this as follows: 

[1.x.1968] 

In the code, we will need to be able to evaluate this function  [2.x.5020]  along with its derivative, 

[1.x.1969] 






[1.x.1970] 

To simplify the problem, the domain is two dimensional and the mesh is uniformly refined (there is no need to adapt the mesh since we use quadratic finite elements and the exact solution is quadratic). Going from a two dimensional domain to a three dimensional domain is not very challenging. However if you intend to solve more complex problems where the mesh must be adapted (as is done, for example, in step-26), then it is important to remember the following issues: 

<ol>  [2.x.5021]  You will need to project the solution to the new mesh when the mesh is changed. Of course,      the mesh      used should be the same from the beginning to the end of each time step,      a question that arises because Runge-Kutta methods use multiple      evaluations of the equations within each time step.  [2.x.5022]  You will need to update the mass matrix and its inverse every time the      mesh is changed.  [2.x.5023]  The techniques for these steps are readily available by looking at step-26. 


examples/step-52/doc/results.dox 



[1.x.1971] 

The point of this program is less to show particular results, but instead to show how it is done. This we have already demonstrated simply by discussing the code above. Consequently, the output the program yields is relatively sparse and consists only of the console output and the solutions given in VTU format for visualization. 

The console output contains both errors and, for some of the methods, the number of steps they performed: 

[1.x.1972] 



As expected the higher order methods give (much) more accurate solutions. We also see that the (rather inaccurate) Heun-Euler method increased the number of time steps in order to satisfy the tolerance. On the other hand, the other embedded methods used a lot less time steps than what was prescribed. 


examples/step-53/doc/intro.dox 

 [2.x.5024]  

[1.x.1973] 

 [2.x.5025]  This program elaborates on concepts of geometry and the classes that implement it. These classes are grouped into the documentation module on  [2.x.5026]  "Manifold description for triangulations". See there for additional information. 

 [2.x.5027]  This tutorial is also available as a Jupyter Python notebook that   uses the deal.II python interface. The notebook is available in the   same directory as the original C++ program. Rendered notebook can also   be viewed on the [1.x.1974]. 


[1.x.1975] 

[1.x.1976] 

Partial differential equations for realistic problems are often posed on domains with complicated geometries. To provide just a few examples, consider these cases: 

- Among the two arguably most important industrial applications for the finite   element method, aerodynamics and more generally fluid dynamics is   one. Computer simulations today are used in the design of every airplane,   car, train and ship. The domain in which the partial differential equation   is posed is, in these cases, the air surrounding the plane with its wings,   flaps and engines; the air surrounding the car with its wheel, wheel wells,   mirrors and, in the case of race cars, all sorts of aerodynamic equipment;   the air surrounding the train with its wheels and gaps between cars. In the   case of ships, the domain is the water surrounding the ship with its rudders   and propellers. 

- The other of the two big applications of the finite element method is   structural engineering in which the domains are bridges, airplane nacelles   and wings, and other solid bodies of often complicated shapes. 

- Finite element modeling is also often used to describe the generation and   propagation of earthquake waves. In these cases, one needs to accurately   represent the geometry of faults in the Earth crust. Since faults intersect,   dip at angles, and are often not completely straight, domains are frequently   very complex. One could cite many more examples of complicated geometries in which one wants to pose and solve a partial differential equation. What this shows is that the "real" world is much more complicated than what we have shown in almost all of the tutorial programs preceding this one. 

This program is therefore devoted to showing how one deals with complex geometries using a concrete application. In particular, what it shows is how we make a mesh fit the domain we want to solve on. On the other hand, what the program does not show is how to create a coarse for a domain. The process to arrive at a coarse mesh is called "mesh generation" and there are a number of high-quality programs that do this much better than we could ever implement. However, deal.II does have the ability to read in meshes in many formats generated by mesh generators and then make them fit a given shape, either by deforming a mesh or refining it a number of times until it fits. The deal.II Frequently Asked Questions page referenced from http://www.dealii.org/ provides resources to mesh generators. 




[1.x.1977] 

Let us assume that you have a complex domain and that you already have a coarse mesh that somehow represents the general features of the domain. Then there are two situations in which it is necessary to describe to a deal.II program the details of your geometry: 

- Mesh refinement: Whenever a cell is refined, it is necessary to introduce   new vertices in the Triangulation. In the simplest case, one assumes that   the objects that make up the Triangulation are straight line segments, a   bi-linear surface or a tri-linear volume. The next vertex is then simply put   into the middle of the old ones. However, for curved boundaries or if we   want to solve a PDE on a curved, lower-dimensional manifold embedded in a   higher-dimensional space, this is insufficient since it will not respect the   actual geometry. We will therefore have to tell Triangulation where to put   new points. 

- Integration: When using higher order finite element methods, it is often   necessary to compute integrals using curved approximations of the boundary,   i.e., describe each edge or face of cells as curves, instead of straight   line segments or bilinear patches. The same is, of course, true when   integrating boundary terms (e.g., inhomogeneous Neumann boundary   conditions). For the purpose of integration, the various Mapping classes   then provide the transformation from the reference cell to the actual cell. 

In both cases, we need a way to provide information about the geometry of the domain at the level of an individual cell, its faces and edges. This is where the Manifold class comes into play. Manifold is an abstract base class that only defines an interface by which the Triangulation and Mapping classes can query geometric information about the domain. Conceptually, Manifold sees the world in a way not dissimilar to how the mathematical subdiscipline geometry sees it: a domain is essentially just a collection of points that is somehow equipped with the notion of a distance between points so that we can obtain a point "in the middle" of some other points. 

deal.II provides a number of classes that implement the interface provided by Manifold for a variety of common geometries. On the other hand, in this program we will consider only a very common and much simpler case, namely the situation where (a part of) the domain we want to solve on can be described by transforming a much simpler domain (we will call this the "reference domain"). In the language of mathematics, this means that the (part of the) domain is a [1.x.1978]. Charts are described by a smooth function that maps from the simpler domain to the chart (the "push-forward" function) and its inverse (the "pull-back" function). If the domain as a whole is not a chart (e.g., the surface of a sphere), then it can often be described as a collection of charts (e.g., the northern hemisphere and the southern hemisphere are each charts) and the domain can then be describe by an [1.x.1979]. 

If a domain can be decomposed into an atlas, all we need to do is provide the pull-back and push-forward functions for each of the charts. In deal.II, this means providing a class derived from ChartManifold, and this is precisely what we will do in this program. 




[1.x.1980] 

To illustrate how one describes geometries using charts in deal.II, we will consider a case that originates in an application of the [1.x.1981], using a data set provided by D. Sarah Stamps. In the concrete application, we were interested in describing flow in the Earth mantle under the [1.x.1982], a zone where two continental plates drift apart. Not to beat around the bush, the geometry we want to describe looks like this: 

 [2.x.5028]  

In particular, though you cannot see this here, the top surface is not just colored by the elevation but is, in fact, deformed to follow the correct topography. While the actual application is not relevant here, the geometry is. The domain we are interested in is a part of the Earth that ranges from the surface to a depth of 500km, from 26 to 35 degrees East of the Greenwich meridian, and from 5 degrees North of the equator to 10 degrees South. 

This description of the geometry suggests to start with a box  [2.x.5029]  (measured in degrees, degrees, and meters) and to provide a map  [2.x.5030]  so that  [2.x.5031]  where  [2.x.5032]  is the domain we seek.  [2.x.5033]  is then a chart,  [2.x.5034]  the pull-back operator, and  [2.x.5035]  the push-forward operator. If we need a point  [2.x.5036]  that is the "average" of other points  [2.x.5037] , the ChartManifold class then first applies the pull-back to obtain  [2.x.5038] , averages these to a point  [2.x.5039]  and then computes  [2.x.5040] . 

Our goal here is therefore to implement a class that describes  [2.x.5041]  and  [2.x.5042] . If Earth was a sphere, then this would not be difficult: if we denote by  [2.x.5043]  the points of  [2.x.5044]  (i.e., longitude counted eastward, latitude counted northward, and elevation relative to zero depth), then 

[1.x.1983] 

provides coordinates in a Cartesian coordinate system, where  [2.x.5045]  is the radius of the sphere. However, the Earth is not a sphere: 

<ol>  [2.x.5046]  It is flattened at the poles and larger at the equator: the semi-major axis   is approximately 22km longer than the semi-minor axis. We will account for   this using the [1.x.1984]   reference standard for the Earth shape. The formula used in WGS 84 to obtain   a position in Cartesian coordinates from longitude, latitude, and elevation   is 

[1.x.1985] 

  where  [2.x.5047] , and radius and   ellipticity are given by  [2.x.5048] . In this formula,   we assume that the arguments to sines and cosines are evaluated in degree, not   radians (though we will have to change this assumption in the code). 

 [2.x.5049]  It has topography in the form of mountains and valleys. We will account for   this using real topography data (see below for a description of where   this data comes from). Using this data set, we can look up elevations on a   latitude-longitude mesh laid over the surface of the Earth. Starting with   the box  [2.x.5050] , we will therefore   first stretch it in vertical direction before handing it off to the WGS 84   function: if  [2.x.5051]  is the height at longitude  [2.x.5052]    and latitude  [2.x.5053] , then we define 

[1.x.1986] 

  Using this function, the top surface of the box  [2.x.5054]  is displaced to the   correct topography, the bottom surface remains where it was, and points in   between are linearly interpolated.  [2.x.5055]  

Using these two functions, we can then define the entire push-forward function  [2.x.5056]  as 

[1.x.1987] 

In addition, we will have to define the inverse of this function, the pull-back operation, which we can write as 

[1.x.1988] 

We can obtain one of the components of this function by inverting the formula above: 

[1.x.1989] 

Computing  [2.x.5057]  is also possible though a lot more awkward. We won't show the formula here but instead only provide the implementation in the program. 




[1.x.1990] 

There are a number of issues we need to address in the program. At the largest scale, we need to write a class that implements the interface of ChartManifold. This involves a function  [2.x.5058]  that takes a point in the reference domain  [2.x.5059]  and transform it into real space using the function  [2.x.5060]  outlined above, and its inverse function  [2.x.5061]  implementing  [2.x.5062] . We will do so in the  [2.x.5063]  class below that looks, in essence, like this: 

[1.x.1991] 



The transformations above have two parts: the WGS 84 transformations and the topography transformation. Consequently, the  [2.x.5064]  class will have additional (non-virtual) member functions  [2.x.5065]  and  [2.x.5066]  that implement these two pieces, and corresponding pull back functions. 

The WGS 84 transformation functions are not particularly interesting (even though the formulas they implement are impressive). The more interesting part is the topography transformation. Recall that for this, we needed to evaluate the elevation function  [2.x.5067] . There is of course no formula for this: Earth is what it is, the best one can do is look up the altitude from some table. This is, in fact what we will do. 

The data we use was originally created by the  [1.x.1992], was downloaded from the US Geologic Survey (USGS) and processed by D. Sarah Stamps who also wrote the initial version of the WGS 84 transformation functions. The topography data so processed is stored in a file  [2.x.5068]  that, when unpacked looks like this: 

[1.x.1993] 

The data is formatted as  [2.x.5069]  where the first two columns are provided in degrees North of the equator and degrees East of the Greenwich meridian. The final column is given in meters above the WGS 84 zero elevation. 

In the transformation functions, we need to evaluate  [2.x.5070]  for a given longitude  [2.x.5071]  and latitude  [2.x.5072] . In general, this data point will not be available and we will have to interpolate between adjacent data points. Writing such an interpolation routine is not particularly difficult, but it is a bit tedious and error prone. Fortunately, we can somehow shoehorn this data set into an existing class:  [2.x.5073]  . Unfortunately, the class does not fit the bill quite exactly and so we need to work around it a bit. The problem comes from the way we initialize this class: in its simplest form, it takes a stream of values that it assumes form an equispaced mesh in the  [2.x.5074]  plane (or, here, the  [2.x.5075]  plane). Which is what they do here, sort of: they are ordered latitude first, longitude second; and more awkwardly, the first column starts at the largest values and counts down, rather than the usual other way around. 

Now, while tutorial programs are meant to illustrate how to code with deal.II, they do not necessarily have to satisfy the same quality standards as one would have to do with production codes. In a production code, we would write a function that reads the data and (i) automatically determines the extents of the first and second column, (ii) automatically determines the number of data points in each direction, (iii) does the interpolation regardless of the order in which data is arranged, if necessary by switching the order between reading and presenting it to the  [2.x.5076]  class. 

On the other hand, tutorial programs are best if they are short and demonstrate key points rather than dwell on unimportant aspects and, thereby, obscure what we really want to show. Consequently, we will allow ourselves a bit of leeway: 

- since this program is intended solely for a particular geometry around the area   of the East-African rift and since this is precisely the area described by the data   file, we will hardcode in the program that there are    [2.x.5077]  pieces of data; 

- we will hardcode the boundaries of the data    [2.x.5078] ; 

- we will lie to the  [2.x.5079]  class: the class will   only see the data in the last column of this data file, and we will pretend that   the data is arranged in a way that there are 1139 data points in the first   coordinate direction that are arranged in [1.x.1994] order but in an   interval  [2.x.5080]  (not the negated bounds). Then,   when we need to look something up for a latitude  [2.x.5081] , we can ask the   interpolating table class for a value at  [2.x.5082] . With this little   trick, we can avoid having to switch around the order of data as read from   file. 

All of this then calls for a class that essentially looks like this: 

[1.x.1995] 



Note how the  [2.x.5083]  function negates the latitude. It also switches from the format  [2.x.5084]  that we use everywhere else to the latitude-longitude format used in the table. Finally, it takes its arguments in radians as that is what we do everywhere else in the program, but then converts them to the degree-based system used for table lookup. As you will see in the implementation below, the function has a few more (static) member functions that we will call in the initialization of the  [2.x.5085]  member variable: the class type of this variable has a constructor that allows us to set everything right at construction time, rather than having to fill data later on, but this constructor takes a number of objects that can't be constructed in-place (at least not in C++98). Consequently, the construction of each of the objects we want to pass in the initialization happens in a number of static member functions. 

Having discussed the general outline of how we want to implement things, let us go to the program and show how it is done in practice. 


examples/step-53/doc/results.dox 



[1.x.1996] 

Running the program produces a mesh file  [2.x.5086]  that we can visualize with any of the usual visualization programs that can read the VTU file format. If one just looks at the mesh itself, it is actually very difficult to see anything that doesn't just look like a perfectly round piece of a sphere (though if one modified the program so that it does produce a sphere and looked at them at the same time, the difference between the overall sphere and WGS 84 shape is quite apparent). Apparently, Earth is actually quite a flat place. Of course we already know this from satellite pictures. However, we can tease out something more by coloring cells by their volume. This both produces slight variations in hue along the top surface and something for the visualization programs to apply their shading algorithms to (because the top surfaces of the cells are now no longer just tangential to a sphere but tilted): 

 [2.x.5087]  

Yet, at least as far as visualizations are concerned, this is still not too impressive. Rather, let us visualize things in a way so that we show the actual elevation along the top surface. In other words, we want a picture like this, with an incredible amount of detail: 

 [2.x.5088]  

A zoom-in of this picture shows the vertical displacement quite clearly (here, looking from the West-Northwest over the rift valley, the triple peaks of [1.x.1997], [1.x.1998], and [1.x.1999] in the [1.x.2000], [1.x.2001] and toward the great flatness of [1.x.2002]): 

 [2.x.5089]  


These image were produced with three small modifications: <ol>    [2.x.5090]  An additional seventh mesh refinement towards the top surface for the   first of these two pictures, and a total of nine for the second. In the   second image, the horizontal mesh size is approximately 1.5km, and just   under 1km in vertical direction. (The picture was also created using a   more resolved data set; however, it is too big to distribute as part of   the tutorial.) 

   [2.x.5091]  The addition of the following function that, given a point    [2.x.5092]  computes the elevation by converting the point to   reference WGS 84 coordinates and only keeping the depth variable (the   function is, consequently, a simplified version of the    [2.x.5093]  function): 

[1.x.2003] 



   [2.x.5094] Adding the following piece to the bottom of the  [2.x.5095]  function: 

[1.x.2004] 

 [2.x.5096]  This last piece of code first creates a  [2.x.5097]  finite element space on the mesh. It then (ab)uses  [2.x.5098]  to evaluate the elevation function for every node at the top boundary (the one with boundary indicator 5). We here wrap the call to  [2.x.5099]  with the ScalarFunctionFromFunctionObject class to make a regular C++ function look like an object of a class derived from the Function class that we want to use in  [2.x.5100]  Having so gotten a list of degrees of freedom located at the top boundary and corresponding elevation values, we just go down this list and set these elevations in the  [2.x.5101]  vector (leaving all interior degrees of freedom at their original zero value). This vector is then output using DataOut as usual and can be visualized as shown above. 




[1.x.2005] 

If you zoomed in on the mesh shown above and looked closely enough, you would find that at hanging nodes, the two small edges connecting to the hanging nodes are not in exactly the same location as the large edge of the neighboring cell. This can be shown more clearly by using a different surface description in which we enlarge the vertical topography to enhance the effect (courtesy of Alexander Grayver): 

 [2.x.5102]  

So what is happening here? Partly, this is only a result of visualization, but there is an underlying real cause as well: 

 [2.x.5103]     [2.x.5104] When you visualize a mesh using any of the common visualization   programs, what they really show you is just a set of edges that are plotted   as straight lines in three-dimensional space. This is so because almost all   data file formats for visualizing data only describe hexahedral cells as a   collection of eight vertices in 3d space, and do not allow to any more   complicated descriptions. (This is the main reason why    [2.x.5105]  takes an argument that can be set to something   larger than one.) These linear edges may be the edges of the cell you do   actual computations on, or they may not, depending on what kind of mapping   you use when you do your integrations using FEValues. By default, of course,   FEValues uses a linear mapping (i.e., an object of class MappingQ1) and in   that case a 3d cell is indeed described exclusively by its 8 vertices and   the volume it fills is a trilinear interpolation between these points,   resulting in linear edges. But, you could also have used tri-quadratic,   tri-cubic, or even higher order mappings and in these cases the volume of   each cell will be bounded by quadratic, cubic or higher order polynomial   curves. Yet, you only get to see these with linear edges in the   visualization program because, as mentioned, file formats do not allow to   describe the real geometry of cells. 

   [2.x.5106] That said, let us for simplicity assume that you are indeed using a   trilinear mapping, then the image shown above is a faithful representation   of the cells on which you form your integrals. In this case, indeed the   small cells at a hanging nodes do not, in general, snugly fit against the   large cell but leave a gap or may intersect the larger cell. Why is this?   Because when the triangulation needs a new vertex on an edge it wants to   refine, it asks the manifold description where this new vertex is supposed   to be, and the manifold description duly returns such a point by (in the   case of a geometry derived from ChartManifold) pulling the adjacent points   of the line back to the reference domain, averaging their locations, and   pushing forward this new location to the real domain. But this new location   is not usually along a straight line (in real space) between the adjacent   vertices and consequently the two small straight lines forming the refined   edge do not lie exactly on the one large straight line forming the unrefined   side of the hanging node.  [2.x.5107]  

The situation is slightly more complicated if you use a higher order mapping using the MappingQ class, but not fundamentally different. Let's take a quadratic mapping for the moment (nothing fundamental changes with even higher order mappings). Then you need to imagine each edge of the cells you integrate on as a quadratic curve despite the fact that you will never actually see it plotted that way by a visualization program. But imagine it that way for a second. So which quadratic curve does MappingQ take? It is the quadratic curve that goes through the two vertices at the end of the edge as well as a point in the middle that it queries from the manifold. In the case of the long edge on the unrefined side, that's of course exactly the location of the hanging node, so the quadratic curve describing the long edge does go through the hanging node, unlike in the case of the linear mapping. But the two small edges are also quadratic curves; for example, the left small edge will go through the left vertex of the long edge and the hanging node, plus a point it queries halfway in between from the manifold. Because, as before, the point the manifold returns halfway along the left small edge is rarely exactly on the quadratic curve describing the long edge, the quadratic short edge will typically not coincide with the left half of the quadratic long edge, and the same is true for the right short edge. In other words, again, the geometries of the large cell and its smaller neighbors at hanging nodes do not touch snuggly. 

This all begs two questions: first, does it matter, and second, could this be fixed. Let us discuss these in the following: 

 [2.x.5108]     [2.x.5109] Does it matter? It is almost certainly true that this depends on the   equation you are solving. For example, it is known that solving the Euler   equations of gas dynamics on complex geometries requires highly accurate   boundary descriptions to ensure convergence of quantities that are measure   the flow close to the boundary. On the other hand, equations with elliptic   components (e.g., the Laplace or Stokes equations) are typically rather   forgiving of these issues: one does quadrature anyway to approximate   integrals, and further approximating the geometry may not do as much harm as   one could fear given that the volume of the overlaps or gaps at every   hanging node is only  [2.x.5110]  even with a linear mapping and  [2.x.5111]  for a mapping of degree  [2.x.5112] . (You can see this by considering   that in 2d the gap/overlap is a triangle with base  [2.x.5113]  and height  [2.x.5114] ; in 3d, it is a pyramid-like structure with base area  [2.x.5115]  and   height  [2.x.5116] . Similar considerations apply for higher order mappings   where the height of the gaps/overlaps is  [2.x.5117] .) In other words,   if you use a linear mapping with linear elements, the error in the volume   you integrate over is already at the same level as the integration error   using the usual Gauss quadrature. Of course, for higher order elements one   would have to choose matching mapping objects. 

  Another point of view on why it is probably not worth worrying too much   about the issue is that there is certainly no narrative in the community of   numerical analysts that these issues are a major concern one needs to watch   out for when using complex geometries. If it does not seem to be discussed   often among practitioners, if ever at all, then it is at least not something   people have identified as a common problem. 

  This issue is not dissimilar to having hanging nodes at curved boundaries   where the geometry description of the boundary typically pulls a hanging   node onto the boundary whereas the large edge remains straight, making the   adjacent small and large cells not match each other. Although this behavior   existed in deal.II since its beginning, 15 years before manifold   descriptions became available, it did not ever come up in mailing list   discussions or conversations with colleagues. 

   [2.x.5118] Could it be fixed? In principle, yes, but it's a complicated   issue. Let's assume for the moment that we would only ever use the MappingQ1   class, i.e., linear mappings. In that case, whenever the triangulation class   requires a new vertex along an edge that would become a hanging node, it   would just take the mean value of the adjacent vertices [1.x.2006], i.e., without asking the manifold description. This way, the   point lies on the long straight edge and the two short straight edges would   match the one long edge. Only when all adjacent cells have been refined and   the point is no longer a hanging node would we replace its coordinates by   coordinates we get by a manifold. This may be awkward to implement, but it   would certainly be possible. 

  The more complicated issue arises because people may want to use a higher   order MappingQ object. In that case, the Triangulation class may freely   choose the location of the hanging node (because the quadratic curve for the   long edge can be chosen in such a way that it goes through the hanging node)   but the MappingQ class, when determining the location of mid-edge points   must make sure that if the edge is one half of a long edge of a neighboring   coarser cell, then the midpoint cannot be obtained from the manifold but   must be chosen along the long quadratic edge. For cubic (and all other odd)   mappings, the matter is again a bit complicated because one typically   arranges the cubic edge to go through points 1/3 and 2/3 along the edge, and   thus necessarily through the hanging node, but this could probably be worked   out. In any case, even then, there are two problems with this: 

  - When refining the triangulation, the Triangulation class can not know what     mapping will be used. In fact it is not uncommon for a triangulation to be     used differently in different contexts within the same program. If the     mapping used determines whether we can freely choose a point or not, how,     then, should the triangulation locate new vertices? 

  - Mappings are purely local constructs: they only work on a cell in     isolation, and this is one of the important features of the finite element     method. Having to ask whether one of the vertices of an edge is a hanging     node requires querying the neighborhood of a cell; furthermore, such a     query does not just involve the 6 face neighbors of a cell in 3d, but may     require traversing a possibly very large number of other cells that     connect to an edge. Even if it can be done, one still needs to do     different things depending on how the neighborhood looks like, producing     code that is likely very complex, hard to maintain, and possibly slow. 

  Consequently, at least for the moment, none of these ideas are   implemented. This leads to the undesirable consequence of discontinuous   geometries, but, as discussed above, the effects of this do not appear to   pose problem in actual practice. 

 [2.x.5119]  


examples/step-54/doc/intro.dox 

 [2.x.5120]  

[1.x.2007] 

 [2.x.5121]  This program elaborates on concepts of industrial geometry, using tools that interface with the OpenCASCADE library (http://www.opencascade.org) that allow the specification of arbitrary IGES files to describe the boundaries for your geometries. 

 [2.x.5122]  

[1.x.2008] 

[1.x.2009] 


In some of the previous tutorial programs (step-1, step-3, step-5, step-6 and step-49 among others) we have learned how to use the mesh refinement methods provided in deal.II. These tutorials have shown how to employ such tools to produce a fine grid for a single simulation, as done in step-3; or to start from a coarse grid and carry out a series of simulations on adaptively refined grids, as is the case of step-6. Regardless of which approach is taken, the mesh refinement requires a suitable geometrical description of the computational domain boundary in order to place, at each refinement, the new mesh nodes onto the boundary surface. For instance, step-5 shows how creating a circular grid automatically attaches a circular manifold object to the computational domain, so that the faces lying on the boundary are refined onto the circle. step-53 shows how to do this with a Manifold defined by experimentally obtained data. But, at least as far as elementary boundary shapes are concerned, deal.II really only provides circles, spheres, boxes and other elementary combinations. In this tutorial, we will show how to use a set of classes developed to import arbitrary CAD geometries, assign them to the desired boundary of the computational domain, and refine a computational grid on such complex shapes. 




[1.x.2010] 

In the most common industrial practice, the geometrical models of arbitrarily shaped objects are realized by means of Computer Aided Design (CAD) tools. The use of CAD modelers has spread in the last decades, as they allow for the generation of a full virtual model of each designed object, which through a computer can be visualized, inspected, and analyzed in its finest details well before it is physically crafted.  From a mathematical perspective, the engine lying under the hood of CAD modelers is represented by analytical geometry, and in particular by parametric curves and surfaces such as B-splines and NURBS that are rich enough that they can represent most surfaces of practical interest.  Once a virtual model is ready, all the geometrical features of the desired object are stored in files which materially contain the coefficients of the parametric surfaces and curves composing the object. Depending on the specific CAD tool used to define the geometrical model, there are of course several different file formats in which the information of a CAD model can be organized. To provide a common ground to exchange data across CAD tools, the U.S. National Bureau of Standards published in 1980 the Initial Graphics Exchange Representation (IGES) neutral file format, which is used in this example. 

[1.x.2011] 

To import and interrogate CAD models, the deal.II library implements a series of wrapper functions for the OpenCASCADE open source library for CAD modeling. These functions allow to import IGES files into OpenCASCADE native objects, and wrap them inside a series of Manifold classes. 

Once imported from an IGES file, the model is stored in a  [2.x.5123] , which is the generic topological entity defined in the OpenCASCADE framework. From a  [2.x.5124] , it is then possible to access all the sub-shapes (such as vertices, edges and faces) composing it, along with their geometrical description. In the deal.II framework, the topological entities composing a shape are used to create a corresponding Manifold representation. In step-6 we saw how to use  [2.x.5125]  to create a hyper sphere, which automatically attaches a SphericalManifold to all boundary faces. This guarantees that boundary faces stay on a sphere or circle during mesh refinement. The functions of the CAD modeling interface have been designed to retain the same structure, allowing the user to build a projector object using the imported CAD shapes, maintaining the same procedure we used in other tutorial programs, i.e., assigning such projector object to cells, faces or edges of a coarse mesh. At each refinement cycle, the new mesh nodes will be then automatically generated by projecting a midpoint of an existing object onto the specified geometry. 

Differently from a spherical or circular boundary, a boundary with a complex geometry poses problems as to where it is best to place the new nodes created upon refinement on the prescribed shape. PolarManifold, for example, transforms the surrounding points to polar coordinates, calculates the average in that coordinate system (for each coordinate individually) and finally transforms the point back to Cartesian coordinates. 

In the case of an arbitrary and complex shape though, an appropriate choice for the placement of a new node cannot be identified that easily. The OpenCASCADE wrappers in deal.II provide several projector classes that employ different projection strategies. A first projector, implemented in the  [2.x.5126]  class, is to be used only for edge refinement. It is built assigning it a topological shape of dimension one, either a  [2.x.5127]  (which is a compound shape, made of several connected  [2.x.5128] s) and refines a mesh edge finding the new vertex as the point splitting in two even parts the curvilinear length of the CAD curve portion that lies between the vertices of the original edge. 

 [2.x.5129]  


A different projection strategy has been implemented in the  [2.x.5130]  class. The  [2.x.5131]  assigned at construction time can be arbitrary (a collection of shapes, faces, edges or a single face or edge will all work). The new cell nodes are first computed by averaging the surrounding points in the same way as FlatManifold does. In a second step, all the new nodes will be projected onto the  [2.x.5132]  along the direction normal to the shape. If no normal projection is available, the point which is closest to the shape---typically lying on the shape boundary---is selected.  If the shape is composed of several sub-shapes, the projection is carried out onto every single sub-shape, and the closest projection point is selected. 

 [2.x.5133]   [2.x.5134]  

As we are about to experience, for some shapes, setting the projection direction as that normal to the CAD surface will not lead to surface mesh elements of suitable quality. This is because the direction normal to the CAD surface has in principle nothing to do with the direction along which the mesh needs the new nodes to be located. The  [2.x.5135]  class, in this case, can help. This class is constructed assigning a  [2.x.5136]  (containing at least a face) and a direction along which all the projections will be carried out. New points will be computed by first averaging the surrounding points (as in the FlatManifold case), and then taking the closest intersection between the topological shape and the line passing through the resulting point, along the direction used at construction time.  In this way, the user will have a higher control on the projection direction to be enforced to ensure good mesh quality. 

 [2.x.5137]  


Of course the latter approach is effective only when the orientation of the surface is rather uniform, so that a single projection direction can be identified. In cases in which the surface direction is approaching the projection direction, it is even possible that the directional projection is not found. To overcome these problems, the  [2.x.5138]  class implements a third projection algorithm. The  [2.x.5139]  class is built assigning a  [2.x.5140]  (containing at least one face) to the constructor, and works exactly like a  [2.x.5141]  But, as the name of the class suggests,  [2.x.5142]  tries to come up with a suitable estimate of the direction normal to the mesh elements to be refined, and uses it for the projection of the new nodes onto the CAD surface. If we consider a mesh edge in a 2D space, the direction of its axis is a direction along which to split it in order to give rise to two new cells of the same length. We here extended this concept in 3D, and project all new nodes in a direction that approximates the cell normal. 

In the next figure, which is inspired by the geometry considered in this tutorial, we make an attempt to compare the behavior of the three projectors considered. As can be seen on the left, given the original cell (in blue), the new point found with the normal projection is in a position which does not allow for the generation of evenly spaced new elements (in red). The situation will get worse in further refinement steps.  Since the geometry we considered is somehow perpendicular to the horizontal direction, the directional projection (central image) defined with horizontal direction as the projection direction, does a rather good job in getting the new mesh point. Yet, since the surface is almost horizontal at the bottom of the picture, we can expect problems in those regions when further refinement steps are carried out. Finally, the picture on the right shows that a node located on the cell axis will result in two new cells having the same length. Of course the situation in 3D gets a little more complicated than that described in this simple 2D case. Nevertheless, the results of this test confirm that the normal to the mesh direction is the best approach among the three tested, when arbitrarily shaped surfaces are considered, and unless you have a geometry for which a more specific approach is known to be appropriate. 


 [2.x.5143]  




[1.x.2012] 

In this program, we will consider creating a surface mesh for a real geometry describing the bow of a ship (this geometry is frequently used in CAD and mesh generation comparisons and is freely available). The surface mesh we get from this could then be used to solve a boundary element equation to simulate the flow of water around the ship (in a way similar to step-34) but we will not try to do this here. To already give you an idea of the geometry we consider, here is a picture: 

 [2.x.5144]  

In the program, we read both the geometry and a coarse mesh from files, and then employ several of the options discussed above to place new vertices for a sequence of mesh refinement steps. 


examples/step-54/doc/results.dox 



[1.x.2013] 

The program execution produces a series of mesh files  [2.x.5145]  that we can visualize with any of the usual visualization programs that can read the VTK file format. 

The following table illustrates the results obtained employing the normal projection strategy. The first two rows of the table show side views of the grids obtained for progressive levels of refinement, overlain on a very fine rendering of the exact geometry. The dark and light red areas simply indicate whether the current mesh or the fine geometry is closer to the observer; the distinction does not carry any particularly deep meaning. The last row of pictures depict front views (mirrored to both sides of the geometry) of the same grids shown in the second row. 


 [2.x.5146]  

As can be seen in the pictures---and as we anticipated---the normal refinement strategy is unable to produce nicely shaped elements when applied to surfaces with significant curvature changes. This is particularly apparent at the bulb of the hull where all new points have been placed in the upper part of the bulb and the lower part remains completely unresolved. 

The following table, which is arranged as the previous one, illustrates the results obtained adopting the directional projection approach, in which the projection direction selected was the y-axis (which is indicated with a small yellow arrow at the bottom left of each image). 


 [2.x.5147]  

The images confirm that the quality of the mesh obtained with a directional projection is sensibly higher than that obtained projecting along the surface normal. Yet, a number of elements elongated in the y-direction are observed around the bottom of the bulb, where the surface is almost parallel to the direction chosen for the projection. 

The final test shows results using instead the projection normal to the faces: 

 [2.x.5148]  

The pictures confirm that the normal to mesh projection approach leads to grids that remain evenly spaced throughtout the refinement steps. At the same time, these meshes represent rather well the original geometry even in the bottom region of the bulb, which is not well recovered employing the directional projector or the normal projector. 


examples/step-55/doc/intro.dox 

 [2.x.5149]  

[1.x.2014] 




 [2.x.5150]  As a prerequisite of this program, you need to have PETSc or Trilinos and the p4est library installed. The installation of deal.II together with these additional libraries is described in the [1.x.2015] file. 

[1.x.2016] 

[1.x.2017] 

Building on step-40, this tutorial shows how to solve linear PDEs with several components in parallel using MPI with PETSc or Trilinos for the linear algebra. For this, we return to the Stokes equations as discussed in step-22. The motivation for writing this tutorial is to provide an intermediate step (pun intended) between step-40 (parallel Laplace) and step-32 (parallel coupled Stokes with Boussinesq for a time dependent problem). 

The learning outcomes for this tutorial are: 

- You are able to solve PDEs with several variables in parallel and can   apply this to different problems. 

- You understand the concept of optimal preconditioners and are able to check   this for a particular problem. 

- You are able to construct manufactured solutions using the free computer   algreba system SymPy (https://sympy.org). 

- You can implement various other tasks for parallel programs: error   computation, writing graphical output, etc. 

- You can visualize vector fields, stream lines, and contours of vector   quantities. 

We are solving for a velocity  [2.x.5151]  and pressure  [2.x.5152]  that satisfy the Stokes equation, which reads 

[1.x.2018] 






[1.x.2019] 

Make sure that you read (even better: try) what is described in "Block Schur complement preconditioner" in the "Possible Extensions" section in step-22. Like described there, we are going to solve the block system using a Krylov method and a block preconditioner. 

Our goal here is to construct a very simple (maybe the simplest?) optimal preconditioner for the linear system. A preconditioner is called "optimal" or "of optimal complexity", if the number of iterations of the preconditioned system is independent of the mesh size  [2.x.5153] . You can extend that definition to also require indepence of the number of processors used (we will discuss that in the results section), the computational domain and the mesh quality, the test case itself, the polynomial degree of the finite element space, and more. 

Why is a constant number of iterations considered to be "optimal"? Assume the discretized PDE gives a linear system with N unknowns. Because the matrix coming from the FEM discretization is sparse, a matrix-vector product can be done in O(N) time. A preconditioner application can also only be O(N) at best (for example doable with multigrid methods). If the number of iterations required to solve the linear system is independent of  [2.x.5154]  (and therefore N), the total cost of solving the system will be O(N). It is not possible to beat this complexity, because even looking at all the entries of the right-hand side already takes O(N) time. For more information see  [2.x.5155] , Chapter 2.5 (Multigrid). 

The preconditioner described here is even simpler than the one described in step-22 and will typically require more iterations and consequently time to solve. When considering preconditioners, optimality is not the only important metric. But an optimal and expensive preconditioner is typically more desirable than a cheaper, non-optimal one. This is because, eventually, as the mesh size becomes smaller and smaller and linear problems become bigger and bigger, the former will eventually beat the latter. 

[1.x.2020] 

We precondition the linear system 

[1.x.2021] 



with the block diagonal preconditioner 

[1.x.2022] 

where  [2.x.5156]  is the Schur complement. 

With this choice of  [2.x.5157] , assuming that we handle  [2.x.5158]  and  [2.x.5159]  exactly (which is an "idealized" situation), the preconditioned linear system has three distinct eigenvalues independent of  [2.x.5160]  and is therefore "optimal".  See section 6.2.1 (especially p. 292) in  [2.x.5161] . For comparison, using the ideal version of the upper block-triangular preconditioner in step-22 (also used in step-56) would have all eigenvalues be equal to one. 

We will use approximations of the inverse operations in  [2.x.5162]  that are (nearly) independent of  [2.x.5163] . In this situation, one can again show, that the eigenvalues are independent of  [2.x.5164] . For the Krylov method we choose MINRES, which is attractive for the analysis (iteration count is proven to be independent of  [2.x.5165] , see the remainder of the chapter 6.2.1 in the book mentioned above), great from the computational standpoint (simpler and cheaper than GMRES for example), and applicable (matrix and preconditioner are symmetric). 

For the approximations we will use a CG solve with the mass matrix in the pressure space for approximating the action of  [2.x.5166] . Note that the mass matrix is spectrally equivalent to  [2.x.5167] . We can expect the number of CG iterations to be independent of  [2.x.5168] , even with a simple preconditioner like ILU. 

For the approximation of the velocity block  [2.x.5169]  we will perform a single AMG V-cycle. In practice this choice is not exactly independent of  [2.x.5170] , which can explain the slight increase in iteration numbers. A possible explanation is that the coarsest level will be solved exactly and the number of levels and size of the coarsest matrix is not predictable. 




[1.x.2023] 

We will construct a manufactured solution based on the classical Kovasznay problem, see  [2.x.5171] . Here is an image of the solution colored by the x velocity including streamlines of the velocity: 

  [2.x.5172]  

We have to cheat here, though, because we are not solving the non-linear Navier-Stokes equations, but the linear Stokes system without convective term. Therefore, to recreate the exact same solution, we use the method of manufactured solutions with the solution of the Kovasznay problem. This will effectively move the convective term into the right-hand side  [2.x.5173] . 

The right-hand side is computed using the script "reference.py" and we use the exact solution for boundary conditions and error computation. 


examples/step-55/doc/results.dox 



[1.x.2024] 

As expected from the discussion above, the number of iterations is independent of the number of processors and only very slightly dependent on  [2.x.5174] : 

 [2.x.5175]  

 [2.x.5176]  

While the PETSc results show a constant number of iterations, the iterations increase when using Trilinos. This is likely because of the different settings used for the AMG preconditioner. For performance reasons we do not allow coarsening below a couple thousand unknowns. As the coarse solver is an exact solve (we are using LU by default), a change in number of levels will influence the quality of a V-cycle. Therefore, a V-cycle is closer to an exact solver for smaller problem sizes. 

[1.x.2025] 

[1.x.2026] 

[1.x.2027] 

Play with the smoothers, smoothing steps, and other properties for the Trilinos AMG to achieve an optimal preconditioner. 

[1.x.2028] 

This change requires changing the outer solver to GMRES or BiCGStab, because the system is no longer symmetric. 

You can prescribe the exact flow solution as  [2.x.5177]  in the convective term  [2.x.5178] . This should give the same solution as the original problem, if you set the right hand side to zero. 

[1.x.2029] 

So far, this tutorial program refines the mesh globally in each step. Replacing the code in  [2.x.5179]  by something like 

[1.x.2030] 

makes it simple to explore adaptive mesh refinement. 


examples/step-56/doc/intro.dox 

[1.x.2031] 

 [2.x.5180]  

[1.x.2032] 

[1.x.2033] 

[1.x.2034] 

The purpose of this tutorial is to create an efficient linear solver for the Stokes equation and compare it to alternative approaches.  Here, we will use FGMRES with geometric multigrid as a preconditioner velocity block, and we will show in the results section that this is a fundamentally better approach than the linear solvers used in step-22 (including the scheme described in "Possible Extensions").  Fundamentally, this is because only with multigrid it is possible to get  [2.x.5181]  solve time, where  [2.x.5182]  is the number of unknowns of the linear system. Using the Timer class, we collect some statistics to compare setup times, solve times, and number of iterations. We also compute errors to make sure that what we have implemented is correct. 

Let  [2.x.5183]  and  [2.x.5184] . The Stokes equations read as follows in non-dimensionalized form: 

[1.x.2035] 



Note that we are using the deformation tensor instead of  [2.x.5185]  (a detailed description of the difference between the two can be found in step-22, but in summary, the deformation tensor is more physical as well as more expensive). 

[1.x.2036] 

The weak form of the discrete equations naturally leads to the following linear system for the nodal values of the velocity and pressure fields: 

[1.x.2037] 



Our goal is to compare several solution approaches.  While step-22 solves the linear system using a "Schur complement approach" in two separate steps, we instead attack the block system at once using FMGRES with an efficient preconditioner, in the spirit of the approach outlined in the "Results" section of step-22. The idea is as follows: if we find a block preconditioner  [2.x.5186]  such that the matrix 

[1.x.2038] 



is simple, then an iterative solver with that preconditioner will converge in a few iterations. Notice that we are doing right preconditioning here.  Using the Schur complement  [2.x.5187] , we find that 

[1.x.2039] 



is a good choice. Let  [2.x.5188]  be an approximation of  [2.x.5189]  and  [2.x.5190]  of  [2.x.5191] , we see 

[1.x.2040] 



Since  [2.x.5192]  is aimed to be a preconditioner only, we shall use the approximations on the right in the equation above. 

As discussed in step-22,  [2.x.5193] , where  [2.x.5194]  is the pressure mass matrix and is solved approximately by using CG with ILU as a preconditioner, and  [2.x.5195]  is obtained by one of multiple methods: solving a linear system with CG and ILU as preconditioner, just using one application of an ILU, solving a linear system with CG and GMG (Geometric Multigrid as described in step-16) as a preconditioner, or just performing a single V-cycle of GMG. 

As a comparison, instead of FGMRES, we also use the direct solver UMFPACK on the whole system to compare our results with.  If you want to use a direct solver (like UMFPACK), the system needs to be invertible. To avoid the one dimensional null space given by the constant pressures, we fix the first pressure unknown  to zero. This is not necessary for the iterative solvers. 




[1.x.2041] 

The test problem is a "Manufactured Solution" (see step-7 for details), and we choose  [2.x.5196]  and  [2.x.5197] . We apply Dirichlet boundary conditions for the velocity on the whole boundary of the domain  [2.x.5198] . To enforce the boundary conditions we can just use our reference solution. 

If you look up in the deal.II manual what is needed to create a class derived from  [2.x.5199] , you will find that this class has numerous  [2.x.5200]  functions, including  [2.x.5201]   [2.x.5202]   [2.x.5203]  etc., all of which can be overloaded.  Different parts of deal.II will require different ones of these particular functions. This can be confusing at first, but luckily the only thing you actually have to implement is  [2.x.5204]   The other virtual functions in the Function class have default implementations inside that will call your implementation of  [2.x.5205]  by default. 

Notice that our reference solution fulfills  [2.x.5206] . In addition, the pressure is chosen to have a mean value of zero.  For the "Method of Manufactured Solutions" of step-7, we need to find  [2.x.5207]  such that: 

[1.x.2042] 



Using the reference solution above, we obtain: 

[1.x.2043] 



[1.x.2044] 

Because we do not enforce the mean pressure to be zero for our numerical solution in the linear system, we need to post process the solution after solving. To do this we use the  [2.x.5208]  function to compute the mean value of the pressure to subtract it from the pressure. 




[1.x.2045] 

The way we implement geometric multigrid here only executes it on the velocity variables (i.e., the  [2.x.5209]  matrix described above) but not the pressure. One could implement this in different ways, including one in which one considers all coarse grid operations as acting on  [2.x.5210]  block systems where we only consider the top left block. Alternatively, we can implement things by really only considering a linear system on the velocity part of the overall finite element discretization. The latter is the way we want to use here. 

To implement this, one would need to be able to ask questions such as "May I have just part of a DoFHandler?". This is not possible at the time when this program was written, so in order to answer this request for our needs, we simply create a separate, second DoFHandler for just the velocities. We then build linear systems for the multigrid preconditioner based on only this second DoFHandler, and simply transfer the first block of (overall) vectors into corresponding vectors for the entire second DoFHandler. To make this work, we have to assure that the [1.x.2046] in which the (velocity) degrees of freedom are ordered in the two DoFHandler objects is the same. This is in fact the case by first distributing degrees of freedom on both, and then using the same sequence of DoFRenumbering operations on both. 




[1.x.2047] 

The main difference between step-56 and step-22 is that we use block solvers instead of the Schur Complement approach used in step-22. Details of this approach can be found under the "Block Schur complement preconditioner" subsection of the "Possible Extensions" section of step-22. For the preconditioner of the velocity block, we borrow a class from [1.x.2048] called  [2.x.5211]  that has the option to solve for the inverse of  [2.x.5212]  or just apply one preconditioner sweep for it instead, which provides us with an expensive and cheap approach, respectively. 


examples/step-56/doc/results.dox 



[1.x.2049] 

[1.x.2050] 

We first run the code and confirm that the finite element solution converges with the correct rates as predicted by the error analysis of mixed finite element problems. Given sufficiently smooth exact solutions  [2.x.5213]  and  [2.x.5214] , the errors of the Taylor-Hood element  [2.x.5215]  should be 

[1.x.2051] 



see for example Ern/Guermond "Theory and Practice of Finite Elements", Section 4.2.5 p195. This is indeed what we observe, using the  [2.x.5216]  element as an example (this is what is done in the code, but is easily changed in  [2.x.5217] ): 

 [2.x.5218]  

[1.x.2052] 

Let us compare the direct solver approach using UMFPACK to the two methods in which we choose  [2.x.5219]  and  [2.x.5220]  by solving linear systems with  [2.x.5221]  using CG. The preconditioner for CG is then either ILU or GMG. The following table summarizes solver iterations, timings, and virtual memory (VM) peak usage: 

 [2.x.5222]  

As can be seen from the table: 

1. UMFPACK uses large amounts of memory, especially in 3d. Also, UMFPACK timings do not scale favorably with problem size. 

2. Because we are using inner solvers for  [2.x.5223]  and  [2.x.5224] , ILU and GMG require the same number of outer iterations. 

3. The number of (inner) iterations for  [2.x.5225]  increases for ILU with refinement, leading to worse than linear scaling in solve time. In contrast, the number of inner iterations for  [2.x.5226]  stays constant with GMG leading to nearly perfect scaling in solve time. 

4. GMG needs slightly more memory than ILU to store the level and interface matrices. 

[1.x.2053] 

[1.x.2054] 

Experiment with higher order stable FE pairs and check that you observe the correct convergence rates. 

[1.x.2055] 

The introduction also outlined another option to precondition the overall system, namely one in which we do not choose  [2.x.5227]  as in the table above, but in which  [2.x.5228]  is only a single preconditioner application with GMG or ILU, respectively. 

This is in fact implemented in the code: Currently, the boolean  [2.x.5229]  is set to  [2.x.5230]  The option mentioned above is obtained by setting it to  [2.x.5231]  

What you will find is that the number of FGMRES iterations stays constant under refinement if you use GMG this way. This means that the Multigrid is optimal and independent of  [2.x.5232] . 


examples/step-57/doc/intro.dox 

 [2.x.5233]  

[1.x.2056] 

 [2.x.5234]  

[1.x.2057] 

[1.x.2058] 

[1.x.2059] 

In this tutorial we show how to solve the incompressible Navier Stokes equations (NSE) with Newton's method. The flow we consider here is assumed to be steady. In a domain  [2.x.5235] ,  [2.x.5236] , with a piecewise smooth boundary  [2.x.5237] , and a given force field  [2.x.5238] , we seek a velocity field  [2.x.5239]  and a pressure field  [2.x.5240]  satisfying 

[1.x.2060] 



Unlike the Stokes equations as discussed in step-22, the NSE are a nonlinear system of equations because of the convective term  [2.x.5241] . The first step of computing a numerical solution is to linearize the system and this will be done using Newton's method. A time-dependent problem is discussed in step-35, where the system is linearized using the solution from the last time step and no nonlinear solve is necessary. 

[1.x.2061] 

We define a nonlinear function whose root is a solution to the NSE by 

[1.x.2062] 



Assuming the initial guess is good enough to guarantee the convergence of Newton's iteration and denoting  [2.x.5242] , Newton's iteration on a vector function can be defined as 

[1.x.2063] 



where  [2.x.5243]  is the approximate solution in step  [2.x.5244] ,  [2.x.5245]  represents the solution from the previous step, and  [2.x.5246]  is the Jacobian matrix evaluated at  [2.x.5247] . A similar iteration can be found in step-15. 

The Newton iteration formula implies the new solution is obtained by adding an update term to the old solution. Instead of evaluating the Jacobian matrix and taking its inverse, we consider the update term as a whole, that is 

[1.x.2064] 



where  [2.x.5248] . 

We can find the update term by solving the system 

[1.x.2065] 



Here, the left of the previous equation represents the directional gradient of  [2.x.5249]  along  [2.x.5250]  at  [2.x.5251] . By definition, the directional gradient is given by 

[1.x.2066] 



Therefore, we arrive at the linearized system: 

[1.x.2067] 



where  [2.x.5252]  and  [2.x.5253]  are the solutions from the previous iteration. Additionally, the right hand side of the second equation is not zero since the discrete solution is not exactly divergence free (divergence free for the continuous solution). The right hand side here acts as a correction which leads the discrete solution of the velocity to be divergence free along Newton's iteration. In this linear system, the only unknowns are the update terms  [2.x.5254]  and  [2.x.5255] , and we can use a similar strategy to the one used in step-22 (and derive the weak form in the same way). 

Now, Newton's iteration can be used to solve for the update terms: 

<ol>    [2.x.5256] Initialization: Initial guess  [2.x.5257]  and  [2.x.5258] , tolerance  [2.x.5259] ; [2.x.5260]     [2.x.5261] Linear solve to compute update term  [2.x.5262]  and        [2.x.5263] ; [2.x.5264]     [2.x.5265] Update the approximation:        [2.x.5266]  and        [2.x.5267] ; [2.x.5268]     [2.x.5269] Check residual norm:  [2.x.5270] :        [2.x.5271]           [2.x.5272] If  [2.x.5273] , STOP. [2.x.5274]           [2.x.5275] If  [2.x.5276] , back to step 2. [2.x.5277]         [2.x.5278]  [2.x.5279]   [2.x.5280]  

[1.x.2068] 

The initial guess needs to be close enough to the solution for Newton's method to converge; hence, finding a good starting value is crucial to the nonlinear solver. 

When the viscosity  [2.x.5281]  is large, a good initial guess can be obtained by solving the Stokes equation with viscosity  [2.x.5282] . While problem dependent, this works for  [2.x.5283]  for the test problem considered here. 

However, the convective term  [2.x.5284]  will be dominant if the viscosity is small, like  [2.x.5285]  in test case 2.  In this situation, we use a continuation method to set up a series of auxiliary NSEs with viscosity approaching the one in the target NSE. Correspondingly, we create a sequence  [2.x.5286]  with  [2.x.5287] , and accept that the solutions to two NSE with viscosity  [2.x.5288]  and  [2.x.5289]  are close if  [2.x.5290]  is small.  Then we use the solution to the NSE with viscosity  [2.x.5291]  as the initial guess of the NSE with  [2.x.5292] . This can be thought of as a staircase from the Stokes equations to the NSE we want to solve. 

That is, we first solve a Stokes problem 

[1.x.2069] 



to get the initial guess for 

[1.x.2070] 



which also acts as the initial guess of the continuation method. Here  [2.x.5293]  is relatively large so that the solution to the Stokes problem with viscosity  [2.x.5294]  can be used as an initial guess for the NSE in Newton's iteration. 

Then the solution to 

[1.x.2071] 



acts as the initial guess for 

[1.x.2072] 



This process is repeated with a sequence of viscosities  [2.x.5295]  that is determined experimentally so that the final solution can used as a starting guess for the Newton iteration. 

[1.x.2073] 

At each step of Newton's iteration, the problem results in solving a saddle point systems of the form 

[1.x.2074] 



This system matrix has the same block structure as the one in step-22. However, the matrix  [2.x.5296]  at the top left corner is not symmetric because of the nonlinear term. Instead of solving the above system, we can solve the equivalent system 

[1.x.2075] 



with a parameter  [2.x.5297]  and an invertible matrix  [2.x.5298] . Here  [2.x.5299]  is the Augmented Lagrangian term; see [1] for details. 

Denoting the system matrix of the new system by  [2.x.5300]  and the right-hand side by  [2.x.5301] , we solve it iteratively with right preconditioning  [2.x.5302]  as  [2.x.5303] , where 

[1.x.2076] 



with  [2.x.5304]  and  [2.x.5305]  is the corresponding Schur complement  [2.x.5306] . We let  [2.x.5307]  where  [2.x.5308]  is the pressure mass matrix, then  [2.x.5309]  can be approximated by 

[1.x.2077] 



See [1] for details. 

We decompose  [2.x.5310]  as 

[1.x.2078] 



Here two inexact solvers will be needed for  [2.x.5311]  and  [2.x.5312] , respectively (see [1]). Since the pressure mass matrix is symmetric and positive definite, CG with ILU as a preconditioner is appropriate to use for  [2.x.5313] . For simplicity, we use the direct solver UMFPACK for  [2.x.5314] . The last ingredient is a sparse matrix-vector product with  [2.x.5315] . Instead of computing the matrix product in the augmented Lagrangian term in  [2.x.5316] , we assemble Grad-Div stabilization  [2.x.5317] , as explained in [2]. 

[1.x.2079] 

We use the lid driven cavity flow as our test case; see [3] for details. The computational domain is the unit square and the right-hand side is  [2.x.5318] . The boundary condition is 

[1.x.2080] 



When solving this problem, the error consists of the nonlinear error (from Newton's iteration) and the discretization error (dependent on mesh size). The nonlinear part decreases with each Newton iteration and the discretization error reduces with mesh refinement. In this example, the solution from the coarse mesh is transferred to successively finer meshes and used as an initial guess. Therefore, the nonlinear error is always brought below the tolerance of Newton's iteration and the discretization error is reduced with each mesh refinement. 

Inside the loop, we involve three solvers: one for  [2.x.5319] , one for  [2.x.5320]  and one for  [2.x.5321] . The first two solvers are invoked in the preconditioner and the outer solver gives us the update term. Overall convergence is controlled by the nonlinear residual; as Newton's method does not require an exact Jacobian, we employ FGMRES with a relative tolerance of only 1e-4 for the outer linear solver. In fact, we use the truncated Newton solve for this system. As described in step-22, the inner linear solves are also not required to be done very accurately. Here we use CG with a relative tolerance of 1e-6 for the pressure mass matrix. As expected, we still see convergence of the nonlinear residual down to 1e-14. Also, we use a simple line search algorithm for globalization of the Newton method. 

The cavity reference values for  [2.x.5322]  and  [2.x.5323]  are from [4] and [5], respectively, where  [2.x.5324]  is the Reynolds number and can be located at [8]. Here the viscosity is defined by  [2.x.5325] . Even though we can still find a solution for  [2.x.5326]  and the references contain results for comparison, we limit our discussion here to  [2.x.5327] . This is because the solution is no longer stationary starting around  [2.x.5328]  but instead becomes periodic, see [7] for details. 

[1.x.2081] <ol> 

   [2.x.5329]   An Augmented Lagrangian-Based Approach to the Oseen Problem, M. Benzi and M. Olshanskii, SIAM J. SCI. COMPUT. 2006    [2.x.5330]   Efficient augmented Lagrangian-type preconditioning for the Oseen problem using Grad-Div stabilization, Timo Heister and Gerd Rapin    [2.x.5331]   http://www.cfd-online.com/Wiki/Lid-driven_cavity_problem    [2.x.5332]   High-Re solution for incompressible flow using the Navier-Stokes Equations and a Multigrid Method, U. Ghia, K. N. Ghia, and C. T. Shin    [2.x.5333]   Numerical solutions of 2-D steady incompressible driven cavity flow at high Reynolds numbers, E. Erturk, T.C. Corke and C. Gokcol    [2.x.5334]  Implicit Weighted ENO Schemes for the Three-Dimensional Incompressible Navier-Stokes Equations, Yang et al, 1998    [2.x.5335]  The 2D lid-driven cavity problem revisited, C. Bruneau and M. Saad, 2006    [2.x.5336]  https://en.wikipedia.org/wiki/Reynolds_number  [2.x.5337]  


examples/step-57/doc/results.dox 



[1.x.2082] 

Now we use the method we discussed above to solve Navier Stokes equations with viscosity  [2.x.5338]  and  [2.x.5339] . 

[1.x.2083] 

In the first test case the viscosity is set to be  [2.x.5340] . As we discussed in the introduction, the initial guess is the solution to the corresponding Stokes problem. In the following table, the residuals at each Newton's iteration on every mesh is shown. The data in the table shows that Newton's iteration converges quadratically. 

 [2.x.5341]  








The following figures show the sequence of generated grids. For the case of  [2.x.5342] , the initial guess is obtained by solving Stokes on an  [2.x.5343]  mesh, and the mesh is refined adaptively. Between meshes, the solution from the coarse mesh is interpolated to the fine mesh to be used as an initial guess. 

 [2.x.5344]  

This picture is the graphical streamline result of lid-driven cavity with  [2.x.5345] .  [2.x.5346]  

Then the solution is compared with a reference solution from [4] and the reference solution data can be found in the file "ref_2d_ghia_u.txt". 

 [2.x.5347]  

[1.x.2084] 

Newton's iteration requires a good initial guess. However, the nonlinear term dominates when the Reynolds number is large, so that the solution to the Stokes equations may be far away from the exact solution. If the Stokes solution acts as the initial guess, the convergence will be lost. The following picture shows that the nonlinear iteration gets stuck and the residual no longer decreases in further iterations. 

 [2.x.5348]  

The initial guess, therefore, has to be obtained via a continuation method which has been discussed in the introduction. Here the step size in the continuation method, that is  [2.x.5349] , is 2000 and the initial mesh is of size  [2.x.5350] . After obtaining an initial guess, the mesh is refined as in the previous test case. The following picture shows that at each refinement Newton's iteration has quadratic convergence. 52 steps of Newton's iterations are executed for solving this test case. 

 [2.x.5351]  

We also show the residual from each step of Newton's iteration on every mesh. The quadratic convergence is clearly visible in the table. 

 [2.x.5352]  








The sequence of generated grids looks like this:  [2.x.5353]  We compare our solution with reference solution from [5].  [2.x.5354]  The following picture presents the graphical result.  [2.x.5355]  

Furthermore, the error consists of the nonlinear error, which decreases as we perform Newton iterations, and the discretization error, which depends on the mesh size. That is why we have to refine the mesh and repeat Newton's iteration on the next finer mesh. From the table above, we can see that the residual (nonlinear error) is below 1e-12 on each mesh, but the following picture shows us the difference between solutions on subsequently finer meshes. 

 [2.x.5356]  

[1.x.2085] 

[1.x.2086] 

[1.x.2087] 

It is easy to compare the currently implemented linear solver to just using UMFPACK for the whole linear system. You need to remove the nullspace containing the constant pressures and it is done in step-56. More interesting is the comparison to other state of the art preconditioners like PCD. It turns out that the preconditioner here is very competitive, as can be seen in the paper [2]. 

The following table shows the timing results between our iterative approach (FGMRES) compared to a direct solver (UMFPACK) for the whole system with viscosity set to 1/400. Even though we use the same direct solver for the velocity block in the iterative solver, it is considerably faster and consumes less memory. This will be even more pronounced in 3d. 

 [2.x.5357]  




[1.x.2088] 

The code is set up to also run in 3d. Of course the reference values are different, see [6] for example. High resolution computations are not doable with this example as is, because a direct solver for the velocity block does not work well in 3d. Rather, a parallel solver based on algebraic or geometric multigrid is needed. See below. 

[1.x.2089] 

For larger computations, especially in 3d, it is necessary to implement MPI parallel solvers and preconditioners. A good starting point would be step-55, which uses algebraic multigrid for the velocity block for the Stokes equations. Another option would be to take a look at the list of codes in the [1.x.2090], which already contains parallel Navier-Stokes solvers. 


examples/step-58/doc/intro.dox 

 [2.x.5358]  

[1.x.2091][1.x.2092] 

[1.x.2093] 

[1.x.2094] 

The [1.x.2095] for a function  [2.x.5359]  and a potential  [2.x.5360]  is a model often used in quantum mechanics and nonlinear optics. If one measures in appropriate quantities (so that  [2.x.5361] ), then it reads as follows: 

[1.x.2096] 

If there is no potential, i.e.  [2.x.5362] , then it can be used to describe the propagation of light in optical fibers. If  [2.x.5363] , the equation is also sometimes called the [1.x.2097] and can be used to model the time dependent behavior of [1.x.2098]. 

For this particular tutorial program, the physical interpretation of the equation is not of much concern to us. Rather, we want to use it as a model that allows us to explain two aspects: 

- It is a [1.x.2099] for  [2.x.5364] . We have previously seen complex-valued equations in step-29,   but there have opted to split the equations into real and imaginary   parts and consequently ended up solving a system of two real-valued   equations. In contrast, the goal here is to show how to solve   problems in which we keep everything as complex numbers. 

- The equation is a nice model problem to explain how [1.x.2100] work. This is because it has terms with   fundamentally different character: on the one hand,  [2.x.5365]  is a regular spatial operator in the way we have seen   many times before; on the other hand,  [2.x.5366]  has no spatial or temporal derivatives, i.e., it is a purely   local operator. It turns out that we have efficient methods for each   of these terms (in particular, we have analytic solutions for the   latter), and that we may be better off treating these terms   differently and separately. We will explain this in more detail   below. 




[1.x.2101] 

At first glance, the equations appear to be parabolic and similar to the heat equation (see step-26) as there is only a single time derivative and two spatial derivatives. But this is misleading. Indeed, that this is not the correct interpretation is more easily seen if we assume for a moment that the potential  [2.x.5367]  and  [2.x.5368] . Then we have the equation 

[1.x.2102] 

If we separate the solution into real and imaginary parts,  [2.x.5369] , with  [2.x.5370] , then we can split the one equation into its real and imaginary parts in the same way as we did in step-29: 

[1.x.2103] 

Not surprisingly, the factor  [2.x.5371]  in front of the time derivative couples the real and imaginary parts of the equation. If we want to understand this equation further, take the time derivative of one of the equations, say 

[1.x.2104] 

(where we have assumed that, at least in some formal sense, we can commute the spatial and temporal derivatives), and then insert the other equation into it: 

[1.x.2105] 

This equation is hyperbolic and similar in character to the wave equation. (This will also be obvious if you look at the video in the "Results" section of this program.) Furthermore, we could have arrived at the same equation for  [2.x.5372]  as well. Consequently, a better assumption for the NLSE is to think of it as a hyperbolic, wave-propagation equation than as a diffusion equation such as the heat equation. (You may wonder whether it is correct that the operator  [2.x.5373]  appears with a positive sign whereas in the wave equation,  [2.x.5374]  has a negative sign. This is indeed correct: After multiplying by a test function and integrating by parts, we want to come out with a positive (semi-)definite form. So, from  [2.x.5375]  we obtain  [2.x.5376] . Likewise, after integrating by parts twice, we obtain from  [2.x.5377]  the form  [2.x.5378] . In both cases do we get the desired positive sign.) 

The real NLSE, of course, also has the terms  [2.x.5379]  and  [2.x.5380] . However, these are of lower order in the spatial derivatives, and while they are obviously important, they do not change the character of the equation. 

In any case, the purpose of this discussion is to figure out what time stepping scheme might be appropriate for the equation. The conclusions is that, as a hyperbolic-kind of equation, we need to choose a time step that satisfies a CFL-type condition. If we were to use an explicit method (which we will not), we would have to investigate the eigenvalues of the matrix that corresponds to the spatial operator. If you followed the discussions of the video lectures ( [2.x.5381]  then you will remember that the pattern is that one needs to make sure that  [2.x.5382]  where  [2.x.5383]  is the time step,  [2.x.5384]  the mesh width, and  [2.x.5385]  are the orders of temporal and spatial derivatives. Whether you take the original equation ( [2.x.5386] ) or the reformulation for only the real or imaginary part, the outcome is that we would need to choose  [2.x.5387]  if we were to use an explicit time stepping method. This is not feasible for the same reasons as in step-26 for the heat equation: It would yield impractically small time steps for even only modestly refined meshes. Rather, we have to use an implicit time stepping method and can then choose a more balanced  [2.x.5388] . Indeed, we will use the implicit Crank-Nicolson method as we have already done in step-23 before for the regular wave equation. 




[1.x.2106] 

 [2.x.5389]  

If one thought of the NLSE as an ordinary differential equation in which the right hand side happens to have spatial derivatives, i.e., write it as 

[1.x.2107] 

one may be tempted to "formally solve" it by integrating both sides over a time interval  [2.x.5390]  and obtain 

[1.x.2108] 

Of course, it's not that simple: the  [2.x.5391]  in the integrand is still changing over time in accordance with the differential equation, so we cannot just evaluate the integral (or approximate it easily via quadrature) because we don't know  [2.x.5392] . But we can write this with separate contributions as follows, and this will allow us to deal with different terms separately: 

[1.x.2109] 

The way this equation can now be read is as follows: For each time interval  [2.x.5393] , the change  [2.x.5394]  in the solution consists of three contributions: 

- The contribution of the Laplace operator. 

- The contribution of the potential  [2.x.5395] . 

- The contribution of the "phase" term  [2.x.5396] . 

[1.x.2110] is now an approximation technique that allows us to treat each of these contributions separately. (If we want: In practice, we will treat the first two together, and the last one separate. But that is a detail, conceptually we could treat all of them differently.) To this end, let us introduce three separate "solutions": 

[1.x.2111] 



These three "solutions" can be thought of as satisfying the following differential equations: 

[1.x.2112] 

In other words, they are all trajectories  [2.x.5397]  that start at  [2.x.5398]  and integrate up the effects of exactly one of the three terms. The increments resulting from each of these terms over our time interval are then  [2.x.5399] ,  [2.x.5400] , and  [2.x.5401] . 

It is now reasonable to assume (this is an approximation!) that the change due to all three of the effects in question is well approximated by the sum of the three separate increments: 

[1.x.2113] 

This intuition is indeed correct, though the approximation is not exact: the difference between the exact left hand side and the term  [2.x.5402]  (i.e., the difference between the [1.x.2114] increment for the exact solution  [2.x.5403]  when moving from  [2.x.5404]  to  [2.x.5405] , and the increment composed of the three parts on the right hand side), is proportional to  [2.x.5406] . In other words, this approach introduces an error of size  [2.x.5407] . Nothing we have done so far has discretized anything in time or space, so the [1.x.2115] error is going to be  [2.x.5408]  plus whatever error we commit when approximating the integrals (the temporal discretization error) plus whatever error we commit when approximating the spatial dependencies of  [2.x.5409]  (the spatial error). 

Before we continue with discussions about operator splitting, let us talk about why one would even want to go this way? The answer is simple: For some of the separate equations for the  [2.x.5410] , we may have ways to solve them more efficiently than if we throw everything together and try to solve it at once. For example, and particularly pertinent in the current case: The equation for  [2.x.5411] , i.e., 

[1.x.2116] 

or equivalently, 

[1.x.2117] 

can be solved exactly: the equation is solved by 

[1.x.2118] 

This is easy to see if (i) you plug this solution into the differential equation, and (ii) realize that the magnitude  [2.x.5412]  is constant, i.e., the term  [2.x.5413]  in the exponent is in fact equal to  [2.x.5414] . In other words, the solution of the ODE for  [2.x.5415]  only changes its [1.x.2119], but the [1.x.2120] of the complex-valued function  [2.x.5416]  remains constant. This makes computing  [2.x.5417]  particularly convenient: we don't actually need to solve any ODE, we can write the solution down by hand. Using the operator splitting approach, none of the methods to compute  [2.x.5418]  therefore have to deal with the nonlinear term and all of the associated unpleasantries: we can get away with solving only [1.x.2121] problems, as long as we allow ourselves the luxury of using an operator splitting approach. 

Secondly, one often uses operator splitting if the different physical effects described by the different terms have different time scales. Imagine, for example, a case where we really did have some sort of diffusion equation. Diffusion acts slowly, but if  [2.x.5419]  is large, then the "phase rotation" by the term  [2.x.5420]  acts quickly. If we treated everything together, this would imply having to take rather small time steps. But with operator splitting, we can take large time steps  [2.x.5421]  for the diffusion, and (assuming we didn't have an analytic solution) use an ODE solver with many small time steps to integrate the "phase rotation" equation for  [2.x.5422]  from  [2.x.5423]  to  [2.x.5424] . In other words, operator splitting allows us to decouple slow and fast time scales and treat them differently, with methods adjusted to each case. 




[1.x.2122] 

While the method above allows to compute the three contributions  [2.x.5425]  in parallel, if we want, the method can be made slightly more accurate and easy to implement if we don't let the trajectories for the  [2.x.5426]  start all at  [2.x.5427] , but instead let the trajectory for  [2.x.5428]  start at the [1.x.2123] of the trajectory for  [2.x.5429] , namely  [2.x.5430] ; similarly, we will start the trajectory for  [2.x.5431]  start at the end point of the trajectory for  [2.x.5432] , namely  [2.x.5433] . This method is then called "Lie splitting" and has the same order of error as the method above, i.e., the splitting error is  [2.x.5434] . 

This variation of operator splitting can be written as follows (carefully compare the initial conditions to the ones above): 

[1.x.2124] 

(Obviously, while the formulas above imply that we should solve these problems in this particular order, it is equally valid to first solve for trajectory 3, then 2, then 1, or any other permutation.) 

The integrated forms of these equations are then 

[1.x.2125] 

From a practical perspective, this has the advantage that we need to keep around fewer solution vectors: Once  [2.x.5435]  has been computed, we don't need  [2.x.5436]  any more; once  [2.x.5437]  has been computed, we don't need  [2.x.5438]  any more. And once  [2.x.5439]  has been computed, we can just call it  [2.x.5440]  because, if you insert the first into the second, and then into the third equation, you see that the right hand side of  [2.x.5441]  now contains the contributions of all three physical effects: 

[1.x.2126] 

(Compare this again with the "exact" computation of  [2.x.5442] : It only differs in how we approximate  [2.x.5443]  in each of the three integrals.) In other words, Lie splitting is a lot simpler to implement that the original method outlined above because data handling is so much simpler. 




[1.x.2127] 

As mentioned above, Lie splitting is only  [2.x.5444]  accurate. This is acceptable if we were to use a first order time discretization, for example using the explicit or implicit Euler methods to solve the differential equations for  [2.x.5445] . This is because these time integration methods introduce an error proportional to  [2.x.5446]  themselves, and so the splitting error is proportional to an error that we would introduce anyway, and does not diminish the overall convergence order. 

But we typically want to use something higher order -- say, a [1.x.2128] or [1.x.2129] method -- since these are often not more expensive than a simple Euler method. It would be a shame if we were to use a time stepping method that is  [2.x.5447] , but then lose the accuracy again through the operator splitting. 

This is where the [1.x.2130] method comes in. It is easier to explain if we had only two parts, and so let us combine the effects of the Laplace operator and of the potential into one, and the phase rotation into a second effect. (Indeed, this is what we will do in the code since solving the equation with the Laplace equation with or without the potential costs the same -- so we merge these two steps.) The Lie splitting method from above would then do the following: It computes solutions of the following two ODEs, 

[1.x.2131] 

and then uses the approximation  [2.x.5448] . In other words, we first make one full time step for physical effect one, then one full time step for physical effect two. The solution at the end of the time step is simply the sum of the increments due to each of these physical effects separately. 

In contrast, [1.x.2132] (one of the titans of numerical analysis starting in the mid-20th century) figured out that it is more accurate to first do one half-step for one physical effect, then a full time step for the other physical effect, and then another half step for the first. Which one is which does not matter, but because it is so simple to do the phase rotation, we will use this effect for the half steps and then only need to do one spatial solve with the Laplace operator plus potential. This operator splitting method is now  [2.x.5449]  accurate. Written in formulas, this yields the following sequence of steps: 

[1.x.2133] 

As before, the first and third step can be computed exactly for this particular equation, yielding 

[1.x.2134] 



This is then how we are going to implement things in this program: In each time step, we execute three steps, namely 

- Update the solution value at each node by analytically integrating   the phase rotation equation by one half time step; 

- Solving the space-time equation that corresponds to the full step   for  [2.x.5450] , namely    [2.x.5451] ,   with initial conditions equal to the solution of the first half step   above. 

- Update the solution value at each node by analytically integrating   the phase rotation equation by another half time step. 

This structure will be reflected in an obvious way in the main time loop of the program. 




[1.x.2135] 

From the discussion above, it should have become clear that the only partial differential equation we have to solve in each time step is 

[1.x.2136] 

This equation is linear. Furthermore, we only have to solve it from  [2.x.5452]  to  [2.x.5453] , i.e., for exactly one time step. 

To do this, we will apply the second order accurate Crank-Nicolson scheme that we have already used in some of the other time dependent codes (specifically: step-23 and step-26). It reads as follows: 

[1.x.2137] 

Here, the "previous" solution  [2.x.5454]  (or the "initial condition" for this part of the time step) is the output of the first phase rotation half-step; the output of the current step will be denoted by  [2.x.5455] .  [2.x.5456]  is the length of the time step. (One could argue whether  [2.x.5457]  and  [2.x.5458]  live at time step  [2.x.5459]  or  [2.x.5460]  and what their upper indices should be. This is a philosophical discussion without practical impact, and one might think of  [2.x.5461]  as something like  [2.x.5462] , and  [2.x.5463]  as  [2.x.5464]  if that helps clarify things -- though, again  [2.x.5465]  is not to be understood as "one third time step after  [2.x.5466] " but more like "we've already done one third of the work necessary for time step  [2.x.5467] ".) 

If we multiply the whole equation with  [2.x.5468]  and sort terms with the unknown  [2.x.5469]  to the left and those with the known  [2.x.5470]  to the right, then we obtain the following (spatial) partial differential equation that needs to be solved in each time step: 

[1.x.2138] 






[1.x.2139] 

As mentioned above, the previous tutorial program dealing with complex-valued solutions (namely, step-29) separated real and imaginary parts of the solution. It thus reduced everything to real arithmetic. In contrast, we here want to keep things complex-valued. 

The first part of this is that we need to define the discretized solution as  [2.x.5471]  where the  [2.x.5472]  are the usual shape functions (which are real valued) but the expansion coefficients  [2.x.5473]  at time step  [2.x.5474]  are now complex-valued. This is easily done in deal.II: We just have to use  [2.x.5475]  instead of Vector<double> to store these coefficients. 

Of more interest is how to build and solve the linear system. Obviously, this will only be necessary for the second step of the Strang splitting discussed above, with the time discretization of the previous subsection. We obtain the fully discrete version through straightforward substitution of  [2.x.5476]  by  [2.x.5477]  and multiplication by a test function: 

[1.x.2140] 

or written in a more compact way: 

[1.x.2141] 

Here, the matrices are defined in their obvious ways: 

[1.x.2142] 

Note that all matrices individually are in fact symmetric, real-valued, and at least positive semidefinite, though the same is obviously not true for the system matrix  [2.x.5478]  and the corresponding matrix  [2.x.5479]  on the right hand side. 




[1.x.2143] 

 [2.x.5480]  

The only remaining important question about the solution procedure is how to solve the complex-valued linear system 

[1.x.2144] 

with the matrix  [2.x.5481]  and a right hand side that is easily computed as the product of a known matrix and the previous part-step's solution. As usual, this comes down to the question of what properties the matrix  [2.x.5482]  has. If it is symmetric and positive definite, then we can for example use the Conjugate Gradient method. 

Unfortunately, the matrix's only useful property is that it is complex symmetric, i.e.,  [2.x.5483] , as is easy to see by recalling that  [2.x.5484]  are all symmetric. It is not, however, [1.x.2145], which would require that  [2.x.5485]  where the bar indicates complex conjugation. 

Complex symmetry can be exploited for iterative solvers as a quick literature search indicates. We will here not try to become too sophisticated (and indeed leave this to the [1.x.2146] section below) and instead simply go with the good old standby for problems without properties: A direct solver. That's not optimal, especially for large problems, but it shall suffice for the purposes of a tutorial program. Fortunately, the SparseDirectUMFPACK class allows solving complex-valued problems. 




[1.x.2147] 

Initial conditions for the NLSE are typically chosen to represent particular physical situations. This is beyond the scope of this program, but suffice it to say that these initial conditions are (i) often superpositions of the wave functions of particles located at different points, and that (ii) because  [2.x.5486]  corresponds to a particle density function, the integral 

[1.x.2148] 

corresponds to the number of particles in the system. (Clearly, if one were to be physically correct,  [2.x.5487]  better be a constant if the system is closed, or  [2.x.5488]  if one has absorbing boundary conditions.) The important point is that one should choose initial conditions so that 

[1.x.2149] 

makes sense. 

What we will use here, primarily because it makes for good graphics, is the following: 

[1.x.2150] 

where  [2.x.5489]  is the distance from the (fixed) locations  [2.x.5490] , and  [2.x.5491]  are chosen so that each of the Gaussians that we are adding up adds an integer number of particles to  [2.x.5492] . We achieve this by making sure that 

[1.x.2151] 

is a positive integer. In other words, we need to choose  [2.x.5493]  as an integer multiple of 

[1.x.2152] 

assuming for the moment that  [2.x.5494]  -- which is of course not the case, but we'll ignore the small difference in integral. 

Thus, we choose  [2.x.5495]  for all, and  [2.x.5496] . This  [2.x.5497]  is small enough that the difference between the exact (infinite) integral and the integral over  [2.x.5498]  should not be too concerning. We choose the four points  [2.x.5499]  as  [2.x.5500]  -- also far enough away from the boundary of  [2.x.5501]  to keep ourselves on the safe side. 

For simplicity, we pose the problem on the square  [2.x.5502] . For boundary conditions, we will use time-independent Neumann conditions of the form 

[1.x.2153] 

This is not a realistic choice of boundary conditions but sufficient for what we want to demonstrate here. We will comment further on this in the [1.x.2154] section below. 

Finally, we choose  [2.x.5503] , and the potential as 

[1.x.2155] 

Using a large potential makes sure that the wave function  [2.x.5504]  remains small outside the circle of radius 0.7. All of the Gaussians that make up the initial conditions are within this circle, and the solution will mostly oscillate within it, with a small amount of energy radiating into the outside. The use of a large potential also makes sure that the nonphysical boundary condition does not have too large an effect. 


examples/step-58/doc/results.dox 



[1.x.2156] 

Running the code results in screen output like the following: ``` Number of active cells: 4096 Number of degrees of freedom: 16641 

Time step 1 at t=0 Time step 2 at t=0.00390625 Time step 3 at t=0.0078125 Time step 4 at t=0.0117188 [...] ``` Running the program also yields a good number of output files that we will visualize in the following. 




[1.x.2157] 

The `output_results()` function of this program generates output files that consist of a number of variables: The solution (split into its real and imaginary parts), the amplitude, and the phase. If we visualize these four fields, we get images like the following after a few time steps (at time  [2.x.5505] , to be precise: 

<div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-58.re.png"          alt="Real part of the solution at t=0.242"          width="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-58.im.png"          alt="Imaginary part of the solution at t=0.242"          width="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-58.magnitude.png"          alt="Amplitude of the solution at t=0.242"          width="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-58.phase.png"          alt="Phase of the solution at t=0.242"          width="400">   </div> </div> 

While the real and imaginary parts of the solution shown above are not particularly interesting (because, from a physical perspective, the global offset of the phase and therefore the balance between real and imaginary components, is meaningless), it is much more interesting to visualize the amplitude  [2.x.5506]  and phase  [2.x.5507]  of the solution and, in particular, their evolution. This leads to pictures like the following: 

The phase picture shown here clearly has some flaws: 

- First, phase is a "cyclic quantity", but the color scale uses a   fundamentally different color for values close to  [2.x.5508]  than   for values close to  [2.x.5509] . This is a nuisance -- what we need   is a "cyclic color map" that uses the same colors for the two   extremes of the range of the phase. Such color maps exist,   see [1.x.2158] or   [1.x.2159], for example. The problem is that the   author's favorite   one of the two big visualization packages, VisIt, does not have any   of these color maps built in. In an act of desperation, I therefore   had to resort to using Paraview given that it has several of the   color maps mentioned in the post above implemented. The picture   below uses the `nic_Edge` map in which both of the extreme values are shown   as black. 

- There is a problem on cells in which the phase wraps around. If   at some evaluation point of the cell the phase value is close to    [2.x.5510]  and at another evaluation point it is close to  [2.x.5511] , then   what we would really like to happen is for the entire cell to have a   color close to the extremes. But, instead, visualization programs   produce a linear interpolation in which the values within the cell,   i.e., between the evaluation points, is linearly interpolated between   these two values, covering essentially the entire range of possible   phase values and, consequently, cycling through the entire   rainbow of colors from dark red to dark green over the course of   one cell. The solution to this problem is to just output   the phase value on each cell as a piecewise constant. Because   averaging values close to the  [2.x.5512]  and  [2.x.5513]  is going to   result in an average that has nothing to do with the actual phase   angle, the `ComplexPhase` class just uses the *maximal* phase   angle encountered on each cell. 

With these modifications, the phase plot now looks as follows: 

<p align="center">   <img src="https://www.dealii.org/images/steps/developer/step-58.phase-cyclic.png"          alt="Phase of the solution at t=0.242, with a cyclic color map"          width="400">  [2.x.5514]  

Finally, we can generate a movie out of this. (To be precise, the video uses two more global refinement cycles and a time step half the size of what is used in the program above.) The author of these lines made the movie with VisIt, because that's what he's more familiar with, and using a hacked color map that is also cyclic -- though this color map lacks all of the skill employed by the people who wrote the posts mentioned in the links above. It does, however, show the character of the solution as a wave equation if you look at the shaded part of the domain outside the circle of radius 0.7 in which the potential is zero -- you can see how every time one of the bumps (showing the amplitude  [2.x.5515] ) bumps into the area where the potential is large: a wave travels outbound from there. Take a look at the video: 

[1.x.2160] 



So why did I end up shading the area where the potential  [2.x.5516]  is large? In that outside region, the solution is relatively small. It is also relatively smooth. As a consequence, to some approximate degree, the equation in that region simplifies to 

[1.x.2161] 

or maybe easier to read: 

[1.x.2162] 

To the degree to which this approximation is valid (which, among other things, eliminates the traveling waves you can see in the video), this equation has a solution 

[1.x.2163] 

Because  [2.x.5517]  is large, this means that the phase *rotates quite rapidly*. If you focus on the semi-transparent outer part of the domain, you can see that. If one colors this region in the same way as the inner part of the domain, this rapidly flashing outer part may be psychedelic, but is also distracting of what's happening on the inside; it's also quite hard to actually see the radiating waves that are easy to see at the beginning of the video. 


[1.x.2164] 

[1.x.2165] 

[1.x.2166] 

The solver chosen here is just too simple. It is also not efficient. What we do here is give the matrix to a sparse direct solver in every time step and let it find the solution of the linear system. But we know that we could do far better: 

- First, we should make use of the fact that the matrix doesn't   actually change from time step to time step. This is an artifact   of the fact that we here have constant boundary values and that   we don't change the time step size -- two assumptions that might   not be true in actual applications. But at least in cases where this   does happen to be the case, it would make sense to only factorize   the matrix once (i.e., compute  [2.x.5518]  and  [2.x.5519]  factors once) and then   use these factors for all following time steps until the matrix    [2.x.5520]  changes and requires a new factorization. The interface of the   SparseDirectUMFPACK class allows for this. 

- Ultimately, however, sparse direct solvers are only efficient for   relatively small problems, say up to a few 100,000 unknowns. Beyond   this, one needs iterative solvers such as the Conjugate Gradient method (for   symmetric and positive definite problems) or GMRES. We have used many   of these in other tutorial programs. In all cases, they need to be   accompanied by good preconditioners. For the current case, one   could in principle use GMRES -- a method that does not require   any specific properties of the matrix -- but would be better   advised to implement an iterative scheme that exploits the one   structural feature we know is true for this problem: That the matrix   is complex-symmetric (albeit not Hermitian). 




[1.x.2167] 

In order to be usable for actual, realistic problems, solvers for the nonlinear Schr&ouml;dinger equation need to utilize boundary conditions that make sense for the problem at hand. We have here restricted ourselves to simple Neumann boundary conditions -- but these do not actually make sense for the problem. Indeed, the equations are generally posed on an infinite domain. But, since we can't compute on infinite domains, we need to truncate it somewhere and instead pose boundary conditions that make sense for this artificially small domain. The approach widely used is to use the [1.x.2168] method that corresponds to a particular kind of attenuation. It is, in a different context, also used in step-62. 




[1.x.2169] 

Finally, we know from experience and many other tutorial programs that it is worthwhile to use adaptively refined meshes, rather than the uniform meshes used here. It would, in fact, not be very difficult to add this here: It just requires periodic remeshing and transfer of the solution from one mesh to the next. step-26 will be a good guide for how this could be implemented. 


examples/step-59/doc/intro.dox 

 [2.x.5521]  

[1.x.2170] 

[1.x.2171] 

[1.x.2172] 

Matrix-free operator evaluation enables very efficient implementations of discretization with high-order polynomial bases due to a method called sum factorization. This concept has been introduced in the step-37 and step-48 tutorial programs. In this tutorial program, we extend those concepts to discontinuous Galerkin (DG) schemes that include face integrals, a class of methods where high orders are particularly widespread. 

The underlying idea of the matrix-free evaluation is the same as for continuous elements: The matrix-vector product that appears in an iterative solver or multigrid smoother is not implemented by a classical sparse matrix kernel, but instead applied implicitly by the evaluation of the underlying integrals on the fly. For tensor product shape functions that are integrated with a tensor product quadrature rule, this evaluation is particularly efficient by using the sum-factorization technique, which decomposes the initially  [2.x.5522]  operations for interpolation involving  [2.x.5523]  vector entries with associated shape functions at degree  [2.x.5524]  in  [2.x.5525]  dimensions to  [2.x.5526]  quadrature points into  [2.x.5527]  one-dimensional operations of cost  [2.x.5528]  each. In 3D, this reduces the order of complexity by two powers in  [2.x.5529] . When measured as the complexity per degree of freedom, the complexity is  [2.x.5530]  in the polynomial degree. Due to the presence of face integrals in DG, and due to the fact that operations on quadrature points involve more memory transfer, which both scale as  [2.x.5531] , the observed complexity is often constant for moderate  [2.x.5532] . This means that a high order method can be evaluated with the same throughput in terms of degrees of freedom per second as a low-order method. 

More information on the algorithms are available in the preprint  [2.x.5533]  [1.x.2173] by Martin Kronbichler and Katharina Kormann, arXiv:1711.03590. 

[1.x.2174] 

For this tutorial program, we exemplify the matrix-free DG framework for the interior penalty discretization of the Laplacian, i.e., the same scheme as the one used for the step-39 tutorial program. The discretization of the Laplacian is given by the following weak form 

[1.x.2175] 

where  [2.x.5534]  denotes the directed jump of the quantity  [2.x.5535]  from the two associated cells  [2.x.5536]  and  [2.x.5537] , and  [2.x.5538]  is the average from both sides. 

The terms in the equation represent the cell integral after integration by parts, the primal consistency term that arises at the element interfaces due to integration by parts and insertion of an average flux, the adjoint consistency term that is added for restoring symmetry of the underlying matrix, and a penalty term with factor  [2.x.5539] , whose magnitude is equal the length of the cells in direction normal to face multiplied by  [2.x.5540] , see step-39. The penalty term is chosen such that an inverse estimate holds and the final weak form is coercive, i.e., positive definite in the discrete setting. The adjoint consistency term and the penalty term involve the jump  [2.x.5541]  at the element interfaces, which disappears for the analytic solution  [2.x.5542] . Thus, these terms are consistent with the original PDE, ensuring that the method can retain optimal orders of convergence. 

In the implementation below, we implement the weak form above by moving the normal vector  [2.x.5543]  from the jump terms to the derivatives to form a [1.x.2176] derivative of the form  [2.x.5544] . This makes the implementation on quadrature points slightly more efficient because we only need to work with scalar terms rather than tensors, and is mathematically equivalent. 

For boundary conditions, we use the so-called mirror principle that defines [1.x.2177] exterior values  [2.x.5545]  by extrapolation from the interior solution  [2.x.5546]  combined with the given boundary data, setting  [2.x.5547]  and  [2.x.5548]  on Dirichlet boundaries and  [2.x.5549]  and  [2.x.5550]  on Neumann boundaries, for given Dirichlet values  [2.x.5551]  and Neumann values  [2.x.5552] . These expressions are then inserted in the above weak form. Contributions involving the known quantities  [2.x.5553]  and  [2.x.5554]  are eventually moved to the right hand side, whereas the unknown value  [2.x.5555]  is retained on the left hand side and contributes to the matrix terms similarly as interior faces. Upon these manipulations, the same weak form as in step-39 is obtained. 

[1.x.2178] 

The matrix-free framework of deal.II provides the necessary infrastructure to implement the action of the discretized equation above. As opposed to the  [2.x.5556]  that we used in step-37 and step-48, we now build a code in terms of  [2.x.5557]  that takes three function pointers, one for the cell integrals, one for the inner face integrals, and one for the boundary face integrals (in analogy to the design of MeshWorker used in the step-39 tutorial program). In each of these three functions, we then implement the respective terms on the quadrature points. For interpolation between the vector entries and the values and gradients on quadrature points, we use the class FEEvaluation for cell contributions and FEFaceEvaluation for face contributions. The basic usage of these functions has been discussed extensively in the step-37 tutorial program. 

In  [2.x.5558]  all interior faces are visited exactly once, so one must make sure to compute the contributions from both the test functions  [2.x.5559]  and  [2.x.5560] . Given the fact that the test functions on both sides are indeed independent, the weak form above effectively means that we submit the same contribution to both an FEFaceEvaluation object called `phi_inner` and `phi_outer` for testing with the normal derivative of the test function, and values with opposite sign for testing with the values of the test function, because the latter involves opposite signs due to the jump term. For faces between cells of different refinement level, the integration is done from the refined side, and FEFaceEvaluation automatically performs interpolation to a subface on the coarse side. Thus, a hanging node never appears explicitly in a user implementation of a weak form. 

The fact that each face is visited exactly once also applies to those faces at subdomain boundaries between different processors when parallelized with MPI, where one cell belongs to one processor and one to the other. The setup in  [2.x.5561]  splits the faces between the two sides, and eventually only reports the faces actually handled locally in  [2.x.5562]  and  [2.x.5563]  respectively. Note that, in analogy to the cell integrals discussed in step-37, deal.II applies vectorization over several faces to use SIMD, working on something we call a [1.x.2179] with a single instruction. The face batches are independent from the cell batches, even though the time at which face integrals are processed is kept close to the time when the cell integrals of the respective cells are processed, in order to increase the data locality. 

Another thing that is new in this program is the fact that we no longer split the vector access like  [2.x.5564]  or  [2.x.5565]  from the evaluation and integration steps, but call combined functions  [2.x.5566]  and  [2.x.5567]  respectively. This is useful for face integrals because, depending on what gets evaluated on the faces, not all vector entries of a cell must be touched in the first place. Think for example of the case of the nodal element FE_DGQ with node points on the element surface: If we are interested in the shape function values on a face, only  [2.x.5568]  degrees of freedom contribute to them in a non-trivial way (in a more technical way of speaking, only  [2.x.5569]  shape functions have a nonzero support on the face and return true for  [2.x.5570]  When compared to the  [2.x.5571]  degrees of freedom of a cell, this is one power less. 

Now of course we are not interested in only the function values, but also the derivatives on the cell. Fortunately, there is an element in deal.II that extends this property of reduced access also for derivatives on faces, the FE_DGQHermite element. 

[1.x.2180] 

The element FE_DGQHermite belongs to the family of FE_DGQ elements, i.e., its shape functions are a tensor product of 1D polynomials and the element is fully discontinuous. As opposed to the nodal character in the usual FE_DGQ element, the FE_DGQHermite element is a mixture of nodal contributions and derivative contributions based on a Hermite-like concept. The underlying polynomial class is  [2.x.5572]  and can be summarized as follows: For cubic polynomials, we use two polynomials to represent the function value and first derivative at the left end of the unit interval,  [2.x.5573] , and two polynomials to represent the function value and first derivative and the right end of the unit interval,  [2.x.5574] . At the opposite ends, both the value and first derivative of the shape functions are zero, ensuring that only two out of the four basis functions contribute to values and derivative on the respective end. However, we deviate from the classical Hermite interpolation in not strictly assigning one degree of freedom for the value and one for the first derivative, but rather allow the first derivative to be a linear combination of the first and the second shape function. This is done to improve the conditioning of the interpolation. Also, when going to degrees beyond three, we add node points in the element interior in a Lagrange-like fashion, combined with double zeros in the points  [2.x.5575]  and  [2.x.5576] . The position of these extra nodes is determined by the zeros of some Jacobi polynomials as explained in the description of the class  [2.x.5577]  

Using this element, we only need to access  [2.x.5578]  degrees of freedom for computing both values and derivatives on a face. The check whether the Hermite property is fulfilled is done transparently inside  [2.x.5579]  and  [2.x.5580]  that check the type of the basis and reduce the access to data if possible. Obviously, this would not be possible if we had separated  [2.x.5581]  from  [2.x.5582]  because the amount of entries we need to read depends on the type of the derivative (only values, first derivative, etc.) and thus must be given to `read_dof_values()`. 

This optimization is not only useful for computing the face integrals, but also for the MPI ghost layer exchange: In a naive exchange, we would need to send all degrees of freedom of a cell to another processor if the other processor is responsible for computing the face's contribution. Since we know that only some of the degrees of freedom in the evaluation with FEFaceEvaluation are touched, it is natural to only exchange the relevant ones. The  [2.x.5583]  function has support for a selected data exchange when combined with  [2.x.5584]  To make this happen, we need to tell the loop what kind of evaluation on faces we are going to do, using an argument of type  [2.x.5585]  as can be seen in the implementation of  [2.x.5586]  below. The way data is exchanged in that case is as follows: The ghost layer data in the vector still pretends to represent all degrees of freedom, such that FEFaceEvaluation can continue to read the values as if the cell were a locally owned one. The data exchange routines take care of the task for packing and unpacking the data into this format. While this sounds pretty complicated, we will show in the results section below that this really pays off by comparing the performance to a baseline code that does not specify the data access on faces. 

[1.x.2181] 

In the tradition of the step-37 program, we again solve a Poisson problem with a geometric multigrid preconditioner inside a conjugate gradient solver. Instead of computing the diagonal and use the basic PreconditionChebyshev as a smoother, we choose a different strategy in this tutorial program. We implement a block-Jacobi preconditioner, where a block refers to all degrees of freedom on a cell. Rather than building the full cell matrix and applying its LU factorization (or inverse) in the preconditioner &mdash; an operation that would be heavily memory bandwidth bound and thus pretty slow &mdash; we approximate the inverse of the block by a special technique called fast diagonalization method. 

The idea of the method is to take use of the structure of the cell matrix. In case of the Laplacian with constant coefficients discretized on a Cartesian mesh, the cell matrix  [2.x.5587]  can be written as 

[1.x.2182] 

in 2D and 

[1.x.2183] 

in 3D. The matrices  [2.x.5588]  and  [2.x.5589]  denote the 1D Laplace matrix (including the cell and face term associated to the current cell values  [2.x.5590]  and  [2.x.5591] ) and  [2.x.5592]  and  [2.x.5593]  are the mass matrices. Note that this simple tensor product structure is lost once there are non-constant coefficients on the cell or the geometry is not constant any more. We mention that a similar setup could also be used to replace the computed integrals with this final tensor product form of the matrices, which would cut the operations for the operator evaluation into less than half. However, given the fact that this only holds for Cartesian cells and constant coefficients, which is a pretty narrow case, we refrain from pursuing this idea. 

Interestingly, the exact inverse of the matrix  [2.x.5594]  can be found through tensor products due to a method introduced by [1.x.2184] from 1964, 

[1.x.2185] 

where  [2.x.5595]  is the matrix of eigenvectors to the generalized eigenvalue problem in the given tensor direction  [2.x.5596] : 

[1.x.2186] 

and  [2.x.5597]  is the diagonal matrix representing the generalized eigenvalues  [2.x.5598] . Note that the vectors  [2.x.5599]  are such that they simultaneously diagonalize  [2.x.5600]  and  [2.x.5601] , i.e.  [2.x.5602]  and  [2.x.5603] . 

The deal.II library implements a class using this concept, called TensorProductMatrixSymmetricSum. 

For the sake of this program, we stick with constant coefficients and Cartesian meshes, even though an approximate version based on tensor products would still be possible for a more general mesh, and the operator evaluation itself is of course generic. Also, we do not bother with adaptive meshes where the multigrid algorithm would need to get access to flux matrices over the edges of different refinement, as explained in step-39. One thing we do, however, is to still wrap our block-Jacobi preconditioner inside PreconditionChebyshev. That class relieves us from finding an appropriate relaxation parameter (which would be around 0.7 in 2D and 0.5 in 3D for the block-Jacobi smoother), and often increases smoothing efficiency a bit over plain Jacobi smoothing in that it enables lower the time to solution when setting the degree of the Chebyshev polynomial to one or two. 

Note that the block-Jacobi smoother has an additional benefit: The fast diagonalization method can also be interpreted as a change from the Hermite-like polynomials underlying FE_DGQHermite to a basis where the cell Laplacian is diagonal. Thus, it cancels the effect of the basis, and we get the same iteration counts irrespective of whether we use FE_DGQHermite or FE_DGQ. This is in contrast to using the PreconditionChebyshev class with only the diagonal (a point-Jacobi scheme), where FE_DGQ and FE_DGQHermite do indeed behave differently and FE_DGQ needs 2-5 less iterations than FE_DGQHermite, despite the modification made to the Hermite-like shape functions to ensure a good conditioning. 


examples/step-59/doc/results.dox 



[1.x.2187] 

[1.x.2188] 

Like in step-37, we evaluate the multigrid solver in terms of run time.  In two space dimensions with elements of degree 8, a possible output could look as follows: 

[1.x.2189] 



Like in step-37, the number of CG iterations remains constant with increasing problem size. The iteration counts are a bit higher, which is because we use a lower degree of the Chebyshev polynomial (2 vs 5 in step-37) and because the interior penalty discretization has a somewhat larger spread in eigenvalues. Nonetheless, 13 iterations to reduce the residual by 12 orders of magnitude, or almost a factor of 9 per iteration, indicates an overall very efficient method. In particular, we can solve a system with 21 million degrees of freedom in 5 seconds when using 12 cores, which is a very good efficiency. Of course, in 2D we are well inside the regime of roundoff for a polynomial degree of 8 &ndash; as a matter of fact, around 83k DoFs or 0.025s would have been enough to fully converge this (simple) analytic solution here. 

Not much changes if we run the program in three spatial dimensions, except for the fact that we now use do something more useful with the higher polynomial degree and increasing mesh sizes, as the roundoff errors are only obtained at the finest mesh. Still, it is remarkable that we can solve a 3D Laplace problem with a wave of three periods to roundoff accuracy on a twelve-core machine pretty easily - using about 3.5 GB of memory in total for the second to largest case with 24m DoFs, taking not more than eight seconds. The largest case uses 30GB of memory with 191m DoFs. 

[1.x.2190] 



[1.x.2191] 

In the introduction and in-code comments, it was mentioned several times that high orders are treated very efficiently with the FEEvaluation and FEFaceEvaluation evaluators. Now, we want to substantiate these claims by looking at the throughput of the 3D multigrid solver for various polynomial degrees. We collect the times as follows: We first run a solver at problem size close to ten million, indicated in the first four table rows, and record the timings. Then, we normalize the throughput by recording the number of million degrees of freedom solved per second (MDoFs/s) to be able to compare the efficiency of the different degrees, which is computed by dividing the number of degrees of freedom by the solver time. 

 [2.x.5604]  

We clearly see how the efficiency per DoF initially improves until it reaches a maximum for the polynomial degree  [2.x.5605] . This effect is surprising, not only because higher polynomial degrees often yield a vastly better solution, but especially also when having matrix-based schemes in mind where the denser coupling at higher degree leads to a monotonously decreasing throughput (and a drastic one in 3D, with  [2.x.5606]  being more than ten times slower than  [2.x.5607] !). For higher degrees, the throughput decreases a bit, which is both due to an increase in the number of iterations (going from 12 at  [2.x.5608]  to 19 at  [2.x.5609] ) and due to the  [2.x.5610]  complexity of operator evaluation. Nonetheless, efficiency as the time to solution would be still better for higher polynomial degrees because they have better convergence rates (at least for problems as simple as this one): For  [2.x.5611] , we reach roundoff accuracy already with 1 million DoFs (solver time less than a second), whereas for  [2.x.5612]  we need 24 million DoFs and 8 seconds. For  [2.x.5613] , the error is around  [2.x.5614]  with 57m DoFs and thus still far away from roundoff, despite taking 16 seconds. 

Note that the above numbers are a bit pessimistic because they include the time it takes the Chebyshev smoother to compute an eigenvalue estimate, which is around 10 percent of the solver time. If the system is solved several times (as e.g. common in fluid dynamics), this eigenvalue cost is only paid once and faster times become available. 

[1.x.2192] 

Finally, we take a look at some of the special ingredients presented in this tutorial program, namely the FE_DGQHermite basis in particular and the specification of  [2.x.5615]  In the following table, the third row shows the optimized solver above, the fourth row shows the timings with only the  [2.x.5616]  set to `unspecified` rather than the optimal `gradients`, and the last one with replacing FE_DGQHermite by the basic FE_DGQ elements where both the MPI exchange are more expensive and the operations done by  [2.x.5617]  and  [2.x.5618]  

 [2.x.5619]  

The data in the table shows that not using  [2.x.5620]  increases costs by around 10% for higher polynomial degrees. For lower degrees, the difference is obviously less pronounced because the volume-to-surface ratio is more beneficial and less data needs to be exchanged. The difference is larger when looking at the matrix-vector product only, rather than the full multigrid solver shown here, with around 20% worse timings just because of the MPI communication. 

For  [2.x.5621]  and  [2.x.5622] , the Hermite-like basis functions do obviously not really pay off (indeed, for  [2.x.5623]  the polynomials are exactly the same as for FE_DGQ) and the results are similar as with the FE_DGQ basis. However, for degrees starting at three, we see an increasing advantage for FE_DGQHermite, showing the effectiveness of these basis functions. 

[1.x.2193] 

As mentioned in the introduction, the fast diagonalization method is tied to a Cartesian mesh with constant coefficients. If we wanted to solve variable-coefficient problems, we would need to invest a bit more time in the design of the smoother parameters by selecting proper generalizations (e.g., approximating the inverse on the nearest box-shaped element). 

Another way of extending the program would be to include support for adaptive meshes, for which interface operations at edges of different refinement level become necessary, as discussed in step-39. 


examples/step-6/doc/intro.dox 

[1.x.2194] 

[1.x.2195] 

 [2.x.5624]  

This program is finally about one of the main features of deal.II: the use of adaptively (locally) refined meshes. The program is still based on step-4 and step-5, and, as you will see, it does not actually take very much code to enable adaptivity. Indeed, while we do a great deal of explaining, adaptive meshes can be added to an existing program with barely a dozen lines of additional code. The program shows what these lines are, as well as another important ingredient of adaptive mesh refinement (AMR): a criterion that can be used to determine whether it is necessary to refine a cell because the error is large on it, whether the cell can be coarsened because the error is particularly small on it, or whether we should just leave the cell as it is. We will discuss all of these issues in the following. 




[1.x.2196] 

There are a number of ways how one can adaptively refine meshes. The basic structure of the overall algorithm is always the same and consists of a loop over the following steps: 

- Solve the PDE on the current mesh; 

- Estimate the error on each cell using some criterion that is indicative   of the error; 

- Mark those cells that have large errors for refinement, mark those that have   particularly small errors for coarsening, and leave the rest alone; 

- Refine and coarsen the cells so marked to obtain a new mesh; 

- Repeat the steps above on the new mesh until the overall error is   sufficiently small. 

For reasons that are probably lost to history (maybe that these functions used to be implemented in FORTRAN, a language that does not care about whether something is spelled in lower or UPPER case letters, with programmers often choosing upper case letters habitually), the loop above is often referenced in publications about mesh adaptivity as the SOLVE-ESTIMATE-MARK-REFINE loop (with this spelling). 

Beyond this structure, however, there are a variety of ways to achieve this. Fundamentally, they differ in how exactly one generates one mesh from the previous one. 

If one were to use triangles (which deal.II does not do), then there are two essential possibilities: 

- Longest-edge refinement: In this strategy, a triangle marked for refinement   is cut into two by introducing one new edge from the midpoint of the longest   edge to the opposite vertex. Of course, the midpoint from the longest edge   has to somehow be balanced by *also* refining the cell on the other side of   that edge (if there is one). If the edge in question is also the longest   edge of the neighboring cell, then we can just run a new edge through the   neighbor to the opposite vertex; otherwise a slightly more involved   construction is necessary that adds more new vertices on at least one   other edge of the neighboring cell, and then may propagate to the neighbors   of the neighbor until the algorithm terminates. This is hard to describe   in words, and because deal.II does not use triangles not worth the time here.   But if you're curious, you can always watch video lecture 15 at the link   shown at the top of this introduction. 

- Red-green refinement: An alternative is what is called "red-green refinement".   This strategy is even more difficult to describe (but also discussed in the   video lecture) and has the advantage that the refinement does not propagate   beyond the immediate neighbors of the cell that we want to refine. It is,   however, substantially more difficult to implement. 

There are other variations of these approaches, but the important point is that they always generate a mesh where the lines where two cells touch are entire edges of both adjacent cells. With a bit of work, this strategy is readily adapted to three-dimensional meshes made from tetrahedra. 

Neither of these methods works for quadrilaterals in 2d and hexahedra in 3d, or at least not easily. The reason is that the transition elements created out of the quadrilateral neighbors of a quadrilateral cell that is to be refined would be triangles, and we don't want this. Consequently, the approach to adaptivity chosen in deal.II is to use grids in which neighboring cells may differ in refinement level by one. This then results in nodes on the interfaces of cells which belong to one side, but are unbalanced on the other. The common term for these is &ldquo;hanging nodes&rdquo;, and these meshes then look like this in a very simple situation: 

 [2.x.5625]  

A more complicated two-dimensional mesh would look like this (and is discussed in the "Results" section below): 

<img src="https://www.dealii.org/images/steps/developer/step_6_grid_5_ladutenko.svg"      alt="Fifth adaptively refined Ladutenko grid: the cells are clustered           along the inner circle."      width="300" height="300"> 

Finally, a three-dimensional mesh (from step-43) with such hanging nodes is shown here: 

<img src="https://www.dealii.org/images/steps/developer/step-43.3d.mesh.png" alt=""      width="300" height="300"> 

The first and third mesh are of course based on a square and a cube, but as the second mesh shows, this is not necessary. The important point is simply that we can refine a mesh independently of its neighbors (subject to the constraint that a cell can be only refined once more than its neighbors), but that we end up with these &ldquo;hanging nodes&rdquo; if we do this. 




[1.x.2197] 

Now that you have seen what these adaptively refined meshes look like, you should ask [1.x.2198] we would want to do this. After all, we know from theory that if we refine the mesh globally, the error will go down to zero as 

[1.x.2199] 

where  [2.x.5626]  is some constant independent of  [2.x.5627]  and  [2.x.5628] ,  [2.x.5629]  is the polynomial degree of the finite element in use, and  [2.x.5630]  is the diameter of the largest cell. So if the [1.x.2200] cell is important, then why would we want to make the mesh fine in some parts of the domain but not all? 

The answer lies in the observation that the formula above is not optimal. In fact, some more work shows that the following is a better estimate (which you should compare to the square of the estimate above): 

[1.x.2201] 

(Because  [2.x.5631] , this formula immediately implies the previous one if you just pull the mesh size out of the sum.) What this formula suggests is that it is not necessary to make the [1.x.2202] cell small, but that the cells really only need to be small [1.x.2203]! In other words: The mesh really only has to be fine where the solution has large variations, as indicated by the  [2.x.5632] st derivative. This makes intuitive sense: if, for example, we use a linear element  [2.x.5633] , then places where the solution is nearly linear (as indicated by  [2.x.5634]  being small) will be well resolved even if the mesh is coarse. Only those places where the second derivative is large will be poorly resolved by large elements, and consequently that's where we should make the mesh small. 

Of course, this [1.x.2204] is not very useful in practice since we don't know the exact solution  [2.x.5635]  of the problem, and consequently, we cannot compute  [2.x.5636] . But, and that is the approach commonly taken, we can compute numerical approximations of  [2.x.5637]  based only on the discrete solution  [2.x.5638]  that we have computed before. We will discuss this in slightly more detail below. This will then help us determine which cells have a large  [2.x.5639] st derivative, and these are then candidates for refining the mesh. 




[1.x.2205] 

The methods using triangular meshes mentioned above go to great lengths to make sure that each vertex is a vertex of all adjacent cells -- i.e., that there are no hanging nodes. This then automatically makes sure that we can define shape functions in such a way that they are globally continuous (if we use the common  [2.x.5640]  Lagrange finite element methods we have been using so far in the tutorial programs, as represented by the FE_Q class). 

On the other hand, if we define shape functions on meshes with hanging nodes, we may end up with shape functions that are not continuous. To see this, think about the situation above where the top right cell is not refined, and consider for a moment the use of a bilinear finite element. In that case, the shape functions associated with the hanging nodes are defined in the obvious way on the two small cells adjacent to each of the hanging nodes. But how do we extend them to the big adjacent cells? Clearly, the function's extension to the big cell cannot be bilinear because then it needs to be linear along each edge of the large cell, and that means that it needs to be zero on the entire edge because it needs to be zero on the two vertices of the large cell on that edge. But it is not zero at the hanging node itself when seen from the small cells' side -- so it is not continuous. The following three figures show three of the shape functions along the edges in question that turn out to not be continuous when defined in the usual way simply based on the cells they are adjacent to: 

<div class="threecolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">        [2.x.5641]      </div>   </div>   <div class="parent">     <div class="img" align="center">        [2.x.5642]      </div>   </div>   <div class="parent">     <div class="img" align="center">        [2.x.5643]      </div>   </div> </div> 


But we do want the finite element solution to be continuous so that we have a &ldquo;conforming finite element method&rdquo; where the discrete finite element space is a proper subset of the  [2.x.5644]  function space in which we seek the solution of the Laplace equation. To guarantee that the global solution is continuous at these nodes as well, we have to state some additional constraints on the values of the solution at these nodes. The trick is to realize that while the shape functions shown above are discontinuous (and consequently an [1.x.2206] linear combination of them is also discontinuous), that linear combinations in which the shape functions are added up as  [2.x.5645]  can be continuous [1.x.2207]. In other words, the coefficients  [2.x.5646]  can not be chosen arbitrarily but have to satisfy certain constraints so that the function  [2.x.5647]  is in fact continuous. What these constraints have to look is relatively easy to understand conceptually, but the implementation in software is complicated and takes several thousand lines of code. On the other hand, in user code, it is only about half a dozen lines you have to add when dealing with hanging nodes. 

In the program below, we will show how we can get these constraints from deal.II, and how to use them in the solution of the linear system of equations. Before going over the details of the program below, you may want to take a look at the  [2.x.5648]  documentation module that explains how these constraints can be computed and what classes in deal.II work on them. 




[1.x.2208] 

The practice of hanging node constraints is rather simpler than the theory we have outlined above. In reality, you will really only have to add about half a dozen lines of additional code to a program like step-4 to make it work with adaptive meshes that have hanging nodes. The interesting part about this is that it is entirely independent of the equation you are solving: The algebraic nature of these constraints has nothing to do with the equation and only depends on the choice of finite element. As a consequence, the code to deal with these constraints is entirely contained in the deal.II library itself, and you do not need to worry about the details. 

The steps you need to make this work are essentially like this: 

- You have to create an AffineConstraints object, which (as the name   suggests) will store all constraints on the finite element space. In   the current context, these are the constraints due to our desire to   keep the solution space continuous even in the presence of hanging   nodes. (Below we will also briefly mention that we will also put   boundary values into this same object, but that is a separate matter.) 

- You have to fill this object using the function    [2.x.5649]  to ensure continuity of   the elements of the finite element space. 

- You have to use this object when you copy the local contributions to   the matrix and right hand side into the global objects, by using    [2.x.5650]  Up until   now, we have done this ourselves, but now with constraints, this   is where the magic happens and we apply the constraints to the   linear system. What this function does is make sure that the   degrees of freedom located at hanging nodes are not, in fact,   really free. Rather, they are factually eliminated from the   linear system by setting their rows and columns to zero and putting   something on the diagonal to ensure the matrix remains invertible.   The matrix resulting from this process remains symmetric and   positive definite for the Laplace equation we solve here, so we can   continue to use the Conjugate Gradient method for it. 

- You then solve the linear system as usual, but at the end of this   step, you need to make sure that the degrees of "freedom" located   on hanging nodes get their correct (constrained) value so that the   solution you then visualize or evaluate in other ways is in   fact continuous. This is done by calling    [2.x.5651]  immediately after solving. 

These four steps are really all that is necessary -- it's that simple from a user perspective. The fact that, in the function calls mentioned above, you will run through several thousand lines of not-so-trivial code is entirely immaterial to this: In user code, there are really only four additional steps. 




[1.x.2209] 

The next question, now that we know how to [1.x.2210] with meshes that have these hanging nodes is how we [1.x.2211] them. 

A simple way has already been shown in step-1: If you [1.x.2212] where it is necessary to refine the mesh, then you can create one by hand. But in reality, we don't know this: We don't know the solution of the PDE up front (because, if we did, we wouldn't have to use the finite element method), and consequently we do not know where it is necessary to add local mesh refinement to better resolve areas where the solution has strong variations. But the discussion above shows that maybe we can get away with using the discrete solution  [2.x.5652]  on one mesh to estimate the derivatives  [2.x.5653] , and then use this to determine which cells are too large and which already small enough. We can then generate a new mesh from the current one using local mesh refinement. If necessary, this step is then repeated until we are happy with our numerical solution -- or, more commonly, until we run out of computational resources or patience. 

So that's exactly what we will do. The locally refined grids are produced using an [1.x.2213] which estimates the energy error for numerical solutions of the Laplace operator. Since it was developed by Kelly and co-workers, we often refer to it as the &ldquo;Kelly refinement indicator&rdquo; in the library, documentation, and mailing list. The class that implements it is called KellyErrorEstimator, and there is a great deal of information to be found in the documentation of that class that need not be repeated here. The summary, however, is that the class computes a vector with as many entries as there are  [2.x.5654]  "active cells", and where each entry contains an estimate of the error on that cell. This estimate is then used to refine the cells of the mesh: those cells that have a large error will be marked for refinement, those that have a particularly small estimate will be marked for coarsening. We don't have to do this by hand: The functions in namespace GridRefinement will do all of this for us once we have obtained the vector of error estimates. 

It is worth noting that while the Kelly error estimator was developed for Laplace's equation, it has proven to be a suitable tool to generate locally refined meshes for a wide range of equations, not even restricted to elliptic only problems. Although it will create non-optimal meshes for other equations, it is often a good way to quickly produce meshes that are well adapted to the features of solutions, such as regions of great variation or discontinuities. 




[1.x.2214] 

It turns out that one can see Dirichlet boundary conditions as just another constraint on the degrees of freedom. It's a particularly simple one, indeed: If  [2.x.5655]  is a degree of freedom on the boundary, with position  [2.x.5656] , then imposing the boundary condition  [2.x.5657]  on  [2.x.5658]  simply yields the constraint  [2.x.5659] . 

The AffineConstraints class can handle such constraints as well, which makes it convenient to let the same object we use for hanging node constraints also deal with these Dirichlet boundary conditions. This way, we don't need to apply the boundary conditions after assembly (like we did in the earlier steps). All that is necessary is that we call the variant of  [2.x.5660]  that returns its information in an AffineConstraints object, rather than the  [2.x.5661]  we have used in previous tutorial programs. 


 [1.x.2215] 


Since the concepts used for locally refined grids are so important, we do not show much other material in this example. The most important exception is that we show how to use biquadratic elements instead of the bilinear ones which we have used in all previous examples. In fact, the use of higher order elements is accomplished by only replacing three lines of the program, namely the initialization of the  [2.x.5662]  member variable in the constructor of the main class of this program, and the use of an appropriate quadrature formula in two places. The rest of the program is unchanged. 

The only other new thing is a method to catch exceptions in the  [2.x.5663]  function in order to output some information in case the program crashes for some reason. This is discussed below in more detail. 


examples/step-6/doc/results.dox 



[1.x.2216] 


The output of the program looks as follows: 

[1.x.2217] 






As intended, the number of cells roughly doubles in each cycle. The number of degrees is slightly more than four times the number of cells; one would expect a factor of exactly four in two spatial dimensions on an infinite grid (since the spacing between the degrees of freedom is half the cell width: one additional degree of freedom on each edge and one in the middle of each cell), but it is larger than that factor due to the finite size of the mesh and due to additional degrees of freedom which are introduced by hanging nodes and local refinement. 




The program outputs the solution and mesh in each cycle of the refinement loop. The solution looks as follows: 

 [2.x.5664]  

It is interesting to follow how the program arrives at the final mesh: 

<div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_0.svg"          alt="Initial grid: the five-cell circle grid with one global refinement."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_1.svg"          alt="First grid: the five-cell circle grid with two global refinements."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_2.svg"          alt="Second grid: the five-cell circle grid with one adaptive refinement."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_3.svg"          alt="Third grid: the five-cell circle grid with two adaptive          refinements, showing clustering around the inner circle."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_4.svg"          alt="Fourth grid: the five-cell circle grid with three adaptive          refinements, showing clustering around the inner circle."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_5.svg"          alt="Fifth grid: the five-cell circle grid with four adaptive          refinements, showing clustering around the inner circle."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_6.svg"          alt="Sixth grid: the five-cell circle grid with five adaptive          refinements, showing clustering around the inner circle."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_7.svg"          alt="Last grid: the five-cell circle grid with six adaptive          refinements, showing that most cells are clustered around the inner circle."          width="300" height="300">   </div> </div> 


It is clearly visible that the region where the solution has a kink, i.e. the circle at radial distance 0.5 from the center, is refined most. Furthermore, the central region where the solution is very smooth and almost flat, is almost not refined at all, but this results from the fact that we did not take into account that the coefficient is large there. The region outside is refined rather arbitrarily, since the second derivative is constant there and refinement is therefore mostly based on the size of the cells and their deviation from the optimal square. 




[1.x.2218] 

[1.x.2219] 

[1.x.2220] 


One thing that is always worth playing around with if one solves problems of appreciable size (much bigger than the one we have here) is to try different solvers or preconditioners. In the current case, the linear system is symmetric and positive definite, which makes the CG algorithm pretty much the canonical choice for solving. However, the SSOR preconditioner we use in the  [2.x.5665]  function is up for grabs. 

In deal.II, it is relatively simple to change the preconditioner. For example, by changing the existing lines of code 

[1.x.2221] 

into 

[1.x.2222] 

we can try out different relaxation parameters for SSOR. By using 

[1.x.2223] 

we can use Jacobi as a preconditioner. And by using 

[1.x.2224] 

we can use a simple incomplete LU decomposition without any thresholding or strengthening of the diagonal (to use this preconditioner, you have to also add the header file  [2.x.5666]  to the include list at the top of the file). 

Using these various different preconditioners, we can compare the number of CG iterations needed (available through the  [2.x.5667]  call, see step-4) as well as CPU time needed (using the Timer class, discussed, for example, in step-28) and get the following results (left: iterations; right: CPU time): 

 [2.x.5668]  

As we can see, all preconditioners behave pretty much the same on this simple problem, with the number of iterations growing like  [2.x.5669]  and because each iteration requires around  [2.x.5670]  operations the total CPU time grows like  [2.x.5671]  (for the few smallest meshes, the CPU time is so small that it doesn't record). Note that even though it is the simplest method, Jacobi is the fastest for this problem. 

The situation changes slightly when the finite element is not a bi-quadratic one as set in the constructor of this program, but a bi-linear one. If one makes this change, the results are as follows: 

 [2.x.5672]  

In other words, while the increase in iterations and CPU time is as before, Jacobi is now the method that requires the most iterations; it is still the fastest one, however, owing to the simplicity of the operations it has to perform. This is not to say that Jacobi is actually a good preconditioner -- for problems of appreciable size, it is definitely not, and other methods will be substantially better -- but really only that it is fast because its implementation is so simple that it can compensate for a larger number of iterations. 

The message to take away from this is not that simplicity in preconditioners is always best. While this may be true for the current problem, it definitely is not once we move to more complicated problems (elasticity or Stokes, for examples step-8 or step-22). Secondly, all of these preconditioners still lead to an increase in the number of iterations as the number  [2.x.5673]  of degrees of freedom grows, for example  [2.x.5674] ; this, in turn, leads to a total growth in effort as  [2.x.5675]  since each iteration takes  [2.x.5676]  work. This behavior is undesirable: we would really like to solve linear systems with  [2.x.5677]  unknowns in a total of  [2.x.5678]  work; there is a class of preconditioners that can achieve this, namely geometric (step-16, step-37, step-39) or algebraic multigrid (step-31, step-40, and several others) preconditioners. They are, however, significantly more complex than the preconditioners outlined above. 

Finally, the last message to take home is that when the data shown above was generated (in 2018), linear systems with 100,000 unknowns are easily solved on a desktop machine in about a second, making the solution of relatively simple 2d problems even to very high accuracy not that big a task as it used to be even in the past. At the time, the situation for 3d problems was entirely different, but even that has changed substantially in the intervening time -- though solving problems in 3d to high accuracy remains a challenge. 




[1.x.2225] 

If you look at the meshes above, you will see even though the domain is the unit disk, and the jump in the coefficient lies along a circle, the cells that make up the mesh do not track this geometry well. The reason, already hinted at in step-1, is that in the absence of other information, the Triangulation class only sees a bunch of coarse grid cells but has, of course, no real idea what kind of geometry they might represent when looked at together. For this reason, we need to tell the Triangulation what to do when a cell is refined: where should the new vertices at the edge midpoints and the cell midpoint be located so that the child cells better represent the desired geometry than the parent cell. 

To visualize what the triangulation actually knows about the geometry, it is not enough to just output the location of vertices and draw a straight line for each edge; instead, we have to output both interior and boundary lines as multiple segments so that they look curved. We can do this by making one change to the gnuplot part of  [2.x.5679] : 

[1.x.2226] 



In the code above, we already do this for faces that sit at the boundary: this happens automatically since we use  [2.x.5680]  which attaches a SphericalManifold to the boundary of the domain. To make the mesh [1.x.2227] also track a circular domain, we need to work a bit harder, though. First, recall that our coarse mesh consists of a central square cell and four cells around it. Now first consider what would happen if we also attached the SphericalManifold object not only to the four exterior faces but also the four cells at the perimeter as well as all of their faces. We can do this by adding the following snippet (testing that the center of a cell is larger than a small multiple, say one tenth, of the cell diameter away from center of the mesh only fails for the central square of the mesh): 

[1.x.2228] 



After a few global refinement steps, this would lead to a mesh of the following kind: 


  <div class="onecolumn" style="width: 80%">     <div>       <img src="https://www.dealii.org/images/steps/developer/step_6_bad_grid_4.svg"            alt="Grid where some central cells are nearly triangular."            width="300" height="300">     </div>   </div> 

This is not a good mesh: the central cell has been refined in such a way that the children located in the four corners of the original central cell [1.x.2229]: they all tend towards triangles as mesh refinement continues. This means that the Jacobian matrix of the transformation from reference cell to actual cell degenerates for these cells, and because all error estimates for finite element solutions contain the norm of the inverse of the Jacobian matrix, you will get very large errors on these cells and, in the limit as mesh refinement, a loss of convergence order because the cells in these corners become worse and worse under mesh refinement. 

So we need something smarter. To this end, consider the following solution originally developed by Konstantin Ladutenko. We will use the following code: 

[1.x.2230] 



This code then generates the following, much better sequence of meshes: 

<div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_0_ladutenko.svg"          alt="Initial grid: the Ladutenko grid with one global refinement."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_1_ladutenko.svg"          alt="First adaptively refined Ladutenko grid."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_2_ladutenko.svg"          alt="Second adaptively refined Ladutenko grid."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_3_ladutenko.svg"          alt="Third adaptively refined Ladutenko grid."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_4_ladutenko.svg"          alt="Fourth adaptively refined Ladutenko grid. The cells are clustered          along the inner circle."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_5_ladutenko.svg"          alt="Fifth adaptively refined Ladutenko grid: the cells are clustered          along the inner circle."          width="300" height="300">   </div> </div> 

Creating good meshes, and in particular making them fit the geometry you want, is a complex topic in itself. You can find much more on this in step-49, step-53, and step-54, among other tutorial programs that cover the issue. step-65 shows another, less manual way to achieve a mesh well fit to the problem here. Information on curved domains can also be found in the documentation module on  [2.x.5681]  "Manifold descriptions". 

Why does it make sense to choose a mesh that tracks the internal interface? There are a number of reasons, but the most essential one comes down to what we actually integrate in our bilinear form. Conceptually, we want to integrate the term  [2.x.5682]  as the contribution of cell  [2.x.5683]  to the matrix entry  [2.x.5684] . We can not compute it exactly and have to resort to quadrature. We know that quadrature is accurate if the integrand is smooth. That is because quadrature in essence computes a polynomial approximation to the integrand that coincides with the integrand in the quadrature points, and then computes the volume under this polynomial as an approximation to the volume under the original integrand. This polynomial interpolant is accurate if the integrand is smooth on a cell, but it is usually rather inaccurate if the integrand is discontinuous on a cell. 

Consequently, it is worthwhile to align cells in such a way that the interfaces across which the coefficient is discontinuous are aligned with cell interfaces. This way, the coefficient is constant on each cell, following which the integrand will be smooth, and its polynomial approximation and the quadrature approximation of the integral will both be accurate. Note that such an alignment is common in many practical cases, so deal.II provides a number of functions (such as  [2.x.5685]  "material_id") to help manage such a scenario. Refer to step-28 and step-46 for examples of how material ids can be applied. 

Finally, let us consider the case of a coefficient that has a smooth and non-uniform distribution in space. We can repeat once again all of the above discussion on the representation of such a function with the quadrature. So, to simulate it accurately there are a few readily available options: you could reduce the cell size, increase the order of the polynomial used in the quadrature formula, select a more appropriate quadrature formula, or perform a combination of these steps. The key is that providing the best fit of the coefficient's spatial dependence with the quadrature polynomial will lead to a more accurate finite element solution of the PDE. 

As a final note: The discussion in the previous paragraphs shows, we here have a very concrete way of stating what we think of a good mesh -- it should be aligned with the jump in the coefficient. But one could also have asked this kind of question in a more general setting: Given some equation with a smooth solution and smooth coefficients, can we say what a good mesh would look like? This is a question for which the answer is easier to state in intuitive terms than mathematically: A good mesh has cells that all, by and large, look like squares (or cubes, in 3d). A bad mesh would contain cells that are very elongated in some directions or, more generally, for which there are cells that have both short and long edges. There are many ways in which one could assign a numerical quality index to each cell that measures whether the cell is "good" or "bad"; some of these are often chosen because they are cheap and easy to compute, whereas others are based on what enters into proofs of convergence. An example of the former would be the ratio of the longest to the shortest edge of a cell: In the ideal case, that ratio would be one; bad cells have values much larger than one. An example of the latter kind would consider the gradient (the "Jacobian") of the mapping from the reference cell  [2.x.5686]  to the real cell  [2.x.5687] ; this gradient is a matrix, and a quantity that enters into error estimates is the maximum over all points on the reference cell of the ratio of the largest to the smallest eigenvalue of this matrix. It is again not difficult to see that this ratio is constant if the cell  [2.x.5688]  is an affine image of  [2.x.5689] , and that it is one for squares and cubes. 

In practice, it might be interesting to visualize such quality measures. The function  [2.x.5690]  provides one way to get this kind of information. Even better, visualization tools such as VisIt often allow you to visualize this sort of information for a variety of measures from within the visualization software itself; in the case of VisIt, just add a "pseudo-color" plot and select one of the mesh quality measures instead of the solution field. 




[1.x.2231] 

From a mathematical perspective, solutions of the Laplace equation 

[1.x.2232] 

on smoothly bounded, convex domains are known to be smooth themselves. The exact degree of smoothness, i.e., the function space in which the solution lives, depends on how smooth exactly the boundary of the domain is, and how smooth the right hand side is. Some regularity of the solution may be lost at the boundary, but one generally has that the solution is twice more differentiable in compact subsets of the domain than the right hand side. If, in particular, the right hand side satisfies  [2.x.5691] , then  [2.x.5692]  where  [2.x.5693]  is any compact subset of  [2.x.5694]  ( [2.x.5695]  is an open domain, so a compact subset needs to keep a positive distance from  [2.x.5696] ). 

The situation we chose for the current example is different, however: we look at an equation with a non-constant coefficient  [2.x.5697] : 

[1.x.2233] 

Here, if  [2.x.5698]  is not smooth, then the solution will not be smooth either, regardless of  [2.x.5699] . In particular, we expect that wherever  [2.x.5700]  is discontinuous along a line (or along a plane in 3d), the solution will have a kink. This is easy to see: if for example  [2.x.5701]  is continuous, then  [2.x.5702]  needs to be continuous. This means that  [2.x.5703]  must be continuously differentiable (not have a kink). Consequently, if  [2.x.5704]  has a discontinuity, then  [2.x.5705]  must have an opposite discontinuity so that the two exactly cancel and their product yields a function without a discontinuity. But for  [2.x.5706]  to have a discontinuity,  [2.x.5707]  must have a kink. This is of course exactly what is happening in the current example, and easy to observe in the pictures of the solution. 

In general, if the coefficient  [2.x.5708]  is discontinuous along a line in 2d, or a plane in 3d, then the solution may have a kink, but the gradient of the solution will not go to infinity. That means, that the solution is at least still in the [1.x.2234]  [2.x.5709]  (i.e., roughly speaking, in the space of functions whose derivatives are bounded). On the other hand, we know that in the most extreme cases -- i.e., where the domain has reentrant corners, the right hand side only satisfies  [2.x.5710] , or the coefficient  [2.x.5711]  is only in  [2.x.5712]  -- all we can expect is that  [2.x.5713]  (i.e., the [1.x.2235] of functions whose derivative is square integrable), a much larger space than  [2.x.5714] . It is not very difficult to create cases where the solution is in a space  [2.x.5715]  where we can get  [2.x.5716]  to become as small as we want. Such cases are often used to test adaptive finite element methods because the mesh will have to resolve the singularity that causes the solution to not be in  [2.x.5717]  any more. 

The typical example one uses for this is called the [1.x.2236] (referring to  [2.x.5718] ), which in the commonly used form has a coefficient  [2.x.5719]  that has different values in the four quadrants of the plane (or eight different values in the octants of  [2.x.5720] ). The exact degree of regularity (the  [2.x.5721]  in the index of the Sobolev space above) depends on the values of  [2.x.5722]  coming together at the origin, and by choosing the jumps large enough, the regularity of the solution can be made as close as desired to  [2.x.5723] . 

To implement something like this, one could replace the coefficient function by the following (shown here only for the 2d case): 

[1.x.2237] 

(Adding the  [2.x.5724]  at the end ensures that either an exception is thrown or that the program aborts if we ever get to that point 

-- which of course we shouldn't, but this is a good way to insure yourself: we all make mistakes by sometimes not thinking of all cases, for example by checking for  [2.x.5725]  to be less than and greater than zero, rather than greater-or-equal to zero, and thereby forgetting some cases that would otherwise lead to bugs that are awkward to find. The  [2.x.5726]  at the end is only there to avoid compiler warnings that the function does not end in a  [2.x.5727]  statement -- the compiler cannot see that the function would never actually get to that point because of the preceding  [2.x.5728]  statement.) 

By playing with such cases where four or more sectors come together and on which the coefficient has different values, one can construct cases where the solution has singularities at the origin. One can also see how the meshes are refined in such cases. 


examples/step-60/doc/intro.dox 

 [2.x.5729]  

[1.x.2238] 

 [2.x.5730]  




[1.x.2239] 

[1.x.2240] 


In this tutorial we consider the case of two domains,  [2.x.5731]  in  [2.x.5732]  and  [2.x.5733]  in  [2.x.5734] , where  [2.x.5735]  is embedded in  [2.x.5736]  ( [2.x.5737] ). We want to solve a partial differential equation on  [2.x.5738] , enforcing some conditions on the solution of the problem *on the embedded domain*  [2.x.5739] . 

There are two interesting scenarios: 

- the geometrical dimension `dim` of the embedded domain  [2.x.5740]  is the same of the domain  [2.x.5741]  (`spacedim`), that is, the spacedim-dimensional measure of  [2.x.5742]  is not zero, or 

- the embedded domain  [2.x.5743]  has an intrinsic dimension `dim` which is smaller than that of  [2.x.5744]  (`spacedim`), thus its spacedim-dimensional measure is zero; for example it is a curve embedded in a two dimensional domain, or a surface embedded in a three-dimensional domain. 

In both cases define the restriction operator  [2.x.5745]  as the operator that, given a continuous function on  [2.x.5746] , returns its (continuous) restriction on  [2.x.5747] , i.e., 

[1.x.2241] 

It is well known that the operator  [2.x.5748]  can be extended to a continuous operator on  [2.x.5749] , mapping functions in  [2.x.5750]  to functions in  [2.x.5751]  when the intrinsic dimension of  [2.x.5752]  is the same of  [2.x.5753] . 

The same is true, with a less regular range space (namely  [2.x.5754] ), when the dimension of  [2.x.5755]  is one less with respect to  [2.x.5756] , and  [2.x.5757]  does not have a boundary. In this second case, the operator  [2.x.5758]  is also known as the *trace* operator, and it is well defined for Lipschitz co-dimension one curves and surfaces  [2.x.5759]  embedded in  [2.x.5760]  (read  [1.x.2242] for further details on the trace operator). 

The co-dimension two case is a little more complicated, and in general it is not possible to construct a continuous trace operator, not even from  [2.x.5761]  to  [2.x.5762] , when the dimension of  [2.x.5763]  is zero or one respectively in two and three dimensions. 

In this tutorial program we're not interested in further details on  [2.x.5764] : we take the extension  [2.x.5765]  for granted, assuming that the dimension of the embedded domain (`dim`) is always smaller by one or equal with respect to the dimension of the embedding domain  [2.x.5766]  (`spacedim`). 

We are going to solve the following differential problem: given a sufficiently regular function  [2.x.5767]  on  [2.x.5768] , find the solution  [2.x.5769]  to 

[1.x.2243] 



This is a constrained problem, where we are looking for a harmonic function  [2.x.5770]  that satisfies homogeneous boundary conditions on  [2.x.5771] , subject to the constraint  [2.x.5772]  using a Lagrange multiplier. 

This problem has a physical interpretation: harmonic functions, i.e., functions that satisfy the Laplace equation, can be thought of as the displacements of a membrane whose boundary values are prescribed. The current situation then corresponds to finding the shape of a membrane for which not only the displacement at the boundary, but also on  [2.x.5773]  is prescribed. For example, if  [2.x.5774]  is a closed curve in 2d space, then that would model a soap film that is held in place by a wire loop along  [2.x.5775]  as well as a second loop along  [2.x.5776] . In cases where  [2.x.5777]  is a whole area, you can think of this as a membrane that is stretched over an obstacle where  [2.x.5778]  is the contact area. (If the contact area is not known we have a different problem -- called the "obstacle problem" -- which is modeled in step-41.) 

As a first example we study the zero Dirichlet boundary condition on  [2.x.5779] . The same equations apply if we apply zero Neumann boundary conditions on  [2.x.5780]  or a mix of the two. 

The variational formulation can be derived by introducing two infinite dimensional spaces  [2.x.5781]  and  [2.x.5782] , respectively for the solution  [2.x.5783]  and for the Lagrange multiplier  [2.x.5784] . 

Multiplying the first equation by  [2.x.5785]  and the second by  [2.x.5786] , integrating by parts when possible, and exploiting the boundary conditions on  [2.x.5787] , we obtain the following variational problem: 

Given a sufficiently regular function  [2.x.5788]  on  [2.x.5789] , find the solution  [2.x.5790]  to 

[1.x.2244] 



where  [2.x.5791]  and  [2.x.5792]  represent, respectively,  [2.x.5793]  scalar products in  [2.x.5794]  and in  [2.x.5795] . 

Inspection of the variational formulation tells us that the space  [2.x.5796]  can be taken to be  [2.x.5797] . The space  [2.x.5798] , in the co-dimension zero case, should be taken as  [2.x.5799] , while in the co-dimension one case should be taken as  [2.x.5800] . 

The function  [2.x.5801]  should therefore be either in  [2.x.5802]  (for the co-dimension zero case) or  [2.x.5803]  (for the co-dimension one case). This leaves us with a Lagrange multiplier  [2.x.5804]  in  [2.x.5805] , which is either  [2.x.5806]  or  [2.x.5807] . 

There are two options for the discretization of the problem above. One could choose matching discretizations, where the Triangulation for  [2.x.5808]  is aligned with the Triangulation for  [2.x.5809] , or one could choose to discretize the two domains in a completely independent way. 

The first option is clearly more indicated for the simple problem we proposed above: it is sufficient to use a single Triangulation for  [2.x.5810]  and then impose certain constraints depending  [2.x.5811] . An example of this approach is studied in step-40, where the solution has to stay above an obstacle and this is achieved imposing constraints on  [2.x.5812] . 

To solve more complex problems, for example one where the domain  [2.x.5813]  is time dependent, the second option could be a more viable solution. Handling non aligned meshes is complex by itself: to illustrate how is done we study a simple problem. 

The technique we describe here is presented in the literature using one of many names: the [1.x.2245], the [1.x.2246], the [1.x.2247], and others. The main principle is that the discretization of the two grids and of the two finite element spaces are kept completely independent. This technique is particularly efficient for the simulation of fluid-structure interaction problems, where the configuration of the embedded structure is part of the problem itself, and one solves a (possibly non-linear) elastic problem to determine the (time dependent) configuration of  [2.x.5814] , and a (possibly non-linear) flow problem in  [2.x.5815] , plus coupling conditions on the interface between the fluid and the solid. 

In this tutorial program we keep things a little simpler, and we assume that the configuration of the embedded domain is given in one of two possible ways: 

- as a deformation mapping  [2.x.5816] , defined on a continuous finite dimensional space on  [2.x.5817]  and representing, for any point  [2.x.5818] , its coordinate  [2.x.5819]  in  [2.x.5820] ; 

- as a displacement mapping  [2.x.5821]  for  [2.x.5822] , representing for any point  [2.x.5823]  the displacement vector applied in order to deform  [2.x.5824]  to its actual configuration  [2.x.5825] . 

We define the embedded reference domain  [2.x.5826]  `embedded_grid`: on this triangulation we construct a finite dimensional space (`embedded_configuration_dh`) to describe either the deformation or the displacement through a FiniteElement system of FE_Q objects (`embedded_configuration_fe`). This finite dimensional space is used only to interpolate a user supplied function (`embedded_configuration_function`) representing either  [2.x.5827]  (if the parameter `use_displacement` is set to  [2.x.5828]  or  [2.x.5829]  (if the parameter `use_displacement` is set to  [2.x.5830]  

The Lagrange multiplier  [2.x.5831]  and the user supplied function  [2.x.5832]  are defined through another finite dimensional space `embedded_dh`, and through another FiniteElement `embedded_fe`, using the same reference domain. In order to take into account the deformation of the domain, either a MappingFEField or a MappingQEulerian object are initialized with the `embedded_configuration` vector. 

In the embedding space, a standard finite dimensional space `space_dh` is constructed on the embedding grid `space_grid`, using the FiniteElement `space_fe`, following almost verbatim the approach taken in step-6. 

We represent the discretizations of the spaces  [2.x.5833]  and  [2.x.5834]  with 

[1.x.2248] and 

[1.x.2249] respectively, where  [2.x.5835]  is the dimension of `space_dh`, and  [2.x.5836]  the dimension of `embedded_dh`. 

Once all the finite dimensional spaces are defined, the variational formulation of the problem above leaves us with the following finite dimensional system of equations: 

[1.x.2250] 

where 

[1.x.2251] 



While the matrix  [2.x.5837]  is the standard stiffness matrix for the Poisson problem on  [2.x.5838] , and the vector  [2.x.5839]  is a standard right-hand-side vector for a finite element problem with forcing term  [2.x.5840]  on  [2.x.5841] , (see, for example, step-3), the matrix  [2.x.5842]  or its transpose  [2.x.5843]  are non-standard since they couple information on two non-matching grids. 

In particular, the integral that appears in the computation of a single entry of  [2.x.5844] , is computed on  [2.x.5845] . As usual in finite elements we split this integral into contributions from all cells of the triangulation used to discretize  [2.x.5846] , we transform the integral on  [2.x.5847]  to an integral on the reference element  [2.x.5848] , where  [2.x.5849]  is the mapping from  [2.x.5850]  to  [2.x.5851] , and compute the integral on  [2.x.5852]  using a quadrature formula: 

[1.x.2252] 

Computing this sum is non-trivial because we have to evaluate  [2.x.5853] . In general, if  [2.x.5854]  and  [2.x.5855]  are not aligned, the point  [2.x.5856]  is completely arbitrary with respect to  [2.x.5857] , and unless we figure out a way to interpolate all basis functions of  [2.x.5858]  on an arbitrary point on  [2.x.5859] , we cannot compute the integral needed for an entry of the matrix  [2.x.5860] . 

To evaluate  [2.x.5861]  the following steps needs to be taken (as shown in the picture below): 

- For a given cell  [2.x.5862]  in  [2.x.5863]  compute the real point  [2.x.5864] , where  [2.x.5865]  is one of the quadrature points used for the integral on  [2.x.5866] . 

- Find the cell of  [2.x.5867]  in which  [2.x.5868]  lies. We shall call this element  [2.x.5869] . 

- To evaluate the basis function use the inverse of the mapping  [2.x.5870]  that transforms the reference element  [2.x.5871]  into the element  [2.x.5872] :  [2.x.5873] . 

<p align="center"> <img   src="https://www.dealii.org/images/steps/developer/step-60.C_interpolation.png"   alt="">  [2.x.5874]  

The three steps above can be computed by calling, in turn, 

-  [2.x.5875]  followed by 

-  [2.x.5876]  We then 

- construct a custom Quadrature formula, containing the point in the reference  cell and then 

- construct an FEValues object, with the given quadrature formula, and  initialized with the cell obtained in the first step. 

This is what the deal.II function  [2.x.5877]  does when evaluating a finite element field (not just a single shape function) at an arbitrary point; but this would be inefficient in this case. 

A better solution is to use a convenient wrapper to perform the first three steps on a collection of points:  [2.x.5878]  If one is actually interested in computing the full coupling matrix, then it is possible to call the method  [2.x.5879]  that performs the above steps in an efficient way, reusing all possible data structures, and gathering expensive steps together. This is the function we'll be using later in this tutorial. 

We solve the final saddle point problem by an iterative solver, applied to the Schur complement  [2.x.5880]  (whose construction is described, for example, in step-20), and we construct  [2.x.5881]  using LinearOperator classes. 




[1.x.2253] 

The problem we solve here is identical to step-4, with the difference that we impose some constraints on an embedded domain  [2.x.5882] . The tutorial is written in a dimension independent way, and in the results section we show how to vary both `dim` and `spacedim`. 

The tutorial is compiled for `dim` equal to one and `spacedim` equal to two. If you want to run the program in embedding dimension `spacedim` equal to three, you will most likely want to change the reference domain for  [2.x.5883]  to be, for example, something you read from file, or a closed sphere that you later deform to something more interesting. 

In the default scenario,  [2.x.5884]  has co-dimension one, and this tutorial program implements the Fictitious Boundary Method. As it turns out, the same techniques are used in the Variational Immersed Finite Element Method, and the coupling operator  [2.x.5885]  defined above is the same in almost all of these non-matching methods. 

The embedded domain is assumed to be included in  [2.x.5886] , which we take as the unit square  [2.x.5887] . The definition of the fictitious domain  [2.x.5888]  can be modified through the parameter file, and can be given as a mapping from the reference interval  [2.x.5889]  to a curve in  [2.x.5890] . 

If the curve is closed, then the results will be similar to running the same problem on a grid whose boundary is  [2.x.5891] . The program will happily run also with a non-closed  [2.x.5892] , although in those cases the mathematical formulation of the problem is more difficult, since  [2.x.5893]  will have a boundary by itself that has co-dimension two with respect to the domain  [2.x.5894] . 




[1.x.2254] 

 [2.x.5895]   [2.x.5896]  Glowinski, R., T.-W. Pan, T.I. Hesla, and D.D. Joseph. 1999. “A Distributed   Lagrange Multiplier/fictitious Domain Method for Particulate Flows.”   International Journal of Multiphase Flow 25 (5). Pergamon: 755–94. 

 [2.x.5897]  Boffi, D., L. Gastaldi, L. Heltai, and C.S. Peskin. 2008. “On the   Hyper-Elastic Formulation of the Immersed Boundary Method.” Computer Methods   in Applied Mechanics and Engineering 197 (25–28). 

 [2.x.5898]  Heltai, L., and F. Costanzo. 2012. “Variational Implementation of Immersed   Finite Element Methods.” Computer Methods in Applied Mechanics and Engineering   229–232.  [2.x.5899]  


examples/step-60/doc/results.dox 



[1.x.2255] 

The directory in which this program is run does not contain a parameter file by default. On the other hand, this program wants to read its parameters from a file called parameters.prm -- and so, when you execute it the first time, you will get an exception that no such file can be found: 

[1.x.2256] 



However, as the error message already states, the code that triggers the exception will also generate a parameters.prm file that simply contains the default values for all parameters this program cares about. By inspection of the parameter file, we see the following: 

[1.x.2257] 



If you now run the program, you will get a file called `used_parameters.prm`, containing a shorter version of the above parameters (without comments and documentation), documenting all parameters that were used to run your program: 

[1.x.2258] 



The rationale behind creating first `parameters.prm` file (the first time the program is run) and then a `used_parameters.prm` (every other times you run the program), is because you may want to leave most parameters to their default values, and only modify a handful of them. 

For example, you could use the following (perfectly valid) parameter file with this tutorial program: 

[1.x.2259] 



and you would obtain exactly the same results as in test case 1 below. 

[1.x.2260] 

For the default problem the value of  [2.x.5900]  on  [2.x.5901]  is set to the constant  [2.x.5902] : this is like imposing a constant Dirichlet boundary condition on  [2.x.5903] , seen as boundary of the portion of  [2.x.5904]  inside  [2.x.5905] . Similarly on  [2.x.5906]  we have zero Dirichlet boundary conditions. 


<div class="twocolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-60.1_no_grid.png"            alt = ""            width="500">     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-60.1_grid.png"            alt = ""            width="500">     </div>   </div> </div> 

The output of the program will look like the following: 

[1.x.2261] 



You may notice that, in terms of CPU time, assembling the coupling system is twice as expensive as assembling the standard Poisson system, even though the matrix is smaller. This is due to the non-matching nature of the discretization. Whether this is acceptable or not, depends on the applications. 

If the problem was set in a three-dimensional setting, and the immersed mesh was time dependent, it would be much more expensive to recreate the mesh at each step rather than use the technique we present here. Moreover, you may be able to create a very fast and optimized solver on a uniformly refined square or cubic grid, and embed the domain where you want to perform your computation using the technique presented here. This would require you to only have a surface representatio of your domain (a much cheaper and easier mesh to produce). 

To play around a little bit, we are going to complicate a little the fictitious domain as well as the boundary conditions we impose on it. 

[1.x.2262] 

If we use the following parameter file: 

[1.x.2263] 



We get a "flowery" looking domain, where we impose a linear boundary condition  [2.x.5907] . This test shows that the method is actually quite accurate in recovering an exactly linear function from its boundary conditions, and even though the meshes are not aligned, we obtain a pretty good result. 

Replacing  [2.x.5908]  with  [2.x.5909] , i.e., modifying the parameter file such that we have 

[1.x.2264] 

produces the saddle on the right. 

<div class="twocolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-60.3_grid.png"            alt = ""            width="500">     </div>   </div>   <div class="parent">     <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-60.4_grid.png"            alt = ""            width="500">     </div>   </div> </div> 

[1.x.2265] 

[1.x.2266] 

[1.x.2267] 

While the current tutorial program is written for `spacedim` equal to two, there are only minor changes you have to do in order for the program to run in different combinations of dimensions. 

If you want to run with `spacedim` equal to three and `dim` equal to two, then you will almost certainly want to perform the following changes: 

- use a different reference domain for the embedded grid, maybe reading it from   a file. It is not possible to construct a smooth closed surface with one   single parametrization of a square domain, therefore you'll most likely want   to use a reference domain that is topologically equivalent to a the boundary   of a sphere. 

- use a displacement instead of the deformation to map  [2.x.5910]  into  [2.x.5911]  

[1.x.2268] 

We have seen in other tutorials (for example in step-5 and step-54) how to read grids from input files. A nice generalization for this tutorial program would be to allow the user to select a grid to read from the parameter file itself, instead of hardcoding the mesh type in the tutorial program itself. 

[1.x.2269] 

At the moment, we have no preconditioner on the Schur complement. This is ok for two dimensional problems, where a few hundred iterations bring the residual down to the machine precision, but it's not going to work in three dimensions. 

It is not obvious what a good preconditioner would be here. The physical problem we are solving with the Schur complement, is to associate to the Dirichlet data  [2.x.5912] , the value of the Lagrange multiplier  [2.x.5913] .  [2.x.5914]  can be interpreted as the *jump* in the normal gradient that needs to be imposed on  [2.x.5915]  across  [2.x.5916] , in order to obtain the Dirichlet data  [2.x.5917] . 

So  [2.x.5918]  is some sort of Neumann to Dirichlet map, and we would like to have a good approximation for the Dirichlet to Neumann map. A possibility would be to use a Boundary Element approximation of the problem on  [2.x.5919] , and construct a rough approximation of the hyper-singular operator for the Poisson problem associated to  [2.x.5920] , which is precisely a Dirichlet to Neumann map. 

[1.x.2270] 

The simple code proposed here can serve as a starting point for more complex problems which, to be solved, need to be run on parallel code, possibly using distributed meshes (see step-17, step-40, and the documentation for  [2.x.5921]  and  [2.x.5922]  

When using non-matching grids in parallel a problem arises: to compute the matrix  [2.x.5923]  a process needs information about both meshes on the same portion of real space but, when working with distributed meshes, this information may not be available, because the locally owned part of the  [2.x.5924]  triangulation stored on a given processor may not be physically co-located with the locally owned part of the  [2.x.5925]  triangulation stored on the same processor. 

Various strategies can be implemented to tackle this problem: 

- distribute the two meshes so that this constraint is satisfied; 

- use communication for the parts of real space where the constraint is not   satisfied; 

- use a distributed triangulation for the embedding space, and a shared   triangulation for the emdedded configuration. 

The latter strategy is clearly the easiest to implement, as most of the functions used in this tutorial program will work unchanged also in the parallel case. Of course one could use the reversal strategy (that is, have a distributed embedded Triangulation and a shared embedding Triangulation). 

However, this strategy is most likely going to be more expensive, since by definition the embedding grid is larger than the embedded grid, and it makes more sense to distribute the largest of the two grids, maintaining the smallest one shared among all processors. 


examples/step-61/doc/intro.dox 

 [2.x.5926]  

[1.x.2271] 

[1.x.2272] 

[1.x.2273] 

This tutorial program presents an implementation of the "weak Galerkin" finite element method for the Poisson equation. In some sense, the motivation for considering this method starts from the same point as in step-51: We would like to consider discontinuous shape functions, but then need to address the fact that the resulting problem has a much larger number of degrees of freedom compared to the usual continuous Galerkin method (because, for example, each vertex carries as many degrees of freedom as there are adjacent cells). We also have to address the fact that, unlike in the continuous Galerkin method, [1.x.2274] degree of freedom on one cell couples with all of the degrees of freedom on each of its face neighbor cells. Consequently, the matrix one gets from the "traditional" discontinuous Galerkin methods are both large and relatively dense. 

Both the hybridized discontinuous Galerkin method (HDG) in step-51 and the weak Galerkin (WG) method in this tutorial address the issue of coupling by introducing additional degrees of freedom whose shape functions only live on a face between cells (i.e., on the "skeleton" of the mesh), and which therefore "insulate" the degrees of freedom on the adjacent cells from each other: cell degrees of freedom only couple with other cell degrees of freedom on the same cell, as well as face degrees of freedom, but not with cell degrees of freedom on neighboring cells. Consequently, the coupling of shape functions for these cell degrees of freedom indeed couple on exactly one cell and the degrees of freedom defined on its faces. 

For a given equation, say the second order Poisson equation, the difference between the HDG and the WG method is how precisely one formulates the problem that connects all of these different shape functions. (Indeed, for some WG and HDG formulation, it is possible to show that they are equivalent.) The HDG does things by reformulating second order problems in terms of a system of first order equations and then conceptually considers the face degrees of freedom to be "fluxes" of this first order system. In contrast, the WG method keeps things in second order form and considers the face degrees of freedom as of the same type as the primary solution variable, just restricted to the lower-dimensional faces. For the purposes of the equation, one then needs to somehow "extend" these shape functions into the interior of the cell when defining what it means to apply a differential operator to them. Compared to the HDG, the method has the advantage that it does not lead to a proliferation of unknowns due to rewriting the equation as a first-order system, but it is also not quite as easy to implement. However, as we will see in the following, this additional effort is not prohibitive. 




[1.x.2275] 

Weak Galerkin Finite Element Methods (WGFEMs) use discrete weak functions to approximate scalar unknowns, and discrete weak gradients to approximate classical gradients. The method was originally introduced by Junping Wang and Xiu Ye in the paper [1.x.2276][1.x.2277]. Compared to the continuous Galerkin method, the weak Galerkin method satisfies important physical properties, namely local mass conservation and bulk normal flux continuity. It results in a SPD linear system, and optimal convergence rates can be obtained with mesh refinement. 




[1.x.2278] This program solves the Poisson equation using the weak Galerkin finite element method: 

[1.x.2279] 

where  [2.x.5927]  is a bounded domain. In the context of the flow of a fluid through a porous medium,  [2.x.5928]  is the pressure,  [2.x.5929]  is a permeability tensor,  [2.x.5930]  is the source term, and  [2.x.5931]  represent Dirichlet and Neumann boundary conditions. We can introduce a flux,  [2.x.5932] , that corresponds to the Darcy velocity (in the way we did in step-20) and this variable will be important in the considerations below. 

In this program, we will consider a test case where the exact pressure is  [2.x.5933]  on the unit square domain, with homogeneous Dirichelet boundary conditions and  [2.x.5934]  the identity matrix. Then we will calculate  [2.x.5935]  errors of pressure, velocity, and flux. 




[1.x.2280] 

The Poisson equation above has a solution  [2.x.5936]  that needs to satisfy the weak formulation of the problem, 

[1.x.2281] 

for all test functions  [2.x.5937] , where 

[1.x.2282] 

and 

[1.x.2283] 

Here, we have integrated by parts in the bilinear form, and we are evaluating the gradient of  [2.x.5938]  in the interior and the values of  [2.x.5939]  on the boundary of the domain. All of this is well defined because we assume that the solution is in  [2.x.5940]  for which taking the gradient and evaluating boundary values are valid operations. 

The idea of the weak Galerkin method is now to approximate the exact  [2.x.5941]  solution with a [1.x.2284]  [2.x.5942] . This function may only be discontinuous along interfaces between cells, and because we will want to evaluate this function also along interfaces, we have to prescribe not only what values it is supposed to have in the cell interiors but also its values along interfaces. We do this by saying that  [2.x.5943]  is actually a tuple,  [2.x.5944] , though it's really just a single function that is either equal to  [2.x.5945]  or  [2.x.5946] , depending on whether it is evaluated at a point  [2.x.5947]  that lies in the cell interior or on cell interfaces. 

We would then like to simply stick this approximation into the bilinear form above. This works for the case where we have to evaluate the test function  [2.x.5948]  on the boundary (where we would simply take its interface part  [2.x.5949] ) but we have to be careful with the gradient because that is only defined in cell interiors. Consequently, the weak Galerkin scheme for the Poisson equation is defined by 

[1.x.2285] 

for all discrete test functions  [2.x.5950] , where 

[1.x.2286] 

and 

[1.x.2287] 

The key point is that here, we have replaced the gradient  [2.x.5951]  by the [1.x.2288] operator  [2.x.5952]  that makes sense for our peculiarly defined approximation  [2.x.5953] . 

The question is then how that operator works. For this, let us first say how we think of the discrete approximation  [2.x.5954]  of the pressure. As mentioned above, the "function"  [2.x.5955]  actually consists of two parts: the values  [2.x.5956]  in the interior of cells, and  [2.x.5957]  on the interfaces. We have to define discrete (finite-dimensional) function spaces for both of these; in this program, we will use FE_DGQ for  [2.x.5958]  as the space in the interior of cells (defined on each cell, but in general discontinuous along interfaces), and FE_FaceQ for  [2.x.5959]  as the space on the interfaces. 

Then let us consider just a single cell (because the integrals above are all defined cell-wise, and because the weak discrete gradient is defined cell-by-cell). The restriction of  [2.x.5960]  to cell  [2.x.5961] ,  [2.x.5962]  then consists of the pair  [2.x.5963] . In essence, we can think of  [2.x.5964]  of some function defined on  [2.x.5965]  that approximates the gradient; in particular, if  [2.x.5966]  was the restriction of a differentiable function (to the interior and boundary of  [2.x.5967]  -- which would make it continuous between the interior and boundary), then  [2.x.5968]  would simply be the exact gradient  [2.x.5969] . But, since  [2.x.5970]  is not continuous between interior and boundary of  [2.x.5971] , we need a more general definition; furthermore, we can not deal with arbitrary functions, and so require that  [2.x.5972]  is also in a finite element space (which, since the gradient is a vector, has to be vector-valued, and because the weak gradient is defined on each cell separately, will also be discontinuous between cells). 

The way this is done is to define this weak gradient operator  [2.x.5973]  (where  [2.x.5974]  is the vector-valued Raviart-Thomas space of order  [2.x.5975]  on cell  [2.x.5976] ) in the following way: 

[1.x.2289] 

for all test functions  [2.x.5977] . This is, in essence, simply an application of the integration-by-parts formula. In other words, for a given  [2.x.5978] , we need to think of  [2.x.5979]  as that Raviart-Thomas function of degree  [2.x.5980]  for which the left hand side and right hand side are equal for all test functions. 

A key point to make is then the following: While the usual gradient  [2.x.5981]  is a *local* operator that computes derivatives based simply on the value of a function at a point and its (infinitesimal) neighborhood, the weak discrete gradient  [2.x.5982]  does not have this property: It depends on the values of the function it is applied to on the entire cell, including the cell's boundary. Both are, however, linear operators as is clear from the definition of  [2.x.5983]  above, and that will allow us to represent  [2.x.5984]  via a matrix in the discussion below. 

 [2.x.5985]  It may be worth pointing out that while the weak discrete   gradient is an element of the Raviart-Thomas space  [2.x.5986]  on each   cell  [2.x.5987] , it is discontinuous between cells. On the other hand, the   Raviart-Thomas space  [2.x.5988]  defined on the entire   mesh and implemented by the FE_RaviartThomas class represents   functions that have continuous normal components at interfaces   between cells. This means that [1.x.2290],  [2.x.5989]    is not in  [2.x.5990] , even though it is on every cell  [2.x.5991]  in  [2.x.5992] .   Rather, it is in a "broken" Raviart-Thomas space that below we will   represent by the symbol  [2.x.5993] . (The term "broken" here refers to   the process of "breaking something apart", and not to the synonym to   the expression "not functional".) One might therefore (rightfully) argue that   the notation used in the weak Galerkin literature is a bit misleading,   but as so often it all depends on the context in which a certain   notation is used -- in the current context, references to the   Raviart-Thomas space or element are always understood to be to the   "broken" spaces. 

 [2.x.5994]  deal.II happens to have an implementation of this broken Raviart-Thomas   space: The FE_DGRT class. As a consequence, in this tutorial we will simply   always use the FE_DGRT class, even though in all of those places where   we have to compute cell-local matrices and vectors, it makes no difference. 




[1.x.2291] 

Since  [2.x.5995]  is an element of a finite element space, we can expand it in a basis as we always do, i.e., we can write 

[1.x.2292] 

Here, since  [2.x.5996]  has two components (the interior and the interface components), the same must hold true for the basis functions  [2.x.5997] , which we can write as  [2.x.5998] . If you've followed the descriptions in step-8, step-20, and the  [2.x.5999]  "documentation module on vector-valued problems", it will be no surprise that for some values of  [2.x.6000] ,  [2.x.6001]  will be zero, whereas for other values of  [2.x.6002] ,  [2.x.6003]  will be zero -- i.e., shape functions will be of either one or the other kind. That is not important, here, however. What is important is that we need to wonder how we can represent  [2.x.6004]  because that is clearly what will appear in the problem when we want to implement the bilinear form 

[1.x.2293] 



The key point is that  [2.x.6005]  is known to be a member of the "broken" Raviart-Thomas space  [2.x.6006] . What this means is that we can represent (on each cell  [2.x.6007]  separately) 

[1.x.2294] 

where the functions  [2.x.6008] , and where  [2.x.6009]  is a matrix of dimension 

[1.x.2295] 

(That the weak discrete gradient can be represented as a matrix should not come as a surprise: It is a linear operator from one finite dimensional space to another finite dimensional space. If one chooses bases for both of these spaces, then [1.x.2296] can of course be written as a matrix mapping the vector of expansion coefficients with regards to the basis of the domain space of the operator, to the vector of expansion coefficients with regards to the basis in the image space.) 

Using this expansion, we can easily use the definition of the weak discrete gradient above to define what the matrix is going to be: 

[1.x.2297] 

for all test functions  [2.x.6010] . 

This clearly leads to a linear system of the form 

[1.x.2298] 

with 

[1.x.2299] 

and consequently 

[1.x.2300] 

(In this last step, we have assumed that the indices  [2.x.6011]  only range over those degrees of freedom active on cell  [2.x.6012] , thereby ensuring that the mass matrix on the space  [2.x.6013]  is invertible.) Equivalently, using the symmetry of the matrix  [2.x.6014] , we have that 

[1.x.2301] 

Also worth pointing out is that the matrices  [2.x.6015]  and  [2.x.6016]  are of course not square but rectangular. 




[1.x.2302] 

Having explained how the weak discrete gradient is defined, we can now come back to the question of how the linear system for the equation in question should be assembled. Specifically, using the definition of the bilinear form  [2.x.6017]  shown above, we then need to compute the elements of the local contribution to the global matrix, 

[1.x.2303] 

As explained above, we can expand  [2.x.6018]  in terms of the Raviart-Thomas basis on each cell, and similarly for  [2.x.6019] : 

[1.x.2304] 

By re-arranging sums, this yields the following expression: 

[1.x.2305] 

So, if we have the matrix  [2.x.6020]  for each cell  [2.x.6021] , then we can easily compute the contribution  [2.x.6022]  for cell  [2.x.6023]  to the matrix  [2.x.6024]  as follows: 

[1.x.2306] 

Here, 

[1.x.2307] 

which is really just the mass matrix on cell  [2.x.6025]  using the Raviart-Thomas basis and weighting by the permeability tensor  [2.x.6026] . The derivation here then shows that the weak Galerkin method really just requires us to compute these  [2.x.6027]  and  [2.x.6028]  matrices on each cell  [2.x.6029] , and then  [2.x.6030] , which is easily computed. The code to be shown below does exactly this. 

Having so computed the contribution  [2.x.6031]  of cell  [2.x.6032]  to the global matrix, all we have to do is to "distribute" these local contributions into the global matrix. How this is done is first shown in step-3 and step-4. In the current program, this will be facilitated by calling  [2.x.6033]  

A linear system of course also needs a right hand side. There is no difficulty associated with computing the right hand side here other than the fact that we only need to use the cell-interior part  [2.x.6034]  for each shape function  [2.x.6035] . 




[1.x.2308][1.x.2309] 

The discussions in the previous sections have given us a linear system that we can solve for the numerical pressure  [2.x.6036] . We can use this to compute an approximation to the variable  [2.x.6037]  that corresponds to the velocity with which the medium flows in a porous medium if this is the model we are trying to solve. This kind of step -- computing a derived quantity from the solution of the discrete problem -- is typically called "post-processing". 

Here, instead of using the exact gradient of  [2.x.6038] , let us instead use the discrete weak gradient of  [2.x.6039]  to calculate the velocity on each element. As discussed above, on each element the gradient of the numerical pressure  [2.x.6040]  can be approximated by discrete weak gradients   [2.x.6041] : 

[1.x.2310] 



On cell  [2.x.6042] , the numerical velocity  [2.x.6043]  can be written as 

[1.x.2311] 

where  [2.x.6044]  is the expansion matrix from above, and  [2.x.6045]  is the basis function of the  [2.x.6046]  space on a cell. 

Unfortunately,  [2.x.6047]  may not be in the  [2.x.6048]  space (unless, of course, if  [2.x.6049]  is constant times the identity matrix). So, in order to represent it in a finite element program, we need to project it back into a finite dimensional space we can work with. Here, we will use the  [2.x.6050] -projection to project it back to the (broken)  [2.x.6051]  space. 

We define the projection as  [2.x.6052]  on each cell  [2.x.6053] . For any  [2.x.6054] ,  [2.x.6055]  So, rather than the formula shown above, the numerical velocity on cell  [2.x.6056]  instead becomes 

[1.x.2312] 

and we have the following system to solve for the coefficients  [2.x.6057] : 

[1.x.2313] 

In the implementation below, the matrix with elements  [2.x.6058]  is called  [2.x.6059] , whereas the matrix with elements  [2.x.6060]  is called  [2.x.6061] . 

Then the elementwise velocity is 

[1.x.2314] 

where  [2.x.6062]  is called `cell_velocity` in the code. 

Using this velocity obtained by "postprocessing" the solution, we can define the  [2.x.6063] -errors of pressure, velocity, and flux by the following formulas: 

[1.x.2315] 

where  [2.x.6064]  is the area of the element,  [2.x.6065]  are faces of the element,  [2.x.6066]  are unit normal vectors of each face. The last of these norms measures the accuracy of the normal component of the velocity vectors over the interfaces between the cells of the mesh. The scaling factor  [2.x.6067]  is chosen so as to scale out the difference in the length (or area) of the collection of interfaces as the mesh size changes. 

The first of these errors above is easily computed using  [2.x.6068]  The others require a bit more work and are implemented in the code below. 


examples/step-61/doc/results.dox 



[1.x.2316] 

We run the program with a right hand side that will produce the solution  [2.x.6069]  and with homogeneous Dirichlet boundary conditions in the domain  [2.x.6070] . In addition, we choose the coefficient matrix in the differential operator  [2.x.6071]  as the identity matrix. We test this setup using  [2.x.6072] ,  [2.x.6073]  and  [2.x.6074]  element combinations, which one can select by using the appropriate constructor argument for the `WGDarcyEquation` object in `main()`. We will then visualize pressure values in interiors of cells and on faces. We want to see that the pressure maximum is around 1 and the minimum is around 0. With mesh refinement, the convergence rates of pressure, velocity and flux should then be around 1 for  [2.x.6075]  , 2 for  [2.x.6076] , and 3 for  [2.x.6077] . 




[1.x.2317][1.x.2318] 

The following figures show interior pressures and face pressures using the  [2.x.6078]  element. The mesh is refined 2 times (top) and 4 times (bottom), respectively. (This number can be adjusted in the `make_grid()` function.) When the mesh is coarse, one can see the face pressures  [2.x.6079]  neatly between the values of the interior pressures  [2.x.6080]  on the two adjacent cells. 

 [2.x.6081]  

From the figures, we can see that with the mesh refinement, the maximum and minimum pressure values are approaching the values we expect. Since the mesh is a rectangular mesh and numbers of cells in each direction is even, we have symmetric solutions. From the 3d figures on the right, we can see that on  [2.x.6082] , the pressure is a constant in the interior of the cell, as expected. 

[1.x.2319][1.x.2320] 

We run the code with differently refined meshes (chosen in the `make_grid()` function) and get the following convergence rates of pressure, velocity, and flux (as defined in the introduction). 

 [2.x.6083]  

We can see that the convergence rates of  [2.x.6084]  are around 1. This, of course, matches our theoretical expectations. 




[1.x.2321][1.x.2322] 

We can repeat the experiment from above using the next higher polynomial degree: The following figures are interior pressures and face pressures implemented using  [2.x.6085] . The mesh is refined 4 times.  Compared to the previous figures using  [2.x.6086] , on each cell, the solution is no longer constant on each cell, as we now use bilinear polynomials to do the approximation. Consequently, there are 4 pressure values in one interior, 2 pressure values on each face. 

 [2.x.6087]  

Compared to the corresponding image for the  [2.x.6088]  combination, the solution is now substantially more accurate and, in particular so close to being continuous at the interfaces that we can no longer distinguish the interface pressures  [2.x.6089]  from the interior pressures  [2.x.6090]  on the adjacent cells. 

[1.x.2323][1.x.2324] 

The following are the convergence rates of pressure, velocity, and flux we obtain from using the  [2.x.6091]  element combination: 

 [2.x.6092]  

The convergence rates of  [2.x.6093]  are around 2, as expected. 




[1.x.2325][1.x.2326] 

Let us go one polynomial degree higher. The following are interior pressures and face pressures implemented using  [2.x.6094] , with mesh size  [2.x.6095]  (i.e., 5 global mesh refinement steps). In the program, we use `data_out_face.build_patches(fe.degree)` when generating graphical output (see the documentation of  [2.x.6096]  which here implies that we divide each 2d cell interior into 4 subcells in order to provide a better visualization of the quadratic polynomials.  [2.x.6097]  




[1.x.2327][1.x.2328] 

As before, we can generate convergence data for the  [2.x.6098]  errors of pressure, velocity, and flux using the  [2.x.6099]  combination: 

 [2.x.6100]  

Once more, the convergence rates of  [2.x.6101]  is as expected, with values around 3. 


examples/step-62/doc/intro.dox 

 [2.x.6102]  

[1.x.2329]  [2.x.6103]  




 [2.x.6104]  As a prerequisite of this program, you need to have HDF5, complex PETSc, and the p4est libraries installed. The installation of deal.II together with these additional libraries is described in the [1.x.2330] file. 

[1.x.2331] A phononic crystal is a periodic nanostructure that modifies the motion of mechanical vibrations or [phonons](https://en.wikipedia.org/wiki/Phonon). Phononic structures can be used to disperse, route and confine mechanical vibrations. These structures have potential applications in [quantum information](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.86.1391) and have been used to study [macroscopic quantum phenomena](https://science.sciencemag.org/content/358/6360/203). Phononic crystals are usually fabricated in [cleanrooms](https://en.wikipedia.org/wiki/Cleanroom). 

In this tutorial we show how to a design a [phononic superlattice cavity](https://doi.org/10.1103/PhysRevA.94.033813) which is a particular type of phononic crystal that can be used to confine mechanical vibrations. A phononic superlattice cavity is formed by two [Distributed Bragg Reflector](https://en.wikipedia.org/wiki/Distributed_Bragg_reflector), mirrors and a  [2.x.6105]  cavity where  [2.x.6106]  is the acoustic wavelength. Acoustic DBRs are  periodic structures where a set of bilayer stacks with contrasting physical properties (sound velocity index) is repeated  [2.x.6107]  times. Superlattice cavities are usually grown on a [Gallium Arsenide](https://en.wikipedia.org/wiki/Gallium_arsenide) wafer by [Molecular Beam Epitaxy](https://en.wikipedia.org/wiki/Molecular-beam_epitaxy). The bilayers correspond to GaAs/AlAs mirror pairs. As shown below, the thickness of the mirror layers (brown and green) is  [2.x.6108]  and the thickness of the cavity (blue) is  [2.x.6109] . 

 [2.x.6110]  

In this tutorial we calculate the [band gap](https://en.wikipedia.org/wiki/Band_gap) and the mechanical resonance of a phononic superlattice cavity but the code presented here can be easily used to design and calculate other types of [phononic crystals](https://science.sciencemag.org/content/358/6360/203). 

The device is a waveguide in which the wave goes from left to right. The simulations of this tutorial are done in 2D, but the code is dimension independent and can be easily used with 3D simulations. The waveguide width is equal to the  [2.x.6111]  dimension of the domain and the waveguide length is equal to the  [2.x.6112]  dimension of the domain. There are two regimes that depend on the waveguide width: 

- Single mode: In this case the width of the structure is much   smaller than the wavelength.   This case can be solved either with FEM (the approach that we take here) or with   a simple semi-analytical   [1D transfer matrix formalism](https://en.wikipedia.org/wiki/Transfer_matrix). 

- Multimode: In this case the width of the structure is larger than the wavelength.   This case can be solved using FEM   or with a [scattering matrix formalism](https://doi.org/10.1103/PhysRevA.94.033813).   Although we do not study this case in this tutorial, it is very easy to reach the multimode   regime by increasing the parameter waveguide width (`dimension_y` in the jupyter notebook). 

The simulations of this tutorial are performed in the frequency domain. To calculate the transmission spectrum, we use a [procedure](https://meep.readthedocs.io/en/latest/Python_Tutorials/Resonant_Modes_and_Transmission_in_a_Waveguide_Cavity/) that is commonly used in time domain [FDTD](https://en.wikipedia.org/wiki/Finite-difference_time-domain_method) simulations. A pulse at a certain frequency is generated on the left side of the structure and the transmitted energy is measured on the right side of the structure. The simulation is run twice. First, we run the simulation with the phononic structure and measure the transmitted energy: 

 [2.x.6113]  

Then, we run the simulation without the phononic structure and measure the transmitted energy. We use the simulation without the structure for the calibration: 

 [2.x.6114]  

The transmission coefficient corresponds to the energy of the first simulation divided by the calibration energy. We repeat this procedure for each frequency step. 




[1.x.2332] What we want to simulate here is the transmission of elastic waves. Consequently, the right description of the problem uses the elastic equations, which in the time domain are given by 

[1.x.2333] 

where the stiffness tensor  [2.x.6115]  depends on the spatial coordinates and the strain is the symmetrized gradient of the displacement, given by 

[1.x.2334] 



[A perfectly matched layer (PML)](https://en.wikipedia.org/wiki/Perfectly_matched_layer) can be used to truncate the solution at the boundaries. A PML is a transformation that results in a complex coordinate stretching. 

Instead of a time domain approach, this tutorial program converts the equations above into the frequency domain by performing a Fourier transform with regard to the time variable. The elastic equations in the frequency domain then read as follows 

[1.x.2335] 

where the coefficients  [2.x.6116]  account for the absorption. There are 3  [2.x.6117]  coefficients in 3D and 2 in 2D. The imaginary par of  [2.x.6118]  is equal to zero outside the PML. The PMLs are reflectionless only for the exact wave equations. When the set of equations is discretized the PML is no longer reflectionless. The reflections can be made arbitrarily small as long as the medium is slowly varying, see [the adiabatic theorem](https://doi.org/10.1103/PhysRevE.66.066608). In the code a quadratic turn-on of the PML has been used. A linear and cubic turn-on is also [known to work](https://doi.org/10.1364/OE.16.011376). These equations can be expanded into 

[1.x.2336] 



[1.x.2337] 

where summation over repeated indices (here  [2.x.6119] , as well as  [2.x.6120]  and  [2.x.6121] ) is as always implied. Note that the strain is no longer symmetric after applying the complex coordinate stretching of the PML. This set of equations can be written as 

[1.x.2338] 



The same as the strain, the stress tensor is not symmetric inside the PML ( [2.x.6122] ). Indeed the fields inside the PML are not physical. It is useful to introduce the tensors  [2.x.6123]  and  [2.x.6124] . 

[1.x.2339] 



We can multiply by  [2.x.6125]  and integrate over the domain  [2.x.6126]  and integrate by parts. 

[1.x.2340] 

It is this set of equations we want to solve for a set of frequencies  [2.x.6127]  in order to compute the transmission coefficient as function of frequency. The linear system becomes 

[1.x.2341] 



[1.x.2342] In this tutorial we use a python [jupyter notebook](https://github.com/dealii/dealii/blob/master/examples/step-62/step-62.ipynb) to set up the parameters and run the simulation. First we create a HDF5 file where we store the parameters and the results of the simulation. 

Each of the simulations (displacement and calibration) is stored in a separate HDF5 group: 

[1.x.2343] 




examples/step-62/doc/results.dox 



[1.x.2344] 

[1.x.2345] 

The results are analyzed in the [jupyter notebook](https://github.com/dealii/dealii/blob/master/examples/step-62/step-62.ipynb) with the following code 

[1.x.2346] 



A phononic cavity is characterized by the [resonance frequency](https://en.wikipedia.org/wiki/Resonance) and the [the quality factor](https://en.wikipedia.org/wiki/Q_factor). The quality factor is equal to the ratio between the stored energy in the resonator and the energy dissipated energy per cycle, which is approximately equivalent to the ratio between the resonance frequency and the [full width at half maximum (FWHM)](https://en.wikipedia.org/wiki/Full_width_at_half_maximum). The FWHM is equal to the bandwidth over which the power of vibration is greater than half the power at the resonant frequency. 

[1.x.2347] 



The square of the amplitude of the mechanical resonance  [2.x.6128]  as a function of the frequency has a gaussian shape 

[1.x.2348] 

where  [2.x.6129]  is the resonance frequency and  [2.x.6130]  is the dissipation rate. We used the previous equation in the jupyter notebook to fit the mechanical resonance. 

Given the values we have chosen for the parameters, one could estimate the resonance frequency analytically. Indeed, this is then confirmed by what we get in this program: the phononic superlattice cavity exhibits a mechanical resonance at 20GHz and a quality factor of 5046. The following images show the transmission amplitude and phase as a function of frequency in the vicinity of the resonance frequency: 

 [2.x.6131]   [2.x.6132]  

The images above suggest that the periodic structure has its intended effect: It really only lets waves of a very specific frequency pass through, whereas all other waves are reflected. This is of course precisely what one builds these sorts of devices for. But it is not quite this easy. In practice, there is really only a "band gap", i.e., the device blocks waves other than the desired one at 20GHz only within a certain frequency range. Indeed, to find out how large this "gap" is within which waves are blocked, we can extend the frequency range to 16 GHz through the appropriate parameters in the input file. We then obtain the following image: 

 [2.x.6133]  

What this image suggests is that in the range of around 18 to around 22 GHz, really only the waves with a frequency of 20 GHz are allowed to pass through, but beyond this range, there are plenty of other frequencies that can pass through the device. 

[1.x.2349] 

We can inspect the mode profile with Paraview or VisIt. As we have discussed, at resonance all the mechanical energy is transmitted and the amplitude of motion is amplified inside the cavity. It can be observed that the PMLs are quite effective to truncate the solution. The following image shows the mode profile at resonance: 

 [2.x.6134]  

On the other hand,  out of resonance all the mechanical energy is reflected. The following image shows the profile at 19.75 GHz. Note the interference between the force pulse and the reflected wave at the position  [2.x.6135] . 

 [2.x.6136]  

[1.x.2350] 

Phononic superlattice cavities find application in [quantum optomechanics](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.86.1391). Here we have presented the simulation of a 2D superlattice cavity, but this code can be used as well to simulate "real world" 3D devices such as [micropillar superlattice cavities](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.060101), which are promising candidates to study macroscopic quantum phenomena. The 20GHz mode of a micropillar superlattice cavity is essentially a mechanical harmonic oscillator that is very well isolated from the environment. If the device is cooled down to 20mK in a dilution fridge, the mode would then become a macroscopic quantum harmonic oscillator. 




[1.x.2351] 

Instead of setting the parameters in the C++ file we could set the parameters using a python script and save them in the HDF5 file that we will use for the simulations. Then the deal.II program will read the parameters from the HDF5 file. 

[1.x.2352] 



In order to read the HDF5 parameters we have to use the  [2.x.6137]  flag. 

[1.x.2353] 




examples/step-63/doc/intro.dox 

 [2.x.6138]  

[1.x.2354] 

 [2.x.6139]  

[1.x.2355] 

[1.x.2356] 

This program solves an advection-diffusion problem using a geometric multigrid (GMG) preconditioner. The basics of this preconditioner are discussed in step-16; here we discuss the necessary changes needed for a non-symmetric PDE. Additionally, we introduce the idea of block smoothing (as compared to point smoothing in step-16), and examine the effects of DoF renumbering for additive and multiplicative smoothers. 

[1.x.2357] The advection-diffusion equation is given by 

[1.x.2358] 

where  [2.x.6140] ,  [2.x.6141]  is the [1.x.2359], and  [2.x.6142]  is a source. A few notes: 

1. If  [2.x.6143] , this is the Laplace equation solved in step-16 (and many other places). 

2. If  [2.x.6144]  then this is the stationary advection equation solved in step-9. 

3. One can define a dimensionless number for this problem, called the [1.x.2360]:  [2.x.6145] , where  [2.x.6146]  is the length scale of the domain. It characterizes the kind of equation we are considering: If  [2.x.6147] , we say the problem is [1.x.2361], else if  [2.x.6148]  we will say the problem is [1.x.2362]. 

For the discussion in this tutorial we will be concerned with advection-dominated flow. This is the complicated case: We know that for diffusion-dominated problems, the standard Galerkin method works just fine, and we also know that simple multigrid methods such as those defined in step-16 are very efficient. On the other hand, for advection-dominated problems, the standard Galerkin approach leads to oscillatory and unstable discretizations, and simple solvers are often not very efficient. This tutorial program is therefore intended to address both of these issues. 




[1.x.2363] 

Using the standard Galerkin finite element method, for suitable test functions  [2.x.6149] , a discrete weak form of the PDE would read 

[1.x.2364] 

where 

[1.x.2365] 



Unfortunately, one typically gets oscillatory solutions with this approach. Indeed, the following error estimate can be shown for this formulation: 

[1.x.2366] 

The infimum on the right can be estimated as follows if the exact solution is sufficiently smooth: 

[1.x.2367] 

where  [2.x.6150]  is the polynomial degree of the finite elements used. As a consequence, we obtain the estimate 

[1.x.2368] 

In other words, the numerical solution will converge. On the other hand, given the definition of  [2.x.6151]  above, we have to expect poor numerical solutions with a large error when  [2.x.6152] , i.e., if the problem has only a small amount of diffusion. 

To combat this, we will consider the new weak form 

[1.x.2369] 

where the sum is done over all cells  [2.x.6153]  with the inner product taken for each cell, and  [2.x.6154]  is a cell-wise constant stabilization parameter defined in  [2.x.6155] . 

Essentially, adding in the discrete strong form residual enhances the coercivity of the bilinear form  [2.x.6156]  which increases the stability of the discrete solution. This method is commonly referred to as [1.x.2370] or [1.x.2371] (streamline upwind/Petrov-Galerkin). 




[1.x.2372] 

One of the goals of this tutorial is to expand from using a simple (point-wise) Gauss-Seidel (SOR) smoother that is used in step-16 (class PreconditionSOR) on each level of the multigrid hierarchy. The term "point-wise" is traditionally used in solvers to indicate that one solves at one "grid point" at a time; for scalar problems, this means to use a solver that updates one unknown of the linear system at a time, keeping all of the others fixed; one would then iterate over all unknowns in the problem and, once done, start over again from the first unknown until these "sweeps" converge. Jacobi, Gauss-Seidel, and SOR iterations can all be interpreted in this way. In the context of multigrid, one does not think of these methods as "solvers", but as "smoothers". As such, one is not interested in actually solving the linear system. It is enough to remove the high-frequency part of the residual for the multigrid method to work, because that allows restricting the solution to a coarser mesh.  Therefore, one only does a few, fixed number of "sweeps" over all unknowns. In the code in this tutorial this is controlled by the "Smoothing steps" parameter. 

But these methods are known to converge rather slowly when used as solvers. While as multigrid smoothers, they are surprisingly good, they can also be improved upon. In particular, we consider "cell-based" smoothers here as well. These methods solve for all unknowns on a cell at once, keeping all other unknowns fixed; they then move on to the next cell, and so on and so forth. One can think of them as "block" versions of Jacobi, Gauss-Seidel, or SOR, but because degrees of freedom are shared among multiple cells, these blocks overlap and the methods are in fact best be explained within the framework of additive and multiplicative Schwarz methods. 

In contrast to step-16, our test problem contains an advective term. Especially with a small diffusion constant  [2.x.6157] , information is transported along streamlines in the given advection direction. This means that smoothers are likely to be more effective if they allow information to travel in downstream direction within a single smoother application. If we want to solve one unknown (or block of unknowns) at a time in the order in which these unknowns (or blocks) are enumerated, then this information propagation property requires reordering degrees of freedom or cells (for the cell-based smoothers) accordingly so that the ones further upstream are treated earlier (have lower indices) and those further downstream are treated later (have larger indices). The influence of the ordering will be visible in the results section. 

Let us now briefly define the smoothers used in this tutorial. For a more detailed introduction, we refer to  [2.x.6158]  and the books  [2.x.6159]  and  [2.x.6160] . A Schwarz preconditioner requires a decomposition 

[1.x.2373] 

of our finite element space  [2.x.6161] . Each subproblem  [2.x.6162]  also has a Ritz projection  [2.x.6163]  based on the bilinear form  [2.x.6164] . This projection induces a local operator  [2.x.6165]  for each subproblem  [2.x.6166] . If  [2.x.6167]  is the orthogonal projector onto  [2.x.6168] , one can show  [2.x.6169] . 

With this we can define an [1.x.2374] for the operator  [2.x.6170]  as 

[1.x.2375] 

In other words, we project our solution into each subproblem, apply the inverse of the subproblem  [2.x.6171] , and sum the contributions up over all  [2.x.6172] . 

Note that one can interpret the point-wise (one unknown at a time) Jacobi method as an additive Schwarz method by defining a subproblem  [2.x.6173]  for each degree of freedom. Then,  [2.x.6174]  becomes a multiplication with the inverse of a diagonal entry of  [2.x.6175] . 

For the "Block Jacobi" method used in this tutorial, we define a subproblem  [2.x.6176]  for each cell of the mesh on the current level. Note that we use a continuous finite element, so these blocks are overlapping, as degrees of freedom on an interface between two cells belong to both subproblems. The logic for the Schwarz operator operating on the subproblems (in deal.II they are called "blocks") is implemented in the class RelaxationBlock. The "Block Jacobi" method is implemented in the class RelaxationBlockJacobi. Many aspects of the class (for example how the blocks are defined and how to invert the local subproblems  [2.x.6177] ) can be configured in the smoother data, see  [2.x.6178]  and  [2.x.6179]  for details. 

So far, we discussed additive smoothers where the updates can be applied independently and there is no information flowing within a single smoother application. A [1.x.2376] addresses this and is defined by 

[1.x.2377] 

In contrast to above, the updates on the subproblems  [2.x.6180]  are applied sequentially. This means that the update obtained when inverting the subproblem  [2.x.6181]  is immediately used in  [2.x.6182] . This becomes visible when writing out the project: 

[1.x.2378] 



When defining the sub-spaces  [2.x.6183]  as whole blocks of degrees of freedom, this method is implemented in the class RelaxationBlockSOR and used when you select "Block SOR" in this tutorial. The class RelaxationBlockSOR is also derived from RelaxationBlock. As such, both additive and multiplicative Schwarz methods are implemented in a unified framework. 

Finally, let us note that the standard Gauss-Seidel (or SOR) method can be seen as a multiplicative Schwarz method with a subproblem for each DoF. 




[1.x.2379] 

We will be considering the following test problem:  [2.x.6184] , i.e., a square with a circle of radius 0.3 centered at the origin removed. In addition, we use  [2.x.6185] ,  [2.x.6186] ,  [2.x.6187] , and Dirichlet boundary values 

[1.x.2380] 



The following figures depict the solutions with (left) and without (right) streamline diffusion. Without streamline diffusion we see large oscillations around the boundary layer, demonstrating the instability of the standard Galerkin finite element method for this problem. 

 [2.x.6188]  


examples/step-63/doc/results.dox 



[1.x.2381] 

[1.x.2382] 

The major advantage for GMG is that it is an  [2.x.6189]  method, that is, the complexity of the problem increases linearly with the problem size. To show then that the linear solver presented in this tutorial is in fact  [2.x.6190] , all one needs to do is show that the iteration counts for the GMRES solve stay roughly constant as we refine the mesh. 

Each of the following tables gives the GMRES iteration counts to reduce the initial residual by a factor of  [2.x.6191] . We selected a sufficient number of smoothing steps (based on the method) to get iteration numbers independent of mesh size. As can be seen from the tables below, the method is indeed  [2.x.6192] . 

[1.x.2383] 

The point-wise smoothers ("Jacobi" and "SOR") get applied in the order the DoFs are numbered on each level. We can influence this using the DoFRenumbering namespace. The block smoothers are applied based on the ordering we set in `setup_smoother()`. We can visualize this numbering. The following pictures show the cell numbering of the active cells in downstream, random, and upstream numbering (left to right): 

 [2.x.6193]  

Let us start with the additive smoothers. The following table shows the number of iterations necessary to obtain convergence from GMRES: 

 [2.x.6194]  

We see that renumbering the DoFs/cells has no effect on convergence speed. This is because these smoothers compute operations on each DoF (point-smoother) or cell (block-smoother) independently and add up the results. Since we can define these smoothers as an application of a sum of matrices, and matrix addition is commutative, the order at which we sum the different components will not affect the end result. 

On the other hand, the situation is different for multiplicative smoothers: 

 [2.x.6195]  

Here, we can speed up convergence by renumbering the DoFs/cells in the advection direction, and similarly, we can slow down convergence if we do the renumbering in the opposite direction. This is because advection-dominated problems have a directional flow of information (in the advection direction) which, given the right renumbering of DoFs/cells, multiplicative methods are able to capture. 

This feature of multiplicative methods is, however, dependent on the value of  [2.x.6196] . As we increase  [2.x.6197]  and the problem becomes more diffusion-dominated, we have a more uniform propagation of information over the mesh and there is a diminished advantage for renumbering in the advection direction. On the opposite end, in the extreme case of  [2.x.6198]  (advection-only), we have a 1st-order PDE and multiplicative methods with the right renumbering become effective solvers: A correct downstream numbering may lead to methods that require only a single iteration because information can be propagated from the inflow boundary downstream, with no information transport in the opposite direction. (Note, however, that in the case of  [2.x.6199] , special care must be taken for the boundary conditions in this case). 




[1.x.2384] 

We will limit the results to runs using the downstream renumbering. Here is a cross comparison of all four smoothers for both  [2.x.6200]  and  [2.x.6201]  elements: 

 [2.x.6202]  

We see that for  [2.x.6203] , both multiplicative smoothers require a smaller combination of smoothing steps and iteration counts than either additive smoother. However, when we increase the degree to a  [2.x.6204]  element, there is a clear advantage for the block smoothers in terms of the number of smoothing steps and iterations required to solve. Specifically, the block SOR smoother gives constant iteration counts over the degree, and the block Jacobi smoother only sees about a 38% increase in iterations compared to 75% and 183% for Jacobi and SOR respectively. 

[1.x.2385] 

Iteration counts do not tell the full story in the optimality of a one smoother over another. Obviously we must examine the cost of an iteration. Block smoothers here are at a disadvantage as they are having to construct and invert a cell matrix for each cell. Here is a comparison of solve times for a  [2.x.6205]  element with 74,496 DoFs: 

 [2.x.6206]  

The smoother that requires the most iterations (Jacobi) actually takes the shortest time (roughly 2/3 the time of the next fastest method). This is because all that is required to apply a Jacobi smoothing step is multiplication by a diagonal matrix which is very cheap. On the other hand, while SOR requires over 3x more iterations (each with 3x more smoothing steps) than block SOR, the times are roughly equivalent, implying that a smoothing step of block SOR is roughly 9x slower than a smoothing step of SOR. Lastly, block Jacobi is almost 6x more expensive than block SOR, which intuitively makes sense from the fact that 1 step of each method has the same cost (inverting the cell matrices and either adding or multiply them together), and block Jacobi has 3 times the number of smoothing steps per iteration with 2 times the iterations. 




[1.x.2386] 

There are a few more important points to mention: 

<ol>  [2.x.6207]  For a mesh distributed in parallel, multiplicative methods cannot be executed over the entire domain. This is because they operate one cell at a time, and downstream cells can only be handled once upstream cells have already been done. This is fine on a single processor: The processor just goes through the list of cells one after the other. However, in parallel, it would imply that some processors are idle because upstream processors have not finished doing the work on cells upstream from the ones owned by the current processor. Once the upstream processors are done, the downstream ones can start, but by that time the upstream processors have no work left. In other words, most of the time during these smoother steps, most processors are in fact idle. This is not how one obtains good parallel scalability! 

One can use a hybrid method where a multiplicative smoother is applied on each subdomain, but as you increase the number of subdomains, the method approaches the behavior of an additive method. This is a major disadvantage to these methods.  [2.x.6208]  

 [2.x.6209]  Current research into block smoothers suggest that soon we will be able to compute the inverse of the cell matrices much cheaper than what is currently being done inside deal.II. This research is based on the fast diagonalization method (dating back to the 1960s) and has been used in the spectral community for around 20 years (see, e.g., [1.x.2387]). There are currently efforts to generalize these methods to DG and make them more robust. Also, it seems that one should be able to take advantage of matrix-free implementations and the fact that, in the interior of the domain, cell matrices tend to look very similar, allowing fewer matrix inverse computations.  [2.x.6210]   [2.x.6211]  

Combining 1. and 2. gives a good reason for expecting that a method like block Jacobi could become very powerful in the future, even though currently for these examples it is quite slow. 




[1.x.2388] 

[1.x.2389] 

Change the number of smoothing steps and the smoother relaxation parameter (set in  [2.x.6212]  inside  [2.x.6213] , only necessary for point smoothers) so that we maintain a constant number of iterations for a  [2.x.6214]  element. 

[1.x.2390] 

Increase/decrease the parameter "Epsilon" in the `.prm` files of the multiplicative methods and observe for which values renumbering no longer influences convergence speed. 

[1.x.2391] 

The code is set up to work correctly with an adaptively refined mesh (the interface matrices are created and set). Devise a suitable refinement criterium or try the KellyErrorEstimator class. 


examples/step-64/doc/intro.dox 

 [2.x.6215]  

[1.x.2392] 




[1.x.2393] 

This example shows how to implement a matrix-free method on the GPU using CUDA for the Helmholtz equation with variable coefficients on a hypercube. The linear system will be solved using the conjugate gradient method and is parallelized  with MPI. 

In the last few years, heterogeneous computing in general and GPUs in particular have gained a lot of popularity. This is because GPUs offer better computing capabilities and memory bandwidth than CPUs for a given power budget. Among the architectures available in early 2019, GPUs are about 2x-3x as power efficient than server CPUs with wide [1.x.2394] for PDE-related tasks. GPUs are also the most popular architecture for machine learning. On the other hand, GPUs are not easy to program. This program explores the deal.II capabilities to see how efficiently such a program can be implemented. 

While we have tried for the interface of the matrix-free classes for the CPU and the GPU to be as close as possible, there are a few differences. When using the matrix-free framework on a GPU, one must write some CUDA code. However, the amount is fairly small and the use of CUDA is limited to a few keywords. 




[1.x.2395] 

In this example, we consider the Helmholtz problem [1.x.2396] 

where  [2.x.6216]  is a variable coefficient. 

We choose as domain  [2.x.6217]  and  [2.x.6218] . Since the coefficient is symmetric around the origin but the domain is not, we will end up with a non-symmetric solution. 

If you've made it this far into the tutorial, you will know how the weak formulation of this problem looks like and how, in principle, one assembles linear systems for it. Of course, in this program we will in fact not actually form the matrix, but rather only represent its action when one multiplies with it. 




[1.x.2397] 

GPUs (we will use the term "device" from now on to refer to the GPU) have their own memory that is separate from the memory accessible to the CPU (we will use the term "host" from now on). A normal calculation on the device can be divided in three separate steps: 

 -# the data is moved from the host to the device, 

 -# the computation is done on the device, 

 -# the result is moved back from the device to the host 

The data movements can either be done explicitly by the user code or done automatically using UVM (Unified Virtual Memory). In deal.II, only the first method is supported. While it means an extra burden for the user, this allows for better control of data movement and more importantly it avoids to mistakenly run important kernels on the host instead of the device. 

The data movement in deal.II is done using  [2.x.6219]  These vectors can be seen as buffers on the host that are used to either store data received from the device or to send data to the device. There are two types of vectors that can be used on the device: 

-  [2.x.6220]  which is similar to the more common Vector<Number>, and 

-  [2.x.6221]   [2.x.6222]  which is a regular  [2.x.6223]  where we have specified which memory space to use. 

If no memory space is specified, the default is  [2.x.6224]  

Next, we show how to move data to/from the device using  [2.x.6225]  

[1.x.2398] 

Both of the vector classes used here only work on a single machine, i.e., one memory space on a host and one on a device. 

But there are cases where one wants to run a parallel computation between multiple MPI processes on a number of machines, each of which is equipped with GPUs. In that case, one wants to use  [2.x.6226]  which is similar but the `import()` stage may involve MPI communication: 

[1.x.2399] 

The `relevant_rw_vector` is an object that stores a subset of all elements of the vector. Typically, these are the  [2.x.6227]  "locally relevant DoFs", which implies that they overlap between different MPI processes. Consequently, the elements stored in that vector on one machine may not coincide with the ones stored by the GPU on that machine, requiring MPI communication to import them. 

In all of these cases, while importing a vector, values can either be inserted (using  [2.x.6228]  or added to prior content of the vector (using  [2.x.6229]  




[1.x.2400] 

The code necessary to evaluate the matrix-free operator on the device is very similar to the one on the host. However, there are a few differences, the main ones being that the `local_apply()` function in Step-37 and the loop over quadrature points both need to be encapsulated in their own functors. 


examples/step-64/doc/results.dox 



[1.x.2401] 

Since the main purpose of this tutorial is to demonstrate how to use the  [2.x.6230]  interface, not to compute anything useful in itself, we just show the expected output here: 

[1.x.2402] 



One can make two observations here: First, the norm of the numerical solution converges, presumably to the norm of the exact (but unknown) solution. And second, the number of iterations roughly doubles with each refinement of the mesh. (This is in keeping with the expectation that the number of CG iterations grows with the square root of the condition number of the matrix; and that we know that the condition number of the matrix of a second-order differential operation grows like  [2.x.6231] .) This is of course rather inefficient, as an optimal solver would have a number of iterations that is independent of the size of the problem. But having such a solver would require using a better preconditioner than the identity matrix we have used here. 


[1.x.2403] 

[1.x.2404] 

Currently, this program uses no preconditioner at all. This is mainly since constructing an efficient matrix-free preconditioner is non-trivial.  However, simple choices just requiring the diagonal of the corresponding matrix are good candidates and these can be computed in a matrix-free way as well. Alternatively, and maybe even better, one could extend the tutorial to use multigrid with Chebyshev smoothers similar to step-37. 


examples/step-65/doc/intro.dox 



 [2.x.6232]  

[1.x.2405] 

[1.x.2406] 

[1.x.2407] 

This tutorial program presents an advanced manifold class, TransfiniteInterpolationManifold, and how to work around its main disadvantage, the relatively high cost. 

[1.x.2408] 

[1.x.2409] 

In many applications, the finite element mesh must be able to represent a relatively complex geometry. In the step-1, step-49, and step-53 tutorial programs, some techniques to generate grids available within the deal.II library have been introduced. Given a base mesh, deal.II is then able to create a finer mesh by subdividing the cells into children, either uniformly or only in selected parts of the computational domain. Besides the basic meshing capabilities collected in the GridGenerator namespace, deal.II also comes with interfaces to read in meshes generated by (quad- and hex-only) mesh generators using the functions of namespace GridIn, as for example demonstrated in step-5. A fundamental limitation of externally generated meshes is that the information provided by the generated cells in the mesh only consists of the position of the vertices and their connectivity, without the context of the underlying geometry that used to be available in the mesh generator that originally created this mesh. This becomes problematic once the mesh is refined within deal.II and additional points need to be placed. The step-54 tutorial program shows how to overcome this limitation by using CAD surfaces in terms of the OpenCASCADE library, and step-53 by providing the same kind of information programmatically from within the source code. 

Within deal.II, the placement of new points during mesh refinement or for the definition of higher order mappings is controlled by manifold objects, see the  [2.x.6233]  "manifold module" for details. To give an example, consider the following situation of a two-dimensional annulus (with pictures taken from the manifold module). If we start with an initial mesh of 10 cells and refine the mesh three times globally without attaching any manifolds, we would obtain the following mesh: 

 [2.x.6234]  

The picture looks like this because, by default, deal.II only knows where to put the vertices of child cells by averaging the locations of the vertices of the parent cell. This yields a polygonal domain whose faces are the edges of the original (coarse mesh) cells. Obviously, we must attach a curved description to the boundary faces of the triangulation to reproduce the circular shape upon mesh refinement, like in the following picture: 

 [2.x.6235]  

This is better: At least the inner and outer boundaries are now approaching real circles if we continue to refine the mesh. However, the mesh in this picture is still not optimal for an annulus in the sense that the [1.x.2410] lines from one cell to the next have kinks at certain vertices, and one would rather like to use the following mesh: 

 [2.x.6236]  

In this last (optimal) case, which is also the default produced by  [2.x.6237]  the curved manifold description (in this case a polar manifold description) is applied not only to the boundary faces, but to the whole domain. Whenever the triangulation requests a new point, e.g., the mid point of the edges or the cells when it refines a cell into four children, it will place them along the respective mid points in the polar coordinate system. By contrast, the case above where only the boundary was subject to the polar manifold, only mid points along the boundary would be placed along the curved description, whereas mid points in the interior would be computed by suitable averages of the surrounding points in the Cartesian coordinate system (see the  [2.x.6238]  "manifold module" for more details). 

At this point, one might assume that curved volume descriptions are the way to go. This is generally not wrong, though it is sometimes not so easy to describe how exactly this should work. Here are a couple of examples: 

- Imagine that the mesh above had actually been a disk, not just a ring.   In that case the polar manifold degenerates at the origin and   would not produce reasonable new points. In fact, defining a   manifold description for things that are supposed "to look round"   but might have points at or close to the origin is surprisingly very   difficult. 

- A similar thing happens at the origin   of the three-dimensional ball when one tries to attach a spherical manifold to   the whole volume &ndash; in this case, the computation of new manifold points   would abort with an exception. 

- CAD geometries often only describe the boundary of the domain, in a   similar way to how we only attached a manifold to the boundary in   the second picture above. Similarly, step-54 only uses the CAD   geometry to generate a surface mesh (maybe because that is what is   needed to solve the problem in question), but if one wanted to solve   a problem in the water or the air around the ship described there,   we would need to have a volume mesh. The question is then how   exactly we should describe what is supposed to happen in the   interior of the domain. 

These simple examples make it clear that for many interesting cases we must step back from the desire to have an analytic curved description for the full volume: There will need to be [1.x.2411] kind of information that leads to curvature also in the interior, but it must be possible to do this without actually writing down an explicit formula that describes the kind of geometry. 

So what happens if we don't do anything at all in the interior and only describe the surface as a manifold? Sometimes, as in the ring shown above, the result is not terrible. But sometimes it is. Consider the case of a torus (e.g. generated with  [2.x.6239]  with a TorusManifold object attached to the surface only, no additional manifolds on the interior cells and faces, and with six cells in toroidal direction before refinement. If the mesh is refined once, we would obtain the following mesh, shown with the upper half of the mesh clipped away: 

 [2.x.6240]  

This is clearly sub-optimal. Indeed, if we had started with fewer than the six cells shown above in toroidal direction, the mapping actually inverts in some regions because the new points placed along interior cells intersect with the boundary as they are not following the circular shape along the toroidal direction. The simple case of a torus can still be fixed because we know that the toroidal direction follows a cylindrical coordinate system, so attaching a TorusManifold to the surface combined with CylindricalManifold with appropriate periodicity in toroidal direction applied to all interior entities would produce a high-quality mesh as follows, now shown with two top cells hidden: 

 [2.x.6241]  

This mesh is pretty good, but obviously it is linked to a good description of the volume, which we lack in other cases. Actually, there is an imperfection also in this case, as we can see some unnatural kinks of two adjacent cells in the interior of the domain which are hidden by the top two boundary cells, as opposed to the following setup (the default manifolds applied by  [2.x.6242]  and using the TransfiniteInterpolationManifold): 

 [2.x.6243]  

[1.x.2412] 

In order to find a better strategy, let us look at the two-dimensional disk again (that is also the base entity rotated along the toroidal direction in the torus). As we learned above, we can only apply the curved polar description to the boundary (or a rim of cells sufficiently far away from the origin) but must eventually transition to a straight description towards the disk's center. If we use a flat manifold in the interior of the cells (i.e., one in which new vertices are created by averaging of the adjacent existing ones) and a polar manifold only for the boundary of the disk, we get the following mesh upon four global refinements: 

 [2.x.6244]  

That's not a terrible mesh. At the same time, if you know that the original coarse mesh consisted of a single square in the middle, with four caps around it, then it's not hard to see every refinement step that happened to this mesh to get the picture above. 

While the triangulation class of deal.II tries to propagate information from the boundary into the interior when creating new points, the reach of this algorithm is limited: 

 [2.x.6245]  

The picture above highlights those cells on the disk that are touching the boundary and where boundary information could in principle be taken into account when only looking at a single cell at the time. Clearly, the area where some curvature can be taken into account gets more limited as the mesh is refined, thus creating the seemingly irregular spots in the mesh: When computing the center of any one of the boundary cells in the leftmost picture, the ideal position is the mid point between the outer circle and the cell in the middle. This is exactly what is used for the first refinement step in the Triangulation class. However, for the second refinement all interior edges as well as the interior cell layers can only add points according to a flat manifold description. 

At this point, we realize what would be needed to create a better mesh: For [1.x.2413] new points in [1.x.2414] child cell that is created within the red shaded layer on the leftmost picture, we want to compute the interpolation with respect to the curvature in the area covered by the respective coarse cell. This is achieved by adding the class TransfiniteInterpolationManifold to the highlighted cells of the coarse grid in the leftmost panel of the figure above. This class adheres to the general manifold interfaces, i.e., given any set of points within its domain of definition, it can compute weighted averages conforming to the manifold (using a formula that will be given in a minute). These weighted averages are used whenever the mesh is refined, or when a higher order mapping (such as MappingQGeneric or MappingC1) is evaluated on a given cell subject to this manifold. Using this manifold on the shaded cells of the coarse grid of the disk (i.e., not only in the outer-most layer of cells) produces the following mesh upon four global steps of refinement: 

 [2.x.6246]  

There are still some kinks in the lines of this mesh, but they are restricted to the faces between coarse mesh cells, whereas the rest of the mesh is about as smooth as one would like. Indeed, given a straight-sided central cell, this representation is the best possible one as all mesh cells follow a smooth transition from the straight sides in the square block in the interior to the circular shape on the boundary. (One could possibly do a bit better by allowing some curvature also in the central square block, that eventually vanishes as the center is approached.) 




[1.x.2415] 

In the simple case of a disk with one curved and three straight edges, we can explicitly write down how to achieve the blending of the shapes. For this, it is useful to map the physical cell, like the top one, back to the reference coordinate system  [2.x.6247]  where we compute averages between certain points. If we were to use a simple bilinear map spanned by four vertices  [2.x.6248] , the image of a point  [2.x.6249]  would be 

[1.x.2416] 



For the case of the curved surface, we want to modify this formula. For the top cell of the coarse mesh of the disk, we can assume that the points  [2.x.6250]  and  [2.x.6251]  sit along the straight line at the lower end and the points  [2.x.6252]  and  [2.x.6253]  are connected by a quarter circle along the top. We would then map a point  [2.x.6254]  as 

[1.x.2417] 

where  [2.x.6255]  is a curve that describes the  [2.x.6256]  coordinates of the quarter circle in terms of an arclength parameter  [2.x.6257] . This represents a linear interpolation between the straight lower edge and the curved upper edge of the cell, and is the basis for the picture shown above. 

This formula is easily generalized to the case where all four edges are described by a curve rather than a straight line. We call the four functions, parameterized by a single coordinate  [2.x.6258]  or  [2.x.6259]  in the horizontal and vertical directions,  [2.x.6260]  for the left, right, lower, and upper edge of a quadrilateral, respectively. The interpolation then reads 

[1.x.2418] 



This formula assumes that the boundary curves match and coincide with the vertices  [2.x.6261] , e.g.  [2.x.6262]  or  [2.x.6263] . The subtraction of the bilinear interpolation in the second line of the formula makes sure that the prescribed curves are followed exactly on the boundary: Along each of the four edges, we need to subtract the contribution of the two adjacent edges evaluated in the corners, which is then simply a vertex position. It is easy to check that the formula for the circle above is reproduced if three of the four curves  [2.x.6264]  are straight and thus coincide with the bilinear interpolation. 

This formula, called transfinite interpolation, was introduced in 1973 by [1.x.2419]. Even though transfinite interpolation essentially only represents a linear blending of the bounding curves, the interpolation exactly follows the boundary curves for each real number  [2.x.6265]  or  [2.x.6266] , i.e., it interpolates in an infinite number of points, which was the original motivation to label this variant of interpolation a transfinite one by Gordon and Hall. Another interpretation is that the transfinite interpolation interpolates from the left and right and the top and bottom linearly, from which we need to subtract the bilinear interpolation to ensure a unit weight in the interior of the domain. 

The transfinite interpolation is easily generalized to three spatial dimensions. In that case, the interpolation allows to blend 6 different surface descriptions for any of the quads of a three-dimensional cell and 12 edge descriptions for the lines of a cell. Again, to ensure a consistent map, it is necessary to subtract the contribution of edges and add the contribution of vertices again to make the curves follow the prescribed surface or edge description. In the three-dimensional case, it is also possible to use a transfinite interpolation from a curved edge both into the adjacent faces and the adjacent cells. 

The interpolation of the transfinite interpolation in deal.II is general in the sense that it can deal with arbitrary curves. It will evaluate the curves in terms of their original coordinates of the  [2.x.6267] -dimensional space but with one (or two, in the case of edges in 3D) coordinate held fixed at  [2.x.6268]  or  [2.x.6269]  to ensure that any other manifold class, including CAD files if desired, can be applied out of the box. Transfinite interpolation is a standard ingredient in mesh generators, so the main strength of the integration of this feature within the deal.II library is to enable it during adaptive refinement and coarsening of the mesh, and for creating higher-degree mappings that use manifolds to insert additional points beyond the mesh vertices. 

As a final remark on transfinite interpolation, we mention that the mesh refinement strategies in deal.II in absence of a volume manifold description are also based on the weights of the transfinite interpolation and optimal in that sense. The difference is that the default algorithm sees only one cell at a time, and so will apply the optimal algorithm only on those cells touching the curved manifolds. In contrast, using the transfinite mapping on entire [1.x.2420] of cells (originating from one coarser cell) allows to use the transfinite interpolation method in a way that propagates information from the boundary to cells far away. 




[1.x.2421] 

A mesh with a transfinite manifold description is typically set up in two steps. The first step is to create a coarse mesh (or read it in from a file) and to attach a curved manifold to some of the mesh entities. For the above example of the disk, we attach a polar manifold to the faces along the outer circle (this is done automatically by  [2.x.6270]  Before we start refining the mesh, we then assign a TransfiniteInterpolationManifold to all interior cells and edges of the mesh, which of course needs to be based on some manifold id that we have assigned to those entities (everything except the circle on the boundary). It does not matter whether we also assign a TransfiniteInterpolationManifold to the inner square of the disk or not because the transfinite interpolation on a coarse cell with straight edges (or flat faces in 3d) simply yields subdivided children with straight edges (flat faces). 

Later, when the mesh is refined or when a higher-order mapping is set up based on this mesh, the cells will query the underlying manifold object for new points. This process takes a set of surrounding points, for example the four vertices of a two-dimensional cell, and a set of weights to each of these points, for definition a new point. For the mid point of a cell, each of the four vertices would get weight 0.25. For the transfinite interpolation manifold, the process of building weighted sums requires some serious work. By construction, we want to combine the points in terms of the reference coordinates  [2.x.6271]  and  [2.x.6272]  (or  [2.x.6273]  in 3D) of the surrounding points. However, the interface of the manifold classes in deal.II does not get the reference coordinates of the surrounding points (as they are not stored globally) but rather the physical coordinates only. Thus, the first step the transfinite interpolation manifold has to do is to invert the mapping and find the reference coordinates within one of the coarse cells of the transfinite interpolation (e.g. one of the four shaded coarse-grid cells of the disk mesh above). This inversion is done by a Newton iteration (or rather, finite-difference based Newton scheme combined with Broyden's method) and queries the transfinite interpolation according to the formula above several times. Each of these queries in turn might call an expensive manifold, e.g. a spherical description of a ball, and be expensive on its own. Since the Manifold interface class of deal.II only provides a set of points, the transfinite interpolation initially does not even know to which coarse grid cell the set of surrounding points belong to and needs to search among several cells based on some heuristics. In terms of [1.x.2422], one could describe the implementation of the transfinite interpolation as an [1.x.2423]-based implementation: Each cell of the initial coarse grid of the triangulation represents a chart with its own reference space, and the surrounding manifolds provide a way to transform from the chart space (i.e., the reference cell) to the physical space. The collection of the charts of the coarse grid cells is an atlas, and as usual, the first thing one does when looking up something in an atlas is to find the right chart. 

Once the reference coordinates of the surrounding points have been found, a new point in the reference coordinate system is computed by a simple weighted sum. Finally, the reference point is inserted into the formula for the transfinite interpolation, which gives the desired new point. 

In a number of cases, the curved manifold is not only used during mesh refinement, but also to ensure a curved representation of boundaries within the cells of the computational domain. This is a necessity to guarantee high-order convergence for high-order polynomials on complex geometries anyway, but sometimes an accurate geometry is also desired with linear shape functions. This is often done by polynomial descriptions of the cells and called the isoparametric concept if the polynomial degree to represent the curved mesh elements is the same as the degree of the polynomials for the numerical solution. If the degree of the geometry is higher or lower than the solution, one calls that a super- or sub-parametric geometry representation, respectively. In deal.II, the standard class for polynomial representation is MappingQGeneric. If, for example, this class is used with polynomial degree  [2.x.6274]  in 3D, a total of 125 (i.e.,  [2.x.6275] ) points are needed for the interpolation. Among these points, 8 are the cell's vertices and already available from the mesh, but the other 117 need to be provided by the manifold. In case the transfinite interpolation manifold is used, we can imagine that going through the pull-back into reference coordinates of some yet to be determined coarse cell, followed by subsequent push-forward on each of the 117 points, is a lot of work and can be very time consuming. 

What makes things worse is that the structure of many programs is such that the mapping is queried several times independently for the same cell. Its primary use is in the assembly of the linear system, i.e., the computation of the system matrix and the right hand side, via the `mapping` argument of the FEValues object. However, also the interpolation of boundary values, the computation of numerical errors, writing the output, and evaluation of error estimators must involve the same mapping to ensure a consistent interpretation of the solution vectors. Thus, even a linear stationary problem that is solved once will evaluate the points of the mapping several times. For the cubic case in 3D mentioned above, this means computing 117 points per cell by an expensive algorithm many times. The situation is more pressing for nonlinear or time-dependent problems where those operations are done over and over again. 

As the manifold description via a transfinite interpolation can easily be hundreds of times more expensive than a similar query on a flat manifold, it makes sense to compute the additional points only once and use them in all subsequent calls. The deal.II library provides the class MappingQCache for exactly this purpose. The cache is typically not overly big compared to the memory consumed by a system matrix, as will become clear when looking at the results of this tutorial program. The usage of MappingQCache is simple: Once the mesh has been set up (or changed during refinement), we call  [2.x.6276]  with the desired triangulation as well as a desired mapping as arguments. The initialization then goes through all cells of the mesh and queries the given mapping for its additional points. Those get stored for an identifier of the cell so that they can later be returned whenever the mapping computes some quantities related to the cell (like the Jacobians of the map between the reference and physical coordinates). 

As a final note, we mention that the TransfiniteInterpolationManifold also makes the refinement of the mesh more expensive. In this case, the MappingQCache does not help because it would compute points that can subsequently not be re-used; there currently does not exist a more efficient mechanism in deal.II. However, the mesh refinement contains many other expensive steps as well, so it is not as big as an issue compared to the rest of the computation. It also only happens at most once per time step or nonlinear iteration. 

[1.x.2424] 

In this tutorial program, the usage of TransfiniteInterpolationManifold is exemplified in combination with MappingQCache. The test case is relatively simple and takes up the solution stages involved in many typical programs, e.g., the step-6 tutorial program. As a geometry, we select one prototype use of TransfiniteInterpolationManifold, namely a setup involving a spherical ball that is in turn surrounded by a cube. Such a setup would be used, for example, for a spherical inclusion embedded in a background medium, and if that inclusion has different material properties that require that the interface between the two materials needs to be tracked by element interfaces. A visualization of the grid is given here: 

 [2.x.6277]  

For this case, we want to attach a spherical description to the surface inside the domain and use the transfinite interpolation to smoothly switch to the straight lines of the outer cube and the cube at the center of the ball. 

Within the program, we will follow a typical flow in finite element programs, starting from the setup of DoFHandler and sparsity patterns, the assembly of a linear system for solving the Poisson equation with a jumping coefficient, its solution with a simple iterative method, computation of some numerical error with  [2.x.6278]  as well as an error estimator. We record timings for each section and run the code twice. In the first run, we hand a MappingQGeneric object to each stage of the program separately, where points get re-computed over and over again. In the second run, we use MappingQCache instead. 


examples/step-65/doc/results.dox 



[1.x.2425] 

[1.x.2426] 

If we run the three-dimensional version of this program with polynomials of degree three, we get the following program output: 

[1.x.2427] 



Before discussing the timings, we look at the memory consumption for the MappingQCache object: Our program prints that it utilizes 23 MB of memory. If we relate this number to the memory consumption of a single (solution or right hand side) vector, which is 1.5 MB (namely, 181,609 elements times 8 bytes per entry in double precision), or to the memory consumed by the system matrix and the sparsity pattern (which is 274 MB), we realize that it is not an overly heavy data structure, given its benefits. 

With respect to the timers, we see a clear improvement in the overall run time of the program by a factor of 2.7. If we disregard the iterative solver, which is the same in both cases (and not optimal, given the simple preconditioner we use, and the fact that sparse matrix-vector products waste operations for cubic polynomials), the advantage is a factor of almost 5. This is pretty impressive for a linear stationary problem, and cost savings would indeed be much more prominent for time-dependent and nonlinear problems where assembly is called several times. If we look into the individual components, we get a clearer picture of what is going on and why the cache is so efficient: In the MappingQGeneric case, essentially every operation that involves a mapping take at least 5 seconds to run. The norm computation runs two  [2.x.6279]  functions, which each take almost 5 seconds. (The computation of constraints is cheaper because it only evaluates the mapping in cells at the boundary for the interpolation of boundary conditions.) If we compare these 5 seconds to the time it takes to fill the MappingQCache, which is 5.2 seconds (for all cells, not just the active ones), it becomes obvious that the computation of the mapping support points dominates over everything else in the MappingQGeneric case. Perhaps the most striking result is the time for the error estimator, labeled "Compute error estimator", where the MappingQGeneric implementation takes 17.3 seconds and the MappingQCache variant less than 0.5 seconds. The reason why the former is so expensive (three times more expensive than the assembly, for instance) is that the error estimation involves evaluation of quantities over faces, where each face in the mesh requests additional points of the mapping that in turn go through the very expensive TransfiniteInterpolationManifold class. As there are six faces per cell, this happens much more often than in assembly. Again, MappingQCache nicely eliminates the repeated evaluation, aggregating all the expensive steps involving the manifold in a single initialization call that gets repeatedly used. 


examples/step-66/doc/intro.dox 

 [2.x.6280]  

[1.x.2428] 


[1.x.2429] 

[1.x.2430] 

The aim of this tutorial program is to demonstrate how to solve a nonlinear problem using Newton's method within the matrix-free framework. This tutorial combines several techniques already introduced in step-15, step-16, step-37, step-48 and others. 




[1.x.2431] On the unit circle  [2.x.6281]  we consider the following nonlinear elliptic boundary value problem subject to a homogeneous Dirichlet boundary condition: Find a function  [2.x.6282]  such that it holds: 

[1.x.2432] 

This problem is also called the [1.x.2433] and is a typical example for problems from combustion theory, see for example  [2.x.6283] . 




[1.x.2434] As usual, we first derive the weak formulation for this problem by multiplying with a smooth test function  [2.x.6284]  respecting the boundary condition and integrating over the domain  [2.x.6285] . Integration by parts and putting the term from the right hand side to the left yields the weak formulation: Find a function  [2.x.6286]  such that for all test functions  [2.x.6287]  it holds: 

[1.x.2435] 



Choosing the Lagrangian finite element space  [2.x.6288] , which directly incorporates the homogeneous Dirichlet boundary condition, we can define a basis  [2.x.6289]  and thus it suffices to test only with those basis functions. So the discrete problem reads as follows: Find  [2.x.6290]  such that for all  [2.x.6291]  it holds: 

[1.x.2436] 

As each finite element function is a linear combination of the basis functions  [2.x.6292] , we can identify the finite element solution by a vector from  [2.x.6293]  consisting of the unknown values in each degree of freedom (DOF). Thus, we define the nonlinear function  [2.x.6294]  representing the discrete nonlinear problem. 

To solve this nonlinear problem we use Newton's method. So given an initial guess  [2.x.6295] , which already fulfills the Dirichlet boundary condition, we determine a sequence of Newton steps  [2.x.6296]  by successively applying the following scheme: 

[1.x.2437] 

So in each Newton step we have to solve a linear problem  [2.x.6297] , where the system matrix  [2.x.6298]  is represented by the Jacobian  [2.x.6299]  and the right hand side  [2.x.6300]  by the negative residual  [2.x.6301] . The solution vector  [2.x.6302]  is in that case the Newton update of the  [2.x.6303] -th Newton step. Note, that we assume an initial guess  [2.x.6304] , which already fulfills the Dirichlet boundary conditions of the problem formulation (in fact this could also be an inhomogeneous Dirichlet boundary condition) and thus the Newton updates  [2.x.6305]  satisfy a homogeneous Dirichlet condition. 

Until now we only tested with the basis functions, however, we can also represent any function of  [2.x.6306]  as linear combination of basis functions. More mathematically this means, that every element of  [2.x.6307]  can be identified with a vector  [2.x.6308]  via the representation formula:  [2.x.6309] . So using this we can give an expression for the discrete Jacobian and the residual: 

[1.x.2438] 

Compared to step-15 we could also have formed the Frech{\'e}t derivative of the nonlinear function corresponding to the strong formulation of the problem and discretized it afterwards. However, in the end we would get the same set of discrete equations. 




[1.x.2439] Note, how the system matrix, actually the Jacobian, depends on the previous Newton step  [2.x.6310] . Hence we need to tell the function that computes the system matrix about the solution at the last Newton step. In an implementation with a classical  [2.x.6311]  function we would gather this information from the last Newton step during assembly by the use of the member functions  [2.x.6312]  and  [2.x.6313]  The  [2.x.6314]  function would then looks like: 

[1.x.2440] 



Since we want to solve this problem without storing a matrix, we need to tell the matrix-free operator this information before we use it. Therefore in the derived class  [2.x.6315]  we will implement a function called  [2.x.6316] , which will process the information of the last Newton step prior to the usage of the matrix-vector implementation. Furthermore we want to use a geometric multigrid (GMG) preconditioner for the linear solver, so in order to apply the multilevel operators we need to pass the last Newton step also to these operators. This is kind of a tricky task, since the vector containing the last Newton step has to be interpolated to all levels of the triangulation. In the code this task will be done by the function  [2.x.6317]  Note, a fundamental difference to the previous cases, where we set up and used a geometric multigrid preconditioner, is the fact, that we can reuse the MGTransferMatrixFree object for the computation of all Newton steps. So we can save some work here by defining a class variable and using an already set up MGTransferMatrixFree object  [2.x.6318]  that was initialized in the  [2.x.6319]  function. 

[1.x.2441] 



The function evaluating the nonlinearity works basically in the same way as the function  [2.x.6320]  from step-37 evaluating a coefficient function. The idea is to use an FEEvaluation object to evaluate the Newton step and store the expression in a table for all cells and all quadrature points: 

[1.x.2442] 






[1.x.2443] As said in step-37 the matrix-free method gets more efficient if we choose a higher order finite element space. Since we want to solve the problem on the  [2.x.6321] -dimensional unit ball, it would be good to have an appropriate boundary approximation to overcome convergence issues. For this reason we use an isoparametric approach with the MappingQGeneric class to recover the smooth boundary as well as the mapping for inner cells. In addition, to get a good triangulation in total we make use of the TransfiniteInterpolationManifold. 


examples/step-66/doc/results.dox 



[1.x.2444] 

The aim of this tutorial step was to demonstrate the solution of a nonlinear PDE with the matrix-free framework. 




[1.x.2445] Running the program on two processes in release mode via 

[1.x.2446] 

gives the following output on the console 

[1.x.2447] 



We show the solution for the two- and three-dimensional problem in the following figure. 

<div class="twocolumn" style="width: 80%; text-align: center;">   <div>     <img src = "https://www.dealii.org/images/steps/developer/step-66.solution-2d.png"      alt     = "Solution of the two-dimensional Gelfand problem."      width   = "100%">   </div>   <div>     <img src = "https://www.dealii.org/images/steps/developer/step-66.solution-3d.png"      alt     = "Solution of the three-dimensional Gelfand problem."      width   = "100%">   </div> </div> 




[1.x.2448] In the program output above we find some interesting information about the Newton iterations. The terminal output in each refinement cycle presents detailed diagnostics of the Newton method, which show first of all the number of Newton steps and for each step the norm of the residual  [2.x.6322] , the norm of the Newton update  [2.x.6323] , and the number of CG iterations  [2.x.6324] . 

We observe that for all cases the Newton method converges in approximately three to four steps, which shows the quadratic convergence of the Newton method with a full step length  [2.x.6325] . However, be aware that for a badly chosen initial guess  [2.x.6326] , the Newton method will also diverge quadratically. Usually if you do not have an appropriate initial guess, you try a few damped Newton steps with a reduced step length  [2.x.6327]  until the Newton step is again in the quadratic convergence domain. This damping and relaxation of the Newton step length truly requires a more sophisticated implementation of the Newton method, which we designate to you as a possible extension of the tutorial. 

Furthermore, we see that the number of CG iterations is approximately constant with successive mesh refinements and an increasing number of DoFs. This is of course due to the geometric multigrid preconditioner and similar to the observations made in other tutorials that use this method, e.g., step-16 and step-37. Just to give an example, in the three-dimensional case after five refinements, we have approximately 14.7 million distributed DoFs with fourth-order Lagrangian finite elements, but the number of CG iterations is still less than ten. 

In addition, there is one more very useful optimization that we applied and that should be mentioned here. In the  [2.x.6328]  function we explicitly reset the vector holding the Newton update before passing it as the output vector to the solver. In that case we use a starting value of zero for the CG method, which is more suitable than the previous Newton update, the actual content of the  [2.x.6329]  before resetting, and thus reduces the number of CG iterations by a few steps. 




[1.x.2449] A couple of possible extensions are available concerning minor updates fo the present code as well as a deeper numerical investigation of the Gelfand problem. 

[1.x.2450] Beside a step size controlled version of the Newton iteration as mentioned already in step-15, one could also implement a more flexible stopping criterion for the Newton iteration. For example one could replace the fixed tolerances for the residual  [2.x.6330]  and implement a mixed error control with a given absolute and relative tolerance, such that the Newton iteration exists with success as, e.g., 

[1.x.2451] 

For more advanced applications with many nonlinear systems to solve, for example at each time step for a time-dependent problem, it turns out that it is not necessary to set up and assemble the Jacobian anew at every single Newton step or even for each time step. Instead, the existing Jacobian from a previous step can be used for the Newton iteration. The Jacobian is then only rebuilt if, for example, the Newton iteration converges too slowly. Such an idea yields a [1.x.2452]. Admittedly, when using the matrix-free framework, the assembly of the Jacobian is omitted anyway, but with in this way one can try to optimize the reassembly of the geometric multigrid preconditioner. Remember that each time the solution from the old Newton step must be distributed to all levels and the mutligrid preconditioner must be reinitialized. 

[1.x.2453] In the results section of step-37 and others, the parallel scalability of the matrix-free framework on a large number of processors has already been demonstrated very impressively. In the nonlinear case we consider here, we note that one of the bottlenecks could become the transfer and evaluation of the matrix-free Jacobi operator and its multistage operators in the previous Newton step, since we need to transfer the old solution at all stages in each step. A first parallel scalability analysis in  [2.x.6331]  shows quite good strong scalability when the problem size is large enough. However, a more detailed analysis needs to be performed for reliable results. Moreover, the problem has been solved only with MPI so far, without using the possibilities of shared memory parallelization with threads. Therefore, for this example, you could try hybrid parallelization with MPI and threads, such as described in step-48. 

[1.x.2454] Analogously to step-50 and the mentioned possible extension of step-75, you can convince yourself which method is faster. 

[1.x.2455] One can consider the corresponding eigenvalue problem, which is called Bratu problem. For example, if we define a fixed eigenvalue  [2.x.6332] , we can compute the corresponding discrete eigenfunction. You will notice that the number of Newton steps will increase with increasing  [2.x.6333] . To reduce the number of Newton steps you can use the following trick: start from a certain  [2.x.6334] , compute the eigenfunction, increase  [2.x.6335] , and then use the previous solution as an initial guess for the Newton iteration. In the end you can plot the  [2.x.6336] -norm over the eigenvalue  [2.x.6337] . What do you observe for further increasing  [2.x.6338] ? 


examples/step-67/doc/intro.dox 



 [2.x.6339]  

[1.x.2456] 

[1.x.2457] 

[1.x.2458] 

This tutorial program solves the Euler equations of fluid dynamics using an explicit time integrator with the matrix-free framework applied to a high-order discontinuous Galerkin discretization in space. For details about the Euler system and an alternative implicit approach, we also refer to the step-33 tutorial program. You might also want to look at step-69 for an alternative approach to solving these equations. 




[1.x.2459] 

The Euler equations are a conservation law, describing the motion of a compressible inviscid gas, 

[1.x.2460] 

where the  [2.x.6340]  components of the solution vector are  [2.x.6341] . Here,  [2.x.6342]  denotes the fluid density,  [2.x.6343]  the fluid velocity, and  [2.x.6344]  the energy density of the gas. The velocity is not directly solved for, but rather the variable  [2.x.6345] , the linear momentum (since this is the conserved quantity). 

The Euler flux function, a  [2.x.6346]  matrix, is defined as 

[1.x.2461] 

with  [2.x.6347]  the  [2.x.6348]  identity matrix and  [2.x.6349]  the outer product; its components denote the mass, momentum, and energy fluxes, respectively. The right hand side forcing is given by 

[1.x.2462] 

where the vector  [2.x.6350]  denotes the direction and magnitude of gravity. It could, however, also denote any other external force per unit mass that is acting on the fluid. (Think, for example, of the electrostatic forces exerted by an external electric field on charged particles.) 

The three blocks of equations, the second involving  [2.x.6351]  components, describe the conservation of mass, momentum, and energy. The pressure is not a solution variable but needs to be expressed through a "closure relationship" by the other variables; we here choose the relationship appropriate for a gas with molecules composed of two atoms, which at moderate temperatures is given by  [2.x.6352]  with the constant  [2.x.6353] . 




[1.x.2463] 

For spatial discretization, we use a high-order discontinuous Galerkin (DG) discretization, using a solution expansion of the form 

[1.x.2464] 

Here,  [2.x.6354]  denotes the  [2.x.6355] th basis function, written in vector form with separate shape functions for the different components and letting  [2.x.6356]  go through the density, momentum, and energy variables, respectively. In this form, the space dependence is contained in the shape functions and the time dependence in the unknown coefficients  [2.x.6357] . As opposed to the continuous finite element method where some shape functions span across element boundaries, the shape functions are local to a single element in DG methods, with a discontinuity from one element to the next. The connection of the solution from one cell to its neighbors is instead imposed by the numerical fluxes specified below. This allows for some additional flexibility, for example to introduce directionality in the numerical method by, e.g., upwinding. 

DG methods are popular methods for solving problems of transport character because they combine low dispersion errors with controllable dissipation on barely resolved scales. This makes them particularly attractive for simulation in the field of fluid dynamics where a wide range of active scales needs to be represented and inadequately resolved features are prone to disturb the important well-resolved features. Furthermore, high-order DG methods are well-suited for modern hardware with the right implementation. At the same time, DG methods are no silver bullet. In particular when the solution develops discontinuities (shocks), as is typical for the Euler equations in some flow regimes, high-order DG methods tend to oscillatory solutions, like all high-order methods when not using flux- or slope-limiters. This is a consequence of [1.x.2465] that states that any total variation limited (TVD) scheme that is linear (like a basic DG discretization) can at most be first-order accurate. Put differently, since DG methods aim for higher order accuracy, they cannot be TVD on solutions that develop shocks. Even though some communities claim that the numerical flux in DG methods can control dissipation, this is of limited value unless [1.x.2466] shocks in a problem align with cell boundaries. Any shock that passes through the interior of cells will again produce oscillatory components due to the high-order polynomials. In the finite element and DG communities, there exist a number of different approaches to deal with shocks, for example the introduction of artificial diffusion on troubled cells (using a troubled-cell indicator based e.g. on a modal decomposition of the solution), a switch to dissipative low-order finite volume methods on a subgrid, or the addition of some limiting procedures. Given the ample possibilities in this context, combined with the considerable implementation effort, we here refrain from the regime of the Euler equations with pronounced shocks, and rather concentrate on the regime of subsonic flows with wave-like phenomena. For a method that works well with shocks (but is more expensive per unknown), we refer to the step-69 tutorial program. 

For the derivation of the DG formulation, we multiply the Euler equations with test functions  [2.x.6358]  and integrate over an individual cell  [2.x.6359] , which gives 

[1.x.2467] 



We then integrate the second term by parts, moving the divergence from the solution slot to the test function slot, and producing an integral over the element boundary: 

[1.x.2468] 

In the surface integral, we have replaced the term  [2.x.6360]  by the term  [2.x.6361] , the numerical flux. The role of the numerical flux is to connect the solution on neighboring elements and weakly impose continuity of the solution. This ensures that the global coupling of the PDE is reflected in the discretization, despite independent basis functions on the cells. The connectivity to the neighbor is included by defining the numerical flux as a function  [2.x.6362]  of the solution from both sides of an interior face,  [2.x.6363]  and  [2.x.6364] . A basic property we require is that the numerical flux needs to be [1.x.2469]. That is, we want all information (i.e., mass, momentum, and energy) that leaves a cell over a face to enter the neighboring cell in its entirety and vice versa. This can be expressed as  [2.x.6365] , meaning that the numerical flux evaluates to the same result from either side. Combined with the fact that the numerical flux is multiplied by the unit outer normal vector on the face under consideration, which points in opposite direction from the two sides, we see that the conservation is fulfilled. An alternative point of view of the numerical flux is as a single-valued intermediate state that links the solution weakly from both sides. 

There is a large number of numerical flux functions available, also called Riemann solvers. For the Euler equations, there exist so-called exact Riemann solvers -- meaning that the states from both sides are combined in a way that is consistent with the Euler equations along a discontinuity -- and approximate Riemann solvers, which violate some physical properties and rely on other mechanisms to render the scheme accurate overall. Approximate Riemann solvers have the advantage of beging cheaper to compute. Most flux functions have their origin in the finite volume community, which are similar to DG methods with polynomial degree 0 within the cells (called volumes). As the volume integral of the Euler operator  [2.x.6366]  would disappear for constant solution and test functions, the numerical flux must fully represent the physical operator, explaining why there has been a large body of research in that community. For DG methods, consistency is guaranteed by higher order polynomials within the cells, making the numerical flux less of an issue and usually affecting only the convergence rate, e.g., whether the solution converges as  [2.x.6367] ,  [2.x.6368]  or  [2.x.6369]  in the  [2.x.6370]  norm for polynomials of degree  [2.x.6371] . The numerical flux can thus be seen as a mechanism to select more advantageous dissipation/dispersion properties or regarding the extremal eigenvalue of the discretized and linearized operator, which affect the maximal admissible time step size in explicit time integrators. 

In this tutorial program, we implement two variants of fluxes that can be controlled via a switch in the program (of course, it would be easy to make them a run time parameter controlled via an input file). The first flux is the local Lax--Friedrichs flux 

[1.x.2470] 



In the original definition of the Lax--Friedrichs flux, a factor  [2.x.6372]  is used (corresponding to the maximal speed at which information is moving on the two sides of the interface), stating that the difference between the two states,  [2.x.6373]  is penalized by the largest eigenvalue in the Euler flux, which is  [2.x.6374] , where  [2.x.6375]  is the speed of sound. In the implementation below, we modify the penalty term somewhat, given that the penalty is of approximate nature anyway. We use 

[1.x.2471] 

The additional factor  [2.x.6376]  reduces the penalty strength (which results in a reduced negative real part of the eigenvalues, and thus increases the admissible time step size). Using the squares within the sums allows us to reduce the number of expensive square root operations, which is 4 for the original Lax--Friedrichs definition, to a single one. This simplification leads to at most a factor of 2 in the reduction of the parameter  [2.x.6377] , since  [2.x.6378] , with the last inequality following from Young's inequality. 

The second numerical flux is one proposed by Harten, Lax and van Leer, called the HLL flux. It takes the different directions of propagation of the Euler equations into account, depending on the speed of sound. It utilizes some intermediate states  [2.x.6379]  and  [2.x.6380]  to define the two branches  [2.x.6381]  and  [2.x.6382] . From these branches, one then defines the flux 

[1.x.2472] 

Regarding the definition of the intermediate state  [2.x.6383]  and  [2.x.6384] , several variants have been proposed. The variant originally proposed uses a density-averaged definition of the velocity,  [2.x.6385] . Since we consider the Euler equations without shocks, we simply use arithmetic means,  [2.x.6386]  and  [2.x.6387] , with  [2.x.6388] , in this tutorial program, and leave other variants to a possible extension. We also note that the HLL flux has been extended in the literature to the so-called HLLC flux, where C stands for the ability to represent contact discontinuities. 

At the boundaries with no neighboring state  [2.x.6389]  available, it is common practice to deduce suitable exterior values from the boundary conditions (see the general literature on DG methods for details). In this tutorial program, we consider three types of boundary conditions, namely [1.x.2473] where all components are prescribed, 

[1.x.2474] 

[1.x.2475], where we do not prescribe exterior solutions as the flow field is leaving the domain and use the interior values instead; we still need to prescribe the energy as there is one incoming characteristic left in the Euler flux, 

[1.x.2476] 

and [1.x.2477] which describe a no-penetration configuration: 

[1.x.2478] 



The polynomial expansion of the solution is finally inserted to the weak form and test functions are replaced by the basis functions. This gives a discrete in space, continuous in time nonlinear system with a finite number of unknown coefficient values  [2.x.6390] ,  [2.x.6391] . Regarding the choice of the polynomial degree in the DG method, there is no consensus in literature as of 2019 as to what polynomial degrees are most efficient and the decision is problem-dependent. Higher order polynomials ensure better convergence rates and are thus superior for moderate to high accuracy requirements for [1.x.2479] solutions. At the same time, the volume-to-surface ratio of where degrees of freedom are located, increases with higher degrees, and this makes the effect of the numerical flux weaker, typically reducing dissipation. However, in most of the cases the solution is not smooth, at least not compared to the resolution that can be afforded. This is true for example in incompressible fluid dynamics, compressible fluid dynamics, and the related topic of wave propagation. In this pre-asymptotic regime, the error is approximately proportional to the numerical resolution, and other factors such as dispersion errors or the dissipative behavior become more important. Very high order methods are often ruled out because they come with more restrictive CFL conditions measured against the number of unknowns, and they are also not as flexible when it comes to representing complex geometries. Therefore, polynomial degrees between two and six are most popular in practice, see e.g. the efficiency evaluation in  [2.x.6392]  and references cited therein. 

[1.x.2480] 

To discretize in time, we slightly rearrange the weak form and sum over all cells: 

[1.x.2481] 

where  [2.x.6393]  runs through all basis functions with from 1 to  [2.x.6394] . 

We now denote by  [2.x.6395]  the mass matrix with entries  [2.x.6396] , and by 

[1.x.2482] 

the operator evaluating the right-hand side of the Euler operator, given a function  [2.x.6397]  associated with a global vector of unknowns and the finite element in use. This function  [2.x.6398]  is explicitly time-dependent as the numerical flux evaluated at the boundary will involve time-dependent data  [2.x.6399] ,  [2.x.6400] , and  [2.x.6401]  on some parts of the boundary, depending on the assignment of boundary conditions. With this notation, we can write the discrete in space, continuous in time system compactly as 

[1.x.2483] 

where we have taken the liberty to also denote the global solution vector by  [2.x.6402]  (in addition to the the corresponding finite element function). Equivalently, the system above has the form 

[1.x.2484] 



For hyperbolic systems discretized by high-order discontinuous Galerkin methods, explicit time integration of this system is very popular. This is due to the fact that the mass matrix  [2.x.6403]  is block-diagonal (with each block corresponding to only variables of the same kind defined on the same cell) and thus easily inverted. In each time step -- or stage of a Runge--Kutta scheme -- one only needs to evaluate the differential operator once using the given data and subsequently apply the inverse of the mass matrix. For implicit time stepping, on the other hand, one would first have to linearize the equations and then iteratively solve the linear system, which involves several residual evaluations and at least a dozen applications of the linearized operator, as has been demonstrated in the step-33 tutorial program. 

Of course, the simplicity of explicit time stepping comes with a price, namely conditional stability due to the so-called Courant--Friedrichs--Lewy (CFL) condition. It states that the time step cannot be larger than the fastest propagation of information by the discretized differential operator. In more modern terms, the speed of propagation corresponds to the largest eigenvalue in the discretized operator, and in turn depends on the mesh size, the polynomial degree  [2.x.6404]  and the physics of the Euler operator, i.e., the eigenvalues of the linearization of  [2.x.6405]  with respect to  [2.x.6406] . In this program, we set the time step as follows: 

[1.x.2485] 



with the maximum taken over all quadrature points and all cells. The dimensionless number  [2.x.6407]  denotes the Courant number and can be chosen up to a maximally stable number  [2.x.6408] , whose value depends on the selected time stepping method and its stability properties. The power  [2.x.6409]  used for the polynomial scaling is heuristic and represents the closest fit for polynomial degrees between 1 and 8, see e.g.  [2.x.6410] . In the limit of higher degrees,  [2.x.6411] , a scaling of  [2.x.6412]  is more accurate, related to the inverse estimates typically used for interior penalty methods. Regarding the [1.x.2486] mesh sizes  [2.x.6413]  and  [2.x.6414]  used in the formula, we note that the convective transport is directional. Thus an appropriate scaling is to use the element length in the direction of the velocity  [2.x.6415] . The code below derives this scaling from the inverse of the Jacobian from the reference to real cell, i.e., we approximate  [2.x.6416] . The acoustic waves, instead, are isotropic in character, which is why we use the smallest feature size, represented by the smallest singular value of  [2.x.6417] , for the acoustic scaling  [2.x.6418] . Finally, we need to add the convective and acoustic limits, as the Euler equations can transport information with speed  [2.x.6419] . 

In this tutorial program, we use a specific variant of [1.x.2487], which in general use the following update procedure from the state  [2.x.6420]  at time  [2.x.6421]  to the new time  [2.x.6422]  with  [2.x.6423] : 

[1.x.2488] 

The vectors  [2.x.6424] ,  [2.x.6425] , in an  [2.x.6426] -stage scheme are evaluations of the operator at some intermediate state and used to define the end-of-step value  [2.x.6427]  via some linear combination. The scalar coefficients in this scheme,  [2.x.6428] ,  [2.x.6429] , and  [2.x.6430] , are defined such that certain conditions are satisfied for higher order schemes, the most basic one being  [2.x.6431] . The parameters are typically collected in the form of a so-called [1.x.2489] that collects all of the coefficients that define the scheme. For a five-stage scheme, it would look like this: 

[1.x.2490] 



In this tutorial program, we use a subset of explicit Runge--Kutta methods, so-called low-storage Runge--Kutta methods (LSRK), which assume additional structure in the coefficients. In the variant used by reference  [2.x.6432] , the assumption is to use Butcher tableaus of the form 

[1.x.2491] 

With such a definition, the update to  [2.x.6433]  shares the storage with the information for the intermediate values  [2.x.6434] . Starting with  [2.x.6435]  and  [2.x.6436] , the update in each of the  [2.x.6437]  stages simplifies to 

[1.x.2492] 

Besides the vector  [2.x.6438]  that is successively updated, this scheme only needs two auxiliary vectors, namely the vector  [2.x.6439]  to hold the evaluation of the differential operator, and the vector  [2.x.6440]  that holds the right-hand side for the differential operator application. In subsequent stages  [2.x.6441] , the values  [2.x.6442]  and  [2.x.6443]  can use the same storage. 

The main advantages of low-storage variants are the reduced memory consumption on the one hand (if a very large number of unknowns must be fit in memory, holding all  [2.x.6444]  to compute subsequent updates can be a limit already for  [2.x.6445]  in between five and eight -- recall that we are using an explicit scheme, so we do not need to store any matrices that are typically much larger than a few vectors), and the reduced memory access on the other. In this program, we are particularly interested in the latter aspect. Since cost of operator evaluation is only a small multiple of the cost of simply streaming the input and output vector from memory with the optimized matrix-free methods of deal.II, we must consider the cost of vector updates, and low-storage variants can deliver up to twice the throughput of conventional explicit Runge--Kutta methods for this reason, see e.g. the analysis in  [2.x.6446] . 

Besides three variants for third, fourth and fifth order accuracy from the reference  [2.x.6447] , we also use a fourth-order accurate variant with seven stages that was optimized for acoustics setups from  [2.x.6448] . Acoustic problems are one of the interesting aspects of the subsonic regime of the Euler equations where compressibility leads to the transmission of sound waves; often, one uses further simplifications of the linearized Euler equations around a background state or the acoustic wave equation around a fixed frame. 




[1.x.2493] 

The major ingredients used in this program are the fast matrix-free techniques we use to evaluate the operator  [2.x.6449]  and the inverse mass matrix  [2.x.6450] . Actually, the term [1.x.2494] is a slight misnomer, because we are working with a nonlinear operator and do not linearize the operator that in turn could be represented by a matrix. However, fast evaluation of integrals has become popular as a replacement of sparse matrix-vector products, as shown in step-37 and step-59, and we have coined this infrastructure [1.x.2495] in deal.II for this reason. Furthermore, the inverse mass matrix is indeed applied in a matrix-free way, as detailed below. 

The matrix-free infrastructure allows us to quickly evaluate the integrals in the weak forms. The ingredients are the fast interpolation from solution coefficients into values and derivatives at quadrature points, point-wise operations at quadrature points (where we implement the differential operator as derived above), as well as multiplication by all test functions and summation over quadrature points. The first and third component make use of sum factorization and have been extensively discussed in the step-37 tutorial program for the cell integrals and step-59 for the face integrals. The only difference is that we now deal with a system of  [2.x.6451]  components, rather than the scalar systems in previous tutorial programs. In the code, all that changes is a template argument of the FEEvaluation and FEFaceEvaluation classes, the one to set the number of components. The access to the vector is the same as before, all handled transparently by the evaluator. We also note that the variant with a single evaluator chosen in the code below is not the only choice -- we could also have used separate evalators for the separate components  [2.x.6452] ,  [2.x.6453] , and  [2.x.6454] ; given that we treat all components similarly (also reflected in the way we state the equation as a vector system), this would be more complicated here. As before, the FEEvaluation class provides explicit vectorization by combining the operations on several cells (and faces), involving data types called VectorizedArray. Since the arithmetic operations are overloaded for this type, we do not have to bother with it all that much, except for the evaluation of functions through the Function interface, where we need to provide particular [1.x.2496] evaluations for several quadrature point locations at once. 

A more substantial change in this program is the operation at quadrature points: Here, the multi-component evaluators provide us with return types not discussed before. Whereas  [2.x.6455]  would return a scalar (more precisely, a VectorizedArray type due to vectorization across cells) for the Laplacian of step-37, it now returns a type that is `Tensor<1,dim+2,VectorizedArray<Number>>`. Likewise, the gradient type is now `Tensor<1,dim+2,Tensor<1,dim,VectorizedArray<Number>>>`, where the outer tensor collects the `dim+2` components of the Euler system, and the inner tensor the partial derivatives in the various directions. For example, the flux  [2.x.6456]  of the Euler system is of this type. In order to reduce the amount of code we have to write for spelling out these types, we use the C++ `auto` keyword where possible. 

From an implementation point of view, the nonlinearity is not a big difficulty: It is introduced naturally as we express the terms of the Euler weak form, for example in the form of the momentum term  [2.x.6457] . To obtain this expression, we first deduce the velocity  [2.x.6458]  from the momentum variable  [2.x.6459] . Given that  [2.x.6460]  is represented as a  [2.x.6461] -degree polynomial, as is  [2.x.6462] , the velocity  [2.x.6463]  is a rational expression in terms of the reference coordinates  [2.x.6464] . As we perform the multiplication  [2.x.6465] , we obtain an expression that is the ratio of two polynomials, with polynomial degree  [2.x.6466]  in the numerator and polynomial degree  [2.x.6467]  in the denominator. Combined with the gradient of the test function, the integrand is of degree  [2.x.6468]  in the numerator and  [2.x.6469]  in the denominator already for affine cells, i.e., for parallelograms/ parallelepipeds. For curved cells, additional polynomial and rational expressions appear when multiplying the integrand by the determinant of the Jacobian of the mapping. At this point, one usually needs to give up on insisting on exact integration, and take whatever accuracy the Gaussian (more precisely, Gauss--Legrende) quadrature provides. The situation is then similar to the one for the Laplace equation, where the integrand contains rational expressions on non-affince cells and is also only integrated approximately. As these formulas only integrate polynomials exactly, we have to live with the [1.x.2497] in the form of an integration error. 

While inaccurate integration is usually tolerable for elliptic problems, for hyperbolic problems inexact integration causes some headache in the form of an effect called [1.x.2498]. The term comes from signal processing and expresses the situation of inappropriate, too coarse sampling. In terms of quadrature, the inappropriate sampling means that we use too few quadrature points compared to what would be required to accurately sample the variable-coefficient integrand. It has been shown in the DG literature that aliasing errors can introduce unphysical oscillations in the numerical solution for [1.x.2499] resolved simulations. The fact that aliasing mostly affects coarse resolutions -- whereas finer meshes with the same scheme work fine -- is not surprising because well-resolved simulations tend to be smooth on length-scales of a cell (i.e., they have small coefficients in the higher polynomial degrees that are missed by too few quadrature points, whereas the main solution contribution in the lower polynomial degrees is still well-captured -- this is simply a consequence of Taylor's theorem). To address this topic, various approaches have been proposed in the DG literature. One technique is filtering which damps the solution components pertaining to higher polynomial degrees. As the chosen nodal basis is not hierarchical, this would mean to transform from the nodal basis into a hierarchical one (e.g., a modal one based on Legendre polynomials) where the contributions within a cell are split by polynomial degrees. In that basis, one could then multiply the solution coefficients associated with higher degrees by a small number, keep the lower ones intact (to not destroy consistency), and then transform back to the nodal basis. However, filters reduce the accuracy of the method. Another, in some sense simpler, strategy is to use more quadrature points to capture non-linear terms more accurately. Using more than  [2.x.6470]  quadrature points per coordinate directions is sometimes called over-integration or consistent integration. The latter name is most common in the context of the incompressible Navier-Stokes equations, where the  [2.x.6471]  nonlinearity results in polynomial integrands of degree  [2.x.6472]  (when also considering the test function), which can be integrated exactly with  [2.x.6473]  quadrature points per direction as long as the element geometry is affine. In the context of the Euler equations with non-polynomial integrands, the choice is less clear. Depending on the variation in the various variables both  [2.x.6474]  or  [2.x.6475]  points (integrating exactly polynomials of degree  [2.x.6476]  or  [2.x.6477] , respectively) are common. 

To reflect this variability in the choice of quadrature in the program, we keep the number of quadrature points a variable to be specified just as the polynomial degree, and note that one would make different choices depending also on the flow configuration. The default choice is  [2.x.6478]  points -- a bit more than the minimum possible of  [2.x.6479]  points. The FEEvaluation and FEFaceEvaluation classes allow to seamlessly change the number of points by a template parameter, such that the program does not get more complicated because of that. 




[1.x.2500] 

The last ingredient is the evaluation of the inverse mass matrix  [2.x.6480] . In DG methods with explicit time integration, mass matrices are block-diagonal and thus easily inverted -- one only needs to invert the diagonal blocks. However, given the fact that matrix-free evaluation of integrals is closer in cost to the access of the vectors only, even the application of a block-diagonal matrix (e.g. via an array of LU factors) would be several times more expensive than evaluation of  [2.x.6481]  simply because just storing and loading matrices of size `dofs_per_cell` times `dofs_per_cell` for higher order finite elements repeatedly is expensive. As this is clearly undesirable, part of the community has moved to bases where the mass matrix is diagonal, for example the [1.x.2501]-orthogonal Legendre basis using hierarchical polynomials or Lagrange polynomials on the points of the Gaussian quadrature (which is just another way of utilizing Legendre information). While the diagonal property breaks down for deformed elements, the error made by taking a diagonal mass matrix and ignoring the rest (a variant of mass lumping, though not the one with an additional integration error as utilized in step-48) has been shown to not alter discretization accuracy. The Lagrange basis in the points of Gaussian quadrature is sometimes also referred to as a collocation setup, as the nodal points of the polynomials coincide (= are "co-located") with the points of quadrature, obviating some interpolation operations. Given the fact that we want to use more quadrature points for nonlinear terms in  [2.x.6482] , however, the collocation property is lost. (More precisely, it is still used in FEEvaluation and FEFaceEvaluation after a change of basis, see the matrix-free paper  [2.x.6483] .) 

In this tutorial program, we use the collocation idea for the application of the inverse mass matrix, but with a slight twist. Rather than using the collocation via Lagrange polynomials in the points of Gaussian quadrature, we prefer a conventional Lagrange basis in Gauss-Lobatto points as those make the evaluation of face integrals cheap. This is because for Gauss-Lobatto points, some of the node points are located on the faces of the cell and it is not difficult to show that on any given face, the only shape functions with non-zero values are exactly the ones whose node points are in fact located on that face. One could of course also use the Gauss-Lobatto quadrature (with some additional integration error) as was done in step-48, but we do not want to sacrifice accuracy as these quadrature formulas are generally of lower order than the general Gauss quadrature formulas. Instead, we use an idea described in the reference  [2.x.6484]  where it was proposed to change the basis for the sake of applying the inverse mass matrix. Let us denote by  [2.x.6485]  the matrix of shape functions evaluated at quadrature points, with shape functions in the row of the matrix and quadrature points in columns. Then, the mass matrix on a cell  [2.x.6486]  is given by 

[1.x.2502] 

Here,  [2.x.6487]  is the diagonal matrix with the determinant of the Jacobian times the quadrature weight (JxW) as entries. The matrix  [2.x.6488]  is constructed as the Kronecker product (tensor product) of one-dimensional matrices, e.g. in 3D as 

[1.x.2503] 

which is the result of the basis functions being a tensor product of one-dimensional shape functions and the quadrature formula being the tensor product of 1D quadrature formulas. For the case that the number of polynomials equals the number of quadrature points, all matrices in  [2.x.6489]  are square, and also the ingredients to  [2.x.6490]  in the Kronecker product are square. Thus, one can invert each matrix to form the overall inverse, 

[1.x.2504] 

This formula is of exactly the same structure as the steps in the forward evaluation of integrals with sum factorization techniques (i.e., the FEEvaluation and MatrixFree framework of deal.II). Hence, we can utilize the same code paths with a different interpolation matrix,  [2.x.6491]  rather than  [2.x.6492] . 

The class  [2.x.6493]  implements this operation: It changes from the basis contained in the finite element (in this case, FE_DGQ) to the Lagrange basis in Gaussian quadrature points. Here, the inverse of a diagonal mass matrix can be evaluated, which is simply the inverse of the `JxW` factors (i.e., the quadrature weight times the determinant of the Jacobian from reference to real coordinates). Once this is done, we can change back to the standard nodal Gauss-Lobatto basis. 

The advantage of this particular way of applying the inverse mass matrix is a cost similar to the forward application of a mass matrix, which is cheaper than the evaluation of the spatial operator  [2.x.6494]  with over-integration and face integrals. (We will demonstrate this with detailed timing information in the [1.x.2505].) In fact, it is so cheap that it is limited by the bandwidth of reading the source vector, reading the diagonal, and writing into the destination vector on most modern architectures. The hardware used for the result section allows to do the computations at least twice as fast as the streaming of the vectors from memory. 




[1.x.2506] 

In this tutorial program, we implement two test cases. The first case is a convergence test limited to two space dimensions. It runs a so-called isentropic vortex which is transported via a background flow field. The second case uses a more exciting setup: We start with a cylinder immersed in a channel, using the  [2.x.6495]  function. Here, we impose a subsonic initial field at Mach number of  [2.x.6496]  with a constant velocity in  [2.x.6497]  direction. At the top and bottom walls as well as at the cylinder, we impose a no-penetration (i.e., tangential flow) condition. This setup forces the flow to re-orient as compared to the initial condition, which results in a big sound wave propagating away from the cylinder. In upstream direction, the wave travels more slowly (as it has to move against the oncoming gas), including a discontinuity in density and pressure. In downstream direction, the transport is faster as sound propagation and fluid flow go in the same direction, which smears out the discontinuity somewhat. Once the sound wave hits the upper and lower walls, the sound is reflected back, creating some nice shapes as illustrated in the [1.x.2507] below. 


examples/step-67/doc/results.dox 



[1.x.2508] 

[1.x.2509] 

Running the program with the default settings on a machine with 40 processes produces the following output: 

[1.x.2510] 



The program output shows that all errors are small. This is due to the fact that we use a relatively fine mesh of  [2.x.6498]  cells with polynomials of degree 5 for a solution that is smooth. An interesting pattern shows for the time step size: whereas it is 0.0069 up to time 5, it increases to 0.0096 for later times. The step size increases once the vortex with some motion on top of the speed of sound (and thus faster propagation) leaves the computational domain between times 5 and 6.5. After that point, the flow is simply uniform in the same direction, and the maximum velocity of the gas is reduced compared to the previous state where the uniform velocity was overlaid by the vortex. Our time step formula recognizes this effect. 

The final block of output shows detailed information about the timing of individual parts of the programs; it breaks this down by showing the time taken by the fastest and the slowest processor, and the average time -- this is often useful in very large computations to find whether there are processors that are consistently overheated (and consequently are throttling their clock speed) or consistently slow for other reasons. The summary shows that 1283 time steps have been performed in 1.02 seconds (looking at the average time among all MPI processes), while the output of 11 files has taken additional 0.96 seconds. Broken down per time step and into the five Runge--Kutta stages, the compute time per evaluation is 0.16 milliseconds. This high performance is typical of matrix-free evaluators and a reason why explicit time integration is very competitive against implicit solvers, especially for large-scale simulations. The breakdown of computational times at the end of the program run shows that the evaluation of integrals in  [2.x.6499]  contributes with around 0.92 seconds and the application of the inverse mass matrix with 0.06 seconds. Furthermore, the estimation of the transport speed for the time step size computation contributes with another 0.05 seconds of compute time. 

If we use three more levels of global refinement and 9.4 million DoFs in total, the final statistics are as follows (for the modified Lax--Friedrichs flux,  [2.x.6500] , and the same system of 40 cores of dual-socket Intel Xeon Gold 6230): 

[1.x.2511] 



Per time step, the solver now takes 0.02 seconds, about 25 times as long as for the small problem with 147k unknowns. Given that the problem involves 64 times as many unknowns, the increase in computing time is not surprising. Since we also do 8 times as many time steps, the compute time should in theory increase by a factor of 512. The actual increase is 205 s / 1.02 s = 202. This is because the small problem size cannot fully utilize the 40 cores due to communication overhead. This becomes clear if we look into the details of the operations done per time step. The evaluation of the differential operator  [2.x.6501]  with nearest neighbor communication goes from 0.92 seconds to 127 seconds, i.e., it increases with a factor of 138. On the other hand, the cost for application of the inverse mass matrix and the vector updates, which do not need to communicate between the MPI processes at all, has increased by a factor of 1195. The increase is more than the theoretical factor of 512 because the operation is limited by the bandwidth from RAM memory for the larger size while for the smaller size, all vectors fit into the caches of the CPU. The numbers show that the mass matrix evaluation and vector update part consume almost 40% of the time spent by the Runge--Kutta stages -- despite using a low-storage Runge--Kutta integrator and merging of vector operations! And despite using over-integration for the  [2.x.6502]  operator. For simpler differential operators and more expensive time integrators, the proportion spent in the mass matrix and vector update part can also reach 70%. If we compute a throughput number in terms of DoFs processed per second and Runge--Kutta stage, we obtain [1.x.2512] This throughput number is very high, given that simply copying one vector to another one runs at only around 10,000 MDoFs/s. 

If we go to the next-larger size with 37.7 million DoFs, the overall simulation time is 2196 seconds, with 1978 seconds spent in the time stepping. The increase in run time is a factor of 9.3 for the L_h operator (1179 versus 127 seconds) and a factor of 10.3 for the inverse mass matrix and vector updates (797 vs 77.5 seconds). The reason for this non-optimal increase in run time can be traced back to cache effects on the given hardware (with 40 MB of L2 cache and 55 MB of L3 cache): While not all of the relevant data fits into caches for 9.4 million DoFs (one vector takes 75 MB and we have three vectors plus some additional data in MatrixFree), there is capacity for one and a half vector nonetheless. Given that modern caches are more sophisticated than the naive least-recently-used strategy (where we would have little re-use as the data is used in a streaming-like fashion), we can assume that a sizeable fraction of data can indeed be delivered from caches for the 9.4 million DoFs case. For the larger case, even with optimal caching less than 10 percent of data would fit into caches, with an associated loss in performance. 




[1.x.2513] 

For the modified Lax--Friedrichs flux and measuring the error in the momentum variable, we obtain the following convergence table (the rates are very similar for the density and energy variables): 

 [2.x.6503]  

If we switch to the Harten-Lax-van Leer flux, the results are as follows:  [2.x.6504]  

The tables show that we get optimal  [2.x.6505]  convergence rates for both numerical fluxes. The errors are slightly smaller for the Lax--Friedrichs flux for  [2.x.6506] , but the picture is reversed for  [2.x.6507] ; in any case, the differences on this testcase are relatively small. 

For  [2.x.6508] , we reach the roundoff accuracy of  [2.x.6509]  with both fluxes on the finest grids. Also note that the errors are absolute with a domain length of  [2.x.6510] , so relative errors are below  [2.x.6511] . The HLL flux is somewhat better for the highest degree, which is due to a slight inaccuracy of the Lax--Friedrichs flux: The Lax--Friedrichs flux sets a Dirichlet condition on the solution that leaves the domain, which results in a small artificial reflection, which is accentuated for the Lax--Friedrichs flux. Apart from that, we see that the influence of the numerical flux is minor, as the polynomial part inside elements is the main driver of the accucary. The limited influence of the flux also has consequences when trying to approach more challenging setups with the higher-order DG setup: Taking for example the parameters and grid of step-33, we get oscillations (which in turn make density negative and make the solution explode) with both fluxes once the high-mass part comes near the boundary, as opposed to the low-order finite volume case ( [2.x.6512] ). Thus, any case that leads to shocks in the solution necessitates some form of limiting or artificial dissipation. For another alternative, see the step-69 tutorial program. 




[1.x.2520] 

For the test case of the flow around a cylinder in a channel, we need to change the first code line to 

[1.x.2521] 

This test case starts with a background field of a constant velocity of Mach number 0.31 and a constant initial density; the flow will have to go around an obstacle in the form of a cylinder. Since we impose a no-penetration condition on the cylinder walls, the flow that initially impinges head-on onto to cylinder has to rearrange, which creates a big sound wave. The following pictures show the pressure at times 0.1, 0.25, 0.5, and 1.0 (top left to bottom right) for the 2D case with 5 levels of global refinement, using 102,400 cells with polynomial degree of 5 and 14.7 million degrees of freedom over all 4 solution variables. We clearly see the discontinuity that propagates slowly in the upstream direction and more quickly in downstream direction in the first snapshot at time 0.1. At time 0.25, the sound wave has reached the top and bottom walls and reflected back to the interior. From the different distances of the reflected waves from lower and upper walls we can see the slight asymmetry of the Sch&auml;fer-Turek test case represented by  [2.x.6513]  with somewhat more space above the cylinder compared to below. At later times, the picture is more chaotic with many sound waves all over the place. 

 [2.x.6514]  

The next picture shows an elevation plot of the pressure at time 1.0 looking from the channel inlet towards the outlet at the same resolution -- here, we can see the large number of reflections. In the figure, two types of waves are visible. The larger-amplitude waves correspond to various reflections that happened as the initial discontinuity hit the walls, whereas the small-amplitude waves of size similar to the elements correspond to numerical artifacts. They have their origin in the finite resolution of the scheme and appear as the discontinuity travels through elements with high-order polynomials. This effect can be cured by increasing resolution. Apart from this effect, the rich wave structure is the result of the transport accuracy of the high-order DG method. 

 [2.x.6515]  

With 2 levels of global refinement with 1,600 cells, the mesh and its partitioning on 40 MPI processes looks as follows: 

 [2.x.6516]  

When we run the code with 4 levels of global refinements on 40 cores, we get the following output: 

[1.x.2522] 



The norms shown here for the various quantities are the deviations  [2.x.6517] ,  [2.x.6518] , and  [2.x.6519]  against the background field (namely, the initial condition). The distribution of run time is overall similar as in the previous test case. The only slight difference is the larger proportion of time spent in  [2.x.6520]  as compared to the inverse mass matrix and vector updates. This is because the geometry is deformed and the matrix-free framework needs to load additional arrays for the geometry from memory that are compressed in the affine mesh case. 

Increasing the number of global refinements to 5, the output becomes: 

[1.x.2523] 



The effect on performance is similar to the analytical test case -- in theory, computation times should increase by a factor of 8, but we actually see an increase by a factor of 11 for the time steps (219.5 seconds versus 2450 seconds). This can be traced back to caches, with the small case mostly fitting in caches. An interesting effect, typical of programs with a mix of local communication (integrals  [2.x.6521] ) and global communication (computation of transport speed) with some load imbalance, can be observed by looking at the MPI ranks that encounter the minimal and maximal time of different phases, respectively. Rank 0 reports the fastest throughput for the "rk time stepping total" part. At the same time, it appears to be slowest for the "compute transport speed" part, almost a factor of 2 slower than the average and almost a factor of 4 compared to the faster rank. Since the latter involves global communication, we can attribute the slowness in this part to the fact that the local Runge--Kutta stages have advanced more quickly on this rank and need to wait until the other processors catch up. At this point, one can wonder about the reason for this imbalance: The number of cells is almost the same on all MPI processes. However, the matrix-free framework is faster on affine and Cartesian cells located towards the outlet of the channel, to which the lower MPI ranks are assigned. On the other hand, rank 32, which reports the highest run time for the Runga--Kutta stages, owns the curved cells near the cylinder, for which no data compression is possible. To improve throughput, we could assign different weights to different cell types when partitioning the  [2.x.6522]  object, or even measure the run time for a few time steps and try to rebalance then. 

The throughput per Runge--Kutta stage can be computed to 2085 MDoFs/s for the 14.7 million DoFs test case over the 346,000 Runge--Kutta stages, slightly slower than the Cartesian mesh throughput of 2360 MDoFs/s reported above. 

Finally, if we add one additional refinement, we record the following output: 

[1.x.2524] 



The "rk time stepping total" part corresponds to a throughput of 2010 MDoFs/s. The overall run time to perform 139k time steps is 20k seconds (5.7 hours) or 7 time steps per second -- not so bad for having nearly 60 million unknowns. More throughput can be achieved by adding more cores to the computation. 




[1.x.2525] 

Switching the channel test case to 3D with 3 global refinements, the output is 

[1.x.2526] 



The physics are similar to the 2D case, with a slight motion in the z direction due to the gravitational force. The throughput per Runge--Kutta stage in this case is 

[1.x.2527] 



The throughput is lower than in 2D because the computation of the  [2.x.6523]  term is more expensive. This is due to over-integration with `degree+2` points and the larger fraction of face integrals (worse volume-to-surface ratio) with more expensive flux computations. If we only consider the inverse mass matrix and vector update part, we record a throughput of 4857 MDoFs/s for the 2D case of the isentropic vortex with 37.7 million unknowns, whereas the 3D case runs with 4535 MDoFs/s. The performance is similar because both cases are in fact limited by the memory bandwidth. 

If we go to four levels of global refinement, we need to increase the number of processes to fit everything in memory -- the computation needs around 350 GB of RAM memory in this case. Also, the time it takes to complete 35k time steps becomes more tolerable by adding additional resources. We therefore use 6 nodes with 40 cores each, resulting in a computation with 240 MPI processes: 

[1.x.2528] 

This simulation had nearly 2 billion unknowns -- quite a large computation indeed, and still only needed around 1.5 seconds per time step. 




[1.x.2529] 

The code presented here straight-forwardly extends to adaptive meshes, given appropriate indicators for setting the refinement flags. Large-scale adaptivity of a similar solver in the context of the acoustic wave equation has been achieved by the [1.x.2530]. However, in the present context, the benefits of adaptivity are often limited to early times and effects close to the origin of sound waves, as the waves eventually reflect and diffract. This leads to steep gradients all over the place, similar to turbulent flow, and a more or less globally refined mesh. 

Another topic that we did not discuss in the results section is a comparison of different time integration schemes. The program provides four variants of low-storage Runga--Kutta integrators that each have slightly different accuracy and stability behavior. Among the schemes implemented here, the higher-order ones provide additional accuracy but come with slightly lower efficiency in terms of step size per stage before they violate the CFL condition. An interesting extension would be to compare the low-storage variants proposed here with standard Runge--Kutta integrators or to use vector operations that are run separate from the mass matrix operation and compare performance. 




[1.x.2531] 

As mentioned in the introduction, the modified Lax--Friedrichs flux and the HLL flux employed in this program are only two variants of a large body of numerical fluxes available in the literature on the Euler equations. One example is the HLLC flux (Harten-Lax-van Leer-Contact) flux which adds the effect of rarefaction waves missing in the HLL flux, or the Roe flux. As mentioned in the introduction, the effect of numerical fluxes on high-order DG schemes is debatable (unlike for the case of low-order discretizations). 

A related improvement to increase the stability of the solver is to also consider the spatial integral terms. A shortcoming in the rather naive implementation used above is the fact that the energy conservation of the original Euler equations (in the absence of shocks) only holds up to a discretization error. If the solution is under-resolved, the discretization error can give rise to an increase in the numerical energy and eventually render the discretization unstable. This is because of the inexact numerical integration of the terms in the Euler equations, which both contain rational nonlinearities and higher-degree content from curved cells. A way out of this dilemma are so-called skew-symmetric formulations, see  [2.x.6524]  for a simple variant. Skew symmetry means that switching the role of the solution  [2.x.6525]  and test functions  [2.x.6526]  in the weak form produces the exact negative of the original quantity, apart from some boundary terms. In the discrete setting, the challenge is to keep this skew symmetry also when the integrals are only computed approximately (in the continuous case, skew-symmetry is a consequence of integration by parts). Skew-symmetric numerical schemes balance spatial derivatives in the conservative form  [2.x.6527]  with contributions in the convective form  [2.x.6528]  for some  [2.x.6529] . The precise terms depend on the equation and the integration formula, and can in some cases by understood by special skew-symmetric finite difference schemes. 

To get started, interested readers could take a look at https://github.com/kronbichler/advection_miniapp, where a skew-symmetric DG formulation is implemented with deal.II for a simple advection equation. 

[1.x.2532] 

As mentioned in the introduction, the solution to the Euler equations develops shocks as the Mach number increases, which require additional mechanisms to stabilize the scheme, e.g. in the form of limiters. The main challenge besides actually implementing the limiter or artificial viscosity approach would be to load-balance the computations, as the additional computations involved for limiting the oscillations in troubled cells would make them more expensive than the plain DG cells without limiting. Furthermore, additional numerical fluxes that better cope with the discontinuities would also be an option. 

One ingredient also necessary for supersonic flows are appropriate boundary conditions. As opposed to the subsonic outflow boundaries discussed in the introduction and implemented in the program, all characteristics are outgoing for supersonic outflow boundaries, so we do not want to prescribe any external data, 

[1.x.2533] 



In the code, we would simply add the additional statement 

[1.x.2534] 

in the `local_apply_boundary_face()` function. 

[1.x.2535] 

When the interest with an Euler solution is mostly in the propagation of sound waves, it often makes sense to linearize the Euler equations around a background state, i.e., a given density, velocity and energy (or pressure) field, and only compute the change against these fields. This is the setting of the wide field of aeroacoustics. Even though the resolution requirements are sometimes considerably reduced, implementation gets somewhat more complicated as the linearization gives rise to additional terms. From a code perspective, in the operator evaluation we also need to equip the code with the state to linearize against. This information can be provided either by analytical functions (that are evaluated in terms of the position of the quadrature points) or by a vector similar to the solution. Based on that vector, we would create an additional FEEvaluation object to read from it and provide the values of the field at quadrature points. If the background velocity is zero and the density is constant, the linearized Euler equations further simplify and can equivalently be written in the form of the acoustic wave equation. 

A challenge in the context of sound propagation is often the definition of boundary conditions, as the computational domain needs to be of finite size, whereas the actual simulation often spans an infinite (or at least much larger) physical domain. Conventional Dirichlet or Neumann boundary conditions give rise to reflections of the sound waves that eventually propagate back to the region of interest and spoil the solution. Therefore, various variants of non-reflecting boundary conditions or sponge layers, often in the form of [1.x.2536] -- where the solution is damped without reflection 

-- are common. 




[1.x.2537] 

The solver presented in this tutorial program can also be extended to the compressible Navier--Stokes equations by adding viscous terms, as described in  [2.x.6530] . To keep as much of the performance obtained here despite the additional cost of elliptic terms, e.g. via an interior penalty method, one can switch the basis from FE_DGQ to FE_DGQHermite like in the step-59 tutorial program. 




[1.x.2538] 

In this tutorial, we used face-centric loops. Here, cell and face integrals are treated in separate loops, resulting in multiple writing accesses into the result vector, which is relatively expensive on modern hardware since writing operations generally result also in an implicit read operation. Element-centric loops, on the other hand, are processing a cell and in direct succession processing all its 2d faces. Although this kind of loop implies that fluxes have to be computed twice (for each side of an interior face), the fact that the result vector has to accessed only once might - and the fact that the resulting algorithm is free of race-conditions and as such perfectly suitable for shared memory - already give a performance boost. If you are interested in these advanced topics, you can take a look at step-76 where we take the present tutorial and modify it so that we can use these features. 


examples/step-68/doc/intro.dox 

 [2.x.6531]  

[1.x.2539] 

[1.x.2540] 

[1.x.2541] 

Particles play an important part in numerical models for a large  number of applications. Particles are routinely used  as massless tracers to visualize the dynamic of a transient flow. They  can also play an intrinsic role as part of a more complex finite element  model, as is the case for the Particle-In-Cell (PIC) method  [2.x.6532]   or they can even be used to simulate the motion of granular matter, as in  the Discrete Element Method (DEM)  [2.x.6533] . In the case  of DEM, the resulting model is not related to the finite element method anymore,  but just leads to a system of ordinary differential equation which describes  the motion of the particles and the dynamic of their collisions. All of  these models can be built using deal.II's particle handling capabilities. 

In the present step, we use particles as massless tracers to illustrate the dynamic of a vortical flow. Since the particles are massless tracers, the position of each particle  [2.x.6534]  is described by the following ordinary differential equation (ODE): 

[1.x.2542] 



where  [2.x.6535]  is the position of particle  [2.x.6536]  and  [2.x.6537]  the flow velocity at its position. In the present step, this ODE is solved using the explicit Euler method. The resulting scheme is: 

[1.x.2543] 



where  [2.x.6538]  and  [2.x.6539]  are the position of particle  [2.x.6540]  at time  [2.x.6541]  and  [2.x.6542] , respectively and where  [2.x.6543]  is the time step. In the present step, the velocity at the location of particles is obtained in two different fashions: 

- By evaluating the velocity function at the location of the particles; 

- By evaluating the velocity function on a background triangulation and, using a  finite element support, interpolating at the position of the particle. 

The first approach is not practical, since the velocity profile is generally not known analytically. The second approach, based on interpolating a solution at the position of the particles, mimics exactly what would be done in a realistic computational fluid dynamic simulation, and this follows the way we have also evaluated the finite element solution at particle locations in step-19. In this step, we illustrate both strategies. 

We note that much greater accuracy could be obtained by using a fourth order Runge-Kutta method or another appropriate scheme for the time integration of the motion of the particles.  Implementing a more advanced time-integration scheme would be a straightforward extension of this step. 

[1.x.2544] 

In deal.II,  [2.x.6544]  are very simple and flexible entities that can be used to build PIC, DEM or any type of particle-based models. Particles have a location in real space, a location in the reference space of the element in which they are located and a unique ID. In the majority of cases, simulations that include particles require a significant number of them. Thus, it becomes interesting to handle all particles through an entity which agglomerates all particles. In deal.II, this is achieved through the use of the  [2.x.6545]  class. 

By default, particles do not have a diameter, a mass or any other physical properties which we would generally expect of physical particles. However, through a ParticleHandler, particles have access to a  [2.x.6546]  This PropertyPool is an array which can be used to store an arbitrary number of properties associated with the particles. Consequently, users can build their own particle solver and attribute the desired properties to the particles (e.g., mass, charge, diameter, temperature, etc.). In the present tutorial, this is used to store the value of the fluid velocity and the process id to which the particles belong. 

[1.x.2545] 

Although the present step is not computationally intensive, simulations that include many particles can be computationally demanding and require parallelization. The present step showcases the distributed parallel capabilities of deal.II for particles. In general, there are three main challenges that specifically arise in parallel distributed simulations that include particles: 

- Generating the particles on the distributed triangulation; 

- Exchanging the particles that leave local domains between the processors; 

- Load balancing the simulation so that every processor has a similar computational load. These challenges and their solution in deal.II have been discussed in more detail in  [2.x.6547] , but we will summarize them below. 

There are of course also questions on simply setting up a code that uses particles. These have largely already been addressed in step-19. Some more advanced techniques will also be discussed in step-70. 

[1.x.2546] 

Generating distributed particles in a scalable way is not straightforward since the processor to which they belong must first be identified before the cell in which they are located is found.  deal.II provides numerous capabilities to generate particles through the  [2.x.6548]  namespace.  Some of these particle generators create particles only on the locally owned subdomain. For example,  [2.x.6549]  creates particles at the same reference locations within each cell of the local subdomain and  [2.x.6550]  uses a globally defined probability density function to determine how many and where to generate particles locally. 

In other situations, such as the present step, particles must be generated at specific locations on cells that may be owned only by a subset of the processors. In  most of these situations, the insertion of the particles is done for a very limited number of time-steps and, consequently, does not constitute a large portion of the computational cost. For these occasions, deal.II provides convenient  [2.x.6551]  that can globally insert the particles even if the particle is not located in a cell owned by the parallel process on which the call to create the particle is initiated. The generators first locate on which subdomain the particles are situated, identify in which cell they are located and exchange the necessary information among the processors to ensure that the particle is generated with the right properties. Consequently, this type of particle generation can be communication intensive. The  [2.x.6552]  and the  [2.x.6553]  generate particles using a triangulation and the points of an associated DoFHandler or quadrature respectively. The triangulation that is used to generate the particles can be the same triangulation that is used for the background mesh, in which case these functions are very similar to the  [2.x.6554]  function described in the previous paragraph. However, the triangulation used to generate particles can also be different (non-matching) from the triangulation of the background grid, which is useful to generate particles in particular shapes (as in this example), or to transfer information between two different computational grids (as in step-70).  Furthermore, the  [2.x.6555]  class provides the  [2.x.6556]  function which enables the global insertion of particles from a vector of arbitrary points and a global vector of bounding boxes. In the present step, we use the  [2.x.6557]  function on a non-matching triangulation to insert particles located at positions in the shape of a disk. 

[1.x.2547] 

As particles move around in parallel distributed computations they may leave the locally owned subdomain and need to be transferred to their new owner processes. This situation can arise in two very different ways: First, if the previous owning process knows the new owner of the particles that were lost (for example because the particles moved from the locally owned cell of one processor into an adjacent ghost cells of a distributed triangulation) then the transfer can be handled efficiently as a point-to-point communication between each process and the new owners. This transfer happens automatically whenever particles are sorted into their new cells. Secondly, the previous owner may not know to which process the particle has moved. In this case the particle is discarded by default, as a global search for the owner can be expensive. step-19 shows how such a discarded particle can still be collected, interpreted, and potentially reinserted by the user. In the present example we prevent the second case by imposing a CFL criterion on the timestep to ensure particles will at most move into the ghost layer of the local process and can therefore be send to neighboring processes automatically. 

[1.x.2548] 

The last challenge that arises in parallel distributed computations using particles is to balance the computational load between work that is done on the grid, for example solving the finite-element problem, and the work that is done on the particles, for example advecting the particles or computing the forces between particles or between particles and grid. By default, for example in step-40, deal.II distributes the background mesh as evenly as possible between the available processes, that is it balances the number of cells on each process. However, if some cells own many more particles than other cells, or if the particles of one cell are much more computationally expensive than the particles in other cells, then this problem no longer scales efficiently (for a discussion of what we consider "scalable" programs, see  [2.x.6558]  "this glossary entry"). Thus, we have to apply a form of "load balancing", which means we estimate the computational load that is associated with each cell and its particles. Repartitioning the mesh then accounts for this combined computational load instead of the simplified assumption of the number of cells  [2.x.6559] . 

In this section we only discussed the particle-specific challenges in distributed computation. Parallel challenges that particles share with finite-element solutions (parallel output, data transfer during mesh refinement) can be addressed with the solutions found for finite-element problems already discussed in other examples. 

[1.x.2549] 

In the present step, we use particles as massless tracers to illustrate the dynamics of a particular vortical flow: the Rayleigh--Kothe vortex. This flow pattern is generally used as a complex test case for interface tracking methods (e.g., volume-of-fluid and level set approaches) since it leads to strong rotation and elongation of the fluid  [2.x.6560] . 

The stream function  [2.x.6561]  of this Rayleigh-Kothe vortex is defined as: 

[1.x.2550] 

where  [2.x.6562]  is half the period of the flow. The velocity profile in 2D ( [2.x.6563] ) is : 

[1.x.2551] 



The velocity profile is illustrated in the following animation: 

[1.x.2552] 



It can be seen that this velocity reverses periodically due to the term  [2.x.6564]  and that material will end up at its starting position after every period of length  [2.x.6565] . We will run this tutorial program for exactly one period and compare the final particle location to the initial location to illustrate this flow property. This example uses the testcase to produce two models that handle the particles slightly differently. The first model prescribes the exact analytical velocity solution as the velocity for each particle. Therefore in this model there is no error in the assigned velocity to the particles, and any deviation of particle positions from the analytical position at a given time results from the error in solving the equation of motion for the particle inexactly, using a time stepping method. In the second model the analytical velocity field is first interpolated to a finite-element vector space (to simulate the case that the velocity was obtained from solving a finite-element problem, in the same way as the ODE for each particle in step-19 depends on a finite element solution). This finite-element "solution" is then evaluated at the locations of the particles to solve their equation of motion. The difference between the two cases allows to assess whether the chosen finite-element space is sufficiently accurate to advect the particles with the optimal convergence rate of the chosen particle advection scheme, a question that is important in practice to determine the accuracy of the combined algorithm (see e.g.  [2.x.6566] ). 


examples/step-68/doc/results.dox 



[1.x.2553] 

The directory in which this program is run contains an example parameter file by default. If you do not specify a parameter file as an argument on the command line, the program will try to read the file "parameters.prm" by default, and will execute the code. 

On any number of cores, the simulation output will look like: 

[1.x.2554] 



We note that, by default, the simulation runs the particle tracking with an analytical velocity for 2000 iterations, then restarts from the beginning and runs the particle tracking with velocity interpolation for the same duration. The results are written every 10th iteration. 

[1.x.2555] 

The following animation displays the trajectory of the particles as they are advected by the flow field. We see that after the complete duration of the flow, the particle go back to their initial configuration as is expected. 

[1.x.2556] 



[1.x.2557] 

The following animation shows the impact of dynamic load balancing. We clearly see that the subdomains adapt themselves to balance the number of particles per subdomain. However, a perfect load balancing is not reached, in part due to the coarseness of the background mesh. 

[1.x.2558] 






[1.x.2559] 

This program highlights some of the main capabilities for handling particles in deal.II, notably their capacity to be used in distributed parallel simulations. However, this step could be extended in numerous manners: 

- High-order time integration (for example using a Runge-Kutta 4 method) could be used to increase the accuracy or allow for larger time-step sizes with the same accuracy. 

- The full equation of motion (with inertia) could be solved for the particles. In this case the particles would need to have additional properties such as their mass, as in step-19, and if one wanted to also consider interactions with the fluid, their diameter. 

- Coupling to a flow solver. This step could be straightforwardly coupled to any parallel program in which the Stokes (step-32, step-70) or the Navier-Stokes equations are solved (e.g., step-57). 

- Computing the difference in final particle positions between the two models would allow to quantify the influence of the interpolation error on particle motion. 


examples/step-69/doc/intro.dox 

[1.x.2560] 

 [2.x.6567]  [2.x.6568] Sandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA0003525. This document describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government. [2.x.6569]  

 [2.x.6570]  This tutorial step implements a first-order accurate [1.x.2561] based on a first-order [1.x.2562] for solving Euler's equations of gas dynamics  [2.x.6571] . As such it is presented primarily for educational purposes. For actual research computations you might want to consider exploring a corresponding [1.x.2563] that uses [1.x.2564] techniques, and strong stability-preserving (SSP) time integration, see  [2.x.6572]  ([1.x.2565]). 

 [2.x.6573]  

[1.x.2566] 

[1.x.2567] 

This tutorial presents a first-order scheme for solving compressible Euler's equations that is based on three ingredients: a [1.x.2568]-type discretization of Euler's equations in the context of finite elements; a graph-viscosity stabilization based on a [1.x.2569] upper bound of the local wave speed; and explicit time-stepping. As such, the ideas and techniques presented in this tutorial step are drastically different from those used in step-33, which focuses on the use of automatic differentiation. From a programming perspective this tutorial will focus on a number of techniques found in large-scale computations: hybrid thread-MPI parallelization; efficient local numbering of degrees of freedom; concurrent post-processing and write-out of results using worker threads; as well as checkpointing and restart. 

It should be noted that first-order schemes in the context of hyperbolic conservation laws require prohibitively many degrees of freedom to resolve certain key features of the simulated fluid, and thus, typically only serve as elementary building blocks in higher-order schemes  [2.x.6574] . However, we hope that the reader still finds the tutorial step to be a good starting point (in particular with respect to the programming techniques) before jumping into full research codes such as the second-order scheme discussed in  [2.x.6575] . 


[1.x.2570] 

[1.x.2571] 

The compressible Euler's equations of gas dynamics are written in conservative form as follows: 

[1.x.2572] 

where  [2.x.6576] , and  [2.x.6577] , and  [2.x.6578]  is the space dimension. We say that  [2.x.6579]  is the state and  [2.x.6580]  is the flux of the system. In the case of Euler's equations the state is given by  [2.x.6581] : where  [2.x.6582]  denotes the density,  [2.x.6583]  is the momentum, and  [2.x.6584]  is the total energy of the system. The flux of the system  [2.x.6585]  is defined as 

[1.x.2573] 

where  [2.x.6586]  is the identity matrix and  [2.x.6587]  denotes the tensor product. Here, we have introduced the pressure  [2.x.6588]  that, in general, is defined by a closed-form equation of state. In this tutorial we limit the discussion to the class of polytropic ideal gases for which the pressure is given by 

[1.x.2574] 

where the factor  [2.x.6589]  denotes the [1.x.2575]. 




[1.x.2576] 

Hyperbolic conservation laws, such as 

[1.x.2577] 

pose a significant challenge with respect to solution theory. An evident observation is that rewriting the equation in variational form and testing with the solution itself does not lead to an energy estimate because the pairing  [2.x.6590]  (understood as the  [2.x.6591]  inner product or duality pairing) is not guaranteed to be non-negative. Notions such as energy-stability or  [2.x.6592] -stability are (in general) meaningless in this context. 

Historically, the most fruitful step taken in order to deepen the understanding of hyperbolic conservation laws was to assume that the solution is formally defined as  [2.x.6593]  where  [2.x.6594]  is the solution of the parabolic regularization 

[1.x.2578] 

Such solutions, which are understood as the solution recovered in the zero-viscosity limit, are often referred to as [1.x.2579]. (This is, because physically  [2.x.6595]  can be understood as related to the viscosity of the fluid, i.e., a quantity that indicates the amount of friction neighboring gas particles moving at different speeds exert on each other. The Euler equations themselves are derived under the assumption of no friction, but can physically be expected to describe the limiting case of vanishing friction or viscosity.) Global existence and uniqueness of such solutions is an open issue. However, we know at least that if such viscosity solutions exists they have to satisfy the constraint  [2.x.6596]  for all  [2.x.6597]  and  [2.x.6598]  where 

[1.x.2580] 

Here,  [2.x.6599]  denotes the specific entropy 

[1.x.2581] 

We will refer to  [2.x.6600]  as the invariant set of Euler's equations. In other words, a state  [2.x.6601]  obeys positivity of the density, positivity of the internal energy, and a local minimum principle on the specific entropy. This condition is a simplified version of a class of pointwise stability constraints satisfied by the exact (viscosity) solution. By pointwise we mean that the constraint has to be satisfied at every point of the domain, not just in an averaged (integral, or high order moments) sense. 

In context of a numerical approximation, a violation of such a constraint has dire consequences: it almost surely leads to catastrophic failure of the numerical scheme, loss of hyperbolicity, and overall, loss of well-posedness of the (discrete) problem. It would also mean that we have computed something that can not be interpreted physically. (For example, what are we to make of a computed solution with a negative density?) In the following we will formulate a scheme that ensures that the discrete approximation of  [2.x.6602]  remains in  [2.x.6603] . 




[1.x.2582] 

Following Step-9, Step-12, Step-33, and Step-67, at this point it might look tempting to base a discretization of Euler's equations on a (semi-discrete) variational formulation: 

[1.x.2583] 

Here,  [2.x.6604]  is an appropriate finite element space, and  [2.x.6605]  is some linear stabilization method (possibly complemented with some ad-hoc shock-capturing technique, see for instance Chapter 5 of  [2.x.6606]  and references therein). Most time-dependent discretization approaches described in the deal.II tutorials are based on such a (semi-discrete) variational approach. Fundamentally, from an analysis perspective, variational discretizations are conceived to provide some notion of global (integral) stability, meaning an estimate of the form 

[1.x.2584] 

holds true, where  [2.x.6607]  could represent the  [2.x.6608] -norm or, more generally, some discrete (possibly mesh dependent) energy-norm. Variational discretizations of hyperbolic conservation laws have been very popular since the mid eighties, in particular combined with SUPG-type stabilization and/or upwinding techniques (see the early work of  [2.x.6609]  and  [2.x.6610] ). They have proven to be some of the best approaches for simulations in the subsonic shockless regime and similarly benign situations. 

<!-- In particular, tutorial Step-67 focuses on Euler's equation of gas dynamics in the subsonic regime using dG techniques. --> 

However, in the transonic and supersonic regimes, and shock-hydrodynamics applications the use of variational schemes might be questionable. In fact, at the time of this writing, most shock-hydrodynamics codes are still firmly grounded on finite volume methods. The main reason for failure of variational schemes in such extreme regimes is the lack of pointwise stability. This stems from the fact that [1.x.2585] bounds on integrated quantities (e.g. integrals of moments) have in general no implications on pointwise properties of the solution. While some of these problems might be alleviated by the (perpetual) chase of the right shock capturing scheme, finite difference-like and finite volume schemes still have an edge in many regards. 

In this tutorial step we therefore depart from variational schemes. We will present a completely algebraic formulation (with the flavor of a collocation-type scheme) that preserves constraints pointwise, i.e., 

[1.x.2586] 

Contrary to finite difference/volume schemes, the scheme implemented in this step maximizes the use of finite element software infrastructure, works on any mesh, in any space dimension, and is theoretically guaranteed to always work, all the time, no exception. This illustrates that deal.II can be used far beyond the context of variational schemes in Hilbert spaces and that a large number of classes, modules and namespaces from deal.II can be adapted for such a purpose. 




[1.x.2587] 

Let  [2.x.6611]  be scalar-valued finite dimensional space spanned by a basis  [2.x.6612]  where:  [2.x.6613]  and  [2.x.6614]  is the set of all indices (nonnegative integers) identifying each scalar Degree of Freedom (DOF) in the mesh. Therefore a scalar finite element functional  [2.x.6615]  can be written as  [2.x.6616]  with  [2.x.6617] . We introduce the notation for vector-valued approximation spaces  [2.x.6618] . Let  [2.x.6619] , then it can be written as  [2.x.6620]  where  [2.x.6621]  and  [2.x.6622]  is a scalar-valued shape function. 

 [2.x.6623]  We purposely refrain from using vector-valued finite element spaces in our notation. Vector-valued finite element spaces are natural for variational formulations of PDE systems (e.g. Navier-Stokes). In such context, the interactions that have to be computed describe [1.x.2588]: with proper renumbering of the vector-valued DoFHandler (i.e. initialized with an FESystem) it is possible to compute the block-matrices (required in order to advance the solution) with relative ease. However, the interactions that have to be computed in the context of time-explicit collocation-type schemes (such as finite differences and/or the scheme presented in this tutorial) can be better described as [1.x.2589] (not between DOFs). In addition, in our case we do not solve a linear equation in order to advance the solution. This leaves very little reason to use vector-valued finite element spaces both in theory and/or practice. 

We will use the usual Lagrange finite elements: let  [2.x.6624]  denote the set of all support points (see  [2.x.6625]  "this glossary entry"), where  [2.x.6626] . Then each index  [2.x.6627]  uniquely identifies a support point  [2.x.6628] , as well as a scalar-valued shape function  [2.x.6629] . With this notation at hand we can define the (explicit time stepping) scheme as: 

[1.x.2590] 

where 

  -  [2.x.6630]      is the lumped mass matrix 

  -  [2.x.6631]  is the time step size 

  -  [2.x.6632]  (note that  [2.x.6633] )     is a vector-valued matrix that was used to approximate the divergence     of the flux in a weak sense. 

  -  [2.x.6634]  is the adjacency list     containing all degrees of freedom coupling to the index  [2.x.6635] . In other     words  [2.x.6636]  contains all nonzero column indices for row     index i.  [2.x.6637]  will also be called a "stencil". 

  -  [2.x.6638]  is the flux  [2.x.6639]  of the     hyperbolic system evaluated for the state  [2.x.6640]  associated     with support point  [2.x.6641] . 

  -  [2.x.6642]  if  [2.x.6643]  is the so     called [1.x.2591]. The graph viscosity serves as a     stabilization term, it is somewhat the discrete counterpart of      [2.x.6644]  that appears in the notion of viscosity     solution described above. We will base our construction of  [2.x.6645]  on     an estimate of the maximal local wavespeed  [2.x.6646]  that     will be explained in detail in a moment. 

  - the diagonal entries of the viscosity matrix are defined as      [2.x.6647] . 

  -  [2.x.6648]  is a     normalization of the  [2.x.6649]  matrix that enters the     approximate Riemann solver with which we compute an the approximations      [2.x.6650]  on the local wavespeed. (This will be explained     further down below). 

The definition of  [2.x.6651]  is far from trivial and we will postpone the precise definition in order to focus first on some algorithmic and implementation questions. We note that 

  -  [2.x.6652]  and  [2.x.6653]  do not evolve in time (provided we keep the     discretization fixed). It thus makes sense to assemble these     matrices/vectors once in a so called [1.x.2592] and reuse     them in every time step. They are part of what we are going to call     off-line data. 

  - At every time step we have to evaluate  [2.x.6654]  and      [2.x.6655] , which will     constitute the bulk of the computational cost. 

Consider the following pseudo-code, illustrating a possible straight forward strategy for computing the solution  [2.x.6656]  at a new time  [2.x.6657]  given a known state  [2.x.6658]  at time  [2.x.6659] : 

[1.x.2593] 



We note here that: 

- This "assembly" does not require any form of quadrature or cell-loops. 

- Here  [2.x.6660]  and  [2.x.6661]  are a global matrix and a global vector containing all the vectors  [2.x.6662]  and all the states  [2.x.6663]  respectively. 

-  [2.x.6664] ,  [2.x.6665] , and  [2.x.6666]  are hypothetical implementations that either collect (from) or write (into) global matrices and vectors. 

- If we assume a Cartesian mesh in two space dimensions, first-order polynomial space  [2.x.6667] , and that  [2.x.6668]  is an interior node (i.e.  [2.x.6669]  is not on the boundary of the domain) then:  [2.x.6670]  should contain nine state vector elements (i.e. all the states in the patch/macro element associated to the shape function  [2.x.6671] ). This is one of the major differences with the usual cell-based loop where the gather functionality (encoded in FEValuesBase<dim, spacedim>.get_function_values() in the case of deal.II) only collects values for the local cell (just a subset of the patch). 

The actual implementation will deviate from above code in one key aspect: the time-step size  [2.x.6672]  has to be chosen subject to a CFL condition 

[1.x.2594] 

where  [2.x.6673]  is a chosen constant. This will require to compute all  [2.x.6674]  in a separate step prior to actually performing above update. The core principle remains unchanged, though: we do not loop over cells but rather over all edges of the sparsity graph. 

 [2.x.6675]  It is not uncommon to encounter such fully-algebraic schemes (i.e. no bilinear forms, no cell loops, and no quadrature) outside of the finite element community in the wider CFD community. There is a rich history of application of this kind of schemes, also called [1.x.2595] or [1.x.2596] finite element schemes (see for instance  [2.x.6676]  for a historical overview). However, it is important to highlight that the algebraic structure of the scheme (presented in this tutorial) and the node-loops are not just a performance gimmick. Actually, the structure of this scheme was born out of theoretical necessity: the proof of pointwise stability of the scheme hinges on the specific algebraic structure of the scheme. In addition, it is not possible to compute the algebraic viscosities  [2.x.6677]  using cell-loops since they depend nonlinearly on information that spans more than one cell (superposition does not hold: adding contributions from separate cells does not lead to the right result). 

[1.x.2597] 

In the example considered in this tutorial step we use three different types of boundary conditions: essential-like boundary conditions (we prescribe a state at the left boundary of our domain), outflow boundary conditions (also called "do-nothing" boundary conditions) at the right boundary of the domain, and "reflecting" boundary conditions  [2.x.6678]  (also called "slip" boundary conditions) at the top, bottom, and surface of the obstacle. We will not discuss much about essential and "do-nothing" boundary conditions since their implementation is relatively easy and the reader will be able to pick-up the implementation directly from the (documented) source code. In this portion of the introduction we will focus only on the "reflecting" boundary conditions which are somewhat more tricky. 

 [2.x.6679]  At the time of this writing (early 2020) it is not unreasonable to say that both analysis and implementation of stable boundary conditions for hyperbolic systems of conservation laws is an open issue. For the case of variational formulations, stable boundary conditions are those leading to a well-posed (coercive) bilinear form. But for general hyperbolic systems of conservation laws (and for the algebraic formulation used in this tutorial) coercivity has no applicability and/or meaning as a notion of stability. In this tutorial step we will use preservation of the invariant set as our main notion of stability which (at the very least) guarantees well-posedness of the discrete problem. 

For the case of the reflecting boundary conditions we will proceed as follows: 

- For every time step advance in time satisfying no boundary condition at all. 

- Let  [2.x.6680]  be the portion of the boundary where we want to   enforce reflecting boundary conditions. At the end of the time step we enforce   reflecting boundary conditions strongly in a post-processing step where we   execute the projection     [1.x.2598] 

  that removes the normal component of  [2.x.6681] . This is a somewhat   naive idea that preserves a few fundamental properties of the PDE as we   explain below. 

This is approach is usually called "explicit treatment of boundary conditions". The well seasoned finite element person might find this approach questionable. No doubt, when solving parabolic, or elliptic equations, we typically enforce essential (Dirichlet-like) boundary conditions by making them part of the approximation space  [2.x.6682] , and treat natural (e.g. Neumann) boundary conditions as part of the variational formulation. We also know that explicit treatment of boundary conditions (in the context of parabolic PDEs) almost surely leads to catastrophic consequences. However, in the context of nonlinear hyperbolic equations we have that: 

- It is relatively easy to prove that (for the case of reflecting boundary conditions) explicit treatment of boundary conditions is not only conservative but also guarantees preservation of the property  [2.x.6683]  for all  [2.x.6684]  (well-posedness). This is perhaps the most important reason to use explicit enforcement of boundary conditions. 

- To the best of our knowledge: we are not aware of any mathematical result proving that it is possible to guarantee the property  [2.x.6685]  for all  [2.x.6686]  when using either direct enforcement of boundary conditions into the approximation space, or weak enforcement using the Nitsche penalty method (which is for example widely used in discontinuous Galerkin schemes). In addition, some of these traditional ideas lead to quite restrictive time step constraints. 

- There is enough numerical evidence suggesting that explicit treatment of Dirichlet-like boundary conditions is stable under CFL conditions and does not introduce any loss in accuracy. 

If  [2.x.6687]  represents Euler's equation with reflecting boundary conditions on the entirety of the boundary (i.e.  [2.x.6688] ) and we integrate in space and time  [2.x.6689]  we would obtain 

[1.x.2599] 

Note that momentum is NOT a conserved quantity (interaction with walls leads to momentum gain/loss): however  [2.x.6690]  has to satisfy a momentum balance. Even though we will not use reflecting boundary conditions in the entirety of the domain, we would like to know that our implementation of reflecting boundary conditions is consistent with the conservation properties mentioned above. In particular, if we use the projection  [2.x.6691]  in the entirety of the domain the following discrete mass-balance can be guaranteed: 

[1.x.2600] 

where  [2.x.6692]  is the pressure at the nodes that lie at the boundary. Clearly  [2.x.6693]  is the discrete counterpart of  [2.x.6694] . The proof of identity  [2.x.6695]  is omitted, but we briefly mention that it hinges on the definition of the [1.x.2601]  [2.x.6696]  provided in  [2.x.6697] . We also note that this enforcement of reflecting boundary conditions is different from the one originally advanced in  [2.x.6698] . 


examples/step-69/doc/results.dox 

[1.x.2602] 

[1.x.2603] 

Running the program with default parameters in release mode takes about 1 minute on a 4 core machine (with hyperthreading): 

[1.x.2604] 



One thing that becomes evident is the fact that the program spends two thirds of the execution time computing the graph viscosity d_ij and about a third of the execution time in performing the update, where computing the flux  [2.x.6699]  is the expensive operation. The preset default resolution is about 37k gridpoints, which amounts to about 148k spatial degrees of freedom in 2D. An animated schlieren plot of the solution looks as follows: 

 [2.x.6700]  

It is evident that 37k gridpoints for the first-order method is nowhere near the resolution needed to resolve any flow features. For comparison, here is a "reference" computation with a second-order method and about 9.5M gridpoints ([1.x.2605]): 

 [2.x.6701]  

So, we give the first-order method a second chance and run it with about 2.4M gridpoints on a small compute server: 

[1.x.2606] 



And with the following result: 

 [2.x.6702]  

That's substantially better, although of course at the price of having run the code for roughly 2 hours on 16 cores. 




[1.x.2607] 

[1.x.2608] 

The program showcased here is really only first-order accurate, as discussed above. The pictures above illustrate how much diffusion that introduces and how far the solution is from one that actually resolves the features we care about. 

This can be fixed, but it would exceed what a *tutorial* is about. Nevertheless, it is worth showing what one can achieve by adding a second-order scheme. For example, here is a video computed with [1.x.2609] that shows (with a different color scheme) a 2d simulation that corresponds to the cases shown above: 

[1.x.2610] 



This simulation was done with 38 million degrees of freedom (continuous  [2.x.6703]  finite elements) per component of the solution vector. The exquisite detail of the solution is remarkable for these kinds of simulations, including in the sub-sonic region behind the obstacle. 

One can also with relative ease further extend this to the 3d case: 

[1.x.2611] 



Solving this becomes expensive, however: The simulation was done with 1,817 million degrees of freedom (continuous  [2.x.6704]  finite elements) per component (for a total of 9.09 billion spatial degrees of freedom) and ran on 30,720 MPI ranks. The code achieved an average througput of 969M grid points per second (0.04M gridpoints per second per CPU). The front and back wall show a "Schlieren plot": the magnitude of the gradient of the density on an exponential scale from white (low) to black (high). All other cutplanes and the surface of the obstacle show the magnitude of the vorticity on a white (low) - yellow (medium) - red (high) scale. (The scales of the individual cutplanes have been adjusted for a nicer visualization.) 


examples/step-7/doc/intro.dox 

[1.x.2612] 

[1.x.2613] 

In this program, we will mainly consider two aspects: <ol>    [2.x.6705]  Verification of correctness of the program and generation of convergence   tables;    [2.x.6706]  Non-homogeneous Neumann boundary conditions for the Helmholtz equation.  [2.x.6707]  Besides these topics, again a variety of improvements and tricks will be shown. 




[1.x.2614] 

There has probably never been a non-trivial finite element program that worked right from the start. It is therefore necessary to find ways to verify whether a computed solution is correct or not. Usually, this is done by choosing the set-up of a simulation in such a way that we know the exact continuous solution and evaluate the difference between continuous and computed discrete solution. If this difference converges to zero with the right order of convergence, this is already a good indication of correctness, although there may be other sources of error persisting which have only a small contribution to the total error or are of higher order. In the context of finite element simulations, this technique of picking the solution by choosing appropriate right hand sides and boundary conditions is often called the [1.x.2615]. 

In this example, we will not go into the theories of systematic software verification which is a very complicated problem. Rather we will demonstrate the tools which deal.II can offer in this respect. This is basically centered around the functionality of a single function,  [2.x.6708]  This function computes the difference between a given continuous function and a finite element field in various norms on each cell. Of course, like with any other integral, we can only evaluate these norms using quadrature formulas; the choice of the right quadrature formula is therefore crucial to the accurate evaluation of the error. This holds in particular for the  [2.x.6709]  norm, where we evaluate the maximal deviation of numerical and exact solution only at the quadrature points; one should then not try to use a quadrature rule whose evaluation occurs only at points where [super-convergence](https://en.wikipedia.org/wiki/Superconvergence) might occur, such as the Gauss points of the lowest-order Gauss quadrature formula for which the integrals in the assembly of the matrix is correct (e.g., for linear elements, do not use the QGauss(2) quadrature formula). In fact, this is generally good advice also for the other norms: if your quadrature points are fortuitously chosen at locations where the error happens to be particularly small due to superconvergence, the computed error will look like it is much smaller than it really is and may even suggest a higher convergence order. Consequently, we will choose a different quadrature formula for the integration of these error norms than for the assembly of the linear system. 

The function  [2.x.6710]  evaluates the desired norm on each cell  [2.x.6711]  of the triangulation and returns a vector which holds these values for each cell. From the local values, we can then obtain the global error. For example, if the vector  [2.x.6712]  with element  [2.x.6713]  for all cells  [2.x.6714]  contains the local  [2.x.6715]  norms  [2.x.6716] , then 

[1.x.2616] 

is the global  [2.x.6717]  error  [2.x.6718] . 

In the program, we will show how to evaluate and use these quantities, and we will monitor their values under mesh refinement. Of course, we have to choose the problem at hand such that we can explicitly state the solution and its derivatives, but since we want to evaluate the correctness of the program, this is only reasonable. If we know that the program produces the correct solution for one (or, if one wants to be really sure: many) specifically chosen right hand sides, we can be rather confident that it will also compute the correct solution for problems where we don't know the exact values. 

In addition to simply computing these quantities, we will show how to generate nicely formatted tables from the data generated by this program that automatically computes convergence rates etc. In addition, we will compare different strategies for mesh refinement. 




[1.x.2617] 

The second, totally unrelated, subject of this example program is the use of non-homogeneous boundary conditions. These are included into the variational form using boundary integrals which we have to evaluate numerically when assembling the right hand side vector. 

Before we go into programming, let's have a brief look at the mathematical formulation. The equation that we want to solve here is the Helmholtz equation "with the nice sign": 

[1.x.2618] 

on the square  [2.x.6719]  with  [2.x.6720] , augmented by Dirichlet boundary conditions 

[1.x.2619] 

on some part  [2.x.6721]  of the boundary  [2.x.6722] , and Neumann conditions 

[1.x.2620] 

on the rest  [2.x.6723] . In our particular testcase, we will use  [2.x.6724] . (We say that this equation has the "nice sign" because the operator  [2.x.6725]  with the identity  [2.x.6726]  and  [2.x.6727]  is a positive definite operator; the [1.x.2621] is  [2.x.6728]  and results from modeling time-harmonic processes. The operator is not positive definite if  [2.x.6729]  is large, and this leads to all sorts of issues we need not discuss here. The operator may also not be invertible -- i.e., the equation does not have a unique solution -- if  [2.x.6730]  happens to be one of the eigenvalues of  [2.x.6731] .) 

Because we want to verify the convergence of our numerical solution  [2.x.6732] , we want a setup so that we know the exact solution  [2.x.6733] . This is where the Method of Manufactured Solutions comes in. To this end, let us choose a function 

[1.x.2622] 

where the centers  [2.x.6734]  of the exponentials are    [2.x.6735] ,    [2.x.6736] , and    [2.x.6737] , and the half width is set to  [2.x.6738] . The method of manufactured solution then says: choose 

[1.x.2623] 

With this particular choice, we infer that of course the solution of the original problem happens to be  [2.x.6739] . In other words, by choosing the right hand sides of the equation and the boundary conditions in a particular way, we have manufactured ourselves a problem to which we know the solution. This allows us then to compute the error of our numerical solution. In the code below, we represent  [2.x.6740]  by the  [2.x.6741]  class, and other classes will be used to denote  [2.x.6742]  and  [2.x.6743] . 

Using the above definitions, we can state the weak formulation of the equation, which reads: find  [2.x.6744]  such that 

[1.x.2624] 

for all test functions  [2.x.6745] . The boundary term  [2.x.6746]  has appeared by integration by parts and using  [2.x.6747]  on  [2.x.6748]  and  [2.x.6749]  on  [2.x.6750] . The cell matrices and vectors which we use to build the global matrices and right hand side vectors in the discrete formulation therefore look like this: 

[1.x.2625] 

Since the generation of the domain integrals has been shown in previous examples several times, only the generation of the contour integral is of interest here. It basically works along the following lines: for domain integrals we have the  [2.x.6751]  class that provides values and gradients of the shape values, as well as Jacobian determinants and other information and specified quadrature points in the cell; likewise, there is a class  [2.x.6752]  that performs these tasks for integrations on faces of cells. One provides it with a quadrature formula for a manifold with dimension one less than the dimension of the domain is, and the cell and the number of its face on which we want to perform the integration. The class will then compute the values, gradients, normal vectors, weights, etc. at the quadrature points on this face, which we can then use in the same way as for the domain integrals. The details of how this is done are shown in the following program. 




[1.x.2626] 

Besides the mathematical topics outlined above, we also want to use this program to illustrate one aspect of good programming practice, namely the use of namespaces. In programming the deal.II library, we have take great care not to use names for classes and global functions that are overly generic, say  [2.x.6753]  etc. Furthermore, we have put everything into namespace  [2.x.6754] . But when one writes application programs that aren't meant for others to use, one doesn't always pay this much attention. If you follow the programming style of step-1 through step-6, these functions then end up in the global namespace where, unfortunately, a lot of other stuff also lives (basically everything the C language provides, along with everything you get from the operating system through header files). To make things a bit worse, the designers of the C language were also not always careful in avoiding generic names; for example, the symbols <code>j1, jn</code> are defined in C header files (they denote Bessel functions). 

To avoid the problems that result if names of different functions or variables collide (often with confusing error messages), it is good practice to put everything you do into a [1.x.2627]. Following this style, we will open a namespace  [2.x.6755]  at the top of the program, import the deal.II namespace into it, put everything that's specific to this program (with the exception of  [2.x.6756] , which must be in the global namespace) into it, and only close it at the bottom of the file. In other words, the structure of the program is of the kind 

[1.x.2628] 

We will follow this scheme throughout the remainder of the deal.II tutorial. 


examples/step-7/doc/results.dox 



[1.x.2629] 


The program generates two kinds of output. The first are the output files  [2.x.6757] ,  [2.x.6758] , and  [2.x.6759] . We show the latter in a 3d view here: 


 [2.x.6760]  





Secondly, the program writes tables not only to disk, but also to the screen while running. The output looks like the following (recall that columns labeled as " [2.x.6761] " actually show the  [2.x.6762]  [1.x.2630]norm of the error, not the full  [2.x.6763]  norm): 




[1.x.2631] 




One can see the error reduction upon grid refinement, and for the cases where global refinement was performed, also the convergence rates can be seen. The linear and quadratic convergence rates of Q1 and Q2 elements in the  [2.x.6764]  semi-norm can clearly be seen, as are the quadratic and cubic rates in the  [2.x.6765]  norm. 





Finally, the program also generated LaTeX versions of the tables (not shown here) that is written into a file in a way so that it could be copy-pasted into a LaTeX document. 




[1.x.2632] 

What we showed above is how to determine the size of the error  [2.x.6766]  in a number of different norms. We did this primarily because we were interested in testing that our solutions *converge*. But from an engineering perspective, the question is often more practical: How fine do I have to make my mesh so that the error is "small enough"? In other words, if in the table above the  [2.x.6767]  semi-norm has been reduced to `4.121e-03`, is this good enough for me to sign the blueprint and declare that our numerical simulation showed that the bridge is strong enough? 

In practice, we are rarely in this situation because I can not typically compare the numerical solution  [2.x.6768]  against the exact solution  [2.x.6769]  in situations that matter -- if I knew  [2.x.6770] , I would not have to compute  [2.x.6771] . But even if I could, the question to ask in general is then: `4.121e-03` *what*? The solution will have physical units, say kg-times-meter-squared, and I'm integrating a function with units square of the above over the domain, and then take the square root. So if the domain is two-dimensional, the units of  [2.x.6772]  are kg-times-meter-cubed. The question is then: Is  [2.x.6773]  kg-times-meter-cubed small? That depends on what you're trying to simulate: If you're an astronomer used to masses measured in solar masses and distances in light years, then yes, this is a fantastically small number. But if you're doing atomic physics, then no: That's not small, and your error is most certainly not sufficiently small; you need a finer mesh. 

In other words, when we look at these sorts of numbers, we generally need to compare against a "scale". One way to do that is to not look at the *absolute* error  [2.x.6774]  in whatever norm, but at the *relative* error  [2.x.6775] . If this ratio is  [2.x.6776] , then you know that *on average*, the difference between  [2.x.6777]  and  [2.x.6778]  is 0.001 per cent -- probably small enough for engineering purposes. 

How do we compute  [2.x.6779] ? We just need to do an integration loop over all cells, quadrature points on these cells, and then sum things up and take the square root at the end. But there is a simpler way often used: You can call 

[1.x.2633] 

which computes  [2.x.6780] . Alternatively, if you're particularly lazy and don't feel like creating the `zero_vector`, you could use that if the mesh is not too coarse, then  [2.x.6781] , and we can compute  [2.x.6782]  by calling 

[1.x.2634] 

In both cases, one then only has to combine the vector of cellwise norms into one global norm as we already do in the program, by calling 

[1.x.2635] 






[1.x.2636] 

[1.x.2637] 

Go ahead and run the program with higher order elements ( [2.x.6783] ,  [2.x.6784] , ...). You will notice that assertions in several parts of the code will trigger (for example in the generation of the filename for the data output). You might have to address these, but it should not be very hard to get the program to work! 

[1.x.2638] 

Is Q1 or Q2 better? What about adaptive versus global refinement? A (somewhat unfair but typical) metric to compare them, is to look at the error as a function of the number of unknowns. 

To see this, create a plot in log-log style with the number of unknowns on the  [2.x.6785]  axis and the  [2.x.6786]  error on the  [2.x.6787]  axis. You can add reference lines for  [2.x.6788]  and  [2.x.6789]  and check that global and adaptive refinement follow those. If one makes the (not completely unreasonable) assumption that with a good linear solver, the computational effort is proportional to the number of unknowns  [2.x.6790] , then it is clear that an error reduction of  [2.x.6791]  is substantially better than a reduction of the form  [2.x.6792] : That is, that adaptive refinement gives us the desired error level with less computational work than if we used global refinement. This is not a particularly surprising conclusion, but it's worth checking these sorts of assumptions in practice. 

Of course, a fairer comparison would be to plot runtime (switch to release mode first!) instead of number of unknowns on the  [2.x.6793]  axis. If you plotted run time against the number of unknowns by timing each refinement step (e.g., using the Timer class), you will notice that the linear solver is not perfect -- its run time grows faster than proportional to the linear system size -- and picking a better linear solver might be appropriate for this kind of comparison. 


examples/step-70/doc/intro.dox 

 [2.x.6794]  

[1.x.2639] 

 [2.x.6795]  




[1.x.2640] 

[1.x.2641] 

In this tutorial we consider a mixing problem in the laminar flow regime. Such problems occur in a wide range of applications ranging from chemical engineering to power generation (e.g. turbomachinery). Mixing problems are particularly hard to solve numerically, because they often involve a container (with fixed boundaries, and possibly complex geometries such as baffles), represented by the domain  [2.x.6796] , and one (or more) immersed and rotating impellers (represented by the domain  [2.x.6797] ). The domain in which we would like to solve the flow equations is the (time dependent) difference between the two domains, namely:  [2.x.6798] . 

For rotating impellers, the use of Arbitrary Lagrangian Eulerian formulations (in which the fluid domain -- along with the mesh! -- is smoothly deformed to follow the deformations of the immersed solid) is not possible, unless only small times (i.e., small fluid domain deformations) are considered. If one wants to track the evolution of the flow across multiple rotations of the impellers, the resulting deformed grid would simply be too distorted to be useful. 

In this case, a viable alternative strategy would be to use non-matching methods (similarly to what we have done in step-60), where a background fixed grid (that may or may not be locally refined in time to better capture the solid motion) is coupled with a rotating, independent, grid. 

In order to maintain the same notations used in step-60, we use  [2.x.6799]  to denote the domain in  [2.x.6800]  representing the container of both the fluid and the impeller, and we use  [2.x.6801]  in  [2.x.6802]  to denote either the full impeller (when its `spacedim` measure is non-negligible, i.e., when we can represent it as a grid of dimension `dim` equal to `spacedim`), a co-dimension one representation of a thin impeller, or just the boundary of the full impeller. 

The domain  [2.x.6803]  is embedded in  [2.x.6804]  ( [2.x.6805] ) and it is non-matching: It does not, in general, align with any of the features of the volume mesh. We solve a partial differential equation on  [2.x.6806] , enforcing some conditions on the solution of the problem on the embedded domain  [2.x.6807]  by some penalization techniques. In the current case, the condition is that the velocity of the fluid at points on  [2.x.6808]  equal the velocity of the solid impeller at that point. 

The technique we describe here is presented in the literature using one of many names: the [1.x.2642] and the [1.x.2643] among others.  The main principle is that the discretization of the two grids are kept completely independent. In the present tutorial, this approach is used to solve for the motion of a viscous fluid, described by the Stokes equation, that is agitated by a rigid non-deformable impeller. 

Thus, the equations solved in  [2.x.6809]  are the Stokes equations for a creeping flow (i.e. a flow where  [2.x.6810] ) and a no-slip boundary condition is applied on the moving *embedded domain*  [2.x.6811]  associated with the impeller. However, this tutorial could be readily extended to other equations (e.g. the Navier-Stokes equations, linear elasticity equation, etc.). It can be seen as a natural extension of step-60 that enables the solution of large problems using a distributed parallel computing architecture via MPI. 

However, contrary to step-60, the Dirichlet boundary conditions on  [2.x.6812]  are imposed weakly instead of through the use of Lagrange multipliers, and we concentrate on dealing with the coupling of two fully distributed triangulations (a combination that was not possible in the implementation of step-60). 

There are two interesting scenarios that occur when one wants to enforce conditions on the embedded domain  [2.x.6813] : 

- The geometrical dimension `dim` of the embedded domain  [2.x.6814]  is the same of the domain  [2.x.6815]  (`spacedim`), that is, the spacedim-dimensional measure of  [2.x.6816]  is not zero. In this case, the imposition of the Dirichlet boundary boundary condition on  [2.x.6817]  is done through a volumetric penalization. If the applied penalization only depends on the velocity, this is often referred to as  [2.x.6818]  penalization whereas if the penalization depends on both the velocity and its gradient, it is an  [2.x.6819]  penalization. The case of the  [2.x.6820]  penalization is very similar to a Darcy-type approach. Both  [2.x.6821]  and  [2.x.6822]  penalizations have been analyzed extensively (see, for example,  [2.x.6823] ). 

- The embedded domain  [2.x.6824]  has an intrinsic dimension `dim` which is smaller than that of  [2.x.6825]  (`spacedim`), thus its spacedim-dimensional measure is zero; for example it is a curve embedded in a two dimensional domain, or a surface embedded in a three-dimensional domain. This is of course physically impossible, but one may consider very thin sheets of metal moving in a fluid as essentially lower-dimensional if the thickness of the sheet is negligible. In this case, the boundary condition is imposed weakly on  [2.x.6826]  by applying the [1.x.2644] method (see  [2.x.6827] ). 

Both approaches have very similar requirements and result in highly similar formulations. Thus, we treat them almost in the same way. 

In this tutorial program we are not interested in further details on  [2.x.6828] : we assume that the dimension of the embedded domain (`dim`) is always smaller by one or equal with respect to the dimension of the embedding domain  [2.x.6829]  (`spacedim`). 

We are going to solve the following differential problem: given a sufficiently regular function  [2.x.6830]  on  [2.x.6831] , find the solution  [2.x.6832]  to 

[1.x.2645] 



This equation, which we have normalized by scaling the time units in such a way that the viscosity has a numerical value of 1, describes slow, viscous flow such as honey or lava. The main goal of this tutorial is to show how to impose the velocity field condition  [2.x.6833]  on a non-matching  [2.x.6834]  in a weak way, using a penalization method. A more extensive discussion of the Stokes problem including body forces, different boundary conditions, and solution strategies can be found in step-22. 

Let us start by considering the Stokes problem alone, in the entire domain  [2.x.6835] . We look for a velocity field  [2.x.6836]  and a pressure field  [2.x.6837]  that satisfy the Stokes equations with homogeneous boundary conditions on  [2.x.6838] . 

The weak form of the Stokes equations is obtained by first writing it in vector form as 

[1.x.2646] 

forming the dot product from the left with a vector-valued test function  [2.x.6839] , and integrating over the domain  [2.x.6840] , yielding the following set of equations: 

[1.x.2647] 

which has to hold for all test functions  [2.x.6841] . 


Integrating by parts and exploiting the boundary conditions on  [2.x.6842] , we obtain the following variational problem: 

[1.x.2648] 



where  [2.x.6843]  represents the  [2.x.6844]  scalar product. This is the same variational form used in step-22. 

This variational formulation does not take into account the embedded domain. Contrary to step-60, we do not enforce strongly the constraints of  [2.x.6845]  on  [2.x.6846] , but enforce them weakly via a penalization term. 

The analysis of this weak imposition of the boundary condition depends on the spacedim-dimensional measure of  [2.x.6847]  as either positive (if `dim` is equal to `spacedim`) or zero (if `dim` is smaller than `spacedim`). We discuss both scenarios. 




[1.x.2649] 

In this case, we assume that  [2.x.6848]  is the boundary of the actual impeller, that is, a closed curve embedded in a two-dimensional domain or a closed surface in a three-dimensional domain. The idea of this method starts by considering a weak imposition of the Dirichlet boundary condition on  [2.x.6849] , following the Nitsche method. This is achieved by using the following modified formulation on the fluid domain, where no strong conditions on the test functions on  [2.x.6850]  are imposed: 

[1.x.2650] 



The integrals over  [2.x.6851]  are lower-dimensional integrals. It can be shown (see  [2.x.6852] ) that there exists a positive constant  [2.x.6853]  so that if  [2.x.6854] , the weak imposition of the boundary will be consistent and stable. The first two additional integrals on  [2.x.6855]  (the second line in the equation above) appear naturally after integrating by parts, when one does not assume that  [2.x.6856]  is zero on  [2.x.6857] . 

The third line in the equation above contains two terms that are added to ensure consistency of the weak form, and a stabilization term, that is there to enforce the boundary condition with an error which is consistent with the approximation error. The consistency terms and the stabilization term are added to the right hand side with the actual boundary data  [2.x.6858] . 

When  [2.x.6859]  satisfies the condition  [2.x.6860]  on  [2.x.6861] , all the consistency and stability integrals on  [2.x.6862]  cancel out, and one is left with the usual weak form of Stokes flow, that is, the above formulation is consistent. 

We note that an alternative (non-symmetric) formulation can be used : 

[1.x.2651] 

Note the different sign of the first terms on the third and fourth lines. In this case, the stability and consistency conditions become  [2.x.6863] . In the symmetric case, the value of  [2.x.6864]  is dependent on  [2.x.6865] , and it is in general chosen such that  [2.x.6866]  with  [2.x.6867]  a measure of size of the face being integrated and  [2.x.6868]  a constant such that  [2.x.6869] . This is as one usually does with the Nitsche penalty method to enforcing Dirichlet boundary conditions. 

The non-symmetric approach, on the other hand, is related to how one enforced continuity for the non-symmetric interior penalty method for discontinuous Galerkin methods (the "NIPG" method  [2.x.6870] ). Even if the non-symmetric case seems advantageous w.r.t. possible choices of stabilization parameters, we opt for the symmetric discretization, since in this case it can be shown that the dual problem is also consistent, leading to a solution where not only the energy norm of the solution converges with the correct order, but also its  [2.x.6871]  norm. Furthermore, the resulting matrix remains symmetric. 

The above formulation works under the assumption that the domain is discretized exactly. However, if the deformation of the impeller is a rigid body motion, it is possible to artificially extend the solution of the Stokes problem inside the propeller itself, since a rigid body motion is also a solution to the Stokes problem. The idea is then to solve the same problem, inside  [2.x.6872] , imposing the same boundary conditions on  [2.x.6873] , using the same penalization technique, and testing with test functions  [2.x.6874]  which are globally continuous over  [2.x.6875] . 

This results in the following (intermediate) formulation: 

[1.x.2652] 

where the jump terms, denoted with  [2.x.6876] , are computed with respect to a fixed orientation of the normal vector  [2.x.6877] . The factor of 2 appears in front of  [2.x.6878]  since we see every part of  [2.x.6879]  twice, once from within the fluid and once from within the obstacle moving around in it. (For all of the other integrals over  [2.x.6880] , we visit each part of  [2.x.6881]  twice, but with opposite signs, and consequently get the jump terms.) 

Here we notice that, unlike in discontinuous Galerkin methods, the test and trial functions are continuous across  [2.x.6882] . Moreover, if  [2.x.6883]  is not aligned with cell boundaries, all the jump terms are also zero, since, in general, finite element function spaces are smooth inside each cell, and if  [2.x.6884]  cuts through an element intersecting its boundary only at a finite number of points, all the contributions on  [2.x.6885] , with the exception of the stabilization ones, can be neglected from the formulation, resulting in the following final form of the variational formulation: 

[1.x.2653] 



In step-60, the imposition of the constraint required the addition of new variables in the form of Lagrange multipliers. This is not the case for this tutorial program. The imposition of the boundary condition using Nitsche's method only modifies the system matrix and the right-hand side without adding additional unknowns. However, the velocity vector  [2.x.6886]  on the embedded domain will not match exactly the prescribed velocity  [2.x.6887] , but only up to a numerical error which is in the same order as the interpolation error of the finite element method. Furthermore, as in step-60, we still need to integrate over the non-matching embedded grid in order to construct the boundary term necessary to impose the boundary condition over  [2.x.6888] . 




[1.x.2654] 

In this case,  [2.x.6889]  has the same dimension, but is embedded into  [2.x.6890] . We can think of this as a thick object moving around in the fluid. In the case of  [2.x.6891]  penalization, the additional penalization term can be interpreted as a Darcy term within  [2.x.6892] , resulting in: 

[1.x.2655] 



Here, integrals over  [2.x.6893]  are simply integrals over a part of the volume. The  [2.x.6894]  penalization thus consists in adding a volumetric term that constrains the velocity of the fluid to adhere to the velocity of the rigid body within  [2.x.6895] . Also in this case,  [2.x.6896]  must be chosen sufficiently large in order to ensure that the Dirichlet boundary condition in  [2.x.6897]  is sufficiently respected, but not too high in order to maintain the proper conditioning of the system matrix. 

A  [2.x.6898]  penalization may be constructed in a similar manner, with the addition of a viscous component to the penalization that dampens the velocity gradient within  [2.x.6899] : 

[1.x.2656] 



Notice that the  [2.x.6900]  penalization (`dim` equal to `spacedim`) and the Nitsche penalization (`dim` equal to `spacedim-1`) result in the exact same numerical implementation, thanks to the dimension independent capabilities of deal.II. 




[1.x.2657] 

In this tutorial, both the embedded grid  [2.x.6901]  and the embedding grid are described using a  [2.x.6902]  These two triangulations can be built from functions in the GridGenerator namespace or by reading a mesh file produced with another application (e.g. GMSH, see the discussion in step-49). This is slightly more general than what was previously done in step-60. 

The addition of the immersed boundary method, whether it is in the `dim=spacedim` or `dim<spacedim` case, only introduces additional terms in the system matrix and the right-hand side of the system which result from the integration over  [2.x.6903] . This does not modify the number of variables for which the problem must be solved. The challenge is thus related to the integrals that must be carried over  [2.x.6904] . 

As usual in finite elements we split this integral into contributions from all cells of the triangulation used to discretize  [2.x.6905] , we transform the integral on  [2.x.6906]  to an integral on the reference element  [2.x.6907] , where  [2.x.6908]  is the mapping from  [2.x.6909]  to  [2.x.6910] , and compute the integral on  [2.x.6911]  using a quadrature formula. For example: 

[1.x.2658] 

Computing this sum is non-trivial because we have to evaluate  [2.x.6912] . In general, if  [2.x.6913]  and  [2.x.6914]  are not aligned, the point  [2.x.6915]  is completely arbitrary with respect to  [2.x.6916] , and unless we figure out a way to interpolate all basis functions of  [2.x.6917]  on an arbitrary point on  [2.x.6918] , we cannot compute the integral needed. 


To evaluate  [2.x.6919]  the following steps needs to be taken (as shown in the picture below): 

- For a given cell  [2.x.6920]  in  [2.x.6921]  compute the real point  [2.x.6922] , where  [2.x.6923]  is one of the quadrature points used for the integral on  [2.x.6924] . This is the easy part:  [2.x.6925]  gives us the real-space locations of all quadrature points. 

- Find the cell of  [2.x.6926]  in which  [2.x.6927]  lies. We shall call this element  [2.x.6928] . 

- Find the reference coordinates within  [2.x.6929]  of  [2.x.6930] . For this, we need the inverse of the mapping  [2.x.6931]  that transforms the reference element  [2.x.6932]  into the element  [2.x.6933] :  [2.x.6934] . 

- Evaluate the basis function  [2.x.6935]  of the  [2.x.6936]  mesh at this   point  [2.x.6937] . This is, again, relatively simple using FEValues. 


<p align="center"> <img   src="https://www.dealii.org/images/steps/developer/step-60.C_interpolation.png"   alt="">  [2.x.6938]  

In step-60, the second through fourth steps above were computed by calling, in turn, 

-  [2.x.6939]  followed by 

-  [2.x.6940]  We then 

- construct a custom Quadrature formula, containing the point in the reference  cell and then 

- construct an FEValues object, with the given quadrature formula, and  initialized with the cell obtained in the first step. 

Although this approach could work for the present case, it does not lends itself readily to parallel simulations using distributed triangulations. Indeed, since the position of the quadrature points on the cells of the embedded domain  [2.x.6941]  do not match that of the embedding triangulation and since  [2.x.6942]  is constantly moving, this would require that the triangulation representing  [2.x.6943]  be stored in it's entirety for all of the processors. As the number of processor and the number of cells in  [2.x.6944]  increases, this leads to a severe bottleneck in terms of memory. Consequently, an alternative strategy is sought in this step. 




[1.x.2659] 

Remember that for both the penalization approach ( [2.x.6945]  or  [2.x.6946] ) and the Nitsche method, we want to compute integrals that are approximated by the quadrature. That is, we need to compute 

[1.x.2660] If you followed the discussion above, then you will recall that  [2.x.6947]  and  [2.x.6948]  are shape functions defined on the fluid mesh. The only things defined on the solid mesh are:  [2.x.6949] , which is the location of a quadrature point on a solid cell that is part of  [2.x.6950] ,  [2.x.6951]  is the determinant of its Jacobian, and  [2.x.6952]  the corresponding quadrature weight. 

The important part to realize is now this:  [2.x.6953]  is a property of the quadrature formula and does not change with time. Furthermore, the Jacobian matrix of  [2.x.6954]  itself changes as the solid obstacle moves around in the fluid, but because the solid is considered non-deforming (it only translates and rotates, but doesn't dilate), the determinant of the Jacobian remains constant. As a consequence, the product  [2.x.6955]  (which we typically denote by `JxW`) remains constant for each quadrature point. So the only thing we need keep track of are the positions  [2.x.6956]  -- but these move with the velocity of the solid domain. 

In other words, we don't actually need to keep the solid mesh at all. All we need is the positions  [2.x.6957]  and corresponding `JxW` values. Since both of these properties are point-properties (or point-vectors) that are attached to the solid material, they can be idealized as a set of disconnected infinitesimally small "particles", which carry the required `JxW` information with the movement of the solid. deal.II has the ability to distribute and store such a set of particles in large-scale parallel computations in the form of the ParticleHandler class (for details on the implementation see  [2.x.6958] ), and we will make use of this functionality in this tutorial. 

Thus, the approach taken in this step is as follows: 

- Create a  [2.x.6959]  for the domain  [2.x.6960] ; 

- Create  [2.x.6961]  at the positions of the quadrature points on  [2.x.6962] ; 

- Call the  [2.x.6963]  function,   to distribute the particles across processors, *following the solid   triangulation*; 

- Attach the `JxW` values as a "property" to each  [2.x.6964]  object. 

This structure is relatively expensive to generate, but must only be generated once per simulation. Once the  [2.x.6965]  is generated and the required information is attached to the particle, the integrals over  [2.x.6966]  can be carried out by exploiting the fact that particles are grouped cellwise inside ParticleHandler, allowing us to: 

- Looping over all cells of  [2.x.6967]  that contain at least one particle 

- Looping over all particles in the given cell 

- Compute the integrals and fill the global matrix. 

Since the  [2.x.6968]  can manage the exchange of particles from one processor to the other, the embedded triangulation can be moved or deformed by displacing the particles. The only constraint associated with this displacement is that particles should be displaced by a distance that is no larger than the size of one cell. That's because that is the limit to which  [2.x.6969]  can track which cell a particle that leaves its current cell now resides in. 

Once the entire problem (the Stokes problem and the immersed boundary imposition) is assembled, the final saddle point problem is solved by an iterative solver, applied to the Schur complement  [2.x.6970]  (whose construction is described, for example, in step-22), and we construct  [2.x.6971]  using LinearOperator classes. 




[1.x.2661] 

The problem we solve here is a demonstration of the time-reversibility of Stokes flow. This is often illustrated in science education experiments with a Taylor-Couette flow and dye droplets that revert back to their original shape after the fluid has been displaced in a periodic manner. 

[1.x.2662] 



In the present problem, a very viscous fluid is agitated by the rotation of an impeller, which, in 2D, is modeled by a rectangular grid. The impeller rotates for a given number of revolutions, after which the flow is reversed such that the same number of revolutions is carried out in the opposite direction. We recall that since the Stokes equations are self-adjoint, creeping flows are reversible. Consequently, if the impeller motion is reversed in the opposite direction, the fluid should return to its original position. In the present case, this is illustrated by inserting a circle of passive tracer particles that are advected by the fluid and which return to their original position, thus demonstrating the time-reversibility of the flow. 




[1.x.2663] 

This tutorial program uses a number of techniques on imposing velocity conditions on non-matching interfaces in the interior of the fluid. For more background material, you may want to look up the following references:  [2.x.6972] ,  [2.x.6973] ,  [2.x.6974] ,  [2.x.6975] ,  [2.x.6976] . 


examples/step-70/doc/results.dox 



[1.x.2664] 

The directory in which this program is run contains a number of sample parameter files that you can use to reproduce the results presented in this section. If you do not specify a parameter file as an argument on the command line, the program will try to read the file "`parameters.prm`" by default, and will execute the two dimensional version of the code. As explained in the discussion of the source code, if your file name contains the string "23", then the program will run a three dimensional problem, with immersed solid of co-dimension one. If it contains the string "3", it will run a three dimensional problem, with immersed solid of co-dimension zero, otherwise it will run a two dimensional problem with immersed solid of co-dimension zero. 

Regardless of the specific parameter file name, if the specified file does not exist, when you execute the program you will get an exception that no such file can be found: 

[1.x.2665] 



However, as the error message already states, the code that triggers the exception will also generate the specified file ("`parameters.prm`" in this case) that simply contains the default values for all parameters this program cares about (for the correct dimension and co-dimension, according to the whether a string "23" or "3" is contained in the file name). By inspection of the default parameter file, we see the following: 

[1.x.2666] 



If you now run the program, you will get a file called `parameters_22.prm` in the directory specified by the parameter `Output directory` (which defaults to the current directory) containing a shorter version of the above parameters (without comments and documentation), documenting all parameters that were used to run your program: 

[1.x.2667] 



The rationale behind creating first `parameters.prm` file (the first time the program is run) and then a `output/parameters_22.prm` (every time you run the program with an existing input file), is because you may want to leave most parameters to their default values, and only modify a handful of them, while still beeing able to reproduce the results and inspect what parameters were used for a specific simulation. It is generally good scientific practice to store the parameter file you used for a simulation along with the simulation output so that you can repeat the exact same run at a later time if necessary. 

Another reason is because the input file may only contain those parameters that differ from their defaults. For example, you could use the following (perfectly valid) parameter file with this tutorial program: 

[1.x.2668] 

and you would run the program with Q3/Q2 Taylor-Hood finite elements, for 101 steps, using a Nitsche penalty of `10`, and leaving all the other parameters to their default value. The output directory then contains a record of not just these parameters, but indeed all parameters used in the simulation. You can inspect all the other parameters in the produced file `parameters_22.prm`. 




[1.x.2669] 

The default problem generates a co-dimension zero impeller, consisting of a rotating rectangular grid, where the rotation is for half a time unit in one direction, and half a time unit in the opposite direction, with constant angular velocity equal to  [2.x.6977] . Consequently, the impeller does half a rotation and returns to its original position. The following animation displays the velocity magnitude, the motion of the solid impeller and of the tracer particles. 


<p align="center">    <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-70.2d_tracing.gif"            alt = ""            width="500">     </div>  [2.x.6978]  

On one core, the output of the program will look like the following: 

[1.x.2670] 



You may notice that assembling the coupling system is more expensive than assembling the Stokes part. This depends highly on the number of Gauss points (solid particles) that are used to apply the Nitsche restriction. In the present case, a relatively low number of tracer particles are used. Consequently, tracking their motion is relatively cheap. 

The following movie shows the evolution of the solution over time: 

[1.x.2671] 



The movie shows the rotating obstacle in gray (actually a superposition of the solid particles plotted with large enough dots that they overlap), [1.x.2672] in light colors (including the corner vertices that form at specific times during the simulation), and the tracer particles in bluish tones. 

The simulation shows that at the end time, the tracer particles have somewhat returned to their original position, although they have been distorted by the flow field. The following image compares the initial and the final position of the particles after one time unit of flow. 

<p align="center">    <div class="img" align="center">       <img src="https://www.dealii.org/images/steps/developer/step-70.tracer_comparison.png"            alt = ""            width="500">     </div>  [2.x.6979]  

In this case, we see that the tracer particles that were outside of the swept volume of the impeller have returned very close to their initial position, whereas those in the swept volume were slightly more deformed. This deformation is non-physical. It is caused by the numerical error induced by the explicit Euler scheme used to advect the particles, by the loss of accuracy due to the fictitious domain and, finally, by the discretization error on the Stokes equations. The first two errors are the leading cause of this deformation and they could be alleviated by the use of a finer mesh and a lower time step. 




[1.x.2673] 

To play around a little bit, we complicate the fictitious domain (taken from https://grabcad.com/library/lungstors-blower-1), and run a co-dimension one simulation in three space dimensions, using the following "`parameters_23.prm`" file: 

[1.x.2674] 



In this case, the timing outputs are a bit different: 

[1.x.2675] 



Now, the solver is taking most of the solution time in three dimensions, and the particle motion and Nitsche assembly remain relatively unimportant as far as run time is concerned. 




[1.x.2676] 




[1.x.2677] 

[1.x.2678] 

The current tutorial program shows a one-way coupling between the fluid and the solid, where the solid motion is imposed (and not solved for), and read in the solid domain by exploiting the location and the weights of the solid quadrature points. 

The structure of the code already allows one to implement a two-way coupling, by exploiting the possibility to read values of the fluid velocity on the quadrature points of the solid grid. For this to be more efficient in terms of MPI communication patterns, one should maintain ownership of the quadrature points on the solid processor that owns the cells where they have been created. In the current code, it is sufficient to define the IndexSet of the vectors used to exchange information of the quadrature points by using the solid partition instead of the initial fluid partition. 

This allows the combination of the technique used in this tutorial program with those presented in the tutorial step-60 to solve a fluid structure interaction problem with distributed Lagrange multipliers, on  [2.x.6980]  objects. 

The timings above show that the current preconditioning strategy does not work well for Nitsche penalization, and we should come up with a better preconditioner if we want to aim at larger problems. Moreover, a checkpoint restart strategy should be implemented to allow for longer simulations to be interrupted and restored, as it is done for example in the step-69 tutorial. 


examples/step-71/doc/intro.dox 

 [2.x.6981]  

[1.x.2679] 




[1.x.2680] 

The aim of this tutorial is, quite simply, to introduce the fundamentals of both [automatic](https://en.wikipedia.org/wiki/Automatic_differentiation) and [symbolic differentiation](https://en.wikipedia.org/wiki/Computer_algebra) (respectively abbreviated as AD and SD): Ways in which one can, in source code, describe a function  [2.x.6982]  and automatically also obtain a representation of derivatives  [2.x.6983]  (the "Jacobian"),  [2.x.6984]  (the "Hessian"), etc., without having to write additional lines of code. Doing this is quite helpful in solving nonlinear or optimization problems where one would like to only describe the nonlinear equation or the objective function in the code, without having to also provide their derivatives (which are necessary for a Newton method for solving a nonlinear problem, or for finding a minimizer). 

Since AD and SD tools are somewhat independent of finite elements and boundary value problems, this tutorial is going to be different to the others that you may have read beforehand. It will focus specifically on how these frameworks work and the principles and thinking behind them, and will forgo looking at them in the direct context of a finite element simulation. 

We will, in fact, look at two different sets of problems that have greatly different levels of complexity, but when framed properly hold sufficient similarity that the same AD and SD frameworks can be leveraged. With these examples the aim is to build up an understanding of the steps that are required to use the AD and SD tools, the differences between them, and hopefully identify where they could be immediately be used in order to improve or simplify existing code. 

It's plausible that you're wondering what AD and SD are, in the first place. Well, that question is easy to answer but without context is not very insightful. So we're not going to cover that in this introduction, but will rather defer this until the first introductory example where we lay out the key points as this example unfolds. To complement this, we should mention that the core theory for both frameworks is extensively discussed in the  [2.x.6985]  module, so it bears little repeating here. 

Since we have to pick *some* sufficiently interesting topic to investigate and identify where AD and SD can be used effectively, the main problem that's implemented in the second half of the tutorial is one of modeling a coupled constitutive law, specifically a magneto-active material (with hysteretic effects). As a means of an introduction to that, later in the introduction some grounding theory for that class of materials will be presented. Naturally, this is not a field (or even a class of materials) that is of interest to a wide audience. Therefore, the author wishes to express up front that this theory and any subsequent derivations mustn't be considered the focus of this tutorial. Instead, keep in mind the complexity of the problem that arises from the relatively innocuous description of the constitutive law, and what we might (in the context of a boundary value problem) need to derive from that. We will perform some computations with these constitutive laws at the level of a representative continuum point (so, remaining in the  realm of continuum mechanics), and will produce some benchmark results around which we can frame a final discussion on the topic of computational performance. 

Once we have the foundation upon which we can build further concepts, we will see how AD in particular can be exploited at a finite element (rather than continuum) level: this is a topic that is covered in step-72, as well as step-33. But before then, let's take a moment to think about why we might want to consider using these sorts of tools, and what benefits they can potentially offer you. 




[1.x.2681] 

The primary driver for using AD or SD is typically that there is some situation that requires differentiation to be performed, and that doing so is sufficiently challenging to make the prospect of using an external tool to perform that specific task appealing. A broad categorization for the circumstances under which AD or SD can be rendered most useful include (but are probably not limited to) the following: 

- [1.x.2682] For a new class of problems where you're trying to   implement a solution quickly, and want to remove some of the intricate details   (in terms of both the mathematics as well as the organizational structure of   the code itself). You might be willing to justify any additional computational   cost, which would be offset by an increased agility in restructuring your code   or modifying the part of the problem that is introducing some complex nonlinearity   with minimal effort. 

- [1.x.2683] It could very well be that some problems just happen to have   a nonlinearity that is incredibly challenging to linearize or formulate by hand.   Having this challenge taken care of for you by a tool that is, for the most part,   robust, reliable, and accurate may alleviate some of the pains in implementing   certain problems. Examples of this include step-15, where the   derivative of the nonlinear PDE we solve is not incredibly difficult   to derive, but sufficiently cumbersome that one has to pay attention   in doing so by hand, and where implementing the corresponding finite   element formulation of the Newton step takes more than just the few   lines that it generally takes to implement the bilinear form;   step-33 (where we actually use AD) is an even more extreme example. 

- [1.x.2684] For materials and simulations that exhibit nonlinear response,   an accurate rather than only approximate material tangent (the term mechanical engineers use for   the derivative of a material law) can be the difference between convergent and   divergent behavior, especially at high external (or coupling) loads.   As the complexity of the problem increases, so do the opportunities to introduce   subtle (or, perhaps, not-so-subtle) errors that produce predictably negative   results.   Additionally, there is a lot to be gained by verifying that the implementation is   completely correct. For example, certain categories of problems are known to exhibit   instabilities, and therefore when you start to lose quadratic convergence in a   nonlinear solver (e.g., Newton's method) then this may not be a huge surprise to   the investigator. However, it is hard (if not impossible) to distinguish between   convergence behavior that is produced as you near an unstable solution and when   you simply have an error in the material or finite element linearization, and   start to drift off the optimal convergence path due to that. Having a   method of verifying the correctness of the implementation of a constitutive law   linearization, for example, is perhaps the only meaningful way that you can   use to catch such errors, assuming that you've got nobody else to scrutinize your code.   Thankfully, with some tactical programming it is quite straight-forward to structure   a code for reuse, such that you can use the same classes in production code and   directly verify them in, for instance, a unit-test framework. 

This tutorial program will have two parts: One where we just introduce the basic ideas of automatic and symbolic differentiation support in deal.II using a simple set of examples; and one where we apply this to a realistic but much more complicated case. For that second half, the next section will provide some background on magneto-mechanical materials -- you can skip this section if all you want to learn about is what AD and SD actually are, but you probably want to read over this section if you are interested in how to apply AD and SD for concrete situations. 




[1.x.2685] 

[1.x.2686] 

As a prelude to introducing the coupled magneto-mechanical material law that we'll use to model a magneto-active polymer, we'll start with a very concise summary of the salient thermodynamics to which these constitutive laws must subscribe. The basis for the theory, as summarized here, is described in copious detail by Truesdell and Toupin  [2.x.6986]  and Coleman and Noll  [2.x.6987] , and follows the logic laid out by Holzapfel  [2.x.6988] . 

Starting from the first law of thermodynamics, and following a few technical assumptions, it can be shown the the balance between the kinetic plus internal energy rates and the power supplied to the system from external sources is given by the following relationship that equates the rate of change of the energy in an (arbitrary) volume  [2.x.6989]  on the left, and the sum of forces acting on that volume on the right: 

[1.x.2687] 

Here  [2.x.6990]  represents the total time derivative,  [2.x.6991]  is the material density as measured in the Lagrangian reference frame,  [2.x.6992]  is the material velocity and  [2.x.6993]  its acceleration,  [2.x.6994]  is the internal energy per unit reference volume,  [2.x.6995]  is the total Piola stress tensor and  [2.x.6996]  is the time rate of the deformation gradient tensor,  [2.x.6997]  and  [2.x.6998]  are, respectively, the magnetic field vector and the magnetic induction (or magnetic flux density) vector,  [2.x.6999]  and  [2.x.7000]  are the electric field vector and electric displacement vector, and  [2.x.7001]  and  [2.x.7002]  represent the referential thermal flux vector and thermal source. The material differential operator  [2.x.7003]  where  [2.x.7004]  is the material position vector. With some rearrangement of terms, invoking the arbitrariness of the integration volume  [2.x.7005] , the total internal energy density rate  [2.x.7006]  can be identified as 

[1.x.2688] 

The total internal energy includes contributions that arise not only due to mechanical deformation (the first term), and thermal fluxes and sources (the fourth and fifth terms), but also due to the intrinsic energy stored in the magnetic and electric fields themselves (the second and third terms, respectively). 

The second law of thermodynamics, known also as the entropy inequality principle, informs us that certain thermodynamic processes are irreversible. After accounting for the total entropy and rate of entropy input, the Clausius-Duhem inequality can be derived. In local form (and in the material configuration), this reads 

[1.x.2689] 

The quantity  [2.x.7007]  is the absolute temperature, and  [2.x.7008]  represents the entropy per unit reference volume. 

Using this to replace  [2.x.7009]  in the result stemming from the first law of thermodynamics, we now have the relation 

[1.x.2690] 

On the basis of Fourier's law, which informs us that heat flows from regions of high temperature to low temperature, the last term is always positive and can be ignored. This renders the local dissipation inequality 

[1.x.2691] 

It is postulated  [2.x.7010]  that the Legendre transformation 

[1.x.2692] 

from which we may define the free energy density function  [2.x.7011]  with the stated parameterization, exists and is valid. Taking the material rate of this equation and substituting it into the local dissipation inequality results in the generic expression 

[1.x.2693] 

Under the assumption of isothermal conditions, and that the electric field does not excite the material in a manner that is considered non-negligible, then this dissipation inequality reduces to 

[1.x.2694] 



[1.x.2695] 

When considering materials that exhibit mechanically dissipative behavior, it can be shown that this can be captured within the dissipation inequality through the augmentation of the material free energy density function with additional parameters that represent internal variables  [2.x.7012] . Consequently, we write it as 

[1.x.2696] 

where  [2.x.7013]  represents the internal variable (which acts like a measure of the deformation gradient) associated with the `i`th mechanical dissipative (viscous) mechanism. As can be inferred from its parameterization, each of these internal parameters is considered to evolve in time. Currently the free energy density function  [2.x.7014]  is parameterized in terms of the magnetic induction  [2.x.7015] . This is the natural parameterization that comes as a consequence of the considered balance laws. Should such a class of materials to be incorporated within a finite-element model, it would be ascertained that a certain formulation of the magnetic problem, known as the magnetic vector potential formulation, would need to be adopted. This has its own set of challenges, so where possible the more simple magnetic scalar potential formulation may be preferred. In that case, the magnetic problem needs to be parameterized in terms of the magnetic field  [2.x.7016] . To make this re-parameterization, we execute a final Legendre transformation 

[1.x.2697] 

At the same time, we may take advantage of the principle of material frame indifference in order to express the energy density function in terms of symmetric deformation measures: 

[1.x.2698] 

The upshot of these two transformations (leaving out considerable explicit and hidden details) renders the final expression for the reduced dissipation inequality as 

[1.x.2699] 

(Notice the sign change on the second term on the right hand side, and the transfer of the time derivative to the magnetic induction vector.) The stress quantity  [2.x.7017]  is known as the total Piola-Kirchhoff stress tensor and its energy conjugate  [2.x.7018]  is the right Cauchy-Green deformation tensor, and  [2.x.7019]  is the re-parameterized internal variable associated with the `i`th mechanical dissipative (viscous) mechanism. 

Expansion of the material rate of the energy density function, and rearrangement of the various terms, results in the expression 

[1.x.2700] 

At this point, its worth noting the use of the [partial derivatives](https://en.wikipedia.org/wiki/Partial_derivative)  [2.x.7020] . This is an important detail that will be fundamental to a certain design choice made within the tutorial. As brief reminder of what this signifies, the partial derivative of a multi-variate function returns the derivative of that function with respect to one of those variables while holding the others constant: 

[1.x.2701] 

More specific to what's encoded in the dissipation inequality (with the very general free energy density function  [2.x.7021]  with its parameterization yet to be formalized), if one of the input variables is a function of another, it is also held constant and the chain rule does not propagate any further, while the computing total derivative would imply judicious use of the chain rule. This can be better understood by comparing the following two statements: 

[1.x.2702] 



Returning to the thermodynamics of the problem, we next exploit the arbitrariness of the quantities  [2.x.7022]  and  [2.x.7023] , by application of the Coleman-Noll procedure  [2.x.7024] ,  [2.x.7025] . This leads to the identification of the kinetic conjugate quantities 

[1.x.2703] 

(Again, note the use of the partial derivatives to define the stress and magnetic induction in this generalized setting.) From what terms remain in the dissipative power (namely those related to the mechanical dissipative mechanisms), if they are assumed to be independent of one another then, for each mechanism `i`, 

[1.x.2704] 

This constraint must be satisfied through the appropriate choice of free energy function, as well as a carefully considered evolution law for the internal variables. 

In the case that there are no dissipative mechanisms to be captured within the constitutive model (e.g., if the material to be modelled is magneto-hyperelastic) then the free energy density function  [2.x.7026]  reduces to a stored energy density function, and the total stress and magnetic induction can be simplified 

[1.x.2705] 

where the operator  [2.x.7027]  denotes the total derivative operation. 

For completeness, the linearization of the stress tensor and magnetic induction are captured within the fourth-order total referential elastic tangent tensor  [2.x.7028] , the second-order magnetostatic tangent tensor  [2.x.7029]  and the third-order total referential magnetoelastic coupling tensor  [2.x.7030] . Irrespective of the parameterization of  [2.x.7031]  and  [2.x.7032] , these quantities may be computed by 

[1.x.2706] 

For the case of rate-dependent materials, this expands to 

[1.x.2707] 

while for rate-independent materials the linearizations are 

[1.x.2708] 

The subtle difference between them is the application of a partial derivative during the calculation of the first derivatives. We'll see later how this affects the choice of AD versus SD for this specific application. For now, we'll simply introduce the two specific materials that are implemented within this tutorial. 

[1.x.2709] 

The first material that we'll consider is one that is governed by a magneto-hyperelastic constitutive law. This material responds to both deformation as well as immersion in a magnetic field, but exhibits no time- or history-dependent behavior (such as dissipation through viscous damping or magnetic hysteresis, etc.). The *stored energy density function* for such a material is only parameterized in terms of the (current) field variables, but not their time derivatives or past values. 

We'll choose the energy density function, which captures both the energy stored in the material due to deformation and magnetization, as well as the energy stored in the magnetic field itself, to be 

[1.x.2710] 

with 

[1.x.2711] 

and for which the variable  [2.x.7033]  ( [2.x.7034]  being the rank-2 identity tensor) represents the spatial dimension and  [2.x.7035]  is the deformation gradient tensor. To give some brief background to the various components of  [2.x.7036] , the first two terms bear a great resemblance to the stored energy density function for a (hyperelastic) Neohookean material. The only difference between what's used here and the Neohookean material is the scaling of the elastic shear modulus by the magnetic field-sensitive saturation function  [2.x.7037]  (see  [2.x.7038] , equation 29). This function will, in effect, cause the material to stiffen in the presence of a strong magnetic field. As it is governed by a sigmoid-type function, the shear modulus will asymptotically converge on the specified saturation shear modulus. It can also be shown that the last term in  [2.x.7039]  is the stored energy density function for magnetic field (as derived from first principles), scaled by the relative permeability constant. This definition collectively implies that the material is linearly magnetized, i.e., the magnetization vector and magnetic field vector are aligned. (This is certainly not obvious with the magnetic energy stated in its current form, but when the magnetic induction and magnetization are derived from  [2.x.7040]  and all magnetic fields are expressed in the  [2.x.7041] current configuration [2.x.7042]  then this correlation becomes clear.) As for the specifics of what the magnetic induction, stress tensor, and the various material tangents look like, we'll defer presenting these to the tutorial body where the full, unassisted implementation of the constitutive law is defined. 

[1.x.2712] 

The second material that we'll formulate is one for a magneto-viscoelastic material with a single dissipative mechanism `i`. The *free energy density function* that we'll be considering is defined as 

[1.x.2713] 

with 

[1.x.2714] 



[1.x.2715] 

and the evolution law 

[1.x.2716] 

for the internal viscous variable. We've chosen the magnetoelastic part of the energy  [2.x.7043]  to match that of the first material model that we explored, so this part needs no further explanation. As for the viscous part  [2.x.7044] , this component of the free energy (in conjunction with the evolution law for the viscous deformation tensor) is taken from  [2.x.7045]  (with the additional scaling by the viscous saturation function described in  [2.x.7046] ). It is derived in a thermodynamically consistent framework that, at its core, models the movement of polymer chains on a micro-scale level. 

To proceed beyond this point, we'll also need to consider the time discretization of the evolution law. Choosing the implicit first-order backwards difference scheme, then 

[1.x.2717] 

where the superscript  [2.x.7047]  denotes that the quantity is taken at the current timestep, and  [2.x.7048]  denotes quantities taken at the previous timestep (i.e., a history variable). The timestep size  [2.x.7049]  is the difference between the current time and that of the previous timestep. Rearranging the terms so that all internal variable quantities at the current time are on the left hand side of the equation, we get 

[1.x.2718] 

that matches  [2.x.7050]  equation 54. 

[1.x.2719] 

Now that we have shown all of these formulas for the thermodynamics and theory governing magneto-mechanics and constitutive models, let us outline what the program will do with all of this. We wish to do something *meaningful* with the materials laws that we've formulated, and so it makes sense to subject them to some mechanical and magnetic loading conditions that are, in some way, representative of some conditions that might be found either in an application or in a laboratory setting. One way to achieve that aim would be to embed these constitutive laws in a finite element model to simulate a device. In this instance, though, we'll keep things simple (we are focusing on the automatic and symbolic differentiation concepts, after all) and will find a concise way to faithfully replicate an industry-standard rheological experiment using an analytical expression for the loading conditions. 

The rheological experiment that we'll reproduce, which idealizes a laboratory experiment that was used to characterize magneto-active polymers, is detailed in  [2.x.7051]  (as well as  [2.x.7052] , in which it is documented along with the real-world experiments). The images below provide a visual description of the problem set up. 

 [2.x.7053]  

Under the assumptions that an incompressible medium is being tested, and that the deformation profile through the sample thickness is linear, then the displacement at some measurement point  [2.x.7054]  within the sample, expressed in radial coordinates, is 

[1.x.2720] 

where  [2.x.7055]  and  [2.x.7056]  are the radius at 

-- and angle of -- the sampling point,  [2.x.7057]  is the (constant) axial deformation,  [2.x.7058]  is the time-dependent torsion angle per unit length that will be prescribed using a sinusoidally repeating oscillation of fixed amplitude  [2.x.7059] . The magnetic field is aligned axially, i.e., in the  [2.x.7060]  direction. 

This summarizes everything that we need to fully characterize the idealized loading at any point within the rheological sample. We'll set up the problem in such a way that we "pick" a representative point with this sample, and subject it to a harmonic shear deformation at a constant axial deformation (by default, a compressive load) and a constant, axially applied magnetic field. We will record the stress and magnetic induction at this point, and will output that data to file for post-processing. Although its not necessary for this particular problem, we will also be computing the tangents as well. Even though they are not directly used in this particular piece of work, these second derivatives are needed to embed the constitutive law within a finite element model (one possible extension to this work). We'll therefore take the opportunity to check our hand calculations for correctness using the assisted differentiation frameworks. 

[1.x.2721] 

In addition to the already mentioned  [2.x.7061]  module, the following are a few references that discuss in more detail 

- magneto-mechanics, and some aspects of automated differentiation frameworks:  [2.x.7062] ,  [2.x.7063] , and 

- the automation of finite element frameworks using AD and/or SD:  [2.x.7064] ,  [2.x.7065] . 

 [2.x.7066]  


examples/step-71/doc/results.dox 



[1.x.2722] 

[1.x.2723] 

The first exploratory example produces the following output. It is verified that all three implementations produce identical results. 

[1.x.2724] 



[1.x.2725] 

To help summarize the results from the virtual experiment itself, below are some graphs showing the shear stress, plotted against the shear strain, at a select location within the material sample. The plots show the stress-strain curves under three different magnetic loads, and for the last cycle of the (mechanical) loading profile, when the rate-dependent material reaches a repeatable ("steady-state") response. These types of graphs are often referred to as [Lissajous plots](https://en.wikipedia.org/wiki/Lissajous_curve). The area of the ellipse that the curve takes for viscoelastic materials provides some measure of how much energy is dissipated by the material, and its ellipticity indicates the phase shift of the viscous response with respect to the elastic response. 

 [2.x.7067]  

It is not surprising to see that the magneto-elastic material response has an unloading curve that matches the loading curve -- the material is non-dissipative after all. But here it's clearly noticeable how the gradient of the curve increases as the applied magnetic field increases. The tangent at any point along this curve is related to the instantaneous shear modulus and, due to the way that the energy density function was defined, we expect that the shear modulus increases as the magnetic field strength increases. We observe much the same behavior for the magneto-viscoelastic material. The major axis of the ellipse traced by the loading-unloading curve has a slope that increases as a greater magnetic load is applied. At the same time, the more energy is dissipated by the material. 

As for the code output, this is what is printed to the console for the part pertaining to the rheological experiment conducted with the magnetoelastic material: 

[1.x.2726] 



And this portion of the output pertains to the experiment performed with the magneto-viscoelastic material: 

[1.x.2727] 



The timer output is also emitted to the console, so we can compare time taken to perform the hand- and assisted- calculations and get some idea of the overhead of using the AD and SD frameworks. Here are the timings taken from the magnetoelastic experiment using the AD framework, based on the Sacado component of the Trilinos library: 

[1.x.2728] 

With respect to the computations performed using automatic differentiation (as a reminder, this is with two levels of differentiation using the Sacado library in conjunction with dynamic forward auto-differentiable types), we observe that the assisted computations takes about  [2.x.7068]  longer to compute the desired quantities. This does seem like quite a lot of overhead but, as mentioned in the introduction, it's entirely subjective and circumstance-dependent as to whether or not this is acceptable or not: Do you value computer time more than human time for doing the necessary hand-computations of derivatives, verify their correctness, implement them, and verify the correctness of the implementation? If you develop a research code that will only be run for a relatively small number of experiments, you might value your own time more. If you develop a production code that will be run over and over on 10,000-core clusters for hours, your considerations might be different. In any case, the one nice feature of the AD approach is the "drop in" capability when functions and classes are templated on the scalar type. This means that minimal effort is required to start working with it. 

In contrast, the timings for magneto-viscoelastic material as implemented using just-in-time (JIT) compiled symbolic algebra indicate that, at some non-negligible cost during initialization, the calculations themselves are a lot more efficiently executed: 

[1.x.2729] 

Since the initialization phase need, most likely, only be executed once per thread, this initial expensive phase can be offset by the repeated use of a single  [2.x.7069]  instance. Even though the magneto-viscoelastic constitutive law has more terms to calculate when compared to its magnetoelastic counterpart, it still is a whole order of magnitude faster to execute the computations of the kinetic variables and tangents. And when compared to the hand computed variant that uses the caching scheme, the calculation time is nearly equal. So although using the symbolic framework requires a paradigm shift in terms of how one implements and manipulates the symbolic expressions, it can offer good performance and flexibility that the AD frameworks lack. 

On the point of data caching, the added cost of value caching for the magneto-viscoelastic material implementation is, in fact, about a  [2.x.7070]  increase in the time spent in `update_internal_data()` when compared to the implementation using intermediate values for the numerical experiments conducted with this material. Here's a sample output of the timing comparison extracted for the "hand calculated" variant when the caching data structure is removed: 

[1.x.2730] 



With some minor adjustment we can quite easily test the different optimization schemes for the batch optimizer. So let's compare the computational expense associated with the `LLVM` batch optimizer setting versus the alternatives. Below are the timings reported for the `lambda` optimization method (retaining the use of CSE): 

[1.x.2731] 

The primary observation here is that an order of magnitude greater time is spent in the "Assisted computation" section when compared to the `LLVM` approach. 

Last of all we'll test how `dictionary` substitution, in conjunction with CSE, performs. Dictionary substitution simply does all of the evaluation within the native CAS framework itself, with no transformation of the underlying data structures taking place. Only the use of CSE, which caches intermediate results, will provide any "acceleration" in this instance. With that in mind, here are the results from this selection: 

[1.x.2732] 

Needless to say, compared to the other two methods, these results took quite some time to produce... The `dictionary` substitution method is perhaps only really viable for simple expressions or when the number of calls is sufficiently small. 

[1.x.2733] 

Perhaps you've been convinced that these tools have some merit, and can be of immediate help or use to you. The obvious question now is which one to use. Focusing specifically at a continuum point level, where you would be using these frameworks to compute derivatives of a constitutive law in particular, we can say the following: 

- Automatic differentiation probably provides the simplest entry point into   the world of assisted differentiation. 

- Given a sufficiently generic implementation of a constitutive framework,   AD can often be used as a drop-in replacement for the intrinsic scalar types   and the helper classes can then be leveraged to compute first (and possibly   higher order) derivatives with minimal effort. 

- As a qualification to the above point, being a "drop-in replacement" does not   mean that you must not be contentious of what the algorithms that these numbers   are being passed through are doing. It is possible to inadvertently perform   an operation that would, upon differentiating, return an incorrect result.   So this is definitely something that one should be aware of.   A concrete example: When computing the eigenvalues of a tensor, if the tensor   is diagonal then a short-cut to the result is simply to return the diagonal   entries directly (as extracted from the input tensor). This is completely   correct in terms of computing the eigenvalues themselves, but not going   through the algorithm that would otherwise compute the eigenvalues for a   non-diagonal tensor has had an unintended side-effect, namely that the   eigenvalues appear (to the AD framework) to be completely decoupled from   one another and their cross-sensitivities are not encoded in the returned   result. Upon differentiating, many entries of the derivative tensor will   be missing. To fix this issue, one has to ensure that the standard eigenvalue   solving algorithm is used so that the sensitivities of the returned eigenvalues   with respect to one another are encoded in the result. 

- Computations involving AD number types may be expensive. The expense increases   (sometimes quite considerably) as the order of the differential operations   increases. This may be mitigated by computational complexity of surrounding   operations (such as a linear solve, for example), but is ultimately problem   specific. 

- AD is restricted to the case where only total derivatives are required. If a   differential operation requires a partial derivative with respect to an   independent variable then it is not appropriate to use it. 

- Each AD library has its own quirks (sad to say but, in the author's experience,   true), so it may take some trial and error to find the appropriate library and   choice of AD number to suit your purposes. The reason for these "quirks"   often boils down to the overall philosophy behind the library (data structures,   the use of template meta-programming, etc.) as well as the mathematical   implementation of the derivative computations (for example, manipulations of   results using logarithmic functions to change basis might restrict the domain   for the input values -- details all hidden from the user, of course).   Furthermore, one library might be able to compute the desired results quicker   than another, so some initial exploration might be beneficial in that regard. 

- Symbolic differentiation (well, the use of a CAS in general) provides the most   flexible framework with which to perform assisted computations. 

- The SD framework can do everything that the AD frameworks can, with the   additional benefit of having low-level control over when certain manipulations   and operations are performed. 

- Acceleration of expression evaluation is possible, potentially leading to   near-native performance of the SD framework compared to some hand implementations   (this comparison being dependent on the overall program design, of course) at   the expense of the initial optimization call. 

- Clever use of the  [2.x.7071]  could minimize the   expense of the costly call that optimizes the dependent expressions.   The possibility to serialize the  [2.x.7072]    that often (but not always) this expensive call can be done once and then   reused in a later simulation. 

- If two or more material laws differ by only their material parameters, for   instance, then a single batch optimizer can be shared between them as long   as those material parameters are considered to be symbolic. The implication   of this is that you can "differentiate once, evaluate in many contexts". 

- The SD framework may partially be used as a "drop-in replacement" for scalar   types, but one (at the very least) has to add some more framework around it   to perform the value substitution step, converting symbolic types to their   numerical counterparts. 

- It may not be possible to use SD numbers within some specialized algorithms.   For example, if an algorithm has an exit point or code branch based off of   some concrete, numerical value that the (symbolic) input argument should take,   then obviously this isn't going to work. One either has to reimplement the   algorithm specifically for SD number types (somewhat inconvenient, but   frequently possible as conditionals are supported by the    [2.x.7073]  class), or one must use a creative means   around this specific issue (e.g., introduce a symbolic expression that   represents the result returned by this algorithm, perhaps declaring it   to be a   [symbolic function](https://dealii.org/developer/doxygen/deal.II/namespaceDifferentiation_1_1SD.html#a876041f6048705c7a8ad0855cdb1bd7a)   if that makes sense within the context in which it is to be used. This can   later be substituted by its numerical values, and if declared a symbolic   function then its deferred derivatives may also be incorporated into the   calculations as substituted results.). 

- The biggest drawback to using SD is that using it requires a paradigm shift,   and that one has to frame most problems differently in order to take the   most advantage of it. (Careful consideration of how the data structures   are used and reused is also essential to get it to work effectively.) This may   mean that one needs to play around with it a bit and build up an understanding   of what the sequence of typical operations is and what specifically each step   does in terms of manipulating the underlying data. If one has the time and   inclination to do so, then the benefits of using this tool may be substantial. 

[1.x.2734] 

There are a few logical ways in which this program could be extended: 

- Perhaps the most obvious extension would be to implement and test other constitutive models.   This could still be within the realm of coupled magneto-mechanical problems, perhaps considering   alternatives to the "Neo-Hookean"-type elastic part of the energy functions, changing the   constitutive law for the dissipative energy (and its associated evolution law), or including   magnetic hysteretic effects or damage models for the composite polymer that these material   seek to model. 

- Of course, the implemented models could be modified or completely replaced with models that are   focused on other aspects of physics, such as electro-active polymers, biomechanical materials,   elastoplastic media, etc. 

- Implement a different time-discretization scheme for the viscoelastic evolution law. 

- Instead of deriving everything directly from an energy density function, use the    [2.x.7074]  to directly linearize the kinetic quantities.   This would mean that only a once-differentiable auto-differentiable number type   would be required, and would certainly improve the performance greatly.   Such an approach would also offer the opportunity for dissipative materials,   such as the magneto-viscoelastic one consider here, to be implemented in   conjunction with AD. This is because the linearization invokes the total   derivative of the dependent variables with respect to the field variables, which   is exactly what the AD frameworks can provide. 

- Investigate using other auto-differentiable number types and frameworks (such as   ADOL-C). Since each AD library has its own implementation, the choice of which   to use could result in performance increases and, in the most unfortunate cases,   more stable computations. It can at least be said that for the AD libraries that   deal.II supports, the accuracy of results should be largely unaffected by this decision. 

- Embed one of these constitutive laws within a finite element simulation. 

With less effort, one could think about re-writing nonlinear problem solvers such as the one implemented in step-15 using AD or SD approaches to compute the Newton matrix. Indeed, this is done in step-72. 


examples/step-72/doc/intro.dox 

 [2.x.7075]  

[1.x.2735] 




[1.x.2736] 

[1.x.2737] 

This program solves the same problem as step-15, that is, it solves for the [minimal surface equation](https://en.wikipedia.org/wiki/Minimal_surface)   [1.x.2738] 



Among the issues we had identified there (see the [1.x.2739] section) was that when wanting to use a Newton iteration, we needed to compute the derivative of the residual of the equation with regard to the solution  [2.x.7076]  (here, because the right hand side is zero, the residual is simply the left hand side). For the equation we have here, this is cumbersome but not impossible -- but one can easily imagine much more complicated equations where just implementing the residual itself correctly is a challenge, let alone doing so for the derivative necessary to compute the Jacobian matrix. We will address this issue in this program: Using the automatic differentiation techniques discussed in great detail in step-71, we will come up with a way how we only have to implement the residual and get the Jacobian for free. 

In fact, we can even go one step further. While in step-15 we have just taken the equation as a given, the minimal surface equation is actually the product of minimizing an energy. Specifically, the minimal surface equations are the Euler-Lagrange equations that correspond to minimizing the energy   [1.x.2740] 

where the *energy density* is given by   [1.x.2741] 

This is the same as saying that we seek to find the stationary point of the variation of the energy functional   [1.x.2742] 

as this is where the equilibrium solution to the boundary value problem lies. 

The key point then is that, maybe, we don't even need to implement the residual, but that implementing the simpler energy density  [2.x.7077]  might actually be enough. 

Our goal then is this: When using a Newton iteration, we need to repeatedly solve the linear partial differential equation   [1.x.2743] 

so that we can compute the update   [1.x.2744] 

with the solution  [2.x.7078]  of the Newton step. As discussed in step-15, we can compute the derivative  [2.x.7079]  by hand and obtain   [1.x.2745] 



So here then is what this program is about: It is about techniques that can help us with computing  [2.x.7080]  without having to implement it explicitly, either by providing an implementation of  [2.x.7081]  or an implementation of  [2.x.7082] . More precisely, we will implement three different approaches and compare them in terms of run-time but also -- maybe more importantly -- how much human effort it takes to implement them: 

- The method used in step-15 to form the Jacobian matrix. 

- Computing the Jacobian matrix from an implementation of the   residual  [2.x.7083] , using automatic differentiation. 

- Computing both the residual and Jacobian matrix from an   implementation of the energy functional  [2.x.7084] , also using automatic   differentiation. 

For the first of these methods, there are no conceptual changes compared to step-15. 




[1.x.2746] 

For the second method, let us outline how we will approach the issue using automatic differentiation to compute the linearization of the residual vector. To this end, let us change notation for a moment and denote by  [2.x.7085]  not the residual of the differential equation, but in fact the *residual vector* -- i.e., the *discrete residual*. We do so because that is what we *actually* do when we discretize the problem on a given mesh: We solve the problem  [2.x.7086]  where  [2.x.7087]  is the vector of unknowns. 

More precisely, the  [2.x.7088] th component of the residual is given by 

[1.x.2747] 

where  [2.x.7089] . Given this, the contribution for cell  [2.x.7090]  is 

[1.x.2748] 

Its first order Taylor expansion is given as 

[1.x.2749] 

and consequently we can compute the contribution of cell  [2.x.7091]  to the Jacobian matrix  [2.x.7092]  as  [2.x.7093] . The important point here is that on cell  [2.x.7094] , we can express 

[1.x.2750] 

For clarity, we have used  [2.x.7095]  and  [2.x.7096]  as counting indices to make clear that they are distinct from each other and from  [2.x.7097]  above. Because in this formula,  [2.x.7098]  only depends on the coefficients  [2.x.7099] , we can compute the derivative  [2.x.7100]  as a matrix via automatic differentiation of  [2.x.7101] . By the same argument as we always use, it is clear that  [2.x.7102]  does not actually depend on *all* unknowns  [2.x.7103] , but only on those unknowns for which  [2.x.7104]  is a shape function that lives on cell  [2.x.7105] , and so in practice, we restrict  [2.x.7106]  and  [2.x.7107]  to that part of the vector and matrix that corresponds to the *local* DoF indices, and then distribute from the local cell  [2.x.7108]  to the global objects. 

Using all of these realizations, the approach will then be to implement  [2.x.7109]  in the program and let the automatic differentiation machinery compute the derivatives  [2.x.7110]  from that. 




[1.x.2751] 

For the final implementation of the assembly process, we will move a level higher than the residual: our entire linear system will be determined directly from the energy functional that governs the physics of this boundary value problem. We can take advantage of the fact that we can calculate the total energy in the domain directly from the local contributions, i.e., 

[1.x.2752] 

In the discrete setting, this means that on each finite element we have 

[1.x.2753] 

If we implement the cell energy, which depends on the field solution, we can compute its first (discrete) variation 

[1.x.2754] 

and, thereafter, its second (discrete) variation 

[1.x.2755] 

So, from the cell contribution to the total energy function, we may expect to have the approximate residual and tangent contributions generated for us as long as we can provide an implementation of the local energy  [2.x.7111] . Again, due to the design of the automatic differentiation variables used in this tutorial, in practice these approximations for the contributions to the residual vector and tangent matrix are actually accurate to machine precision. 


examples/step-72/doc/results.dox 



[1.x.2756] 

Since there was no change to the physics of the problem that has first been analyzed in step-15, there is nothing to report about that. The only outwardly noticeable difference between them is that, by default, this program will only run 9 mesh refinement steps (as opposed to step-15, which executes 11 refinements). This will be observable in the simulation status that appears between the header text that prints which assembly method is being used, and the final timings. (All timings reported below were obtained in release mode.) 

[1.x.2757] 



So what is interesting for us to compare is how long the assembly process takes for the three different implementations, and to put that into some greater context. Below is the output for the hand linearization (as computed on a circa 2012 four core, eight thread laptop -- but we're only really interested in the relative time between the different implementations): 

[1.x.2758] 

And for the implementation that linearizes the residual in an automated manner using the Sacado dynamic forward AD number type: 

[1.x.2759] 

And, lastly, for the implementation that computes both the residual and its linearization directly from an energy functional (using nested Sacado dynamic forward AD numbers): 

[1.x.2760] 



It's evident that the more work that is passed off to the automatic differentiation framework to perform, the more time is spent during the assembly process. Accumulated over all refinement steps, using one level of automatic differentiation resulted in  [2.x.7112]  more computational time being spent in the assembly stage when compared to unassisted assembly, while assembling the discrete linear system took  [2.x.7113]  longer when deriving directly from the energy functional. Unsurprisingly, the overall time spent solving the linear system remained unchanged. This means that the proportion of time spent in the solve phase to the assembly phase shifted significantly as the number of times automated differentiation was performed at the finite element level. For many, this might mean that leveraging higher-order differentiation (at the finite element level) in production code leads to an unacceptable overhead, but it may still be useful during the prototyping phase. A good compromise between the two may, therefore, be the automated linearization of the finite element residual, which offers a lot of convenience at a measurable, but perhaps not unacceptable, cost. Alternatively, one could consider not re-building the Newton matrix in every step -- a topic that is explored in substantial depth in step-77. 

Of course, in practice the actual overhead is very much dependent on the problem being evaluated (e.g., how many components there are in the solution, what the nature of the function being differentiated is, etc.). So the exact results presented here should be interpreted within the context of this scalar problem alone, and when it comes to other problems, some preliminary investigation by the user is certainly warranted. 




[1.x.2761] 

Like step-71, there are a few items related to automatic differentiation that could be evaluated further: 

- The use of other AD frameworks should be investigated, with the outlook that   alternative implementations may provide performance benefits. 

- It is also worth evaluating AD number types other than those that have been   hard-coded into this tutorial. With regard to twice differentiable types   employed at the finite-element level, mixed differentiation modes ("RAD-FAD")   should in principle be more computationally efficient than the single   mode ("FAD-FAD") types employed here. The reason that the RAD-FAD type was not   selected by default is that, at the time of writing, there remain some   bugs in its implementation within the Sacado library that lead to memory leaks.   This is documented in the  [2.x.7114]  module. 

- It might be the case that using reduced precision types (i.e., `float`) as the   scalar types for the AD numbers could render a reduction in computational   expense during assembly. Using `float` as the data type for the   matrix and the residual is not unreasonable, given that the Newton   update is only meant to get us closer to the solution, but not   actually *to* the solution; as a consequence, it makes sense to   consider using reduced-precision data types for computing these   updates, and then accumulating these updates in a solution vector   that uses the full `double` precision accuracy. 

- One further method of possibly reducing resources during assembly is to frame   the AD implementations as a constitutive model. This would be similar to the   approach adopted in step-71, and pushes the starting point for the automatic   differentiation one level higher up the chain of computations. This, in turn,   means that less operations are tracked by the AD library, thereby reducing the   cost of differentiating (even though one would perform the differentiation at   each cell quadrature point). 

- step-77 is yet another variation of step-15 that addresses a very   different part of the problem: Line search and whether it is   necessary to re-build the Newton matrix in every nonlinear   iteration. Given that the results above show that using automatic   differentiation comes at a cost, the techniques in step-77 have the   potential to offset these costs to some degree. It would therefore   be quite interesting to combine the current program with the   techniques in step-77. For production codes, this would certainly be   the way to go. 


examples/step-74/doc/intro.dox 

 [2.x.7115]  

[1.x.2762] 


[1.x.2763] 

[1.x.2764] 

[1.x.2765] In this tutorial, we display the usage of the FEInterfaceValues class, which is designed for assembling face terms arising from discontinuous Galerkin (DG) methods. The FEInterfaceValues class provides an easy way to obtain the jump and the average of shape functions and of the solution across cell faces. This tutorial includes the following topics. <ol>    [2.x.7116]  The SIPG method for Poisson's equation, which has already been used in step-39 and step-59.    [2.x.7117]  Assembling of face terms using FEInterfaceValues and the system matrix using  [2.x.7118]  which is similar to step-12.    [2.x.7119]  Adaptive mesh refinement using an error estimator.    [2.x.7120]  Two test cases: convergence test for a smooth function and adaptive mesh refinement test for a singular solution.  [2.x.7121]  

[1.x.2766] In this example, we consider Poisson's equation 

[1.x.2767] 

subject to the boundary condition 

[1.x.2768] 

For simplicity, we assume that the diffusion coefficient  [2.x.7122]  is constant here. Note that if  [2.x.7123]  is discontinuous, we need to take this into account when computing jump terms on cell faces. 

We denote the mesh by  [2.x.7124] , and  [2.x.7125]  is a mesh cell. The sets of interior and boundary faces are denoted by  [2.x.7126]  and  [2.x.7127]  respectively. Let  [2.x.7128]  and  [2.x.7129]  be the two cells sharing a face  [2.x.7130] , and  [2.x.7131]  be the outer normal vector of  [2.x.7132] . Then the jump operator is given by the "here minus there" formula, 

[1.x.2769] 

and the averaging operator as 

[1.x.2770] 

respectively. Note that when  [2.x.7133] , we define  [2.x.7134]  and  [2.x.7135] . The discretization using the SIPG is given by the following weak formula (more details can be found in  [2.x.7136]  and the references therein) 

[1.x.2771] 






[1.x.2772] The penalty parameter is defined as  [2.x.7137] , where  [2.x.7138]  a local length scale associated with the cell face; here we choose an approximation of the length of the cell in the direction normal to the face:  [2.x.7139] , where  [2.x.7140]  are the two cells adjacent to the face  [2.x.7141]  and we we compute  [2.x.7142] . 

In the formula above,  [2.x.7143]  is the penalization constant. To ensure the discrete coercivity, the penalization constant has to be large enough  [2.x.7144] . People do not really have consensus on which of the formulas proposed in the literature should be used. (This is similar to the situation discussed in the "Results" section of step-47.) One can just pick a large constant, while other options could be the multiples of  [2.x.7145]  or  [2.x.7146] . In this code, we follow step-39 and use  [2.x.7147] . 




[1.x.2773] In this example, with a slight modification, we use the error estimator by Karakashian and Pascal  [2.x.7148]  

[1.x.2774] 

where 

[1.x.2775] 

Here we use  [2.x.7149]  instead of  [2.x.7150]  for the jump terms of  [2.x.7151]  (the first term in  [2.x.7152]  and  [2.x.7153] ). 

In order to compute this estimator, in each cell  [2.x.7154]  we compute 

[1.x.2776] 

Then the square of the error estimate per cell is 

[1.x.2777] 

The factor of  [2.x.7155]  results from the fact that the overall error estimator includes each interior face only once, and so the estimators per cell count it with a factor of one half for each of the two adjacent cells. Note that we compute  [2.x.7156]  instead of  [2.x.7157]  to simplify the implementation. The error estimate square per cell is then stored in a global vector, whose  [2.x.7158]  norm is equal to  [2.x.7159] . 

[1.x.2778] In the first test problem, we run a convergence test using a smooth manufactured solution with  [2.x.7160]  in 2D 

[1.x.2779] 

and  [2.x.7161] . We compute errors against the manufactured solution and evaluate the convergence rate. 

In the second test, we choose  [2.x.7162]  on a L-shaped domain  [2.x.7163]  in 2D. The solution is given in the polar coordinates by  [2.x.7164] , which has a singularity at the origin. An error estimator is constructed to detect the region with large errors, according to which the mesh is refined adaptively. 


examples/step-74/doc/results.dox 



[1.x.2780] 

The output of this program consist of the console output and solutions in vtu format. 

In the first test case, when you run the program, the screen output should look like the following: 

[1.x.2781] 



When using the smooth case with polynomial degree 3, the convergence table will look like this:  [2.x.7165]  

Theoretically, for polynomial degree  [2.x.7166] , the order of convergence in  [2.x.7167]  norm and  [2.x.7168]  seminorm should be  [2.x.7169]  and  [2.x.7170] , respectively. Our numerical results are in good agreement with theory. 

In the second test case, when you run the program, the screen output should look like the following: 

[1.x.2782] 



The following figure provides a log-log plot of the errors versus the number of degrees of freedom for this test case on the L-shaped domain. In order to interpret it, let  [2.x.7171]  be the number of degrees of freedom, then on uniformly refined meshes,  [2.x.7172]  is of order  [2.x.7173]  in 2D. Combining the theoretical results in the previous case, we see that if the solution is sufficiently smooth, we can expect the error in the  [2.x.7174]  norm to be of order  [2.x.7175]  and in  [2.x.7176]  seminorm to be  [2.x.7177] . It is not a priori clear that one would get the same kind of behavior as a function of  [2.x.7178]  on adaptively refined meshes like the ones we use for this second test case, but one can certainly hope. Indeed, from the figure, we see that the SIPG with adaptive mesh refinement produces asymptotically the kinds of hoped-for results: 

 [2.x.7179]  

In addition, we observe that the error estimator decreases at almost the same rate as the errors in the energy norm and  [2.x.7180]  seminorm, and one order lower than the  [2.x.7181]  error. This suggests its ability to predict regions with large errors. 

While this tutorial is focused on the implementation, the step-59 tutorial program achieves an efficient large-scale solver in terms of computing time with matrix-free solution techniques. Note that the step-59 tutorial does not work with meshes containing hanging nodes at this moment, because the multigrid interface matrices are not as easily determined, but that is merely the lack of some interfaces in deal.II, nothing fundamental. 


examples/step-75/doc/intro.dox 

 [2.x.7182]  

[1.x.2783] 




 [2.x.7183]  As a prerequisite of this program, you need to have the p4est library and the Trilinos library installed. The installation of deal.II together with these additional libraries is described in the [1.x.2784] file. 




[1.x.2785] 

[1.x.2786] 

In the finite element context, more degrees of freedom usually yield a more accurate solution but also require more computational effort. 

Throughout previous tutorials, we found ways to effectively distribute degrees of freedom by aligning the grid resolution locally with the complexity of the solution (adaptive mesh refinement, step-6). This approach is particularly effective if we do not only adapt the grid alone, but also locally adjust the polynomial degree of the associated finite element on each cell (hp-adaptation, step-27). 

In addition, assigning more processes to run your program simultaneously helps to tackle the computational workload in lesser time. Depending on the hardware architecture of your machine, your program must either be prepared for the case that all processes have access to the same memory (shared memory, step-18), or that processes are hosted on several independent nodes (distributed memory, step-40). 

In the high-performance computing segment, memory access turns out to be the current bottleneck on supercomputers. We can avoid storing matrices altogether by computing the effect of matrix-vector products on the fly with MatrixFree methods (step-37). They can be used for geometric multigrid methods (step-50) and also for polynomial multigrid methods to speed solving the system of equation tremendously. 

This tutorial combines all of these particularities and presents a state-of-the-art way how to solve a simple Laplace problem: utilizing both hp-adaptation and matrix-free hybrid multigrid methods on machines with distributed memory. 




[1.x.2787] 

For parallel applications in FEM, we partition the grid into subdomains (aka domain decomposition), which are assigned to processes. This partitioning happens on active cells in deal.II as demonstrated in step-40. There, each cell has the same finite element and the same number of degrees of freedom assigned, and approximately the same workload. To balance the workload among all processes, we have to balance the number of cells on all participating processes. 

With hp-adaptive methods, this is no longer the case: the finite element type may vary from cell to cell and consequently also the number of degrees of freedom. Matching the number of cells does not yield a balanced workload. In the matrix-free context, the workload can be assumed to be proportional the number of degrees of freedom of each process, since in the best case only the source and the destination vector have to be loaded. 

One could balance the workload by assigning weights to every cell which are proportional to the number of degrees of freedom, and balance the sum of all weights between all processes. Assigning individual weights to each cell can be realized with the class  [2.x.7184]  which we will use later. 




[1.x.2788] 

With hp-adaptive methods, we not only have to decide which cells we want to refine or coarsen, but we also have the choice how we want to do that: either by adjusting the grid resolution or the polynomial degree of the finite element. 

We will again base the decision on which cells to adapt on (a posteriori) computed error estimates of the current solution, e.g., using the KellyErrorEstimator. We will similarly decide how to adapt with (a posteriori) computed smoothness estimates: large polynomial degrees work best on smooth parts of the solution while fine grid resolutions are favorable on irregular parts. In step-27, we presented a way to calculate smoothness estimates based on the decay of Fourier coefficients. Let us take here the opportunity and present an alternative that follows the same idea, but with Legendre coefficients. 

We will briefly present the idea of this new technique, but limit its description to 1D for simplicity. Suppose  [2.x.7185]  is a finite element function defined on a cell  [2.x.7186]  as 

[1.x.2789] 

where each  [2.x.7187]  is a shape function. We can equivalently represent  [2.x.7188]  in the basis of Legendre polynomials  [2.x.7189]  as 

[1.x.2790] 

Our goal is to obtain a mapping between the finite element coefficients  [2.x.7190]  and the Legendre coefficients  [2.x.7191] . We will accomplish this by writing the problem as a  [2.x.7192] -projection of  [2.x.7193]  onto the Legendre basis. Each coefficient  [2.x.7194]  can be calculated via 

[1.x.2791] 

By construction, the Legendre polynomials are orthogonal under the  [2.x.7195] -inner product on  [2.x.7196] . Additionally, we assume that they have been normalized, so their inner products can be written as 

[1.x.2792] 

where  [2.x.7197]  is the Kronecker delta, and  [2.x.7198]  is the Jacobian of the mapping from  [2.x.7199]  to  [2.x.7200] , which (in this tutorial) is assumed to be constant (i.e., the mapping must be affine). 

Hence, combining all these assumptions, the projection matrix for expressing  [2.x.7201]  in the Legendre basis is just  [2.x.7202]  -- that is,  [2.x.7203]  times the identity matrix. Let  [2.x.7204]  be the Mapping from  [2.x.7205]  to its reference cell  [2.x.7206] . The entries in the right-hand side in the projection system are, therefore, 

[1.x.2793] 

Recalling the shape function representation of  [2.x.7207] , we can write this as  [2.x.7208] , where  [2.x.7209]  is the change-of-basis matrix with entries 

[1.x.2794] 

so the values of  [2.x.7210]  can be written  [2.x.7211] independently [2.x.7212]  of  [2.x.7213]  by factoring  [2.x.7214]  out front after transforming to reference coordinates. Hence, putting it all together, the projection problem can be written as 

[1.x.2795] 

which can be rewritten as simply 

[1.x.2796] 



At this point, we need to emphasize that most finite element applications use unstructured meshes for which mapping is almost always non-affine. Put another way: the assumption that  [2.x.7215]  is constant across the cell is not true for general meshes. Hence, a correct calculation of  [2.x.7216]  requires not only that we calculate the corresponding transformation matrix  [2.x.7217]  for every single cell, but that we also define a set of Legendre-like orthogonal functions on a cell  [2.x.7218]  which may have an arbitrary and very complex geometry. The second part, in particular, is very computationally expensive. The current implementation of the FESeries transformation classes relies on the simplification resulting from having a constant Jacobian to increase performance and thus only yields correct results for affine mappings. The transformation is only used for the purpose of smoothness estimation to decide on the type of adaptation, which is not a critical component of a finite element program. Apart from that, this circumstance does not pose a problem for this tutorial as we only use square-shaped cells. 

Eibner and Melenk  [2.x.7219]  argued that a function is analytic, i.e., representable by a power series, if and only if the absolute values of the Legendre coefficients decay exponentially with increasing index  [2.x.7220] : 

[1.x.2797] 

The rate of decay  [2.x.7221]  can be interpreted as a measure for the smoothness of that function. We can get it as the slope of a linear regression fit of the transformation coefficients: 

[1.x.2798] 



We will perform this fit on each cell  [2.x.7222]  to get a local estimate for the smoothness of the finite element approximation. The decay rate  [2.x.7223]  then acts as the decision indicator for hp-adaptation. For a finite element on a cell  [2.x.7224]  with a polynomial degree  [2.x.7225] , calculating the coefficients for  [2.x.7226]  proved to be a reasonable choice to estimate smoothness. You can find a more detailed and dimension independent description in  [2.x.7227] . 

All of the above is already implemented in the  [2.x.7228]  class and the  [2.x.7229]  namespace. With the error estimates and smoothness indicators, we are then left to flag the cells for actual refinement and coarsening. Some functions from the  [2.x.7230]  and  [2.x.7231]  namespaces will help us with that later. 




[1.x.2799] 

Finite element matrices are typically very sparse. Additionally, hp-adaptive methods correspond to matrices with highly variable numbers of nonzero entries per row. Some state-of-the-art preconditioners, like the algebraic multigrid (AMG) ones as used in step-40, behave poorly in these circumstances. 

We will thus rely on a matrix-free hybrid multigrid preconditioner. step-50 has already demonstrated the superiority of geometric multigrid methods method when combined with the MatrixFree framework. The application on hp-adaptive FEM requires some additional work though since the children of a cell might have different polynomial degrees. As a remedy, we perform a p-relaxation to linear elements first (similar to Mitchell  [2.x.7232] ) and then perform h-relaxation in the usual manner. On the coarsest level, we apply an algebraic multigrid solver. The combination of p-multigrid, h-multigrid, and AMG makes the solver to a hybrid multigrid solver. 

We will create a custom hybrid multigrid preconditioner with the special level requirements as described above with the help of the existing global-coarsening infrastructure via the use of MGTransferGlobalCoarsening. 




[1.x.2800] 

For elliptic equations, each reentrant corner typically invokes a singularity  [2.x.7233] . We can use this circumstance to put our hp-decision algorithms to a test: on all cells to be adapted, we would prefer a fine grid near the singularity, and a high polynomial degree otherwise. 

As the simplest elliptic problem to solve under these conditions, we chose the Laplace equation in a L-shaped domain with the reentrant corner in the origin of the coordinate system. 

To be able to determine the actual error, we manufacture a boundary value problem with a known solution. On the above mentioned domain, one solution to the Laplace equation is, in polar coordinates,  [2.x.7234] : 

[1.x.2801] 



See also  [2.x.7235]  or  [2.x.7236] . The solution looks as follows: 

<div style="text-align:center;">   <img src="https://www.dealii.org/images/steps/developer/step-75.solution.svg"        alt="Analytic solution."> </div> 

The singularity becomes obvious by investigating the solution's gradient in the vicinity of the reentrant corner, i.e., the origin 

[1.x.2802] 



As we know where the singularity will be located, we expect that our hp-decision algorithm decides for a fine grid resolution in this particular region, and high polynomial degree anywhere else. 

So let's see if that is actually the case, and how hp-adaptation performs compared to pure h-adaptation. But first let us have a detailed look at the actual code. 


examples/step-75/doc/results.dox 



[1.x.2803] 

When you run the program with the given parameters on four processes in release mode, your terminal output should look like this: 

[1.x.2804] 



When running the code with more processes, you will notice slight differences in the number of active cells and degrees of freedom. This is due to the fact that solver and preconditioner depend on the partitioning of the problem, which might yield to slight differences of the solution in the last digits and ultimately yields to different adaptation behavior. 

Furthermore, the number of iterations for the solver stays about the same in all cycles despite hp-adaptation, indicating the robustness of the proposed algorithms and promising good scalability for even larger problem sizes and on more processes. 

Let us have a look at the graphical output of the program. After all refinement cycles in the given parameter configuration, the actual discretized function space looks like the following with its partitioning on twelve processes on the left and the polynomial degrees of finite elements on the right. In the left picture, each color represents a unique subdomain. In the right picture, the lightest color corresponds to the polynomial degree two and the darkest one corresponds to degree six: 

<div class="twocolumn" style="width: 80%; text-align: center;">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-75.subdomains-07.svg"          alt="Partitioning after seven refinements.">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-75.fedegrees-07.svg"          alt="Local approximation degrees after seven refinements.">   </div> </div> 




[1.x.2805] 

[1.x.2806] 

[1.x.2807] 

The deal.II library offers multiple strategies to decide which type of adaptation to impose on cells: either adjust the grid resolution or change the polynomial degree. We only presented the [1.x.2808] strategy in this tutorial, while step-27 demonstrated the [1.x.2809] equivalent of the same idea. 

See the "possibilities for extensions" section of step-27 for an overview over these strategies, or the corresponding documentation for a detailed description. 

There, another strategy is mentioned that has not been shown in any tutorial so far: the strategy based on [1.x.2810]. The usage of this method for parallel distributed applications is more tricky than the others, so we will highlight the challenges that come along with it. We need information about the final state of refinement flags, and we need to transfer the solution across refined meshes. For the former, we need to attach the  [2.x.7237]  function to the  [2.x.7238]  signal in a way that it will be called [1.x.2811] the  [2.x.7239]  function. At this stage, all refinement flags and future FE indices are terminally set and a reliable prediction of the error is possible. The predicted error then needs to be transferred across refined meshes with the aid of  [2.x.7240]  

Try implementing one of these strategies into this tutorial and observe the subtle changes to the results. You will notice that all strategies are capable of identifying the singularities near the reentrant corners and will perform  [2.x.7241] -refinement in these regions, while preferring  [2.x.7242] -refinement in the bulk domain. A detailed comparison of these strategies is presented in  [2.x.7243]  . 




[1.x.2812] 

This tutorial focuses solely on matrix-free strategies. All hp-adaptive algorithms however also work with matrix-based approaches in the parallel distributed context. 

To create a system matrix, you can either use the  [2.x.7244]  function, or use an  [2.x.7245]  function similar to the one of step-27. You can then pass the system matrix to the solver as usual. 

You can time the results of both matrix-based and matrix-free implementations, quantify the speed-up, and convince yourself which variant is faster. 




[1.x.2813] 

For sake of simplicity, we have restricted ourselves to a single type of coarse-grid solver (CG with AMG), smoother (Chebyshev smoother with point Jacobi preconditioner), and geometric-coarsening scheme (global coarsening) within the multigrid algorithm. Feel free to try out alternatives and investigate their performance and robustness. 


examples/step-76/doc/intro.dox 



 [2.x.7246]  

[1.x.2814] 

[1.x.2815] 

[1.x.2816] 

This tutorial program solves the Euler equations of fluid dynamics, using an explicit time integrator with the matrix-free framework applied to a high-order discontinuous Galerkin discretization in space. The numerical approach used here is identical to that used in step-67, however, we utilize different advanced MatrixFree techniques to reach even a higher throughput. 

The two main features of this tutorial are: 

- the usage of shared-memory features from MPI-3.0 and 

- the usage of cell-centric loops, which allow to write to the global vector only   once and, therefore, are ideal for the usage of shared memory. 

Further topics we discuss in this tutorial are the usage and benefits of the template argument VectorizedArrayType (instead of simply using VectorizedArray<Number>) as well as the possibility to pass lambdas to MatrixFree loops. 

For details on the numerics, we refer to the documentation of step-67. We concentrate here only on the key differences. 

[1.x.2817] 

[1.x.2818] 

There exist many shared-memory libraries that are based on threads like TBB, OpenMP, or TaskFlow. Integrating such libraries into existing MPI programs allows one to use shared memory. However, these libraries come with an overhead for the programmer, since all parallelizable code sections have to be found and transformed according to the library used, including the difficulty when some third-party numerical library, like an iterative solver package, only relies on MPI. 

Considering a purely MPI-parallelized FEM application, one can identify that the major time and memory benefit of using shared memory would come from accessing the part of the solution vector owned by the processes on the same compute node without the need to make explicit copies and buffering them. Fur this propose, MPI-3.0 provides shared-memory features based on so-called windows, where processes can directly access the data of the neighbors on the same shared-memory domain. 

[1.x.2819] 

A few relevant MPI-3.0 commands are worth discussing in detail. A new MPI communicator  [2.x.7247] , which consists of processes from the communicator  [2.x.7248]  that have access to the same shared memory, can be created via: 

[1.x.2820] 



The following code snippet shows the simplified allocation routines of shared memory for the value type  [2.x.7249]  and the size  [2.x.7250] , as well as, how to query pointers to the data belonging to processes in the same shared-memory domain: 

[1.x.2821] 



Once the data is not needed anymore, the window has to be freed, which also frees the locally-owned data: 

[1.x.2822] 



[1.x.2823] 

The commands mentioned in the last section are integrated into  [2.x.7251]  and are used to allocate shared memory if an optional (second) communicator is provided to the reinit()-functions. 

For example, a vector can be set up with a partitioner (containing the global communicator) and a sub-communicator (containing the processes on the same compute node): 

[1.x.2824] 



Locally owned values and ghost values can be processed as usual. However, now users also have read access to the values of the shared-memory neighbors via the function: 

[1.x.2825] 



[1.x.2826] 

While  [2.x.7252]  provides the option to allocate shared memory and to access the values of shared memory of neighboring processes in a coordinated way, it does not actually exploit the benefits of the usage of shared memory itself. 

The MatrixFree infrastructure, however, does: 

- On the one hand, within the matrix-free loops  [2.x.7253]     [2.x.7254]  and  [2.x.7255]  only ghost   values that need to be updated  [2.x.7256] are [2.x.7257]  updated. Ghost values from   shared-memory neighbors can be accessed directly, making buffering, i.e.,   copying of the values into the ghost region of a vector possibly redundant.   To deal with possible race conditions, necessary synchronizations are   performed within MatrixFree. In the case that values have to be buffered,   values are copied directly from the neighboring shared-memory process,   bypassing more expensive MPI operations based on  [2.x.7258]  and    [2.x.7259] . 

- On the other hand, classes like FEEvaluation and FEFaceEvaluation can read   directly from the shared memory, so buffering the values is indeed   not necessary in certain cases. 

To be able to use the shared-memory capabilities of MatrixFree, MatrixFree has to be appropriately configured by providing the user-created sub-communicator: 

[1.x.2827] 






[1.x.2828] 

[1.x.2829] 

"Face-centric loops" (short FCL) visit cells and faces (inner and boundary ones) in separate loops. As a consequence, each entity is visited only once and fluxes between cells are evaluated only once. How to perform face-centric loops with the help of  [2.x.7260]  by providing three functions (one for the cell integrals, one for the inner, and one for the boundary faces) has been presented in step-59 and step-67. 

"Cell-centric loops" (short CCL or ECL (for element-centric loops) in the hyper.deal release paper), in contrast, process a cell and in direct succession process all its faces (i.e., visit all faces twice). Their benefit has become clear for modern CPU processor architecture in the literature  [2.x.7261] , although this kind of loop implies that fluxes have to be computed twice (for each side of an interior face). CCL has two primary advantages: 

- On the one hand, entries in the solution vector are written exactly once   back to main memory in the case of CCL, while in the case of FCL at least once   despite of cache-efficient scheduling of cell and face loops-due to cache   capacity misses. 

- On the other hand, since each entry of the solution vector is accessed exactly   once, no synchronization between threads is needed while accessing the solution   vector in the case of CCL. This absence of race conditions during writing into   the destination vector makes CCL particularly suitable for shared-memory   parallelization. 

One should also note that although fluxes are computed twice in the case of CCL, this does not automatically translate into doubling of the computation, since values already interpolated to the cell quadrature points can be interpolated to a face with a simple 1D interpolation. 

[1.x.2830] 

For cell-centric loop implementations, the function  [2.x.7262]  can be used, to which the user can pass a function that should be performed on each cell. 

To derive an appropriate function, which can be passed in  [2.x.7263]  one might, in principle, transform/merge the following three functions, which can be passed to a  [2.x.7264]  

[1.x.2831] 



in the following way: 

[1.x.2832] 



It should be noted that FEFaceEvaluation is initialized now with two numbers, the cell number and the local face number. The given example only highlights how to transform face-centric loops into cell-centric loops and is by no means efficient, since data is read and written multiple times from and to the global vector as well as computations are performed redundantly. Below, we will discuss advanced techniques that target these issues. 

To be able to use  [2.x.7265]  following flags of  [2.x.7266]  have to be enabled: 

[1.x.2833] 



In particular, these flags enable that the internal data structures are set up for all faces of the cells. 

Currently, cell-centric loops in deal.II only work for uniformly refined meshes and if no constraints are applied (which is the standard case DG is normally used). 




[1.x.2834] 

The examples given above have already used lambdas, which have been provided to matrix-free loops. The following short examples present how to transform functions between a version where a class and a pointer to one of its methods are used and a variant where lambdas are utilized. 

In the following code, a class and a pointer to one of its methods, which should be interpreted as cell integral, are passed to  [2.x.7267]  

[1.x.2835] 



[1.x.2836] 



However, it is also possible to pass an anonymous function via a lambda function with the same result: 

[1.x.2837] 



[1.x.2838] 

The class VectorizedArray<Number> is a key component to achieve the high node-level performance of the matrix-free algorithms in deal.II. It is a wrapper class around a short vector of  [2.x.7268]  entries of type Number and maps arithmetic operations to appropriate single-instruction/multiple-data (SIMD) concepts by intrinsic functions. The length of the vector can be queried by  [2.x.7269]  and its underlying number type by  [2.x.7270]  

In the default case ( [2.x.7271] ), the vector length is set at compile time of the library to match the highest value supported by the given processor architecture. However, also a second optional template argument can be specified as  [2.x.7272]  explicitly controls the  vector length within the capabilities of a particular instruction set. A full list of supported vector lengths is presented in the following table: 

 [2.x.7273]  

This allows users to select the vector length/ISA and, as a consequence, the number of cells to be processed at once in matrix-free operator evaluations, possibly reducing the pressure on the caches, an severe issue for very high degrees (and dimensions). 

A possible further reason to reduce the number of filled lanes is to simplify debugging: instead of having to look at, e.g., 8 cells, one can concentrate on a single cell. 

The interface of VectorizedArray also enables the replacement by any type with a matching interface. Specifically, this prepares deal.II for the  [2.x.7274]  class that is planned to become part of the C++23 standard. The following table compares the deal.II-specific SIMD classes and the equivalent C++23 classes: 


 [2.x.7275]  


examples/step-76/doc/results.dox 



[1.x.2839] 

Running the program with the default settings on a machine with 40 processes produces the following output: 

[1.x.2840] 



and the following visual output: 

 [2.x.7276]  

As a reference, the results of step-67 using FCL are: 

[1.x.2841] 



By the modifications shown in this tutorial, we were able to achieve a speedup of 27% for the Runge-Kutta stages. 

[1.x.2842] 

The algorithms are easily extendable to higher dimensions: a high-dimensional [1.x.2843] is part of the hyper.deal library. An extension of cell-centric loops to locally-refined meshes is more involved. 

[1.x.2844] 

The solver presented in this tutorial program can also be extended to the compressible Navier–Stokes equations by adding viscous terms, as also suggested in step-67. To keep as much of the performance obtained here despite the additional cost of elliptic terms, e.g. via an interior penalty method, that tutorial has proposed to switch the basis from FE_DGQ to FE_DGQHermite like in the step-59 tutorial program. The reasoning behind this switch is that in the case of FE_DGQ all values of neighboring cells (i.e.,  [2.x.7277]  layers) are needed, whilst in the case of FE_DGQHermite only 2 layers, making the latter significantly more suitable for higher degrees. The additional layers have to be, on the one hand, loaded from main memory during flux computation and, one the other hand, have to be communicated. Using the shared-memory capabilities introduced in this tutorial, the second point can be eliminated on a single compute node or its influence can be reduced in a hybrid context. 

[1.x.2845] 

Cell-centric loops could be used to create block Gauss-Seidel preconditioners that are multiplicative within one process and additive over processes. These type of preconditioners use during flux computation, in contrast to Jacobi-type preconditioners, already updated values from neighboring cells. The following pseudo-code visualizes how this could in principal be achieved: 

[1.x.2846] 



For this purpose, one can exploit the cell-data vector capabilities of MatrixFree and the range-based iteration capabilities of VectorizedArray. 

Please note that in the given example we process  [2.x.7278]  number of blocks, since each lane corresponds to one block. We consider blocks as updated if all blocks processed by a vector register have been updated. In the case of Cartesian meshes this is a reasonable approach, however, for general unstructured meshes this conservative approach might lead to a decrease in the efficiency of the preconditioner. A reduction of cells processed in parallel by explicitly reducing the number of lanes used by  [2.x.7279]  might increase the quality of the preconditioner, but with the cost that each iteration might be more expensive. This dilemma leads us to a further "possibility for extension": vectorization within an element. 


examples/step-77/doc/intro.dox 

 [2.x.7280]  

[1.x.2847]  [2.x.7281]  

[1.x.2848] 

[1.x.2849] 

The step-15 program solved the following, nonlinear equation describing the minimal surface problem: 

[1.x.2850] 

step-15 uses a Newton method, and Newton's method works by repeatedly solving a *linearized* problem for an update  [2.x.7282]  -- called the "search direction" --, computing a "step length"  [2.x.7283] , and then combining them to compute the new guess for the solution via 

[1.x.2851] 



In the course of the discussions in step-15, we found that it is awkward to compute the step length, and so just settled for simple choice: Always choose  [2.x.7284] . This is of course not efficient: We know that we can only realize Newton's quadratic convergence rate if we eventually are able to choose  [2.x.7285] , though we may have to choose it smaller for the first few iterations where we are still too far away to use this long a step length. 

Among the goals of this program is therefore to address this shortcoming. Since line search algorithms are not entirely trivial to implement, one does as one should do anyway: Import complicated functionality from an external library. To this end, we will make use of the interfaces deal.II has to one of the big nonlinear solver packages, namely the [KINSOL](https://computing.llnl.gov/projects/sundials/kinsol) sub-package of the [SUNDIALS](https://computing.llnl.gov/projects/sundials) suite. %SUNDIALS is, at its heart, a package meant to solve complex ordinary differential equations (ODEs) and differential-algebraic equations (DAEs), and the deal.II interfaces allow for this via the classes in the SUNDIALS namespace: Notably the  [2.x.7286]  and  [2.x.7287]  classes. But, because that is an important step in the solution of ODEs and DAEs with implicit methods, %SUNDIALS also has a solver for nonlinear problems called KINSOL, and deal.II has an interface to it in the form of the  [2.x.7288]  class. This is what we will use for the solution of our problem. 

But %SUNDIALS isn't just a convenient way for us to avoid writing a line search algorithm. In general, the solution of nonlinear problems is quite expensive, and one typically wants to save as much compute time as possible. One way one can achieve this is as follows: The algorithm in step-15 discretizes the problem and then in every iteration solves a linear system of the form 

[1.x.2852] 

where  [2.x.7289]  is the residual vector computed using the current vector of nodal values  [2.x.7290] ,  [2.x.7291]  is its derivative (called the "Jacobian"), and  [2.x.7292]  is the update vector that corresponds to the function  [2.x.7293]  mentioned above. The construction of  [2.x.7294]  has been thoroughly discussed in step-15, as has the way to solve the linear system in each Newton iteration. So let us focus on another aspect of the nonlinear solution procedure: Computing  [2.x.7295]  is expensive, and assembling the matrix  [2.x.7296]  even more so. Do we actually need to do that in every iteration? It turns out that in many applications, this is not actually necessary: These methods often converge even if we replace  [2.x.7297]  by an approximation  [2.x.7298]  and solve 

[1.x.2853] 

instead, then update 

[1.x.2854] 

This may require an iteration or two more because our update  [2.x.7299]  is not quite as good as  [2.x.7300] , but it may still be a win because we don't have to assemble  [2.x.7301]  quite as often. 

What kind of approximation  [2.x.7302]  would we like for  [2.x.7303] ? Theory says that as  [2.x.7304]  converges to the exact solution  [2.x.7305] , we need to ensure that  [2.x.7306]  needs to converge to  [2.x.7307] . In particular, since  [2.x.7308] , a valid choice is  [2.x.7309] . But so is choosing  [2.x.7310]  every, say, fifth iteration  [2.x.7311]  and for the other iterations, we choose  [2.x.7312]  equal to the last computed  [2.x.7313] . This is what we will do here: we will just re-use  [2.x.7314]  from the previous iteration, which may again be what we had used in the iteration before that,  [2.x.7315] . 

This scheme becomes even more interesting if, for the solution of the linear system with  [2.x.7316] , we don't just have to assemble a matrix, but also compute a good preconditioner. For example, if we were to use a sparse LU decomposition via the SparseDirectUMFPACK class, or used a geometric or algebraic multigrid. In those cases, we would also not have to update the preconditioner, whose computation may have taken about as long or longer than the assembly of the matrix in the first place. Indeed, with this mindset, we should probably think about using the *best* preconditioner we can think of, even though their construction is typically quite expensive: We will hope to amortize the cost of computing this preconditioner by applying it to more than one just one linear solve. 

The big question is, of course: By what criterion do we decide whether we can get away with the approximation  [2.x.7317]  based on a previously computed Jacobian matrix  [2.x.7318]  that goes back  [2.x.7319]  steps, or whether we need to -- at least in this iteration -- actually re-compute the Jacobian  [2.x.7320]  and the corresponding preconditioner? This is, like the issue with line search, one that requires a non-trivial amount of code that monitors the convergence of the overall algorithm. We *could* implement these sorts of things ourselves, but we probably *shouldn't*: KINSOL already does that for us. It will tell our code when to "update" the Jacobian matrix. 

One last consideration if we were to use an iterative solver instead of the sparse direct one mentioned above: Not only is it possible to get away with replacing  [2.x.7321]  by some approximation  [2.x.7322]  when solving for the update  [2.x.7323] , but one can also ask whether it is necessary to solve the linear system 

[1.x.2855] 

to high accuracy. The thinking goes like this: While our current solution  [2.x.7324]  is still far away from  [2.x.7325] , why would we solve this linear system particularly accurately? The update  [2.x.7326]  is likely still going to be far away from the exact solution, so why spend much time on solving the linear system to great accuracy? This is the kind of thinking that underlies algorithms such as the "Eisenstat-Walker trick"  [2.x.7327]  in which one is given a tolerance to which the linear system above in iteration  [2.x.7328]  has to be solved, with this tolerance dependent on the progress in the overall nonlinear solver. As before, one could try to implement this oneself, but KINSOL already provides this kind of information for us -- though we will not use it in this program since we use a direct solver that requires no solver tolerance and just solves the linear system exactly up to round-off. 

As a summary of all of these considerations, we could say the following: There is no need to reinvent the wheel. Just like deal.II provides a vast amount of finite-element functionality, %SUNDIALS' KINSOL package provides a vast amount of nonlinear solver functionality, and we better use it. 




[1.x.2856] 

KINSOL, like many similar packages, works in a pretty abstract way. At its core, it sees a nonlinear problem of the form 

[1.x.2857] 

and constructs a sequence of iterates  [2.x.7329]  which, in general, are vectors of the same length as the vector returned by the function  [2.x.7330] . To do this, there are a few things it needs from the user: 

- A way to resize a given vector to the correct size. 

- A way to evaluate, for a given vector  [2.x.7331] , the function  [2.x.7332] . This   function is generally called the "residual" operation because the   goal is of course to find a point  [2.x.7333]  for which  [2.x.7334] ;   if  [2.x.7335]  returns a nonzero vector, then this is the   [1.x.2858]   (i.e., the "rest", or whatever is "left over"). The function   that will do this is in essence the same as the computation of   the right hand side vector in step-15, but with an important difference:   There, the right hand side denoted the *negative* of the residual,   so we have to switch a sign. 

- A way to compute the matrix  [2.x.7336]  if that is necessary in the   current iteration, along with possibly a preconditioner or other   data structures (e.g., a sparse decomposition via   SparseDirectUMFPACK if that's what we choose to use to solve a   linear system). This operation will generally be called the   "setup" operation. 

- A way to solve a linear system  [2.x.7337]  with whatever   matrix  [2.x.7338]  was last computed. This operation will generally   be called the "solve" operation. 

All of these operations need to be provided to KINSOL by  [2.x.7339]  objects that take the appropriate set of arguments and that generally return an integer that indicates success (a zero return value) or failure (a nonzero return value). Specifically, the objects we will access are the  [2.x.7340]   [2.x.7341]   [2.x.7342]  and  [2.x.7343]  member variables. (See the documentation of these variables for their details.) In our implementation, we will use [lambda functions](https://en.cppreference.com/w/cpp/language/lambda) to implement these "callbacks" that in turn can call member functions; KINSOL will then call these callbacks whenever its internal algorithms think it is useful. 




[1.x.2859] 

The majority of the code of this tutorial program is as in step-15, and we will not comment on it in much detail. There is really just one aspect one has to pay some attention to, namely how to compute  [2.x.7344]  given a vector  [2.x.7345]  on the one hand, and  [2.x.7346]  given a vector  [2.x.7347]  separately. At first, this seems trivial: We just take the `assemble_system()` function and in the one case throw out all code that deals with the matrix and in the other case with the right hand side vector. There: Problem solved. 

But it isn't quite as simple. That's because the two are not independent if we have nonzero Dirichlet boundary values, as we do here. The linear system we want to solve contains both interior and boundary degrees of freedom, and when eliminating those degrees of freedom from those that are truly "free", using for example  [2.x.7348]  we need to know the matrix when assembling the right hand side vector. 

Of course, this completely contravenes the original intent: To *not* assemble the matrix if we can get away without it. We solve this problem as follows: 

- We set the starting guess for the solution vector,  [2.x.7349] , to one   where boundary degrees of freedom already have their correct values. 

- This implies that all updates can have zero updates for these   degrees of freedom, and we can build both residual vectors  [2.x.7350]    and Jacobian matrices  [2.x.7351]  that corresponds to linear systems whose   solutions are zero in these vector components. For this special   case, the assembly of matrix and right hand side vectors is   independent, and can be broken into separate functions. 

There is an assumption here that whenever KINSOL asks for a linear solver with the (approximation of the) Jacobian, that this will be for for an update  [2.x.7352]  (which has zero boundary values), a multiple of which will be added to the solution (which already has the right boundary values).  This may not be true and if so, we might have to rethink our approach. That said, it turns out that in practice this is exactly what KINSOL does when using a Newton method, and so our approach is successful. 


examples/step-77/doc/results.dox 



[1.x.2860] 

When running the program, you get output that looks like this: 

[1.x.2861] 



The way this should be interpreted is most easily explained by looking at the first few lines of the output on the first mesh: 

[1.x.2862] 

What is happening is this: 

- In the first residual computation, KINSOL computes the residual to see whether   the desired tolerance has been reached. The answer is no, so it requests the   user program to compute the Jacobian matrix (and the function then also   factorizes the matrix via SparseDirectUMFPACK). 

- KINSOL then instructs us to solve a linear system of the form    [2.x.7353]  with this matrix and the previously computed   residual vector. 

- It is then time to determine how far we want to go in this direction,   i.e., do line search. To this end, KINSOL requires us to compute the   residual vector  [2.x.7354]  for different step lengths    [2.x.7355] . For the first step above, it finds an acceptable  [2.x.7356]    after two tries, the second time around it takes three tries. 

- Having found a suitable updated solution  [2.x.7357] , the process is   repeated except now KINSOL is happy with the current Jacobian matrix   and does not instruct us to re-build the matrix and its factorization,   and instead asks us to solve a linear system with that same matrix. 

The program also writes the solution to a VTU file at the end of each mesh refinement cycle, and it looks as follows:  [2.x.7358]  


The key takeaway messages of this program are the following: 

- The solution is the same as the one we computed in step-15, i.e., the   interfaces to %SUNDIALS' KINSOL package really did what they were supposed   to do. This should not come as a surprise, but the important point is that   we don't have to spend the time implementing the complex algorithms that   underlie advanced nonlinear solvers ourselves. 

- KINSOL is able to avoid all sorts of operations such as rebuilding the   Jacobian matrix when that is not actually necessary. Comparing the   number of linear solves in the output above with the number of times   we rebuild the Jacobian and compute its factorization should make it   clear that this leads to very substantial savings in terms of compute   times, without us having to implement the intricacies of algorithms   that determine when we need to rebuild this information. 

[1.x.2863] 

[1.x.2864] 

For all but the small problems we consider here, a sparse direct solver requires too much time and memory -- we need an iterative solver like we use in many other programs. The trade-off between constructing an expensive preconditioner (say, a geometric or algebraic multigrid method) is different in the current case, however: Since we can re-use the same matrix for numerous linear solves, we can do the same for the preconditioner and putting more work into building a good preconditioner can more easily be justified than if we used it only for a single linear solve as one does for many other situations. 

But iterative solvers also afford other opportunities. For example (and as discussed briefly in the introduction), we may not need to solve to very high accuracy (small tolerances) in early nonlinear iterations as long as we are still far away from the actual solution. This was the basis of the Eisenstat-Walker trick mentioned there. 

KINSOL provides the function that does the linear solution with a target tolerance that needs to be reached. We ignore it in the program above because the direct solver we use does not need a tolerance and instead solves the linear system exactly (up to round-off, of course), but iterative solvers could make use of this kind of information -- and, in fact, should. 


examples/step-78/doc/intro.dox 

[1.x.2865] 

[1.x.2866] 

The Black-Scholes equation is a partial differential equation that falls a bit out of the ordinary scheme. It describes what the fair price of a "European call" stock option is. Without going into too much detail, a stock "option" is a contract one can buy from a bank that allows me, but not requires me, to buy a specific stock at a fixed price  [2.x.7359]  at a fixed future time  [2.x.7360]  in the future. The question one would then want to answer as a buyer of such an option is "How much do I think such a contract is worth?", or as the seller "How much do I need to charge for this contract?", both as a function of the time  [2.x.7361]  before the contract is up at time  [2.x.7362]  and as a function of the stock price  [2.x.7363] . Fischer Black and Myron Scholes derived a partial differential equation for the fair price  [2.x.7364]  for such options under the assumption that stock prices exhibit random price fluctuations with a given level of "volatility" plus a background exponential price increase (which one can think of as the inflation rate that simply devalues all money over time). For their work, Black and Scholes received the Nobel Prize in Economic Sciences in 1997, making this the first tutorial program dealing with a problem for which someone has gotten a Nobel Prize  [2.x.7365] . 

The equation reads as follows: 

[1.x.2867] 

where 

[1.x.2868] 



The way we should interpret this equation is that it is a time-dependent partial differential equation of one "space" variable  [2.x.7366]  as the price of the stock, and  [2.x.7367]  is the price of the option at time  [2.x.7368]  if the stock price at that time were  [2.x.7369] . 

[1.x.2869] 

There are a number of oddities in this equation that are worth discussing before moving on to its numerical solution. First, the "spatial" domain  [2.x.7370]  is unbounded, and thus  [2.x.7371]  can be unbounded in value. This is because there may be a practical upper bound for stock prices, but not a conceptual one. The boundary conditions  [2.x.7372]  as  [2.x.7373]  can then be interpreted as follows: What is the value of an option that allows me to buy a stock at price  [2.x.7374]  if the stock price (today or at time  [2.x.7375] ) is  [2.x.7376] ? One would expect that it is  [2.x.7377]  plus some adjustment for inflation, or, if we really truly consider huge values of  [2.x.7378] , we can neglect  [2.x.7379]  and arrive at the statement that the boundary values at the infinite boundary should be of the form  [2.x.7380]  as stated above. 

In practice, for us to use a finite element method to solve this, we are going to need to bound  [2.x.7381] . Since this equation describes prices, and it doesn't make sense to talk about prices being negative, we will set the lower bound of  [2.x.7382]  to be 0. Then, for an upper bound, we will choose a very large number, one that  [2.x.7383]  is not very likely to ever get to. We will call this  [2.x.7384] . So,  [2.x.7385] . 

Second, after truncating the domain, we need to ask what boundary values we should pose at this now finite boundary. To take care of this, we use "put-call" parity  [2.x.7386] . A "pull option" is one in which we are allowed, but not required, to *sell* a stock at price  [2.x.7387]  to someone at a future time  [2.x.7388] . This says 

[1.x.2870] 

where  [2.x.7389]  is the value of the call option, and  [2.x.7390]  is the value of the put option. Since we expect  [2.x.7391]  as  [2.x.7392] , this says 

[1.x.2871] 

and we can use this as a reasonable boundary condition at our finite point  [2.x.7393] . 

The second complication of the Block-Scholes equation is that we are given a final condition, and not an initial condition. This is because we know what the option is worth at time  [2.x.7394] : If the stock price at  [2.x.7395]  is  [2.x.7396] , then we have no incentive to use our option of buying a price  [2.x.7397]  because we can buy that stock for cheaper on the open market. So  [2.x.7398]  for  [2.x.7399] . On the other hand, if at time  [2.x.7400]  we have  [2.x.7401] , then we can buy the stock at price  [2.x.7402]  via the option and immediately sell it again on the market for price  [2.x.7403] , giving me a profit of  [2.x.7404] . In other words,  [2.x.7405]  for  [2.x.7406] . So, we only know values for  [2.x.7407]  at the *end time* but not the initial time -- in fact, finding out what a fair price at the current time (conventionally taken to be  [2.x.7408] ) is what solving these equations is all about. 

This means that this is not an equation that is posed going forward in time, but in fact going *backward* in time. Thus it makes sense to solve this problem in reverse by making the change of variables  [2.x.7409]  where now  [2.x.7410]  denotes the time before the strike time  [2.x.7411] . 

With all of this, we finally end up with the following problem: 

[1.x.2872] 



Conceptually, this is an advection-diffusion-reaction problem for the variable  [2.x.7412] : There is both a second-order derivative diffusion term, a first-order derivative advection term, and a zeroth-order reaction term. We can expect this problem to be a little bit forgiving in practice because for realistic values of the coefficients, it is diffusive dominated. But, because of the advective terms in the problem, we will have to be careful with mesh refinement and time step choice. There is also the issue that the diffusion term  is written in a non-conservative form and so integration by parts is not  immediately obvious. This will be discussed in the next section. 

[1.x.2873] 

We will solve this problem using an IMEX method. In particular, we first discretize in time with the theta method and will later pick different values of theta for the advective and diffusive terms. Let  [2.x.7413]  approximate  [2.x.7414] : 

[1.x.2874] 

Here,  [2.x.7415]  is the time step size. Given this time discretization, we can proceed to discretize space by multiplying with test functions and then integrating by parts. Because there are some interesting details in this due to the advective and non-advective terms in this equation, this process will be explained in detail. 

So, we begin by multiplying by test functions,  [2.x.7416] : 

[1.x.2875] 




As usual, (1) becomes  [2.x.7417]  and (4) becomes  [2.x.7418] , where  [2.x.7419] , and where we have taken the liberty of denoting by  [2.x.7420]  not only the function  [2.x.7421]  but also the vector of nodal values after discretization. 

The interesting parts come from (2) and (3). 


For (2), we have: 

[1.x.2876] 



There are two integrals here, that are more or less the same, with the differences being a slightly different coefficient in front of the integral, and a different time step for V. Therefore, we will outline this integral in the general case, and account for the differences at the end. So, consider the general integral, which we will solve using integration by parts: 

[1.x.2877] 



So, after adding in the constants and exchanging  [2.x.7422]  for  [2.x.7423]  where applicable, we arrive at the following for (2): 

[1.x.2878] 

But, because the matrix  [2.x.7424]  involves an advective term, we will choose  [2.x.7425]  there -- in other words, we use an explicit Euler method to treat advection. Conversely, since the matrix  [2.x.7426]  involves the diffusive term, we will choose  [2.x.7427]  there -- i.e., we treat diffusion using the second order Crank-Nicolson method. 

So, we arrive at the following: 

[1.x.2879] 



Now, to handle (3). For this, we will again proceed by considering the general case like above. 

[1.x.2880] 



So, again after adding in the constants and exchanging  [2.x.7428]  for  [2.x.7429]  where applicable, we arrive at the following for (3): 

[1.x.2881] 

Just as before, we will use  [2.x.7430]  for the matrix  [2.x.7431]  and  [2.x.7432]  for the matrix  [2.x.7433] . So, we arrive at the following for (3): 

[1.x.2882] 



Now, putting everything together, we obtain the following discrete form for the Black-Scholes Equation: 

[1.x.2883] 

So, altogether we have: 

[1.x.2884] 



As usual, we can write this with the unknown quantities on the left and the known ones on the right. This leads to the following linear system that would have to be solved in each time step: 

[1.x.2885] 









[1.x.2886] For this program, we will use the Method of Manufactured Solutions (MMS) to test  that it is working correctly. This means that we will choose our solution to be   a certain function similar to step-7. For our case, we will use: 

[1.x.2887] 

This means that, using our PDE, we arrive at the following problem: 

[1.x.2888] 

Where,  [2.x.7434] . This set-up now has right hand sides for the equation itself and for the boundary conditions at  [2.x.7435]  that we did not have before, along with "final" conditions (or, with  [2.x.7436] -time "initial conditions") that do not match the real situation. We will implement this in such a way in the code that it is easy to exchange -- the introduction of the changes above is just meant to enable the  use of a manufactured solution. 

If the program is working correctly, then it should produce (**) as the solution. This does mean that we need to modify our variational form somewhat to account for the non-zero right hand side. 

First, we define the following: 

[1.x.2889] 

So, we arrive at the new equation: 

[1.x.2890] 



We then solve this equation as outlined above. 


examples/step-78/doc/results.dox 



[1.x.2891] 


Below is the output of the program: 

[1.x.2892] 



What is more interesting is the output of the convergence tables. They are outputted into the console, as well into a LaTeX file. The convergence tables are shown above. Here, you can see that the the solution has a convergence rate of  [2.x.7437]  with respect to the  [2.x.7438] -norm, and the solution has a convergence rate of  [2.x.7439]  with respect to the  [2.x.7440] -norm. 


Below is the visualization of the solution. 

<div style="text-align:center;">   <img src="https://www.dealii.org/images/steps/developer/step-78.mms-solution.png"        alt="Solution of the MMS problem."> </div> 


examples/step-79/doc/intro.dox 

[1.x.2893] 

[1.x.2894] 

Topology Optimization of Elastic Media is a technique used to optimize a structure that is bearing some load. Ideally, we would like to minimize the maximum stress placed on a structure by selecting a region  [2.x.7441]  where material is placed. In other words, 

[1.x.2895] 



[1.x.2896] 



[1.x.2897] 



Here,  [2.x.7442]  is the stress within the body that is caused by the external forces  [2.x.7443] , where we have for simplicity assumed that the material is linear-elastic and so  [2.x.7444]  is the stress-strain tensor and  [2.x.7445]  is the small-deformation strain as a function of the displacement  [2.x.7446]  -- see step-8 and step-17 for more on linear elasticity. In the formulation above,  [2.x.7447]  is the maximal amount of material we are willing to provide to build the object. The last of the constraints is the partial differential equation that relates stress  [2.x.7448]  and forces  [2.x.7449]  and is simply the steady-state force balance. 

That said, the infinity norm above creates a problem: As a function of location of material, this objective function is necessarily not differentiable, making prospects of optimization rather bleak. So instead, a common approach in topology optimization is to find an approximate solution by optimizing a related problem: We would like to minimize the strain energy. This is a measure of the potential energy stored in an object due to its deformation, but also works as a measure of total deformation over the structure. 

[1.x.2898] 



[1.x.2899] 



[1.x.2900] 



The value of the objective function is calculated using a finite element method, where the solution is the displacements. This is placed inside of a nonlinear solver loop that solves for a vector denoting placement of material. 

[1.x.2901] 

In actual practice, we can only build objects in which the material is either present, or not present, at any given point -- i.e., we would have an indicator function  [2.x.7450]  that describes the material-filled region and that we want to find through the optimization problem. In this case, the optimization problem becomes combinatorial, and very expensive to solve. Instead, we use an approach called Solid Isotropic Material with Penalization, or SIMP.  [2.x.7451]  

The SIMP method is based on an idea of allowing the material to exist in a location with a density  [2.x.7452]  between 0 and 1. A density of 0 suggests the material is not there, and it is not a part of the structure, while a density of 1 suggests the material is present. Values between 0 and 1 do not reflect a design we can create in the real-world, but allow us to turn the combinatorial problem into a continuous one. One then looks at density values  [2.x.7453] , with the constraint that  [2.x.7454] . The minimum value  [2.x.7455] , typically chosen to be around  [2.x.7456] , avoids the possibility of having an infinite strain energy, but is small enough to provide accurate results. 

The straightforward application of the effect of this "density" on the elasticity of the media would be to simply multiply the stiffness tensor  [2.x.7457]  of the medium by the given density, that is,  [2.x.7458] . However, this approach often gives optimal solutions where density values are far from both 0 and 1. As one wants to find a real-world solution, meaning the material either is present or it is not, a penalty is applied to these in-between values. A simple and effective way to do this is to multiply the stiffness tensor by the density raised to some integer power penalty parameter  [2.x.7459] , so that  [2.x.7460] . This makes density values farther away from 0 or 1 less effective. It has been shown that using  [2.x.7461]  is sufficiently high to create 'black-and-white' solutions: that is, one gets optimal solutions in which material is either present or not present at all points. 

More material should always provide a structure with a lower strain energy, and so the inequality constraint can be viewed as an equality where the total volume used is the maximum volume. 

Using this density idea also allows us to reframe the volume constraint on the optimization problem. Use of SIMP then turns the optimization problem into the following: 

[1.x.2902] 



[1.x.2903] 



[1.x.2904] 



[1.x.2905] 

The final constraint, the balance of linear momentum (which we will refer to as the elasticity equation),  gives a method for finding  [2.x.7462]  and  [2.x.7463]  given the density  [2.x.7464] . 

[1.x.2906] The elasticity equation in the time independent limit reads 

[1.x.2907] 

In the situations we will care about, we will assume that the medium has a linear material response and in that case, we have that 

[1.x.2908] 

In everything we will do below, we will always consider the displacement field  [2.x.7465]  as the only solution variable, rather than considering  [2.x.7466]  and  [2.x.7467]  as solution variables (as is done in mixed formulations). 

Furthermore, we will make the assumption that the material is linear isotropic, in which case the stress-strain tensor can be expressed in terms of the Lam&eacute; parameters  [2.x.7468]  such that 

[1.x.2909] 

See step-8 for how this transformation works. 

Integrating the objective function by parts gives 

[1.x.2910] 

into which the linear elasticity equation can then be substituted, giving 

[1.x.2911] 

Because we are assuming no body forces, this simplifies further to 

[1.x.2912] 

which is the final form of the governing equation that we'll be considering from this point forward. 

[1.x.2913] 

Typically, the solutions to topology optimization problems are mesh-dependent, and as such the problem is ill-posed. This is because fractal structures are often formed as the mesh is refined further. As the mesh gains resolution, the optimal solution typically gains smaller and smaller structures. There are a few competing workarounds to this issue, but the most popular for first order optimization is the sensitivity filter, while second order optimization methods tend to prefer use of a density filter. 

As the filters affect the gradient and Hessian of the strain energy (i.e., the objective function), the choice of filter has an effect on the solution of the problem. The density filter as part of a second order method works by introducing an unfiltered density, which we refer to as  [2.x.7469] , and then requiring that the density be a convolution of the unfiltered density: 

[1.x.2914] 

Here,  [2.x.7470]  is an operator so that  [2.x.7471]  is some kind of average of the values of  [2.x.7472]  in the area around  [2.x.7473]  -- i.e., it is a smoothed version of  [2.x.7474] . 

This prevents checkerboarding; the radius of the filter allows the user to define an effective minimal beam width for the optimal structures we seek to find. 

<div style="text-align:center;">   <img src="https://www.dealii.org/images/steps/developer/step-79.checkerboard.png"        alt="Checkerboarding occurring in an MBB Beam"> </div> 

[1.x.2915] 

The minimization problem is now 

[1.x.2916] 



[1.x.2917] 



[1.x.2918] 



[1.x.2919] 



[1.x.2920] 



The inequality constraints are dealt with by first introducing slack variables, and second using log barriers to ensure that we obtain an interior-point method. The penalty parameter is going to be  [2.x.7475] , and the following slack variables are <ol>      [2.x.7476]   [2.x.7477]  - a slack variable corresponding to the lower bound [2.x.7478]       [2.x.7479]   [2.x.7480]  - a slack variable corresponding to the upper bound. [2.x.7481]   [2.x.7482]  This now gives the following problem: 

[1.x.2921] 



[1.x.2922] 



[1.x.2923] 



[1.x.2924] 



[1.x.2925] 



[1.x.2926] 



With these variables in place, we can then follow the usual approach to solving constrained optimization problems: We introduce a Lagrangian in which we combine the objective function and the constraints by multiplying the constraints by Lagrange multipliers. Specifically, we will use the following symbols for the Lagrange multipliers for the various constraints: <ol>      [2.x.7483]   [2.x.7484] : a Lagrange multiplier corresponding to the     elasticity constraint,  [2.x.7485]       [2.x.7486]   [2.x.7487] : a Lagrange multiplier corresponding to the convolution     filter constraint,  [2.x.7488]       [2.x.7489]   [2.x.7490] : a Lagrange multiplier corresponding to the lower slack variable, and  [2.x.7491]       [2.x.7492]   [2.x.7493] : a Lagrange multiplier corresponding to the upper slack variable.  [2.x.7494]   [2.x.7495]  With these variables, the Lagrangian function reads as follows: 

[1.x.2927] 



The solution of the optimization problem then needs to satisfy what are known as the [Karush-Kuhn-Tucker (KKT) conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions): The derivatives of the Lagrangian with respect to all of its arguments need to be equal to zero, and because we have inequality constraints, we also have "complementarity" conditions. Since we here have an infinite-dimensional problem, these conditions all involve directional derivatives of the Lagrangian with regard to certain test functions -- in other words, all of these conditions have to be stated in weak form as is typically the basis for finite element methods anyway. 

The barrier method allows us to initially weaken the "complementary slackness" as required by the typical KKT conditions. Typically, we would require that  [2.x.7496] , but the barrier formulations give KKT conditions where  [2.x.7497] , where  [2.x.7498]  is our barrier parameter. As part of the barrier method, this parameter must be driven close to 0 to give a good approximation of the original problem. 

In the following, let us state all of these conditions where  [2.x.7499]  is a test function that is naturally paired with variational derivatives of the Lagrangian with respect to the  [2.x.7500]  function. For simplicity, we introduce  [2.x.7501]  to indicate the portion of the boundary where forces are applied, and Neumann boundary conditions are used. 

<ol>  [2.x.7502]  Stationarity: 

[1.x.2928] 



[1.x.2929] 



[1.x.2930] 

 [2.x.7503]   [2.x.7504]  Primal Feasibility: 

[1.x.2931] 



[1.x.2932] 



[1.x.2933] 



[1.x.2934] 

 [2.x.7505]   [2.x.7506] Complementary Slackness: 

[1.x.2935] 



[1.x.2936] 

 [2.x.7507]   [2.x.7508]  Dual Feasibility: 

[1.x.2937] 

 [2.x.7509]   [2.x.7510]  

[1.x.2938] 

The optimality conditions above are, in addition to being convoluted, of a kind that is not easy to solve: They are generally nonlinear, and some of the relationships are also inequalities. We will address the nonlinearity using a Newton method to compute search directions, and come back to how to deal with the inequalities below when talking about step length procedures. 

Newton's method applied to the equations above results in the system of equations listed below. Therein, variational derivatives with respect to the  [2.x.7511]  variable are taken in the  [2.x.7512]  direction. 

<ol>  [2.x.7513]  Stationarity: These equations ensure we are at a critical point of the objective function when constrained. 

Equation 1 

[1.x.2939] 



Equation 2 

[1.x.2940] 



Equation 3 

[1.x.2941] 

 [2.x.7514]  

 [2.x.7515]  Primal Feasibility: These equations ensure the equality constraints are met. 

Equation 4 

[1.x.2942] 



Equation 5 

[1.x.2943] 



Equation 6 

[1.x.2944] 



Equation 7 

[1.x.2945] 

 [2.x.7516]  

 [2.x.7517] Complementary Slackness: These equations essentially ensure the barrier is met - in the final solution, we need  [2.x.7518] . 

Equation 8 

[1.x.2946] 



Equation 9 

[1.x.2947] 

 [2.x.7519]  

 [2.x.7520] Dual Feasibility: The Lagrange multiplier on slacks and slack variables must be kept greater than 0. (This is the only part not implemented in the  [2.x.7521]  function.) 

[1.x.2948] 

 [2.x.7522]   [2.x.7523]  




[1.x.2949] We use a quadrilateral mesh with  [2.x.7524]  elements to discretize the displacement and displacement Lagrange multiplier. Piecewise constant  [2.x.7525]  elements are used to discretize the density, unfiltered density, density slack variables, and multipliers for the slack variables and filter constraint. 

[1.x.2950] 

While most of the discussion above follows traditional and well-known approaches to solving nonlinear optimization problems, it turns out that the problem is actually quite difficult to solve in practice. In particular, it is quite nonlinear and an important question is not just to find search directions  [2.x.7526]  as discussed above based on a Newton method, but that one needs to spend quite a lot of attention to how far one wants to go in this direction. This is often called "line search" and comes down to the question of how to choose the step length  [2.x.7527]  so that we move from the current iterate  [2.x.7528]  to the next iterate  [2.x.7529]  in as efficient a way as possible. It is well understood that we need to eventually choose  [2.x.7530]  to realize the Newton's method's quadratic convergence; however, in the early iterations, taking such a long step might actually make things worse, either by leading to a point that has a worse objective function or at which the constraints are satisfied less well than they are at  [2.x.7531] . 

Very complex algorithms have been proposed to deal with this issue  [2.x.7532]   [2.x.7533] . Here, we implement a watchdog-search algorithm  [2.x.7534] . When discussing this algorithm, we will use the vector  [2.x.7535]  to represent all primal variables - the filtered and unfiltered densities, slack variables and displacement - and use the vector  [2.x.7536]  to represent all of the dual vectors. The (incremental) solution to the nonlinear system of equations stated above will now be referred to as  [2.x.7537]  and  [2.x.7538]  instead of  [2.x.7539] . A merit function (explained in more detail later) is referred to here as  [2.x.7540] . 

The watchdog algorithm applied to a subproblem with a given barrier parameter works in the following way: First, the current iteration is saved as a "watchdog" state, and the merit of the watchdog state is recorded. A maximal feasible Newton step is then taken. If the merit sufficiently decreased from the first step, this new step is accepted. If not, another maximal feasible Newton step is taken, and the merit is again compared to the watchdog merit. If after some number (typically between 5 and 8) of Newton steps, the merit did not adequately decrease, the algorithm takes a scaled Newton step from either the watchdog state or the last iteration that guarantees a sufficient decrease of the merit, and that step is accepted. Once a step is accepted, the norm of the KKT error is measured, and if it is sufficiently small, the barrier value is decreased. If it is not sufficiently small, the last accepted step is taken to be the new watchdog step, and the process is repeated. 


Above, the "maximal feasible step" is a scaling of the Newton step in both the primal and dual variables given by 

[1.x.2951] 



[1.x.2952] 



Above,  [2.x.7541]  is the "fraction to boundary" that is allowed on any step. Because the derivatives become ill-conditioned near the boundary, this technique stands in for a [trust region](https://en.wikipedia.org/wiki/Trust_region) and is necessary to ensure good approximations in the future.  [2.x.7542]  is taken to be  [2.x.7543] , which allows movement closer to the boundary as the barrier becomes smaller. In the future, when implementing the LOQO algorithm for barrier reduction, this must be kept to 0.8 as the barrier parameter can vary wildly. 

Separately, we need to deal with the log-barrier that we have used to enforce the positivity constraint on the slack variables  [2.x.7544] : In the statement of the final optimization problem we solve, we have added the term 

[1.x.2953] 

The question is how we should choose the penalty factor  [2.x.7545] . As with all penalty methods, we are in reality only interested in the limit as  [2.x.7546] , since this is then the problem we really wanted to solve, subject to the positivity constraints on the slack variables. On the other hand, we need to choose  [2.x.7547]  large enough to make the problem solvable in practice. Actual implementations therefore start with a larger value of  [2.x.7548]  and gradually decrease it as the outer iterations proceed. 

In the monotone method implemented here, the barrier parameter is updated whenever some level of convergence is achieved at the current barrier parameter. We use the  [2.x.7549]  norm of the KKT conditions to check for convergence at each barrier size. The requirement is that  [2.x.7550]  where  [2.x.7551]  is a constant over any barrier size and  [2.x.7552]  is the barrier parameter. This forces better convergence in later iterations, and is the same requirement as is used in [IPOPT](https://coin-or.github.io/Ipopt/) (an open source software package for large-scale nonlinear optimization). 

Here, the barrier is reduced linearly at larger values, and superlinearly at smaller values. At larger values, it is multiplied by a constant (around 0.6), and at lower values the barrier value is replaced by the barrier value raised to some exponent (around 1.2). This method has proven to be effective at keeping  the subproblem solvable at large barrier values, while still allowing  superlinear convergence at smaller barrier values. In practice, this looks like  the following: 

[1.x.2954] 



While taking large steps at reducing the barrier size when convergence is reached is widely used, more recent research has shown that it is typically faster to use algorithms that adaptively update barrier each iteration, i.e., methods in which we use concrete criteria at the end of each iteration to determine what the penalty parameter should be in the next iteration, rather than using reduction factors that are independent of the current solution. That said, such methods are also more complicated and we will not do this here. 

[1.x.2955] 

The algorithm outlined above makes use of a "merit function". Merit functions are used to determine whether a step from  [2.x.7553]  to a proposed point  [2.x.7554]  is beneficial. In unconstrained optimization problems, one can simply check this with the objective function we try to minimize, and typically uses conditions such as the [Wolfe and Goldstein conditions](https://en.wikipedia.org/wiki/Wolfe_conditions). 

In constrained optimization problems, the question is how to balance reduction in the objective function against a possible increase in the violation of constraints: A proposed step might make the objective function smaller but be further away from the set of points that satisfy the constraints -- or the other way around. This trade-off is typically resolved by using a merit function that combines the two criteria. 

Here, we use an exact  [2.x.7555]  merit function to test the steps: 

[1.x.2956] 



Here,  [2.x.7556]  is a penalty parameter. This merit function being exact means that there exists some  [2.x.7557]  so that for any  [2.x.7558] , the merit function has its minima at the same location as the original problem. This penalty parameter is updated (by recommendation of Nocedal and Wright  [2.x.7559] ) as follows: 

[1.x.2957] 

where  [2.x.7560]  is the Hessian of the objective function,  [2.x.7561]  is a vector of our decision (primal) variables,  [2.x.7562]  is the objective function, and  [2.x.7563]  is the error on a current equality constraint. 

Our use of this method is partially due to already having most of the necessary parts calculated in finding the right hand side, but also the use of an exact merit function ensures that it is minimized in the same location as the overall problem. Recent research has shown that one can replace merit functions by what are called "filter methods", and one should consider using these instead as they prove to be more efficient. 


examples/step-79/doc/results.dox 



[1.x.2958] 

[1.x.2959] The algorithms used above are tested against a traditional topology optimization  problem called the Messerschmitt-Bolkow-Blohm Beam (MBB Beam). 

This problem considers the optimal 2-d structure that can be built on a rectangle 6 units wide, and 1 unit tall. The bottom corners are fixed in place in the  [2.x.7564]  direction using a zero Dirichlet boundary condition, and a downward force is applied in the center of the top of the beam by enforcing a Neumann boundary condition. The rest of the boundary is allowed to move, and has no external force applied, which takes the form of a zero Neumann boundary condition. In essence, we are asking the following question: How should we design a bridge in a way so that if the bottom left and bottom right point of the bridge are on rollers that allow these points to move horizontally but not vertically, and so that the displacement in response to the vertical force in the center is minimal. 

While the total volume of the domain is 6 units, 3 units of material are allowed for the structure. Because of the symmetry of the problem, it could be posed on a rectangle of width 3 and height 1 by cutting the original domain in half, and using zero Dirichlet boundary conditions in the  [2.x.7565]  direction along the cut edge. That said, symmetry of the solution is a good indicator that the program is working as expected, so we solved the problem on the whole domain, as shown below.  [2.x.7566]  

<div style="text-align:center;">   <img src="https://www.dealii.org/images/steps/developer/step-79.mbbgeometry.png"        alt="The MBB problem domain and boundary conditions"> </div> 


Using the program discussed above, we find the minimum volume of the MBB Beam and the individual components of the solution look as follows: 

<div class="onecolumn" style="width: 80%; text-align: center;">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-79.filtereddensity.png"          alt="Filtered Density Solution">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-79.unfiltereddensity.png"          alt="Unfiltered Density Solution">   </div> </div> 


These pictures show that what we find here is in accordance with what one typically sees in other publications on the topic  [2.x.7567] . Maybe more interestingly, the result looks like a truss bridge (except that we apply the load at the top of the trusses, rather than the bottom as in real truss bridges, akin to a "deck truss" bridge), suggesting that the designs that have been used in bridge- building for centuries are indeed based on ideas we can now show to be optimal in some sense. 




[1.x.2960] 

The results shown above took around 75 iterations to find, which is quite concerning given the expense in solving the large linear systems in each iteration. Looking at the evolution, it does look as though the convergence has moments of happening quickly and moments of happening slowly. We believe this to be due to both a lack of precision on when and how to decrease the boundary values, as well as our choice of merit function being sub-optimal. In the future, a LOQO barrier update replacing the monotone reduction, as well as a Markov Filter in place of a merit function will decrease the number of necessary iterations significantly. 

The barrier decrease is most sensitive in the middle of the convergence, which is problematic, as it seems like we need it to decrease quickly, then slowly, then quickly again. 

Secondly, the linear solver used here is just the sparse direct solver based on the SparseDirectUMFPACK class. This works reasonably well on small problems, but the formulation of the optimization problem detailed above has quite a large number of variables and so the linear problem is not only large but also has a lot of nonzero entries in many rows, even on meshes that overall are still relatively coarse. As a consequence, the solver time dominates the computations, and more sophisticated approaches at solving the linear system are necessary. 


examples/step-8/doc/intro.dox 

[1.x.2961] 

[1.x.2962] 


In real life, most partial differential equations are really systems of equations. Accordingly, the solutions are usually vector-valued. The deal.II library supports such problems (see the extensive documentation in the  [2.x.7568]  module), and we will show that that is mostly rather simple. The only more complicated problems are in assembling matrix and right hand side, but these are easily understood as well. 

 [2.x.7569]  

In this tutorial program we will want to solve the [1.x.2963]. They are an extension to Laplace's equation with a vector-valued solution that describes the displacement in each space direction of a rigid body which is subject to a force. Of course, the force is also vector-valued, meaning that in each point it has a direction and an absolute value. 

One can write the elasticity equations in a number of ways. The one that shows the symmetry with the Laplace equation in the most obvious way is to write it as 

[1.x.2964] 

where  [2.x.7570]  is the vector-valued displacement at each point,  [2.x.7571]  the force, and  [2.x.7572]  is a rank-4 tensor (i.e., it has four indices) that encodes the stress-strain relationship -- in essence, it represents the [1.x.2965] in Hookes law that relates the displacement to the forces.  [2.x.7573]  will, in many cases, depend on  [2.x.7574]  if the body whose deformation we want to simulate is composed of different materials. 

While the form of the equations above is correct, it is not the way they are usually derived. In truth, the gradient of the displacement  [2.x.7575]  (a matrix) has no physical meaning whereas its symmetrized version, 

[1.x.2966] 

does and is typically called the "strain". (Here and in the following,  [2.x.7576] . We will also use the [1.x.2967] that whenever the same index appears twice in an equation, summation over this index is implied; we will, however, not distinguish between upper and lower indices.) With this definition of the strain, the elasticity equations then read as 

[1.x.2968] 

which you can think of as the more natural generalization of the Laplace equation to vector-valued problems. (The form shown first is equivalent to this form because the tensor  [2.x.7577]  has certain symmetries, namely that  [2.x.7578] , and consequently  [2.x.7579] .) 

One can of course alternatively write these equations in component form: 

[1.x.2969] 



In many cases, one knows that the material under consideration is isotropic, in which case by introduction of the two coefficients  [2.x.7580]  and  [2.x.7581]  the coefficient tensor reduces to 

[1.x.2970] 



The elastic equations can then be rewritten in much simpler a form: 

[1.x.2971] 

and the respective bilinear form is then 

[1.x.2972] 

or also writing the first term a sum over components: 

[1.x.2973] 



 [2.x.7582]  As written, the equations above are generally considered to be the right description for the displacement of three-dimensional objects if the displacement is small and we can assume that [1.x.2974] is valid. In that case, the indices  [2.x.7583]  above all run over the set  [2.x.7584]  (or, in the C++ source, over  [2.x.7585] ). However, as is, the program runs in 2d, and while the equations above also make mathematical sense in that case, they would only describe a truly two-dimensional solid. In particular, they are not the appropriate description of an  [2.x.7586]  cross-section of a body infinite in the  [2.x.7587]  direction; this is in contrast to many other two-dimensional equations that can be obtained by assuming that the body has infinite extent in  [2.x.7588] -direction and that the solution function does not depend on the  [2.x.7589]  coordinate. On the other hand, there are equations for two-dimensional models of elasticity; see for example the Wikipedia article on [1.x.2975], [1.x.2976] and [1.x.2977]. 

But let's get back to the original problem. How do we assemble the matrix for such an equation? A very long answer with a number of different alternatives is given in the documentation of the  [2.x.7590]  module. Historically, the solution shown below was the only one available in the early years of the library. It turns out to also be the fastest. On the other hand, if a few per cent of compute time do not matter, there are simpler and probably more intuitive ways to assemble the linear system than the one discussed below but that weren't available until several years after this tutorial program was first written; if you are interested in them, take a look at the  [2.x.7591]  module. 

Let us go back to the question of how to assemble the linear system. The first thing we need is some knowledge about how the shape functions work in the case of vector-valued finite elements. Basically, this comes down to the following: let  [2.x.7592]  be the number of shape functions for the scalar finite element of which we build the vector element (for example, we will use bilinear functions for each component of the vector-valued finite element, so the scalar finite element is the  [2.x.7593]  element which we have used in previous examples already, and  [2.x.7594]  in two space dimensions). Further, let  [2.x.7595]  be the number of shape functions for the vector element; in two space dimensions, we need  [2.x.7596]  shape functions for each component of the vector, so  [2.x.7597] . Then, the  [2.x.7598] th shape function of the vector element has the form 

[1.x.2978] 

where  [2.x.7599]  is the  [2.x.7600] th unit vector,  [2.x.7601]  is the function that tells us which component of  [2.x.7602]  is the one that is nonzero (for each vector shape function, only one component is nonzero, and all others are zero).  [2.x.7603]  describes the space dependence of the shape function, which is taken to be the  [2.x.7604] -th shape function of the scalar element. Of course, while  [2.x.7605]  is in the range  [2.x.7606] , the functions  [2.x.7607]  and  [2.x.7608]  have the ranges  [2.x.7609]  (in 2D) and  [2.x.7610] , respectively. 

For example (though this sequence of shape functions is not guaranteed, and you should not rely on it), the following layout could be used by the library: 

[1.x.2979] 

where here 

[1.x.2980] 



[1.x.2981] 



In all but very rare cases, you will not need to know which shape function  [2.x.7611]  of the scalar element belongs to a shape function  [2.x.7612]  of the vector element. Let us therefore define 

[1.x.2982] 

by which we can write the vector shape function as 

[1.x.2983] 

You can now safely forget about the function  [2.x.7613] , at least for the rest of this example program. 

Now using this vector shape functions, we can write the discrete finite element solution as 

[1.x.2984] 

with scalar coefficients  [2.x.7614] . If we define an analog function  [2.x.7615]  as test function, we can write the discrete problem as follows: Find coefficients  [2.x.7616]  such that 

[1.x.2985] 



If we insert the definition of the bilinear form and the representation of  [2.x.7617]  and  [2.x.7618]  into this formula: 

[1.x.2986] 

We note that here and in the following, the indices  [2.x.7619]  run over spatial directions, i.e.  [2.x.7620] , and that indices  [2.x.7621]  run over degrees of freedom. 

The local stiffness matrix on cell  [2.x.7622]  therefore has the following entries: 

[1.x.2987] 

where  [2.x.7623]  now are local degrees of freedom and therefore  [2.x.7624] . In these formulas, we always take some component of the vector shape functions  [2.x.7625] , which are of course given as follows (see their definition): 

[1.x.2988] 

with the Kronecker symbol  [2.x.7626] . Due to this, we can delete some of the sums over  [2.x.7627]  and  [2.x.7628] : 

[1.x.2989] 



Likewise, the contribution of cell  [2.x.7629]  to the right hand side vector is 

[1.x.2990] 



This is the form in which we will implement the local stiffness matrix and right hand side vectors. 

As a final note: in the step-17 example program, we will revisit the elastic problem laid out here, and will show how to solve it in %parallel on a cluster of computers. The resulting program will thus be able to solve this problem to significantly higher accuracy, and more efficiently if this is required. In addition, in step-20,  [2.x.7630]  "step-21", as well as a few other of the later tutorial programs, we will revisit some vector-valued problems and show a few techniques that may make it simpler to actually go through all the stuff shown above, with  [2.x.7631]  etc. 


examples/step-8/doc/results.dox 



[1.x.2991] 


There is not much to be said about the results of this program, other than that they look nice. All images were made using VisIt from the output files that the program wrote to disk. The first two pictures show the  [2.x.7632] - and  [2.x.7633] -displacements as scalar components: 

 [2.x.7634]  


You can clearly see the sources of  [2.x.7635] -displacement around  [2.x.7636]  and  [2.x.7637] , and of  [2.x.7638] -displacement at the origin. 

What one frequently would like to do is to show the displacement as a vector field, i.e., vectors that for each point illustrate the direction and magnitude of displacement. Unfortunately, that's a bit more involved. To understand why this is so, remember that we have just defined our finite element as a collection of two  components (in  [2.x.7639]  dimensions). Nowhere have we said that this is not just a pressure and a concentration (two scalar quantities) but that the two components actually are the parts of a vector-valued quantity, namely the displacement. Absent this knowledge, the DataOut class assumes that all individual variables we print are separate scalars, and VisIt and Paraview then faithfully assume that this is indeed what it is. In other words, once we have written the data as scalars, there is nothing in these programs that allows us to paste these two scalar fields back together as a vector field. Where we would have to attack this problem is at the root, namely in  [2.x.7640] . We won't do so here but instead refer the reader to the step-22 program where we show how to do this for a more general situation. That said, we couldn't help generating the data anyway that would show how this would look if implemented as discussed in step-22. The vector field then looks like this (VisIt and Paraview randomly select a few hundred vertices from which to draw the vectors; drawing them from each individual vertex would make the picture unreadable): 

 [2.x.7641]  


We note that one may have intuitively expected the solution to be symmetric about the  [2.x.7642] - and  [2.x.7643] -axes since the  [2.x.7644] - and  [2.x.7645] -forces are symmetric with respect to these axes. However, the force considered as a vector is not symmetric and consequently neither is the solution. 


examples/step-9/doc/intro.dox 

[1.x.2992] 

[1.x.2993] 


In this example, our aims are the following: <ol>    [2.x.7646] solve the advection equation  [2.x.7647] ;    [2.x.7648] show how we can use multiple threads to get results quicker if we have a     multi-processor machine;    [2.x.7649] develop a simple refinement criterion.  [2.x.7650]  While the second aim is difficult to describe in general terms without reference to the code, we will discuss the other two aims in the following. The use of multiple threads will then be detailed at the relevant places within the program. We will, however, follow the general discussion of the WorkStream approach detailed in the  [2.x.7651]  "Parallel computing with multiple processors accessing shared memory" documentation module. 




[1.x.2994] 

In the present example program, we want to numerically approximate the solution of the advection equation 

[1.x.2995] 

where  [2.x.7652]  is a vector field that describes the advection direction and speed (which may be dependent on the space variables if  [2.x.7653] ),  [2.x.7654]  is a source function, and  [2.x.7655]  is the solution. The physical process that this equation describes is that of a given flow field  [2.x.7656] , with which another substance is transported, the density or concentration of which is given by  [2.x.7657] . The equation does not contain diffusion of this second species within its carrier substance, but there are source terms. 

It is obvious that at the inflow, the above equation needs to be augmented by boundary conditions: 

[1.x.2996] 

where  [2.x.7658]  describes the inflow portion of the boundary and is formally defined by 

[1.x.2997] 

and  [2.x.7659]  being the outward normal to the domain at point  [2.x.7660] . This definition is quite intuitive, since as  [2.x.7661]  points outward, the scalar product with  [2.x.7662]  can only be negative if the transport direction  [2.x.7663]  points inward, i.e. at the inflow boundary. The mathematical theory states that we must not pose any boundary condition on the outflow part of the boundary. 

Unfortunately, the equation stated above cannot be solved in a stable way using the standard finite element method. The problem is that solutions to this equation possess insufficient regularity perpendicular to the transport direction: while they are smooth along the streamlines defined by the "wind field"  [2.x.7664] , they may be discontinuous perpendicular to this direction. This is easy to understand: what the equation  [2.x.7665]  means is in essence that the [1.x.2998]. But the equation has no implications for the derivatives in the perpendicular direction, and consequently if  [2.x.7666]  is discontinuous at a point on the inflow boundary, then this discontinuity will simply be transported along the streamline of the wind field that starts at this boundary point. These discontinuities lead to numerical instabilities that make a stable solution by a standard continuous finite element discretization impossible. 

A standard approach to address this difficulty is the  [2.x.7667] "streamline-upwind Petrov-Galerkin" [2.x.7668]  (SUPG) method, sometimes also called the streamline diffusion method. A good explanation of the method can be found in  [2.x.7669]  . Formally, this method replaces the step in which we derive the the weak form of the differential equation from the strong form: Instead of multiplying the equation by a test function  [2.x.7670]  and integrating over the domain, we instead multiply by  [2.x.7671] , where  [2.x.7672]  is a parameter that is chosen in the range of the (local) mesh width  [2.x.7673] ; good results are usually obtained by setting  [2.x.7674] . (Why this is called "streamline diffusion" will be explained below; for the moment, let us simply take for granted that this is how we derive a stable discrete formulation.) The value for  [2.x.7675]  here is small enough that we do not introduce excessive diffusion, but large enough that the resulting problem is well-posed. 

Using the test functions as defined above, an initial weak form of the problem would ask for finding a function  [2.x.7676]  so that for all test functions  [2.x.7677]  we have 

[1.x.2999] 

However, we would like to include inflow boundary conditions  [2.x.7678]  weakly into this problem, and this can be done by requiring that in addition to the equation above we also have 

[1.x.3000] 

for all test functions  [2.x.7679]  that live on the boundary and that are from a suitable test space. It turns out that a suitable space of test functions happens to be  [2.x.7680]  times the traces of the functions  [2.x.7681]  in the test space we already use for the differential equation in the domain. Thus, we require that for all test functions  [2.x.7682]  we have 

[1.x.3001] 

Without attempting a justification (see again the literature on the finite element method in general, and the streamline diffusion method in particular), we can combine the equations for the differential equation and the boundary values in the following weak formulation of our stabilized problem: find a discrete function  [2.x.7683]  such that for all discrete test functions  [2.x.7684]  there holds 

[1.x.3002] 




One would think that this leads to a system matrix to be inverted of the form 

[1.x.3003] 

with basis functions  [2.x.7685] .  However, this is a pitfall that happens to every numerical analyst at least once (including the author): we have here expanded the solution  [2.x.7686] , but if we do so, we will have to solve the problem 

[1.x.3004] 

where  [2.x.7687]  is the vector of expansion coefficients, i.e., we have to solve the transpose problem of what we might have expected naively. 

This is a point we made in the introduction of step-3. There, we argued that to avoid this very kind of problem, one should get in the habit of always multiplying with test functions [1.x.3005] instead of from the right to obtain the correct matrix right away. In order to obtain the form of the linear system that we need, it is therefore best to rewrite the weak formulation to 

[1.x.3006] 

and then to obtain 

[1.x.3007] 

as system matrix. We will assemble this matrix in the program. 




[1.x.3008] 

Looking at the bilinear form mentioned above, we see that the discrete solution has to satisfy an equation of which the left hand side in weak form has a domain term of the kind 

[1.x.3009] 

or if we split this up, the form 

[1.x.3010] 

If we wanted to see what strong form of the equation that would correspond to, we need to integrate the second term. This yields the following formulation, where for simplicity we'll ignore boundary terms for now: 

[1.x.3011] 

Let us assume for a moment that the wind field  [2.x.7688]  is divergence-free, i.e., that  [2.x.7689] . Then applying the product rule to the derivative of the term in square brackets on the right and using the divergence-freeness will give us the following: 

[1.x.3012] 

That means that the strong form of the equation would be of the sort 

[1.x.3013] 

What is important to recognize now is that  [2.x.7690]  is the  [2.x.7691] derivative in direction  [2.x.7692]  [2.x.7693] . So, if we denote this by  [2.x.7694]  (in the same way as we often write  [2.x.7695]  for the derivative in normal direction at the boundary), then the strong form of the equation is 

[1.x.3014] 

In other words, the unusual choice of test function is equivalent to the addition of term to the strong form that corresponds to a second order (i.e., diffusion) differential operator in the direction of the wind field  [2.x.7696] , i.e., in "streamline direction". A fuller account would also have to explore the effect of the test function on boundary values and why it is necessary to also use the same test function for the right hand side, but the discussion above might make clear where the name "streamline diffusion" for the method originates from. 




[1.x.3015] 

A "Galerkin method" is one where one obtains the weak formulation by multiplying the equation by a test function  [2.x.7697]  (and then integrating over  [2.x.7698] ) where the functions  [2.x.7699]  are from the same space as the solution  [2.x.7700]  (though possibly with different boundary values). But this is not strictly necessary: One could also imagine choosing the test functions from a different set of functions, as long as that different set has "as many dimensions" as the original set of functions so that we end up with as many independent equations as there are degrees of freedom (where all of this needs to be appropriately defined in the infinite-dimensional case). Methods that make use of this possibility (i.e., choose the set of test functions differently than the set of solutions) are called "Petrov-Galerkin" methods. In the current case, the test functions all have the form  [2.x.7701]  where  [2.x.7702]  is from the set of solutions. 




[1.x.3016] 

[Upwind methods](https://en.wikipedia.org/wiki/Upwind_scheme) have a long history in the derivation of stabilized schemes for advection equations. Generally, the idea is that instead of looking at a function "here", we look at it a small distance further "upstream" or "upwind", i.e., where the information "here" originally came from. This might suggest not considering  [2.x.7703] , but something like  [2.x.7704] . Or, equivalently upon integration, we could evaluate  [2.x.7705]  and instead consider  [2.x.7706]  a bit downstream:  [2.x.7707] . This would be cumbersome for a variety of reasons: First, we would have to define what  [2.x.7708]  should be if  [2.x.7709]  happens to be outside  [2.x.7710] ; second, computing integrals numerically would be much more awkward since we no longer evaluate  [2.x.7711]  and  [2.x.7712]  at the same quadrature points. But since we assume that  [2.x.7713]  is small, we can do a Taylor expansion: 

[1.x.3017] 

This form for the test function should by now look familiar. 




[1.x.3018] 

As the resulting matrix is no longer symmetric positive definite, we cannot use the usual Conjugate Gradient method (implemented in the SolverCG class) to solve the system. Instead, we use the GMRES (Generalized Minimum RESidual) method (implemented in SolverGMRES) that is suitable for problems of the kind we have here. 




[1.x.3019] 

For the problem which we will solve in this tutorial program, we use the following domain and functions (in  [2.x.7714]  space dimensions): 

[1.x.3020] 

For  [2.x.7715] , we extend  [2.x.7716]  and  [2.x.7717]  by simply duplicating the last of the components shown above one more time. 

With all of this, the following comments are in order: <ol>  [2.x.7718]  The advection field  [2.x.7719]  transports the solution roughly in diagonal direction from lower left to upper right, but with a wiggle structure superimposed.  [2.x.7720]  The right hand side adds to the field generated by the inflow boundary conditions a blob in the lower left corner, which is then transported along.  [2.x.7721]  The inflow boundary conditions impose a weighted sinusoidal structure that is transported along with the flow field. Since  [2.x.7722]  on the boundary, the weighting term never gets very large.  [2.x.7723]  




[1.x.3021] 

In all previous examples with adaptive refinement, we have used an error estimator first developed by Kelly et al., which assigns to each cell  [2.x.7724]  the following indicator: 

[1.x.3022] 

where  [2.x.7725]  denotes the jump of the normal derivatives across a face  [2.x.7726]  of the cell  [2.x.7727] . It can be shown that this error indicator uses a discrete analogue of the second derivatives, weighted by a power of the cell size that is adjusted to the linear elements assumed to be in use here: 

[1.x.3023] 

which itself is related to the error size in the energy norm. 

The problem with this error indicator in the present case is that it assumes that the exact solution possesses second derivatives. This is already questionable for solutions to Laplace's problem in some cases, although there most problems allow solutions in  [2.x.7728] . If solutions are only in  [2.x.7729] , then the second derivatives would be singular in some parts (of lower dimension) of the domain and the error indicators would not reduce there under mesh refinement. Thus, the algorithm would continuously refine the cells around these parts, i.e. would refine into points or lines (in 2d). 

However, for the present case, solutions are usually not even in  [2.x.7730]  (and this missing regularity is not the exceptional case as for Laplace's equation), so the error indicator described above is not really applicable. We will thus develop an indicator that is based on a discrete approximation of the gradient. Although the gradient often does not exist, this is the only criterion available to us, at least as long as we use continuous elements as in the present example. To start with, we note that given two cells  [2.x.7731] ,  [2.x.7732]  of which the centers are connected by the vector  [2.x.7733] , we can approximate the directional derivative of a function  [2.x.7734]  as follows: 

[1.x.3024] 

where  [2.x.7735]  and  [2.x.7736]  denote  [2.x.7737]  evaluated at the centers of the respective cells. We now multiply the above approximation by  [2.x.7738]  and sum over all neighbors  [2.x.7739]  of  [2.x.7740] : 

[1.x.3025] 

If the vectors  [2.x.7741]  connecting  [2.x.7742]  with its neighbors span the whole space (i.e. roughly:  [2.x.7743]  has neighbors in all directions), then the term in parentheses in the left hand side expression forms a regular matrix, which we can invert to obtain an approximation of the gradient of  [2.x.7744]  on  [2.x.7745] : 

[1.x.3026] 

We will denote the approximation on the right hand side by  [2.x.7746] , and we will use the following quantity as refinement criterion: 

[1.x.3027] 

which is inspired by the following (not rigorous) argument: 

[1.x.3028] 




examples/step-9/doc/results.dox 



[1.x.3029] 


The results of this program are not particularly spectacular. They consist of the console output, some grid files, and the solution on each of these grids. First for the console output: 

[1.x.3030] 



Quite a number of cells are used on the finest level to resolve the features of the solution. Here are the fourth and tenth grids: <div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-9-grid-3.png"          alt="Fourth grid in the refinement cycle, showing some adaptivity to features."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-9-grid-9.png"          alt="Tenth grid in the refinement cycle, showing that the waves are fully captured."          width="400" height="400">   </div> </div> and the fourth and tenth solutions: <div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-9-solution-3.png"          alt="Fourth solution, showing that we resolve most features but some          are sill unresolved and appear blury."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-9-solution-9.png"          alt="Tenth solution, showing a fully resolved flow."          width="400" height="400">   </div> </div> and both the grid and solution zoomed in: <div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-9-solution-3-zoom.png"          alt="Detail of the fourth solution, showing that we resolve most          features but some are sill unresolved and appear blury. In particular,          the larger cells need to be refined."          width="400" height="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-9-solution-9-zoom.png"          alt="Detail of the tenth solution, showing that we needed a lot more          cells than were present in the fourth solution."          width="400" height="400">   </div> </div> 

The solution is created by that part that is transported along the wiggly advection field from the left and lower boundaries to the top right, and the part that is created by the source in the lower left corner, and the results of which are also transported along. The grid shown above is well-adapted to resolve these features. The comparison between plots shows that, even though we are using a high-order approximation, we still need adaptive mesh refinement to fully resolve the wiggles. 


