ls. 




[1.x.1021] 

Running convection codes in 3d with significant Rayleigh numbers requires a lot of computations &mdash; in the case of whole earth simulations on the order of one or several hundred million unknowns. This can obviously not be done with a single machine any more (at least not in 2010 when we started writing this code). Consequently, we need to parallelize it. Parallelization of scientific codes across multiple machines in a cluster of computers is almost always done using the Message Passing Interface (MPI). This program is no exception to that, and it follows the general spirit of the step-17 and step-18 programs in this though in practice it borrows more from step-40 in which we first introduced the classes and strategies we use when we want to [1.x.1022] distribute all computations, and step-55 that shows how to do that for  [2.x.2822]  "vector-valued problems": including, for example, splitting the mesh up into a number of parts so that each processor only stores its own share plus some ghost cells, and using strategies where no processor potentially has enough memory to hold the entries of the combined solution vector locally. The goal is to run this code on hundreds or maybe even thousands of processors, at reasonable scalability. 

 [2.x.2823]  Even though it has a larger number, step-40 comes logically before the current program. The same is true for step-55. You will probably want to look at these programs before you try to understand what we do here. 

MPI is a rather awkward interface to program with. It is a semi-object oriented set of functions, and while one uses it to send data around a network, one needs to explicitly describe the data types because the MPI functions insist on getting the address of the data as  [2.x.2824]  objects rather than deducing the data type automatically through overloading or templates. We've already seen in step-17 and step-18 how to avoid almost all of MPI by putting all the communication necessary into either the deal.II library or, in those programs, into PETSc. We'll do something similar here: like in step-40 and step-55, deal.II and the underlying p4est library are responsible for all the communication necessary for distributing the mesh, and we will let the Trilinos library (along with the wrappers in namespace TrilinosWrappers) deal with parallelizing the linear algebra components. We have already used Trilinos in step-31, and will do so again here, with the difference that we will use its %parallel capabilities. 

Trilinos consists of a significant number of packages, implementing basic %parallel linear algebra operations (the Epetra package), different solver and preconditioner packages, and on to things that are of less importance to deal.II (e.g., optimization, uncertainty quantification, etc). deal.II's Trilinos interfaces encapsulate many of the things Trilinos offers that are of relevance to PDE solvers, and provides wrapper classes (in namespace TrilinosWrappers) that make the Trilinos matrix, vector, solver and preconditioner classes look very much the same as deal.II's own implementations of this functionality. However, as opposed to deal.II's classes, they can be used in %parallel if we give them the necessary information. As a consequence, there are two Trilinos classes that we have to deal with directly (rather than through wrappers), both of which are part of Trilinos' Epetra library of basic linear algebra and tool classes:  [2.x.2825]   [2.x.2826]  The Epetra_Comm class is an abstraction of an MPI "communicator", i.e.,   it describes how many and which machines can communicate with each other.   Each distributed object, such as a sparse matrix or a vector for which we   may want to store parts on different machines, needs to have a communicator   object to know how many parts there are, where they can be found, and how   they can be accessed. 

  In this program, we only really use one communicator object -- based on the   MPI variable  [2.x.2827]  -- that encompasses [1.x.1023]   processes that work together. It would be perfectly legitimate to start a   process on  [2.x.2828]  machines but only store vectors on a subset of these by   producing a communicator object that only encompasses this subset of   machines; there is really no compelling reason to do so here, however. 

 [2.x.2829]  The IndexSet class is used to describe which elements of a vector or which   rows of a matrix should reside on the current machine that is part of a   communicator. To create such an object, you need to know (i) the total   number of elements or rows, (ii) the indices of the elements you want to   store locally. We will set up these  [2.x.2830]  in the    [2.x.2831]  function below and then hand   it to every %parallel object we create. 

  Unlike PETSc, Trilinos makes no assumption that the elements of a vector   need to be partitioned into contiguous chunks. At least in principle, we   could store all elements with even indices on one processor and all odd ones   on another. That's not very efficient, of course, but it's   possible. Furthermore, the elements of these partitionings do not   necessarily be mutually exclusive. This is important because when   postprocessing solutions, we need access to all locally relevant or at least   the locally active degrees of freedom (see the module on  [2.x.2832]    for a definition, as well as the discussion in step-40). Which elements the   Trilinos vector considers as locally owned is not important to us then. All   we care about is that it stores those elements locally that we need.  [2.x.2833]  

There are a number of other concepts relevant to distributing the mesh to a number of processors; you may want to take a look at the  [2.x.2834]  module and step-40 or step-55 before trying to understand this program.  The rest of the program is almost completely agnostic about the fact that we don't store all objects completely locally. There will be a few points where we have to limit loops over all cells to those that are locally owned, or where we need to distinguish between vectors that store only locally owned elements and those that store everything that is locally relevant (see  [2.x.2835]  "this glossary entry"), but by and large the amount of heavy lifting necessary to make the program run in %parallel is well hidden in the libraries upon which this program builds. In any case, we will comment on these locations as we get to them in the program code. 




[1.x.1024] 

The second strategy to parallelize a program is to make use of the fact that most computers today have more than one processor that all have access to the same memory. In other words, in this model, we don't explicitly have to say which pieces of data reside where -- all of the data we need is directly accessible and all we have to do is split [1.x.1025] this data between the available processors. We will then couple this with the MPI parallelization outlined above, i.e., we will have all the processors on a machine work together to, for example, assemble the local contributions to the global matrix for the cells that this machine actually "owns" but not for those cells that are owned by other machines. We will use this strategy for four kinds of operations we frequently do in this program: assembly of the Stokes and temperature matrices, assembly of the matrix that forms the Stokes preconditioner, and assembly of the right hand side of the temperature system. 

All of these operations essentially look as follows: we need to loop over all cells for which  [2.x.2836]  equals the index our machine has within the communicator object used for all communication (i.e.,  [2.x.2837] , as explained above). The test we are actually going to use for this, and which describes in a concise way why we test this condition, is  [2.x.2838] . On each such cell we need to assemble the local contributions to the global matrix or vector, and then we have to copy each cell's contribution into the global matrix or vector. Note that the first part of this (the loop) defines a range of iterators on which something has to happen. The second part, assembly of local contributions is something that takes the majority of CPU time in this sequence of steps, and is a typical example of things that can be done in %parallel: each cell's contribution is entirely independent of all other cells' contributions. The third part, copying into the global matrix, must not happen in %parallel since we are modifying one object and so several threads can not at the same time read an existing matrix element, add their contribution, and write the sum back into memory without danger of producing a [1.x.1026]. 

deal.II has a class that is made for exactly this workflow: WorkStream, first discussed in step-9 and step-13. Its use is also extensively documented in the module on  [2.x.2839]  (in the section on  [2.x.2840]  "the WorkStream class") and we won't repeat here the rationale and detailed instructions laid out there, though you will want to read through this module to understand the distinction between scratch space and per-cell data. Suffice it to mention that we need the following: 

- An iterator range for those cells on which we are supposed to work. This is   provided by the FilteredIterator class which acts just like every other cell   iterator in deal.II with the exception that it skips all cells that do not   satisfy a particular predicate (i.e., a criterion that evaluates to true or   false). In our case, the predicate is whether a cell is locally owned. 

- A function that does the work on each cell for each of the tasks identified   above, i.e., functions that assemble the local contributions to Stokes matrix   and preconditioner, temperature matrix, and temperature right hand   side. These are the    [2.x.2841] ,    [2.x.2842] ,    [2.x.2843] , and    [2.x.2844]  functions in   the code below. These four functions can all have several instances   running in %parallel at the same time. 

- %Functions that copy the result of the previous ones into the global object   and that run sequentially to avoid race conditions. These are the    [2.x.2845] ,    [2.x.2846] ,    [2.x.2847] , and    [2.x.2848]    functions. 

We will comment on a few more points in the actual code, but in general their structure should be clear from the discussion in  [2.x.2849] . 

The underlying technology for WorkStream identifies "tasks" that need to be worked on (e.g. assembling local contributions on a cell) and schedules these tasks automatically to available processors. WorkStream creates these tasks automatically, by splitting the iterator range into suitable chunks. 

 [2.x.2850]  Using multiple threads within each MPI process only makes sense if you have fewer MPI processes running on each node of your cluster than there are processor cores on this machine. Otherwise, MPI will already keep your processors busy and you won't get any additional speedup from using threads. For example, if your cluster nodes have 8 cores as they often have at the time of writing this, and if your batch scheduler puts 8 MPI processes on each node, then using threads doesn't make the program any faster. Consequently, you probably want to either configure your deal.II without threads, or set the number of threads in  [2.x.2851]  to 1 (third argument), or "export DEAL_II_NUM_THREADS=1" before running. That said, at the time of writing this, we only use the WorkStream class for assembling (parts of) linear systems, while 75% or more of the run time of the program is spent in the linear solvers that are not parallelized &mdash; in other words, the best we could hope is to parallelize the remaining 25%. 




[1.x.1027] 

The setup for this program is mildly reminiscent of the problem we wanted to solve in the first place (see the introduction of step-31): convection in the earth mantle. As a consequence, we choose the following data, all of which appears in the program in units of meters and seconds (the SI system) even if we list them here in other units. We do note, however, that these choices are essentially still only exemplary, and not meant to result in a completely realistic description of convection in the earth mantle: for that, more and more difficult physics would have to be implemented, and several other aspects are currently missing from this program as well. We will come back to this issue in the results section again, but state for now that providing a realistic description is a goal of the [1.x.1028] code in development at the time of writing this. 

As a reminder, let us again state the equations we want to solve are these: 

[1.x.1029] 

augmented by boundary and initial conditions. We then have to choose data for the following quantities:  [2.x.2852]     [2.x.2853] The domain is an annulus (in 2d) or a spherical shell (in 3d) with inner   and outer radii that match that of the earth: the total radius of the earth   is 6371km, with the mantle starting at a depth of around 35km (just under   the solid earth [1.x.1030] composed of   [1.x.1031] and [1.x.1032]) to a depth of 2890km (where the   [1.x.1033] starts). The radii are therefore  [2.x.2854] . This domain is conveniently generated using the    [2.x.2855]  function. 

   [2.x.2856] At the interface between crust and mantle, the temperature is between   500 and 900 degrees Celsius, whereas at its bottom it is around 4000 degrees   Celsius (see, for example, [1.x.1034]). In Kelvin, we therefore choose  [2.x.2857] ,    [2.x.2858]  as boundary conditions at the inner and outer edge. 

  In addition to this, we also have to specify some initial conditions for   the temperature field. The real temperature field of the earth is quite   complicated as a consequence of the convection that has been going on for   more than four billion years -- in fact, it is the properties of this   temperature distribution that we want to explore with programs like   this. As a consequence, we   don't really have anything useful to offer here, but we can hope that if we   start with something and let things run for a while that the exact initial   conditions don't matter that much any more &mdash; as is in fact suggested   by looking at the pictures shown in the [1.x.1035]. The initial temperature field we use here is given in terms of   the radius by   [1.x.1036] 

  where   [1.x.1037] 

  This complicated function is essentially a perturbation of a linear profile   between the inner and outer temperatures. In 2d, the function    [2.x.2859]  looks like this (I got the picture from   [1.x.1038]): 

   [2.x.2860]  

  The point of this profile is that if we had used  [2.x.2861]  instead of  [2.x.2862]  in   the definition of  [2.x.2863]  then it would simply be a linear   interpolation.  [2.x.2864]  has the same function values as  [2.x.2865]  on the inner and   outer boundaries (zero and one, respectively), but it stretches the   temperature profile a bit depending on the angle and the  [2.x.2866]  value in 3d,   producing an angle-dependent perturbation of the linearly interpolating   field. We will see in the results section that this is an   entirely unphysical temperature field (though it will make for   interesting images) as the equilibrium state for the temperature   will be an almost constant temperature with boundary layers at the   inner and outer boundary. 

   [2.x.2867] The right hand side of the temperature equation contains the rate of   %internal heating  [2.x.2868] . The earth does heat naturally through several mechanisms:   radioactive decay, chemical separation (heavier elements sink to the bottom,   lighter ones rise to the top; the countercurrents dissipate energy equal to   the loss of potential energy by this separation process); heat release   by crystallization of liquid metal as the solid inner core of the earth   grows; and heat dissipation from viscous friction as the fluid moves. 

  Chemical separation is difficult to model since it requires modeling mantle   material as multiple phases; it is also a relatively small   effect. Crystallization heat is even more difficult since it is confined to   areas where temperature and pressure allow for phase changes, i.e., a   discontinuous process. Given the difficulties in modeling these two   phenomena, we will neglect them. 

  The other two are readily handled and, given the way we scaled the   temperature equation, lead to the equation   [1.x.1039] 

  where  [2.x.2869]  is the radiogenic heating in  [2.x.2870] , and the second   term in the enumerator is viscous friction heating.  [2.x.2871]  is the density   and  [2.x.2872]  is the specific heat. The literature provides the following   approximate values:  [2.x.2873] .   The other parameters are discussed elsewhere in this section. 

  We neglect one internal heat source, namely adiabatic heating here,   which will lead to a surprising temperature field. This point is   commented on in detail in the results section below. 

   [2.x.2874] For the velocity we choose as boundary conditions  [2.x.2875]  at the   inner radius (i.e., the fluid sticks to the earth core) and    [2.x.2876]  at the outer radius (i.e., the fluid flows   tangentially along the bottom of the earth crust). Neither of these is   physically overly correct: certainly, on both boundaries, fluids can flow   tangentially, but they will incur a shear stress through friction against   the medium at the other side of the interface (the metallic core and the   crust, respectively). Such a situation could be modeled by a Robin-type   boundary condition for the tangential velocity; in either case, the normal (vertical)   velocity would be zero, although even that is not entirely correct since   continental plates also have vertical motion (see, for example, the   phenomenon of [1.x.1040]). But to already make things worse for the tangential velocity,   the medium on the other side is in motion as well, so the shear stress   would, in the simplest case, be proportional to the [1.x.1041], leading to a boundary condition of the form   [1.x.1042] 

  with a proportionality constant  [2.x.2877] . Rather than going down this route,   however, we go with the choice of zero (stick) and tangential   flow boundary conditions. 

  As a side note of interest, we may also have chosen tangential flow   conditions on both inner and outer boundary. That has a significant   drawback, however: it leaves the velocity not uniquely defined. The reason   is that all velocity fields  [2.x.2878]  that correspond to a solid   body rotation around the center of the domain satisfy  [2.x.2879] , and    [2.x.2880] . As a consequence, if  [2.x.2881]    satisfies equations and boundary conditions, then so does  [2.x.2882] . That's certainly not a good situation that we would like   to avoid. The traditional way to work around this is to pick an arbitrary   point on the boundary and call this your fixed point by choosing the   velocity to be zero in all components there. (In 3d one has to choose two   points.) Since this program isn't meant to be too realistic to begin with,   we avoid this complication by simply fixing the velocity along the entire   interior boundary. 

   [2.x.2883] To first order, the gravity vector always points downward. The question for   a body as big as the earth is just: where is "up". The naive answer of course is   "radially inward, towards the center of the earth". So at the surface of the   earth, we have   [1.x.1043] 

  where  [2.x.2884]  happens to be the average gravity   acceleration at the earth surface. But in the earth interior, the question   becomes a bit more complicated: at the (bary-)center of the earth, for   example, you have matter pulling equally hard in all directions, and so    [2.x.2885] . In between, the net force is described as follows: let us   define the [1.x.1044] by   [1.x.1045] 

  then  [2.x.2886] . If we assume that   the density  [2.x.2887]  is constant throughout the earth, we can produce an   analytical expression for the gravity vector (don't try to integrate above   equation somehow -- it leads to elliptic integrals; a simpler way is to   notice that  [2.x.2888]  and solving this   partial differential equation in all of  [2.x.2889]  exploiting the   radial symmetry):   [1.x.1046] 

  The factor  [2.x.2890]  is the unit vector pointing   radially inward. Of course, within this problem, we are only interested in   the branch that pertains to within the earth, i.e.,  [2.x.2891] . We would therefore only consider the expression   [1.x.1047] 

  where we can infer the last expression because we know Earth's gravity at   the surface (where  [2.x.2892] ). 

  One can derive a more general expression by integrating the   differential equation for  [2.x.2893]  in the case that the density   distribution is radially symmetric, i.e.,  [2.x.2894] . In that case, one would get   [1.x.1048] 




  There are two problems with this, however: (i) The Earth is not homogeneous,   i.e., the density  [2.x.2895]  depends on  [2.x.2896] ; in fact it is not even a   function that only depends on the radius  [2.x.2897] . In reality, gravity therefore   does not always decrease as we get deeper: because the earth core is so much   denser than the mantle, gravity actually peaks at around  [2.x.2898]  at the core mantle boundary (see [1.x.1049]). (ii) The density, and by   consequence the gravity vector, is not even constant in time: after all, the   problem we want to solve is the time dependent upwelling of hot, less dense   material and the downwelling of cold dense material. This leads to a gravity   vector that varies with space and time, and does not always point straight   down. 

  In order to not make the situation more complicated than necessary, we could   use the approximation that at the inner boundary of the mantle,   gravity is  [2.x.2899]  and at the outer   boundary it is  [2.x.2900] , in each case   pointing radially inward, and that in between gravity varies   linearly with the radial distance from the earth center. That said, it isn't   that hard to actually be slightly more realistic and assume (as we do below)   that the earth mantle has constant density. In that case, the equation above   can be integrated and we get an expression for  [2.x.2901]  where we   can fit constants to match the gravity at the top and bottom of the earth   mantle to obtain   [1.x.1050] 



   [2.x.2902] The density of the earth mantle varies spatially, but not by very   much.  [2.x.2903]  is a relatively good average   value for the density at reference temperature  [2.x.2904]  Kelvin. 

   [2.x.2905] The thermal expansion coefficient  [2.x.2906]  also varies with depth   (through its dependence on temperature and pressure). Close to the surface,   it appears to be on the order of  [2.x.2907] ,   whereas at the core mantle boundary, it may be closer to  [2.x.2908] . As a reasonable value, let us choose    [2.x.2909] . The density as a function   of temperature is then    [2.x.2910] . 

   [2.x.2911] The second to last parameter we need to specify is the viscosity    [2.x.2912] . This is a tough one, because rocks at the temperatures and pressure   typical for the earth mantle flow so slowly that the viscosity can not be   determined accurately in the laboratory. So how do we know about the   viscosity of the mantle? The most commonly used route is to consider that   during and after ice ages, ice shields form and disappear on time scales   that are shorter than the time scale of flow in the mantle. As a   consequence, continents slowly sink into the earth mantle under the added   weight of an ice shield, and they rise up again slowly after the ice shield   has disappeared again (this is called [1.x.1051][1.x.1052]). By measuring the speed of this rebound, we can infer the   viscosity of the material that flows into the area vacated under the   rebounding continental plates. 

  Using this technique, values around  [2.x.2913]  have been found as the most   likely, though the error bar on this is at least one order of magnitude. 

  While we will use this value, we again have to caution that there are many   physical reasons to assume that this is not the correct value. First, it   should really be made dependent on temperature: hotter material is most   likely to be less viscous than colder material. In reality, however, the   situation is even more complex. Most rocks in the mantle undergo phase   changes as temperature and pressure change: depending on temperature and   pressure, different crystal configurations are thermodynamically favored   over others, even if the chemical composition of the mantle were   homogeneous. For example, the common mantle material MgSiO<sub>3</sub> exists   in its [1.x.1053] throughout most of the mantle, but in the lower mantle the   same substance is stable only as [1.x.1054]. Clearly,   to compute realistic viscosities, we would not only need to know the exact   chemical composition of the mantle and the viscosities of all materials, but   we would also have to compute the thermodynamically most stable   configurations for all materials at each quadrature point. This is at the   time of writing this program not a feasible suggestion. 

   [2.x.2914] Our last material parameter is the thermal diffusivity  [2.x.2915] , which   is defined as  [2.x.2916]  where  [2.x.2917]  is the thermal   conductivity,  [2.x.2918]  the density, and  [2.x.2919]  the specific heat. For   this, the literature indicates that it increases from around  [2.x.2920]  in the   upper mantle to around  [2.x.2921]  in the lower   mantle, though the exact value   is not really all that important: heat transport through convection is   several orders of magnitude more important than through thermal   conduction. It may be of interest to know that perovskite, the most abundant   material in the earth mantle, appears to become transparent at pressures   above around 120 GPa (see, for example, J. Badro et al., Science 305,   383-386 (2004)); in the lower mantle, it may therefore be that heat   transport through radiative transfer is more efficient than through thermal   conduction. 

  In view of these considerations, let us choose    [2.x.2922]    for the purpose of this program.  [2.x.2923]  

All of these pieces of equation data are defined in the program in the  [2.x.2924]  namespace. When run, the program produces long-term maximal velocities around 10-40 centimeters per year (see the results section below), approximately the physically correct order of magnitude. We will set the end time to 1 billion years. 

 [2.x.2925]  The choice of the constants and material parameters above follows in large part the comprehensive book "Mantle Convection in the Earth and Planets, Part 1" by G. Schubert and D. L. Turcotte and P. Olson (Cambridge, 2001). It contains extensive discussion of ways to make the program more realistic. 




[1.x.1055] 

Compared to step-31, this program has a number of noteworthy differences: 

- The  [2.x.2926]  namespace is significantly larger, reflecting   the fact that we now have much more physics to deal with. That said, most of   this additional physical detail is rather self-contained in functions in   this one namespace, and does not proliferate throughout the rest of the   program. 

- Of more obvious visibility is the fact that we have put a good number of   parameters into an input file handled by the ParameterHandler class (see,   for example, step-29, for ways to set up run-time parameter files with this   class). This often makes sense when one wants to avoid re-compiling the   program just because one wants to play with a single parameter (think, for   example, of parameter studies determining the best values of the   stabilization constants discussed above), in particular given that it takes   a nontrivial amount of time to re-compile programs of the current size. To   just give an overview of the kinds of parameters we have moved from fixed   values into the input file, here is a listing of a typical    [2.x.2927]  file:   [1.x.1056] 



- There are, obviously, a good number of changes that have to do with the fact   that we want to run our program on a possibly very large number of   machines. Although one may suspect that this requires us to completely   re-structure our code, that isn't in fact the case (although the classes   that implement much of this functionality in deal.II certainly look very   different from an implementation viewpoint, but this doesn't reflect in   their public interface). Rather, the changes are mostly subtle, and the   overall structure of the main class is pretty much unchanged. That said, the   devil is in the detail: getting %parallel computing right, without   deadlocks, ensuring that the right data is available at the right place   (see, for example, the discussion on fully distributed vectors vs. vectors   with ghost elements), and avoiding bottlenecks is difficult and discussions   on this topic will appear in a good number of places in this program. 




[1.x.1057] 

This is a tutorial program. That means that at least most of its focus needs to lie on demonstrating ways of using deal.II and associated libraries, and not diluting this teaching lesson by focusing overly much on physical details. Despite the lengthy section above on the choice of physical parameters, the part of the program devoted to this is actually quite short and self contained. 

That said, both step-31 and the current step-32 have not come about by chance but are certainly meant as wayposts along the path to a more comprehensive program that will simulate convection in the earth mantle. We call this code [1.x.1058] (short for [1.x.1059]); its development is funded by the [1.x.1060] initiative with support from the National Science Foundation. More information on [1.x.1061] is available at its [1.x.1062]. 


examples/step-32/doc/results.dox 



[1.x.1063] 

When run, the program simulates convection in 3d in much the same way as step-31 did, though with an entirely different testcase. 




[1.x.1064] 

Before we go to this testcase, however, let us show a few results from a slightly earlier version of this program that was solving exactly the testcase we used in step-31, just that we now solve it in parallel and with much higher resolution. We show these results mainly for comparison. 

Here are two images that show this higher resolution if we choose a 3d computation in  [2.x.2928]  and if we set  [2.x.2929]  and  [2.x.2930] . At the time steps shown, the meshes had around 72,000 and 236,000 cells, for a total of 2,680,000 and 8,250,000 degrees of freedom, respectively, more than an order of magnitude more than we had available in step-31: 

 [2.x.2931]  

The computation was done on a subset of 50 processors of the Brazos cluster at Texas A&amp;M University. 




[1.x.1065] 

Next, we will run step-32 with the parameter file in the directory with one change: we increase the final time to 1e9. Here we are using 16 processors. The command to launch is (note that step-32.prm is the default): 

<code> <pre> \ [2.x.2932]  mpirun -np 16 ./step-32 Number of active cells: 12,288 (on 6 levels) Number of degrees of freedom: 186,624 (99,840+36,864+49,920) 

Timestep 0:  t=0 years 

   Rebuilding Stokes preconditioner...    Solving Stokes system... 41 iterations.    Maximal velocity: 60.4935 cm/year    Time step: 18166.9 years    17 CG iterations for temperature    Temperature range: 973 4273.16 

Number of active cells: 15,921 (on 7 levels) Number of degrees of freedom: 252,723 (136,640+47,763+68,320) 

Timestep 0:  t=0 years 

   Rebuilding Stokes preconditioner...    Solving Stokes system... 50 iterations.    Maximal velocity: 60.3223 cm/year    Time step: 10557.6 years    19 CG iterations for temperature    Temperature range: 973 4273.16 

Number of active cells: 19,926 (on 8 levels) Number of degrees of freedom: 321,246 (174,312+59,778+87,156) 

Timestep 0:  t=0 years 

   Rebuilding Stokes preconditioner...    Solving Stokes system... 50 iterations.    Maximal velocity: 57.8396 cm/year    Time step: 5453.78 years    18 CG iterations for temperature    Temperature range: 973 4273.16 

Timestep 1:  t=5453.78 years 

   Solving Stokes system... 49 iterations.    Maximal velocity: 59.0231 cm/year    Time step: 5345.86 years    18 CG iterations for temperature    Temperature range: 973 4273.16 

Timestep 2:  t=10799.6 years 

   Solving Stokes system... 24 iterations.    Maximal velocity: 60.2139 cm/year    Time step: 5241.51 years    17 CG iterations for temperature    Temperature range: 973 4273.16 

[...] 

Timestep 100:  t=272151 years 

   Solving Stokes system... 21 iterations.    Maximal velocity: 161.546 cm/year    Time step: 1672.96 years    17 CG iterations for temperature    Temperature range: 973 4282.57 

Number of active cells: 56,085 (on 8 levels) Number of degrees of freedom: 903,408 (490,102+168,255+245,051) 




+---------------------------------------------+------------+------------+ | Total wallclock time elapsed since start    |       115s |            | |                                             |            |            | | Section                         | no. calls |  wall time | % of total | +---------------------------------+-----------+------------+------------+ | Assemble Stokes system          |       103 |      2.82s |       2.5% | | Assemble temperature matrices   |        12 |     0.452s |      0.39% | | Assemble temperature rhs        |       103 |      11.5s |        10% | | Build Stokes preconditioner     |        12 |      2.09s |       1.8% | | Solve Stokes system             |       103 |      90.4s |        79% | | Solve temperature system        |       103 |      1.53s |       1.3% | | Postprocessing                  |         3 |     0.532s |      0.46% | | Refine mesh structure, part 1   |        12 |      0.93s |      0.81% | | Refine mesh structure, part 2   |        12 |     0.384s |      0.33% | | Setup dof systems               |        13 |      2.96s |       2.6% | +---------------------------------+-----------+------------+------------+ 

[...] 

+---------------------------------------------+------------+------------+ | Total wallclock time elapsed since start    |  9.14e+04s |            | |                                             |            |            | | Section                         | no. calls |  wall time | % of total | +---------------------------------+-----------+------------+------------+ | Assemble Stokes system          |     47045 |  2.05e+03s |       2.2% | | Assemble temperature matrices   |      4707 |       310s |      0.34% | | Assemble temperature rhs        |     47045 |   8.7e+03s |       9.5% | | Build Stokes preconditioner     |      4707 |  1.48e+03s |       1.6% | | Solve Stokes system             |     47045 |  7.34e+04s |        80% | | Solve temperature system        |     47045 |  1.46e+03s |       1.6% | | Postprocessing                  |      1883 |       222s |      0.24% | | Refine mesh structure, part 1   |      4706 |       641s |       0.7% | | Refine mesh structure, part 2   |      4706 |       259s |      0.28% | | Setup dof systems               |      4707 |  1.86e+03s |         2% | +---------------------------------+-----------+------------+------------+ </pre> </code> 

The simulation terminates when the time reaches the 1 billion years selected in the input file.  You can extrapolate from this how long a simulation would take for a different final time (the time step size ultimately settles on somewhere around 20,000 years, so computing for two billion years will take 100,000 time steps, give or take 20%).  As can be seen here, we spend most of the compute time in assembling linear systems and &mdash; above all &mdash; in solving Stokes systems. 


To demonstrate the output we show the output from every 1250th time step here:  [2.x.2933]  

The last two images show the grid as well as the partitioning of the mesh for the same computation with 16 subdomains and 16 processors. The full dynamics of this simulation are really only visible by looking at an animation, for example the one [1.x.1066]. This image is well worth watching due to its artistic quality and entrancing depiction of the evolution of the magma plumes. 

If you watch the movie, you'll see that the convection pattern goes through several stages: First, it gets rid of the instable temperature layering with the hot material overlain by the dense cold material. After this great driver is removed and we have a sort of stable situation, a few blobs start to separate from the hot boundary layer at the inner ring and rise up, with a few cold fingers also dropping down from the outer boundary layer. During this phase, the solution remains mostly symmetric, reflecting the 12-fold symmetry of the original mesh. In a final phase, the fluid enters vigorous chaotic stirring in which all symmetries are lost. This is a pattern that then continues to dominate flow. 

These different phases can also be identified if we look at the maximal velocity as a function of time in the simulation: 

 [2.x.2934]  

Here, the velocity (shown in centimeters per year) becomes very large, to the order of several meters per year) at the beginning when the temperature layering is instable. It then calms down to relatively small values before picking up again in the chaotic stirring regime. There, it remains in the range of 10-40 centimeters per year, quite within the physically expected region. 




[1.x.1067] 

3d computations are very expensive computationally. Furthermore, as seen above, interesting behavior only starts after quite a long time requiring more CPU hours than is available on a typical cluster. Consequently, rather than showing a complete simulation here, let us simply show a couple of pictures we have obtained using the successor to this program, called [1.x.1068] (short for [1.x.1069]), that is being developed independently of deal.II and that already incorporates some of the extensions discussed below. The following two pictures show isocontours of the temperature and the partition of the domain (along with the mesh) onto 512 processors: 

<p align="center">  [2.x.2935]  

 [2.x.2936]   [2.x.2937]  


[1.x.1070] 

[1.x.1071] 

There are many directions in which this program could be extended. As mentioned at the end of the introduction, most of these are under active development in the [1.x.1072] (short for [1.x.1073]) code at the time this tutorial program is being finished. Specifically, the following are certainly topics that one should address to make the program more useful: 

 [2.x.2938]     [2.x.2939]  [1.x.1074]   The temperature field we get in our simulations after a while   is mostly constant with boundary layers at the inner and outer   boundary, and streamers of cold and hot material mixing   everything. Yet, this doesn't match our expectation that things   closer to the earth core should be hotter than closer to the   surface. The reason is that the energy equation we have used does   not include a term that describes adiabatic cooling and heating:   rock, like gas, heats up as you compress it. Consequently, material   that rises up cools adiabatically, and cold material that sinks down   heats adiabatically. The correct temperature equation would   therefore look somewhat like this:   [1.x.1075] 

  or, expanding the advected derivative  [2.x.2940] :   [1.x.1076] 

  In other words, as pressure increases in a rock volume   ( [2.x.2941] ) we get an additional heat source, and vice   versa. 

  The time derivative of the pressure is a bit awkward to   implement. If necessary, one could approximate using the fact   outlined in the introduction that the pressure can be decomposed   into a dynamic component due to temperature differences and the   resulting flow, and a static component that results solely from the   static pressure of the overlying rock. Since the latter is much   bigger, one may approximate  [2.x.2942] , and consequently    [2.x.2943] .   In other words, if the fluid is moving in the direction of gravity   (downward) it will be compressed and because in that case  [2.x.2944]  we get a positive heat source. Conversely, the   fluid will cool down if it moves against the direction of gravity. 

 [2.x.2945]  [1.x.1077]   As already hinted at in the temperature model above,   mantle rocks are not incompressible. Rather, given the enormous pressures in   the earth mantle (at the core-mantle boundary, the pressure is approximately   140 GPa, equivalent to 1,400,000 times atmospheric pressure), rock actually   does compress to something around 1.5 times the density it would have   at surface pressure. Modeling this presents any number of   difficulties. Primarily, the mass conservation equation is no longer    [2.x.2946]  but should read    [2.x.2947]  where the density  [2.x.2948]  is now no longer   spatially constant but depends on temperature and pressure. A consequence is   that the model is now no longer linear; a linearized version of the Stokes   equation is also no longer symmetric requiring us to rethink preconditioners   and, possibly, even the discretization. We won't go into detail here as to   how this can be resolved. 

 [2.x.2949]  [1.x.1078] As already hinted at in various places,   material parameters such as the density, the viscosity, and the various   thermal parameters are not constant throughout the earth mantle. Rather,   they nonlinearly depend on the pressure and temperature, and in the case of   the viscosity on the strain rate  [2.x.2950] . For complicated   models, the only way to solve such models accurately may be to actually   iterate this dependence out in each time step, rather than simply freezing   coefficients at values extrapolated from the previous time step(s). 

 [2.x.2951]  [1.x.1079] Running this program in 2d on a number of   processors allows solving realistic models in a day or two. However, in 3d,   compute times are so large that one runs into two typical problems: (i) On   most compute clusters, the queuing system limits run times for individual   jobs are to 2 or 3 days; (ii) losing the results of a computation due to   hardware failures, misconfigurations, or power outages is a shame when   running on hundreds of processors for a couple of days. Both of these   problems can be addressed by periodically saving the state of the program   and, if necessary, restarting the program at this point. This technique is   commonly called [1.x.1080] and it requires that the entire   state of the program is written to a permanent storage location (e.g. a hard   drive). Given the complexity of the data structures of this program, this is   not entirely trivial (it may also involve writing gigabytes or more of   data), but it can be made easier by realizing that one can save the state   between two time steps where it essentially only consists of the mesh and   solution vectors; during restart one would then first re-enumerate degrees   of freedom in the same way as done before and then re-assemble   matrices. Nevertheless, given the distributed nature of the data structures   involved here, saving and restoring the state of a program is not   trivial. An additional complexity is introduced by the fact that one may   want to change the number of processors between runs, for example because   one may wish to continue computing on a mesh that is finer than the one used   to precompute a starting temperature field at an intermediate time. 

 [2.x.2952]  [1.x.1081] The point of computations like this is   not simply to solve the equations. Rather, it is typically the exploration   of different physical models and their comparison with things that we can   measure at the earth surface, in order to find which models are realistic   and which are contradicted by reality. To this end, we need to compute   quantities from our solution vectors that are related to what we can   observe. Among these are, for example, heatfluxes at the surface of the   earth, as well as seismic velocities throughout the mantle as these affect   earthquake waves that are recorded by seismographs. 

 [2.x.2953]  [1.x.1082] As can be seen above for the 3d case, the mesh in 3d is primarily refined along the inner boundary. This is because the boundary layer there is stronger than any other transition in the domain, leading us to refine there almost exclusively and basically not at all following the plumes. One certainly needs better refinement criteria to track the parts of the solution we are really interested in better than the criterion used here, namely the KellyErrorEstimator applied to the temperature, is able to.  [2.x.2954]  


There are many other ways to extend the current program. However, rather than discussing them here, let us point to the much larger open source code ASPECT (see https://aspect.geodynamics.org/ ) that constitutes the further development of step-32 and that already includes many such possible extensions. 


examples/step-33/doc/intro.dox 

 [2.x.2955]  

[1.x.1083] 

 [2.x.2956]  The program uses the [1.x.1084] linear solvers (these can be found in Trilinos in the Aztec/Amesos packages) and an automatic differentiation package, Sacado, also part of Trilinos. deal.II must be configured to use Trilinos. Refer to the [1.x.1085] file for instructions how to do this. 

 [2.x.2957]  While this program demonstrates the use of automatic differentiation well, it does not express the state of the art in Euler equation solvers. There are much faster and more accurate method for this equation, and you should take a look at step-67 and step-69 to see how this equation can be solved more efficiently. 




[1.x.1086] [1.x.1087] 

[1.x.1088] 

The equations that describe the movement of a compressible, inviscid gas (the so-called Euler equations of gas dynamics) are a basic system of conservation laws. In spatial dimension  [2.x.2958]  they read 

[1.x.1089] 

with the solution  [2.x.2959]  consisting of  [2.x.2960]  the fluid density,  [2.x.2961]  the flow velocity (and thus  [2.x.2962]  being the linear momentum density), and  [2.x.2963]  the energy density of the gas. We interpret the equations above as  [2.x.2964] ,  [2.x.2965] . 

For the Euler equations, the flux matrix  [2.x.2966]  (or system of flux functions) is defined as (shown here for the case  [2.x.2967] ) 

[1.x.1090] 

and we will choose as particular right hand side forcing only the effects of gravity, described by 

[1.x.1091] 

where  [2.x.2968]  denotes the gravity vector. With this, the entire system of equations reads: 

[1.x.1092] 

These equations describe, respectively, the conservation of momentum, mass, and energy. The system is closed by a relation that defines the pressure:  [2.x.2969] . For the constituents of air (mainly nitrogen and oxygen) and other diatomic gases, the ratio of specific heats is  [2.x.2970] . 

This problem obviously falls into the class of vector-valued problems. A general overview of how to deal with these problems in deal.II can be found in the  [2.x.2971]  module. 

[1.x.1093] 

Discretization happens in the usual way, taking into account that this is a hyperbolic problem in the same style as the simple one discussed in step-12: We choose a finite element space  [2.x.2972] , and integrate our conservation law against our (vector-valued) test function  [2.x.2973] .  We then integrate by parts and approximate the boundary flux with a [1.x.1094] flux  [2.x.2974] , 

[1.x.1095] 

where a superscript  [2.x.2975]  denotes the interior trace of a function, and  [2.x.2976]  represents the outer trace. The diffusion term  [2.x.2977]  is introduced strictly for stability,  where  [2.x.2978]  is the mesh size and  [2.x.2979]  is a parameter prescribing how  much diffusion to add. 

On the boundary, we have to say what the outer trace  [2.x.2980]  is. Depending on the boundary condition, we prescribe either of the following:  [2.x.2981]   [2.x.2982]  Inflow boundary:  [2.x.2983]  is prescribed to be the desired value.  [2.x.2984]  Supersonic outflow boundary:  [2.x.2985]   [2.x.2986]  Subsonic outflow boundary:  [2.x.2987]  except that the energy variable is modified to support a prescribed pressure  [2.x.2988] , i.e.  [2.x.2989]   [2.x.2990]  Reflective boundary: we set  [2.x.2991]  so that  [2.x.2992]  and  [2.x.2993] .  [2.x.2994]  

More information on these issues can be found, for example, in Ralf Hartmann's PhD thesis ("Adaptive Finite Element Methods for the Compressible Euler Equations", PhD thesis, University of Heidelberg, 2002). 

We use a time stepping scheme to substitute the time derivative in the above equations. For simplicity, we define  [2.x.2995]  as the spatial residual at time step  [2.x.2996]  : 

[1.x.1096] 



At each time step, our full discretization is thus that the residual applied to any test function  [2.x.2997]  equals zero: 

[1.x.1097] 

where  [2.x.2998]  and  [2.x.2999] . Choosing  [2.x.3000]  results in the explicit (forward) Euler scheme,  [2.x.3001]  in the stable implicit (backward) Euler scheme, and  [2.x.3002]  in the Crank-Nicolson scheme. 

In the implementation below, we choose the Lax-Friedrichs flux for the function  [2.x.3003] , i.e.   [2.x.3004] , where  [2.x.3005]  is either a fixed number specified in the input file, or where  [2.x.3006]  is a mesh dependent value. In the latter case, it is chosen as  [2.x.3007]  with  [2.x.3008]  the diameter of the face to which the flux is applied, and  [2.x.3009]  the current time step. 

With these choices, equating the residual to zero results in a nonlinear system of equations  [2.x.3010] . We solve this nonlinear system by a Newton iteration (in the same way as explained in step-15), i.e. by iterating 

[1.x.1098] 

until  [2.x.3011]  (the residual) is sufficiently small. By testing with the nodal basis of a finite element space instead of all  [2.x.3012] , we arrive at a linear system for  [2.x.3013] : 

[1.x.1099] 

This linear system is, in general, neither symmetric nor has any particular definiteness properties. We will either use a direct solver or Trilinos' GMRES implementation to solve it. As will become apparent from the [1.x.1100], this fully implicit iteration converges very rapidly (typically in 3 steps) and with the quadratic convergence order expected from a Newton method. 




[1.x.1101] 

Since computing the Jacobian matrix  [2.x.3014]  is a terrible beast, we use an automatic differentiation package, Sacado, to do this.  Sacado is a package within the [1.x.1102] framework and offers a C++ template class  [2.x.3015]  ( [2.x.3016]  standing for "forward automatic differentiation") that supports basic arithmetic operators and functions such as  [2.x.3017]  etc. In order to use this feature, one declares a collection of variables of this type and then denotes some of this collection as degrees of freedom, the rest of the variables being functions of the independent variables.  These variables are used in an algorithm, and as the variables are used, their sensitivities with respect to the degrees of freedom are continuously updated. 

One can imagine that for the full Jacobian matrix as a whole, this could be prohibitively expensive: the number of independent variables are the  [2.x.3018] , the dependent variables the elements of the vector  [2.x.3019] . Both of these vectors can easily have tens of thousands of elements or more.  However, it is important to note that not all elements of  [2.x.3020]  depend on all elements of  [2.x.3021] : in fact, an entry in  [2.x.3022]  only depends on an element of  [2.x.3023]  if the two corresponding shape functions overlap and couple in the weak form. 

Specifically, it is wise to define a minimum set of independent AD variables that the residual on the current cell may possibly depend on: on every element, we define those variables as independent that correspond to the degrees of freedom defined on this cell (or, if we have to compute jump terms between cells, that correspond to degrees of freedom defined on either of the two adjacent cells), and the dependent variables are the elements of the local residual vector. Not doing this, i.e. defining [1.x.1103] elements of  [2.x.3024]  as independent, will result a very expensive computation of a lot of zeros: the elements of the local residual vector are independent of almost all elements of the solution vector, and consequently their derivatives are zero; however, trying to compute these zeros can easily take 90% or more of the compute time of the entire program, as shown in an experiment inadvertently made by a student a few years after this program was first written. 


Coming back to the question of computing the Jacobian automatically: The author has used this approach side by side with a hand coded Jacobian for the incompressible Navier-Stokes problem and found the Sacado approach to be just as fast as using a hand coded Jacobian, but infinitely simpler and less error prone: Since using the auto-differentiation requires only that one code the residual  [2.x.3025] , ensuring code correctness and maintaining code becomes tremendously more simple -- the Jacobian matrix  [2.x.3026]  is computed by essentially the same code that also computes the residual  [2.x.3027] . 

All this said, here's a very simple example showing how Sacado can be used: 

[1.x.1104] 



The output are the derivatives  [2.x.3028]  of  [2.x.3029]  at  [2.x.3030] . 

It should be noted that Sacado provides more auto-differentiation capabilities than the small subset used in this program.  However, understanding the example above is enough to understand the use of Sacado in this Euler flow program. 

[1.x.1105] The program uses either the Aztec iterative solvers, or the Amesos sparse direct solver, both provided by the Trilinos package.  This package is inherently designed to be used in a parallel program, however, it may be used in serial just as easily, as is done here.  The Epetra package is the basic vector/matrix library upon which the solvers are built.  This very powerful package can be used to describe the parallel distribution of a vector, and to define sparse matrices that operate on these vectors.  Please view the commented code for more details on how these solvers are used within the example. 

[1.x.1106] The example uses an ad hoc refinement indicator that shows some usefulness in shock-type problems, and in the downhill flow example included.  We refine according to the squared gradient of the density. Hanging nodes are handled by computing the numerical flux across cells that are of differing refinement levels, rather than using the AffineConstraints class as in all other tutorial programs so far.  In this way, the example combines the continuous and DG methodologies. It also simplifies the generation of the Jacobian because we do not have to track constrained degrees of freedom through the automatic differentiation used to compute it. 

 [2.x.3031]  Whereas this program was written in 2008, we were unaware of any publication that would actually have used this approach. However, a more recent paper by A. Dedner, R. Kl&ouml;fkorn, and M. Kr&auml;nkel ("Continuous Finite-Elements on Non-Conforming Grids Using Discontinuous Galerkin Stabilization", Proceedings of Finite Volumes for Complex Applications VII - Methods and Theoretical Aspects, Springer, 2014) comes close. 

Further, we enforce a maximum number of refinement levels to keep refinement under check.  It is the author's experience that for adaptivity for a time dependent problem, refinement can easily lead the simulation to a screeching halt, because of time step restrictions if the mesh becomes too fine in any part of the domain, if care is not taken.  The amount of refinement is limited in the example by letting the user specify the maximum level of refinement that will be present anywhere in the mesh.  In this way, refinement tends not to slow the simulation to a halt.  This, of course, is purely a heuristic strategy, and if the author's advisor heard about it, the author would likely be exiled forever from the finite  element error estimation community. 

[1.x.1107] 

We use an input file deck to drive the simulation.  In this way, we can alter the boundary conditions and other important properties of the simulation without having to recompile.  For more information on the format, look at the [1.x.1108], where we describe an example input file in more detail. 

In previous example programs, we have usually hard-coded the initial and boundary conditions. In this program, we instead use the expression parser class FunctionParser so that we can specify a generic expression in the input file and have it parsed at run time &mdash; this way, we can change initial conditions without the need to recompile the program. Consequently, no classes named InitialConditions or BoundaryConditions will be declared in the program below. 




[1.x.1109] 

The implementation of this program is split into three essential parts:  [2.x.3032]     [2.x.3033] The  [2.x.3034]  class that encapsulates everything that   completely describes the specifics of the Euler equations. This includes the   flux matrix  [2.x.3035] , the numerical flux  [2.x.3036] , the right hand side  [2.x.3037] ,   boundary conditions, refinement indicators, postprocessing the output, and   similar things that require knowledge of the meaning of the individual   components of the solution vectors and the equations. 

   [2.x.3038] A namespace that deals with everything that has to do with run-time   parameters. 

   [2.x.3039] The  [2.x.3040]  class that deals with time stepping,   outer nonlinear and inner linear solves, assembling the linear systems, and   the top-level logic that drives all this.  [2.x.3041]  

The reason for this approach is that it separates the various concerns in a program: the  [2.x.3042]  is written in such a way that it would be relatively straightforward to adapt it to a different set of equations: One would simply re-implement the members of the  [2.x.3043]  class for some other hyperbolic equation, or augment the existing equations by additional ones (for example by advecting additional variables, or by adding chemistry, etc). Such modifications, however, would not affect the time stepping, or the nonlinear solvers if correctly done, and consequently nothing in the  [2.x.3044]  would have to be modified. 

Similarly, if we wanted to improve on the linear or nonlinear solvers, or on the time stepping scheme (as hinted at the end of the [1.x.1110]), then this would not require changes in the  [2.x.3045]  at all. 


examples/step-33/doc/results.dox 

[1.x.1111] 

[1.x.1112] 

We run the problem with the mesh  [2.x.3046]  (this file is in the same directory as the source code for this program) and the following input deck (available as  [2.x.3047]  in the same directory): 

[1.x.1113] 



When we run the program, we get the following kind of output: 

[1.x.1114] 



This output reports the progress of the Newton iterations and the time stepping. Note that our implementation of the Newton iteration indeed shows the expected quadratic convergence order: the norm of the nonlinear residual in each step is roughly the norm of the previous step squared. This leads to the very rapid convergence we can see here. This holds until times up to  [2.x.3048]  at which time the nonlinear iteration reports a lack of convergence: 

[1.x.1115] 



We may find out the cause and possible remedies by looking at the animation of the solution. 

The result of running these computations is a bunch of output files that we can pass to our visualization program of choice. When we collate them into a movie, the results of last several time steps looks like this: 

 [2.x.3049]  

As we see, when the heavy mass of fluid hits the left bottom corner, some oscillation occurs and lead to the divergence of the iteration. A lazy solution to this issue is add more viscosity. If we set the diffusion power  [2.x.3050]  instead of  [2.x.3051] , the simulation would be able to survive this crisis. Then, the result looks like this: 


 [2.x.3052]  

The heavy mass of fluid is drawn down the slope by gravity, where it collides with the ski lodge and is flung into the air!  Hopefully everyone escapes! And also, we can see the boundary between heavy mass and light mass blur quickly due to the artificial viscosity. 

We can also visualize the evolution of the adaptively refined grid: 

 [2.x.3053]  

The adaptivity follows and precedes the flow pattern, based on the heuristic refinement scheme discussed above. 





[1.x.1116] 

[1.x.1117] 

[1.x.1118] 

The numerical scheme we have chosen is not particularly stable when the artificial viscosity is small while is too diffusive when the artificial viscosity is large. Furthermore, it is known there are more advanced techniques to stabilize the solution, for example streamline diffusion, least-squares stabilization terms, entropy viscosity. 




[1.x.1119] 

While the Newton method as a nonlinear solver appears to work very well if the time step is small enough, the linear solver can be improved. For example, in the current scheme whenever we use an iterative solver, an ILU is computed anew for each Newton step; likewise, for the direct solver, an LU decomposition of the Newton matrix is computed in each step. This is obviously wasteful: from one Newton step to another, and probably also between time steps, the Newton matrix does not radically change: an ILU or a sparse LU decomposition for one Newton step is probably still a very good preconditioner for the next Newton or time step. Avoiding the recomputation would therefore be a good way to reduce the amount of compute time. 

One could drive this a step further: since close to convergence the Newton matrix changes only a little bit, one may be able to define a quasi-Newton scheme where we only re-compute the residual (i.e. the right hand side vector) in each Newton iteration, and re-use the Newton matrix. The resulting scheme will likely not be of quadratic convergence order, and we have to expect to do a few more nonlinear iterations; however, given that we don't have to spend the time to build the Newton matrix each time, the resulting scheme may well be faster. 




[1.x.1120] 

The residual calculated in  [2.x.3054]  function reads     [2.x.3055]  This means that we calculate the spatial residual twice at one Newton iteration step: once respect to the current solution  [2.x.3056]  and once more respect to the last time step solution  [2.x.3057]  which remains the same during all Newton iterations through one timestep. Cache up the explicit part of residual   [2.x.3058]  during Newton iteration will save lots of labor. 




[1.x.1121] 

Finally, as a direction beyond the immediate solution of the Euler equations, this program tries very hard to separate the implementation of everything that is specific to the Euler equations into one class (the  [2.x.3059]  class), and everything that is specific to assembling the matrices and vectors, nonlinear and linear solvers, and the general top-level logic into another (the  [2.x.3060]  class). 

By replacing the definitions of flux matrices and numerical fluxes in this class, as well as the various other parts defined there, it should be possible to apply the  [2.x.3061]  class to other hyperbolic conservation laws as well. 


examples/step-34/doc/intro.dox 

 [2.x.3062]  

[1.x.1122] 

 [2.x.3063]  

[1.x.1123] 

[1.x.1124] 

[1.x.1125] The incompressible motion of an inviscid fluid past a body (for example air past an airplane wing, or air or water past a propeller) is usually modeled by the Euler equations of fluid dynamics: 

[1.x.1126] where the fluid density  [2.x.3064]  and the acceleration  [2.x.3065]  due to external forces are given and the velocity  [2.x.3066]  and the pressure  [2.x.3067]  are the unknowns. Here  [2.x.3068]  is a closed bounded region representing the body around which the fluid moves. 

The above equations can be derived from Navier-Stokes equations assuming that the effects due to viscosity are negligible compared to those due to the pressure gradient, inertial forces and the external forces. This is the opposite case of the Stokes equations discussed in step-22 which are the limit case of dominant viscosity, i.e. where the velocity is so small that inertia forces can be neglected. On the other hand, owing to the assumed incompressibility, the equations are not suited for very high speed gas flows where compressibility and the equation of state of the gas have to be taken into account, leading to the Euler equations of gas dynamics, a hyperbolic system. 

For the purpose of this tutorial program, we will consider only stationary flow without external forces: 

[1.x.1127] 


Uniqueness of the solution of the Euler equations is ensured by adding the boundary conditions 

[1.x.1128] 

which is to say that the body is at rest in our coordinate systems and is not permeable, and that the fluid has (constant) velocity  [2.x.3069]  at infinity. An alternative viewpoint is that our coordinate system moves along with the body whereas the background fluid is at rest at infinity. Notice that we define the normal  [2.x.3070]  as the [1.x.1129] normal to the domain  [2.x.3071] , which is the opposite of the outer normal to the integration domain. 

For both stationary and non stationary flow, the solution process starts by solving for the velocity in the second equation and substituting in the first equation in order to find the pressure. The solution of the stationary Euler equations is typically performed in order to understand the behavior of the given (possibly complex) geometry when a prescribed motion is enforced on the system. 

The first step in this process is to change the frame of reference from a coordinate system moving along with the body to one in which the body moves through a fluid that is at rest at infinity. This can be expressed by introducing a new velocity  [2.x.3072]  for which we find that the same equations hold (because  [2.x.3073] ) and we have boundary conditions 

[1.x.1130] 

If we assume that the fluid is irrotational, i.e.,  [2.x.3074]  in  [2.x.3075] , we can represent the velocity, and consequently also the perturbation velocity, as the gradient of a scalar function: 

[1.x.1131] and so the second part of Euler equations above can be rewritten as the homogeneous Laplace equation for the unknown  [2.x.3076] : 

[1.x.1132] while the momentum equation reduces to Bernoulli's equation that expresses the pressure  [2.x.3077]  as a function of the potential  [2.x.3078] : 

[1.x.1133] 

So we can solve the problem by solving the Laplace equation for the potential.  We recall that the following functions, called fundamental solutions of the Laplace equation, 

[1.x.1134] 

satisfy in a distributional sense the equation: 

[1.x.1135] 

where the derivative is done in the variable  [2.x.3079] . By using the usual Green identities, our problem can be written on the boundary  [2.x.3080]  only. We recall the general definition of the second Green %identity: 

[1.x.1136] 

where  [2.x.3081]  is the normal to the surface of  [2.x.3082]  pointing outwards from the domain of integration  [2.x.3083] . 

In our case the domain of integration is the domain  [2.x.3084] , whose boundary is  [2.x.3085] , where the "boundary" at infinity is defined as 

[1.x.1137] 

In our program the normals are defined as [1.x.1138] to the domain  [2.x.3086] , that is, they are in fact [1.x.1139] to the integration domain, and some care is required in defining the various integrals with the correct signs for the normals, i.e. replacing  [2.x.3087]  by  [2.x.3088] . 

If we substitute  [2.x.3089]  and  [2.x.3090]  in the Green %identity with the solution  [2.x.3091]  and with the fundamental solution of the Laplace equation respectively, as long as  [2.x.3092]  is chosen in the region  [2.x.3093] , we obtain: 

[1.x.1140] 

where the normals are now pointing [1.x.1141] the domain of integration. 

Notice that in the above equation, we also have the integrals on the portion of the boundary at  [2.x.3094] . Using the boundary conditions of our problem, we have that  [2.x.3095]  is zero at infinity (which simplifies the integral on  [2.x.3096]  on the right hand side). 

The integral on  [2.x.3097]  that appears on the left hand side can be treated by observing that  [2.x.3098]  implies that  [2.x.3099]  at infinity is necessarily constant. We define its value to be  [2.x.3100] .  It is an easy exercise to prove that 

[1.x.1142] 

Using this result, we can reduce the above equation only on the boundary  [2.x.3101]  using the so-called Single and Double Layer Potential operators: 

[1.x.1143] 

(The name of these operators comes from the fact that they describe the electric potential in  [2.x.3102]  due to a single thin sheet of charges along a surface, and due to a double sheet of charges and anti-charges along the surface, respectively.) 

In our case, we know the Neumann values of  [2.x.3103]  on the boundary:  [2.x.3104] . Consequently, 

[1.x.1144] If we take the limit for  [2.x.3105]  tending to  [2.x.3106]  of the above equation, using well known properties of the single and double layer operators, we obtain an equation for  [2.x.3107]  just on the boundary  [2.x.3108]  of  [2.x.3109] : 

[1.x.1145] 

which is the Boundary Integral Equation (BIE) we were looking for, where the quantity  [2.x.3110]  is the fraction of angle or solid angle by which the point  [2.x.3111]  sees the domain of integration  [2.x.3112] . 

In particular, at points  [2.x.3113]  where the boundary  [2.x.3114]  is differentiable (i.e. smooth) we have  [2.x.3115] , but the value may be smaller or larger at points where the boundary has a corner or an edge. 

Substituting the single and double layer operators we get: 

[1.x.1146] for two dimensional flows and 

[1.x.1147] for three dimensional flows, where the normal derivatives of the fundamental solutions have been written in a form that makes computation easier. In either case,  [2.x.3116]  is the solution of an integral equation posed entirely on the boundary since both  [2.x.3117] . 

Notice that the fraction of angle (in 2d) or solid angle (in 3d)  [2.x.3118]  by which the point  [2.x.3119]  sees the domain  [2.x.3120]  can be defined using the double layer potential itself: 

[1.x.1148] 

The reason why this is possible can be understood if we consider the fact that the solution of a pure Neumann problem is known up to an arbitrary constant  [2.x.3121] , which means that, if we set the Neumann data to be zero, then any constant  [2.x.3122]  will be a solution. Inserting the constant solution and the Neumann boundary condition in the boundary integral equation, we have 

[1.x.1149] 

The integral on  [2.x.3123]  is unity, see above, so division by the constant  [2.x.3124]  gives us the explicit expression above for  [2.x.3125] . 

While this example program is really only focused on the solution of the boundary integral equation, in a realistic setup one would still need to solve for the velocities. To this end, note that we have just computed  [2.x.3126]  for all  [2.x.3127] . In the next step, we can compute (analytically, if we want) the solution  [2.x.3128]  in all of  [2.x.3129] . To this end, recall that we had 

[1.x.1150] where now we have everything that is on the right hand side ( [2.x.3130]  and  [2.x.3131]  are integrals we can evaluate, the normal velocity on the boundary is given, and  [2.x.3132]  on the boundary we have just computed). Finally, we can then recover the velocity as  [2.x.3133] . 

Notice that the evaluation of the above formula for  [2.x.3134]  should yield zero as a result, since the integration of the Dirac delta  [2.x.3135]  in the domain  [2.x.3136]  is always zero by definition. 

As a final test, let us verify that this velocity indeed satisfies the momentum balance equation for a stationary flow field, i.e., whether  [2.x.3137]  where  [2.x.3138]  for some (unknown) pressure  [2.x.3139]  and a given constant  [2.x.3140] . In other words, we would like to verify that Bernoulli's law as stated above indeed holds. To show this, we use that the left hand side of this equation equates to 

[1.x.1151] 

where we have used that  [2.x.3141]  is constant. We would like to write this expression as the gradient of something (remember that  [2.x.3142]  is a constant). The next step is more convenient if we consider the components of the equation individually (summation over indices that appear twice is implied): 

[1.x.1152] 

because  [2.x.3143]  and  [2.x.3144] . Next, 

[1.x.1153] 

Again, the last term disappears because  [2.x.3145]  is constant and we can merge the first and third term into one: 

[1.x.1154] 



We now only need to massage that last term a bit more. Using the product rule, we get 

[1.x.1155] 

The first of these terms is zero (because, again, the summation over  [2.x.3146]  gives  [2.x.3147] , which is zero). The last term can be written as  [2.x.3148]  which is in the desired gradient form. As a consequence, we can now finally state that 

[1.x.1156] 

or in vector form: 

[1.x.1157] 

or in other words: 

[1.x.1158] 

Because the pressure is only determined up to a constant (it appears only with a gradient in the equations), an equally valid definition is 

[1.x.1159] 

This is exactly Bernoulli's law mentioned above. 




[1.x.1160] 

Numerical approximations of Boundary Integral Equations (BIE) are commonly referred to as the boundary element method or panel method (the latter expression being used mostly in the computational fluid dynamics community). The goal of the following test problem is to solve the integral formulation of the Laplace equation with Neumann boundary conditions, using a circle and a sphere respectively in two and three space dimensions, illustrating along the way the features that allow one to treat boundary element problems almost as easily as finite element problems using the deal.II library. 

To this end, let  [2.x.3149]  be a subdivision of the manifold  [2.x.3150]  into  [2.x.3151]  line segments if  [2.x.3152] , or  [2.x.3153]  quadrilaterals if  [2.x.3154] . We will call each individual segment or quadrilateral an [1.x.1161] or [1.x.1162], independently of the dimension  [2.x.3155]  of the surrounding space  [2.x.3156] . We define the finite dimensional space  [2.x.3157]  as 

[1.x.1163] with basis functions  [2.x.3158]  for which we will use the usual FE_Q finite element, with the catch that this time it is defined on a manifold of codimension one (which we do by using the second template argument that is usually defaulted to equal the first; here, we will create objects  [2.x.3159]  dimensional cells in a  [2.x.3160]  dimensional space). An element  [2.x.3161]  of  [2.x.3162]  is uniquely identified by the vector  [2.x.3163]  of its coefficients  [2.x.3164] , that is: 

[1.x.1164] where summation  is implied over repeated indexes. Note that we could use discontinuous elements here &mdash; in fact, there is no real reason to use continuous ones since the integral formulation does not imply any derivatives on our trial functions so continuity is unnecessary, and often in the literature only piecewise constant elements are used. 

[1.x.1165] 

By far, the most common approximation of boundary integral equations is by use of the collocation based boundary element method. 

This method requires the evaluation of the boundary integral equation at a number of collocation points which is equal to the number of unknowns of the system. The choice of these points is a delicate matter, that requires a careful study. Assume that these points are known for the moment, and call them  [2.x.3165]  with  [2.x.3166] . 

The problem then becomes: Given the datum  [2.x.3167] , find a function  [2.x.3168]  in  [2.x.3169]  such that the following  [2.x.3170]  equations are satisfied: 

[1.x.1166] 

where the quantity  [2.x.3171]  is the fraction of (solid) angle by which the point  [2.x.3172]  sees the domain  [2.x.3173] , as explained above, and we set  [2.x.3174]  to be zero.  If the support points  [2.x.3175]  are chosen appropriately, then the problem can be written as the following linear system: 

[1.x.1167] 

where 

[1.x.1168] 

From a linear algebra point of view, the best possible choice of the collocation points is the one that renders the matrix  [2.x.3176]  the most diagonally dominant. A natural choice is then to select the  [2.x.3177]  collocation points to be the support points of the nodal basis functions  [2.x.3178] . In that case,  [2.x.3179] , and as a consequence the matrix  [2.x.3180]  is diagonal with entries 

[1.x.1169] where we have used that  [2.x.3181]  for the usual Lagrange elements. With this choice of collocation points, the computation of the entries of the matrices  [2.x.3182] ,  [2.x.3183]  and of the right hand side  [2.x.3184]  requires the evaluation of singular integrals on the elements  [2.x.3185]  of the triangulation  [2.x.3186] . As usual in these cases, all integrations are performed on a reference simple domain, i.e., we assume that each element  [2.x.3187]  of  [2.x.3188]  can be expressed as a linear (in two dimensions) or bi-linear (in three dimensions) transformation of the reference boundary element  [2.x.3189] , and we perform the integrations after a change of variables from the real element  [2.x.3190]  to the reference element  [2.x.3191] . 

[1.x.1170] 

In two dimensions it is not necessary to compute the diagonal elements  [2.x.3192]  of the system matrix, since, even if the denominator goes to zero when  [2.x.3193] , the numerator is always zero because  [2.x.3194]  and  [2.x.3195]  are orthogonal (on our polygonal approximation of the boundary of  [2.x.3196] ), and the only singular integral arises in the computation of  [2.x.3197]  on the i-th element of  [2.x.3198] : 

[1.x.1171] 

This can be easily treated by the QGaussLogR quadrature formula. 

Similarly, it is possible to use the QGaussOneOverR quadrature formula to perform the singular integrations in three dimensions. The interested reader will find detailed explanations on how these quadrature rules work in their documentation. 

The resulting matrix  [2.x.3199]  is full. Depending on its size, it might be convenient to use a direct solver or an iterative one. For the purpose of this example code, we chose to use only an iterative solver, without providing any preconditioner. 

If this were a production code rather than a demonstration of principles, there are techniques that are available to not store full matrices but instead store only those entries that are large and/or relevant. In the literature on boundary element methods, a plethora of methods is available that allows to determine which elements are important and which are not, leading to a significantly sparser representation of these matrices that also facilitates rapid evaluations of the scalar product between vectors and matrices. This not being the goal of this program, we leave this for more sophisticated implementations. 




[1.x.1172] 

The implementation is rather straight forward. The main point that hasn't been used in any of the previous tutorial programs is that most classes in deal.II are not only templated on the dimension, but in fact on the dimension of the manifold on which we pose the differential equation as well as the dimension of the space into which this manifold is embedded. By default, the second template argument equals the first, meaning for example that we want to solve on a two-dimensional region of two-dimensional space. The triangulation class to use in this case would be  [2.x.3200] , which is an equivalent way of writing  [2.x.3201] . 

However, this doesn't have to be so: in the current example, we will for example want to solve on the surface of a sphere, which is a two-dimensional manifold embedded in a three-dimensional space. Consequently, the right class will be  [2.x.3202] , and correspondingly we will use  [2.x.3203]  as the DoF handler class and  [2.x.3204]  for finite elements. 

Some further details on what one can do with things that live on curved manifolds can be found in the report [1.x.1173][1.x.1174]. In addition, the step-38 tutorial program extends what we show here to cases where the equation posed on the manifold is not an integral operator but in fact involves derivatives. 




[1.x.1175] 

The testcase we will be solving is for a circular (in 2d) or spherical (in 3d) obstacle. Meshes for these geometries will be read in from files in the current directory and an object of type SphericalManifold will then be attached to the triangulation to allow mesh refinement that respects the continuous geometry behind the discrete initial mesh. 

For a sphere of radius  [2.x.3205]  translating at a velocity of  [2.x.3206]  in the  [2.x.3207]  direction, the potential reads 

[1.x.1176] 

see, e.g. J. N. Newman, [1.x.1177], 1977, pp. 127. For unit speed and radius, and restricting  [2.x.3208]  to lie on the surface of the sphere,  [2.x.3209] . In the test problem, the flow is  [2.x.3210] , so the appropriate exact solution on the surface of the sphere is the superposition of the above solution with the analogous solution along the  [2.x.3211]  and  [2.x.3212]  axes, or  [2.x.3213] . 


examples/step-34/doc/results.dox 



[1.x.1178] 

We ran the program using the following  [2.x.3214]  file (which can also be found in the directory in which all the other source files are): 

[1.x.1179] 



When we run the program, the following is printed on screen: 

[1.x.1180] 



As we can see from the convergence table in 2d, if we choose quadrature formulas which are accurate enough, then the error we obtain for  [2.x.3215]  should be exactly the inverse of the number of elements. The approximation of the circle with N segments of equal size generates a regular polygon with N faces, whose angles are exactly  [2.x.3216] , therefore the error we commit should be exactly  [2.x.3217] . In fact this is a very good indicator that we are performing the singular integrals in an appropriate manner. 

The error in the approximation of the potential  [2.x.3218]  is largely due to approximation of the domain. A much better approximation could be obtained by using higher order mappings. 

If we modify the main() function, setting fe_degree and mapping_degree to two, and raise the order of the quadrature formulas  in the parameter file, we obtain the following convergence table for the two dimensional simulation 

[1.x.1181] 



and 

[1.x.1182] 



for the three dimensional case. As we can see, convergence results are much better with higher order mapping, mainly due to a better resolution of the curved geometry. Notice that, given the same number of degrees of freedom, for example in step 3 of the Q1 case and step 2 of Q2 case in the three dimensional simulation, the error is roughly three orders of magnitude lower. 

The result of running these computations is a bunch of output files that we can pass to our visualization program of choice. The output files are of two kind: the potential on the boundary element surface, and the potential extended to the outer and inner domain. The combination of the two for the two dimensional case looks like 

 [2.x.3219]  

while in three dimensions we show first the potential on the surface, together with a contour plot, 

 [2.x.3220]  

and then the external contour plot of the potential, with opacity set to 25%: 

 [2.x.3221]  


[1.x.1183] 

[1.x.1184] 

This is the first tutorial program that considers solving equations defined on surfaces embedded in higher dimensional spaces. But the equation discussed here was relatively simple because it only involved an integral operator, not derivatives which are more difficult to define on the surface. The step-38 tutorial program considers such problems and provides the necessary tools. 

From a practical perspective, the Boundary Element Method (BEM) used here suffers from two bottlenecks. The first is that assembling the matrix has a cost that is *quadratic* in the number of unknowns, that is  [2.x.3222]  where  [2.x.3223]  is the total number of unknowns. This can be seen by looking at the `assemble_system()` function, which has this structure: 

[1.x.1185] 

Here, the first loop walks over all cells (one factor of  [2.x.3224] ) whereas the inner loop contributes another factor of  [2.x.3225] . 

This has to be contrasted with the finite element method for *local* differential operators: There, we loop over all cells (one factor of  [2.x.3226] ) and on each cell do an amount of work that is independent of how many cells or unknowns there are. This clearly presents a bottleneck. 

The second bottleneck is that the system matrix is dense (i.e., is of type FullMatrix) because every degree of freedom couples with every other degree of freedom. As pointed out above, just *computing* this matrix with its  [2.x.3227]  nonzero entries necessarily requires at least  [2.x.3228]  operations, but it's worth pointing out that it also costs this many operations to just do one matrix-vector product. If the GMRES method used to solve the linear system requires a number of iterations that grows with the size of the problem, as is typically the case, then solving the linear system will require a number of operations that grows even faster than just  [2.x.3229] . 

"Real" boundary element methods address these issues by strategies that determine which entries of the matrix will be small and can consequently be neglected (at the cost of introducing an additional error, of course). This is possible by recognizing that the matrix entries decay with the (physical) distance between the locations where degrees of freedom  [2.x.3230]  and  [2.x.3231]  are defined. This can be exploited in methods such as the Fast Multipole Method (FMM) that control which matrix entries must be stored and computed to achieve a certain accuracy, and -- if done right -- result in methods in which both assembly and solution of the linear system requires less than  [2.x.3232]  operations. 

Implementing these methods clearly presents opportunities to extend the current program. 


examples/step-35/doc/intro.dox 

 [2.x.3233]  

[1.x.1186] 

[1.x.1187] 

[1.x.1188] 

[1.x.1189] 

[1.x.1190] The purpose of this program is to show how to effectively solve the incompressible time-dependent Navier-Stokes equations. These equations describe the flow of a viscous incompressible fluid and read 

[1.x.1191] 

where  [2.x.3234]  represents the velocity of the flow and  [2.x.3235]  the pressure. This system of equations is supplemented by the initial condition 

[1.x.1192] 

with  [2.x.3236]  sufficiently smooth and solenoidal, and suitable boundary conditions. For instance, an admissible boundary condition, is 

[1.x.1193] 

It is possible to prescribe other boundary conditions as well. In the test case that we solve here the boundary is partitioned into two disjoint subsets  [2.x.3237]  and we have 

[1.x.1194] 

and 

[1.x.1195] 

where  [2.x.3238]  is the outer unit normal. The boundary conditions on  [2.x.3239]  are often used to model outflow conditions. 

In previous tutorial programs (see for instance step-20 and step-22) we have seen how to solve the time-independent Stokes equations using a Schur complement approach. For the time-dependent case, after time discretization, we would arrive at a system like 

[1.x.1196] 

where  [2.x.3240]  is the time-step. Although the structure of this system is similar to the Stokes system and thus it could be solved using a Schur complement approach, it turns out that the condition number of the Schur complement is proportional to  [2.x.3241] . This makes the system very difficult to solve, and means that for the Navier-Stokes equations, this is not a useful avenue to the solution. 

[1.x.1197] 

[1.x.1198] 

Rather, we need to come up with a different approach to solve the time-dependent Navier-Stokes equations. The difficulty in their solution comes from the fact that the velocity and the pressure are coupled through the constraint 

[1.x.1199] 

for which the pressure is the Lagrange multiplier. Projection methods aim at decoupling this constraint from the diffusion (Laplace) operator. 

Let us shortly describe how the projection methods look like in a semi-discrete setting. The objective is to obtain a sequence of velocities  [2.x.3242]  and pressures  [2.x.3243] . We will also obtain a sequence  [2.x.3244]  of auxiliary variables. Suppose that from the initial conditions, and an application of a first order method we have found  [2.x.3245]  and  [2.x.3246] . Then the projection method consists of the following steps:  [2.x.3247]     [2.x.3248]  [1.x.1200]: Extrapolation. Define:   [1.x.1201] 

   [2.x.3249]  [1.x.1202]: Diffusion step. We find  [2.x.3250]  that solves the single   linear equation   [1.x.1203] 



   [2.x.3251]  [1.x.1204]: Projection. Find  [2.x.3252]  that solves   [1.x.1205] 

   [2.x.3253]  [1.x.1206]: Pressure correction. Here we have two options:      [2.x.3254]         [2.x.3255]  [1.x.1207]. The pressure is updated by:       [1.x.1208] 

       [2.x.3256]  [1.x.1209]. In this case       [1.x.1210] 

     [2.x.3257]   [2.x.3258]  

Without going into details, let us remark a few things about the projection methods that we have just described:  [2.x.3259]     [2.x.3260]  The advection term  [2.x.3261]  is replaced by its [1.x.1211]   [1.x.1212] 

  This is consistent with the continuous equation (because  [2.x.3262] ,   though this is not true pointwise for the discrete solution) and it is needed to   guarantee unconditional stability of the   time-stepping scheme. Moreover, to linearize the term we use the second order extrapolation  [2.x.3263]  of    [2.x.3264] .    [2.x.3265]  The projection step is a realization of the Helmholtz decomposition   [1.x.1213] 

  where   [1.x.1214] 

  and   [1.x.1215] 

  Indeed, if we use this decomposition on  [2.x.3266]  we obtain   [1.x.1216] 

  with  [2.x.3267] . Taking the divergence of this equation we arrive at the projection equation.    [2.x.3268]  The more accurate of the two variants outlined above is the rotational   one. However, the program below implements both variants. Moreover, in the author's experience,   the standard form is the one that should be used if, for instance, the viscosity  [2.x.3269]  is variable.  [2.x.3270]  


 [2.x.3271]  The standard incremental scheme and the rotational incremental scheme were first considered by van Kan in  [2.x.3272]     [2.x.3273]  J. van Kan, "A second-order accurate pressure-correction scheme for viscous incompressible flow",        SIAM Journal on Scientific and Statistical Computing, vol. 7, no. 3, pp. 870891, 1986  [2.x.3274]  and is analyzed by Guermond in  [2.x.3275]     [2.x.3276]  J.-L. Guermond, "Un rsultat de convergence dordre deux en temps pour                         lapproximation des quations de NavierStokes par une technique de projection incrmentale",        ESAIM: Mathematical Modelling and Numerical Analysis, vol. 33, no. 1, pp. 169189, 1999  [2.x.3277]  for the case  [2.x.3278] . It turns out that this technique suffers from unphysical boundary conditions for the kinematic pressure that lead to reduced rates of convergence. To prevent this, Timmermans et al. proposed in  [2.x.3279]     [2.x.3280]  L. Timmermans, P. Minev, and F. Van De Vosse,        "An approximate projection scheme for incompressible flow using spectral elements",        International Journal for Numerical Methods in Fluids, vol. 22, no. 7, pp. 673688, 1996  [2.x.3281]  the rotational pressure-correction projection method that uses a divergence correction for the kinematic pressure. A thorough analysis for scheme has first been performed in  [2.x.3282]     [2.x.3283]  J.-L. Guermond and J. Shen, "On the error estimates for the rotational pressure-correction projection methods",        Mathematics of Computation, vol. 73, no. 248, pp. 17191737, 2004  [2.x.3284]  for the Stokes problem.  [2.x.3285]  

[1.x.1217] 

[1.x.1218] To obtain a fully discrete setting of the method we, as always, need a variational formulation. There is one subtle issue here given the nature of the boundary conditions. When we multiply the equation by a suitable test function one of the term that arises is 

[1.x.1219] 

If we, say, had Dirichlet boundary conditions on the whole boundary then after integration by parts we would obtain 

[1.x.1220] 

One of the advantages of this formulation is that it fully decouples the components of the velocity. Moreover, they all share the same system matrix. This can be exploited in the program. 

However, given the nonstandard boundary conditions, to be able to take them into account we need to use the following %identity 

[1.x.1221] 

so that when we integrate by parts and take into account the boundary conditions we obtain 

[1.x.1222] 

which is the form that we would have to use. Notice that this couples the components of the velocity. Moreover, to enforce the boundary condition on the pressure, we need to rewrite 

[1.x.1223] 

where the boundary integral in  [2.x.3286]  equals zero given the boundary conditions for the velocity, and the one in  [2.x.3287]  given the boundary conditions for the pressure. 

In the simplified case where the boundary  [2.x.3288]  is %parallel to a coordinate axis, which holds for the testcase that we carry out below, it can actually be shown that 

[1.x.1224] 

This issue is not very often addressed in the literature. For more information the reader can consult, for instance,  [2.x.3289]     [2.x.3290]  J.-L. GUERMOND, L. QUARTAPELLE, On the approximation of the unsteady Navier-Stokes equations by   finite element projection methods, Numer. Math., 80  (1998) 207-238    [2.x.3291]  J.-L. GUERMOND, P. MINEV, J. SHEN, Error analysis of pressure-correction schemes for the   Navier-Stokes equations with open boundary conditions, SIAM J. Numer. Anal., 43  1 (2005) 239--258.  [2.x.3292]  




[1.x.1225] 

[1.x.1226] 

Our implementation of the projection methods follows [1.x.1227] the description given above. We must note, however, that as opposed to most other problems that have several solution components, we do not use vector-valued finite elements. Instead, we use separate finite elements the components of the velocity and the pressure, respectively, and use different  [2.x.3293] 's for those as well. The main reason for doing this is that, as we see from the description of the scheme, the  [2.x.3294]  components of the velocity and the pressure are decoupled. As a consequence, the equations for all the velocity components look all the same, have the same system matrix, and can be solved in %parallel. Obviously, this approach has also its disadvantages. For instance, we need to keep several  [2.x.3295] s and iterators synchronized when assembling matrices and right hand sides; obtaining quantities that are inherent to vector-valued functions (e.g. divergences) becomes a little awkward, and others. 

[1.x.1228] 

[1.x.1229] 

The testcase that we use for this program consists of the flow around a square obstacle. The geometry is as follows: 

 [2.x.3296]  

with  [2.x.3297] , making the geometry slightly non-symmetric. 

We impose no-slip boundary conditions on both the top and bottom walls and the obstacle. On the left side we have the inflow boundary condition 

[1.x.1230] 

with  [2.x.3298] , i.e. the inflow boundary conditions correspond to Poiseuille flow for this configuration. Finally, on the right vertical wall we impose the condition that the vertical component of the velocity and the pressure should both be zero. The final time  [2.x.3299] . 


examples/step-35/doc/results.dox 

[1.x.1231] 

[1.x.1232] 

[1.x.1233] 

[1.x.1234] 

We run the code with the following  [2.x.3300] , which can be found in the same directory as the source: 

[1.x.1235] 



Since the  [2.x.3301] , we do not get any kind of output besides the number of the time step the program is currently working on. If we were to set it to  [2.x.3302]  we would get information on what the program is doing and how many steps each iterative process had to make to converge, etc. 

Let us plot the obtained results for  [2.x.3303]  (i.e. time steps 200, 1000, 2400, 4000, and 5000), where in the left column we show the vorticity and in the right the velocity field: 

 [2.x.3304]  

The images show nicely the development and extension of a vortex chain behind the obstacles, with the sign of the vorticity indicating whether this is a left or right turning vortex. 


[1.x.1236] 

[1.x.1237] 

We can change the Reynolds number,  [2.x.3305] , in the parameter file to a value of  [2.x.3306] . Doing so, and reducing the time step somewhat as well, yields the following images at times  [2.x.3307] : 

 [2.x.3308]  

For this larger Reynolds number, we observe unphysical oscillations, especially for the vorticity. The discretization scheme has now difficulties in correctly resolving the flow, which should still be laminar and well-organized. These phenomena are typical of discretization schemes that lack robustness in under-resolved scenarios, where under-resolved means that the Reynolds number computed with the mesh size instead of the physical dimensions of the geometry is large. We look at a zoom at the region behind the obstacle, and the mesh size we have there: 


 [2.x.3309]  

We can easily test our hypothesis by re-running the simulation with one more mesh refinement set in the parameter file: 

 [2.x.3310]  

Indeed, the vorticity field now looks much smoother. While we can expect that further refining the mesh will suppress the remaining oscillations as well, one should take measures to obtain a robust scheme in the limit of coarse resolutions, as described below. 


[1.x.1238] 

[1.x.1239] 

This program can be extended in the following directions:  [2.x.3311]     [2.x.3312]  Adaptive mesh refinement: As we have seen, we computed everything on a single fixed mesh.   Using adaptive mesh refinement can lead to increased accuracy while not significantly increasing the   computational time. 

   [2.x.3313]  Adaptive time-stepping: Although there apparently is currently no theory about   projection methods with variable time step,   practice shows that they perform very well. 

   [2.x.3314]  High Reynolds %numbers: As we can see from the results, increasing the Reynolds number changes significantly   the behavior of the discretization scheme. Using well-known stabilization techniques we could be able to   compute the flow in this, or many other problems, when the Reynolds number is very large and where computational   costs demand spatial resolutions for which the flow is only marginally resolved, especially for 3D turbulent   flows. 

   [2.x.3315]  Variable density incompressible flows: There are projection-like methods for the case of incompressible   flows with variable density. Such flows play a role if fluids of different   density mix, for example fresh water and salt water, or alcohol and water. 

   [2.x.3316]  Compressible Navier-Stokes equations: These equations are relevant for   cases where   velocities are high enough so that the fluid becomes compressible, but not   fast enough that we get into a regime where viscosity becomes negligible   and the Navier-Stokes equations need to be replaced by the hyperbolic Euler   equations of gas dynamics. Compressibility starts to become a factor if the   velocity becomes greater than about one third of the speed of sound, so it   is not a factor for almost all terrestrial vehicles. On the other hand,   commercial jetliners fly at about 85 per cent of the speed of sound, and   flow over the wings becomes significantly supersonic, a regime in which the   compressible Navier-Stokes equations are not applicable any more   either. There are significant applications for the range in between,   however, such as for small aircraft or the fast trains in many European and   East Asian countries.  [2.x.3317]  


examples/step-36/doc/intro.dox 

 [2.x.3318]  

[1.x.1240] 

[1.x.1241] 

[1.x.1242] 

The problem we want to solve in this example is an eigenspectrum problem. Eigenvalue problems appear in a wide context of problems, for example in the computation of electromagnetic standing waves in cavities, vibration modes of drum membranes, or oscillations of lakes and estuaries. One of the most enigmatic applications is probably the computation of stationary or quasi-static wave functions in quantum mechanics. The latter application is what we would like to investigate here, though the general techniques outlined in this program are of course equally applicable to the other applications above. 

Eigenspectrum problems have the general form 

[1.x.1243] 

where the Dirichlet boundary condition on  [2.x.3319]  could also be replaced by Neumann or Robin conditions;  [2.x.3320]  is an operator that generally also contains differential operators. 

Under suitable conditions, the above equations have a set of solutions  [2.x.3321] ,  [2.x.3322] , where  [2.x.3323]  can be a finite or infinite set (and in the latter case it may be a discrete or sometimes at least in part a continuous set). In either case, let us note that there is no longer just a single solution, but a set of solutions (the various eigenfunctions and corresponding eigenvalues) that we want to compute. The problem of numerically finding all eigenvalues (eigenfunctions) of such eigenvalue problems is a formidable challenge. In fact, if the set  [2.x.3324]  is infinite, the challenge is of course intractable.  Most of the time however we are really only interested in a small subset of these values (functions); and fortunately, the interface to the SLEPc library that we will use for this tutorial program allows us to select which portion of the eigenspectrum and how many solutions we want to solve for. 

In this program, the eigenspectrum solvers we use are classes provided by deal.II that wrap around the linear algebra implementation of the [1.x.1244] library; SLEPc itself builds on the [1.x.1245] library for linear algebra contents. 

[1.x.1246] 

[1.x.1247] 

The basic equation of stationary quantum mechanics is the Schrdinger equation which models the motion of particles in an external potential  [2.x.3325] . The particle is described by a wave function  [2.x.3326]  that satisfies a relation of the (nondimensionalized) form 

[1.x.1248] 

As a consequence, this particle can only exist in a certain number of eigenstates that correspond to the energy eigenvalues  [2.x.3327]  admitted as solutions of this equation. The orthodox (Copenhagen) interpretation of quantum mechanics posits that, if a particle has energy  [2.x.3328]  then the probability of finding it at location  [2.x.3329]  is proportional to  [2.x.3330]  where  [2.x.3331]  is the eigenfunction that corresponds to this eigenvalue. 

In order to numerically find solutions to this equation, i.e. a set of pairs of eigenvalues/eigenfunctions, we use the usual finite element approach of multiplying the equation from the left with test functions, integrating by parts, and searching for solutions in finite dimensional spaces by approximating  [2.x.3332] , where  [2.x.3333]  is a vector of expansion coefficients. We then immediately arrive at the following equation that discretizes the continuous eigenvalue problem: [1.x.1249] In matrix and vector notation, this equation then reads: [1.x.1250] where  [2.x.3334]  is the stiffness matrix arising from the differential operator  [2.x.3335] , and  [2.x.3336]  is the mass matrix. The solution to the eigenvalue problem is an eigenspectrum  [2.x.3337] , with associated eigenfunctions  [2.x.3338] . 




[1.x.1251] 

In this program, we use Dirichlet boundary conditions for the wave function  [2.x.3339] . What this means, from the perspective of a finite element code, is that only the interior degrees of freedom are real degrees of [1.x.1252]: the ones on the boundary are not free but are forced to have a zero value, after all. On the other hand, the finite element method gains much of its power and simplicity from the fact that we just do the same thing on every cell, without having to think too much about where a cell is, whether it bounds on a less refined cell and consequently has a hanging node, or is adjacent to the boundary. All such checks would make the assembly of finite element linear systems unbearably difficult to write and even more so to read. 

Consequently, of course, when you distribute degrees of freedom with your DoFHandler object, you don't care whether some of the degrees of freedom you enumerate are at a Dirichlet boundary. They all get numbers. We just have to take care of these degrees of freedom at a later time when we apply boundary values. There are two basic ways of doing this (either using  [2.x.3340]  [1.x.1253] assembling the linear system, or using  [2.x.3341]  [1.x.1254] assembly; see the  [2.x.3342]  "constraints module" for more information), but both result in the same: a linear system that has a total number of rows equal to the number of [1.x.1255] degrees of freedom, including those that lie on the boundary. However, degrees of freedom that are constrained by Dirichlet conditions are separated from the rest of the linear system by zeroing out the corresponding row and column, putting a single positive entry on the diagonal, and the corresponding Dirichlet value on the right hand side. 

If you assume for a moment that we had renumbered degrees of freedom in such a way that all of those on the Dirichlet boundary come last, then the linear system we would get when solving a regular PDE with a right hand side would look like this: 

[1.x.1256] 

Here, subscripts  [2.x.3343]  and  [2.x.3344]  correspond to interior and boundary degrees of freedom, respectively. The interior degrees of freedom satisfy the linear system  [2.x.3345]  which yields the correct solution in the interior, and boundary values are determined by  [2.x.3346]  where  [2.x.3347]  is a diagonal matrix that results from the process of eliminating boundary degrees of freedom, and  [2.x.3348]  is chosen in such a way that  [2.x.3349]  has the correct boundary values for every boundary degree of freedom  [2.x.3350] . (For the curious, the entries of the matrix  [2.x.3351]  result from adding modified local contributions to the global matrix where for the local matrices the diagonal elements, if non-zero, are set to their absolute value; otherwise, they are set to the average of absolute values of the diagonal. This process guarantees that the entries of  [2.x.3352]  are positive and of a size comparable to the rest of the diagonal entries, ensuring that the resulting matrix does not incur unreasonable losses of accuracy due to roundoff involving matrix entries of drastically different size. The actual values that end up on the diagonal are difficult to predict and you should treat them as arbitrary and unpredictable, but positive.) 

For "regular" linear systems, this all leads to the correct solution. On the other hand, for eigenvalue problems, this is not so trivial. There, eliminating boundary values affects both matrices  [2.x.3353]  and  [2.x.3354]  that we will solve with in the current tutorial program. After elimination of boundary values, we then receive an eigenvalue problem that can be partitioned like this: 

[1.x.1257] 

This form makes it clear that there are two sets of eigenvalues: the ones we care about, and spurious eigenvalues from the separated problem 

[1.x.1258] 

These eigenvalues are spurious since they result from an eigenvalue system that operates only on boundary nodes -- nodes that are not real degrees of [1.x.1259]. Of course, since the two matrices  [2.x.3355]  are diagonal, we can exactly quantify these spurious eigenvalues: they are  [2.x.3356]  (where the indices  [2.x.3357]  corresponds exactly to the degrees of freedom that are constrained by Dirichlet boundary values). 

So how does one deal with them? The fist part is to recognize when our eigenvalue solver finds one of them. To this end, the program computes and prints an interval within which these eigenvalues lie, by computing the minimum and maximum of the expression  [2.x.3358]  over all constrained degrees of freedom. In the program below, this already suffices: we find that this interval lies outside the set of smallest eigenvalues and corresponding eigenfunctions we are interested in and compute, so there is nothing we need to do here. 

On the other hand, it may happen that we find that one of the eigenvalues we compute in this program happens to be in this interval, and in that case we would not know immediately whether it is a spurious or a true eigenvalue. In that case, one could simply scale the diagonal elements of either matrix after computing the two matrices, thus shifting them away from the frequency of interest in the eigen-spectrum. This can be done by using the following code, making sure that all spurious eigenvalues are exactly equal to  [2.x.3359] : 

[1.x.1260] 

However, this strategy is not pursued here as the spurious eigenvalues we get from our program as-is happen to be greater than the lowest five that we will calculate and are interested in. 




[1.x.1261] 

The program below is essentially just a slightly modified version of step-4. The things that are different are the following: 

 [2.x.3360]  

 [2.x.3361] The main class (named  [2.x.3362] ) now no longer has a single solution vector, but a whole set of vectors for the various eigenfunctions we want to compute. Moreover, the  [2.x.3363]  function, which has the top-level control over everything here, initializes and finalizes the interface to SLEPc and PETSc simultaneously via  [2.x.3364]  and  [2.x.3365] . [2.x.3366]  

 [2.x.3367] We use PETSc matrices and vectors as in step-17 and step-18 since that is what the SLEPc eigenvalue solvers require. [2.x.3368]  

 [2.x.3369] The function  [2.x.3370]  is entirely different from anything seen so far in the tutorial, as it does not just solve a linear system but actually solves the eigenvalue problem. It is built on the SLEPc library, and more immediately on the deal.II SLEPc wrappers in the class  [2.x.3371]  

 [2.x.3372] We use the ParameterHandler class to describe a few input parameters, such as the exact form of the potential  [2.x.3373] , the number of global refinement steps of the mesh, or the number of eigenvalues we want to solve for. We could go much further with this but stop at making only a few of the things that one could select at run time actual input file parameters. In order to see what could be done in this regard, take a look at  [2.x.3374]  "step-29" and step-33. [2.x.3375]  

 [2.x.3376] We use the FunctionParser class to make the potential  [2.x.3377]  a run-time parameter that can be specified in the input file as a formula. [2.x.3378]  

 [2.x.3379]  

The rest of the program follows in a pretty straightforward way from step-4. 


examples/step-36/doc/results.dox 



[1.x.1262] 

[1.x.1263] 

The problem's input is parameterized by an input file  [2.x.3380]  which could, for example, contain the following text: 

[1.x.1264] 



Here, the potential is zero inside the domain, and we know that the eigenvalues are given by  [2.x.3381]  where  [2.x.3382] . Eigenfunctions are sines and cosines with  [2.x.3383]  and  [2.x.3384]  periods in  [2.x.3385]  and  [2.x.3386]  directions. This matches the output our program generates: 

[1.x.1265] These eigenvalues are exactly the ones that correspond to pairs  [2.x.3387] ,  [2.x.3388]  and  [2.x.3389] ,  [2.x.3390] , and  [2.x.3391] . A visualization of the corresponding eigenfunctions would look like this: 

 [2.x.3392]  

[1.x.1266] 

It is always worth playing a few games in the playground! So here goes with a few suggestions: 

 [2.x.3393]  

 [2.x.3394]  The potential used above (called the [1.x.1267] because it is a flat potential surrounded by infinitely high walls) is interesting because it allows for analytically known solutions. Apart from that, it is rather boring, however. That said, it is trivial to play around with the potential by just setting it to something different in the input file. For example, let us assume that we wanted to work with the following potential in 2d: 

[1.x.1268] 

In other words, the potential is -100 in two sectors of a circle of radius 0.75, -5 in the other two sectors, and zero outside the circle. We can achieve this by using the following in the input file: 

[1.x.1269] 

If in addition we also increase the mesh refinement by one level, we get the following results: 

[1.x.1270] 



The output file also contains an interpolated version of the potential, which looks like this (note that as expected the lowest few eigenmodes have probability densities  [2.x.3395]  that are significant only where the potential is the lowest, i.e. in the top right and bottom left sector of inner circle of the potential): 

 [2.x.3396]  

The first five eigenfunctions are now like this: 

 [2.x.3397]  

 [2.x.3398]  In our derivation of the problem we have assumed that the particle is confined to a domain  [2.x.3399]  and that at the boundary of this domain its probability  [2.x.3400]  of being is zero. This is equivalent to solving the eigenvalue problem on all of  [2.x.3401]  and assuming that the energy potential is finite only inside a region  [2.x.3402]  and infinite outside. It is relatively easy to show that  [2.x.3403]  at all locations  [2.x.3404]  where  [2.x.3405] . So the question is what happens if our potential is not of this form, i.e. there is no bounded domain outside of which the potential is infinite? In that case, it may be worth to just consider a very large domain at the boundary of which  [2.x.3406]  is at least very large, if not infinite. Play around with a few cases like this and explore how the spectrum and eigenfunctions change as we make the computational region larger and larger. 

 [2.x.3407]  What happens if we investigate the simple harmonic oscillator problem  [2.x.3408] ? This potential is exactly of the form discussed in the previous paragraph and has hyper spherical symmetry. One may want to use a large spherical domain with a large outer radius, to approximate the whole-space problem (say, by invoking  [2.x.3409]  

 [2.x.3410]  The plots above show the wave function  [2.x.3411] , but the physical quantity of interest is actually the probability density  [2.x.3412]  for the particle to be at location  [2.x.3413] . Some visualization programs can compute derived quantities from the data in an input file, but we can also do so right away when creating the output file. The facility to do that is the DataPostprocessor class that can be used in conjunction with the DataOut class. Examples of how this can be done can be found in step-29 and step-33. 

 [2.x.3414]  What happens if the particle in the box has %internal degrees of freedom? For example, if the particle were a spin- [2.x.3415]  particle? In that case, we may want to start solving a vector-valued problem instead. 

 [2.x.3416]  Our implementation of the deal.II library here uses the PETScWrappers and SLEPcWrappers and is suitable for running on serial machine architecture. However, for larger grids and with a larger number of degrees-of-freedom, we may want to run our application on parallel architectures. A parallel implementation of the above code can be particularly useful here since the generalized eigenspectrum problem is somewhat more expensive to solve than the standard problems considered in most of the earlier tutorials. Fortunately, modifying the above program to be MPI compliant is a relatively straightforward procedure. A sketch of how this can be done can be found in  [2.x.3417]  "step-17". 

 [2.x.3418]  Finally, there are alternatives to using the SLEPc eigenvalue solvers. deal.II has interfaces to one of them, ARPACK (see [1.x.1271] for setup instructions), implemented in the ArpackSolver class. Here is a short and quick overview of what one would need to change to use it, provided you have a working installation of ARPACK and deal.II has been configured properly for it (see the deal.II [1.x.1272] file): 

First, in order to use the ARPACK interfaces, we can go back to using standard deal.II matrices and vectors, so we start by replacing the PETSc and SLEPc headers 

[1.x.1273] 

with these: 

[1.x.1274] 

ARPACK allows complex eigenvalues, so we will also need 

[1.x.1275] 



Secondly, we switch back to the deal.II matrix and vector definitions in the main class: 

[1.x.1276] 

and initialize them as usual in  [2.x.3419] : 

[1.x.1277] 



For solving the eigenvalue problem with ARPACK, we finally need to modify  [2.x.3420] : 

[1.x.1278] 

Note how we have used an exact decomposition (using SparseDirectUMFPACK) as a preconditioner to ARPACK.  [2.x.3421]  


examples/step-37/doc/intro.dox 

 [2.x.3422]  

[1.x.1279][1.x.1280] 

[1.x.1281] 

[1.x.1282] 

This example shows how to implement a matrix-free method, that is, a method that does not explicitly store the matrix elements, for a second-order Poisson equation with variable coefficients on a hypercube. The linear system will be solved with a multigrid method and uses large-scale parallelism with MPI. 

The major motivation for matrix-free methods is the fact that on today's processors access to main memory (i.e., for objects that do not fit in the caches) has become the bottleneck in many solvers for partial differential equations: To perform a matrix-vector product based on matrices, modern CPUs spend far more time waiting for data to arrive from memory than on actually doing the floating point multiplications and additions. Thus, if we could substitute looking up matrix elements in memory by re-computing them &mdash; or rather, the operator represented by these entries &mdash;, we may win in terms of overall run-time even if this requires a significant number of additional floating point operations. That said, to realize this with a trivial implementation is not enough and one needs to really look at the details to gain in performance. This tutorial program and the papers referenced above show how one can implement such a scheme and demonstrates the speedup that can be obtained. 




[1.x.1283] 

In this example, we consider the Poisson problem [1.x.1284] where  [2.x.3423]  is a variable coefficient. Below, we explain how to implement a matrix-vector product for this problem without explicitly forming the matrix. The construction can, of course, be done in a similar way for other equations as well. 

We choose as domain  [2.x.3424]  and  [2.x.3425] . Since the coefficient is symmetric around the origin but the domain is not, we will end up with a non-symmetric solution. 




[1.x.1285] 

In order to find out how we can write a code that performs a matrix-vector product, but does not need to store the matrix elements, let us start at looking how a finite element matrix [1.x.1286] is assembled: 

[1.x.1287] 

In this formula, the matrix [1.x.1288]<sub>cell,loc-glob</sub> is a rectangular matrix that defines the index mapping from local degrees of freedom in the current cell to the global degrees of freedom. The information from which this operator can be built is usually encoded in the  [2.x.3426]  variable and is used in the assembly calls filling matrices in deal.II. Here, [1.x.1289]<sub>cell</sub> denotes the cell matrix associated with [1.x.1290]. 

If we are to perform a matrix-vector product, we can hence use that 

[1.x.1291] 

where [1.x.1292]<sub>cell</sub> are the values of [1.x.1293] at the degrees of freedom of the respective cell, and [1.x.1294]<sub>cell</sub>=[1.x.1295]<sub>cell</sub>[1.x.1296]<sub>cell</sub> correspondingly for the result. A naive attempt to implement the local action of the Laplacian would hence be to use the following code: 

[1.x.1297] 



Here we neglected boundary conditions as well as any hanging nodes we may have, though neither would be very difficult to include using the AffineConstraints class. Note how we first generate the local matrix in the usual way as a sum over all quadrature points for each local matrix entry. To form the actual product as expressed in the above formula, we extract the values of  [2.x.3427]  of the cell-related degrees of freedom (the action of [1.x.1298]<sub>cell,loc-glob</sub>), multiply by the local matrix (the action of [1.x.1299]<sub>cell</sub>), and finally add the result to the destination vector  [2.x.3428]  (the action of [1.x.1300]<sub>cell,loc-glob</sub><sup>T</sup>, added over all the elements). It is not more difficult than that, in principle. 

While this code is completely correct, it is very slow. For every cell, we generate a local matrix, which takes three nested loops with loop length equal to the number of local degrees of freedom to compute. The multiplication itself is then done by two nested loops, which means that it is much cheaper. 

One way to improve this is to realize that conceptually the local matrix can be thought of as the product of three matrices, 

[1.x.1301] 

where for the example of the Laplace operator the ([1.x.1302]*dim+[1.x.1303])-th element of [1.x.1304]<sub>cell</sub> is given by  [2.x.3429] . This matrix consists of  [2.x.3430]  rows and  [2.x.3431]  columns. The matrix [1.x.1305]<sub>cell</sub> is diagonal and contains the values  [2.x.3432]  (or, rather,  [2.x.3433]  dim copies of each of these values). This kind of representation of finite element matrices can often be found in the engineering literature. 

When the cell matrix is applied to a vector, 

[1.x.1306] 

one would then not form the matrix-matrix products, but rather multiply one matrix at a time with a vector from right to left so that only three successive matrix-vector products are formed. This approach removes the three nested loops in the calculation of the local matrix, which reduces the complexity of the work on one cell from something like  [2.x.3434]  to  [2.x.3435] . An interpretation of this algorithm is that we first transform the vector of values on the local DoFs to a vector of gradients on the quadrature points. In the second loop, we multiply these gradients by the integration weight and the coefficient. The third loop applies the second gradient (in transposed form), so that we get back to a vector of (Laplacian) values on the cell dofs. 

The bottleneck in the above code is the operations done by the call to  [2.x.3436]  for every  [2.x.3437] , which take about as much time as the other steps together (at least if the mesh is unstructured; deal.II can recognize that the gradients are often unchanged on structured meshes). That is certainly not ideal and we would like to do better than this. What the reinit function does is to calculate the gradient in real space by transforming the gradient on the reference cell using the Jacobian of the transformation from real to reference cell. This is done for each basis function on the cell, for each quadrature point. The Jacobian does not depend on the basis function, but it is different on different quadrature points in general. If you only build the matrix once as we've done in all previous tutorial programs, there is nothing to be optimized since  [2.x.3438]  needs to be called on every cell. In this process, the transformation is applied while computing the local matrix elements. 

In a matrix-free implementation, however, we will compute those integrals very often because iterative solvers will apply the matrix many times during the solution process. Therefore, we need to think about whether we may be able to cache some data that gets reused in the operator applications, i.e., integral computations. On the other hand, we realize that we must not cache too much data since otherwise we get back to the situation where memory access becomes the dominating factor. Therefore, we will not store the transformed gradients in the matrix [1.x.1307], as they would in general be different for each basis function and each quadrature point on every element for curved meshes. 

The trick is to factor out the Jacobian transformation and first apply the gradient on the reference cell only. This operation interpolates the vector of values on the local dofs to a vector of (unit-coordinate) gradients on the quadrature points. There, we first apply the Jacobian that we factored out from the gradient, then apply the weights of the quadrature, and finally apply the transposed Jacobian for preparing the third loop which tests by the gradients on the unit cell and sums over quadrature points. 

Let us again write this in terms of matrices. Let the matrix [1.x.1308]<sub>cell</sub> denote the cell-related gradient matrix, with each row containing the values on the quadrature points. It is constructed by a matrix-matrix product as [1.x.1309] where [1.x.1310]<sub>ref_cell</sub> denotes the gradient on the reference cell and [1.x.1311]<sup>-T</sup><sub>cell</sub> denotes the inverse transpose Jacobian of the transformation from unit to real cell (in the language of transformations, the operation represented by [1.x.1312]<sup>-T</sup><sub>cell</sub> represents a covariant transformation). [1.x.1313]<sup>-T</sup><sub>cell</sub> is block-diagonal, and the blocks size is equal to the dimension of the problem. Each diagonal block is the Jacobian transformation that goes from the reference cell to the real cell. 

Putting things together, we find that 

[1.x.1314] 

so we calculate the product (starting the local product from the right) 

[1.x.1315] 



[1.x.1316] 



Note how we create an additional FEValues object for the reference cell gradients and how we initialize it to the reference cell. The actual derivative data is then applied by the inverse, transposed Jacobians (deal.II calls the Jacobian matrix from real to unit cell inverse_jacobian, as the forward transformation is from unit to real cell). The factor  [2.x.3439]  is block-diagonal over quadrature. In this form, one realizes that variable coefficients (possibly expressed through a tensor) and general grid topologies with Jacobian transformations have a similar effect on the coefficient transforming the unit-cell derivatives. 

At this point, one might wonder why we store the matrix  [2.x.3440]  and the coefficient separately, rather than only the complete factor  [2.x.3441] . The latter would use less memory because the tensor is symmetric with six independent values in 3D, whereas for the former we would need nine entries for the inverse transposed Jacobian, one for the quadrature weight and Jacobian determinant, and one for the coefficient, totaling to 11 doubles. The reason is that the former approach allows for implementing generic differential operators through a common framework of cached data, whereas the latter specifically stores the coefficient for the Laplacian. In case applications demand for it, this specialization could pay off and would be worthwhile to consider. Note that the implementation in deal.II is smart enough to detect Cartesian or affine geometries where the Jacobian is constant throughout the cell and needs not be stored for every cell (and indeed often is the same over different cells as well). 

The final optimization that is most crucial from an operation count point of view is to make use of the tensor product structure in the basis functions. This is possible because we have factored out the gradient from the reference cell operation described by [1.x.1317]<sub>ref_cell</sub>, i.e., an interpolation operation over the completely regular data fields of the reference cell. We illustrate the process of complexity reduction in two space dimensions, but the same technique can be used in higher dimensions. On the reference cell, the basis functions are of the tensor product form  [2.x.3442] . The part of the matrix [1.x.1318]<sub>ref_cell</sub> that computes the first component has the form  [2.x.3443] , where [1.x.1319]<sub>grad,x</sub> and [1.x.1320]<sub>val,y</sub> contain the evaluation of all the 1D basis functions on all the 1D quadrature points. Forming a matrix [1.x.1321] with [1.x.1322] containing the coefficient belonging to basis function  [2.x.3444] , we get  [2.x.3445] . This reduces the complexity for computing this product from  [2.x.3446]  to  [2.x.3447] , where [1.x.1323]-1 is the degree of the finite element (i.e., equivalently, [1.x.1324] is the number of shape functions in each coordinate direction), or  [2.x.3448]  to  [2.x.3449]  in general. The reason why we look at the complexity in terms of the polynomial degree is since we want to be able to go to high degrees and possibly increase the polynomial degree [1.x.1325] instead of the grid resolution. Good algorithms for moderate degrees like the ones used here are linear in the polynomial degree independent on the dimension, as opposed to matrix-based schemes or naive evaluation through FEValues. The techniques used in the implementations of deal.II have been established in the spectral element community since the 1980s. 

Implementing a matrix-free and cell-based finite element operator requires a somewhat different program design as compared to the usual matrix assembly codes shown in previous tutorial programs. The data structures for doing this are the MatrixFree class that collects all data and issues a (parallel) loop over all cells and the FEEvaluation class that evaluates finite element basis functions by making use of the tensor product structure. 

The implementation of the matrix-free matrix-vector product shown in this tutorial is slower than a matrix-vector product using a sparse matrix for linear elements, but faster for all higher order elements thanks to the reduced complexity due to the tensor product structure and due to less memory transfer during computations. The impact of reduced memory transfer is particularly beneficial when working on a multicore processor where several processing units share access to memory. In that case, an algorithm which is computation bound will show almost perfect parallel speedup (apart from possible changes of the processor's clock frequency through turbo modes depending on how many cores are active), whereas an algorithm that is bound by memory transfer might not achieve similar speedup (even when the work is perfectly parallel and one could expect perfect scaling like in sparse matrix-vector products). An additional gain with this implementation is that we do not have to build the sparse matrix itself, which can also be quite expensive depending on the underlying differential equation. Moreover, the above framework is simple to generalize to nonlinear operations, as we demonstrate in step-48. 




[1.x.1326] 

Above, we have gone to significant lengths to implement a matrix-vector product that does not actually store the matrix elements. In many user codes, however, one wants more than just doing a few matrix-vector products &mdash; one wants to do as few of these operations as possible when solving linear systems. In theory, we could use the CG method without preconditioning; however, that would not be very efficient for the Laplacian. Rather, preconditioners are used for increasing the speed of convergence. Unfortunately, most of the more frequently used preconditioners such as SSOR, ILU or algebraic multigrid (AMG) cannot be used here because their implementation requires knowledge of the elements of the system matrix. 

One solution is to use geometric multigrid methods as shown in step-16. They are known to be very fast, and they are suitable for our purpose since all ingredients, including the transfer between different grid levels, can be expressed in terms of matrix-vector products related to a collection of cells. All one needs to do is to find a smoother that is based on matrix-vector products rather than all the matrix entries. One such candidate would be a damped Jacobi iteration that requires access to the matrix diagonal, but it is often not sufficiently good in damping all high-frequency errors. The properties of the Jacobi method can be improved by iterating it a few times with the so-called Chebyshev iteration. The Chebyshev iteration is described by a polynomial expression of the matrix-vector product where the coefficients can be chosen to achieve certain properties, in this case to smooth the high-frequency components of the error which are associated to the eigenvalues of the Jacobi-preconditioned matrix. At degree zero, the Jacobi method with optimal damping parameter is retrieved, whereas higher order corrections are used to improve the smoothing properties. The effectiveness of Chebyshev smoothing in multigrid has been demonstrated, e.g., in the article [1.x.1327][1.x.1328]. This publication also identifies one more advantage of Chebyshev smoothers that we exploit here, namely that they are easy to parallelize, whereas SOR/Gauss&ndash;Seidel smoothing relies on substitutions, for which a naive parallelization works on diagonal sub-blocks of the matrix, thereby decreases efficiency (for more detail see e.g. Y. Saad, Iterative Methods for Sparse Linear Systems, SIAM, 2nd edition, 2003, chapters 11 & 12). 

The implementation into the multigrid framework is then straightforward. The multigrid implementation in this program is similar to step-16 and includes adaptivity. 




[1.x.1329] 

The computational kernels for evaluation in FEEvaluation are written in a way to optimally use computational resources. To achieve this, they do not operate on double data types, but something we call VectorizedArray (check e.g. the return type of  [2.x.3450]  which is VectorizedArray for a scalar element and a Tensor of VectorizedArray for a vector finite element). VectorizedArray is a short array of doubles or float whose length depends on the particular computer system in use. For example, systems based on x86-64 support the streaming SIMD extensions (SSE), where the processor's vector units can process two doubles (or four single-precision floats) by one CPU instruction. Newer processors (from about 2012 and onwards) support the so-called advanced vector extensions (AVX) with 256 bit operands, which can use four doubles and eight floats, respectively. Vectorization is a single-instruction/multiple-data (SIMD) concept, that is, one CPU instruction is used to process multiple data values at once. Often, finite element programs do not use vectorization explicitly as the benefits of this concept are only in arithmetic intensive operations. The bulk of typical finite element workloads are memory bandwidth limited (operations on sparse matrices and vectors) where the additional computational power is useless. 

Behind the scenes, optimized BLAS packages might heavily rely on vectorization, though. Also, optimizing compilers might automatically transform loops involving standard code into more efficient vectorized form (deal.II uses OpenMP SIMD pragmas inside the regular loops of vector updates). However, the data flow must be very regular in order for compilers to produce efficient code. For example, already the automatic vectorization of the prototype operation that benefits from vectorization, matrix-matrix products, fails on most compilers (as of writing this tutorial in early 2012 and updating in late 2016, neither gcc nor the Intel compiler manage to produce useful vectorized code for the  [2.x.3451]  function, and not even on the simpler case where the matrix bounds are compile-time constants instead of run-time constants as in  [2.x.3452]  The main reason for this is that the information to be processed at the innermost loop (that is where vectorization is applied) is not necessarily a multiple of the vector length, leaving parts of the resources unused. Moreover, the data that can potentially be processed together might not be laid out in a contiguous way in memory or not with the necessary alignment to address boundaries that are needed by the processor. Or the compiler might not be able to prove that data arrays do not overlap when loading several elements at once. 

In the matrix-free implementation in deal.II, we have therefore chosen to apply vectorization at the level which is most appropriate for finite element computations: The cell-wise computations are typically exactly the same for all cells (except for indices in the indirect addressing used while reading from and writing to vectors), and hence SIMD can be used to process several cells at once. In all what follows, you can think of a VectorizedArray to hold data from several cells. Remember that it is not related to the spatial dimension and the number of elements e.g. in a Tensor or Point. 

Note that vectorization depends on the CPU the code is running on and for which the code is compiled. In order to generate the fastest kernels of FEEvaluation for your computer, you should compile deal.II with the so-called [1.x.1330] processor variant. When using the gcc compiler, it can be enabled by setting the variable <tt>CMAKE_CXX_FLAGS</tt> to <tt>"-march=native"</tt> in the cmake build settings (on the command line, specify <tt>-DCMAKE_CXX_FLAGS="-march=native"</tt>, see the deal.II README for more information). Similar options exist for other compilers. We output the current vectorization length in the run() function of this example. 




[1.x.1331] 

As mentioned above, all components in the matrix-free framework can easily be parallelized with MPI using domain decomposition. Thanks to the easy access to large-scale parallel meshes through p4est (see step-40 for details) in deal.II, and the fact that cell-based loops with matrix-free evaluation [1.x.1332] need a decomposition of the mesh into chunks of roughly equal size on each processor, there is relatively little to do to write a parallel program working with distributed memory. While other tutorial programs using MPI have relied on either PETSc or Trilinos, this program uses deal.II's own parallel vector facilities. 

The deal.II parallel vector class,  [2.x.3453]  holds the processor-local part of the solution as well as data fields for ghosted DoFs, i.e. DoFs that are owned by a remote processor but accessed by cells that are owned by the present processor. In the  [2.x.3454]  "glossary" these degrees of freedom are referred to as locally active degrees of freedom. The function  [2.x.3455]  provides a method that sets this design. Note that hanging nodes can relate to additional ghosted degrees of freedom that must be included in the distributed vector but are not part of the locally active DoFs in the sense of the  [2.x.3456]  "glossary". Moreover, the distributed vector holds the MPI metadata for DoFs that are owned locally but needed by other processors. A benefit of the design of this vector class is the way ghosted entries are accessed. In the storage scheme of the vector, the data array extends beyond the processor-local part of the solution with further vector entries available for the ghosted degrees of freedom. This gives a contiguous index range for all locally active degrees of freedom. (Note that the index range depends on the exact configuration of the mesh.) Since matrix-free operations can be thought of doing linear algebra that is performance critical, and performance-critical code cannot waste time on doing MPI-global to MPI-local index translations, the availability of an index spaces local to one MPI rank is fundamental. The way things are accessed here is a direct array access. This is provided through  [2.x.3457]  but it is actually rarely needed because all of this happens internally in FEEvaluation. 

The design of  [2.x.3458]  is similar to the  [2.x.3459]  and  [2.x.3460]  data types we have used in step-40 and step-32 before, but since we do not need any other parallel functionality of these libraries, we use the  [2.x.3461]  class of deal.II instead of linking in another large library in this tutorial program. Also note that the PETSc and Trilinos vectors do not provide the fine-grained control over ghost entries with direct array access because they abstract away the necessary implementation details. 


examples/step-37/doc/results.dox 



[1.x.1333] 

[1.x.1334] 

Since this example solves the same problem as step-5 (except for a different coefficient), there is little to say about the solution. We show a picture anyway, illustrating the size of the solution through both isocontours and volume rendering: 

 [2.x.3462]  

Of more interest is to evaluate some aspects of the multigrid solver. When we run this program in 2D for quadratic ( [2.x.3463] ) elements, we get the following output (when run on one core in release mode): 

[1.x.1335] 



As in step-16, we see that the number of CG iterations remains constant with increasing number of degrees of freedom. A constant number of iterations (together with optimal computational properties) means that the computing time approximately quadruples as the problem size quadruples from one cycle to the next. The code is also very efficient in terms of storage. Around 2-4 million degrees of freedom fit into 1 GB of memory, see also the MPI results below. An interesting fact is that solving one linear system is cheaper than the setup, despite not building a matrix (approximately half of which is spent in the  [2.x.3464]  and  [2.x.3465]  calls). This shows the high efficiency of this approach, but also that the deal.II data structures are quite expensive to set up and the setup cost must be amortized over several system solves. 

Not much changes if we run the program in three spatial dimensions. Since we use uniform mesh refinement, we get eight times as many elements and approximately eight times as many degrees of freedom with each cycle: 

[1.x.1336] 



Since it is so easy, we look at what happens if we increase the polynomial degree. When selecting the degree as four in 3D, i.e., on  [2.x.3466]  elements, by changing the line <code>const unsigned int degree_finite_element=4;</code> at the top of the program, we get the following program output: 

[1.x.1337] 



Since  [2.x.3467]  elements on a certain mesh correspond to  [2.x.3468]  elements on half the mesh size, we can compare the run time at cycle 4 with fourth degree polynomials with cycle 5 using quadratic polynomials, both at 2.1 million degrees of freedom. The surprising effect is that the solver for  [2.x.3469]  element is actually slightly faster than for the quadratic case, despite using one more linear iteration. The effect that higher-degree polynomials are similarly fast or even faster than lower degree ones is one of the main strengths of matrix-free operator evaluation through sum factorization, see the [1.x.1338]. This is fundamentally different to matrix-based methods that get more expensive per unknown as the polynomial degree increases and the coupling gets denser. 

In addition, also the setup gets a bit cheaper for higher order, which is because fewer elements need to be set up. 

Finally, let us look at the timings with degree 8, which corresponds to another round of mesh refinement in the lower order methods: 

[1.x.1339] 



Here, the initialization seems considerably slower than before, which is mainly due to the computation of the diagonal of the matrix, which actually computes a 729 x 729 matrix on each cell and throws away everything but the diagonal. The solver times, however, are again very close to the quartic case, showing that the linear increase with the polynomial degree that is theoretically expected is almost completely offset by better computational characteristics and the fact that higher order methods have a smaller share of degrees of freedom living on several cells that add to the evaluation complexity. 

[1.x.1340] 

In order to understand the capabilities of the matrix-free implementation, we compare the performance of the 3d example above with a sparse matrix implementation based on  [2.x.3470]  by measuring both the computation times for the initialization of the problem (distribute DoFs, setup and assemble matrices, setup multigrid structures) and the actual solution for the matrix-free variant and the variant based on sparse matrices. We base the preconditioner on float numbers and the actual matrix and vectors on double numbers, as shown above. Tests are run on an Intel Core i7-5500U notebook processor (two cores and [1.x.1341] support, i.e., four operations on doubles can be done with one CPU instruction, which is heavily used in FEEvaluation), optimized mode, and two MPI ranks. 

 [2.x.3471]  

The table clearly shows that the matrix-free implementation is more than twice as fast for the solver, and more than six times as fast when it comes to initialization costs. As the problem size is made a factor 8 larger, we note that the times usually go up by a factor eight, too (as the solver iterations are constant at six). The main deviation is in the sparse matrix between 5k and 36k degrees of freedom, where the time increases by a factor 12. This is the threshold where the (L3) cache in the processor can no longer hold all data necessary for the matrix-vector products and all matrix elements must be fetched from main memory. 

Of course, this picture does not necessarily translate to all cases, as there are problems where knowledge of matrix entries enables much better solvers (as happens when the coefficient is varying more strongly than in the above example). Moreover, it also depends on the computer system. The present system has good memory performance, so sparse matrices perform comparably well. Nonetheless, the matrix-free implementation gives a nice speedup already for the [1.x.1342]<sub>2</sub> elements used in this example. This becomes particularly apparent for time-dependent or nonlinear problems where sparse matrices would need to be reassembled over and over again, which becomes much easier with this class. And of course, thanks to the better complexity of the products, the method gains increasingly larger advantages when the order of the elements increases (the matrix-free implementation has costs 4[1.x.1343]<sup>2</sup>[1.x.1344] per degree of freedom, compared to 2[1.x.1345] for the sparse matrix, so it will win anyway for order 4 and higher in 3d). 

[1.x.1346] 

As explained in the introduction and the in-code comments, this program can be run in parallel with MPI. It turns out that geometric multigrid schemes work really well and can scale to very large machines. To the authors' knowledge, the geometric multigrid results shown here are the largest computations done with deal.II as of late 2016, run on up to 147,456 cores of the [1.x.1347]. The ingredients for scalability beyond 1000 cores are that no data structure that depends on the global problem size is held in its entirety on a single processor and that the communication is not too frequent in order not to run into latency issues of the network.  For PDEs solved with iterative solvers, the communication latency is often the limiting factor, rather than the throughput of the network. For the example of the SuperMUC system, the point-to-point latency between two processors is between 1e-6 and 1e-5 seconds, depending on the proximity in the MPI network. The matrix-vector products with  [2.x.3472]  from this class involves several point-to-point communication steps, interleaved with computations on each core. The resulting latency of a matrix-vector product is around 1e-4 seconds. Global communication, for example an  [2.x.3473]  operation that accumulates the sum of a single number per rank over all ranks in the MPI network, has a latency of 1e-4 seconds. The multigrid V-cycle used in this program is also a form of global communication. Think about the coarse grid solve that happens on a single processor: It accumulates the contributions from all processors before it starts. When completed, the coarse grid solution is transferred to finer levels, where more and more processors help in smoothing until the fine grid. Essentially, this is a tree-like pattern over the processors in the network and controlled by the mesh. As opposed to the  [2.x.3474]  operations where the tree in the reduction is optimized to the actual links in the MPI network, the multigrid V-cycle does this according to the partitioning of the mesh. Thus, we cannot expect the same optimality. Furthermore, the multigrid cycle is not simply a walk up and down the refinement tree, but also communication on each level when doing the smoothing. In other words, the global communication in multigrid is more challenging and related to the mesh that provides less optimization opportunities. The measured latency of the V-cycle is between 6e-3 and 2e-2 seconds, i.e., the same as 60 to 200 MPI_Allreduce operations. 

The following figure shows a scaling experiments on  [2.x.3475]  elements. Along the lines, the problem size is held constant as the number of cores is increasing. When doubling the number of cores, one expects a halving of the computational time, indicated by the dotted gray lines. The results show that the implementation shows almost ideal behavior until an absolute time of around 0.1 seconds is reached. The solver tolerances have been set such that the solver performs five iterations. This way of plotting data is the [1.x.1348] of the algorithm. As we go to very large core counts, the curves flatten out a bit earlier, which is because of the communication network in SuperMUC where communication between processors farther away is slightly slower. 

 [2.x.3476]  

In addition, the plot also contains results for [1.x.1349] that lists how the algorithm behaves as both the number of processor cores and elements is increased at the same pace. In this situation, we expect that the compute time remains constant. Algorithmically, the number of CG iterations is constant at 5, so we are good from that end. The lines in the plot are arranged such that the top left point in each data series represents the same size per processor, namely 131,072 elements (or approximately 3.5 million degrees of freedom per core). The gray lines indicating ideal strong scaling are by the same factor of 8 apart. The results show again that the scaling is almost ideal. The parallel efficiency when going from 288 cores to 147,456 cores is at around 75% for a local problem size of 750,000 degrees of freedom per core which takes 1.0s on 288 cores, 1.03s on 2304 cores, 1.19s on 18k cores, and 1.35s on 147k cores. The algorithms also reach a very high utilization of the processor. The largest computation on 147k cores reaches around 1.7 PFLOPs/s on SuperMUC out of an arithmetic peak of 3.2 PFLOPs/s. For an iterative PDE solver, this is a very high number and significantly more is often only reached for dense linear algebra. Sparse linear algebra is limited to a tenth of this value. 

As mentioned in the introduction, the matrix-free method reduces the memory consumption of the data structures. Besides the higher performance due to less memory transfer, the algorithms also allow for very large problems to fit into memory. The figure below shows the computational time as we increase the problem size until an upper limit where the computation exhausts memory. We do this for 1k cores, 8k cores, and 65k cores and see that the problem size can be varied over almost two orders of magnitude with ideal scaling. The largest computation shown in this picture involves 292 billion ( [2.x.3477] ) degrees of freedom. On a DG computation of 147k cores, the above algorithms were also run involving up to 549 billion (2^39) DoFs. 

 [2.x.3478]  

Finally, we note that while performing the tests on the large-scale system shown above, improvements of the multigrid algorithms in deal.II have been developed. The original version contained the sub-optimal code based on MGSmootherPrecondition where some MPI_Allreduce commands (checking whether all vector entries are zero) were done on each smoothing operation on each level, which only became apparent on 65k cores and more. However, the following picture shows that the improvement already pay off on a smaller scale, here shown on computations on up to 14,336 cores for  [2.x.3479]  elements: 

 [2.x.3480]  




[1.x.1350] 

As explained in the code, the algorithm presented here is prepared to run on adaptively refined meshes. If only part of the mesh is refined, the multigrid cycle will run with local smoothing and impose Dirichlet conditions along the interfaces which differ in refinement level for smoothing through the  [2.x.3481]  class. Due to the way the degrees of freedom are distributed over levels, relating the owner of the level cells to the owner of the first descendant active cell, there can be an imbalance between different processors in MPI, which limits scalability by a factor of around two to five. 

[1.x.1351] 

[1.x.1352] 

As mentioned above the code is ready for locally adaptive h-refinement. For the Poisson equation one can employ the Kelly error indicator, implemented in the KellyErrorEstimator class. However one needs to be careful with the ghost indices of parallel vectors. In order to evaluate the jump terms in the error indicator, each MPI process needs to know locally relevant DoFs. However  [2.x.3482]  function initializes the vector only with some locally relevant DoFs. The ghost indices made available in the vector are a tight set of only those indices that are touched in the cell integrals (including constraint resolution). This choice has performance reasons, because sending all locally relevant degrees of freedom would be too expensive compared to the matrix-vector product. Consequently the solution vector as-is is not suitable for the KellyErrorEstimator class. The trick is to change the ghost part of the partition, for example using a temporary vector and  [2.x.3483]  as shown below. 

[1.x.1353] 



[1.x.1354] 

This program is parallelized with MPI only. As an alternative, the MatrixFree loop can also be issued in hybrid mode, for example by using MPI parallelizing over the nodes of a cluster and with threads through Intel TBB within the shared memory region of one node. To use this, one would need to both set the number of threads in the MPI_InitFinalize data structure in the main function, and set the  [2.x.3484]  to partition_color to actually do the loop in parallel. This use case is discussed in step-48. 

[1.x.1355] 

The presented program assumes homogeneous Dirichlet boundary conditions. When going to non-homogeneous conditions, the situation is a bit more intricate. To understand how to implement such a setting, let us first recall how these arise in the mathematical formulation and how they are implemented in a matrix-based variant. In essence, an inhomogeneous Dirichlet condition sets some of the nodal values in the solution to given values rather than determining them through the variational principles, 

[1.x.1356] 

where  [2.x.3485]  denotes the nodal values of the solution and  [2.x.3486]  denotes the set of all nodes. The set  [2.x.3487]  is the subset of the nodes that are subject to Dirichlet boundary conditions where the solution is forced to equal  [2.x.3488]  as the interpolation of boundary values on the Dirichlet-constrained node points  [2.x.3489] . We then insert this solution representation into the weak form, e.g. the Laplacian shown above, and move the known quantities to the right hand side: 

[1.x.1357] 

In this formula, the equations are tested for all basis functions  [2.x.3490]  with  [2.x.3491]  that are not related to the nodes constrained by Dirichlet conditions. 

In the implementation in deal.II, the integrals  [2.x.3492]  on the right hand side are already contained in the local matrix contributions we assemble on each cell. When using  [2.x.3493]  as first described in the step-6 and step-7 tutorial programs, we can account for the contribution of inhomogeneous constraints [1.x.1358] by multiplying the columns [1.x.1359] and rows [1.x.1360] of the local matrix according to the integrals  [2.x.3494]  by the inhomogeneities and subtracting the resulting from the position [1.x.1361] in the global right-hand-side vector, see also the  [2.x.3495]  module. In essence, we use some of the integrals that get eliminated from the left hand side of the equation to finalize the right hand side contribution. Similar mathematics are also involved when first writing all entries into a left hand side matrix and then eliminating matrix rows and columns by  [2.x.3496]  

In principle, the components that belong to the constrained degrees of freedom could be eliminated from the linear system because they do not carry any information. In practice, in deal.II we always keep the size of the linear system the same to avoid handling two different numbering systems and avoid confusion about the two different index sets. In order to ensure that the linear system does not get singular when not adding anything to constrained rows, we then add dummy entries to the matrix diagonal that are otherwise unrelated to the real entries. 

In a matrix-free method, we need to take a different approach, since the  [2.x.3497]  LaplaceOperator class represents the matrix-vector product of a [1.x.1362] operator (the left-hand side of the last formula).  It does not matter whether the AffineConstraints object passed to the  [2.x.3498]  contains inhomogeneous constraints or not, the  [2.x.3499]  call will only resolve the homogeneous part of the constraints as long as it represents a [1.x.1363] operator. 

In our matrix-free code, the right hand side computation where the contribution of inhomogeneous conditions ends up is completely decoupled from the matrix operator and handled by a different function above. Thus, we need to explicitly generate the data that enters the right hand side rather than using a byproduct of the matrix assembly. Since we already know how to apply the operator on a vector, we could try to use those facilities for a vector where we only set the Dirichlet values: 

[1.x.1364] 

or, equivalently, if we already had filled the inhomogeneous constraints into an AffineConstraints object, 

[1.x.1365] 



We could then pass the vector  [2.x.3500]  to the  [2.x.3501]   [2.x.3502]  function and add the new contribution to the  [2.x.3503]  system_rhs vector that gets filled in the  [2.x.3504]  function. However, this idea does not work because the  [2.x.3505]  call used inside the vmult() functions assumes homogeneous values on all constraints (otherwise the operator would not be a linear operator but an affine one). To also retrieve the values of the inhomogeneities, we could select one of two following strategies. 

[1.x.1366] 

The class FEEvaluation has a facility that addresses precisely this requirement: For non-homogeneous Dirichlet values, we do want to skip the implicit imposition of homogeneous (Dirichlet) constraints upon reading the data from the vector  [2.x.3506]  For example, we could extend the  [2.x.3507]   [2.x.3508]  function to deal with inhomogeneous Dirichlet values as follows, assuming the Dirichlet values have been interpolated into the object  [2.x.3509]  

[1.x.1367] 



In this code, we replaced the  [2.x.3510]  function for the tentative solution vector by  [2.x.3511]  that ignores all constraints. Due to this setup, we must make sure that other constraints, e.g. by hanging nodes, are correctly distributed to the input vector already as they are not resolved as in  [2.x.3512]  Inside the loop, we then evaluate the Laplacian and repeat the second derivative call with  [2.x.3513]  from the  [2.x.3514]  class, but with the sign switched since we wanted to subtract the contribution of Dirichlet conditions on the right hand side vector according to the formula above. When we invoke the  [2.x.3515]  call, we then set both arguments regarding the value slot and first derivative slot to true to account for both terms added in the loop over quadrature points. Once the right hand side is assembled, we then go on to solving the linear system for the homogeneous problem, say involving a variable  [2.x.3516]  After solving, we can add  [2.x.3517]  to the  [2.x.3518]  vector that contains the final (inhomogeneous) solution. 

Note that the negative sign for the Laplacian alongside with a positive sign for the forcing that we needed to build the right hand side is a more general concept: We have implemented nothing else than Newton's method for nonlinear equations, but applied to a linear system. We have used an initial guess for the variable  [2.x.3519]  in terms of the Dirichlet boundary conditions and computed a residual  [2.x.3520] . The linear system was then solved as  [2.x.3521]  and we finally computed  [2.x.3522] . For a linear system, we obviously reach the exact solution after a single iteration. If we wanted to extend the code to a nonlinear problem, we would rename the  [2.x.3523]  function into a more descriptive name like  [2.x.3524]  assemble_residual() that computes the (weak) form of the residual, whereas the  [2.x.3525]  function would get the linearization of the residual with respect to the solution variable. 

[1.x.1368] 

A second alternative to get the right hand side that re-uses the  [2.x.3526]   [2.x.3527]  function is to instead add a second LaplaceOperator that skips Dirichlet constraints. To do this, we initialize a second MatrixFree object which does not have any boundary value constraints. This  [2.x.3528]  object is then passed to a  [2.x.3529]  class instance  [2.x.3530]  inhomogeneous_operator that is only used to create the right hand side: 

[1.x.1369] 



A more sophisticated implementation of this technique could reuse the original MatrixFree object. This can be done by initializing the MatrixFree object with multiple blocks, where each block corresponds to a different AffineConstraints object. Doing this would require making substantial modifications to the LaplaceOperator class, but the  [2.x.3531]  class that comes with the library can do this. See the discussion on blocks in  [2.x.3532]  for more information on how to set up blocks. 


examples/step-38/doc/intro.dox 

 [2.x.3533]  

[1.x.1370] 

[1.x.1371] 

[1.x.1372] 

In this example, we show how to solve a partial differential equation (PDE) on a codimension one surface  [2.x.3534]  made of quadrilaterals, i.e. on a surface in 3d or a line in 2d. We focus on the following elliptic second order PDE 

[1.x.1373] 

which generalized the Laplace equation we have previously solved in several of the early tutorial programs. Our implementation is based on step-4. step-34 also solves problems on lower dimensional surfaces; however, there we only consider integral equations that do not involve derivatives on the solution variable, while here we actually have to investigate what it means to take derivatives of a function only defined on a (possibly curved) surface. 

In order to define the above operator, we start by introducing some notations. Let  [2.x.3535]  be a parameterization of a surface  [2.x.3536]  from a reference element  [2.x.3537] , i.e. each point  [2.x.3538]  induces a point  [2.x.3539] . Then let 

[1.x.1374] 

denotes the corresponding first fundamental form, where  [2.x.3540]  is the derivative (Jacobian) of the mapping. In the following,  [2.x.3541]  will be either the entire surface  [2.x.3542]  or, more convenient for the finite element method, any face  [2.x.3543] , where  [2.x.3544]  is a partition (triangulation) of  [2.x.3545]  constituted of quadrilaterals. We are now in position to define the tangential gradient of a function  [2.x.3546]  by 

[1.x.1375] 

The surface Laplacian (also called the Laplace-Beltrami operator) is then defined as   [2.x.3547] . Note that an alternate way to compute the surface gradient on smooth surfaces  [2.x.3548]  is 

[1.x.1376] 

where  [2.x.3549]  is a "smooth" extension of  [2.x.3550]  in a tubular neighborhood of  [2.x.3551]  and  [2.x.3552]  is the normal of  [2.x.3553] . Since  [2.x.3554] , we deduce 

[1.x.1377] 

Worth mentioning, the term  [2.x.3555]  appearing in the above expression is the total curvature of the surface (sum of principal curvatures). 

As usual, we are only interested in weak solutions for which we can use  [2.x.3556]  finite elements (rather than requiring  [2.x.3557]  continuity as for strong solutions). We therefore resort to the weak formulation 

[1.x.1378] 

and take advantage of the partition  [2.x.3558]  to further write 

[1.x.1379] 

Moreover, each integral in the above expression is computed in the reference element  [2.x.3559]  so that 

[1.x.1380] 

and 

[1.x.1381] 

Finally, we use a quadrature formula defined by points  [2.x.3560]  and weights  [2.x.3561]  to evaluate the above integrals and obtain 

[1.x.1382] 

and 

[1.x.1383] 




Fortunately, deal.II has already all the tools to compute the above expressions. In fact, they barely differ from the ways in which we solve the usual Laplacian, only requiring the surface coordinate mapping to be provided in the constructor of the FEValues class. This surface description given, in the codimension one surface case, the two routines  [2.x.3562]  and  [2.x.3563]  return 

[1.x.1384] 

This provides exactly the terms we need for our computations. 

On a more general note, details for the finite element approximation on surfaces can be found for instance in [Dziuk, in Partial differential equations and calculus of variations 1357, Lecture Notes in Math., 1988], [Demlow, SIAM J. Numer. Anal.  47(2), 2009] and [Bonito, Nochetto, and Pauletti, SIAM J. Numer. Anal. 48(5), 2010]. 




[1.x.1385] 

In general when you want to test numerically the accuracy and/or order of convergence of an algorithm you need to provide an exact solution. The usual trick is to pick a function that we want to be the solution, then apply the differential operator to it that defines a forcing term for the right hand side. This is what we do in this example. In the current case, the form of the domain is obviously also essential. 

We produce one test case for a 2d problem and another one for 3d: 

 [2.x.3564]   [2.x.3565]    In 2d, let's choose as domain a half circle. On this domain, we choose the   function  [2.x.3566]  as the solution. To compute the right hand   side, we have to compute the surface Laplacian of the   solution function. There are (at least) two ways to do that. The first one   is to project away the normal derivative as described above using the natural extension of  [2.x.3567]  (still denoted by  [2.x.3568] ) over  [2.x.3569] , i.e. to compute   [1.x.1386] 

  where  [2.x.3570]  is the total curvature of  [2.x.3571] .   Since we are on the unit circle,  [2.x.3572]  and  [2.x.3573]  so that   [1.x.1387] 



  A somewhat simpler way, at least for the current case of a curve in   two-dimensional space, is to note that we can map the interval  [2.x.3574]  onto the domain  [2.x.3575]  using the transformation    [2.x.3576] .   At position  [2.x.3577] , the value of the solution is then    [2.x.3578] .   Taking into account that the transformation is length preserving, i.e. a   segment of length  [2.x.3579]  is mapped onto a piece of curve of exactly the same   length, the tangential Laplacian then satisfies   [1.x.1388] 

  which is of course the same result as we had above.  [2.x.3580]   [2.x.3581]    In 3d, the domain is again half of the surface of the unit ball, i.e. a half   sphere or dome. We choose  [2.x.3582]  as   the solution. We can compute the right hand side of the   equation,  [2.x.3583] , in the same way as the method above (with  [2.x.3584] ), yielding an   awkward and lengthy expression. You can find the full expression in the   source code.  [2.x.3585]   [2.x.3586]  

In the program, we will also compute the  [2.x.3587]  seminorm error of the solution. Since the solution function and its numerical approximation are only defined on the manifold, the obvious definition of this error functional is  [2.x.3588] . This requires us to provide the [1.x.1389] gradient  [2.x.3589]  to the function  [2.x.3590]  (first introduced in step-7), which we will do by implementing the function  [2.x.3591]  in the program below. 




[1.x.1390] 

If you've read through step-4 and understand the discussion above of how solution and right hand side correspond to each other, you will be immediately familiar with this program as well. In fact, there are only two things that are of significance: 

- The way we generate the mesh that triangulates the computational domain. 

- The way we use Mapping objects to describe that the domain on which we solve   the partial differential equation is not planar but in fact curved. 

Mapping objects were already introduced in step-10 and step-11 and as explained there, there is usually not a whole lot you have to know about how they work as long as you have a working description of how the boundary looks. In essence, we will simply declare an appropriate object of type MappingQ that will automatically obtain the boundary description from the Triangulation. The mapping object will then be passed to the appropriate functions, and we will get a boundary description for half circles or half spheres that is predefined in the library. 

The rest of the program follows closely step-4 and, as far as computing the error, step-7. Some aspects of this program, in particular the use of two template arguments on the classes Triangulation, DoFHandler, and similar, are already described in detail in step-34; you may wish to read through this tutorial program as well. 


examples/step-38/doc/results.dox 



[1.x.1391] 

When you run the program, the following output should be printed on screen: 

[1.x.1392] 




By playing around with the number of global refinements in the  [2.x.3592]  function you increase or decrease mesh refinement. For example, doing one more refinement and only running the 3d surface problem yields the following output: 

[1.x.1393] 



This is what we expect: make the mesh size smaller by a factor of two and the error goes down by a factor of four (remember that we use bi-quadratic elements). The full sequence of errors from one to five refinements looks like this, neatly following the theoretically predicted pattern: 

[1.x.1394] 



Finally, the program produces graphical output that we can visualize. Here is a plot of the results: 

 [2.x.3593]  

The program also works for 1d curves in 2d, not just 2d surfaces in 3d. You can test this by changing the template argument in  [2.x.3594]  like so: 

[1.x.1395] 

The domain is a curve in 2d, and we can visualize the solution by using the third dimension (and color) to denote the value of the function  [2.x.3595] . This then looks like so (the white curve is the domain, the colored curve is the solution extruded into the third dimension, clearly showing the change in sign as the curve moves from one quadrant of the domain into the adjacent one): 

 [2.x.3596]  


[1.x.1396] 

[1.x.1397] 

Computing on surfaces only becomes interesting if the surface is more interesting than just a half sphere. To achieve this, deal.II can read meshes that describe surfaces through the usual GridIn class. Or, in case you have an analytic description, a simple mesh can sometimes be stretched and bent into a shape we are interested in. 

Let us consider a relatively simple example: we take the half sphere we used before, we stretch it by a factor of 10 in the z-direction, and then we jumble the x- and y-coordinates a bit. Let's show the computational domain and the solution first before we go into details of the implementation below: 

 [2.x.3597]  

 [2.x.3598]  

The way to produce such a mesh is by using the  [2.x.3599]  function. It needs a way to transform each individual mesh point to a different position. Let us here use the following, rather simple function (remember: stretch in one direction, jumble in the other two): 

[1.x.1398] 



If we followed the  [2.x.3600]  function, we would extract the half spherical surface mesh as before, warp it into the shape we want, and refine as often as necessary. This is not quite as simple as we'd like here, though: refining requires that we have an appropriate manifold object attached to the triangulation that describes where new vertices of the mesh should be located upon refinement. I'm sure it's possible to describe this manifold in a not-too-complicated way by simply undoing the transformation above (yielding the spherical surface again), finding the location of a new point on the sphere, and then re-warping the result. But I'm a lazy person, and since doing this is not really the point here, let's just make our lives a bit easier: we'll extract the half sphere, refine it as often as necessary, get rid of the object that describes the manifold since we now no longer need it, and then finally warp the mesh. With the function above, this would look as follows: 

[1.x.1399] 



Note that the only essential addition is the line marked with asterisks. It is worth pointing out one other thing here, though: because we detach the manifold description from the surface mesh, whenever we use a mapping object in the rest of the program, it has no curves boundary description to go on any more. Rather, it will have to use the implicit, FlatManifold class that is used on all parts of the domain not explicitly assigned a different manifold object. Consequently, whether we use MappingQ(2), MappingQ(15) or MappingQ1, each cell of our mesh will be mapped using a bilinear approximation. 

All these drawbacks aside, the resulting pictures are still pretty. The only other differences to what's in step-38 is that we changed the right hand side to  [2.x.3601]  and the boundary values (through the  [2.x.3602]  class) to  [2.x.3603] . Of course, we now no longer know the exact solution, so the computation of the error at the end of  [2.x.3604]  will yield a meaningless number. 


examples/step-39/doc/intro.dox 

[1.x.1400] 

In this program, we use the interior penalty method and Nitsche's weak boundary conditions to solve Poisson's equation. We use multigrid methods on locally refined meshes, which are generated using a bulk criterion and a standard error estimator based on cell and face residuals. All operators are implemented using the MeshWorker interface. 

Like in step-12, the discretization relies on finite element spaces, which are polynomial inside the mesh cells  [2.x.3605] , but have no continuity between cells. Since such functions have two values on each interior face  [2.x.3606] , one from each side, we define mean value and jump operators as follows: let [1.x.1401]<sub>1</sub> and [1.x.1402]<sub>2</sub> be the two cells sharing a face, and let the traces of functions [1.x.1403] and the outer normal vectors [1.x.1404][1.x.1405] be labeled accordingly. Then, on the face, we let 

[1.x.1406] 



Note, that if such an expression contains a normal vector, the averaging operator turns into a jump. The interior penalty method for the problem 

[1.x.1407] 

becomes 

[1.x.1408] 



Here,  [2.x.3607]  is the penalty parameter, which is chosen as follows: for a face [1.x.1409] of a cell [1.x.1410], compute the value 

[1.x.1411] 

where [1.x.1412] is the polynomial degree of the finite element functions and  [2.x.3608]  and  [2.x.3609]  denote the  [2.x.3610]  and  [2.x.3611]  dimensional Hausdorff measure of the corresponding object. If the face is at the boundary, choose  [2.x.3612] . For an interior face, we take the average of the two values at this face. 

In our finite element program, we distinguish three different integrals, corresponding to the sums over cells, interior faces and boundary faces above. Since the  [2.x.3613]  organizes the sums for us, we only need to implement the integrals over each mesh element. The class MatrixIntegrator below has these three functions for the left hand side of the formula, the class RHSIntegrator for the right. 

As we will see below, even the error estimate is of the same structure, since it can be written as 

[1.x.1413] 



Thus, the functions for assembling matrices, right hand side and error estimates below exhibit that these loops are all generic and can be programmed in the same way. 

This program is related to step-12b, in that it uses MeshWorker and discontinuous Galerkin methods. While there, we solved an advection problem, here it is a diffusion problem. Here, we also use multigrid preconditioning and a theoretically justified error estimator, see Karakashian and Pascal (2003). The multilevel scheme was discussed in detail in Kanschat (2004). The adaptive iteration and its convergence have been discussed (for triangular meshes) in Hoppe, Kanschat, and Warburton (2009). 


examples/step-39/doc/results.dox 



[1.x.1414] 

[1.x.1415] First, the program produces the usual logfile here stored in <tt>deallog</tt>. It reads (with omission of intermediate steps) 

[1.x.1416] 



This log for instance shows that the number of conjugate gradient iteration steps is constant at approximately 15. 

[1.x.1417] 

 [2.x.3614]  Using the perl script <tt>postprocess.pl</tt>, we extract relevant data into <tt>output.dat</tt>, which can be used to plot graphs with <tt>gnuplot</tt>. The graph above for instance was produced using the gnuplot script <tt>plot_errors.gpl</tt> via 

[1.x.1418] 



Reference data can be found in <tt>output.reference.dat</tt>. 


examples/step-4/doc/intro.dox 

[1.x.1419] 

[1.x.1420] 

 [2.x.3615]  

deal.II has a unique feature which we call ``dimension independent programming''. You may have noticed in the previous examples that many classes had a number in angle brackets suffixed to them. This is to indicate that for example the triangulation in two and three space dimensions are different, but related data %types. We could as well have called them  [2.x.3616]  instead of  [2.x.3617]  and  [2.x.3618]  to name the two classes, but this has an important drawback: assume you have a function which does exactly the same functionality, but on 2d or 3d triangulations, depending on which dimension we would like to solve the equation in presently (if you don't believe that it is the common case that a function does something that is the same in all dimensions, just take a look at the code below - there are almost no distinctions between 2d and 3d!). We would have to write the same function twice, once working on  [2.x.3619]  and once working with a  [2.x.3620] . This is an unnecessary obstacle in programming and leads to a nuisance to keep the two function in sync (at best) or difficult to find errors if the two versions get out of sync (at worst; this would probably the more common case). 





Such obstacles can be circumvented by using some template magic as provided by the C++ language: templatized classes and functions are not really classes or functions but only a pattern depending on an as-yet undefined data type parameter or on a numerical value which is also unknown at the point of definition. However, the compiler can build proper classes or functions from these templates if you provide it with the information that is needed for that. Of course, parts of the template can depend on the template parameters, and they will be resolved at the time of compilation for a specific template parameter. For example, consider the following piece of code: 

[1.x.1421] 






At the point where the compiler sees this function, it does not know anything about the actual value of  [2.x.3621] . The only thing the compiler has is a template, i.e. a blueprint, to generate functions  [2.x.3622]  if given a particular value of  [2.x.3623]  has an unknown value, there is no code the compiler can generate for the moment. 




However, if later down the compiler would encounter code that looks, for example, like this, 

[1.x.1422] 

then the compiler will deduce that the function  [2.x.3624]  for  [2.x.3625]  was requested and will compile the template above into a function with dim replaced by 2 everywhere, i.e. it will compile the function as if it were defined as 

[1.x.1423] 






However, it is worth to note that the function  [2.x.3626]  depends on the dimension as well, so in this case, the compiler will call the function  [2.x.3627]  while if dim were 3, it would call  [2.x.3628]  which might be (and actually is) a totally unrelated  function. 




The same can be done with member variables. Consider the following function, which might in turn call the above one: 

[1.x.1424] 

This function has a member variable of type  [2.x.3629] . Again, the compiler can't compile this function until it knows for which dimension. If you call this function for a specific dimension as above, the compiler will take the template, replace all occurrences of dim by the dimension for which it was called, and compile it. If you call the function several times for different dimensions, it will compile it several times, each time calling the right  [2.x.3630]  function and reserving the right amount of memory for the member variable; note that the size of a  [2.x.3631]  might, and indeed does, depend on the space dimension. 




The deal.II library is built around this concept of dimension-independent programming, and therefore allows you to program in a way that will not need to distinguish between the space dimensions. It should be noted that in only a very few places is it necessary to actually compare the dimension using  [2.x.3632] es. However, since the compiler has to compile each function for each dimension separately, even there it knows the value of  [2.x.3633]  at the time of compilation and will therefore be able to optimize away the  [2.x.3634]  statement along with the unused branch. 




In this example program, we will show how to program dimension independently (which in fact is even simpler than if you had to take care about the dimension) and we will extend the Laplace problem of the last example to a program that runs in two and three space dimensions at the same time. Other extensions are the use of a non-constant right hand side function and of non-zero boundary values. 




 [2.x.3635]  When using templates, C++ imposes all sorts of syntax constraints that make it sometimes a bit difficult to understand why exactly something has to be written this way. A typical example is the need to use the keyword  [2.x.3636]  in so many places. If you are not entirely familiar with this already, then several of these difficulties are explained in the deal.II Frequently Asked Questions (FAQ) linked to from the [1.x.1425]. 

<!--We need a blank line to end the above block properly.--> 


examples/step-4/doc/results.dox 



[1.x.1426] 


The output of the program looks as follows (the number of iterations may vary by one or two, depending on your computer, since this is often dependent on the round-off accuracy of floating point operations, which differs between processors): 

[1.x.1427] 

It is obvious that in three spatial dimensions the number of cells and therefore also the number of degrees of freedom is much higher. What cannot be seen here, is that besides this higher number of rows and columns in the matrix, there are also significantly more entries per row of the matrix in three space dimensions. Together, this leads to a much higher numerical effort for solving the system of equation, which you can feel in the run time of the two solution steps when you actually run the program. 




The program produces two files:  [2.x.3637]  and  [2.x.3638] , which can be viewed using the programs VisIt or Paraview (in case you do not have these programs, you can easily change the output format in the program to something which you can view more easily). Visualizing solutions is a bit of an art, but it can also be fun, so you should play around with your favorite visualization tool to get familiar with its functionality. Here's what I have come up with for the 2d solution: 

<p align="center">    [2.x.3639]   [2.x.3640]  

( [2.x.3641]  The picture shows the solution of the problem under consideration as a 3D plot. As can be seen, the solution is almost flat in the interior of the domain and has a higher curvature near the boundary. This, of course, is due to the fact that for Laplace's equation the curvature of the solution is equal to the right hand side and that was chosen as a quartic polynomial which is nearly zero in the interior and is only rising sharply when approaching the boundaries of the domain; the maximal values of the right hand side function are at the corners of the domain, where also the solution is moving most rapidly. It is also nice to see that the solution follows the desired quadratic boundary values along the boundaries of the domain. It can also be useful to verify a computed solution against an analytical solution. For an explanation of this technique, see step-7. 

On the other hand, even though the picture does not show the mesh lines explicitly, you can see them as little kinks in the solution. This clearly indicates that the solution hasn't been computed to very high accuracy and that to get a better solution, we may have to compute on a finer mesh. 

In three spatial dimensions, visualization is a bit more difficult. The left picture shows the solution and the mesh it was computed on on the surface of the domain. This is nice, but it has the drawback that it completely hides what is happening on the inside. The picture on the right is an attempt at visualizing the interior as well, by showing surfaces where the solution has constant values (as indicated by the legend at the top left). Isosurface pictures look best if one makes the individual surfaces slightly transparent so that it is possible to see through them and see what's behind. 

 [2.x.3642]  

 [2.x.3643]  A final remark on visualization: the idea of visualization is to give insight, which is not the same as displaying information. In particular, it is easy to overload a picture with information, but while it shows more information it makes it also more difficult to glean insight. As an example, the program I used to generate these pictures, VisIt, by default puts tick marks on every axis, puts a big fat label "X Axis" on the  [2.x.3644]  axis and similar for the other axes, shows the file name from which the data was taken in the top left and the name of the user doing so and the time and date on the bottom right. None of this is important here: the axes are equally easy to make out because the tripod at the bottom left is still visible, and we know from the program that the domain is  [2.x.3645] , so there is no need for tick marks. As a consequence, I have switched off all the extraneous stuff in the picture: the art of visualization is to reduce the picture to those parts that are important to see what one wants to see, but no more. 




[1.x.1428] 

[1.x.1429] 


Essentially the possibilities for playing around with the program are the same as for the previous one, except that they will now also apply to the 3d case. For inspiration read up on [1.x.1430]. 


examples/step-40/doc/intro.dox 

 [2.x.3646]  

[1.x.1431] 




 [2.x.3647]  As a prerequisite of this program, you need to have both PETSc and the p4est library installed. The installation of deal.II together with these two additional libraries is described in the [1.x.1432] file. Note also that to work properly, this program needs access to the Hypre preconditioner package implementing algebraic multigrid; it can be installed as part of PETSc but has to be explicitly enabled during PETSc configuration; see the page linked to from the installation instructions for PETSc. 


[1.x.1433] 

[1.x.1434] 

 [2.x.3648]  

Given today's computers, most finite element computations can be done on a single machine. The majority of previous tutorial programs therefore shows only this, possibly splitting up work among a number of processors that, however, can all access the same, shared memory space. That said, there are problems that are simply too big for a single machine and in that case the problem has to be split up in a suitable way among multiple machines each of which contributes its part to the whole. A simple way to do that was shown in step-17 and step-18, where we show how a program can use [1.x.1435] to parallelize assembling the linear system, storing it, solving it, and computing error estimators. All of these operations scale relatively trivially (for a definition of what it means for an operation to "scale", see  [2.x.3649]  "this glossary entry"), but there was one significant drawback: for this to be moderately simple to implement, each MPI processor had to keep its own copy of the entire Triangulation and DoFHandler objects. Consequently, while we can suspect (with good reasons) that the operations listed above can scale to thousands of computers and problem sizes of billions of cells and billions of degrees of freedom, building the one big mesh for the entire problem these thousands of computers are solving on every last processor is clearly not going to scale: it is going to take forever, and maybe more importantly no single machine will have enough memory to store a mesh that has a billion cells (at least not at the time of writing this). In reality, programs like step-17 and step-18 can therefore not be run on more than maybe 100 or 200 processors and even there storing the Triangulation and DoFHandler objects consumes the vast majority of memory on each machine. 

Consequently, we need to approach the problem differently: to scale to very large problems each processor can only store its own little piece of the Triangulation and DoFHandler objects. deal.II implements such a scheme in the  [2.x.3650]  namespace and the classes therein. It builds on an external library, [1.x.1436] (a play on the expression [1.x.1437] that describes the parallel storage of a hierarchically constructed mesh as a forest of quad- or oct-trees). You need to [1.x.1438] but apart from that all of its workings are hidden under the surface of deal.II. 

In essence, what the  [2.x.3651]  class and code inside the DoFHandler class do is to split the global mesh so that every processor only stores a small bit it "owns" along with one layer of "ghost" cells that surround the ones it owns. What happens in the rest of the domain on which we want to solve the partial differential equation is unknown to each processor and can only be inferred through communication with other machines if such information is needed. This implies that we also have to think about problems in a different way than we did in, for example, step-17 and step-18: no processor can have the entire solution vector for postprocessing, for example, and every part of a program has to be parallelized because no processor has all the information necessary for sequential operations. 

A general overview of how this parallelization happens is described in the  [2.x.3652]  documentation module. You should read it for a top-level overview before reading through the source code of this program. A concise discussion of many terms we will use in the program is also provided in the  [2.x.3653]  "Distributed Computing paper". It is probably worthwhile reading it for background information on how things work internally in this program. 




[1.x.1439] 

This program essentially re-solves what we already do in step-6, i.e. it solves the Laplace equation 

[1.x.1440] 

The difference of course is now that we want to do so on a mesh that may have a billion cells, with a billion or so degrees of freedom. There is no doubt that doing so is completely silly for such a simple problem, but the point of a tutorial program is, after all, not to do something useful but to show how useful programs can be implemented using deal.II. Be that as it may, to make things at least a tiny bit interesting, we choose the right hand side as a discontinuous function, 

[1.x.1441] 

so that the solution has a singularity along the sinusoidal line snaking its way through the domain. As a consequence, mesh refinement will be concentrated along this line. You can see this in the mesh picture shown below in the results section. 

Rather than continuing here and giving a long introduction, let us go straight to the program code. If you have read through step-6 and the  [2.x.3654]  documentation module, most of things that are going to happen should be familiar to you already. In fact, comparing the two programs you will notice that the additional effort necessary to make things work in %parallel is almost insignificant: the two programs have about the same number of lines of code (though step-6 spends more space on dealing with coefficients and output). In either case, the comments below will only be on the things that set step-40 apart from step-6 and that aren't already covered in the  [2.x.3655]  documentation module. 




 [2.x.3656]  This program will be able to compute on as many processors as you want to throw at it, and for as large a problem as you have the memory and patience to solve. However, there [1.x.1442] a limit: the number of unknowns can not exceed the largest number that can be stored with an object of type  [2.x.3657]  By default, this is an alias for <code>unsigned int</code>, which on most machines today is a 32-bit integer, limiting you to some 4 billion (in reality, since this program uses PETSc, you will be limited to half that as PETSc uses signed integers). However, this can be changed during configuration to use 64-bit integers, see the ReadMe file. This will give problem sizes you are unlikely to exceed anytime soon. 


examples/step-40/doc/results.dox 



[1.x.1443] 

When you run the program, on a single processor or with your local MPI installation on a few, you should get output like this: 

[1.x.1444] 



The exact numbers differ, depending on how many processors we use; this is due to the fact that the preconditioner depends on the partitioning of the problem, the solution then differs in the last few digits, and consequently the mesh refinement differs slightly. The primary thing to notice here, though, is that the number of iterations does not increase with the size of the problem. This guarantees that we can efficiently solve even the largest problems. 

When run on a sufficiently large number of machines (say a few thousand), this program can relatively easily solve problems with well over one billion unknowns in less than a minute. On the other hand, such big problems can no longer be visualized, so we also ran the program on only 16 processors. Here are a mesh, along with its partitioning onto the 16 processors, and the corresponding solution: 

 [2.x.3658]  

The mesh on the left has a mere 7,069 cells. This is of course a problem we would easily have been able to solve already on a single processor using step-6, but the point of the program was to show how to write a program that scales to many more machines. For example, here are two graphs that show how the run time of a large number of parts of the program scales on problems with around 52 and 375 million degrees of freedom if we take more and more processors (these and the next couple of graphs are taken from an earlier version of the  [2.x.3659]  "Distributed Computing paper"; updated graphs showing data of runs on even larger numbers of processors, and a lot more interpretation can be found in the final version of the paper): 

 [2.x.3660]  

As can clearly be seen, the program scales nicely to very large numbers of processors. (For a discussion of what we consider "scalable" programs, see  [2.x.3661]  "this glossary entry".) The curves, in particular the linear solver, become a bit wobbly at the right end of the graphs since each processor has too little to do to offset the cost of communication (the part of the whole problem each processor has to solve in the above two examples is only 13,000 and 90,000 degrees of freedom when 4,096 processors are used; a good rule of thumb is that parallel programs work well if each processor has at least 100,000 unknowns). 

While the strong scaling graphs above show that we can solve a problem of fixed size faster and faster if we take more and more processors, the more interesting question may be how big problems can become so that they can still be solved within a reasonable time on a machine of a particular size. We show this in the following two graphs for 256 and 4096 processors: 

 [2.x.3662]  

What these graphs show is that all parts of the program scale linearly with the number of degrees of freedom. This time, lines are wobbly at the left as the size of local problems is too small. For more discussions of these results we refer to the  [2.x.3663]  "Distributed Computing paper". 

So how large are the largest problems one can solve? At the time of writing this problem, the limiting factor is that the program uses the BoomerAMG algebraic multigrid method from the [1.x.1445] as a preconditioner, which unfortunately uses signed 32-bit integers to index the elements of a %distributed matrix. This limits the size of problems to  [2.x.3664]  degrees of freedom. From the graphs above it is obvious that the scalability would extend beyond this number, and one could expect that given more than the 4,096 machines shown above would also further reduce the compute time. That said, one can certainly expect that this limit will eventually be lifted by the hypre developers. 

On the other hand, this does not mean that deal.II cannot solve bigger problems. Indeed, step-37 shows how one can solve problems that are not just a little, but very substantially larger than anything we have shown here. 




[1.x.1446] 

[1.x.1447] 

In a sense, this program is the ultimate solver for the Laplace equation: it can essentially solve the equation to whatever accuracy you want, if only you have enough processors available. Since the Laplace equation by itself is not terribly interesting at this level of accuracy, the more interesting possibilities for extension therefore concern not so much this program but what comes beyond it. For example, several of the other programs in this tutorial have significant run times, especially in 3d. It would therefore be interesting to use the techniques explained here to extend other programs to support parallel distributed computations. We have done this for step-31 in the step-32 tutorial program, but the same would apply to, for example, step-23 and step-25 for hyperbolic time dependent problems, step-33 for gas dynamics, or step-35 for the Navier-Stokes equations. 

Maybe equally interesting is the problem of postprocessing. As mentioned above, we only show pictures of the solution and the mesh for 16 processors because 4,096 processors solving 1 billion unknowns would produce graphical output on the order of several 10 gigabyte. Currently, no program is able to visualize this amount of data in any reasonable way unless it also runs on at least several hundred processors. There are, however, approaches where visualization programs directly communicate with solvers on each processor with each visualization process rendering the part of the scene computed by the solver on this processor. Implementing such an interface would allow to quickly visualize things that are otherwise not amenable to graphical display. 


examples/step-41/doc/intro.dox 

 [2.x.3665]  

[1.x.1448] 


[1.x.1449] 

[1.x.1450] 

This example is based on the Laplace equation in 2d and deals with the question what happens if a membrane is deflected by some external force but is also constrained by an obstacle. In other words, think of a elastic membrane clamped at the boundary to a rectangular frame (we choose  [2.x.3666] ) and that sags through due to gravity acting on it. What happens now if there is an obstacle under the membrane that prevents it from reaching its equilibrium position if gravity was the only existing force? In the current example program, we will consider that under the membrane is a stair step obstacle against which gravity pushes the membrane. 

This problem is typically called the "obstacle problem" (see also [1.x.1451]), and it results in a variational inequality, rather than a variational equation when put into the weak form. We will below derive it from the classical formulation, but before we go on to discuss the mathematics let us show how the solution of the problem we will consider in this tutorial program looks to gain some intuition of what we should expect: 

 [2.x.3667]  

Here, at the left, we see the displacement of the membrane. The shape of the obstacle underneath is clearly visible. On the right, we overlay which parts of the membrane are in contact with the obstacle. We will later call this set of points the "active set" to indicate that an inequality constraint is active there. 




[1.x.1452] 

The classical formulation of the problem possesses the following form: 

[1.x.1453] 

with  [2.x.3668] .   [2.x.3669]  is a scalar valued function that denotes the vertical displacement of the membrane. The first equation is called equilibrium condition with a force of areal density  [2.x.3670] . Here, we will consider this force to be gravity. The second one is known as Hooke's Law that says that the stresses  [2.x.3671]  are proportional to the gradient of the displacements  [2.x.3672]  (the proportionality constant, often denoted by  [2.x.3673] , has been set to one here, without loss of generality; if it is constant, it can be put into the right hand side function). At the boundary we have zero Dirichlet conditions. Obviously, the first two equations can be combined to yield  [2.x.3674] . 

Intuitively, gravity acts downward and so  [2.x.3675]  is a negative function (we choose  [2.x.3676]  in this program). The first condition then means that the total force acting on the membrane is gravity plus something positive: namely the upward force that the obstacle exerts on the membrane at those places where the two of them are in contact. How big is this additional force? We don't know yet (and neither do we know "where" it actually acts) but it must be so that the membrane doesn't penetrate the obstacle. 

The fourth equality above together with the last inequality forms the obstacle condition which has to hold at every point of the whole domain. The latter of these two means that the membrane must be above the obstacle  [2.x.3677]  everywhere. The second to last equation, often called the "complementarity condition" says that where the membrane is not in contact with the obstacle (i.e., those  [2.x.3678]  where  [2.x.3679] ), then  [2.x.3680]  at these locations; in other words, no additional forces act there, as expected. On the other hand, where  [2.x.3681]  we can have  [2.x.3682] , i.e., there can be additional forces (though there don't have to be: it is possible for the membrane to just touch, not press against, the obstacle). 




[1.x.1454] 

An obvious way to obtain the variational formulation of the obstacle problem is to consider the total potential energy: 

[1.x.1455] 

We have to find a solution  [2.x.3683]  of the following minimization problem: 

[1.x.1456] 

with the convex set of admissible displacements: 

[1.x.1457] 

This set takes care of the third and fifth conditions above (the boundary values and the complementarity condition). 

Consider now the minimizer  [2.x.3684]  of  [2.x.3685]  and any other function  [2.x.3686] . Then the function 

[1.x.1458] 

takes its minimum at  [2.x.3687]  (because  [2.x.3688]  is a minimizer of the energy functional  [2.x.3689] ), so that  [2.x.3690]  for any choice of  [2.x.3691] . Note that  [2.x.3692]  because of the convexity of  [2.x.3693] . If we compute  [2.x.3694]  it yields the variational formulation we are searching for: 

[1.x.1459] 

[1.x.1460] 



This is the typical form of variational inequalities, where not just  [2.x.3695]  appears in the bilinear form but in fact  [2.x.3696] . The reason is this: if  [2.x.3697]  is not constrained, then we can find test functions  [2.x.3698]  in  [2.x.3699]  so that  [2.x.3700]  can have any sign. By choosing test functions  [2.x.3701]  so that  [2.x.3702]  it follows that the inequality can only hold for both  [2.x.3703]  and  [2.x.3704]  if the two sides are in fact equal, i.e., we obtain a variational equality. 

On the other hand, if  [2.x.3705]  then  [2.x.3706]  only allows test functions  [2.x.3707]  so that in fact  [2.x.3708] . This means that we can't test the equation with both  [2.x.3709]  and  [2.x.3710]  as above, and so we can no longer conclude that the two sides are in fact equal. Thus, this mimics the way we have discussed the complementarity condition above. 




[1.x.1461] 

The variational inequality above is awkward to work with. We would therefore like to reformulate it as an equivalent saddle point problem. We introduce a Lagrange multiplier  [2.x.3711]  and the convex cone  [2.x.3712] ,  [2.x.3713]  dual space of  [2.x.3714] ,  [2.x.3715]  of Lagrange multipliers, where  [2.x.3716]  denotes the duality pairing between  [2.x.3717]  and  [2.x.3718] . Intuitively,  [2.x.3719]  is the cone of all "non-positive functions", except that  [2.x.3720]  and so contains other objects besides regular functions as well. This yields: 

[1.x.1462] 

[1.x.1463] 

[1.x.1464] 

[1.x.1465] 

In other words, we can consider  [2.x.3721]  as the negative of the additional, positive force that the obstacle exerts on the membrane. The inequality in the second line of the statement above only appears to have the wrong sign because we have  [2.x.3722]  at points where  [2.x.3723] , given the definition of  [2.x.3724] . 

The existence and uniqueness of  [2.x.3725]  of this saddle point problem has been stated in Glowinski, Lions and Tr&eacute;moli&egrave;res: Numerical Analysis of Variational Inequalities, North-Holland, 1981. 




[1.x.1466] 

There are different methods to solve the variational inequality. As one possibility you can understand the saddle point problem as a convex quadratic program (QP) with inequality constraints. 

To get there, let us assume that we discretize both  [2.x.3726]  and  [2.x.3727]  with the same finite element space, for example the usual  [2.x.3728]  spaces. We would then get the equations 

[1.x.1467] 

where  [2.x.3729]  is the mass matrix on the chosen finite element space and the indices  [2.x.3730]  above are for all degrees of freedom in the set  [2.x.3731]  of degrees of freedom located in the interior of the domain (we have Dirichlet conditions on the perimeter). However, we can make our life simpler if we use a particular quadrature rule when assembling all terms that yield this mass matrix, namely a quadrature formula where quadrature points are only located at the interpolation points at which shape functions are defined; since all but one shape function are zero at these locations, we get a diagonal mass matrix with 

[1.x.1468] 

To define  [2.x.3732]  we use the same technique as for  [2.x.3733] . In other words, we define 

[1.x.1469] 

where  [2.x.3734]  is a suitable approximation of  [2.x.3735] . The integral in the definition of  [2.x.3736]  and  [2.x.3737]  are then approximated by the trapezoidal rule. With this, the equations above can be restated as 

[1.x.1470] 



Now we define for each degree of freedom  [2.x.3738]  the function 

[1.x.1471] 

with some  [2.x.3739] . (In this program we choose  [2.x.3740] . It is a kind of a penalty parameter which depends on the problem itself and needs to be chosen large enough; for example there is no convergence for  [2.x.3741]  using the current program if we use 7 global refinements.) 

After some head-scratching one can then convince oneself that the inequalities above can equivalently be rewritten as 

[1.x.1472] 

The primal-dual active set strategy we will use here is an iterative scheme which is based on this condition to predict the next active and inactive sets  [2.x.3742]  and  [2.x.3743]  (that is, those complementary sets of indices  [2.x.3744]  for which  [2.x.3745]  is either equal to or not equal to the value of the obstacle  [2.x.3746] ). For a more in depth treatment of this approach, see Hintermueller, Ito, Kunisch: The primal-dual active set strategy as a semismooth newton method, SIAM J. OPTIM., 2003, Vol. 13, No. 3, pp. 865-888. 

[1.x.1473] 

The algorithm for the primal-dual active set method works as follows (NOTE:  [2.x.3747] ): 

1. Initialize  [2.x.3748]  and  [2.x.3749] , such that   [2.x.3750]  and   [2.x.3751]  and set  [2.x.3752] . 2. Find the primal-dual pair  [2.x.3753]  that satisfies  [1.x.1474] 

 Note that the second and third conditions imply that exactly  [2.x.3754]  unknowns  are fixed, with the first condition yielding the remaining  [2.x.3755]  equations  necessary to determine both  [2.x.3756]  and  [2.x.3757] . 3. Define the new active and inactive sets by  [1.x.1475] 

4. If  [2.x.3758]  (and then, obviously, also   [2.x.3759] ) then stop, else set  [2.x.3760]  and go to step  (2). 

The method is called "primal-dual" because it uses both primal (the displacement  [2.x.3761] ) as well as dual variables (the Lagrange multiplier  [2.x.3762] ) to determine the next active set. 

At the end of this section, let us add two observations. First, for any primal-dual pair  [2.x.3763]  that satisfies these condition, we can distinguish the following cases: 

1.  [2.x.3764]  (i active):    [2.x.3765]    Then either  [2.x.3766]  and  [2.x.3767]  (penetration) or  [2.x.3768]  and  [2.x.3769]  (pressing load). 2.  [2.x.3770]  (i inactive):    [2.x.3771]    Then either  [2.x.3772]  and  [2.x.3773]  (no contact) or  [2.x.3774]  and  [2.x.3775]  (unpressing load). 

Second, the method above appears intuitively correct and useful but a bit ad hoc. However, it can be derived in a concisely in the following way. To this end, note that we'd like to solve the nonlinear system 

[1.x.1476] 

We can iteratively solve this by always linearizing around the previous iterate (i.e., applying a Newton method), but for this we need to linearize the function  [2.x.3776]  that is not differentiable. That said, it is slantly differentiable, and in fact we have 

[1.x.1477] 



[1.x.1478] 

This suggest a semismooth Newton step of the form 

[1.x.1479] 

where we have split matrices  [2.x.3777]  as well as vectors in the natural way into rows and columns whose indices belong to either the active set  [2.x.3778]  or the inactive set  [2.x.3779] . 

Rather than solving for updates  [2.x.3780] , we can also solve for the variables we are interested in right away by setting  [2.x.3781]  and  [2.x.3782]  and bringing all known terms to the right hand side. This yields 

[1.x.1480] 

These are the equations outlined above in the description of the basic algorithm. 

We could even drive this a bit further. It's easy to see that we can eliminate the third row and the third column because it implies  [2.x.3783] : 

[1.x.1481] 

This shows that one in fact only needs to solve for the Lagrange multipliers located on the active set. By considering the second row one would then recover the full Lagrange multiplier vector through 

[1.x.1482] 

Because of the third row and the fact that  [2.x.3784]  is a diagonal matrix we are able to calculate  [2.x.3785]  directly. We can therefore also write the linear system as follows: 

[1.x.1483] 

Fortunately, this form is easy to arrive at: we simply build the usual Laplace linear system 

[1.x.1484] 

and then let the AffineConstraints class eliminate all constrained degrees of freedom, namely  [2.x.3786] , in the same way as if the dofs in  [2.x.3787]  were Dirichlet data. The result linear system (the second to last one above) is symmetric and positive definite and we solve it with a CG-method and the AMG preconditioner from Trilinos. 




[1.x.1485] 

This tutorial is quite similar to step-4. The general structure of the program follows step-4 with minor differences: 

- We need two new methods,  [2.x.3788]  and    [2.x.3789] . 

- We need new member variables that denote the constraints we have here. 

- We change the preconditioner for the solver. 


You may want to read up on step-4 if you want to understand the current program. 


examples/step-41/doc/results.dox 



[1.x.1486] 

Running the program produces output like this: 

[1.x.1487] 



The iterations end once the active set doesn't change any more (it has 5,399 constrained degrees of freedom at that point). The algebraic precondition is apparently working nicely since we only need 4-6 CG iterations to solve the linear system (although this also has a lot to do with the fact that we are not asking for very high accuracy of the linear solver). 

More revealing is to look at a sequence of graphical output files (every third step is shown, with the number of the iteration in the leftmost column): 

 [2.x.3790]  

The pictures show that in the first step, the solution (which has been computed without any of the constraints active) bends through so much that pretty much every interior point has to be bounced back to the stairstep function, producing a discontinuous solution. Over the course of the active set iterations, this unphysical membrane shape is smoothed out, the contact with the lower-most stair step disappears, and the solution stabilizes. 

In addition to this, the program also outputs the values of the Lagrange multipliers. Remember that these are the contact forces and so should only be positive on the contact set, and zero outside. If, on the other hand, a Lagrange multiplier is negative in the active set, then this degree of freedom must be removed from the active set. The following pictures show the multipliers in iterations 1, 9 and 18, where we use red and browns to indicate positive values, and blue for negative values. 

 [2.x.3791]  

It is easy to see that the positive values converge nicely to moderate values in the interior of the contact set and large upward forces at the edges of the steps, as one would expect (to support the large curvature of the membrane there); at the fringes of the active set, multipliers are initially negative, causing the set to shrink until, in iteration 18, there are no more negative multipliers and the algorithm has converged. 




[1.x.1488] 

[1.x.1489] 

As with any of the programs of this tutorial, there are a number of obvious possibilities for extensions and experiments. The first one is clear: introduce adaptivity. Contact problems are prime candidates for adaptive meshes because the solution has lines along which it is less regular (the places where contact is established between membrane and obstacle) and other areas where the solution is very smooth (or, in the present context, constant wherever it is in contact with the obstacle). Adding this to the current program should not pose too many difficulties, but it is not trivial to find a good error estimator for that purpose. 

A more challenging task would be an extension to 3d. The problem here is not so much to simply make everything run in 3d. Rather, it is that when a 3d body is deformed and gets into contact with an obstacle, then the obstacle does not act as a constraining body force within the domain as is the case here. Rather, the contact force only acts on the boundary of the object. The inequality then is not in the differential equation but in fact in the (Neumann-type) boundary conditions, though this leads to a similar kind of variational inequality. Mathematically, this means that the Lagrange multiplier only lives on the surface, though it can of course be extended by zero into the domain if that is convenient. As in the current program, one does not need to form and store this Lagrange multiplier explicitly. 

A further interesting problem for the 3d case is to consider contact problems with friction. In almost every mechanical process friction has a big influence. For the modelling we have to take into account tangential stresses at the contact surface. Also we have to observe that friction adds another nonlinearity to our problem. 

Another nontrivial modification is to implement a more complex constitutive law like nonlinear elasticity or elasto-plastic  material behavior. The difficulty here is to handle the additional nonlinearity arising through the nonlinear constitutive law. 


examples/step-42/doc/intro.dox 

 [2.x.3792]  

[1.x.1490][1.x.1491] 




[1.x.1492] 

[1.x.1493] 

This example is an extension of step-41, considering a 3d contact problem with an elasto-plastic material behavior with isotropic hardening in three dimensions. In other words, it considers how a three-dimensional body deforms if one pushes into it a rigid obstacle (the contact problem) where deformation is governed by an elasto-plastic material law (a material that can only accommodate a certain maximal stress) that hardens as deformation accumulates. To show what we intend to do before going into too many details, let us just show a picture of what the solution will look like (the deformable body is a cube - only half of which is actually shown -, the obstacle corresponds to a Chinese character that is discussed below): 

 [2.x.3793]  


This problem description implies that we have to take care of an additional nonlinearity compared to step-41: the material behavior. Since we consider a three dimensional problem here, we also have to account for the fact that the contact area is at the boundary of the deformable body now, rather than in the interior. Finally, compared to step-41, we also have to deal with hanging nodes in both the handling of the linear system as well as of the inequality constraints as we would like to use an adaptive mesh; in the latter case, we will have to deal with prioritizing whether the constraints from the hanging nodes or from the inequalities are more important. 

Since you can very easily reach a few million degrees of freedom in three dimensions, even with adaptive mesh refinement, we decided to use Trilinos and p4est to run our code in parallel, building on the framework of step-40 for the parallelization. Additional pointers for parallelization can be found in step-32. 




[1.x.1494] 

The classical formulation of the problem possesses the following form: 

[1.x.1495] 

Here, the first of these equations defines the relationship between strain  [2.x.3794]  and stress  [2.x.3795]  via the fourth-order compliance tensor  [2.x.3796] ;  [2.x.3797]  provides the plastic component of the strain to ensure that the stress does not exceed the yield stress. We will only consider isotropic materials for which  [2.x.3798]  can be expressed in terms of the Lam&eacute; moduli  [2.x.3799]  and  [2.x.3800]  or alternatively in terms of the bulk modulus  [2.x.3801]  and  [2.x.3802] . The second equation is the force balance; we will here not consider any body forces and henceforth assume that  [2.x.3803] . The complementarity condition in the third line implies that  [2.x.3804]  if  [2.x.3805]  but that  [2.x.3806]  may be a nonzero tensor if and only if  [2.x.3807] , and in particular that in this case  [2.x.3808]  must point in the direction  [2.x.3809] . The inequality  [2.x.3810]  is a statement of the fact that plastic materials can only support a finite amount of stress; in other words, they react with plastic deformations  [2.x.3811]  if external forces would result in a stress  [2.x.3812]  for which  [2.x.3813]  would result. A typical form for this [1.x.1496] is  [2.x.3814]  where  [2.x.3815]  is the deviatoric part of a tensor and  [2.x.3816]  denotes the Frobenius norm. 

Further equations describe a fixed, zero displacement on  [2.x.3817]  and that on the surface  [2.x.3818]  where contact may appear, the normal force  [2.x.3819]  exerted by the obstacle is inward (no "pull" by the obstacle on our body) and with zero tangential component  [2.x.3820] . The last condition is again a complementarity condition that implies that on  [2.x.3821] , the normal force can only be nonzero if the body is in contact with the obstacle; the second part describes the impenetrability of the obstacle and the body. The last two equations are commonly referred to as the Signorini contact conditions. 

Most materials - especially metals - have the property that they show some hardening as a result of deformation. In other words,  [2.x.3822]  increases with deformation. In practice, it is not the elastic deformation that results in hardening, but the plastic component. There are different constitutive laws to describe those material behaviors. The simplest one is called linear isotropic hardening described by the flow function  [2.x.3823] . 




[1.x.1497] 

It is generally rather awkward to deal with inequalities. Here, we have to deal with two: plasticity and the contact problem. As described in more detail in the paper mentioned at the top of this page, one can at least reformulate the plasticity in a way that makes it look like a nonlinearity that we can then treat with Newton's method. This is slightly tricky mathematically since the nonlinearity is not just some smooth function but instead has kinks where the stress reaches the yield stress; however, it can be shown for such [1.x.1498] functions that Newton's method still converges. 

Without going into details, we will also get rid of the stress as an independent variable and instead work exclusively with the displacements  [2.x.3824] . Ultimately, the goal of this reformulation is that we will want to end up with a symmetric, positive definite problem - such as a linearized elasticity problem with spatially variable coefficients resulting from the plastic behavior - that needs to be solved in each Newton step. We want this because there are efficient and scalable methods for the solution of such linear systems, such as CG preconditioned with an algebraic multigrid. This is opposed to the saddle point problem akin to the mixed Laplace (see step-20) we would get were we to continue with the mixed formulation containing both displacements and stresses, and for which step-20 already gives a hint at how difficult it is to construct good solvers and preconditioners. 

With this said, let us simply state the problem we obtain after reformulation (again, details can be found in the paper): Find a displacement  [2.x.3825]  so that 

[1.x.1499] 

where the projector  [2.x.3826]  is defined as 

[1.x.1500] 

and the space  [2.x.3827]  is the space of all displacements that satisfy the contact condition: 

[1.x.1501] 



In the actual code, we will use the abbreviation  [2.x.3828] . 

Given this formulation, we will apply two techniques: 

- Run a Newton method to iterate out the nonlinearity in the projector. 

- Run an active set method for the contact condition, in much the same   way as we did in step-41. 

A strict approach would keep the active set fixed while we iterate the Newton method to convergence (or maybe the other way around: find the final active set before moving on to the next Newton iteration). In practice, it turns out that it is sufficient to do only a single Newton step per active set iteration, and so we will iterate over them concurrently. We will also, every once in a while, refine the mesh. 




[1.x.1502] 

As mentioned, we will treat the nonlinearity of the operator  [2.x.3829]  by applying a Newton method, despite the fact that the operator is not differentiable in the strict sense. However, it satisfies the conditions of [1.x.1503] differentiability and this turns out to be enough for Newton's method to work. The resulting method then goes by the name [1.x.1504], which sounds impressive but is, in reality, just a Newton method applied to a semi-smooth function with an appropriately chosen "derivative". 

In the current case, we will run our iteration by solving in each iteration  [2.x.3830]  the following equation (still an inequality, but linearized): 

[1.x.1505] 

where the rank-4 tensor  [2.x.3831]  given by 

[1.x.1506] 

This tensor is the (formal) linearization of  [2.x.3832]  around  [2.x.3833] . For the linear isotropic material we consider here, the bulk and shear components of the projector are given by 

[1.x.1507] 

where  [2.x.3834]  and  [2.x.3835]  are the identity tensors of rank 2 and 4, respectively. 

Note that this problem corresponds to a linear elastic contact problem where  [2.x.3836]  plays the role of the elasticity tensor  [2.x.3837] . Indeed, if the material is not plastic at a point, then  [2.x.3838] . However, at places where the material is plastic,  [2.x.3839]  is a spatially varying function. In any case, the system we have to solve for the Newton iterate  [2.x.3840]  gets us closer to the goal of rewriting our problem in a way that allows us to use well-known solvers and preconditioners for elliptic systems. 

As a final note about the Newton method let us mention that as is common with Newton methods we need to globalize it by controlling the step length. In other words, while the system above solves for  [2.x.3841] , the final iterate will rather be 

[1.x.1508] 

where the difference in parentheses on the right takes the role of the traditional Newton direction,  [2.x.3842] . We will determine  [2.x.3843]  using a standard line search. 




[1.x.1509] 

This linearized problem to be solved in each Newton step is essentially like in step-41. The only difference consists in the fact that the contact area is at the boundary instead of in the domain. But this has no further consequence so that we refer to the documentation of step-41 with the only hint that  [2.x.3844]  contains all the vertices at the contact boundary  [2.x.3845]  this time. As there, what we need to do is keep a subset of degrees of freedom fixed, leading to additional constraints that one can write as a saddle point problem. However, as discussed in the paper, by writing these constraints in an appropriate way that removes the coupling between degrees of freedom, we end up with a set of nodes that essentially just have Dirichlet values attached to them. 




[1.x.1510] 

The algorithm outlined above combines the damped semismooth Newton-method, which we use for the nonlinear constitutive law, with the semismooth Newton method for the contact. It works as follows: <ol>   [2.x.3846]  Initialize the active and inactive sets  [2.x.3847]  and  [2.x.3848]   such that  [2.x.3849]  and  [2.x.3850]  and set  [2.x.3851] . Here,  [2.x.3852]  is the set of  all degrees of freedom located at the surface of the domain where contact  may happen.  The start value  [2.x.3853]  fulfills our obstacle condition, i.e., we project an  initial zero displacement onto the set of feasible displacements. 

  [2.x.3854]  Assemble the Newton matrix  [2.x.3855]  and the right-hand-side  [2.x.3856] .  These correspond to the linearized Newton step, ignoring for the moment  the contact inequality. 

  [2.x.3857]  Find the primal-dual pair  [2.x.3858]  that satisfies  [1.x.1511] 

 As in step-41, we can obtain the solution to this problem by eliminating  those degrees of freedom in  [2.x.3859]  from the first equation and  obtain a linear system  [2.x.3860] . 




  [2.x.3861]  Damp the Newton iteration for  [2.x.3862]  by applying a line search and  calculating a linear combination of  [2.x.3863]  and  [2.x.3864] . This  requires finding an   [2.x.3865]  so that  [1.x.1512] 

 satisfies  [1.x.1513]  with  [2.x.3866]  with  the exceptions of (i) elements  [2.x.3867]  where we set  [2.x.3868] ,  and (ii) elements that correspond to hanging nodes, which we eliminate in the usual manner. 

  [2.x.3869]  Define the new active and inactive sets by  [1.x.1514] 

 [1.x.1515] 



  [2.x.3870] Project  [2.x.3871]  so that it satisfies the contact inequality,  [1.x.1516] 

 Here,   [2.x.3872]  is the projection of the active  components in  [2.x.3873]  to the gap  [1.x.1517] 

 where  [2.x.3874]  is the [1.x.1518] denoting the distance of the obstacle  from the undisplaced configuration of the body. 

  [2.x.3875]  If  [2.x.3876]  and  [2.x.3877]  then stop, else set  [2.x.3878]  and go to  step (1). This step ensures that we only stop iterations if both the correct  active set has been found and the plasticity has been iterated to sufficient  accuracy.  [2.x.3879]  

In step 3 of this algorithm, the matrix  [2.x.3880] ,  [2.x.3881]  describes the coupling of the bases for the displacements and Lagrange multiplier (contact forces) and it is not quadratic in our situation since  [2.x.3882]  is only defined on  [2.x.3883] , i.e., the surface where contact may happen. As shown in the paper, we can choose  [2.x.3884]  to be a matrix that has only one entry per row, (see also H&uuml;eber, Wohlmuth: A primal-dual active set strategy for non-linear multibody contact problems, Comput. Methods Appl. Mech. Engrg. 194, 2005, pp. 3147-3166). The vector  [2.x.3885]  is defined by a suitable approximation  [2.x.3886]  of the gap  [2.x.3887]  

[1.x.1519] 






[1.x.1520] 

Since we run our program in 3d, the computations the program performs are expensive. Consequently using adaptive mesh refinement is an important step towards staying within acceptable run-times. To make our lives easier we simply choose the KellyErrorEstimator that is already implemented in deal.II. We hand the solution vector to it which contains the displacement  [2.x.3888] . As we will see in the results it yields a quite reasonable adaptive mesh for the contact zone as well as for plasticity. 




[1.x.1521] 

This tutorial is essentially a mixture of step-40 and step-41 but instead of PETSc we let the Trilinos library deal with parallelizing the linear algebra (like in step-32). Since we are trying to solve a similar problem like in step-41 we will use the same methods but now in parallel. 

A difficulty is handling of the constraints from the Dirichlet conditions, hanging nodes and the inequality condition that arises from the contact. For this purpose we create three objects of type AffineConstraints that describe the various constraints and that we will combine as appropriate in each iteration. 

Compared to step-41, the programs has a few new classes: 

 [2.x.3889]   [2.x.3890]   [2.x.3891]  describes the plastic behavior of the   material 

 [2.x.3892]   [2.x.3893]  describes a sphere that serves as the   obstacle that is pushed into the deformable, elastoplastic body.   Whether this or the next class is used to describe the obstacle is   determined from the input parameter file. 

 [2.x.3894]   [2.x.3895]  (and a helper class) is a class that   allows us to read in an obstacle from a file. In the example we   will show in the results section, this file will be    [2.x.3896]  and will correspond to data that shows the   Chinese, Japanese or   Korean symbol for force or power (see http://www.orientaloutpost.com/ :   "This word can be used for motivation - it   can also mean power/motion/propulsion/force. It can be anything   internal or external that keeps you going. This is the safest way to express   motivation in Chinese. If your audience is Japanese, please see the other entry   for motivation. This is a word in Japanese and Korean, but it means "motive   power" or "kinetic energy" (without the motivation meaning that you are   probably looking for)"). In essence, we will pretend that we have a stamp   (i.e., a mask that corresponds to a flat bottomed obstacle with no pieces   of intermediate height) that we press into the body. The symbol in question   looks as follows (see also the picture at   the top of this section on how the end result looks like): 

   [2.x.3897]   [2.x.3898]  

Other than that, let us comment only on the following aspects:  [2.x.3899]   [2.x.3900]  The program allows you to select from two different coarse meshes   through the parameter file. These are either a cube  [2.x.3901]  or   a half sphere with the open side facing the positive  [2.x.3902]  direction. 

 [2.x.3903] In either case, we will assume the convention that the part of the   boundary that may be in contact with the obstacle has boundary   indicator one. For both kinds of meshes, we assume that this is a free   surface, i.e., the body is either in contact there or there is no force   acting on it. For the half sphere, the curved part has boundary   indicator zero and we impose zero displacement there. For the box,   we impose zero displacement along the bottom but allow vertical   displacement along the sides (though no horizontal displacement).  [2.x.3904]  


examples/step-42/doc/results.dox 



[1.x.1522] 

The directory that contains this program also contains a number of input parameter files that can be used to create various different simulations. For example, running the program with the  [2.x.3905]  parameter file (using a ball as obstacle and the box as domain) on 16 cores produces output like this: 

[1.x.1523] 



The tables at the end of each cycle show information about computing time (these numbers are of course specific to the machine on which this output was produced) and the number of calls of different parts of the program like assembly or calculating the residual, for the most recent mesh refinement cycle. Some of the numbers above can be improved by transferring the solution from one mesh to the next, an option we have not exercised here. Of course, you can also make the program run faster, especially on the later refinement cycles, by just using more processors: the accompanying paper shows good scaling to at least 1000 cores. 

In a typical run, you can observe that for every refinement step, the active set - the contact points - are iterated out at first. After that the Newton method has only to resolve the plasticity. For the finer meshes, quadratic convergence can be observed for the last 4 or 5 Newton iterations. 

We will not discuss here in all detail what happens with each of the input files. Rather, let us just show pictures of the solution (the left half of the domain is omitted if cells have zero quadrature points at which the plastic inequality is active): 

 [2.x.3906]  

The picture shows the adaptive refinement and as well how much a cell is plastified during the contact with the ball. Remember that we consider the norm of the deviator part of the stress in each quadrature point to see if there is elastic or plastic behavior. The blue color means that this cell contains only elastic quadrature points in contrast to the red cells in which all quadrature points are plastified. In the middle of the top surface - where the mesh is finest - a very close look shows the dimple caused by the obstacle. This is the result of the  [2.x.3907]  function. However, because the indentation of the obstacles we consider here is so small, it is hard to discern this effect; one could play with displacing vertices of the mesh by a multiple of the computed displacement. 

Further discussion of results that can be obtained using this program is provided in the publication mentioned at the very top of this page. 


[1.x.1524] 

[1.x.1525] 

There are, as always, multiple possibilities for extending this program. From an algorithmic perspective, this program goes about as far as one can at the time of writing, using the best available algorithms for the contact inequality, the plastic nonlinearity, and the linear solvers. However, there are things one would like to do with this program as far as more realistic situations are concerned:  [2.x.3908]   [2.x.3909]  Extend the program from a static to a quasi-static situation, perhaps by choosing a backward-Euler-scheme for the time discretization. Some theoretical results can be found in the PhD thesis by Jrg Frohne, [1.x.1526], University of Siegen, Germany, 2011. 

 [2.x.3910]  It would also be an interesting advance to consider a contact problem with friction. In almost every mechanical process friction has a big influence.  To model this situation, we have to take into account tangential stresses at the contact surface. Friction also adds another inequality to our problem since body and obstacle will typically stick together as long as the tangential stress does not exceed a certain limit, beyond which the two bodies slide past each other. 

 [2.x.3911]  If we already simulate a frictional contact, the next step to consider is heat generation over the contact zone. The heat that is caused by friction between two bodies raises the temperature in the deformable body and entails an change of some material parameters. 

 [2.x.3912]  It might be of interest to implement more accurate, problem-adapted error estimators for contact as well as for the plasticity.  [2.x.3913]  


examples/step-43/doc/intro.dox 

 [2.x.3914]  

[1.x.1527] 


[1.x.1528] [1.x.1529] 

The simulation of multiphase flow in porous media is a ubiquitous problem, and we have previously addressed it already in some form in step-20 and step-21. However, as was easy to see there, it faces two major difficulties: numerical accuracy and efficiency. The first is easy to see in the stationary solver step-20: using lowest order Raviart-Thomas elements can not be expected to yield highly accurate solutions. We need more accurate methods. The second reason is apparent from the time dependent step-21: that program is excruciatingly slow, and there is no hope to get highly accurate solutions in 3d within reasonable time frames. 

In this program, in order to overcome these two problems, there are five areas which we are trying to improve for a high performance simulator: 

 [2.x.3915]   [2.x.3916]  Higher order spatial discretizations  [2.x.3917]  Adaptive mesh refinement  [2.x.3918]  Adaptive time stepping  [2.x.3919]  Operator splitting  [2.x.3920]  Efficient solver and preconditioning  [2.x.3921]  

Much inspiration for this program comes from step-31 but several of the techniques discussed here are original. 




[1.x.1530] 

We consider the flow of a two-phase immiscible, incompressible fluid. Capillary and gravity effects are neglected, and viscous effects are assumed dominant. The governing equations for such a flow that are identical to those used in step-21 and are 

[1.x.1531] 

where  [2.x.3922]  is the saturation (volume fraction between zero and one) of the second (wetting) phase,  [2.x.3923]  is the pressure,  [2.x.3924]  is the permeability tensor,  [2.x.3925]  is the total mobility,  [2.x.3926]  is the porosity,  [2.x.3927]  is the fractional flow of the wetting phase,  [2.x.3928]  is the source term and  [2.x.3929]  is the total velocity. The total mobility, fractional flow of the wetting phase and total velocity are respectively given by 

[1.x.1532] 

where subscripts  [2.x.3930]  represent the wetting and non-wetting phases, respectively. 

For convenience, the porosity  [2.x.3931]  in the saturation equation, which can be considered a scaling factor for the time variable, is set to one. Following a commonly used prescription for the dependence of the relative permeabilities  [2.x.3932]  and  [2.x.3933]  on saturation, we use 

[1.x.1533] 



The porous media equations above are augmented by initial conditions for the saturation and boundary conditions for the pressure. Since saturation and the gradient of the pressure uniquely determine the velocity, no boundary conditions are necessary for the velocity. Since the flow equations do not contain time derivatives, initial conditions for the velocity and pressure variables are not required. The flow field separates the boundary into inflow or outflow parts. Specifically, 

[1.x.1534] 

and we arrive at a complete model by also imposing boundary values for the saturation variable on the inflow boundary  [2.x.3934] . 




[1.x.1535] 

As seen in step-21, solving the flow equations for velocity and pressure are the parts of the program that take far longer than the (explicit) updating step for the saturation variable once we know the flow variables. On the other hand,  the pressure and velocity depend only weakly on saturation, so one may think about only solving for pressure and velocity every few time steps while updating the saturation in every step. If we can find a criterion for when the flow variables need to be updated, we call this splitting an "adaptive operator splitting" scheme. 

Here, we use the following a posteriori criterion to decide when to re-compute pressure and velocity variables (detailed derivations and descriptions can be found in [Chueh, Djilali and Bangerth 2011]): 

[1.x.1536] 

where superscripts in parentheses denote the number of the saturation time step at which any quantity is defined and  [2.x.3935]  represents the last step where we actually computed the pressure and velocity. If  [2.x.3936]  exceeds a certain threshold we re-compute the flow variables; otherwise, we skip this computation in time step  [2.x.3937]  and only move the saturation variable one time step forward. 

In short, the algorithm allows us to perform a number of saturation time steps of length  [2.x.3938]  until the criterion above tells us to re-compute velocity and pressure variables, leading to a macro time step of length 

[1.x.1537] 

We choose the length of (micro) steps subject to the Courant-Friedrichs-Lewy (CFL) restriction according to the criterion 

[1.x.1538] 

which we have confirmed to be stable for the choice of finite element and time stepping scheme for the saturation equation discussed below ( [2.x.3939]  denotes the diameter of cell  [2.x.3940] ). The result is a scheme where neither micro nor macro time steps are of uniform length, and both are chosen adaptively. 

[1.x.1539] Using this time discretization, we obtain the following set of equations for each time step from the IMPES approach (see step-21): 

[1.x.1540] 




Using the fact that  [2.x.3941] , the time discrete saturation equation becomes 

[1.x.1541] 



[1.x.1542] 

By multiplying the equations defining the total velocity  [2.x.3942]  and the equation that expresses its divergence in terms of source terms, with test functions  [2.x.3943]  and  [2.x.3944]  respectively and then integrating terms by parts as necessary, the weak form of the problem reads: Find  [2.x.3945]  so that for all test functions  [2.x.3946]  there holds 

[1.x.1543] 

Here,  [2.x.3947]  represents the unit outward normal vector to  [2.x.3948]  and the pressure  [2.x.3949]  can be prescribed weakly on the open part of the boundary  [2.x.3950]  whereas on those parts where a velocity is prescribed (for example impermeable boundaries with  [2.x.3951]  the term disappears altogether because  [2.x.3952] . 

We use continuous finite elements to discretize the velocity and pressure equations. Specifically, we use mixed finite elements to ensure high order approximation for both vector (e.g. a fluid velocity) and scalar variables (e.g. pressure) simultaneously. For saddle point problems, it is well established that the so-called Babuska-Brezzi or Ladyzhenskaya-Babuska-Brezzi (LBB) conditions [Brezzi 1991, Chen 2005] need to be satisfied to ensure stability of the pressure-velocity system. These stability conditions are satisfied in the present work by using elements for velocity that are one order higher than for the pressure, i.e.  [2.x.3953]  and  [2.x.3954] , where  [2.x.3955] ,  [2.x.3956]  is the space dimension, and  [2.x.3957]  denotes the space of tensor product Lagrange polynomials of degree  [2.x.3958]  in each variable. 

[1.x.1544] The chosen  [2.x.3959]  elements for the saturation equation do not lead to a stable discretization without upwinding or other kinds of stabilization, and spurious oscillations will appear in the numerical solution. Adding an artificial diffusion term is one approach to eliminating these oscillations [Chen 2005]. On the other hand, adding too much diffusion smears sharp fronts in the solution and suffers from grid-orientation difficulties [Chen 2005]. To avoid these effects, we use the artificial diffusion term proposed by [Guermond and Pasquetti 2008] and validated in [Chueh, Djilali, Bangerth 2011] and [Kronbichler, Heister and Bangerth, 2011], as well as in step-31. 

This method modifies the (discrete) weak form of the saturation equation to read 

[1.x.1545] 

where  [2.x.3960]  is the artificial diffusion parameter and  [2.x.3961]  is an appropriately chosen numerical flux on the boundary of the domain (we choose the obvious full upwind flux for this). 

Following [Guermond and Pasquetti 2008] (and as detailed in [Chueh, Djilali and Bangerth 2011]), we use the parameter as a piecewise constant function set on each cell  [2.x.3962]  with the diameter  [2.x.3963]  as 

[1.x.1546] 

where  [2.x.3964]  is a stabilization exponent and  [2.x.3965]  is a dimensionless user-defined stabilization constant. Following [Guermond and Pasquetti 2008] as well as the implementation in step-31, the velocity and saturation global normalization constant,  [2.x.3966] , and the residual  [2.x.3967]  are respectively given by 

[1.x.1547] 

and 

[1.x.1548] 

where  [2.x.3968]  is a second dimensionless user-defined constant,  [2.x.3969]  is the diameter of the domain and  [2.x.3970]  is the range of the present saturation values in the entire computational domain  [2.x.3971] . 

This stabilization scheme has a number of advantages over simpler schemes such as finite volume (or discontinuous Galerkin) methods or streamline upwind Petrov Galerkin (SUPG) discretizations. In particular, the artificial diffusion term acts primarily in the vicinity of discontinuities since the residual is small in areas where the saturation is smooth. It therefore provides for a higher degree of accuracy. On the other hand, it is nonlinear since  [2.x.3972]  depends on the saturation  [2.x.3973] . We avoid this difficulty by treating all nonlinear terms explicitly, which leads to the following fully discrete problem at time step  [2.x.3974] : 

[1.x.1549] 

where  [2.x.3975]  is the velocity linearly extrapolated from  [2.x.3976]  and  [2.x.3977]  to the current time  [2.x.3978]  if  [2.x.3979]  while  [2.x.3980]  is  [2.x.3981]  if  [2.x.3982] . Consequently, the equation is linear in  [2.x.3983]  and all that is required is to solve with a mass matrix on the saturation space. 

Since the Dirichlet boundary conditions for saturation are only imposed on the inflow boundaries, the third term on the left hand side of the equation above needs to be split further into two parts: 

[1.x.1550] 

where  [2.x.3984]  and  [2.x.3985]  represent inflow and outflow boundaries, respectively. We choose values using an upwind formulation, i.e.  [2.x.3986]  and  [2.x.3987]  correspond to the values taken from the present cell, while the values of  [2.x.3988]  and  [2.x.3989]  are those taken from the neighboring boundary  [2.x.3990] . 




[1.x.1551] 

Choosing meshes adaptively to resolve sharp saturation fronts is an essential ingredient to achieve efficiency in our algorithm. Here, we use the same shock-type refinement approach used in [Chueh, Djilali and Bangerth 2011] to select those cells that should be refined or coarsened. The refinement indicator for each cell  [2.x.3991]  of the triangulation is computed by 

[1.x.1552] 

where  [2.x.3992]  is the gradient of the discrete saturation variable evaluated at the center  [2.x.3993]  of cell  [2.x.3994] . This approach is analogous to ones frequently used in compressible flow problems, where density gradients are used to indicate refinement. That said, as we will discuss at the end of the [1.x.1553], this turns out to not be a very useful criterion since it leads to refinement basically everywhere. We only show it here for illustrative purposes. 




[1.x.1554] 

Following the discretization of the governing equations discussed above, we obtain a linear system of equations in time step  [2.x.3995]  of the following form: 

[1.x.1555] 

where the individual matrices and vectors are defined as follows using shape functions  [2.x.3996]  for velocity, and  [2.x.3997]  for both pressure and saturation: 

[1.x.1556] 

and  [2.x.3998]  as given in the definition of the stabilized transport equation. 

The linear system above is of block triangular form if we consider the top left  [2.x.3999]  panel of matrices as one block. We can therefore first solve for the velocity and pressure (unless we decide to use  [2.x.4000]  in place of the velocity) followed by a solve for the saturation variable. The first of these steps requires us to solve 

[1.x.1557] 

We apply the Generalized Minimal Residual (GMRES) method [Saad and Schultz 1986] to this linear system. The ideal preconditioner for the velocity-pressure system is 

[1.x.1558] 

where  [2.x.4001]  is the Schur complement [Zhang 2005] of the system. This preconditioner is optimal since 

[1.x.1559] 

for which it can be shown that GMRES converges in two iterations. 

However, we cannot of course expect to use exact inverses of the velocity mass matrix and the Schur complement. We therefore follow the approach by [Silvester and Wathen 1994] originally proposed for the Stokes system. Adapting it to the current set of equations yield the preconditioner 

[1.x.1560] 

where a tilde indicates an approximation of the exact inverse matrix. In particular, since  [2.x.4002]  is a sparse symmetric and positive definite matrix, we choose for  [2.x.4003]  a single application of a sparse incomplete Cholesky decomposition of this matrix [Golub and Van Loan 1996]. We note that the Schur complement that corresponds to the porous media flow operator in non-mixed form,  [2.x.4004]  and  [2.x.4005]  should be a good approximation of the actual Schur complement matrix  [2.x.4006] . Since both of these matrices are again symmetric and positive definite, we use an incomplete Cholesky decomposition of  [2.x.4007]  for  [2.x.4008] . It is important to note that  [2.x.4009]  needs to be built with Dirichlet boundary conditions to ensure its invertibility. 

Once the velocity  [2.x.4010]   is available, we can assemble  [2.x.4011]  and  [2.x.4012]  and solve for the saturations using 

[1.x.1561] 

where the mass matrix  [2.x.4013]  is solved by the conjugate gradient method, using an incomplete Cholesky decomposition as preconditioner once more. 

[1.x.1562] 

 [2.x.4014]  The implementation discussed here uses and extends parts of the step-21, step-31 and step-33 tutorial programs of this library. In particular, if you want to understand how it works, please consult step-21 for a discussion of the mathematical problem, and step-31 from which most of the implementation is derived. We will not discuss aspects of the implementation that have already been discussed in step-31. 

We show numerical results for some two-phase flow equations augmented by appropriate initial and boundary conditions in conjunction with two different choices of the permeability model. In the problems considered, there is no internal source term ( [2.x.4015] ). As mentioned above, quantitative numerical results are presented in [Chueh, Djilali and Bangerth 2011]. 

For simplicity, we choose  [2.x.4016] , though all methods (as well as our implementation) should work equally well on general unstructured meshes. 

Initial conditions are only required for the saturation variable, and we choose  [2.x.4017] , i.e. the porous medium is initially filled by a mixture of the non-wetting (80%) and wetting (20%) phases. This differs from the initial condition in step-21 where we had taken  [2.x.4018] , but for complicated mathematical reasons that are mentioned there in a longish remark, the current method using an entropy-based artificial diffusion term does not converge to the viscosity solution with this initial condition without additional modifications to the method. We therefore choose this modified version for the current program. 

Furthermore, we prescribe a linear pressure on the boundaries: 

[1.x.1563] 

Pressure and saturation uniquely determine a velocity, and the velocity determines whether a boundary segment is an inflow or outflow boundary. On the inflow part of the boundary,  [2.x.4019] , we impose 

[1.x.1564] 

In other words, the domain is flooded by the wetting phase from the left. No boundary conditions for the saturation are required for the outflow parts of the boundary. 

All the numerical and physical parameters used for the 2D/3D cases are listed in the following table: 

 [2.x.4020]  




[1.x.1565] 


<ol>  [2.x.4021]  CC Chueh, N Djilali and W Bangerth.  [2.x.4022]  An h-adaptive operator splitting method for two-phase flow in 3D   heterogeneous porous media.  [2.x.4023]  SIAM Journal on Scientific Computing, vol. 35 (2013), pp. B149-B175 

 [2.x.4024]  M. Kronbichler, T. Heister, and W. Bangerth  [2.x.4025]  High Accuracy Mantle Convection Simulation through Modern Numerical Methods.  [2.x.4026]  Geophysics Journal International, vol. 191 (2012), pp. 12-29 

 [2.x.4027]  F Brezzi and M Fortin.  [2.x.4028]  [1.x.1566].  [2.x.4029]  Springer-Verlag, 1991. 

 [2.x.4030]  Z Chen.  [2.x.4031]  [1.x.1567].  [2.x.4032]  Springer, 2005. 

 [2.x.4033]  JL Guermond and R Pasquetti.  [2.x.4034]  Entropy-based nonlinear viscosity for Fourier approximations of   conservation laws.  [2.x.4035]  [1.x.1568], 346(13-14):801-806, 2008. 

 [2.x.4036]  CC Chueh, M Secanell, W Bangerth, and N Djilali.  [2.x.4037]  Multi-level adaptive simulation of transient two-phase flow in   heterogeneous porous media.  [2.x.4038]  [1.x.1569], 39:1585-1596, 2010. 

 [2.x.4039]  Y Saad and MH Schultz.  [2.x.4040]  Gmres: A generalized minimal residual algorithm for solving   nonsymmetric linear systems.  [2.x.4041]  [1.x.1570],   7(3):856-869, 1986. 

 [2.x.4042]  F Zhang.  [2.x.4043]  [1.x.1571].  [2.x.4044]  Springer, 2005. 

 [2.x.4045]  D Silvester and A Wathen.  [2.x.4046]  Fast iterative solution of stabilised Stokes systems part ii: Using   general block preconditioners.  [2.x.4047]  [1.x.1572], 31(5):1352-1367, 1994. 

 [2.x.4048]  GH Golub and CF van Loan.  [2.x.4049]  [1.x.1573].  [2.x.4050]  3rd Edition, Johns Hopkins, 1996. 

 [2.x.4051]  SE Buckley and MC Leverett.  [2.x.4052]  Mechanism of fluid displacements in sands.  [2.x.4053]  [1.x.1574], 146:107-116, 1942. 

 [2.x.4054]  


examples/step-43/doc/results.dox 



[1.x.1575] 


The output of this program is not really much different from that of step-21: it solves the same problem, after all. Of more importance are quantitative metrics such as the accuracy of the solution as well as the time needed to compute it. These are documented in detail in the two publications listed at the top of this page and we won't repeat them here. 

That said, no tutorial program is complete without a couple of good pictures, so here is some output of a run in 3d: 

 [2.x.4055]  


[1.x.1576] 

[1.x.1577] 

The primary objection one may have to this program is that it is still too slow: 3d computations on reasonably fine meshes are simply too expensive to be done routinely and with reasonably quick turn-around. This is similar to the situation we were in when we wrote step-31, from which this program has taken much inspiration. The solution is similar as it was there as well: We need to parallelize the program in a way similar to how we derived step-32 out of step-31. In fact, all of the techniques used in step-32 would be transferable to this program as well, making the program run on dozens or hundreds of processors immediately. 

A different direction is to make the program more relevant to many other porous media applications. Specifically, one avenue is to go to the primary user of porous media flow simulators, namely the oil industry. There, applications in this area are dominated by multiphase flow (i.e., more than the two phases we have here), and the reactions they may have with each other (or any other way phases may exchange mass, such as through dissolution in and bubbling out of gas from the oil phase). Furthermore, the presence of gas often leads to compressibility effects of the fluid. Jointly, these effects are typically formulated in the widely-used "black oil model". True reactions between multiple phases also play a role in oil reservoir modeling when considering controlled burns of oil in the reservoir to raise pressure and temperature. These are much more complex problems, though, and left for future projects. 

Finally, from a mathematical perspective, we have derived the criterion for re-computing the velocity/pressure solution at a given time step under the assumption that we want to compare the solution we would get at the current time step with that computed the last time we actually solved this system. However, in the program, whenever we did not re-compute the solution, we didn't just use the previously computed solution but instead extrapolated from the previous two times we solved the system. Consequently, the criterion was pessimistically stated: what we should really compare is the solution we would get at the current time step with the extrapolated one. Re-stating the theorem in this regard is left as an exercise. 

There are also other ways to extend the mathematical foundation of this program; for example, one may say that it isn't the velocity we care about, but in fact the saturation. Thus, one may ask whether the criterion we use here to decide whether  [2.x.4056]  needs to be recomputed is appropriate; one may, for example, suggest that it is also important to decide whether (and by how much) a wrong velocity field in fact affects the solution of the saturation equation. This would then naturally lead to a sensitivity analysis. 

From an algorithmic viewpoint, we have here used a criterion for refinement that is often used in engineering, namely by looking at the gradient of the solution. However, if you inspect the solution, you will find that it quickly leads to refinement almost everywhere, even in regions where it is clearly not necessary: frequently used therefore does not need to imply that it is a useful criterion to begin with. On the other hand, replacing this criterion by a different and better one should not be very difficult. For example, the KellyErrorEstimator class used in many other programs should certainly be applicable to the current problem as well. 


examples/step-44/doc/intro.dox 

 [2.x.4057]  

[1.x.1578] 

 [2.x.4058]  

[1.x.1579] 

[1.x.1580] 

The subject of this tutorial is nonlinear solid mechanics. Classical single-field approaches (see e.g. step-18) can not correctly describe the response of quasi-incompressible materials. The response is overly stiff; a phenomenon known as locking. Locking problems can be circumvented using a variety of alternative strategies. One such strategy is the  three-field formulation. It is used here  to model the three-dimensional, fully-nonlinear (geometrical and material) response of an isotropic continuum body. The material response is approximated as hyperelastic. Additionally, the three-field formulation employed is valid for quasi-incompressible as well as compressible materials. 

The objective of this presentation is to provide a basis for using deal.II for problems in nonlinear solid mechanics. The linear problem was addressed in step-8. A non-standard, hypoelastic-type form of the geometrically nonlinear problem was partially considered in step-18: a rate form of the linearised constitutive relations is used and the problem domain evolves with the motion. Important concepts surrounding the nonlinear kinematics are absent in the theory and implementation. Step-18 does, however, describe many of the key concepts to implement elasticity within the framework of deal.II. 

We begin with a crash-course in nonlinear kinematics. For the sake of simplicity, we restrict our attention to the quasi-static problem. Thereafter, various key stress measures are introduced and the constitutive model described. We then describe the three-field formulation in detail prior to explaining the structure of the class used to manage the material. The setup of the example problem is then presented. 

 [2.x.4059]  This tutorial has been developed (and is described in the introduction) for the problem of elasticity in three dimensions.  While the space dimension could be changed in the main() routine, care needs to be taken.  Two-dimensional elasticity problems, in general, exist only as idealizations of three-dimensional ones.  That is, they are either plane strain or plane stress.  The assumptions that follow either of these choices needs to be consistently imposed.  For more information see the note in step-8. 

[1.x.1581] 

The three-field formulation implemented here was pioneered by Simo et al. (1985) and is known as the mixed Jacobian-pressure formulation. Important related contributions include those by Simo and Taylor (1991), and Miehe (1994). The notation adopted here draws heavily on the excellent overview of the theoretical aspects of nonlinear solid mechanics by Holzapfel (2001). A nice overview of issues pertaining to incompressible elasticity (at small strains) is given in Hughes (2000). 

<ol> 	 [2.x.4060]  J.C. Simo, R.L. Taylor and K.S. Pister (1985), 		Variational and projection methods for the volume constraint in finite deformation elasto-plasticity, 		 [2.x.4061]  Computer Methods in Applied Mechanics and Engineering  [2.x.4062] , 		<strong> 51 </strong>, 1-3, 		177-208. 		DOI: [1.x.1582]; 	 [2.x.4063]  J.C. Simo and R.L. Taylor (1991),   		Quasi-incompressible finite elasticity in principal stretches. Continuum 			basis and numerical algorithms, 		 [2.x.4064]  Computer Methods in Applied Mechanics and Engineering  [2.x.4065] , 		<strong> 85 </strong>, 3, 		273-310. 		DOI: [1.x.1583]; 	 [2.x.4066]  C. Miehe (1994), 		Aspects of the formulation and finite element implementation of large strain isotropic elasticity 		 [2.x.4067]  International Journal for Numerical Methods in Engineering  [2.x.4068]  		<strong> 37 </strong>, 12, 		1981-2004. 		DOI: [1.x.1584]; 	 [2.x.4069]  G.A. Holzapfel (2001), 		Nonlinear Solid Mechanics. A Continuum Approach for Engineering, 		John Wiley & Sons. 		ISBN: 0-471-82304-X; 	 [2.x.4070]  T.J.R. Hughes (2000), 		The Finite Element Method: Linear Static and Dynamic Finite Element Analysis, 		Dover. 		ISBN: 978-0486411811  [2.x.4071]  

An example where this three-field formulation is used in a coupled problem is documented in <ol> 	 [2.x.4072]  J-P. V. Pelteret, D. Davydov, A. McBride, D. K. Vu, and P. Steinmann (2016), 		Computational electro- and magneto-elasticity for quasi-incompressible media immersed in free space, 		 [2.x.4073]  International Journal for Numerical Methods in Engineering  [2.x.4074] . 		DOI: [1.x.1585]  [2.x.4075]  

[1.x.1586] 

One can think of fourth-order tensors as linear operators mapping second-order tensors (matrices) onto themselves in much the same way as matrices map vectors onto vectors. There are various fourth-order unit tensors that will be required in the forthcoming presentation. The fourth-order unit tensors  [2.x.4076]  and  [2.x.4077]  are defined by 

[1.x.1587] 

Note  [2.x.4078] . Furthermore, we define the symmetric and skew-symmetric fourth-order unit tensors by 

[1.x.1588] 

such that 

[1.x.1589] 

The fourth-order  [2.x.4079]  returned by identity_tensor() is  [2.x.4080] . 




[1.x.1590] 

Let the time domain be denoted  [2.x.4081] , where  [2.x.4082]  and  [2.x.4083]  is the total problem duration. Consider a continuum body that occupies the reference configuration  [2.x.4084]  at time  [2.x.4085] . %Particles in the reference configuration are identified by the position vector  [2.x.4086] . The configuration of the body at a later time  [2.x.4087]  is termed the current configuration, denoted  [2.x.4088] , with particles identified by the vector  [2.x.4089] . The nonlinear map between the reference and current configurations, denoted  [2.x.4090] , acts as follows: 

[1.x.1591] 

The material description of the displacement of a particle is defined by 

[1.x.1592] 



The deformation gradient  [2.x.4091]  is defined as the material gradient of the motion: 

[1.x.1593] 

The determinant of the of the deformation gradient  [2.x.4092]  maps corresponding volume elements in the reference and current configurations, denoted  [2.x.4093]  and  [2.x.4094] , respectively, as 

[1.x.1594] 



Two important measures of the deformation in terms of the spatial and material coordinates are the left and right Cauchy-Green tensors, respectively, and denoted  [2.x.4095]  and  [2.x.4096] . They are both symmetric and positive definite. 

The Green-Lagrange strain tensor is defined by 

[1.x.1595] 

If the assumption of infinitesimal deformations is made, then the second term on the right can be neglected, and  [2.x.4097]  (the linearised strain tensor) is the only component of the strain tensor. This assumption is, looking at the setup of the problem, not valid in step-18, making the use of the linearized  [2.x.4098]  as the strain measure in that tutorial program questionable. 

In order to handle the different response that materials exhibit when subjected to bulk and shear type deformations we consider the following decomposition of the deformation gradient  [2.x.4099]   and the left Cauchy-Green tensor  [2.x.4100]  into volume-changing (volumetric) and volume-preserving (isochoric) parts: 

[1.x.1596] 

Clearly,  [2.x.4101] . 

The spatial velocity field is denoted  [2.x.4102] . The derivative of the spatial velocity field with respect to the spatial coordinates gives the spatial velocity gradient  [2.x.4103] , that is 

[1.x.1597] 

where  [2.x.4104] . 




[1.x.1598] 

Cauchy's stress theorem equates the Cauchy traction  [2.x.4105]  acting on an infinitesimal surface element in the current configuration  [2.x.4106]  to the product of the Cauchy stress tensor  [2.x.4107]  (a spatial quantity)  and the outward unit normal to the surface  [2.x.4108]  as 

[1.x.1599] 

The Cauchy stress is symmetric. Similarly,  the first Piola-Kirchhoff traction  [2.x.4109]  which acts on an infinitesimal surface element in the reference configuration  [2.x.4110]  is the product of the first Piola-Kirchhoff stress tensor  [2.x.4111]  (a two-point tensor)  and the outward unit normal to the surface  [2.x.4112]  as 

[1.x.1600] 

The Cauchy traction  [2.x.4113]  and the first Piola-Kirchhoff traction  [2.x.4114]  are related as 

[1.x.1601] 

This can be demonstrated using [1.x.1602]. 

The first Piola-Kirchhoff stress tensor is related to the Cauchy stress as 

[1.x.1603] 

Further important stress measures are the (spatial) Kirchhoff stress   [2.x.4115]  and the (referential) second Piola-Kirchhoff stress  [2.x.4116] . 




[1.x.1604] 

Push-forward and pull-back operators allow one to transform various measures between the material and spatial settings. The stress measures used here are contravariant, while the strain measures are covariant. 

The push-forward and-pull back operations for second-order covariant tensors  [2.x.4117]  are respectively given by: 

[1.x.1605] 



The push-forward and pull back operations for second-order contravariant tensors  [2.x.4118]  are respectively given by: 

[1.x.1606] 

For example  [2.x.4119] . 




[1.x.1607] 

A hyperelastic material response is governed by a Helmholtz free energy function  [2.x.4120]  which serves as a potential for the stress. For example, if the Helmholtz free energy depends on the right Cauchy-Green tensor  [2.x.4121]  then the isotropic hyperelastic response is 

[1.x.1608] 

If the Helmholtz free energy depends on the left Cauchy-Green tensor  [2.x.4122]  then the isotropic hyperelastic response is 

[1.x.1609] 



Following the multiplicative decomposition of the deformation gradient, the Helmholtz free energy can be decomposed as 

[1.x.1610] 

Similarly, the Kirchhoff stress can be decomposed into volumetric and isochoric parts as  [2.x.4123]  where: 

[1.x.1611] 

where  [2.x.4124]  is the pressure response.  [2.x.4125]  is the projection tensor which provides the deviatoric operator in the Eulerian setting. The fictitious Kirchhoff stress tensor  [2.x.4126]  is defined by 

[1.x.1612] 






 [2.x.4127]  The pressure response as defined above differs from the widely-used definition of the pressure in solid mechanics as  [2.x.4128] . Here  [2.x.4129]  is the hydrostatic pressure. We make use of the pressure response throughout this tutorial (although we refer to it as the pressure). 

[1.x.1613] 

The Helmholtz free energy corresponding to a compressible [1.x.1614] is given by 

[1.x.1615] 

where  [2.x.4130]  is the bulk modulus ( [2.x.4131]  and  [2.x.4132]  are the Lam&eacute; parameters) and  [2.x.4133] . The function  [2.x.4134]  is required to be strictly convex and satisfy the condition  [2.x.4135] , among others, see Holzapfel (2001) for further details. In this work  [2.x.4136] . 

Incompressibility imposes the isochoric constraint that  [2.x.4137]  for all motions  [2.x.4138] . The Helmholtz free energy corresponding to an incompressible neo-Hookean material is given by 

[1.x.1616] 

where  [2.x.4139] . Thus, the incompressible response is obtained by removing the volumetric component from the compressible free energy and enforcing  [2.x.4140] . 




[1.x.1617] 

We will use a Newton-Raphson strategy to solve the nonlinear boundary value problem. Thus, we will need to linearise the constitutive relations. 

The fourth-order elasticity tensor in the material description is defined by 

[1.x.1618] 

The fourth-order elasticity tensor in the spatial description  [2.x.4141]  is obtained from the push-forward of  [2.x.4142]  as 

[1.x.1619] 

The fourth-order elasticity tensors (for hyperelastic materials) possess both major and minor symmetries. 

The fourth-order spatial elasticity tensor can be written in the following decoupled form: 

[1.x.1620] 

where 

[1.x.1621] 

where the fictitious elasticity tensor  [2.x.4143]  in the spatial description is defined by 

[1.x.1622] 



[1.x.1623] 

The total potential energy of the system  [2.x.4144]  is the sum of the internal and external potential energies, denoted  [2.x.4145]  and  [2.x.4146] , respectively. We wish to find the equilibrium configuration by minimising the potential energy. 

As mentioned above, we adopt a three-field formulation. We denote the set of primary unknowns by  [2.x.4147] . The independent kinematic variable  [2.x.4148]  enters the formulation as a constraint on  [2.x.4149]  enforced by the Lagrange multiplier  [2.x.4150]  (the pressure, as we shall see). 

The three-field variational principle used here is given by 

[1.x.1624] 

where the external potential is defined by 

[1.x.1625] 

The boundary of the current configuration   [2.x.4151]  is composed into two parts as  [2.x.4152] , where  [2.x.4153] . The prescribed Cauchy traction, denoted  [2.x.4154] , is applied to  [2.x.4155]  while the motion is prescribed on the remaining portion of the boundary  [2.x.4156] . The body force per unit current volume is denoted  [2.x.4157] . 




The stationarity of the potential follows as 

[1.x.1626] 

for all virtual displacements  [2.x.4158]  subject to the constraint that  [2.x.4159]  on  [2.x.4160] , and all virtual pressures  [2.x.4161]  and virtual dilatations  [2.x.4162] . 

One should note that the definitions of the volumetric Kirchhoff stress in the three field formulation  [2.x.4163]   and the subsequent volumetric tangent differs slightly from the general form given in the section on hyperelastic materials where  [2.x.4164] . This is because the pressure  [2.x.4165]  is now a primary field as opposed to a constitutively derived quantity. One needs to carefully distinguish between the primary fields and those obtained from the constitutive relations. 

 [2.x.4166]  Although the variables are all expressed in terms of spatial quantities, the domain of integration is the initial configuration. This approach is called a  [2.x.4167]  total-Lagrangian formulation  [2.x.4168] . The approach given in step-18, where the domain of integration is the current configuration, could be called an  [2.x.4169]  updated Lagrangian formulation  [2.x.4170] . The various merits of these two approaches are discussed widely in the literature. It should be noted however that they are equivalent. 


The Euler-Lagrange equations corresponding to the residual are: 

[1.x.1627] 

The first equation is the (quasi-static) equilibrium equation in the spatial setting. The second is the constraint that  [2.x.4171] . The third is the definition of the pressure  [2.x.4172] . 

 [2.x.4173]  The simplified single-field derivation ( [2.x.4174]  is the only primary variable) below makes it clear how we transform the limits of integration to the reference domain: 

[1.x.1628] 

where  [2.x.4175] . 

We will use an iterative Newton-Raphson method to solve the nonlinear residual equation  [2.x.4176] . For the sake of simplicity we assume dead loading, i.e. the loading does not change due to the deformation. 

The change in a quantity between the known state at  [2.x.4177]  and the currently unknown state at  [2.x.4178]  is denoted  [2.x.4179] . The value of a quantity at the current iteration  [2.x.4180]  is denoted  [2.x.4181] . The incremental change between iterations  [2.x.4182]  and  [2.x.4183]  is denoted  [2.x.4184] . 

Assume that the state of the system is known for some iteration  [2.x.4185] . The linearised approximation to nonlinear governing equations to be solved using the  Newton-Raphson method is: Find  [2.x.4186]  such that 

[1.x.1629] 

then set  [2.x.4187] . The tangent is given by 

[1.x.1630] 

Thus, 

[1.x.1631] 

where 

[1.x.1632] 



Note that the following terms are termed the geometrical stress and  the material contributions to the tangent matrix: 

[1.x.1633] 






[1.x.1634] 

The three-field formulation used here is effective for quasi-incompressible materials, that is where  [2.x.4188]  (where  [2.x.4189]  is [1.x.1635]), subject to a good choice of the interpolation fields for  [2.x.4190]  and  [2.x.4191] . Typically a choice of  [2.x.4192]  is made. Here  [2.x.4193]  is the FE_DGPMonomial class. A popular choice is  [2.x.4194]  which is known as the mean dilatation method (see Hughes (2000) for an intuitive discussion). This code can accommodate a  [2.x.4195]  formulation. The discontinuous approximation allows  [2.x.4196]  and  [2.x.4197]  to be condensed out and a classical displacement based method is recovered. 

For fully-incompressible materials  [2.x.4198]  and the three-field formulation will still exhibit locking behavior. This can be overcome by introducing an additional constraint into the free energy of the form  [2.x.4199] . Here  [2.x.4200]  is a Lagrange multiplier to enforce the isochoric constraint. For further details see Miehe (1994). 

The linearised problem can be written as 

[1.x.1636] 

where 

[1.x.1637] 



There are no derivatives of the pressure and dilatation (primary) variables present in the formulation. Thus the discontinuous finite element interpolation of the pressure and dilatation yields a block diagonal matrix for  [2.x.4201] ,  [2.x.4202]  and  [2.x.4203] . Therefore we can easily express the fields  [2.x.4204]  and  [2.x.4205]  on each cell simply by inverting a local matrix and multiplying it by the local right hand side. We can then insert the result into the remaining equations and recover a classical displacement-based method. In order to condense out the pressure and dilatation contributions at the element level we need the following results: 

[1.x.1638] 

and thus 

[1.x.1639] 

where 

[1.x.1640] 

Note that due to the choice of  [2.x.4206]  and  [2.x.4207]  as discontinuous at the element level, all matrices that need to be inverted are defined at the element level. 

The procedure to construct the various contributions is as follows: 

- Construct  [2.x.4208] . 

- Form  [2.x.4209]  for element and store where  [2.x.4210]  was stored in  [2.x.4211] . 

- Form  [2.x.4212]  and add to  [2.x.4213]  to get  [2.x.4214]  

- The modified system matrix is called  [2.x.4215] .   That is   [1.x.1641] 






[1.x.1642] 

A good object-oriented design of a Material class would facilitate the extension of this tutorial to a wide range of material types. In this tutorial we simply have one Material class named Material_Compressible_Neo_Hook_Three_Field. Ideally this class would derive from a class HyperelasticMaterial which would derive from the base class Material. The three-field nature of the formulation used here also complicates the matter. 

The Helmholtz free energy function for the three field formulation is  [2.x.4216] . The isochoric part of the Kirchhoff stress  [2.x.4217]  is identical to that obtained using a one-field formulation for a hyperelastic material. However, the volumetric part of the free energy is now a function of the primary variable  [2.x.4218] . Thus, for a three field formulation the constitutive response for the volumetric part of the Kirchhoff stress  [2.x.4219]  (and the tangent) is not given by the hyperelastic constitutive law as in a one-field formulation. One can label the term  [2.x.4220]  as the volumetric Kirchhoff stress, but the pressure  [2.x.4221]  is not derived from the free energy; it is a primary field. 

In order to have a flexible approach, it was decided that the Material_Compressible_Neo_Hook_Three_Field would still be able to calculate and return a volumetric Kirchhoff stress and tangent. In order to do this, we choose to store the interpolated primary fields  [2.x.4222]  and  [2.x.4223]  in the Material_Compressible_Neo_Hook_Three_Field class associated with the quadrature point. This decision should be revisited at a later stage when the tutorial is extended to account for other materials. 




[1.x.1643] 

The numerical example considered here is a nearly-incompressible block under compression. This benchmark problem is taken from 

- S. Reese, P. Wriggers, B.D. Reddy (2000),   A new locking-free brick element technique for large deformation problems in elasticity,    [2.x.4224]  Computers and Structures  [2.x.4225] ,   <strong> 75 </strong>,   291-304.   DOI: [1.x.1644] 

  [2.x.4226]  

The material is quasi-incompressible neo-Hookean with [1.x.1645]  [2.x.4227]  and  [2.x.4228] . For such a choice of material properties a conventional single-field  [2.x.4229]  approach would lock. That is, the response would be overly stiff. The initial and final configurations are shown in the image above. Using symmetry, we solve for only one quarter of the geometry (i.e. a cube with dimension  [2.x.4230] ). The inner-quarter of the upper surface of the domain is subject to a load of  [2.x.4231] . 


examples/step-44/doc/results.dox 



[1.x.1646] 

Firstly, we present a comparison of a series of 3-d results with those in the literature (see Reese et al (2000)) to demonstrate that the program works as expected. 

We begin with a comparison of the convergence with mesh refinement for the  [2.x.4232]  and  [2.x.4233]  formulations, as summarised in the figure below. The vertical displacement of the midpoint of the upper surface of the block is used to assess convergence. Both schemes demonstrate good convergence properties for varying values of the load parameter  [2.x.4234] . The results agree with those in the literature. The lower-order formulation typically overestimates the displacement for low levels of refinement, while the higher-order interpolation scheme underestimates it, but be a lesser degree. This benchmark, and a series of others not shown here, give us confidence that the code is working as it should. 

 [2.x.4235]  


A typical screen output generated by running the problem is shown below. The particular case demonstrated is that of the  [2.x.4236]  formulation. It is clear that, using the Newton-Raphson method, quadratic convergence of the solution is obtained. Solution convergence is achieved within 5 Newton increments for all time-steps. The converged displacement's  [2.x.4237] -norm is several orders of magnitude less than the geometry scale. 

[1.x.1647] 






Using the Timer class, we can discern which parts of the code require the highest computational expense. For a case with a large number of degrees-of-freedom (i.e. a high level of refinement), a typical output of the Timer is given below. Much of the code in the tutorial has been developed based on the optimizations described, discussed and demonstrated in Step-18 and others. With over 93% of the time being spent in the linear solver, it is obvious that it may be necessary to invest in a better solver for large three-dimensional problems. The SSOR preconditioner is not multithreaded but is effective for this class of solid problems. It may be beneficial to investigate the use of another solver such as those available through the Trilinos library. 




[1.x.1648] 




We then used ParaView to visualize the results for two cases. The first was for the coarsest grid and the lowest-order interpolation method:  [2.x.4238] . The second was on a refined grid using a  [2.x.4239]  formulation. The vertical component of the displacement, the pressure  [2.x.4240]  and the dilatation  [2.x.4241]  fields are shown below. 


For the first case it is clear that the coarse spatial discretization coupled with large displacements leads to a low quality solution (the loading ratio is   [2.x.4242] ). Additionally, the pressure difference between elements is very large. The constant pressure field on the element means that the large pressure gradient is not captured. However, it should be noted that locking, which would be present in a standard  [2.x.4243]  displacement formulation does not arise even in this poorly discretised case. The final vertical displacement of the tracked node on the top surface of the block is still within 12.5% of the converged solution. The pressure solution is very coarse and has large jumps between adjacent cells. It is clear that the volume nearest to the applied traction undergoes compression while the outer extents of the domain are in a state of expansion. The dilatation solution field and pressure field are clearly linked, with positive dilatation indicating regions of positive pressure and negative showing regions placed in compression. As discussed in the Introduction, a compressive pressure has a negative sign while an expansive pressure takes a positive sign. This stems from the definition of the volumetric strain energy function and is opposite to the physically realistic interpretation of pressure. 


 [2.x.4244]  

Combining spatial refinement and a higher-order interpolation scheme results in a high-quality solution. Three grid refinements coupled with a  [2.x.4245]  formulation produces a result that clearly captures the mechanics of the problem. The deformation of the traction surface is well resolved. We can now observe the actual extent of the applied traction, with the maximum force being applied at the central point of the surface causing the largest compression. Even though very high strains are experienced in the domain, especially at the boundary of the region of applied traction, the solution remains accurate. The pressure field is captured in far greater detail than before. There is a clear distinction and transition between regions of compression and expansion, and the linear approximation of the pressure field allows a refined visualization of the pressure at the sub-element scale. It should however be noted that the pressure field remains discontinuous and could be smoothed on a continuous grid for the post-processing purposes. 




 [2.x.4246]  

This brief analysis of the results demonstrates that the three-field formulation is effective in circumventing volumetric locking for highly-incompressible media. The mixed formulation is able to accurately simulate the displacement of a near-incompressible block under compression. The command-line output indicates that the volumetric change under extreme compression resulted in less than 0.01% volume change for a Poisson's ratio of 0.4999. 

In terms of run-time, the  [2.x.4247]  formulation tends to be more computationally expensive than the  [2.x.4248]  for a similar number of degrees-of-freedom (produced by adding an extra grid refinement level for the lower-order interpolation). This is shown in the graph below for a batch of tests run consecutively on a single 4-core (8-thread) machine. The increase in computational time for the higher-order method is likely due to the increased band-width required for the higher-order elements. As previously mentioned, the use of a better solver and preconditioner may mitigate the expense of using a higher-order formulation. It was observed that for the given problem using the multithreaded Jacobi preconditioner can reduce the computational runtime by up to 72% (for the worst case being a higher-order formulation with a large number of degrees-of-freedom) in comparison to the single-thread SSOR preconditioner. However, it is the author's experience that the Jacobi method of preconditioning may not be suitable for some finite-strain problems involving alternative constitutive models. 


 [2.x.4249]  


Lastly, results for the displacement solution for the 2-d problem are showcased below for two different levels of grid refinement. It is clear that due to the extra constraints imposed by simulating in 2-d that the resulting displacement field, although qualitatively similar, is different to that of the 3-d case. 


 [2.x.4250]  

[1.x.1649] 

[1.x.1650] 

There are a number of obvious extensions for this work: 

- Firstly, an additional constraint could be added to the free-energy   function in order to enforce a high degree of incompressibility in   materials. An additional Lagrange multiplier would be introduced,   but this could most easily be dealt with using the principle of   augmented Lagrange multipliers. This is demonstrated in  [2.x.4251] Simo and   Taylor (1991)  [2.x.4252] . 

- The constitutive relationship used in this   model is relatively basic. It may be beneficial to split the material   class into two separate classes, one dealing with the volumetric   response and the other the isochoric response, and produce a generic   materials class (i.e. having abstract virtual functions that derived   classes have to implement) that would allow for the addition of more complex   material models. Such models could include other hyperelastic   materials, plasticity and viscoelastic materials and others. 

- The program has been developed for solving problems on single-node   multicore machines. With a little effort, the program could be   extended to a large-scale computing environment through the use of   Petsc or Trilinos, using a similar technique to that demonstrated in   step-40. This would mostly involve changes to the setup, assembly,    [2.x.4253]  and linear solver routines. 

- As this program assumes quasi-static equilibrium, extensions to   include dynamic effects would be necessary to study problems where   inertial effects are important, e.g. problems involving impact. 

- Load and solution limiting procedures may be necessary for highly   nonlinear problems. It is possible to add a linesearch algorithm to   limit the step size within a Newton increment to ensure optimum   convergence. It may also be necessary to use a load limiting method,   such as the Riks method, to solve unstable problems involving   geometric instability such as buckling and snap-through. 

- Many physical problems involve contact. It is possible to include   the effect of frictional or frictionless contact between objects   into this program. This would involve the addition of an extra term   in the free-energy functional and therefore an addition to the   assembly routine. One would also need to manage the contact problem   (detection and stress calculations) itself. An alternative to   additional penalty terms in the free-energy functional would be to   use active set methods such as the one used in step-41. 

- The complete condensation procedure using LinearOperators has been   coded into the linear solver routine. This could also have been   achieved through the application of the schur_complement()   operator to condense out one or more of the fields in a more   automated manner. 

- Finally, adaptive mesh refinement, as demonstrated in step-6 and   step-18, could provide additional solution accuracy. 


examples/step-45/doc/intro.dox 

 [2.x.4254]  

[1.x.1651] [1.x.1652] 

[1.x.1653] 

In this example we present how to use periodic boundary conditions in deal.II. Periodic boundary conditions are algebraic constraints that typically occur in computations on representative regions of a larger domain that repeat in one or more directions. 

An example is the simulation of the electronic structure of photonic crystals, because they have a lattice-like structure and, thus, it often suffices to do the actual computation on only one box of the lattice. To be able to proceed this way one has to assume that the model can be periodically extended to the other boxes; this requires the solution to have a periodic structure. 

[1.x.1654] 

[1.x.1655] 

deal.II provides a number of high level entry points to impose periodic boundary conditions. The general approach to apply periodic boundary conditions consists of three steps (see also the  [2.x.4255]  "Glossary entry on periodic boundary conditions"): 

-# Create a mesh 

-# Identify those pairs of faces on different parts of the boundary across which    the solution should be symmetric, using  [2.x.4256]  

-# Add the periodicity information to the mesh    using  [2.x.4257]  

-# Add periodicity constraints using  [2.x.4258]  

The second and third step are necessary for parallel meshes using the  [2.x.4259]  class to ensure that cells on opposite sides of the domain but connected by periodic faces are part of the ghost layer if one of them is stored on the local processor. If the Triangulation is not a  [2.x.4260]  these steps are not necessary. 

The first step consists of collecting matching periodic faces and storing them in a  [2.x.4261]  of  [2.x.4262]  This is done with the function  [2.x.4263]  that can be invoked for example like this: 

[1.x.1656] 



This call loops over all faces of the container dof_handler on the periodic boundaries with boundary indicator  [2.x.4264]  and  [2.x.4265]  respectively. (You can assign these boundary indicators by hand after creating the coarse mesh, see  [2.x.4266]  "Boundary indicator". Alternatively, you can also let many of the functions in namespace GridGenerator do this for if you specify the "colorize" flag; in that case, these functions will assign different boundary indicators to different parts of the boundary, with the details typically spelled out in the documentation of these functions.) 

Concretely, if  [2.x.4267]  are the vertices of two faces  [2.x.4268] , then the function call above will match pairs of faces (and dofs) such that the difference between  [2.x.4269]  and  [2.x.4270]  vanishes in every component apart from direction and stores the resulting pairs with associated data in  [2.x.4271]  (See  [2.x.4272]  for detailed information about the matching process.) 

Consider, for example, the colored unit square  [2.x.4273]  with boundary indicator 0 on the left, 1 on the right, 2 on the bottom and 3 on the top faces. (See the documentation of  [2.x.4274]  for this convention on how boundary indicators are assigned.) Then, 

[1.x.1657] 

would yield periodicity constraints such that  [2.x.4275]  for all  [2.x.4276] . 

If we instead consider the parallelogram given by the convex hull of  [2.x.4277] ,  [2.x.4278] ,  [2.x.4279] ,  [2.x.4280]  we can achieve the constraints  [2.x.4281]  by specifying an  [2.x.4282]  

[1.x.1658] 

or 

[1.x.1659] 

Here, again, the assignment of boundary indicators 0 and 1 stems from what  [2.x.4283]  documents. 

The resulting  [2.x.4284]  can be used in  [2.x.4285]  for populating an AffineConstraints object with periodicity constraints: 

[1.x.1660] 



Apart from this high level interface there are also variants of  [2.x.4286]  available that combine those two steps (see the variants of  [2.x.4287]  

There is also a low level interface to  [2.x.4288]  if more flexibility is needed. The low level variant allows to directly specify two faces that shall be constrained: 

[1.x.1661] 

Here, we need to specify the orientation of the two faces using  [2.x.4289]   [2.x.4290]  and  [2.x.4291]  For a closer description have a look at the documentation of  [2.x.4292]  The remaining parameters are the same as for the high level interface apart from the self-explaining  [2.x.4293]  and  [2.x.4294]  


[1.x.1662] 

[1.x.1663] 

In the following, we show how to use the above functions in a more involved example. The task is to enforce rotated periodicity constraints for the velocity component of a Stokes flow. 

On a quarter-circle defined by  [2.x.4295]  we are going to solve the Stokes problem 

[1.x.1664] 

where the boundary  [2.x.4296]  is defined as  [2.x.4297] . For the remaining parts of the boundary we are going to use periodic boundary conditions, i.e. 

[1.x.1665] 



The mesh will be generated by  [2.x.4298]  which also documents how it assigns boundary indicators to its various boundaries if its `colorize` argument is set to `true`. 


examples/step-45/doc/results.dox 



[1.x.1666] 

The created output is not very surprising. We simply see that the solution is periodic with respect to the left and lower boundary: 

 [2.x.4299]  

Without the periodicity constraints we would have ended up with the following solution: 

 [2.x.4300]  


examples/step-46/doc/intro.dox 

 [2.x.4301]  

[1.x.1667] 


[1.x.1668] 

[1.x.1669] 

This program deals with the problem of coupling different physics in different parts of the domain. Specifically, let us consider the following situation that couples a Stokes fluid with an elastic solid (these two problems were previously discussed separately in step-22 and step-8, where you may want to read up on the individual equations): 

- In a part  [2.x.4302]  of  [2.x.4303] , we have a fluid flowing that satisfies the   time independent Stokes equations (in the form that involves the strain   tensor):   [1.x.1670] 

  Here,  [2.x.4304]  are the fluid velocity and pressure, respectively.   We prescribe the velocity on part of the external boundary,   [1.x.1671] 

  while we assume free-flow conditions on the remainder of the external   boundary,   [1.x.1672] 



- The remainder of the domain,  [2.x.4305]  is   occupied by a solid whose deformation field  [2.x.4306]  satisfies the   elasticity equation,   [1.x.1673] 

  where  [2.x.4307]  is the rank-4 elasticity tensor (for which we will use a   particularly simple form by assuming that the solid is isotropic).   It deforms in reaction to the forces exerted by the   fluid flowing along the boundary of the solid. We assume this deformation to   be so small that it has no feedback effect on the fluid, i.e. the coupling   is only in one direction. For simplicity, we will assume that the   solid's external boundary is clamped, i.e.   [1.x.1674] 



- As a consequence of the small displacement assumption, we will pose the   following boundary conditions on the interface between the fluid and solid:   first, we have no slip boundary conditions for the fluid,   [1.x.1675] 

  Secondly, the forces (traction) on the solid equal the normal stress from the fluid,   [1.x.1676] 

  where  [2.x.4308]  is the normal vector on  [2.x.4309]  pointing from   the solid to the fluid. 

We get a weak formulation of this problem by following our usual rule of multiplying from the left by a test function and integrating over the domain. It then looks like this: Find  [2.x.4310]  such that 

[1.x.1677] 

for all test functions  [2.x.4311] ; the first, second, and third lines correspond to the fluid, solid, and interface contributions, respectively. Note that  [2.x.4312]  is only a subspace of the spaces listed above to accommodate for the various Dirichlet boundary conditions. 

This sort of coupling is of course possible by simply having two Triangulation and two DoFHandler objects, one each for each of the two subdomains. On the other hand, deal.II is much simpler to use if there is a single DoFHandler object that knows about the discretization of the entire problem. 

This program is about how this can be achieved. Note that the goal is not to present a particularly useful physical model (a realistic fluid-structure interaction model would have to take into account the finite deformation of the solid and the effect this has on the fluid): this is, after all, just a tutorial program intended to demonstrate techniques, not to solve actual problems. Furthermore, we will make the assumption that the interface between the subdomains is aligned with coarse mesh cell faces. 




[1.x.1678] 

Before going into more details let us state the obvious: this is a problem with multiple solution variables; for this, you will probably want to read the  [2.x.4313]  documentation module first, which presents the basic philosophical framework in which we address problems with more than one solution variable. But back to the problem at hand: 

The fundamental idea to implement these sort of problems in deal.II goes as follows: in the problem formulation, the velocity and pressure variables  [2.x.4314]  only live in the fluid subdomain  [2.x.4315] . But let's assume that we extend them by zero to the entire domain  [2.x.4316]  (in the general case this means that they will be discontinuous along  [2.x.4317] ). So what is the appropriate function space for these variables? We know that on  [2.x.4318]  we should require  [2.x.4319] , so for the extensions  [2.x.4320]  to the whole domain the following appears a useful set of function spaces: 

[1.x.1679] 

(Since this is not important for the current discussion, we have omitted the question of boundary values from the choice of function spaces; this question also affects whether we can choose  [2.x.4321]  for the pressure or whether we have to choose the space  [2.x.4322]  for the pressure. None of these questions are relevant to the following discussion, however.) 

Note that these are indeed a linear function spaces with obvious norm. Since no confusion is possible in practice, we will henceforth omit the tilde again to denote the extension of a function to the whole domain and simply refer by  [2.x.4323]  to both the original and the extended function. 

For discretization, we need finite dimensional subspaces  [2.x.4324]  of  [2.x.4325] . For Stokes, we know from step-22 that an appropriate choice is  [2.x.4326]  but this only holds for that part of the domain occupied by the fluid. For the extended field, let's use the following subspaces defined on the triangulation  [2.x.4327] : 

[1.x.1680] 

In other words, on  [2.x.4328]  we choose the usual discrete spaces but we keep the (discontinuous) extension by zero. The point to make is that we now need a description of a finite element space for functions that are zero on a cell &mdash; and this is where the FE_Nothing class comes in: it describes a finite dimensional function space of functions that are constant zero. A particular property of this peculiar linear vector space is that it has no degrees of freedom: it isn't just finite dimensional, it is in fact zero dimensional, and consequently for objects of this type,  [2.x.4329]  will return zero. For discussion below, let us give this space a proper symbol: 

[1.x.1681] 

The symbol  [2.x.4330]  reminds of the fact that functions in this space are zero. Obviously, we choose  [2.x.4331] . 

This entire discussion above can be repeated for the variables we use to describe the elasticity equation. Here, for the extended variables, we have 

[1.x.1682] 

and we will typically use a finite element space of the kind 

[1.x.1683] 

of polynomial degree  [2.x.4332] . 

So to sum up, we are going to look for a discrete vector-valued solution  [2.x.4333]  in the following space: 

[1.x.1684] 






[1.x.1685] 

So how do we implement this sort of thing? First, we realize that the discrete space  [2.x.4334]  essentially calls for two different finite elements: First, on the fluid subdomain, we need the element  [2.x.4335]  which in deal.II is readily implemented by 

[1.x.1686] 

where  [2.x.4336]  implements the space of functions that are always zero. Second, on the solid subdomain, we need the element  [2.x.4337] , which we get using 

[1.x.1687] 



The next step is that we associate each of these two elements with the cells that occupy each of the two subdomains. For this we realize that in a sense the two elements are just variations of each other in that they have the same number of vector components but have different polynomial degrees &mdash; this smells very much like what one would do in  [2.x.4338]  finite element methods, and it is exactly what we are going to do here: we are going to (ab)use the classes and facilities of the hp-namespace to assign different elements to different cells. In other words, we will use collect the two finite elements in an  [2.x.4339]  will integrate with an appropriate  [2.x.4340]  using an  [2.x.4341]  object, and our DoFHandler will be in [1.x.1688]-mode. You may wish to take a look at step-27 for an overview of all of these concepts. 

Before going on describing the testcase, let us clarify a bit [1.x.1689] this approach of extending the functions by zero to the entire domain and then mapping the problem on to the hp-framework makes sense: 

- It makes things uniform: On all cells, the number of vector components is   the same (here,  [2.x.4342] ). This makes all sorts of   things possible since a uniform description allows for code   re-use. For example, counting degrees of freedom per vector   component  [2.x.4343]  sorting degrees of   freedom by component  [2.x.4344]  subsequent   partitioning of matrices and vectors into blocks and many other   functions work as they always did without the need to add special   logic to them that describes cases where some of the variables only   live on parts of the domain. Consequently, you have all sorts of   tools already available to you in programs like the current one that   weren't originally written for the multiphysics case but work just   fine in the current context. 

- It allows for easy graphical output: All graphical output formats we support   require that each field in the output is defined on all nodes of the   mesh. But given that now all solution components live everywhere,   our existing DataOut routines work as they always did, and produce   graphical output suitable for visualization -- the fields will   simply be extended by zero, a value that can easily be filtered out   by visualization programs if not desired. 

- There is essentially no cost: The trick with the FE_Nothing does not add any   degrees of freedom to the overall problem, nor do we ever have to handle a   shape function that belongs to these components &mdash; the FE_Nothing has   no degrees of freedom, not does it have shape functions, all it does is take   up vector components. 




[1.x.1690] 

More specifically, in the program we have to address the following points: 

- Implementing the bilinear form, and in particular dealing with the   interface term, both in the matrix and the sparsity pattern. 

- Implementing Dirichlet boundary conditions on the external and   internal parts of the boundaries    [2.x.4345] . 




[1.x.1691] 

Let us first discuss implementing the bilinear form, which at the discrete level we recall to be 

[1.x.1692] 

Given that we have extended the fields by zero, we could in principle write the integrals over subdomains to the entire domain  [2.x.4346] , though it is little additional effort to first ask whether a cell is part of the elastic or fluid region before deciding which terms to integrate. Actually integrating these terms is not very difficult; for the Stokes equations, the relevant steps have been shown in step-22, whereas for the elasticity equation we take essentially the form shown in the  [2.x.4347]  module (rather than the one from step-8). 

The term that is of more interest is the interface term, 

[1.x.1693] 

Based on our assumption that the interface  [2.x.4348]  coincides with cell boundaries, this can in fact be written as a set of face integrals. If we denote the velocity, pressure and displacement components of shape function  [2.x.4349]  using the extractor notation  [2.x.4350] , then the term above yields the following contribution to the global matrix entry  [2.x.4351] : 

[1.x.1694] 

Although it isn't immediately obvious, this term presents a slight complication: while  [2.x.4352]  and  [2.x.4353]  are evaluated on the solid side of the interface (they are test functions for the displacement and the normal vector to  [2.x.4354] , respectively, we need to evaluate  [2.x.4355]  on the fluid side of the interface since they correspond to the stress/force exerted by the fluid. In other words, in our implementation, we will need FEFaceValue objects for both sides of the interface. To make things slightly worse, we may also have to deal with the fact that one side or the other may be refined, leaving us with the need to integrate over parts of a face. Take a look at the implementation below on how to deal with this. 

As an additional complication, the matrix entries that result from this term need to be added to the sparsity pattern of the matrix somehow. This is the realm of various functions in the DoFTools namespace like  [2.x.4356]  and  [2.x.4357]  Essentially, what these functions do is simulate what happens during assembly of the system matrix: whenever assembly would write a nonzero entry into the global matrix, the functions in DoFTools would add an entry to the sparsity pattern. We could therefore do the following: let  [2.x.4358]  add all those entries to the sparsity pattern that arise from the regular cell-by-cell integration, and then do the same by hand that arise from the interface terms. If you look at the implementation of the interface integrals in the program below, it should be obvious how to do that and would require no more than maybe 100 lines of code at most. 

But we're lazy people: the interface term couples degrees of freedom from two adjacent cells along a face, which is exactly the kind of thing one would do in discontinuous Galerkin schemes for which the function  [2.x.4359]  was written. This is a superset of matrix entries compared to the usual  [2.x.4360]  it will also add all entries that result from computing terms coupling the degrees of freedom from both sides of all faces. Unfortunately, for the simplest version of this function, this is a pretty big superset. Consider for example the following mesh with two cells and a  [2.x.4361]  finite element: 

[1.x.1695] 

Here, the sparsity pattern produced by  [2.x.4362]  will only have entries for degrees of freedom that couple on a cell. However, it will not have sparsity pattern entries  [2.x.4363] . The sparsity pattern generated by  [2.x.4364]  will have these entries, however: it assumes that you want to build a sparsity pattern for a bilinear form that couples [1.x.1696] degrees of freedom from adjacent cells. This is not what we want: our interface term acts only on a small subset of cells, and we certainly don't need all the extra couplings between two adjacent fluid cells, or two adjacent solid cells. Furthermore, the fact that we use higher order elements means that we would really generate many many more entries than we actually need: on the coarsest mesh, in 2d, 44,207 nonzero entries instead of 16,635 for  [2.x.4365]  leading to plenty of zeros in the matrix we later build (of course, the 16,635 are not enough since they don't include the interface entries). This ratio would be even worse in 3d. 

So being extremely lazy comes with a cost: too many entries in the matrix. But we can get away with being moderately lazy: there is a variant of  [2.x.4366]  that allows us to specify which vector components of the finite element couple with which other components, both in cell terms as well as in face terms. For cells that are in the solid subdomain, we couple all displacements with each other; for fluid cells, all velocities with all velocities and the pressure, but not the pressure with itself. Since no cell has both sets of variables, there is no need to distinguish between the two kinds of cells, so we can write the mask like this: 

[1.x.1697] 

Here, we have used the fact that the first  [2.x.4367]  components of the finite element are the velocities, then the pressure, and then the  [2.x.4368]  displacements. (We could as well have stated that the velocities/pressure also couple with the displacements since no cell ever has both sets of variables.) On the other hand, the interface terms require a mask like this: 

[1.x.1698] 

In other words, all displacement test functions (components  [2.x.4369] ) couple with all velocity and pressure shape functions on the other side of an interface. This is not entirely true, though close: in fact, the exact form of the interface term only those pressure displacement shape functions that are indeed nonzero on the common interface, which is not true for all shape functions; on the other hand, it really couples all velocities (since the integral involves gradients of the velocity shape functions, which are all nonzero on all faces of the cell). However, the mask we build above, is not capable of these subtleties. Nevertheless, through these masks we manage to get the number of sparsity pattern entries down to 21,028 &mdash; good enough for now. 




[1.x.1699] 

The second difficulty is that while we know how to enforce a zero velocity or stress on the external boundary (using  [2.x.4370]  called with an appropriate component mask and setting different boundary indicators for solid and fluid external boundaries), we now also needed the velocity to be zero on the interior interface, i.e.  [2.x.4371] . At the time of writing this, there is no function in deal.II that handles this part, but it isn't particularly difficult to implement by hand: essentially, we just have to loop over all cells, and if it is a fluid cell and its neighbor is a solid cell, then add constraints that ensure that the velocity degrees of freedom on this face are zero. Some care is necessary to deal with the case that the adjacent solid cell is refined, yielding the following code: 

[1.x.1700] 



The call  [2.x.4372]  tells the AffineConstraints to start a new constraint for degree of freedom  [2.x.4373]  of the form  [2.x.4374] . Typically, one would then proceed to set individual coefficients  [2.x.4375]  to nonzero values (using  [2.x.4376]  or set  [2.x.4377]  to something nonzero (using  [2.x.4378]  doing nothing as above, funny as it looks, simply leaves the constraint to be  [2.x.4379] , which is exactly what we need in the current context. The call to  [2.x.4380]  makes sure that we only set boundary values to zero for velocity but not pressure components. 

Note that there are cases where this may yield incorrect results: notably, once we find a solid neighbor child to a current fluid cell, we assume that all neighbor children on the common face are in the solid subdomain. But that need not be so; consider, for example, the following mesh: 

[1.x.1701] 



In this case, we would set all velocity degrees of freedom on the right face of the left cell to zero, which is incorrect for the top degree of freedom on that face. That said, that can only happen if the fluid and solid subdomains do not coincide with a set of complete coarse mesh cells &mdash; but this is a contradiction to the assumption stated at the end of the first section of this introduction. 




[1.x.1702] 

We will consider the following situation as a testcase: 

 [2.x.4381]  

As discussed at the top of this document, we need to assume in a few places that a cell is either entirely in the fluid or solid part of the domain and, furthermore, that all children of an inactive cell also belong to the same subdomain. This can definitely be ensured if the coarse mesh already subdivides the mesh into solid and fluid coarse mesh cells; given the geometry outlined above, we can do that by using an  [2.x.4382]  coarse mesh, conveniently provided by the  [2.x.4383]  function. 

The fixed boundary at the bottom implies  [2.x.4384] , and we also prescribe Dirichlet conditions for the flow at the top so that we get inflow at the left and outflow at the right. At the left and right boundaries, no boundary conditions are imposed explicitly for the flow, yielding the implicit no-stress condition  [2.x.4385] . The conditions on the interface between the two domains has already been discussed above. 

For simplicity, we choose the material parameters to be  [2.x.4386] . In the results section below, we will also show a 3d simulation that can be obtained from the same program. The boundary conditions and geometry are defined nearly analogously to the 2d situation above. 




[1.x.1703] 

In the program, we need a way to identify which part of the domain a cell is in. There are many different ways of doing this. A typical way would be to use the  [2.x.4387]  "subdomain_id" tag available with each cell, though this field has a special meaning in %parallel computations. An alternative is the  [2.x.4388]  "material_id" field also available with every cell. It has the additional advantage that it is inherited from the mother to the child cell upon mesh refinement; in other words, we would set the material id once upon creating the mesh and it will be correct for all active cells even after several refinement cycles. We therefore go with this alternative: we define an  [2.x.4389]  with symbolic names for material_id numbers and will use them to identify which part of the domain a cell is on. 

Secondly, we use an object of type DoFHandler operating in [1.x.1704]-mode. This class needs to know which cells will use the Stokes and which the elasticity finite element. At the beginning of each refinement cycle we will therefore have to walk over all cells and set the (in hp-parlance) active FE index to whatever is appropriate in the current situation. While we can use symbolic names for the material id, the active FE index is in fact a number that will frequently be used to index into collections of objects (e.g. of type  [2.x.4390]  and  [2.x.4391]  that means that the active FE index actually has to have value zero for the fluid and one for the elastic part of the domain. 




[1.x.1705] 

This program is primarily intended to show how to deal with different physics in different parts of the domain, and how to implement such models in deal.II. As a consequence, we won't bother coming up with a good solver: we'll just use the SparseDirectUMFPACK class which always works, even if not with optimal complexity. We will, however, comment on possible other solvers in the [1.x.1706] section. 




[1.x.1707] 

One of the trickier aspects of this program is how to estimate the error. Because it works on almost any program, we'd like to use the KellyErrorEstimator, and we can relatively easily do that here as well using code like the following: 

[1.x.1708] 

This gives us two sets of error indicators for each cell. We would then somehow combine them into one for mesh refinement, for example using something like the following (note that we normalize the squared error indicator in the two vectors because error quantities have physical units that do not match in the current situation, leading to error indicators that may differ by orders of magnitude between the two subdomains): 

[1.x.1709] 

(In the code, we actually weigh the error indicators 4:1 in favor of the ones computed on the Stokes subdomain since refinement is otherwise heavily biased towards the elastic subdomain, but this is just a technicality. The factor 4 has been determined heuristically to work reasonably well.) 

While this principle is sound, it doesn't quite work as expected. The reason is that the KellyErrorEstimator class computes error indicators by integrating the jump in the solution's gradient around the faces of each cell. This jump is likely to be very large at the locations where the solution is discontinuous and extended by zero; it also doesn't become smaller as the mesh is refined. The KellyErrorEstimator class can't just ignore the interface because it essentially only sees a DoFHandler in [1.x.1710]-mode where the element type changes from one cell to another &mdash; precisely the thing that the [1.x.1711]-mode was designed for, the interface in the current program looks no different than the interfaces in step-27, for example, and certainly no less legitimate. Be that as it may, the end results is that there is a layer of cells on both sides of the interface between the two subdomains where error indicators are irrationally large. Consequently, most of the mesh refinement is focused on the interface. 

This clearly wouldn't happen if we had a refinement indicator that actually understood something about the problem and simply ignore the interface between subdomains when integrating jump terms. On the other hand, this program is about showing how to represent problems where we have different physics in different subdomains, not about the peculiarities of the KellyErrorEstimator, and so we resort to the big hammer called "heuristics": we simply set the error indicators of cells at the interface to zero. This cuts off the spikes in the error indicators. At first sight one would also think that it prevents the mesh from being refined at the interface, but the requirement that neighboring cells may only differ by one level of refinement will still lead to a reasonably refined mesh. 

While this is clearly a suboptimal solution, it works for now and leaves room for future improvement. 


examples/step-46/doc/results.dox 

[1.x.1712] 

[1.x.1713] 

[1.x.1714] 


When running the program, you should get output like the following: 

[1.x.1715] 



The results are easily visualized: 

 [2.x.4392]  

The plots are easily interpreted: as the flow drives down on the left side and up on the right side of the upright part of the solid, it produces a pressure that is high on the left and low on the right, and these forces bend the vertical part of the solid to the right. 




[1.x.1716] 

By changing the dimension of the  [2.x.4393]  class in  [2.x.4394]  to 3, we can also run the same problem 3d. You'd get output along the following lines: 

[1.x.1717] 

You'll notice that the big bottleneck is the solver: SparseDirectUmfpack needs nearly 5 hours and some 80 GB of memory to solve the last iteration of this problem on a 2016 workstation (the second to last iteration took only 16 minutes). Clearly a better solver is needed here, a topic discussed below. 

The results can also be visualized and yield good pictures as well. Here is one, showing both a vector plot for the velocity (in oranges), the solid displacement (in blues), and shading the solid region: 

<p align="center">    [2.x.4395]   [2.x.4396]  

In addition to the lack of a good solver, the mesh is a bit unbalanced: mesh refinement heavily favors the fluid subdomain (in 2d, it was the other way around, prompting us to weigh the fluid error indicators higher). Clearly, some tweaking of the relative importance of error indicators in the two subdomains is important if one wanted to go on doing more 3d computations. 


[1.x.1718] 

[1.x.1719] 

[1.x.1720] 

An obvious place to improve the program would be to use a more sophisticated solver &mdash; in particular one that scales well and will also work for realistic 3d problems. This shouldn't actually be too hard to achieve here, because of the one-way coupling from fluid into solid. To this end, assume we had re-ordered degrees of freedom in such a way that we first have all velocity and pressure degrees of freedom, and then all displacements (this is easily possible using  [2.x.4397]  Then the system matrix could be split into the following block form: 

[1.x.1721] 

where  [2.x.4398]  is the Stokes matrix for velocity and pressure (it could be further subdivided into a  [2.x.4399]  matrix as in step-22, though this is immaterial for the current purpose),  [2.x.4400]  results from the elasticity equations for the displacements, and  [2.x.4401]  is the matrix that comes from the interface conditions. Now notice that the matrix 

[1.x.1722] 

is the inverse of  [2.x.4402] . Applying this matrix requires only one solve with  [2.x.4403]  and  [2.x.4404]  each since 

[1.x.1723] 

can be computed as  [2.x.4405]  followed by  [2.x.4406] . 

One can therefore expect that 

[1.x.1724] 

would be a good preconditioner if  [2.x.4407] . 

That means, we only need good preconditioners for Stokes and the elasticity equations separately. These are well known: for Stokes, we can use the preconditioner discussed in the results section of step-22; for elasticity, a good preconditioner would be a single V-cycle of a geometric or algebraic multigrid. There are more open questions, however: For an "optimized" solver block-triangular preconditioner built from two sub-preconditioners, one point that often comes up is that, when choosing parameters for the sub-preconditioners, values that work well when solving the two problems separately may not be optimal when combined into a multiphysics preconditioner.  In particular, when solving just a solid or fluid mechanics problem separately, the balancing act between the number of iterations to convergence and the cost of applying the preconditioner on a per iteration basis may lead one to choose an expensive preconditioner for the Stokes problem and a cheap preconditioner for the elasticity problem (or vice versa).  When combined, however, there is the additional constraint that you want the two sub-preconditioners to converge at roughly the same rate, or else the cheap one may drive up the global number of iterations while the expensive one drives up the cost-per-iteration. For example, while a single AMG V-cycle is a good approach for elasticity by itself, when combined into a multiphysics problem there may be an incentive to using a full W-cycle or multiple cycles to help drive down the total solve time. 




[1.x.1725] 

As mentioned in the introduction, the refinement indicator we use for this program is rather ad hoc. A better one would understand that the jump in the gradient of the solution across the interface is not indicative of the error but to be expected and ignore the interface when integrating the jump terms. Nevertheless, this is not what the KellyErrorEstimator class does. Another, bigger question, is whether this kind of estimator is a good strategy in the first place: for example, if we want to have maximal accuracy in one particular aspect of the displacement (e.g. the displacement at the top right corner of the solid), then is it appropriate to scale the error indicators for fluid and solid to the same magnitude? Maybe it is necessary to solve the fluid problem with more accuracy than the solid because the fluid solution directly affects the solids solution? Maybe the other way around? 

Consequently, an obvious possibility for improving the program would be to implement a better refinement criterion. There is some literature on this topic; one of a variety of possible starting points would be the paper by Thomas Wick on "Adaptive finite elements for monolithic fluid-structure interaction on a prolongated domain: Applied to an heart valve simulation", Proceedings of the Computer Methods in Mechanics Conference 2011 (CMM-2011), 9-12 May 2011, Warszaw, Poland. 




[1.x.1726] 

The results above are purely qualitative as there is no evidence that our scheme in fact converges. An obvious thing to do would therefore be to add some quantitative measures to check that the scheme at least converges to [1.x.1727]. For example, we could output for each refinement cycle the deflection of the top right corner of the part of the solid that protrudes into the fluid subdomain. Or we could compute the net force vector or torque the fluid exerts on the solid. 




[1.x.1728] 

In reality, most fluid structure interaction problems are so that the movement of the solid does affect the flow of the fluid. For example, the forces of the air around an air foil cause it to flex and to change its shape. Likewise, a flag flaps in the wind, completely changing its shape. 

Such problems where the coupling goes both ways are typically handled in an Arbitrary Lagrangian Eulerian (ALE) framework, in which the displacement of the solid is extended into the fluid domain in some smooth way, rather than by zero as we do here. The extended displacement field is then used to deform the mesh on which we compute the fluid flow. Furthermore, the boundary conditions for the fluid on the interface are no longer that the velocity is zero; rather, in a time dependent program, the fluid velocity must be equal to the time derivative of the displacement along the interface. 


examples/step-47/doc/intro.dox 

 [2.x.4408]  

[1.x.1729] 

[1.x.1730] 

[1.x.1731] 

This program deals with the [1.x.1732], 

[1.x.1733] 

This equation appears in the modeling of thin structures such as roofs of stadiums. These objects are of course in reality three-dimensional with a large aspect ratio of lateral extent to perpendicular thickness, but one can often very accurately model these structures as two dimensional by making assumptions about how internal forces vary in the perpendicular direction. These assumptions lead to the equation above. 

The model typically comes in two different kinds, depending on what kinds of boundary conditions are imposed. The first case, 

[1.x.1734] 

corresponds to the edges of the thin structure attached to the top of a wall of height  [2.x.4409]  in such a way that the bending forces that act on the structure are  [2.x.4410] ; in most physical situations, one will have  [2.x.4411] , corresponding to the structure simply sitting atop the wall. 

In the second possible case of boundary values, one would have 

[1.x.1735] 

This corresponds to a "clamped" structure for which a nonzero  [2.x.4412]  implies a certain angle against the horizontal. 

As with Dirichlet and Neumann boundary conditions for the Laplace equation, it is of course possible to have one kind of boundary conditions on one part of the boundary, and the other on the remainder. 




[1.x.1736] 

The fundamental issue with the equation is that it takes four derivatives of the solution. In the case of the Laplace equation we treated in step-3, step-4, and several other tutorial programs, one multiplies by a test function, integrates, integrates by parts, and ends up with only one derivative on both the test function and trial function -- something one can do with functions that are continuous globally, but may have kinks at the interfaces between cells: The derivative may not be defined at the interfaces, but that is on a lower-dimensional manifold (and so doesn't show up in the integrated value). 

But for the biharmonic equation, if one followed the same procedure using integrals over the entire domain (i.e., the union of all cells), one would end up with two derivatives on the test functions and trial functions each. If one were to use the usual piecewise polynomial functions with their kinks on cell interfaces, the first derivative would yield a discontinuous gradient, and the second derivative with delta functions on the interfaces -- but because both the second derivatives of the test functions and of the trial functions yield a delta function, we would try to integrate the product of two delta functions. For example, in 1d, where  [2.x.4413]  are the usual piecewise linear "hat functions", we would get integrals of the sort 

[1.x.1737] 

where  [2.x.4414]  is the node location at which the shape function  [2.x.4415]  is defined, and  [2.x.4416]  is the mesh size (assumed uniform). The problem is that delta functions in integrals are defined using the relationship 

[1.x.1738] 

But that only works if (i)  [2.x.4417]  is actually well defined at  [2.x.4418] , and (ii) if it is finite. On the other hand, an integral of the form 

[1.x.1739] 

does not make sense. Similar reasoning can be applied for 2d and 3d situations. 

In other words: This approach of trying to integrate over the entire domain and then integrating by parts can't work. 

Historically, numerical analysts have tried to address this by inventing finite elements that are "C<sup>1</sup> continuous", i.e., that use shape functions that are not just continuous but also have continuous first derivatives. This is the realm of elements such as the Argyris element, the Clough-Tocher element and others, all developed in the late 1960s. From a twenty-first century perspective, they can only be described as bizarre in their construction. They are also exceedingly cumbersome to implement if one wants to use general meshes. As a consequence, they have largely fallen out of favor and deal.II currently does not contain implementations of these shape functions. 




[1.x.1740] 

So how does one approach solving such problems then? That depends a bit on the boundary conditions. If one has the first set of boundary conditions, i.e., if the equation is 

[1.x.1741] 

then the following trick works (at least if the domain is convex, see below): In the same way as we obtained the mixed Laplace equation of step-20 from the regular Laplace equation by introducing a second variable, we can here introduce a variable  [2.x.4419]  and can then replace the equations above by the following, "mixed" system: 

[1.x.1742] 

In other words, we end up with what is in essence a system of two coupled Laplace equations for  [2.x.4420] , each with Dirichlet-type boundary conditions. We know how to solve such problems, and it should not be very difficult to construct good solvers and preconditioners for this system either using the techniques of step-20 or step-22. So this case is pretty simple to deal with. 

 [2.x.4421]  It is worth pointing out that this only works for domains whose   boundary has corners if the domain is also convex -- in other words,   if there are no re-entrant corners.   This sounds like a rather random condition, but it makes   sense in view of the following two facts: The solution of the   original biharmonic equation must satisfy  [2.x.4422] . On the   other hand, the mixed system reformulation above suggests that both    [2.x.4423]  and  [2.x.4424]  satisfy  [2.x.4425]  because both variables only   solve a Poisson equation. In other words, if we want to ensure that   the solution  [2.x.4426]  of the mixed problem is also a solution of the   original biharmonic equation, then we need to be able to somehow   guarantee that the solution of  [2.x.4427]  is in fact more smooth   than just  [2.x.4428] . This can be argued as follows: For convex   domains,   [1.x.1743] implies that if the right hand side  [2.x.4429] , then    [2.x.4430]  if the domain is convex and the boundary is smooth   enough. (This could also be guaranteed if the domain boundary is   sufficiently smooth -- but domains whose boundaries have no corners   are not very practical in real life.)   We know that  [2.x.4431]  because it solves the equation    [2.x.4432] , but we are still left with the condition on convexity   of the boundary; one can show that polygonal, convex domains are   good enough to guarantee that  [2.x.4433]  in this case (smoothly   bounded, convex domains would result in  [2.x.4434] , but we don't   need this much regularity). On the other hand, if the domain is not   convex, we can not guarantee that the solution of the mixed system   is in  [2.x.4435] , and consequently may obtain a solution that can't be   equal to the solution of the original biharmonic equation. 

The more complicated situation is if we have the "clamped" boundary conditions, i.e., if the equation looks like this: 

[1.x.1744] 

The same trick with the mixed system does not work here, because we would end up with [1.x.1745] Dirichlet and Neumann boundary conditions for  [2.x.4436] , but none for  [2.x.4437] . 


The solution to this conundrum arrived with the Discontinuous Galerkin method wave in the 1990s and early 2000s: In much the same way as one can use [1.x.1746] shape functions for the Laplace equation by penalizing the size of the discontinuity to obtain a scheme for an equation that has one derivative on each shape function, we can use a scheme that uses [1.x.1747] (but not  [2.x.4438]  continuous) shape functions and penalize the jump in the derivative to obtain a scheme for an equation that has two derivatives on each shape function. In analogy to the Interior Penalty (IP) method for the Laplace equation, this scheme for the biharmonic equation is typically called the  [2.x.4439]  IP (or C0IP) method, since it uses  [2.x.4440]  (continuous but not continuously differentiable) shape functions with an interior penalty formulation. 




[1.x.1748] 

We base this program on the  [2.x.4441]  IP method presented by Susanne Brenner and Li-Yeng Sung in the paper "C [2.x.4442]  Interior Penalty Method for Linear Fourth Order Boundary Value Problems on polygonal domains''  [2.x.4443]  , where the method is derived for the biharmonic equation with "clamped" boundary conditions. 

As mentioned, this method relies on the use of  [2.x.4444]  Lagrange finite elements where the  [2.x.4445]  continuity requirement is relaxed and has been replaced with interior penalty techniques. To derive this method, we consider a  [2.x.4446]  shape function  [2.x.4447]  which vanishes on  [2.x.4448] . We introduce notation  [2.x.4449]  as the set of all faces of  [2.x.4450] ,  [2.x.4451]  as the set of boundary faces, and  [2.x.4452]  as the set of interior faces for use further down below. Since the higher order derivatives of  [2.x.4453]  have two values on each interface  [2.x.4454]  (shared by the two cells  [2.x.4455] ), we cope with this discontinuity by defining the following single-valued functions on  [2.x.4456] : 

[1.x.1749] 

for  [2.x.4457]  (i.e., for the gradient and the matrix of second derivatives), and where  [2.x.4458]  denotes a unit vector normal to  [2.x.4459]  pointing from  [2.x.4460]  to  [2.x.4461] . In the literature, these functions are referred to as the "jump" and "average" operations, respectively. 

To obtain the  [2.x.4462]  IP approximation  [2.x.4463] , we left multiply the biharmonic equation by  [2.x.4464] , and then integrate over  [2.x.4465] . As explained above, we can't do the integration by parts on all of  [2.x.4466]  with these shape functions, but we can do it on each cell individually since the shape functions are just polynomials on each cell. Consequently, we start by using the following integration-by-parts formula on each mesh cell  [2.x.4467] : 

[1.x.1750] 

At this point, we have two options: We can integrate the domain term's  [2.x.4468]  one more time to obtain 

[1.x.1751] 

For a variety of reasons, this turns out to be a variation that is not useful for our purposes. 

Instead, what we do is recognize that  [2.x.4469] , and we can re-sort these operations as  [2.x.4470]  where we typically write  [2.x.4471]  to indicate that this is the "Hessian" matrix of second derivatives. With this re-ordering, we can now integrate the divergence, rather than the gradient operator, and we get the following instead: 

[1.x.1752] 

Here, the colon indicates a double-contraction over the indices of the matrices to its left and right, i.e., the scalar product between two tensors. The outer product of two vectors  [2.x.4472]  yields the matrix  [2.x.4473] . 

Then, we sum over all cells  [2.x.4474] , and take into account that this means that every interior face appears twice in the sum. If we therefore split everything into a sum of integrals over cell interiors and a separate sum over cell interfaces, we can use the jump and average operators defined above. There are two steps left: First, because our shape functions are continuous, the gradients of the shape functions may be discontinuous, but the continuity guarantees that really only the normal component of the gradient is discontinuous across faces whereas the tangential component(s) are continuous. Second, the discrete formulation that results is not stable as the mesh size goes to zero, and to obtain a stable formulation that converges to the correct solution, we need to add the following terms: 

[1.x.1753] 

Then, after making cancellations that arise, we arrive at the following C0IP formulation of the biharmonic equation: find  [2.x.4475]  such that  [2.x.4476]  on  [2.x.4477]  and 

[1.x.1754] 

where 

[1.x.1755] 

and 

[1.x.1756] 

Here,  [2.x.4478]  is the penalty parameter which both weakly enforces the boundary condition 

[1.x.1757] 

on the boundary interfaces  [2.x.4479] , and also ensures that in the limit  [2.x.4480] ,  [2.x.4481]  converges to a  [2.x.4482]  continuous function.  [2.x.4483]  is chosen to be large enough to guarantee the stability of the method. We will discuss our choice in the program below. 




[1.x.1758] On polygonal domains, the weak solution  [2.x.4484]  to the biharmonic equation lives in  [2.x.4485]  where  [2.x.4486]  is determined by the interior angles at the corners of  [2.x.4487] . For instance, whenever  [2.x.4488]  is convex,  [2.x.4489] ;  [2.x.4490]  may be less than one if the domain has re-entrant corners but  [2.x.4491]  is close to  [2.x.4492]  if one of all interior angles is close to  [2.x.4493] . 

Now suppose that the  [2.x.4494]  IP solution  [2.x.4495]  is approximated by  [2.x.4496]  shape functions with polynomial degree  [2.x.4497] . Then the discretization outlined above yields the convergence rates as discussed below. 


[1.x.1759] 

Ideally, we would like to measure convergence in the "energy norm"  [2.x.4498] . However, this does not work because, again, the discrete solution  [2.x.4499]  does not have two (weak) derivatives. Instead, one can define a discrete ( [2.x.4500]  IP) seminorm that is "equivalent" to the energy norm, as follows: 

[1.x.1760] 



In this seminorm, the theory in the paper mentioned above yields that we can expect 

[1.x.1761] 

much as one would expect given the convergence rates we know are true for the usual discretizations of the Laplace equation. 

Of course, this is true only if the exact solution is sufficiently smooth. Indeed, if  [2.x.4501]  with  [2.x.4502] ,  [2.x.4503]  where  [2.x.4504] , then the convergence rate of the  [2.x.4505]  IP method is  [2.x.4506] . In other words, the optimal convergence rate can only be expected if the solution is so smooth that  [2.x.4507] ; this can only happen if (i) the domain is convex with a sufficiently smooth boundary, and (ii)  [2.x.4508] . In practice, of course, the solution is what it is (independent of the polynomial degree we choose), and the last condition can then equivalently be read as saying that there is definitely no point in choosing  [2.x.4509]  large if  [2.x.4510]  is not also large. In other words, the only reasonably choices for  [2.x.4511]  are  [2.x.4512]  because larger polynomial degrees do not result in higher convergence orders. 

For the purposes of this program, we're a bit too lazy to actually implement this equivalent seminorm -- though it's not very difficult and would make for a good exercise. Instead, we'll simply check in the program what the "broken"  [2.x.4513]  seminorm 

[1.x.1762] 

yields. The convergence rate in this norm can, from a theoretical perspective, of course not be [1.x.1763] than the one for  [2.x.4514]  because it contains only a subset of the necessary terms, but it could at least conceivably be better. It could also be the case that we get the optimal convergence rate even though there is a bug in the program, and that that bug would only show up in sub-optimal rates for the additional terms present in  [2.x.4515] . But, one might hope that if we get the optimal rate in the broken norm and the norms discussed below, then the program is indeed correct. The results section will demonstrate that we obtain optimal rates in all norms shown. 


[1.x.1764] 

The optimal convergence rate in the  [2.x.4516] -norm is  [2.x.4517]  provided  [2.x.4518] . More details can be found in Theorem 4.6 of  [2.x.4519]  . 

The default in the program below is to choose  [2.x.4520] . In that case, the theorem does not apply, and indeed one only gets  [2.x.4521]  instead of  [2.x.4522]  as we will show in the results section. 


[1.x.1765] 

Given that we expect  [2.x.4523]  in the best of cases for a norm equivalent to the  [2.x.4524]  seminorm, and  [2.x.4525]  for the  [2.x.4526]  norm, one may ask about what happens in the  [2.x.4527]  seminorm that is intermediate to the two others. A reasonable guess is that one should expect  [2.x.4528] . There is probably a paper somewhere that proves this, but we also verify that this conjecture is experimentally true below. 




[1.x.1766] 

We remark that the derivation of the  [2.x.4529]  IP method for the biharmonic equation with other boundary conditions -- for instance, for the first set of boundary conditions namely  [2.x.4530]  and  [2.x.4531]  on  [2.x.4532]  -- can be obtained with suitable modifications to  [2.x.4533]  and  [2.x.4534]  described in the book chapter  [2.x.4535]  . 




[1.x.1767] 

The last step that remains to describe is what this program solves for. As always, a trigonometric function is both a good and a bad choice because it does not lie in any polynomial space in which we may seek the solution while at the same time being smoother than real solutions typically are (here, it is in  [2.x.4536]  while real solutions are typically only in  [2.x.4537]  or so on convex polygonal domains, or somewhere between  [2.x.4538]  and  [2.x.4539]  if the domain is not convex). But, since we don't have the means to describe solutions of realistic problems in terms of relatively simple formulas, we just go with the following, on the unit square for the domain  [2.x.4540] : 

[1.x.1768] 

As a consequence, we then need choose as boundary conditions the following: 

[1.x.1769] 

The right hand side is easily computed as 

[1.x.1770] 

The program has classes  [2.x.4541]  and  [2.x.4542]  that encode this information. 


examples/step-47/doc/results.dox 



[1.x.1771] 

We run the program with right hand side and boundary values as discussed in the introduction. These will produce the solution  [2.x.4543]  on the domain  [2.x.4544] . We test this setup using  [2.x.4545] ,  [2.x.4546] , and  [2.x.4547]  elements, which one can change via the `fe_degree` variable in the `main()` function. With mesh refinement, the  [2.x.4548]  convergence rates,  [2.x.4549] -seminorm rate, and  [2.x.4550] -seminorm convergence of  [2.x.4551]  should then be around 2, 2, 1 for  [2.x.4552]  (with the  [2.x.4553]  norm sub-optimal as discussed in the introduction); 4, 3, 2 for  [2.x.4554] ; and 5, 4, 3 for  [2.x.4555] , respectively. 

From the literature, it is not immediately clear what the penalty parameter  [2.x.4556]  should be. For example,  [2.x.4557]  state that it needs to be larger than one, and choose  [2.x.4558] . The FEniCS/Dolphin tutorial chooses it as  [2.x.4559] , see https://fenicsproject.org/docs/dolfin/1.6.0/python/demo/documented/biharmonic/python/documentation.html .  [2.x.4560]  uses a value for  [2.x.4561]  larger than the number of edges belonging to an element for Kirchhoff plates (see their Section 4.2). This suggests that maybe  [2.x.4562] ,  [2.x.4563] , are too small; on the other hand, a value  [2.x.4564]  would be reasonable, where  [2.x.4565]  is the degree of polynomials. The last of these choices is the one one would expect to work by comparing to the discontinuous Galerkin formulations for the Laplace equation (see, for example, the discussions in step-39 and step-74), and it will turn out to also work here. But we should check what value of  [2.x.4566]  is right, and we will do so below; changing  [2.x.4567]  is easy in the two `face_worker` and `boundary_worker` functions defined in `assemble_system()`. 




[1.x.1772][1.x.1773][1.x.1774] 

We run the code with differently refined meshes and get the following convergence rates. 

 [2.x.4568]  We can see that the  [2.x.4569]  convergence rates are around 2,  [2.x.4570] -seminorm convergence rates are around 2, and  [2.x.4571] -seminorm convergence rates are around 1. The latter two match the theoretically expected rates; for the former, we have no theorem but are not surprised that it is sub-optimal given the remark in the introduction. 




[1.x.1775][1.x.1776][1.x.1777] 


 [2.x.4572]  We can see that the  [2.x.4573]  convergence rates are around 4,  [2.x.4574] -seminorm convergence rates are around 3, and  [2.x.4575] -seminorm convergence rates are around 2. This, of course, matches our theoretical expectations. 




[1.x.1778][1.x.1779][1.x.1780] 

 [2.x.4576]  We can see that the  [2.x.4577]  norm convergence rates are around 5,  [2.x.4578] -seminorm convergence rates are around 4, and  [2.x.4579] -seminorm convergence rates are around 3. On the finest mesh, the  [2.x.4580]  norm convergence rate is much smaller than our theoretical expectations because the linear solver becomes the limiting factor due to round-off. Of course the  [2.x.4581]  error is also very small already in that case. 




[1.x.1781][1.x.1782][1.x.1783] 

For comparison with the results above, let us now also consider the case where we simply choose  [2.x.4582] : 

 [2.x.4583]  Although  [2.x.4584]  norm convergence rates of  [2.x.4585]  more or less follows the theoretical expectations, the  [2.x.4586] -seminorm and  [2.x.4587] -seminorm do not seem to converge as expected. Comparing results from  [2.x.4588]  and  [2.x.4589] , it is clear that  [2.x.4590]  is a better penalty. Given that  [2.x.4591]  is already too small for  [2.x.4592]  elements, it may not be surprising that if one repeated the experiment with a  [2.x.4593]  element, the results are even more disappointing: One again only obtains convergence rates of 2, 1, zero -- i.e., no better than for the  [2.x.4594]  element (although the errors are smaller in magnitude). Maybe surprisingly, however, one obtains more or less the expected convergence orders when using  [2.x.4595]  elements. Regardless, this uncertainty suggests that  [2.x.4596]  is at best a risky choice, and at worst an unreliable one and that we should choose  [2.x.4597]  larger. 




[1.x.1784][1.x.1785][1.x.1786] 

Since  [2.x.4598]  is clearly too small, one might conjecture that  [2.x.4599]  might actually work better. Here is what one obtains in that case: 

 [2.x.4600]  In this case, the convergence rates more or less follow the theoretical expectations, but, compared to the results from  [2.x.4601] , are more variable. Again, we could repeat this kind of experiment for  [2.x.4602]  and  [2.x.4603]  elements. In both cases, we will find that we obtain roughly the expected convergence rates. Of more interest may then be to compare the absolute size of the errors. While in the table above, for the  [2.x.4604]  case, the errors on the finest grid are comparable between the  [2.x.4605]  and  [2.x.4606]  case, for  [2.x.4607]  the errors are substantially larger for  [2.x.4608]  than for  [2.x.4609] . The same is true for the  [2.x.4610]  case. 




[1.x.1787] 

The conclusions for which of the "reasonable" choices one should use for the penalty parameter is that  [2.x.4611]  yields the expected results. It is, consequently, what the code uses as currently written. 




[1.x.1788] 

There are a number of obvious extensions to this program that would make sense: 

- The program uses a square domain and a uniform mesh. Real problems   don't come this way, and one should verify convergence also on   domains with other shapes and, in particular, curved boundaries. One   may also be interested in resolving areas of less regularity by   using adaptive mesh refinement. 

- From a more theoretical perspective, the convergence results above   only used the "broken"  [2.x.4612]  seminorm  [2.x.4613]  instead   of the "equivalent" norm  [2.x.4614] . This is good enough to   convince ourselves that the program isn't fundamentally   broken. However, it might be interesting to measure the error in the   actual norm for which we have theoretical results. Implementing this   addition should not be overly difficult using, for example, the   FEInterfaceValues class combined with  [2.x.4615]  in the   same spirit as we used for the assembly of the linear system. 


  [1.x.1789] 

  Similar to the "clamped" boundary condition addressed in the implementation,   we will derive the  [2.x.4616]  IP finite element scheme for simply supported plates:   [1.x.1790] 

  We multiply the biharmonic equation by the test function  [2.x.4617]  and integrate over  [2.x.4618]  and get:   [1.x.1791] 



  Summing up over all cells  [2.x.4619] ,since normal directions of  [2.x.4620]  are pointing at   opposite directions on each interior edge shared by two cells and  [2.x.4621]  on  [2.x.4622] ,   [1.x.1792] 

  and by the definition of jump over cell interfaces,   [1.x.1793] 

  We separate interior faces and boundary faces of the domain,   [1.x.1794] 

  where  [2.x.4623]  is the set of interior faces.   This leads us to   [1.x.1795] 



  In order to symmetrize and stabilize the discrete problem,   we add symmetrization and stabilization term.   We finally get the  [2.x.4624]  IP finite element scheme for the biharmonic equation:   find  [2.x.4625]  such that  [2.x.4626]  on  [2.x.4627]  and   [1.x.1796] 

  where   [1.x.1797] 

  and   [1.x.1798] 

  The implementation of this boundary case is similar to the "clamped" version   except that `boundary_worker` is no longer needed for system assembling   and the right hand side is changed according to the formulation. 


examples/step-48/doc/intro.dox 



[1.x.1799][1.x.1800] 

[1.x.1801] 

[1.x.1802] 

This program demonstrates how to use the cell-based implementation of finite element operators with the MatrixFree class, first introduced in step-37, to solve nonlinear partial differential equations. Moreover, we have another look at the handling of constraints within the matrix-free framework. Finally, we will use an explicit time-stepping method to solve the problem and introduce Gauss-Lobatto finite elements that are very convenient in this case since their mass matrix can be accurately approximated by a diagonal, and thus trivially invertible, matrix. The two ingredients to this property are firstly a distribution of the nodal points of Lagrange polynomials according to the point distribution of the Gauss-Lobatto quadrature rule. Secondly, the quadrature is done with the same Gauss-Lobatto quadrature rule. In this formula, the integrals  [2.x.4628]  become zero whenever  [2.x.4629] , because exactly one function  [2.x.4630]  is one and all others zero in the points defining the Lagrange polynomials. Moreover, the Gauss-Lobatto distribution of nodes of Lagrange polynomials clusters the nodes towards the element boundaries. This results in a well-conditioned polynomial basis for high-order discretization methods. Indeed, the condition number of an FE_Q elements with equidistant nodes grows exponentially with the degree, which destroys any benefit for orders of about five and higher. For this reason, Gauss-Lobatto points are the default distribution for the FE_Q element (but at degrees one and two, those are equivalent to the equidistant points). 

[1.x.1803] 

As an example, we choose to solve the sine-Gordon soliton equation 

[1.x.1804] 

that was already introduced in step-25. As a simple explicit time integration method, we choose leap frog scheme using the second-order formulation of the equation. With this time stepping, the scheme reads in weak form 

[1.x.1805] where [1.x.1806] denotes a test function and the index [1.x.1807] stands for the time step number. 

For the spatial discretization, we choose FE_Q elements with basis functions defined to interpolate the support points of the Gauss-Lobatto quadrature rule. Moreover, when we compute the integrals over the basis functions to form the mass matrix and the operator on the right hand side of the equation above, we use the Gauss-Lobatto quadrature rule with the same support points as the node points of the finite element to evaluate the integrals. Since the finite element is Lagrangian, this will yield a diagonal mass matrix on the left hand side of the equation, making the solution of the linear system in each time step trivial. 

Using this quadrature rule, for a [1.x.1808]th order finite element, we use a [1.x.1809]th order accurate formula to evaluate the integrals. Since the product of two [1.x.1810]th order basis functions when computing a mass matrix gives a function with polynomial degree [1.x.1811] in each direction, the integrals are not computed exactly.  However, the overall convergence properties are not disturbed by the quadrature error on meshes with affine element shapes with L2 errors proportional to [1.x.1812]. Note however that order reduction with sub-optimal convergence rates of the L2 error of [1.x.1813] or even [1.x.1814] for some 3D setups has been reported [1.x.1815] on deformed (non-affine) element shapes for wave equations when the integrand is not a polynomial any more. 

Apart from the fact that we avoid solving linear systems with this type of elements when using explicit time-stepping, they come with two other advantages. When we are using the sum-factorization approach to evaluate the finite element operator (cf. step-37), we have to evaluate the function at the quadrature points. In the case of Gauss-Lobatto elements, where quadrature points and node points of the finite element coincide, this operation is trivial since the value of the function at the quadrature points is given by its one-dimensional coefficients. In this way, the arithmetic work for the finite element operator evaluation is reduced by approximately a factor of two compared to the generic Gaussian quadrature. 

To sum up the discussion, by using the right finite element and quadrature rule combination, we end up with a scheme where we only need to compute the right hand side vector corresponding to the formulation above and then multiply it by the inverse of the diagonal mass matrix in each time step. In practice, of course, we extract the diagonal elements and invert them only once at the beginning of the program. 

[1.x.1816] 

The usual way to handle constraints in  [2.x.4631]  is to use the AffineConstraints class that builds a sparse matrix storing information about which degrees of freedom (DoF) are constrained and how they are constrained. This format uses an unnecessarily large amount of memory since there are not so many different types of constraints: for example, in the case of hanging nodes when using linear finite element on every cell, most constraints have the form  [2.x.4632]  where the coefficients  [2.x.4633]  are always the same and only  [2.x.4634]  are different. While storing this redundant information is not a problem in general because it is only needed once during matrix and right hand side assembly, it becomes a bottleneck in the matrix-free approach since there this information has to be accessed every time we apply the operator, and the remaining components of the operator evaluation are so fast. Thus, instead of an AffineConstraints object, MatrixFree uses a variable that we call  [2.x.4635]  that collects the weights of the different constraints. Then, only an identifier of each constraint in the mesh instead of all the weights have to be stored. Moreover, the constraints are not applied in a pre- and postprocessing step but rather as we evaluate the finite element operator. Therefore, the constraint information is embedded into the variable  [2.x.4636]  that is used to extract the cell information from the global vector. If a DoF is constrained, the  [2.x.4637]  variable contains the global indices of the DoFs that it is constrained to. Then, we have another variable  [2.x.4638]  at hand that holds, for each cell, the local indices of DoFs that are constrained as well as the identifier of the type of constraint. Fortunately, you will not see these data structures in the example program since the class  [2.x.4639]  takes care of the constraints without user interaction. 

In the presence of hanging nodes, the diagonal mass matrix obtained on the element level via the Gauss-Lobatto quadrature/node point procedure does not directly translate to a diagonal global mass matrix, as following the constraints on rows and columns would also add off-diagonal entries. As explained in [1.x.1817], interpolating constraints on a vector, which maintains the diagonal shape of the mass matrix, is consistent with the equations up to an error of the same magnitude as the quadrature error. In the program below, we will simply assemble the diagonal of the mass matrix as if it were a vector to enable this approximation. 




[1.x.1818] 

The MatrixFree class comes with the option to be parallelized on three levels: MPI parallelization on clusters of distributed nodes, thread parallelization scheduled by the Threading Building Blocks library, and finally with a vectorization by working on a batch of two (or more) cells via SIMD data type (sometimes called cross-element or external vectorization). As we have already discussed in step-37, you will get best performance by using an instruction set specific to your system, e.g. with the cmake variable <tt>-DCMAKE_CXX_FLAGS="-march=native"</tt>. The MPI parallelization was already exploited in step-37. Here, we additionally consider thread parallelization with TBB. This is fairly simple, as all we need to do is to tell the initialization of the MatrixFree object about the fact that we want to use a thread parallel scheme through the variable  [2.x.4640]  During setup, a dependency graph is set up similar to the one described in the  [2.x.4641]  , which allows to schedule the work of the  [2.x.4642]  function on chunks of cells without several threads accessing the same vector indices. As opposed to the WorkStream loops, some additional clever tricks to avoid global synchronizations as described in [1.x.1819] are also applied. 

Note that this program is designed to be run with a distributed triangulation  [2.x.4643]  which requires deal.II to be configured with [1.x.1820] as described in the [1.x.1821] file. However, a non-distributed triangulation is also supported, in which case the computation will be run in serial. 

[1.x.1822] 

In our example, we choose the initial value to be [1.x.1823] and solve the equation over the time interval [-10,10]. The constants are chosen to be  [2.x.4644]  and [1.x.1824]. As mentioned in step-25, in one dimension [1.x.1825] as a function of [1.x.1826] is the exact solution of the sine-Gordon equation. For higher dimension, this is however not the case. 


examples/step-48/doc/results.dox 



[1.x.1827] 

[1.x.1828] 

In order to demonstrate the gain in using the MatrixFree class instead of the standard  [2.x.4645]  assembly routines for evaluating the information from old time steps, we study a simple serial run of the code on a nonadaptive mesh. Since much time is spent on evaluating the sine function, we do not only show the numbers of the full sine-Gordon equation but also for the wave equation (the sine-term skipped from the sine-Gordon equation). We use both second and fourth order elements. The results are summarized in the following table. 

 [2.x.4646]  

It is apparent that the matrix-free code outperforms the standard assembly routines in deal.II by far. In 3D and for fourth order elements, one operator evaluation is also almost ten times as fast as a sparse matrix-vector product. 

[1.x.1829] 

We start with the program output obtained on a workstation with 12 cores / 24 threads (one Intel Xeon E5-2687W v4 CPU running at 3.2 GHz, hyperthreading enabled), running the program in release mode: 

[1.x.1830] 



In 3D, the respective output looks like 

[1.x.1831] 



It takes 0.008 seconds for one time step with more than a million degrees of freedom (note that we would need many processors to reach such numbers when solving linear systems). 

If we replace the thread-parallelization by a pure MPI parallelization, the timings change into: 

[1.x.1832] 



We observe a dramatic speedup for the output (which makes sense, given that most code of the output is not parallelized via threads, whereas it is for MPI), but less than the theoretical factor of 12 we would expect from the parallelism. More interestingly, the computations also get faster when switching from the threads-only variant to the MPI-only variant. This is a general observation for the MatrixFree framework (as of updating this data in 2019). The main reason is that the decisions regarding work on conflicting cell batches made to enable execution in parallel are overly pessimistic: While they ensure that no work on neighboring cells is done on different threads at the same time, this conservative setting implies that data from neighboring cells is also evicted from caches by the time neighbors get touched. Furthermore, the current scheme is not able to provide a constant load for all 24 threads for the given mesh with 17,592 cells. 

The current program allows to also mix MPI parallelization with thread parallelization. This is most beneficial when running programs on clusters with multiple nodes, using MPI for the inter-node parallelization and threads for the intra-node parallelization. On the workstation used above, we can run threads in the hyperthreading region (i.e., using 2 threads for each of the 12 MPI ranks). An important setting for mixing MPI with threads is to ensure proper binning of tasks to CPUs. On many clusters the placing is either automatically via the `mpirun/mpiexec` environment, or there can be manual settings. Here, we simply report the run times the plain version of the program (noting that things could be improved towards the timings of the MPI-only program when proper pinning is done): 

[1.x.1833] 






[1.x.1834] 

There are several things in this program that could be improved to make it even more efficient (besides improved boundary conditions and physical stuff as discussed in step-25): 

 [2.x.4647]   [2.x.4648]  [1.x.1835] As becomes obvious   from the comparison of the plain wave equation and the sine-Gordon   equation above, the evaluation of the sine terms dominates the total   time for the finite element operator application. There are a few   reasons for this: Firstly, the deal.II sine computation of a   VectorizedArray field is not vectorized (as opposed to the rest of   the operator application). This could be cured by handing the sine   computation to a library with vectorized sine computations like   Intel's math kernel library (MKL). By using the function    [2.x.4649]  in MKL, the program uses half the computing time   in 2D and 40 percent less time in 3D. On the other hand, the sine   computation is structurally much more complicated than the simple   arithmetic operations like additions and multiplications in the rest   of the local operation. 

   [2.x.4650]  [1.x.1836] While the implementation allows for   arbitrary order in the spatial part (by adjusting the degree of the finite   element), the time stepping scheme is a standard second-order leap-frog   scheme. Since solutions in wave propagation problems are usually very   smooth, the error is likely dominated by the time stepping part. Of course,   this could be cured by using smaller time steps (at a fixed spatial   resolution), but it would be more efficient to use higher order time   stepping as well. While it would be straight-forward to do so for a   first-order system (use some Runge&ndash;Kutta scheme of higher order,   probably combined with adaptive time step selection like the [1.x.1837]), it is more challenging for the second-order formulation. At   least in the finite difference community, people usually use the PDE to find   spatial correction terms that improve the temporal error. 

 [2.x.4651]  


examples/step-49/doc/intro.dox 

[1.x.1838] 

[1.x.1839] 

[1.x.1840] This tutorial is an extension to step-1 and demonstrates several ways to obtain more involved meshes than the ones shown there. 

 [2.x.4652]  This tutorial is also available as a Jupyter Python notebook that   uses the deal.II python interface. The notebook is available in the   same directory as the original C++ program. 

Generating complex geometries is a challenging task, especially in three space dimensions. We will discuss several ways to do this, but this list is not exhaustive. Additionally, there is not one approach that fits all problems. 

This example program shows some of ways to create and modify meshes for computations and outputs them as  [2.x.4653]  files in much the same way as we do in step-1. No other computations or adaptive refinements are done; the idea is that you can use the techniques used here as building blocks in other, more involved simulators. Please note that the example program does not show all the ways to generate meshes that are discussed in this introduction. 




[1.x.1841] 

When you use adaptive mesh refinement, you definitely want the initial mesh to be as coarse as possible. The reason is that you can make it as fine as you want using adaptive refinement as long as you have memory and CPU time available. However, this requires that you don't waste mesh cells in parts of the domain where they don't pay off. As a consequence, you don't want to start with a mesh that is too fine to start with, because that takes up a good part of your cell budget already, and because you can't coarsen away cells that are in the initial mesh. 

That said, your mesh needs to capture the given geometry adequately. 




[1.x.1842] 

There are several ways to create an initial mesh. Meshes can be modified or combined in many ways as discussed later on. 

[1.x.1843] 

The easiest way to generate meshes is to use the functions in namespace GridGenerator, as already discussed in step-1.  There are many different helper functions available, including  [2.x.4654]   [2.x.4655]   [2.x.4656]  and  [2.x.4657]  




[1.x.1844] 

If there is no good fit in the GridGenerator namespace for what you want to do, you can always create a Triangulation in your program "by hand". For that, you need a list of vertices with their coordinates and a list of cells referencing those vertices. You can find an example in the function <tt>create_coarse_grid()</tt> in step-14. All the functions in GridGenerator are implemented in this fashion. 

We are happy to accept more functions to be added to GridGenerator. So, if you end up writing a function that might be useful for a larger audience, please contribute it. 




[1.x.1845] 

The class GridIn can read many different mesh formats from a file from disk. How this is done is explained in step-5 and can be seen in the function  [2.x.4658]  in this example, see the code below. 

Meshes can be generated from different tools like [1.x.1846], [1.x.1847] and [1.x.1848]. See the documentation of GridIn for more information. The problem is that deal.II needs meshes that only consist of quadrilaterals and hexahedra -- tetrahedral meshes won't work (this means tools like tetgen can not be used directly). 

We will describe a possible workflow using %Gmsh. %Gmsh is the smallest and most quickly set up open source tool we are aware of. It can generate unstructured 2d quad meshes. In 3d, it can extrude 2d meshes to get hexahedral meshes; 3D meshing of unstructured geometry into hexahedra is possible, though there are some issues with the quality of these meshes that imply that these meshes only sometimes work in deal.II. 

In %Gmsh, a mesh is fundamentally described in a text-based  [2.x.4659]  file whose format can contain computations, loops, variables, etc. This format is quite flexible in allowing the description of complex geometries. The mesh is then generated from a surface representation, which is built from a list of line loops, which is built from a list of lines, which are in turn built from points. The  [2.x.4660]  script can be written and edited by hand or it can be generated automatically by creating objects graphically inside %Gmsh. In many cases it is best to combine both approaches. The file can be easily reloaded by pressing "reload" under the "Geometry" tab if you want to write it by hand and see the effects in the graphical user interface of gmsh. 

This tutorial contains an example  [2.x.4661]  file that describes a box with two objects cut out in the interior. This is how  [2.x.4662]  looks like in %Gmsh (displaying the boundary indicators as well as the mesh discussed further down below): 

 [2.x.4663]  

You might want to open the  [2.x.4664]  file in a text editor (it is located in the same directory as the <tt>step-49.cc</tt> source file) to see how it is structured. You can see how the boundary of the domain is composed of a number of lines and how later on we combine several lines into "physical lines" (or "physical surfaces") that list the logical lines' numbers. "Physical" object are the ones that carry information about the boundary indicator (see  [2.x.4665]  "this glossary entry"). 

 [2.x.4666]  It is important that this file contain "physical lines" and "physical   surfaces". These give the boundary indicators and material ids for use   in deal.II. Without these physical entities, nothing will be imported into   deal.II. 

deal.II's GridIn class can read the  [2.x.4667]  format written by %Gmsh and that contains a mesh created for the geometry described by the  [2.x.4668]  from the  [2.x.4669]  by running the commands 

[1.x.1849] 



on the command line, or by clicking "Mesh" and then "2D" inside %Gmsh after loading the file.  Now this is the mesh read from the  [2.x.4670]  file and saved again by deal.II as an image (see the  [2.x.4671]  function of the current program): 

 [2.x.4672]  

 [2.x.4673]  %Gmsh has a number of other interfaces by which one can describe   geometries to it. In particular, it has the ability to interface with   scripting languages like Python and Julia, but it can also be scripted   from C++. These interfaces are useful if one doesn't just want to generate   a mesh for a single geometry (in which case the graphical interface or,   in simple cases, a hand-written `.geo` file is probably the simplest   approach), but instead wants to do parametric studies over the geometry   for which it is necessary to generate many meshes for geometries that   differ in certain parameters. Another case where this is useful is if there   is already a CAD geometry for which one only needs a mesh; indeed, this   can be done from within deal.II using the    [2.x.4674]  function. 




[1.x.1850] 

After acquiring one (or several) meshes in the ways described above, there are many ways to manipulate them before using them in a finite element computation. 




[1.x.1851] 

The GridTools namespace contains a collection of small functions to transform a given mesh in various ways. The usage of the functions  [2.x.4675]   [2.x.4676]   [2.x.4677]  is fairly obvious, so we won't discuss those functions here. 

The function  [2.x.4678]  allows you to transform the vertices of a given mesh using a smooth function. An example of its use is also given in the results section of step-38 but let us show a simpler example here: In the function  [2.x.4679]  of the current program, we perturb the y coordinate of a mesh with a sine curve: 

 [2.x.4680]  

Similarly, we can transform a regularly refined unit square to a wall-adapted mesh in y direction using the formula  [2.x.4681] . This is done in  [2.x.4682]  of this tutorial:  [2.x.4683]  

Finally, the function  [2.x.4684]  allows you to move vertices in the mesh (optionally ignoring boundary nodes) by a random amount. This is demonstrated in  [2.x.4685]  and the result is as follows: 

 [2.x.4686]  

This function is primarily intended to negate some of the superconvergence effects one gets when studying convergence on regular meshes, as well as to suppress some optimizations in deal.II that can exploit the fact that cells are similar in shape. (Superconvergence refers to the fact that if a mesh has certain symmetries -- for example, if the edges running into a vertex are symmetric to this vertex, and if this is so for all vertices of a cell 

-- that the solution is then often convergent with a higher order than one would have expected from the usual error analysis. In the end, this is a result of the fact that if one were to make a Taylor expansion of the error, the symmetry leads to the fact that the expected next term of the expansion happens to be zero, and the error order is determined by the *second next* term. A distorted mesh does not have these symmetries and consequently the error reflects what one will see when solving the equation on *any* kind of mesh, rather than showing something that is only reflective of a particular situation.) 




[1.x.1852] 

The function  [2.x.4687]  allows you to merge two given Triangulation objects into a single one.  For this to work, the vertices of the shared edge or face have to match exactly.  Lining up the two meshes can be achieved using  [2.x.4688]  and  [2.x.4689]   In the function  [2.x.4690]  of this tutorial, we merge a square with a round hole (generated with  [2.x.4691]  and a rectangle (generated with  [2.x.4692]  The function  [2.x.4693]  allows you to specify the number of repetitions and the positions of the corners, so there is no need to shift the triangulation manually here. You should inspect the mesh graphically to make sure that cells line up correctly and no unpaired nodes exist in the merged Triangulation. 

These are the input meshes and the output mesh: 

 [2.x.4694]  




[1.x.1853] 

The function  [2.x.4695]  demonstrates the ability to pick individual vertices and move them around in an existing mesh. Note that this has the potential to produce degenerate or inverted cells and you shouldn't expect anything useful to come of using such meshes. Here, we create a box with a cylindrical hole that is not exactly centered by moving the top vertices upwards: 

 [2.x.4696]  

For the exact way how this is done, see the code below. 




[1.x.1854] 

If you need a 3d mesh that can be created by extruding a given 2d mesh (that can be created in any of the ways given above), you can use the function  [2.x.4697]  See the  [2.x.4698]  function in this tutorial for an example. Note that for this particular case, the given result could also be achieved using the 3d version of  [2.x.4699]  The main usage is a 2d mesh, generated for example with %Gmsh, that is read in from a  [2.x.4700]  file as described above. This is the output from grid_4(): 

 [2.x.4701]  




[1.x.1855] 

Creating a coarse mesh using the methods discussed above is only the first step. When you have it, it will typically serve as the basis for further mesh refinement. This is not difficult &mdash; in fact, there is nothing else to do &mdash; if your geometry consists of only straight faces. However, this is often not the case if you have a more complex geometry and more steps than just creating the mesh are necessary. We will go over some of these steps in the [1.x.1856] below. 


examples/step-49/doc/results.dox 



[1.x.1857] 

The program produces a series of  [2.x.4702]  files of the triangulations. The methods are discussed above. 




[1.x.1858] 

As mentioned in the introduction, creating a coarse mesh using the methods discussed here is only the first step. In order to refine a mesh, the Triangulation needs to know where to put new vertices on the mid-points of edges, faces, and cells. By default, these new points will be placed at the arithmetic mean of the surrounding points, but this isn't what you want if you need curved boundaries that aren't already adequately resolved by the coarse mesh. For example, for this mesh the central hole is supposed to be round: 

 [2.x.4703]  

If you simply refine it, the Triangulation class can not know whether you wanted the hole to be round or to be an octagon. The default is to place new points along existing straight lines. After two mesh refinement steps, this would yield the following mesh, which is not what we wanted: 

 [2.x.4704]  

What needs to happen is that you tell the triangulation that you in fact want to use a curved geometry. The way to do this requires three steps: 

- Create an object that describes the desired geometry. This object will be   queried when refining the Triangulation for new point placement. It will also   be used to calculate shape function values if a high degree mapping, like   MappingQ or MappingQGeneric, is used during system assembly.   In deal.II the Manifold class and classes inheriting from it (e.g.,   PolarManifold and FlatManifold) perform these calculations. 

- Notify the Triangulation object which Manifold classes to use. By default, a   Triangulation uses FlatManifold to do all geometric calculations,   which assumes that all cell edges are straight lines and all quadrilaterals   are flat. You can attach Manifold classes to a Triangulation by calling    [2.x.4705]  function, which associates a    [2.x.4706]  with a Manifold object. For more information on this   see the  [2.x.4707]  "glossary entry on this topic". 

- Finally, you must mark cells and cell faces with the correct    [2.x.4708] . For example, you could get an annular sector with   curved cells in Cartesian coordinates (but rectangles in polar coordinates)   by doing the following:   [1.x.1859] 

  Now, when the grid is refined, all cell splitting calculations will be done in   polar coordinates. 

All functions in the GridGenerator namespace which create a mesh where some cells should be curved also attach the correct Manifold object to the provided Triangulation: i.e., for those functions we get the correct behavior by default. For a hand-generated mesh, however, the situation is much more interesting. 

To illustrate this process in more detail, let us consider an example created by Yuhan Zhou as part of a 2013 semester project at Texas A&amp;M University. The goal was to generate (and use) a geometry that describes a microstructured electric device. In a CAD program, the geometry looks like this: 

 [2.x.4709]  

In the following, we will walk you through the entire process of creating a mesh for this geometry, including a number of common pitfalls by showing the things that can go wrong. 

The first step in getting there was to create a coarse mesh, which was done by creating a 2d coarse mesh for each of cross sections, extruding them into the third direction, and gluing them together. The following code does this, using the techniques previously described: 

[1.x.1860] 



This creates the following mesh: 

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.8.png"      alt="" width="400" height="355"> 

This mesh has the right general shape, but the top cells are now polygonal: their edges are no longer along circles and we do not have a very accurate representation of the original geometry. The next step is to teach the top part of the domain that it should be curved. Put another way, all calculations done on the top boundary cells should be done in cylindrical coordinates rather than Cartesian coordinates. We can do this by creating a CylindricalManifold object and associating it with the cells above  [2.x.4710] . This way, when we refine the cells on top, we will place new points along concentric circles instead of straight lines. 

In deal.II we describe all geometries with classes that inherit from Manifold. The default geometry is Cartesian and is implemented in the FlatManifold class. As the name suggests, Manifold and its inheriting classes provide a way to describe curves and curved cells in a general way with ideas and terminology from differential geometry: for example, CylindricalManifold inherits from ChartManifold, which describes a geometry through pull backs and push forwards. In general, one should think that the Triangulation class describes the topology of a domain (in addition, of course, to storing the locations of the vertices) while the Manifold classes describe the geometry of a domain (e.g., whether or not a pair of vertices lie along a circular arc or a straight line). A Triangulation will refine cells by doing computations with the Manifold associated with that cell regardless of whether or not the cell is on the boundary. Put another way: the Manifold classes do not need any information about where the boundary of the Triangulation actually is: it is up to the Triangulation to query the right Manifold for calculations on a cell. Most Manifold functions (e.g.,  [2.x.4711]  know nothing about the domain itself and just assume that the points given to it lie along a geodesic. In this case, with the CylindricalManifold constructed below, the geodesics are arcs along circles orthogonal to the  [2.x.4712] -axis centered along the line  [2.x.4713] . 

Since all three top parts of the domain use the same geodesics, we will mark all cells with centers above the  [2.x.4714]  line as being cylindrical in nature: 

[1.x.1861] 



With this code, we get a mesh that looks like this: 

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.9.png"      alt="" width="400" height="355"> 

This change fixes the boundary but creates a new problem: the cells adjacent to the cylinder's axis are badly distorted. We should use Cartesian coordinates for calculations on these central cells to avoid this issue. The cells along the center line all have a face that touches the line  [2.x.4715]  so, to implement this, we go back and overwrite the  [2.x.4716] s on these cells to be zero (which is the default): 

[1.x.1862] 



This gives us the following grid: 

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.10.png"      alt="" width="400" height="355"> 

This gives us a good mesh, where cells at the center of each circle are still Cartesian and cells around the boundary lie along a circle. We can really see the nice detail of the boundary fitted mesh if we refine two more times: 

<img src="https://www.dealii.org/images/steps/developer/step-49.yuhan.11.png"      alt="" width="400" height="355"> 




[1.x.1863] 

[1.x.1864] 

It is often useful to assign different boundary ids to a mesh that is generated in one form or another as described in this tutorial to apply different boundary conditions. 

For example, you might want to apply a different boundary condition for the right boundary of the first grid in this program. To do this, iterate over the cells and their faces and identify the correct faces (for example using `cell->center()` to query the coordinates of the center of a cell as we do in step-1, or using `cell->face(f)->get_boundary_id()` to query the current boundary indicator of the  [2.x.4717] th face of the cell). You can then use `cell->face(f)->set_boundary_id()` to set the boundary id to something different. You can take a look back at step-1 how iteration over the meshes is done there. 

[1.x.1865] 

Computations on manifolds, like they are done in step-38, require a surface mesh embedded into a higher dimensional space. While some can be constructed using the GridGenerator namespace or loaded from a file, it is sometimes useful to extract a surface mesh from a volume mesh. 

Use the function  [2.x.4718]  to extract the surface elements of a mesh. Using the function on a 3d mesh (a `Triangulation<3,3>`, for example from `grid_4()`), this will return a `Triangulation<2,3>` that you can use in step-38.  Also try extracting the boundary mesh of a `Triangulation<2,2>`. 


<!-- 

Possible Extensions for this tutorial: 

- Database of unstructured meshes for convergence studies 

- how to remove or disable a cell inside a mesh 

--> 


examples/step-5/doc/intro.dox 

[1.x.1866] 

[1.x.1867] 

 [2.x.4719]  

This example does not show revolutionary new things, but it shows many small improvements over the previous examples, and also many small things that can usually be found in finite element programs. Among them are:  [2.x.4720]     [2.x.4721]  Computations on successively refined grids. At least in the        mathematical sciences, it is common to compute solutions on        a hierarchy of grids, in order to get a feeling for the accuracy        of the solution; if you only have one solution on a single grid, you        usually can't guess the accuracy of the        solution. Furthermore, deal.II is designed to support adaptive        algorithms where iterative solution on successively refined        grids is at the heart of algorithms. Although adaptive grids        are not used in this example, the foundations for them is laid        here.    [2.x.4722]  In practical applications, the domains are often subdivided        into triangulations by automatic mesh generators. In order to        use them, it is important to read coarse grids from a file. In        this example, we will read a coarse grid in UCD (unstructured        cell data) format. When this program was first written around        2000, UCD format was what the AVS Explorer used -- a program        reasonably widely used at the time but now no longer of        importance. (Nonetheless, the file format has survived and is        still understood by a number of programs.)    [2.x.4723]  Finite element programs usually use extensive amounts of        computing time, so some optimizations are sometimes        necessary. We will show some of them.    [2.x.4724]  On the other hand, finite element programs tend to be rather        complex, so debugging is an important aspect. We support safe        programming by using assertions that check the validity of        parameters and %internal states in a debug mode, but are removed        in optimized mode. ( [2.x.4725]     [2.x.4726]  Regarding the mathematical side, we show how to support a        variable coefficient in the elliptic operator and how to use        preconditioned iterative solvers for the linear systems of        equations.  [2.x.4727]  

The equation to solve here is as follows: 

[1.x.1868] 

If  [2.x.4728]  was a constant coefficient, this would simply be the Poisson equation. However, if it is indeed spatially variable, it is a more complex equation (often referred to as the "extended Poisson equation"). Depending on what the variable  [2.x.4729]  refers to it models a variety of situations with wide applicability: 

- If  [2.x.4730]  is the electric potential, then  [2.x.4731]  is the electric current   in a medium and the coefficient  [2.x.4732]  is the conductivity of the medium at any   given point. (In this situation, the right hand side of the equation would   be the electric source density and would usually be zero or consist of   localized, Delta-like, functions.) 

- If  [2.x.4733]  is the vertical deflection of a thin membrane, then  [2.x.4734]  would be a   measure of the local stiffness. This is the interpretation that will allow   us to interpret the images shown in the results section below. 

Since the Laplace/Poisson equation appears in so many contexts, there are many more interpretations than just the two listed above. 

When assembling the linear system for this equation, we need the weak form which here reads as follows: 

[1.x.1869] 

The implementation in the  [2.x.4735]  function follows immediately from this. 


examples/step-5/doc/results.dox 



[1.x.1870] 


Here is the console output: 

[1.x.1871] 






In each cycle, the number of cells quadruples and the number of CG iterations roughly doubles. Also, in each cycle, the program writes one output graphic file in VTU format. They are depicted in the following: 

 [2.x.4736]  




Due to the variable coefficient (the curvature there is reduced by the same factor by which the coefficient is increased), the top region of the solution is flattened. The gradient of the solution is discontinuous along the interface, although this is not very clearly visible in the pictures above. We will look at this in more detail in the next example. 

The pictures also show that the solution computed by this program is actually pretty wrong on a very coarse mesh (its magnitude is wrong). That's because no numerical method guarantees that the solution on a coarse mesh is particularly accurate -- but we know that the solution [1.x.1872] to the exact solution, and indeed you can see how the solutions from one mesh to the next seem to not change very much any more at the end. 


examples/step-50/doc/intro.dox 

 [2.x.4737]  

[1.x.1873] 

 [2.x.4738]  

 [2.x.4739]  As a prerequisite of this program, you need to have both p4est and either the PETSc or Trilinos library installed. The installation of deal.II together with these additional libraries is described in the [1.x.1874] file. 


[1.x.1875] 

[1.x.1876] 


This example shows the usage of the multilevel functions in deal.II on parallel, distributed meshes and gives a comparison between geometric and algebraic multigrid methods. The algebraic multigrid (AMG) preconditioner is the same used in step-40. Two geometric multigrid (GMG) preconditioners are considered: a matrix-based version similar to that in step-16 (but for parallel computations) and a matrix-free version discussed in step-37. The goal is to find out which approach leads to the best solver for large parallel computations. 

This tutorial is based on one of the numerical examples in  [2.x.4740] . Please see that publication for a detailed background on the multigrid implementation in deal.II. We will summarize some of the results in the following text. 

Algebraic multigrid methods are obviously the easiest to implement with deal.II since classes such as  [2.x.4741]  and  [2.x.4742]  are, in essence, black box preconditioners that require only a couple of lines to set up even for parallel computations. On the other hand, geometric multigrid methods require changes throughout a code base -- not very many, but one has to know what one is doing. 

What the results of this program will show is that algebraic and geometric multigrid methods are roughly comparable in performance [1.x.1877], and that matrix-free geometric multigrid methods are vastly better for the problem under consideration here. A secondary conclusion will be that matrix-based geometric multigrid methods really don't scale well strongly when the number of unknowns per processor becomes smaller than 20,000 or so. 




[1.x.1878] 

We consider the variable-coefficient Laplacian weak formulation 

[1.x.1879] 

on the domain  [2.x.4743]  (an L-shaped domain for 2D and a Fichera corner for 3D) with  [2.x.4744]  if  [2.x.4745]  and  [2.x.4746]  otherwise. In other words,  [2.x.4747]  is small along the edges or faces of the domain that run into the reentrant corner, as will be visible in the figure below. 

The boundary conditions are  [2.x.4748]  on the whole boundary and the right-hand side is  [2.x.4749] . We use continuous  [2.x.4750]  elements for the discrete finite element space  [2.x.4751] , and use a residual-based, cell-wise a posteriori error estimator  [2.x.4752]  from  [2.x.4753]  with 

[1.x.1880] 

to adaptively refine the mesh. (This is a generalization of the Kelly error estimator used in the KellyErrorEstimator class that drives mesh refinement in most of the other tutorial programs.) The following figure visualizes the solution and refinement for 2D:  [2.x.4754]  In 3D, the solution looks similar (see below). On the left you can see the solution and on the right we show a slice for  [2.x.4755]  close to the center of the domain showing the adaptively refined mesh.  [2.x.4756]  Both in 2D and 3D you can see the adaptive refinement picking up the corner singularity and the inner singularity where the viscosity jumps, while the interface along the line that separates the two viscosities is (correctly) not refined as it is resolved adequately. This is because the kink in the solution that results from the jump in the coefficient is aligned with cell interfaces. 




[1.x.1881] 

As mentioned above, the purpose of this program is to demonstrate the use of algebraic and geometric multigrid methods for this problem, and to do so for parallel computations. An important component of making algorithms scale to large parallel machines is ensuring that every processor has the same amount of work to do. (More precisely, what matters is that there are no small fraction of processors that have substantially more work than the rest since, if that were so, a large fraction of processors will sit idle waiting for the small fraction to finish. Conversely, a small fraction of processors having substantially [1.x.1882] work is not a problem because the majority of processors continues to be productive and only the small fraction sits idle once finished with their work.) 

For the active mesh, we use the  [2.x.4757]  class as done in step-40 which uses functionality in the external library [1.x.1883] for the distribution of the active cells among processors. For the non-active cells in the multilevel hierarchy, deal.II implements what we will refer to as the "first-child rule" where, for each cell in the hierarchy, we recursively assign the parent of a cell to the owner of the first child cell. The following figures give an example of such a distribution. Here the left image represents the active cells for a sample 2D mesh partitioned using a space-filling curve (which is what p4est uses to partition cells); the center image gives the tree representation of the active mesh; and the right image gives the multilevel hierarchy of cells. The colors and numbers represent the different processors. The circular nodes in the tree are the non-active cells which are distributed using the "first-child rule". 

 [2.x.4758]  

Included among the output to screen in this example is a value "Partition efficiency" given by one over  [2.x.4759]  This value, which will be denoted by  [2.x.4760] ,  quantifies the overhead produced by not having a perfect work balance on each level of the multigrid hierarchy. This imbalance is evident from the example above: while level  [2.x.4761]  is about as well balanced as is possible with four cells among three processors, the coarse level  [2.x.4762]  has work for only one processor, and level  [2.x.4763]  has work for only two processors of which one has three times as much work as the other. 

For defining  [2.x.4764] , it is important to note that, as we are using local smoothing to define the multigrid hierarchy (see the  [2.x.4765]  "multigrid paper" for a description of local smoothing), the refinement level of a cell corresponds to that cell's multigrid level. Now, let  [2.x.4766]  be the number of cells on level  [2.x.4767]  (both active and non-active cells) and  [2.x.4768]  be the subset owned by process  [2.x.4769] . We will also denote by  [2.x.4770]  the total number of processors. Assuming that the workload for any one processor is proportional to the number of cells owned by that processor, the optimal workload per processor is given by 

[1.x.1884] 

Next, assuming a synchronization of work on each level (i.e., on each level of a V-cycle, work must be completed by all processors before moving on to the next level), the limiting effort on each level is given by 

[1.x.1885] 

and the total parallel complexity 

[1.x.1886] 

Then we define  [2.x.4771]  as a ratio of the optimal partition to the parallel complexity of the current partition 

[1.x.1887] 

For the example distribution above, we have 

[1.x.1888] 

The value  [2.x.4772]     [2.x.4773]  then represents the factor increase in timings we expect for GMG methods (vmults, assembly, etc.) due to the imbalance of the mesh partition compared to a perfectly load-balanced workload. We will report on these in the results section below for a sequence of meshes, and compare with the observed slow-downs as we go to larger and larger processor numbers (where, typically, the load imbalance becomes larger as well). 

These sorts of considerations are considered in much greater detail in  [2.x.4774] , which contains a full discussion of the partition efficiency model and the effect the imbalance has on the GMG V-cycle timing. In summary, the value of  [2.x.4775]  is highly dependent on the degree of local mesh refinement used and has an optimal value  [2.x.4776]  for globally refined meshes. Typically for adaptively refined meshes, the number of processors used to distribute a single mesh has a negative impact on  [2.x.4777]  but only up to a leveling off point, where the imbalance remains relatively constant for an increasing number of processors, and further refinement has very little impact on  [2.x.4778] . Finally,  [2.x.4779]  was shown to give an accurate representation of the slowdown in parallel scaling expected for the timing of a V-cycle. 

It should be noted that there is potential for some asynchronous work between multigrid levels, specifically with purely nearest neighbor MPI communication, and an adaptive mesh could be constructed such that the efficiency model would far overestimate the V-cycle slowdown due to the asynchronous work "covering up" the imbalance (which assumes synchronization over levels). However, for most realistic adaptive meshes the expectation is that this asynchronous work will only cover up a very small portion of the imbalance and the efficiency model will describe the slowdown very well. 




[1.x.1889] 

The considerations above show that one has to expect certain limits on the scalability of the geometric multigrid algorithm as it is implemented in deal.II because even in cases where the finest levels of a mesh are perfectly load balanced, the coarser levels may not be. At the same time, the coarser levels are weighted less (the contributions of  [2.x.4780]  to  [2.x.4781]  are small) because coarser levels have fewer cells and, consequently, do not contribute to the overall run time as much as finer levels. In other words, imbalances in the coarser levels may not lead to large effects in the big picture. 

Algebraic multigrid methods are of course based on an entirely different approach to creating a hierarchy of levels. In particular, they create these purely based on analyzing the system matrix, and very sophisticated algorithms for ensuring that the problem is well load-balanced on every level are implemented in both the hypre and ML/MueLu packages that underly the  [2.x.4782]  and  [2.x.4783]  classes. In some sense, these algorithms are simpler than for geometric multigrid methods because they only deal with the matrix itself, rather than all of the connotations of meshes, neighbors, parents, and other geometric entities. At the same time, much work has also been put into making algebraic multigrid methods scale to very large problems, including questions such as reducing the number of processors that work on a given level of the hierarchy to a subset of all processors, if otherwise processors would spend less time on computations than on communication. (One might note that it is of course possible to implement these same kinds of ideas also in geometric multigrid algorithms where one purposefully idles some processors on coarser levels to reduce the amount of communication. deal.II just doesn't do this at this time.) 

These are not considerations we typically have to worry about here, however: For most purposes, we use algebraic multigrid methods as black-box methods. 




[1.x.1890] 

As mentioned above, this program can use three different ways of solving the linear system: matrix-based geometric multigrid ("MB"), matrix-free geometric multigrid ("MF"), and algebraic multigrid ("AMG"). The directory in which this program resides has input files with suffix `.prm` for all three of these options, and for both 2d and 3d. 

You can execute the program as in 

[1.x.1891] 

and this will take the run-time parameters from the given input file (here, `gmg_mb_2d.prm`). 

The program is intended to be run in parallel, and you can achieve this using a command such as 

[1.x.1892] 

if you want to, for example, run on four processors. (That said, the program is also ready to run with, say, `-np 28672` if that's how many processors you have available.) 


examples/step-50/doc/results.dox 



[1.x.1893] 

When you run the program using the following command 

[1.x.1894] 

the screen output should look like the following: 

[1.x.1895] 

Here, the timing of the `solve()` function is split up in 3 parts: setting up the multigrid preconditioner, execution of a single multigrid V-cycle, and the CG solver. The V-cycle that is timed is unnecessary for the overall solve and only meant to give an insight at the different costs for AMG and GMG. Also it should be noted that when using the AMG solver, "Workload imbalance" is not included in the output since the hierarchy of coarse meshes is not required. 

All results in this section are gathered on Intel Xeon Platinum 8280 (Cascade Lake) nodes which have 56 cores and 192GB per node and support AVX-512 instructions, allowing for vectorization over 8 doubles (vectorization used only in the matrix-free computations). The code is compiled using gcc 7.1.0 with intel-mpi 17.0.3. Trilinos 12.10.1 is used for the matrix-based GMG/AMG computations. 

We can then gather a variety of information by calling the program with the input files that are provided in the directory in which step-50 is located. Using these, and adjusting the number of mesh refinement steps, we can produce information about how well the program scales. 

The following table gives weak scaling timings for this program on up to 256M DoFs and 7,168 processors. (Recall that weak scaling keeps the number of degrees of freedom per processor constant while increasing the number of processors; i.e., it considers larger and larger problems.) Here,  [2.x.4784]  is the partition efficiency from the  introduction (also equal to 1.0/workload imbalance), "Setup" is a combination of setup, setup multigrid, assemble, and assemble multigrid from the timing blocks, and "Prec" is the preconditioner setup. Ideally all times would stay constant over each problem size for the individual solvers, but since the partition efficiency decreases from 0.371 to 0.161 from largest to smallest problem size, we expect to see an approximately  [2.x.4785]  times increase in timings for GMG. This is, in fact, pretty close to what we really get: 

 [2.x.4786]  

On the other hand, the algebraic multigrid in the last set of columns is relatively unaffected by the increasing imbalance of the mesh hierarchy (because it doesn't use the mesh hierarchy) and the growth in time is rather driven by other factors that are well documented in the literature (most notably that the algorithmic complexity of some parts of algebraic multigrid methods appears to be  [2.x.4787]  instead of  [2.x.4788]  for geometric multigrid). 

The upshort of the table above is that the matrix-free geometric multigrid method appears to be the fastest approach to solving this equation if not by a huge margin. Matrix-based methods, on the other hand, are consistently the worst. 

The following figure provides strong scaling results for each method, i.e., we solve the same problem on more and more processors. Specifically, we consider the problems after 16 mesh refinement cycles (32M DoFs) and 19 cycles (256M DoFs), on between 56 to 28,672 processors: 

 [2.x.4789]  

While the matrix-based GMG solver and AMG scale similarly and have a similar time to solution (at least as long as there is a substantial number of unknowns per processor -- say, several 10,000), the matrix-free GMG solver scales much better and solves the finer problem in roughly the same time as the AMG solver for the coarser mesh with only an eighth of the number of processors. Conversely, it can solve the same problem on the same number of processors in about one eighth the time. 




[1.x.1896] 

[1.x.1897] 

The finite element degree is currently hard-coded as 2, see the template arguments of the main class. It is easy to change. To test, it would be interesting to switch to a test problem with a reference solution. This way, you can compare error rates. 

[1.x.1898] 

A more interesting example would involve a more complicated coarse mesh (see step-49 for inspiration). The issue in that case is that the coarsest level of the mesh hierarchy is actually quite large, and one would have to think about ways to solve the coarse level problem efficiently. (This is not an issue for algebraic multigrid methods because they would just continue to build coarser and coarser levels of the matrix, regardless of their geometric origin.) 

In the program here, we simply solve the coarse level problem with a Conjugate Gradient method without any preconditioner. That is acceptable if the coarse problem is really small -- for example, if the coarse mesh had a single cell, then the coarse mesh problems has a  [2.x.4790]  matrix in 2d, and a  [2.x.4791]  matrix in 3d; for the coarse mesh we use on the  [2.x.4792] -shaped domain of the current program, these sizes are  [2.x.4793]  in 2d and  [2.x.4794]  in 3d. But if the coarse mesh consists of hundreds or thousands of cells, this approach will no longer work and might start to dominate the overall run-time of each V-cyle. A common approach is then to solve the coarse mesh problem using an algebraic multigrid preconditioner; this would then, however, require assembling the coarse matrix (even for the matrix-free version) as input to the AMG implementation. 


examples/step-51/doc/intro.dox 

 [2.x.4795]  

[1.x.1899] 

[1.x.1900] 

[1.x.1901] 

This tutorial program presents the implementation of a hybridizable discontinuous Galkerin method for the convection-diffusion equation. 

[1.x.1902] 

One common argument against the use of discontinuous Galerkin elements is the large number of globally coupled degrees of freedom that one must solve in an implicit system.  This is because, unlike continuous finite elements, in typical discontinuous elements there is one degree of freedom at each vertex [1.x.1903], rather than just one, and similarly for edges and faces.  As an example of how fast the number of unknowns grows, consider the FE_DGPMonomial basis: each scalar solution component is represented by polynomials of degree  [2.x.4796]  with  [2.x.4797]  degrees of freedom per element. Typically, all degrees of freedom in an element are coupled to all of the degrees of freedom in the adjacent elements.  The resulting discrete equations yield very large linear systems very quickly, especially for systems of equations in 2 or 3 dimensions. 

[1.x.1904] To alleviate the computational cost of solving such large linear systems, the hybridizable discontinuous Galerkin (HDG) methodology was introduced by Cockburn and co-workers (see the references in the recent HDG overview article by Nguyen and Peraire  [2.x.4798] ). 

The HDG method achieves this goal by formulating the mathematical problem using Dirichlet-to-Neumann mappings.  The partial differential equations are first written as a first order system, and each field is then discretized via a DG method.  At this point, the single-valued "trace" values on the skeleton of the mesh, i.e., element faces, are taken to be independent unknown quantities. This yields unknowns in the discrete formulation that fall into two categories: 

- Face unknowns that only couple with the cell unknowns from both sides of the face; 

- Cell unknowns that only couple with the cell and face unknowns   defined within the same cell. Crucially, no cell interior degree of freedom   on one cell ever couples to any interior cell degree of freedom of a   different cell. 

The Dirichlet-to-Neumann map concept then permits the following solution procedure: <ol>    [2.x.4799]   Use local element interior data to enforce a Neumann condition on the skeleton of the triangulation.  The global problem is then to solve for the trace values, which are the only globally coupled unknowns.    [2.x.4800]   Use the known skeleton values as Dirichlet data for solving local element-level solutions.  This is known as the 'local solver', and is an [1.x.1905] element-by-element solution process.  [2.x.4801]  

[1.x.1906] The above procedure also has a linear algebra interpretation---referred to as [1.x.1907]---that was exploited to reduce the size of the global linear system by Guyan in the context of continuous Finite Elements  [2.x.4802] , and by Fraeijs de Veubeke for mixed methods  [2.x.4803] . In the latter case (mixed formulation), the system reduction was achieved through the use of discontinuous fluxes combined with the introduction of an additional auxiliary [1.x.1908] variable that approximates the trace of the unknown at the boundary of every element. This procedure became known as hybridization and---by analogy---is the reason why the local discontinuous Galerkin method introduced by Cockburn, Gopalakrishnan, and Lazarov in 2009  [2.x.4804] , and subsequently developed by their collaborators, eventually came to be known as the [1.x.1909] (HDG) method. 

Let us write the complete linear system associated to the HDG problem as a block system with the discrete DG (cell interior) variables  [2.x.4805]  as first block and the skeleton (face) variables  [2.x.4806]  as the second block: 

[1.x.1910] 

Our aim is now to eliminate the  [2.x.4807]  block with a Schur complement approach similar to step-20, which results in the following two steps: 

[1.x.1911] 

The point is that the presence of  [2.x.4808]  is not a problem because  [2.x.4809]  is a block diagonal matrix where each block corresponds to one cell and is therefore easy enough to invert. The coupling to other cells is introduced by the matrices  [2.x.4810]  and  [2.x.4811]  over the skeleton variable. The block-diagonality of  [2.x.4812]  and the structure in  [2.x.4813]  and  [2.x.4814]  allow us to invert the matrix  [2.x.4815]  element by element (the local solution of the Dirichlet problem) and subtract  [2.x.4816]  from  [2.x.4817] . The steps in the Dirichlet-to-Neumann map concept hence correspond to <ol>    [2.x.4818]  constructing the Schur complement matrix  [2.x.4819]  and right hand     side  [2.x.4820]   [1.x.1912]     and inserting the contribution into the global trace matrix in the usual way,    [2.x.4821]  solving the Schur complement system for  [2.x.4822] , and    [2.x.4823]  solving for  [2.x.4824]  using the second equation, given  [2.x.4825] .  [2.x.4826]  




[1.x.1913] Another criticism of traditional DG methods is that the approximate fluxes converge suboptimally.  The local HDG solutions can be shown to converge as  [2.x.4827] , i.e., at optimal order.  Additionally, a super-convergence property can be used to post-process a new approximate solution that converges at the rate  [2.x.4828] . 




[1.x.1914] 

The hybridizable discontinuous Galerkin method is only one way in which the problems of the discontinuous Galerkin method can be addressed. Another idea is what is called the "weak Galerkin" method. It is explored in step-61. 




[1.x.1915] 

The HDG formulation used for this example is taken from  [2.x.4829]  [1.x.1916][1.x.1917][1.x.1918] 

We consider the convection-diffusion equation over the domain  [2.x.4830]  with Dirichlet boundary  [2.x.4831]  and Neumann boundary  [2.x.4832] : 

[1.x.1919] 



Introduce the auxiliary variable  [2.x.4833]  and rewrite the above equation as the first order system: 

[1.x.1920] 



We multiply these equations by the weight functions  [2.x.4834]  and integrate by parts over every element  [2.x.4835]  to obtain: 

[1.x.1921] 



The terms decorated with a hat denote the numerical traces (also commonly referred to as numerical fluxes).  They are approximations to the interior values on the boundary of the element.  To ensure conservation, these terms must be single-valued on any given element edge  [2.x.4836]  even though, with discontinuous shape functions, there may of course be multiple values coming from the cells adjacent to an interface. We eliminate the numerical trace  [2.x.4837]  by using traces of the form: 

[1.x.1922] 



The variable  [2.x.4838]  is introduced as an additional independent variable and is the one for which we finally set up a globally coupled linear system. As mentioned above, it is defined on the element faces and discontinuous from one face to another wherever faces meet (at vertices in 2d, and at edges and vertices in 3d). Values for  [2.x.4839]  and  [2.x.4840]  appearing in the numerical trace function are taken to be the cell's interior solution restricted to the boundary  [2.x.4841] . 

The local stabilization parameter  [2.x.4842]  has effects on stability and accuracy of HDG solutions; see the literature for a further discussion. A stabilization parameter of unity is reported to be the choice which gives best results. A stabilization parameter  [2.x.4843]  that tends to infinity prohibits jumps in the solution over the element boundaries, making the HDG solution approach the approximation with continuous finite elements. In the program below, we choose the stabilization parameter as 

[1.x.1923] 

where we set the diffusion  [2.x.4844]  and the diffusion length scale to  [2.x.4845] . 

The trace/skeleton variables in HDG methods are single-valued on element faces.  As such, they must strongly represent the Dirichlet data on  [2.x.4846] .  This means that 

[1.x.1924] 

where the equal sign actually means an  [2.x.4847]  projection of the boundary function  [2.x.4848]  onto the space of the face variables (e.g. linear functions on the faces). This constraint is then applied to the skeleton variable  [2.x.4849]  using inhomogeneous constraints by the method  [2.x.4850]  

Summing the elemental contributions across all elements in the triangulation, enforcing the normal component of the numerical flux, and integrating by parts on the equation weighted by  [2.x.4851] , we arrive at the final form of the problem: Find  [2.x.4852]  such that 

[1.x.1925] 



The unknowns  [2.x.4853]  are referred to as local variables; they are represented as standard DG variables.  The unknown  [2.x.4854]  is the skeleton variable which has support on the codimension-1 surfaces (faces) of the mesh. 

We use the notation  [2.x.4855]  to denote the sum of integrals over all cells and  [2.x.4856]  to denote integration over all faces of all cells, i.e., interior faces are visited twice, once from each side and with the corresponding normal vectors. When combining the contribution from both elements sharing a face, the above equation yields terms familiar from the DG method, with jumps of the solution over the cell boundaries. 

In the equation above, the space  [2.x.4857]  for the scalar variable  [2.x.4858]  is defined as the space of functions that are tensor product polynomials of degree  [2.x.4859]  on each cell and discontinuous over the element boundaries  [2.x.4860] , i.e., the space described by  [2.x.4861] . The space for the gradient or flux variable  [2.x.4862]  is a vector element space where each component is a locally polynomial and discontinuous  [2.x.4863] . In the code below, we collect these two local parts together in one FESystem where the first  [2.x.4864]  dim components denote the gradient part and the last scalar component corresponds to the scalar variable. For the skeleton component  [2.x.4865] , we define a space that consists of discontinuous tensor product polynomials that live on the element faces, which in deal.II is implemented by the class FE_FaceQ. This space is otherwise similar to FE_DGQ, i.e., the solution function is not continuous between two neighboring faces, see also the results section below for an illustration. 

In the weak form given above, we can note the following coupling patterns: <ol>    [2.x.4866]  The matrix  [2.x.4867]  consists of local-local coupling terms.  These arise when the   local weighting functions  [2.x.4868]  multiply the local solution terms    [2.x.4869] . Because the elements are discontinuous,  [2.x.4870]    is block diagonal.    [2.x.4871]  The matrix  [2.x.4872]  represents the local-face coupling.  These are the terms   with weighting functions  [2.x.4873]  multiplying the skeleton variable    [2.x.4874] .    [2.x.4875]  The matrix  [2.x.4876]  represents the face-local coupling, which involves the   weighting function  [2.x.4877]  multiplying the local solutions  [2.x.4878] .    [2.x.4879]   The matrix  [2.x.4880]  is the face-face coupling;   terms involve both  [2.x.4881]  and  [2.x.4882] .  [2.x.4883]  

[1.x.1926] 

One special feature of the HDG methods is that they typically allow for constructing an enriched solution that gains accuracy. This post-processing takes the HDG solution in an element-by-element fashion and combines it such that one can get  [2.x.4884]  order of accuracy when using polynomials of degree  [2.x.4885] . For this to happen, there are two necessary ingredients: <ol>    [2.x.4886]  The computed solution gradient  [2.x.4887]  converges at optimal rate,    i.e.,  [2.x.4888] .    [2.x.4889]  The cell-wise average of the scalar part of the solution,     [2.x.4890] , super-converges at rate     [2.x.4891] .  [2.x.4892]  

We now introduce a new variable  [2.x.4893] , which we find by minimizing the expression  [2.x.4894]  over the cell  [2.x.4895]  under the constraint  [2.x.4896] . The constraint is necessary because the minimization functional does not determine the constant part of  [2.x.4897] . This translates to the following system of equations: 

[1.x.1927] 



Since we test by the whole set of basis functions in the space of tensor product polynomials of degree  [2.x.4898]  in the second set of equations, this is an overdetermined system with one more equation than unknowns. We fix this in the code below by omitting one of these equations (since the rows in the Laplacian are linearly dependent when representing a constant function). As we will see below, this form of the post-processing gives the desired super-convergence result with rate  [2.x.4899] .  It should be noted that there is some freedom in constructing  [2.x.4900]  and this minimization approach to extract the information from the gradient is not the only one. In particular, the post-processed solution defined here does not satisfy the convection-diffusion equation in any sense. As an alternative, the paper by Nguyen, Peraire and Cockburn cited above suggests another somewhat more involved formula for convection-diffusion that can also post-process the flux variable into an  [2.x.4901] -conforming variant and better represents the local convection-diffusion operator when the diffusion is small. We leave the implementation of a more sophisticated post-processing as a possible extension to the interested reader. 

Note that for vector-valued problems, the post-processing works similarly. One simply sets the constraint for the mean value of each vector component separately and uses the gradient as the main source of information. 

[1.x.1928] 

For this tutorial program, we consider almost the same test case as in step-7. The computational domain is  [2.x.4902]  and the exact solution corresponds to the one in step-7, except for a scaling. We use the following source centers  [2.x.4903]  for the exponentials  [2.x.4904]     [2.x.4905]  1D:   [2.x.4906] ,    [2.x.4907]  2D:  [2.x.4908] ,    [2.x.4909]  3D:  [2.x.4910] .  [2.x.4911]  

With the exact solution given, we then choose the forcing on the right hand side and the Neumann boundary condition such that we obtain this solution (manufactured solution technique). In this example, we choose the diffusion equal to one and the convection as 

[1.x.1929] Note that the convection is divergence-free,  [2.x.4912] . 

[1.x.1930] 

Besides implementing the above equations, the implementation below provides the following features:  [2.x.4913]     [2.x.4914]  WorkStream to parallelize local solvers. Workstream has been presented   in detail in step-9.    [2.x.4915]  Reconstruct the local DG solution from the trace.    [2.x.4916]  Post-processing the solution for superconvergence.    [2.x.4917]  DataOutFaces for direct output of the global skeleton solution.  [2.x.4918]  


examples/step-51/doc/results.dox 



[1.x.1931] 

[1.x.1932] 

We first have a look at the output generated by the program when run in 2D. In the four images below, we show the solution for polynomial degree  [2.x.4919]  and cycles 2, 3, 4, and 8 of the program. In the plots, we overlay the data generated from the internal data (DG part) with the skeleton part ( [2.x.4920] ) into the same plot. We had to generate two different data sets because cells and faces represent different geometric entities, the combination of which (in the same file) is not supported in the VTK output of deal.II. 

The images show the distinctive features of HDG: The cell solution (colored surfaces) is discontinuous between the cells. The solution on the skeleton variable sits on the faces and ties together the local parts. The skeleton solution is not continuous on the vertices where the faces meet, even though its values are quite close along lines in the same coordinate direction. The skeleton solution can be interpreted as a rubber spring between the two sides that balances the jumps in the solution (or rather, the flux  [2.x.4921] ). From the picture at the top left, it is clear that the bulk solution frequently over- and undershoots and that the skeleton variable in indeed a better approximation to the exact solution; this explains why we can get a better solution using a postprocessing step. 

As the mesh is refined, the jumps between the cells get small (we represent a smooth solution), and the skeleton solution approaches the interior parts. For cycle 8, there is no visible difference in the two variables. We also see how boundary conditions are implemented weakly and that the interior variables do not exactly satisfy boundary conditions. On the lower and left boundaries, we set Neumann boundary conditions, whereas we set Dirichlet conditions on the right and top boundaries. 

 [2.x.4922]  

Next, we have a look at the post-processed solution, again at cycles 2, 3, 4, and 8. This is a discontinuous solution that is locally described by second order polynomials. While the solution does not look very good on the mesh of cycle two, it looks much better for cycles three and four. As shown by the convergence table below, we find that is also converges more quickly to the analytical solution. 

 [2.x.4923]  

Finally, we look at the solution for  [2.x.4924]  at cycle 2. Despite the coarse mesh with only 64 cells, the post-processed solution is similar in quality to the linear solution (not post-processed) at cycle 8 with 4,096 cells. This clearly shows the superiority of high order methods for smooth solutions. 

 [2.x.4925]  

[1.x.1933] 

When the program is run, it also outputs information about the respective steps and convergence tables with errors in the various components in the end. In 2D, the convergence tables look the following: 

[1.x.1934] 




One can see the error reduction upon grid refinement, and for the cases where global refinement was performed, also the convergence rates. The quadratic convergence rates of Q1 elements in the  [2.x.4926]  norm for both the scalar variable and the gradient variable is apparent, as is the cubic rate for the postprocessed scalar variable in the  [2.x.4927]  norm. Note this distinctive feature of an HDG solution. In typical continuous finite elements, the gradient of the solution of order  [2.x.4928]  converges at rate  [2.x.4929]  only, as opposed to  [2.x.4930]  for the actual solution. Even though superconvergence results for finite elements are also available (e.g. superconvergent patch recovery first introduced by Zienkiewicz and Zhu), these are typically limited to structured meshes and other special cases. For Q3 HDG variables, the scalar variable and gradient converge at fourth order and the postprocessed scalar variable at fifth order. 

The same convergence rates are observed in 3d. 

[1.x.1935] 



[1.x.1936] 

[1.x.1937] 

The convergence tables verify the expected convergence rates stated in the introduction. Now, we want to show a quick comparison of the computational efficiency of the HDG method compared to a usual finite element (continuous Galkerin) method on the problem of this tutorial. Of course, stability aspects of the HDG method compared to continuous finite elements for transport-dominated problems are also important in practice, which is an aspect not seen on a problem with smooth analytic solution. In the picture below, we compare the  [2.x.4931]  error as a function of the number of degrees of freedom (left) and of the computing time spent in the linear solver (right) for two space dimensions of continuous finite elements (CG) and the hybridized discontinuous Galerkin method presented in this tutorial. As opposed to the tutorial where we only use unpreconditioned BiCGStab, the times shown in the figures below use the Trilinos algebraic multigrid preconditioner in  [2.x.4932]  For the HDG part, a wrapper around ChunkSparseMatrix for the trace variable has been used in order to utilize the block structure in the matrix on the finest level. 

 [2.x.4933]  

The results in the graphs show that the HDG method is slower than continuous finite elements at  [2.x.4934] , about equally fast for cubic elements and faster for sixth order elements. However, we have seen above that the HDG method actually produces solutions which are more accurate than what is represented in the original variables. Therefore, in the next two plots below we instead display the error of the post-processed solution for HDG (denoted by  [2.x.4935]  for example). We now see a clear advantage of HDG for the same amount of work for both  [2.x.4936]  and  [2.x.4937] , and about the same quality for  [2.x.4938] . 

 [2.x.4939]  

Since the HDG method actually produces results converging as  [2.x.4940] , we should compare it to a continuous Galerkin solution with the same asymptotic convergence behavior, i.e., FE_Q with degree  [2.x.4941] . If we do this, we get the convergence curves below. We see that CG with second order polynomials is again clearly better than HDG with linears. However, the advantage of HDG for higher orders remains. 

 [2.x.4942]  

The results are in line with properties of DG methods in general: Best performance is typically not achieved for linear elements, but rather at somewhat higher order, usually around  [2.x.4943] . This is because of a volume-to-surface effect for discontinuous solutions with too much of the solution living on the surfaces and hence duplicating work when the elements are linear. Put in other words, DG methods are often most efficient when used at relatively high order, despite their focus on a discontinuous (and hence, seemingly low accurate) representation of solutions. 

[1.x.1938] 

We now show the same figures in 3D: The first row shows the number of degrees of freedom and computing time versus the  [2.x.4944]  error in the scalar variable  [2.x.4945]  for CG and HDG at order  [2.x.4946] , the second row shows the post-processed HDG solution instead of the original one, and the third row compares the post-processed HDG solution with CG at order  [2.x.4947] . In 3D, the volume-to-surface effect makes the cost of HDG somewhat higher and the CG solution is clearly better than HDG for linears by any metric. For cubics, HDG and CG are of similar quality, whereas HDG is again more efficient for sixth order polynomials. One can alternatively also use the combination of FE_DGP and FE_FaceP instead of (FE_DGQ, FE_FaceQ), which do not use tensor product polynomials of degree  [2.x.4948]  but Legendre polynomials of [1.x.1939] degree  [2.x.4949] . There are fewer degrees of freedom on the skeleton variable for FE_FaceP for a given mesh size, but the solution quality (error vs. number of DoFs) is very similar to the results for FE_FaceQ. 

 [2.x.4950]  

One final note on the efficiency comparison: We tried to use general-purpose sparse matrix structures and similar solvers (optimal AMG preconditioners for both without particular tuning of the AMG parameters on any of them) to give a fair picture of the cost versus accuracy of two methods, on a toy example. It should be noted however that geometric multigrid (GMG) for continuous finite elements is about a factor four to five faster for  [2.x.4951]  and  [2.x.4952] . As of 2019, optimal-complexity iterative solvers for HDG are still under development in the research community. Also, there are other implementation aspects for CG available such as fast matrix-free approaches as shown in step-37 that make higher order continuous elements more competitive. Again, it is not clear to the authors of the tutorial whether similar improvements could be made for HDG. We refer to [1.x.1940] for a recent efficiency evaluation. 




[1.x.1941] 

As already mentioned in the introduction, one possibility is to implement another post-processing technique as discussed in the literature. 

A second item that is not done optimally relates to the performance of this program, which is of course an issue in practical applications (weighing in also the better solution quality of (H)DG methods for transport-dominated problems). Let us look at the computing time of the tutorial program and the share of the individual components: 

 [2.x.4953]  

As can be seen from the table, the solver and assembly calls dominate the runtime of the program. This also gives a clear indication of where improvements would make the most sense: 

<ol>    [2.x.4954]  Better linear solvers: We use a BiCGStab iterative solver without   preconditioner, where the number of iteration increases with increasing   problem size (the number of iterations for Q1 elements and global   refinements starts at 35 for the small sizes but increase up to 701 for the   largest size). To do better, one could for example use an algebraic   multigrid preconditioner from Trilinos, or some more advanced variants as   the one discussed in [1.x.1942]. For diffusion-dominated problems such as the problem at hand   with finer meshes, such a solver can be designed that uses the matrix-vector   products from the more efficient ChunkSparseMatrix on the finest level, as   long as we are not working in parallel with MPI. For MPI-parallelized   computations, a standard  [2.x.4955]  can be used. 

   [2.x.4956]  Speed up assembly by pre-assembling parts that do not change from one   cell to another (those that do neither contain variable coefficients nor   mapping-dependent terms).  [2.x.4957]  


examples/step-52/doc/intro.dox 

 [2.x.4958]  

[1.x.1943] 

 [2.x.4959]  In order to run this program, deal.II must be configured to use the UMFPACK sparse direct solver. Refer to the [1.x.1944] for instructions how to do this. 

[1.x.1945] 

[1.x.1946] 

This program shows how to use Runge-Kutta methods to solve a time-dependent problem. It solves a small variation of the heat equation discussed first in step-26 but, since the purpose of this program is only to demonstrate using more advanced ways to interface with deal.II's time stepping algorithms, only solves a simple problem on a uniformly refined mesh. 




[1.x.1947] 

In this example, we solve the one-group time-dependent diffusion approximation of the neutron transport equation (see step-28 for the time-independent multigroup diffusion). This is a model for how neutrons move around highly scattering media, and consequently it is a variant of the time-dependent diffusion equation -- which is just a different name for the heat equation discussed in step-26, plus some extra terms. We assume that the medium is not fissible and therefore, the neutron flux satisfies the following equation: 

[1.x.1948] 

augmented by appropriate boundary conditions. Here,  [2.x.4960]  is the velocity of neutrons (for simplicity we assume it is equal to 1 which can be achieved by simply scaling the time variable),  [2.x.4961]  is the diffusion coefficient,  [2.x.4962]  is the absorption cross section, and  [2.x.4963]  is a source. Because we are only interested in the time dependence, we assume that  [2.x.4964]  and  [2.x.4965]  are constant. 

Since this program only intends to demonstrate how to use advanced time stepping algorithms, we will only look for the solutions of relatively simple problems. Specifically, we are looking for a solution on a square domain  [2.x.4966]  of the form 

[1.x.1949] 

By using quadratic finite elements, we can represent this function exactly at any particular time, and all the error will be due to the time discretization. We do this because it is then easy to observe the order of convergence of the various time stepping schemes we will consider, without having to separate spatial and temporal errors. 

We impose the following boundary conditions: homogeneous Dirichlet for  [2.x.4967]  and  [2.x.4968]  and homogeneous Neumann conditions for  [2.x.4969]  and  [2.x.4970] . We choose the source term so that the corresponding solution is in fact of the form stated above: 

[1.x.1950] 

Because the solution is a sine in time, we know that the exact solution satisfies  [2.x.4971] . Therefore, the error at time  [2.x.4972]  is simply the norm of the numerical solution, i.e.,  [2.x.4973] , and is particularly easily evaluated. In the code, we evaluate the  [2.x.4974]  norm of the vector of nodal values of  [2.x.4975]  instead of the  [2.x.4976]  norm of the associated spatial function, since the former is simpler to compute; however, on uniform meshes, the two are just related by a constant and we can consequently observe the temporal convergence order with either. 




[1.x.1951] 

The Runge-Kutta methods implemented in deal.II assume that the equation to be solved can be written as: 

[1.x.1952] 

On the other hand, when using finite elements, discretized time derivatives always result in the presence of a mass matrix on the left hand side. This can easily be seen by considering that if the solution vector  [2.x.4977]  in the equation above is in fact the vector of nodal coefficients  [2.x.4978]  for a variable of the form 

[1.x.1953] 

with spatial shape functions  [2.x.4979] , then multiplying an equation of the form 

[1.x.1954] 

by test functions, integrating over  [2.x.4980] , substituting  [2.x.4981]  and restricting the test functions to the  [2.x.4982]  from above, then this spatially discretized equation has the form 

[1.x.1955] 

where  [2.x.4983]  is the mass matrix and  [2.x.4984]  is the spatially discretized version of  [2.x.4985]  (where  [2.x.4986]  is typically the place where spatial derivatives appear, but this is not of much concern for the moment given that we only consider time derivatives). In other words, this form fits the general scheme above if we write 

[1.x.1956] 



Runke-Kutta methods are time stepping schemes that approximate  [2.x.4987]  through a particular one-step approach. They are typically written in the form 

[1.x.1957] 

where for the form of the right hand side above 

[1.x.1958] 

Here  [2.x.4988] ,  [2.x.4989] , and  [2.x.4990]  are known coefficients that identify which particular Runge-Kutta scheme you want to use, and  [2.x.4991]  is the time step used. Different time stepping methods of the Runge-Kutta class differ in the number of stages  [2.x.4992]  and the values they use for the coefficients  [2.x.4993] ,  [2.x.4994] , and  [2.x.4995]  but are otherwise easy to implement since one can look up tabulated values for these coefficients. (These tables are often called Butcher tableaus.) 

At the time of the writing of this tutorial, the methods implemented in deal.II can be divided in three categories: <ol>  [2.x.4996]  Explicit Runge-Kutta; in order for a method to be explicit, it is necessary that in the formula above defining  [2.x.4997] ,  [2.x.4998]  does not appear on the right hand side. In other words, these methods have to satisfy  [2.x.4999] .  [2.x.5000]  Embedded (or adaptive) Runge-Kutta; we will discuss their properties below.  [2.x.5001]  Implicit Runge-Kutta; this class of methods require the solution of a possibly nonlinear system the stages  [2.x.5002]  above, i.e., they have  [2.x.5003]  for at least one of the stages  [2.x.5004] .  [2.x.5005]  Many well known time stepping schemes that one does not typically associate with the names Runge or Kutta can in fact be written in a way so that they, too, can be expressed in these categories. They oftentimes represent the lowest-order members of these families. 




[1.x.1959] 

These methods, only require a function to evaluate  [2.x.5006]  but not (as implicit methods) to solve an equation that involves  [2.x.5007]  for  [2.x.5008] . As all explicit time stepping methods, they become unstable when the time step chosen is too large. 

Well known methods in this class include forward Euler, third order Runge-Kutta, and fourth order Runge-Kutta (often abbreviated as RK4). 




[1.x.1960] 

These methods use both a lower and a higher order method to estimate the error and decide if the time step needs to be shortened or can be increased. The term "embedded" refers to the fact that the lower-order method does not require additional evaluates of the function  [2.x.5009]  but reuses data that has to be computed for the high order method anyway. It is, in other words, essentially free, and we get the error estimate as a side product of using the higher order method. 

This class of methods include Heun-Euler, Bogacki-Shampine, Dormand-Prince (ode45 in Matlab and often abbreviated as RK45 to indicate that the lower and higher order methods used here are 4th and 5th order Runge-Kutta methods, respectively), Fehlberg, and Cash-Karp. 

At the time of the writing, only embedded explicit methods have been implemented. 




[1.x.1961] 

Implicit methods require the solution of (possibly nonlinear) systems of the form  [2.x.5010]  for  [2.x.5011]  in each (sub-)timestep. Internally, this is done using a Newton-type method and, consequently, they require that the user provide functions that can evaluate  [2.x.5012]  and  [2.x.5013]  or equivalently  [2.x.5014] . 

The particular form of this operator results from the fact that each Newton step requires the solution of an equation of the form 

[1.x.1962] 

for some (given)  [2.x.5015] . Implicit methods are always stable, regardless of the time step size, but too large time steps of course affect the [1.x.1963] of the solution, even if the numerical solution remains stable and bounded. 

Methods in this class include backward Euler, implicit midpoint, Crank-Nicolson, and the two stage SDIRK method (short for "singly diagonally implicit Runge-Kutta", a term coined to indicate that the diagonal elements  [2.x.5016]  defining the time stepping method are all equal; this property allows for the Newton matrix  [2.x.5017]  to be re-used between stages because  [2.x.5018]  is the same every time). 




[1.x.1964] 

By expanding the solution of our model problem as always using shape functions  [2.x.5019]  and writing 

[1.x.1965] 

we immediately get the spatially discretized version of the diffusion equation as 

[1.x.1966] 

where 

[1.x.1967] 

See also step-24 and step-26 to understand how we arrive here. Boundary terms are not necessary due to the chosen boundary conditions for the current problem. To use the Runge-Kutta methods, we recast this as follows: 

[1.x.1968] 

In the code, we will need to be able to evaluate this function  [2.x.5020]  along with its derivative, 

[1.x.1969] 






[1.x.1970] 

To simplify the problem, the domain is two dimensional and the mesh is uniformly refined (there is no need to adapt the mesh since we use quadratic finite elements and the exact solution is quadratic). Going from a two dimensional domain to a three dimensional domain is not very challenging. However if you intend to solve more complex problems where the mesh must be adapted (as is done, for example, in step-26), then it is important to remember the following issues: 

<ol>  [2.x.5021]  You will need to project the solution to the new mesh when the mesh is changed. Of course,      the mesh      used should be the same from the beginning to the end of each time step,      a question that arises because Runge-Kutta methods use multiple      evaluations of the equations within each time step.  [2.x.5022]  You will need to update the mass matrix and its inverse every time the      mesh is changed.  [2.x.5023]  The techniques for these steps are readily available by looking at step-26. 


examples/step-52/doc/results.dox 



[1.x.1971] 

The point of this program is less to show particular results, but instead to show how it is done. This we have already demonstrated simply by discussing the code above. Consequently, the output the program yields is relatively sparse and consists only of the console output and the solutions given in VTU format for visualization. 

The console output contains both errors and, for some of the methods, the number of steps they performed: 

[1.x.1972] 



As expected the higher order methods give (much) more accurate solutions. We also see that the (rather inaccurate) Heun-Euler method increased the number of time steps in order to satisfy the tolerance. On the other hand, the other embedded methods used a lot less time steps than what was prescribed. 


examples/step-53/doc/intro.dox 

 [2.x.5024]  

[1.x.1973] 

 [2.x.5025]  This program elaborates on concepts of geometry and the classes that implement it. These classes are grouped into the documentation module on  [2.x.5026]  "Manifold description for triangulations". See there for additional information. 

 [2.x.5027]  This tutorial is also available as a Jupyter Python notebook that   uses the deal.II python interface. The notebook is available in the   same directory as the original C++ program. Rendered notebook can also   be viewed on the [1.x.1974]. 


[1.x.1975] 

[1.x.1976] 

Partial differential equations for realistic problems are often posed on domains with complicated geometries. To provide just a few examples, consider these cases: 

- Among the two arguably most important industrial applications for the finite   element method, aerodynamics and more generally fluid dynamics is   one. Computer simulations today are used in the design of every airplane,   car, train and ship. The domain in which the partial differential equation   is posed is, in these cases, the air surrounding the plane with its wings,   flaps and engines; the air surrounding the car with its wheel, wheel wells,   mirrors and, in the case of race cars, all sorts of aerodynamic equipment;   the air surrounding the train with its wheels and gaps between cars. In the   case of ships, the domain is the water surrounding the ship with its rudders   and propellers. 

- The other of the two big applications of the finite element method is   structural engineering in which the domains are bridges, airplane nacelles   and wings, and other solid bodies of often complicated shapes. 

- Finite element modeling is also often used to describe the generation and   propagation of earthquake waves. In these cases, one needs to accurately   represent the geometry of faults in the Earth crust. Since faults intersect,   dip at angles, and are often not completely straight, domains are frequently   very complex. One could cite many more examples of complicated geometries in which one wants to pose and solve a partial differential equation. What this shows is that the "real" world is much more complicated than what we have shown in almost all of the tutorial programs preceding this one. 

This program is therefore devoted to showing how one deals with complex geometries using a concrete application. In particular, what it shows is how we make a mesh fit the domain we want to solve on. On the other hand, what the program does not show is how to create a coarse for a domain. The process to arrive at a coarse mesh is called "mesh generation" and there are a number of high-quality programs that do this much better than we could ever implement. However, deal.II does have the ability to read in meshes in many formats generated by mesh generators and then make them fit a given shape, either by deforming a mesh or refining it a number of times until it fits. The deal.II Frequently Asked Questions page referenced from http://www.dealii.org/ provides resources to mesh generators. 




[1.x.1977] 

Let us assume that you have a complex domain and that you already have a coarse mesh that somehow represents the general features of the domain. Then there are two situations in which it is necessary to describe to a deal.II program the details of your geometry: 

- Mesh refinement: Whenever a cell is refined, it is necessary to introduce   new vertices in the Triangulation. In the simplest case, one assumes that   the objects that make up the Triangulation are straight line segments, a   bi-linear surface or a tri-linear volume. The next vertex is then simply put   into the middle of the old ones. However, for curved boundaries or if we   want to solve a PDE on a curved, lower-dimensional manifold embedded in a   higher-dimensional space, this is insufficient since it will not respect the   actual geometry. We will therefore have to tell Triangulation where to put   new points. 

- Integration: When using higher order finite element methods, it is often   necessary to compute integrals using curved approximations of the boundary,   i.e., describe each edge or face of cells as curves, instead of straight   line segments or bilinear patches. The same is, of course, true when   integrating boundary terms (e.g., inhomogeneous Neumann boundary   conditions). For the purpose of integration, the various Mapping classes   then provide the transformation from the reference cell to the actual cell. 

In both cases, we need a way to provide information about the geometry of the domain at the level of an individual cell, its faces and edges. This is where the Manifold class comes into play. Manifold is an abstract base class that only defines an interface by which the Triangulation and Mapping classes can query geometric information about the domain. Conceptually, Manifold sees the world in a way not dissimilar to how the mathematical subdiscipline geometry sees it: a domain is essentially just a collection of points that is somehow equipped with the notion of a distance between points so that we can obtain a point "in the middle" of some other points. 

deal.II provides a number of classes that implement the interface provided by Manifold for a variety of common geometries. On the other hand, in this program we will consider only a very common and much simpler case, namely the situation where (a part of) the domain we want to solve on can be described by transforming a much simpler domain (we will call this the "reference domain"). In the language of mathematics, this means that the (part of the) domain is a [1.x.1978]. Charts are described by a smooth function that maps from the simpler domain to the chart (the "push-forward" function) and its inverse (the "pull-back" function). If the domain as a whole is not a chart (e.g., the surface of a sphere), then it can often be described as a collection of charts (e.g., the northern hemisphere and the southern hemisphere are each charts) and the domain can then be describe by an [1.x.1979]. 

If a domain can be decomposed into an atlas, all we need to do is provide the pull-back and push-forward functions for each of the charts. In deal.II, this means providing a class derived from ChartManifold, and this is precisely what we will do in this program. 




[1.x.1980] 

To illustrate how one describes geometries using charts in deal.II, we will consider a case that originates in an application of the [1.x.1981], using a data set provided by D. Sarah Stamps. In the concrete application, we were interested in describing flow in the Earth mantle under the [1.x.1982], a zone where two continental plates drift apart. Not to beat around the bush, the geometry we want to describe looks like this: 

 [2.x.5028]  

In particular, though you cannot see this here, the top surface is not just colored by the elevation but is, in fact, deformed to follow the correct topography. While the actual application is not relevant here, the geometry is. The domain we are interested in is a part of the Earth that ranges from the surface to a depth of 500km, from 26 to 35 degrees East of the Greenwich meridian, and from 5 degrees North of the equator to 10 degrees South. 

This description of the geometry suggests to start with a box  [2.x.5029]  (measured in degrees, degrees, and meters) and to provide a map  [2.x.5030]  so that  [2.x.5031]  where  [2.x.5032]  is the domain we seek.  [2.x.5033]  is then a chart,  [2.x.5034]  the pull-back operator, and  [2.x.5035]  the push-forward operator. If we need a point  [2.x.5036]  that is the "average" of other points  [2.x.5037] , the ChartManifold class then first applies the pull-back to obtain  [2.x.5038] , averages these to a point  [2.x.5039]  and then computes  [2.x.5040] . 

Our goal here is therefore to implement a class that describes  [2.x.5041]  and  [2.x.5042] . If Earth was a sphere, then this would not be difficult: if we denote by  [2.x.5043]  the points of  [2.x.5044]  (i.e., longitude counted eastward, latitude counted northward, and elevation relative to zero depth), then 

[1.x.1983] 

provides coordinates in a Cartesian coordinate system, where  [2.x.5045]  is the radius of the sphere. However, the Earth is not a sphere: 

<ol>  [2.x.5046]  It is flattened at the poles and larger at the equator: the semi-major axis   is approximately 22km longer than the semi-minor axis. We will account for   this using the [1.x.1984]   reference standard for the Earth shape. The formula used in WGS 84 to obtain   a position in Cartesian coordinates from longitude, latitude, and elevation   is 

[1.x.1985] 

  where  [2.x.5047] , and radius and   ellipticity are given by  [2.x.5048] . In this formula,   we assume that the arguments to sines and cosines are evaluated in degree, not   radians (though we will have to change this assumption in the code). 

 [2.x.5049]  It has topography in the form of mountains and valleys. We will account for   this using real topography data (see below for a description of where   this data comes from). Using this data set, we can look up elevations on a   latitude-longitude mesh laid over the surface of the Earth. Starting with   the box  [2.x.5050] , we will therefore   first stretch it in vertical direction before handing it off to the WGS 84   function: if  [2.x.5051]  is the height at longitude  [2.x.5052]    and latitude  [2.x.5053] , then we define 

[1.x.1986] 

  Using this function, the top surface of the box  [2.x.5054]  is displaced to the   correct topography, the bottom surface remains where it was, and points in   between are linearly interpolated.  [2.x.5055]  

Using these two functions, we can then define the entire push-forward function  [2.x.5056]  as 

[1.x.1987] 

In addition, we will have to define the inverse of this function, the pull-back operation, which we can write as 

[1.x.1988] 

We can obtain one of the components of this function by inverting the formula above: 

[1.x.1989] 

Computing  [2.x.5057]  is also possible though a lot more awkward. We won't show the formula here but instead only provide the implementation in the program. 




[1.x.1990] 

There are a number of issues we need to address in the program. At the largest scale, we need to write a class that implements the interface of ChartManifold. This involves a function  [2.x.5058]  that takes a point in the reference domain  [2.x.5059]  and transform it into real space using the function  [2.x.5060]  outlined above, and its inverse function  [2.x.5061]  implementing  [2.x.5062] . We will do so in the  [2.x.5063]  class below that looks, in essence, like this: 

[1.x.1991] 



The transformations above have two parts: the WGS 84 transformations and the topography transformation. Consequently, the  [2.x.5064]  class will have additional (non-virtual) member functions  [2.x.5065]  and  [2.x.5066]  that implement these two pieces, and corresponding pull back functions. 

The WGS 84 transformation functions are not particularly interesting (even though the formulas they implement are impressive). The more interesting part is the topography transformation. Recall that for this, we needed to evaluate the elevation function  [2.x.5067] . There is of course no formula for this: Earth is what it is, the best one can do is look up the altitude from some table. This is, in fact what we will do. 

The data we use was originally created by the  [1.x.1992], was downloaded from the US Geologic Survey (USGS) and processed by D. Sarah Stamps who also wrote the initial version of the WGS 84 transformation functions. The topography data so processed is stored in a file  [2.x.5068]  that, when unpacked looks like this: 

[1.x.1993] 

The data is formatted as  [2.x.5069]  where the first two columns are provided in degrees North of the equator and degrees East of the Greenwich meridian. The final column is given in meters above the WGS 84 zero elevation. 

In the transformation functions, we need to evaluate  [2.x.5070]  for a given longitude  [2.x.5071]  and latitude  [2.x.5072] . In general, this data point will not be available and we will have to interpolate between adjacent data points. Writing such an interpolation routine is not particularly difficult, but it is a bit tedious and error prone. Fortunately, we can somehow shoehorn this data set into an existing class:  [2.x.5073]  . Unfortunately, the class does not fit the bill quite exactly and so we need to work around it a bit. The problem comes from the way we initialize this class: in its simplest form, it takes a stream of values that it assumes form an equispaced mesh in the  [2.x.5074]  plane (or, here, the  [2.x.5075]  plane). Which is what they do here, sort of: they are ordered latitude first, longitude second; and more awkwardly, the first column starts at the largest values and counts down, rather than the usual other way around. 

Now, while tutorial programs are meant to illustrate how to code with deal.II, they do not necessarily have to satisfy the same quality standards as one would have to do with production codes. In a production code, we would write a function that reads the data and (i) automatically determines the extents of the first and second column, (ii) automatically determines the number of data points in each direction, (iii) does the interpolation regardless of the order in which data is arranged, if necessary by switching the order between reading and presenting it to the  [2.x.5076]  class. 

On the other hand, tutorial programs are best if they are short and demonstrate key points rather than dwell on unimportant aspects and, thereby, obscure what we really want to show. Consequently, we will allow ourselves a bit of leeway: 

- since this program is intended solely for a particular geometry around the area   of the East-African rift and since this is precisely the area described by the data   file, we will hardcode in the program that there are    [2.x.5077]  pieces of data; 

- we will hardcode the boundaries of the data    [2.x.5078] ; 

- we will lie to the  [2.x.5079]  class: the class will   only see the data in the last column of this data file, and we will pretend that   the data is arranged in a way that there are 1139 data points in the first   coordinate direction that are arranged in [1.x.1994] order but in an   interval  [2.x.5080]  (not the negated bounds). Then,   when we need to look something up for a latitude  [2.x.5081] , we can ask the   interpolating table class for a value at  [2.x.5082] . With this little   trick, we can avoid having to switch around the order of data as read from   file. 

All of this then calls for a class that essentially looks like this: 

[1.x.1995] 



Note how the  [2.x.5083]  function negates the latitude. It also switches from the format  [2.x.5084]  that we use everywhere else to the latitude-longitude format used in the table. Finally, it takes its arguments in radians as that is what we do everywhere else in the program, but then converts them to the degree-based system used for table lookup. As you will see in the implementation below, the function has a few more (static) member functions that we will call in the initialization of the  [2.x.5085]  member variable: the class type of this variable has a constructor that allows us to set everything right at construction time, rather than having to fill data later on, but this constructor takes a number of objects that can't be constructed in-place (at least not in C++98). Consequently, the construction of each of the objects we want to pass in the initialization happens in a number of static member functions. 

Having discussed the general outline of how we want to implement things, let us go to the program and show how it is done in practice. 


examples/step-53/doc/results.dox 



[1.x.1996] 

Running the program produces a mesh file  [2.x.5086]  that we can visualize with any of the usual visualization programs that can read the VTU file format. If one just looks at the mesh itself, it is actually very difficult to see anything that doesn't just look like a perfectly round piece of a sphere (though if one modified the program so that it does produce a sphere and looked at them at the same time, the difference between the overall sphere and WGS 84 shape is quite apparent). Apparently, Earth is actually quite a flat place. Of course we already know this from satellite pictures. However, we can tease out something more by coloring cells by their volume. This both produces slight variations in hue along the top surface and something for the visualization programs to apply their shading algorithms to (because the top surfaces of the cells are now no longer just tangential to a sphere but tilted): 

 [2.x.5087]  

Yet, at least as far as visualizations are concerned, this is still not too impressive. Rather, let us visualize things in a way so that we show the actual elevation along the top surface. In other words, we want a picture like this, with an incredible amount of detail: 

 [2.x.5088]  

A zoom-in of this picture shows the vertical displacement quite clearly (here, looking from the West-Northwest over the rift valley, the triple peaks of [1.x.1997], [1.x.1998], and [1.x.1999] in the [1.x.2000], [1.x.2001] and toward the great flatness of [1.x.2002]): 

 [2.x.5089]  


These image were produced with three small modifications: <ol>    [2.x.5090]  An additional seventh mesh refinement towards the top surface for the   first of these two pictures, and a total of nine for the second. In the   second image, the horizontal mesh size is approximately 1.5km, and just   under 1km in vertical direction. (The picture was also created using a   more resolved data set; however, it is too big to distribute as part of   the tutorial.) 

   [2.x.5091]  The addition of the following function that, given a point    [2.x.5092]  computes the elevation by converting the point to   reference WGS 84 coordinates and only keeping the depth variable (the   function is, consequently, a simplified version of the    [2.x.5093]  function): 

[1.x.2003] 



   [2.x.5094] Adding the following piece to the bottom of the  [2.x.5095]  function: 

[1.x.2004] 

 [2.x.5096]  This last piece of code first creates a  [2.x.5097]  finite element space on the mesh. It then (ab)uses  [2.x.5098]  to evaluate the elevation function for every node at the top boundary (the one with boundary indicator 5). We here wrap the call to  [2.x.5099]  with the ScalarFunctionFromFunctionObject class to make a regular C++ function look like an object of a class derived from the Function class that we want to use in  [2.x.5100]  Having so gotten a list of degrees of freedom located at the top boundary and corresponding elevation values, we just go down this list and set these elevations in the  [2.x.5101]  vector (leaving all interior degrees of freedom at their original zero value). This vector is then output using DataOut as usual and can be visualized as shown above. 




[1.x.2005] 

If you zoomed in on the mesh shown above and looked closely enough, you would find that at hanging nodes, the two small edges connecting to the hanging nodes are not in exactly the same location as the large edge of the neighboring cell. This can be shown more clearly by using a different surface description in which we enlarge the vertical topography to enhance the effect (courtesy of Alexander Grayver): 

 [2.x.5102]  

So what is happening here? Partly, this is only a result of visualization, but there is an underlying real cause as well: 

 [2.x.5103]     [2.x.5104] When you visualize a mesh using any of the common visualization   programs, what they really show you is just a set of edges that are plotted   as straight lines in three-dimensional space. This is so because almost all   data file formats for visualizing data only describe hexahedral cells as a   collection of eight vertices in 3d space, and do not allow to any more   complicated descriptions. (This is the main reason why    [2.x.5105]  takes an argument that can be set to something   larger than one.) These linear edges may be the edges of the cell you do   actual computations on, or they may not, depending on what kind of mapping   you use when you do your integrations using FEValues. By default, of course,   FEValues uses a linear mapping (i.e., an object of class MappingQ1) and in   that case a 3d cell is indeed described exclusively by its 8 vertices and   the volume it fills is a trilinear interpolation between these points,   resulting in linear edges. But, you could also have used tri-quadratic,   tri-cubic, or even higher order mappings and in these cases the volume of   each cell will be bounded by quadratic, cubic or higher order polynomial   curves. Yet, you only get to see these with linear edges in the   visualization program because, as mentioned, file formats do not allow to   describe the real geometry of cells. 

   [2.x.5106] That said, let us for simplicity assume that you are indeed using a   trilinear mapping, then the image shown above is a faithful representation   of the cells on which you form your integrals. In this case, indeed the   small cells at a hanging nodes do not, in general, snugly fit against the   large cell but leave a gap or may intersect the larger cell. Why is this?   Because when the triangulation needs a new vertex on an edge it wants to   refine, it asks the manifold description where this new vertex is supposed   to be, and the manifold description duly returns such a point by (in the   case of a geometry derived from ChartManifold) pulling the adjacent points   of the line back to the reference domain, averaging their locations, and   pushing forward this new location to the real domain. But this new location   is not usually along a straight line (in real space) between the adjacent   vertices and consequently the two small straight lines forming the refined   edge do not lie exactly on the one large straight line forming the unrefined   side of the hanging node.  [2.x.5107]  

The situation is slightly more complicated if you use a higher order mapping using the MappingQ class, but not fundamentally different. Let's take a quadratic mapping for the moment (nothing fundamental changes with even higher order mappings). Then you need to imagine each edge of the cells you integrate on as a quadratic curve despite the fact that you will never actually see it plotted that way by a visualization program. But imagine it that way for a second. So which quadratic curve does MappingQ take? It is the quadratic curve that goes through the two vertices at the end of the edge as well as a point in the middle that it queries from the manifold. In the case of the long edge on the unrefined side, that's of course exactly the location of the hanging node, so the quadratic curve describing the long edge does go through the hanging node, unlike in the case of the linear mapping. But the two small edges are also quadratic curves; for example, the left small edge will go through the left vertex of the long edge and the hanging node, plus a point it queries halfway in between from the manifold. Because, as before, the point the manifold returns halfway along the left small edge is rarely exactly on the quadratic curve describing the long edge, the quadratic short edge will typically not coincide with the left half of the quadratic long edge, and the same is true for the right short edge. In other words, again, the geometries of the large cell and its smaller neighbors at hanging nodes do not touch snuggly. 

This all begs two questions: first, does it matter, and second, could this be fixed. Let us discuss these in the following: 

 [2.x.5108]     [2.x.5109] Does it matter? It is almost certainly true that this depends on the   equation you are solving. For example, it is known that solving the Euler   equations of gas dynamics on complex geometries requires highly accurate   boundary descriptions to ensure convergence of quantities that are measure   the flow close to the boundary. On the other hand, equations with elliptic   components (e.g., the Laplace or Stokes equations) are typically rather   forgiving of these issues: one does quadrature anyway to approximate   integrals, and further approximating the geometry may not do as much harm as   one could fear given that the volume of the overlaps or gaps at every   hanging node is only  [2.x.5110]  even with a linear mapping and  [2.x.5111]  for a mapping of degree  [2.x.5112] . (You can see this by considering   that in 2d the gap/overlap is a triangle with base  [2.x.5113]  and height  [2.x.5114] ; in 3d, it is a pyramid-like structure with base area  [2.x.5115]  and   height  [2.x.5116] . Similar considerations apply for higher order mappings   where the height of the gaps/overlaps is  [2.x.5117] .) In other words,   if you use a linear mapping with linear elements, the error in the volume   you integrate over is already at the same level as the integration error   using the usual Gauss quadrature. Of course, for higher order elements one   would have to choose matching mapping objects. 

  Another point of view on why it is probably not worth worrying too much   about the issue is that there is certainly no narrative in the community of   numerical analysts that these issues are a major concern one needs to watch   out for when using complex geometries. If it does not seem to be discussed   often among practitioners, if ever at all, then it is at least not something   people have identified as a common problem. 

  This issue is not dissimilar to having hanging nodes at curved boundaries   where the geometry description of the boundary typically pulls a hanging   node onto the boundary whereas the large edge remains straight, making the   adjacent small and large cells not match each other. Although this behavior   existed in deal.II since its beginning, 15 years before manifold   descriptions became available, it did not ever come up in mailing list   discussions or conversations with colleagues. 

   [2.x.5118] Could it be fixed? In principle, yes, but it's a complicated   issue. Let's assume for the moment that we would only ever use the MappingQ1   class, i.e., linear mappings. In that case, whenever the triangulation class   requires a new vertex along an edge that would become a hanging node, it   would just take the mean value of the adjacent vertices [1.x.2006], i.e., without asking the manifold description. This way, the   point lies on the long straight edge and the two short straight edges would   match the one long edge. Only when all adjacent cells have been refined and   the point is no longer a hanging node would we replace its coordinates by   coordinates we get by a manifold. This may be awkward to implement, but it   would certainly be possible. 

  The more complicated issue arises because people may want to use a higher   order MappingQ object. In that case, the Triangulation class may freely   choose the location of the hanging node (because the quadratic curve for the   long edge can be chosen in such a way that it goes through the hanging node)   but the MappingQ class, when determining the location of mid-edge points   must make sure that if the edge is one half of a long edge of a neighboring   coarser cell, then the midpoint cannot be obtained from the manifold but   must be chosen along the long quadratic edge. For cubic (and all other odd)   mappings, the matter is again a bit complicated because one typically   arranges the cubic edge to go through points 1/3 and 2/3 along the edge, and   thus necessarily through the hanging node, but this could probably be worked   out. In any case, even then, there are two problems with this: 

  - When refining the triangulation, the Triangulation class can not know what     mapping will be used. In fact it is not uncommon for a triangulation to be     used differently in different contexts within the same program. If the     mapping used determines whether we can freely choose a point or not, how,     then, should the triangulation locate new vertices? 

  - Mappings are purely local constructs: they only work on a cell in     isolation, and this is one of the important features of the finite element     method. Having to ask whether one of the vertices of an edge is a hanging     node requires querying the neighborhood of a cell; furthermore, such a     query does not just involve the 6 face neighbors of a cell in 3d, but may     require traversing a possibly very large number of other cells that     connect to an edge. Even if it can be done, one still needs to do     different things depending on how the neighborhood looks like, producing     code that is likely very complex, hard to maintain, and possibly slow. 

  Consequently, at least for the moment, none of these ideas are   implemented. This leads to the undesirable consequence of discontinuous   geometries, but, as discussed above, the effects of this do not appear to   pose problem in actual practice. 

 [2.x.5119]  


examples/step-54/doc/intro.dox 

 [2.x.5120]  

[1.x.2007] 

 [2.x.5121]  This program elaborates on concepts of industrial geometry, using tools that interface with the OpenCASCADE library (http://www.opencascade.org) that allow the specification of arbitrary IGES files to describe the boundaries for your geometries. 

 [2.x.5122]  

[1.x.2008] 

[1.x.2009] 


In some of the previous tutorial programs (step-1, step-3, step-5, step-6 and step-49 among others) we have learned how to use the mesh refinement methods provided in deal.II. These tutorials have shown how to employ such tools to produce a fine grid for a single simulation, as done in step-3; or to start from a coarse grid and carry out a series of simulations on adaptively refined grids, as is the case of step-6. Regardless of which approach is taken, the mesh refinement requires a suitable geometrical description of the computational domain boundary in order to place, at each refinement, the new mesh nodes onto the boundary surface. For instance, step-5 shows how creating a circular grid automatically attaches a circular manifold object to the computational domain, so that the faces lying on the boundary are refined onto the circle. step-53 shows how to do this with a Manifold defined by experimentally obtained data. But, at least as far as elementary boundary shapes are concerned, deal.II really only provides circles, spheres, boxes and other elementary combinations. In this tutorial, we will show how to use a set of classes developed to import arbitrary CAD geometries, assign them to the desired boundary of the computational domain, and refine a computational grid on such complex shapes. 




[1.x.2010] 

In the most common industrial practice, the geometrical models of arbitrarily shaped objects are realized by means of Computer Aided Design (CAD) tools. The use of CAD modelers has spread in the last decades, as they allow for the generation of a full virtual model of each designed object, which through a computer can be visualized, inspected, and analyzed in its finest details well before it is physically crafted.  From a mathematical perspective, the engine lying under the hood of CAD modelers is represented by analytical geometry, and in particular by parametric curves and surfaces such as B-splines and NURBS that are rich enough that they can represent most surfaces of practical interest.  Once a virtual model is ready, all the geometrical features of the desired object are stored in files which materially contain the coefficients of the parametric surfaces and curves composing the object. Depending on the specific CAD tool used to define the geometrical model, there are of course several different file formats in which the information of a CAD model can be organized. To provide a common ground to exchange data across CAD tools, the U.S. National Bureau of Standards published in 1980 the Initial Graphics Exchange Representation (IGES) neutral file format, which is used in this example. 

[1.x.2011] 

To import and interrogate CAD models, the deal.II library implements a series of wrapper functions for the OpenCASCADE open source library for CAD modeling. These functions allow to import IGES files into OpenCASCADE native objects, and wrap them inside a series of Manifold classes. 

Once imported from an IGES file, the model is stored in a  [2.x.5123] , which is the generic topological entity defined in the OpenCASCADE framework. From a  [2.x.5124] , it is then possible to access all the sub-shapes (such as vertices, edges and faces) composing it, along with their geometrical description. In the deal.II framework, the topological entities composing a shape are used to create a corresponding Manifold representation. In step-6 we saw how to use  [2.x.5125]  to create a hyper sphere, which automatically attaches a SphericalManifold to all boundary faces. This guarantees that boundary faces stay on a sphere or circle during mesh refinement. The functions of the CAD modeling interface have been designed to retain the same structure, allowing the user to build a projector object using the imported CAD shapes, maintaining the same procedure we used in other tutorial programs, i.e., assigning such projector object to cells, faces or edges of a coarse mesh. At each refinement cycle, the new mesh nodes will be then automatically generated by projecting a midpoint of an existing object onto the specified geometry. 

Differently from a spherical or circular boundary, a boundary with a complex geometry poses problems as to where it is best to place the new nodes created upon refinement on the prescribed shape. PolarManifold, for example, transforms the surrounding points to polar coordinates, calculates the average in that coordinate system (for each coordinate individually) and finally transforms the point back to Cartesian coordinates. 

In the case of an arbitrary and complex shape though, an appropriate choice for the placement of a new node cannot be identified that easily. The OpenCASCADE wrappers in deal.II provide several projector classes that employ different projection strategies. A first projector, implemented in the  [2.x.5126]  class, is to be used only for edge refinement. It is built assigning it a topological shape of dimension one, either a  [2.x.5127]  (which is a compound shape, made of several connected  [2.x.5128] s) and refines a mesh edge finding the new vertex as the point splitting in two even parts the curvilinear length of the CAD curve portion that lies between the vertices of the original edge. 

 [2.x.5129]  


A different projection strategy has been implemented in the  [2.x.5130]  class. The  [2.x.5131]  assigned at construction time can be arbitrary (a collection of shapes, faces, edges or a single face or edge will all work). The new cell nodes are first computed by averaging the surrounding points in the same way as FlatManifold does. In a second step, all the new nodes will be projected onto the  [2.x.5132]  along the direction normal to the shape. If no normal projection is available, the point which is closest to the shape---typically lying on the shape boundary---is selected.  If the shape is composed of several sub-shapes, the projection is carried out onto every single sub-shape, and the closest projection point is selected. 

 [2.x.5133]   [2.x.5134]  

As we are about to experience, for some shapes, setting the projection direction as that normal to the CAD surface will not lead to surface mesh elements of suitable quality. This is because the direction normal to the CAD surface has in principle nothing to do with the direction along which the mesh needs the new nodes to be located. The  [2.x.5135]  class, in this case, can help. This class is constructed assigning a  [2.x.5136]  (containing at least a face) and a direction along which all the projections will be carried out. New points will be computed by first averaging the surrounding points (as in the FlatManifold case), and then taking the closest intersection between the topological shape and the line passing through the resulting point, along the direction used at construction time.  In this way, the user will have a higher control on the projection direction to be enforced to ensure good mesh quality. 

 [2.x.5137]  


Of course the latter approach is effective only when the orientation of the surface is rather uniform, so that a single projection direction can be identified. In cases in which the surface direction is approaching the projection direction, it is even possible that the directional projection is not found. To overcome these problems, the  [2.x.5138]  class implements a third projection algorithm. The  [2.x.5139]  class is built assigning a  [2.x.5140]  (containing at least one face) to the constructor, and works exactly like a  [2.x.5141]  But, as the name of the class suggests,  [2.x.5142]  tries to come up with a suitable estimate of the direction normal to the mesh elements to be refined, and uses it for the projection of the new nodes onto the CAD surface. If we consider a mesh edge in a 2D space, the direction of its axis is a direction along which to split it in order to give rise to two new cells of the same length. We here extended this concept in 3D, and project all new nodes in a direction that approximates the cell normal. 

In the next figure, which is inspired by the geometry considered in this tutorial, we make an attempt to compare the behavior of the three projectors considered. As can be seen on the left, given the original cell (in blue), the new point found with the normal projection is in a position which does not allow for the generation of evenly spaced new elements (in red). The situation will get worse in further refinement steps.  Since the geometry we considered is somehow perpendicular to the horizontal direction, the directional projection (central image) defined with horizontal direction as the projection direction, does a rather good job in getting the new mesh point. Yet, since the surface is almost horizontal at the bottom of the picture, we can expect problems in those regions when further refinement steps are carried out. Finally, the picture on the right shows that a node located on the cell axis will result in two new cells having the same length. Of course the situation in 3D gets a little more complicated than that described in this simple 2D case. Nevertheless, the results of this test confirm that the normal to the mesh direction is the best approach among the three tested, when arbitrarily shaped surfaces are considered, and unless you have a geometry for which a more specific approach is known to be appropriate. 


 [2.x.5143]  




[1.x.2012] 

In this program, we will consider creating a surface mesh for a real geometry describing the bow of a ship (this geometry is frequently used in CAD and mesh generation comparisons and is freely available). The surface mesh we get from this could then be used to solve a boundary element equation to simulate the flow of water around the ship (in a way similar to step-34) but we will not try to do this here. To already give you an idea of the geometry we consider, here is a picture: 

 [2.x.5144]  

In the program, we read both the geometry and a coarse mesh from files, and then employ several of the options discussed above to place new vertices for a sequence of mesh refinement steps. 


examples/step-54/doc/results.dox 



[1.x.2013] 

The program execution produces a series of mesh files  [2.x.5145]  that we can visualize with any of the usual visualization programs that can read the VTK file format. 

The following table illustrates the results obtained employing the normal projection strategy. The first two rows of the table show side views of the grids obtained for progressive levels of refinement, overlain on a very fine rendering of the exact geometry. The dark and light red areas simply indicate whether the current mesh or the fine geometry is closer to the observer; the distinction does not carry any particularly deep meaning. The last row of pictures depict front views (mirrored to both sides of the geometry) of the same grids shown in the second row. 


 [2.x.5146]  

As can be seen in the pictures---and as we anticipated---the normal refinement strategy is unable to produce nicely shaped elements when applied to surfaces with significant curvature changes. This is particularly apparent at the bulb of the hull where all new points have been placed in the upper part of the bulb and the lower part remains completely unresolved. 

The following table, which is arranged as the previous one, illustrates the results obtained adopting the directional projection approach, in which the projection direction selected was the y-axis (which is indicated with a small yellow arrow at the bottom left of each image). 


 [2.x.5147]  

The images confirm that the quality of the mesh obtained with a directional projection is sensibly higher than that obtained projecting along the surface normal. Yet, a number of elements elongated in the y-direction are observed around the bottom of the bulb, where the surface is almost parallel to the direction chosen for the projection. 

The final test shows results using instead the projection normal to the faces: 

 [2.x.5148]  

The pictures confirm that the normal to mesh projection approach leads to grids that remain evenly spaced throughtout the refinement steps. At the same time, these meshes represent rather well the original geometry even in the bottom region of the bulb, which is not well recovered employing the directional projector or the normal projector. 


examples/step-55/doc/intro.dox 

 [2.x.5149]  

[1.x.2014] 




 [2.x.5150]  As a prerequisite of this program, you need to have PETSc or Trilinos and the p4est library installed. The installation of deal.II together with these additional libraries is described in the [1.x.2015] file. 

[1.x.2016] 

[1.x.2017] 

Building on step-40, this tutorial shows how to solve linear PDEs with several components in parallel using MPI with PETSc or Trilinos for the linear algebra. For this, we return to the Stokes equations as discussed in step-22. The motivation for writing this tutorial is to provide an intermediate step (pun intended) between step-40 (parallel Laplace) and step-32 (parallel coupled Stokes with Boussinesq for a time dependent problem). 

The learning outcomes for this tutorial are: 

- You are able to solve PDEs with several variables in parallel and can   apply this to different problems. 

- You understand the concept of optimal preconditioners and are able to check   this for a particular problem. 

- You are able to construct manufactured solutions using the free computer   algreba system SymPy (https://sympy.org). 

- You can implement various other tasks for parallel programs: error   computation, writing graphical output, etc. 

- You can visualize vector fields, stream lines, and contours of vector   quantities. 

We are solving for a velocity  [2.x.5151]  and pressure  [2.x.5152]  that satisfy the Stokes equation, which reads 

[1.x.2018] 






[1.x.2019] 

Make sure that you read (even better: try) what is described in "Block Schur complement preconditioner" in the "Possible Extensions" section in step-22. Like described there, we are going to solve the block system using a Krylov method and a block preconditioner. 

Our goal here is to construct a very simple (maybe the simplest?) optimal preconditioner for the linear system. A preconditioner is called "optimal" or "of optimal complexity", if the number of iterations of the preconditioned system is independent of the mesh size  [2.x.5153] . You can extend that definition to also require indepence of the number of processors used (we will discuss that in the results section), the computational domain and the mesh quality, the test case itself, the polynomial degree of the finite element space, and more. 

Why is a constant number of iterations considered to be "optimal"? Assume the discretized PDE gives a linear system with N unknowns. Because the matrix coming from the FEM discretization is sparse, a matrix-vector product can be done in O(N) time. A preconditioner application can also only be O(N) at best (for example doable with multigrid methods). If the number of iterations required to solve the linear system is independent of  [2.x.5154]  (and therefore N), the total cost of solving the system will be O(N). It is not possible to beat this complexity, because even looking at all the entries of the right-hand side already takes O(N) time. For more information see  [2.x.5155] , Chapter 2.5 (Multigrid). 

The preconditioner described here is even simpler than the one described in step-22 and will typically require more iterations and consequently time to solve. When considering preconditioners, optimality is not the only important metric. But an optimal and expensive preconditioner is typically more desirable than a cheaper, non-optimal one. This is because, eventually, as the mesh size becomes smaller and smaller and linear problems become bigger and bigger, the former will eventually beat the latter. 

[1.x.2020] 

We precondition the linear system 

[1.x.2021] 



with the block diagonal preconditioner 

[1.x.2022] 

where  [2.x.5156]  is the Schur complement. 

With this choice of  [2.x.5157] , assuming that we handle  [2.x.5158]  and  [2.x.5159]  exactly (which is an "idealized" situation), the preconditioned linear system has three distinct eigenvalues independent of  [2.x.5160]  and is therefore "optimal".  See section 6.2.1 (especially p. 292) in  [2.x.5161] . For comparison, using the ideal version of the upper block-triangular preconditioner in step-22 (also used in step-56) would have all eigenvalues be equal to one. 

We will use approximations of the inverse operations in  [2.x.5162]  that are (nearly) independent of  [2.x.5163] . In this situation, one can again show, that the eigenvalues are independent of  [2.x.5164] . For the Krylov method we choose MINRES, which is attractive for the analysis (iteration count is proven to be independent of  [2.x.5165] , see the remainder of the chapter 6.2.1 in the book mentioned above), great from the computational standpoint (simpler and cheaper than GMRES for example), and applicable (matrix and preconditioner are symmetric). 

For the approximations we will use a CG solve with the mass matrix in the pressure space for approximating the action of  [2.x.5166] . Note that the mass matrix is spectrally equivalent to  [2.x.5167] . We can expect the number of CG iterations to be independent of  [2.x.5168] , even with a simple preconditioner like ILU. 

For the approximation of the velocity block  [2.x.5169]  we will perform a single AMG V-cycle. In practice this choice is not exactly independent of  [2.x.5170] , which can explain the slight increase in iteration numbers. A possible explanation is that the coarsest level will be solved exactly and the number of levels and size of the coarsest matrix is not predictable. 




[1.x.2023] 

We will construct a manufactured solution based on the classical Kovasznay problem, see  [2.x.5171] . Here is an image of the solution colored by the x velocity including streamlines of the velocity: 

  [2.x.5172]  

We have to cheat here, though, because we are not solving the non-linear Navier-Stokes equations, but the linear Stokes system without convective term. Therefore, to recreate the exact same solution, we use the method of manufactured solutions with the solution of the Kovasznay problem. This will effectively move the convective term into the right-hand side  [2.x.5173] . 

The right-hand side is computed using the script "reference.py" and we use the exact solution for boundary conditions and error computation. 


examples/step-55/doc/results.dox 



[1.x.2024] 

As expected from the discussion above, the number of iterations is independent of the number of processors and only very slightly dependent on  [2.x.5174] : 

 [2.x.5175]  

 [2.x.5176]  

While the PETSc results show a constant number of iterations, the iterations increase when using Trilinos. This is likely because of the different settings used for the AMG preconditioner. For performance reasons we do not allow coarsening below a couple thousand unknowns. As the coarse solver is an exact solve (we are using LU by default), a change in number of levels will influence the quality of a V-cycle. Therefore, a V-cycle is closer to an exact solver for smaller problem sizes. 

[1.x.2025] 

[1.x.2026] 

[1.x.2027] 

Play with the smoothers, smoothing steps, and other properties for the Trilinos AMG to achieve an optimal preconditioner. 

[1.x.2028] 

This change requires changing the outer solver to GMRES or BiCGStab, because the system is no longer symmetric. 

You can prescribe the exact flow solution as  [2.x.5177]  in the convective term  [2.x.5178] . This should give the same solution as the original problem, if you set the right hand side to zero. 

[1.x.2029] 

So far, this tutorial program refines the mesh globally in each step. Replacing the code in  [2.x.5179]  by something like 

[1.x.2030] 

makes it simple to explore adaptive mesh refinement. 


examples/step-56/doc/intro.dox 

[1.x.2031] 

 [2.x.5180]  

[1.x.2032] 

[1.x.2033] 

[1.x.2034] 

The purpose of this tutorial is to create an efficient linear solver for the Stokes equation and compare it to alternative approaches.  Here, we will use FGMRES with geometric multigrid as a preconditioner velocity block, and we will show in the results section that this is a fundamentally better approach than the linear solvers used in step-22 (including the scheme described in "Possible Extensions").  Fundamentally, this is because only with multigrid it is possible to get  [2.x.5181]  solve time, where  [2.x.5182]  is the number of unknowns of the linear system. Using the Timer class, we collect some statistics to compare setup times, solve times, and number of iterations. We also compute errors to make sure that what we have implemented is correct. 

Let  [2.x.5183]  and  [2.x.5184] . The Stokes equations read as follows in non-dimensionalized form: 

[1.x.2035] 



Note that we are using the deformation tensor instead of  [2.x.5185]  (a detailed description of the difference between the two can be found in step-22, but in summary, the deformation tensor is more physical as well as more expensive). 

[1.x.2036] 

The weak form of the discrete equations naturally leads to the following linear system for the nodal values of the velocity and pressure fields: 

[1.x.2037] 



Our goal is to compare several solution approaches.  While step-22 solves the linear system using a "Schur complement approach" in two separate steps, we instead attack the block system at once using FMGRES with an efficient preconditioner, in the spirit of the approach outlined in the "Results" section of step-22. The idea is as follows: if we find a block preconditioner  [2.x.5186]  such that the matrix 

[1.x.2038] 



is simple, then an iterative solver with that preconditioner will converge in a few iterations. Notice that we are doing right preconditioning here.  Using the Schur complement  [2.x.5187] , we find that 

[1.x.2039] 



is a good choice. Let  [2.x.5188]  be an approximation of  [2.x.5189]  and  [2.x.5190]  of  [2.x.5191] , we see 

[1.x.2040] 



Since  [2.x.5192]  is aimed to be a preconditioner only, we shall use the approximations on the right in the equation above. 

As discussed in step-22,  [2.x.5193] , where  [2.x.5194]  is the pressure mass matrix and is solved approximately by using CG with ILU as a preconditioner, and  [2.x.5195]  is obtained by one of multiple methods: solving a linear system with CG and ILU as preconditioner, just using one application of an ILU, solving a linear system with CG and GMG (Geometric Multigrid as described in step-16) as a preconditioner, or just performing a single V-cycle of GMG. 

As a comparison, instead of FGMRES, we also use the direct solver UMFPACK on the whole system to compare our results with.  If you want to use a direct solver (like UMFPACK), the system needs to be invertible. To avoid the one dimensional null space given by the constant pressures, we fix the first pressure unknown  to zero. This is not necessary for the iterative solvers. 




[1.x.2041] 

The test problem is a "Manufactured Solution" (see step-7 for details), and we choose  [2.x.5196]  and  [2.x.5197] . We apply Dirichlet boundary conditions for the velocity on the whole boundary of the domain  [2.x.5198] . To enforce the boundary conditions we can just use our reference solution. 

If you look up in the deal.II manual what is needed to create a class derived from  [2.x.5199] , you will find that this class has numerous  [2.x.5200]  functions, including  [2.x.5201]   [2.x.5202]   [2.x.5203]  etc., all of which can be overloaded.  Different parts of deal.II will require different ones of these particular functions. This can be confusing at first, but luckily the only thing you actually have to implement is  [2.x.5204]   The other virtual functions in the Function class have default implementations inside that will call your implementation of  [2.x.5205]  by default. 

Notice that our reference solution fulfills  [2.x.5206] . In addition, the pressure is chosen to have a mean value of zero.  For the "Method of Manufactured Solutions" of step-7, we need to find  [2.x.5207]  such that: 

[1.x.2042] 



Using the reference solution above, we obtain: 

[1.x.2043] 



[1.x.2044] 

Because we do not enforce the mean pressure to be zero for our numerical solution in the linear system, we need to post process the solution after solving. To do this we use the  [2.x.5208]  function to compute the mean value of the pressure to subtract it from the pressure. 




[1.x.2045] 

The way we implement geometric multigrid here only executes it on the velocity variables (i.e., the  [2.x.5209]  matrix described above) but not the pressure. One could implement this in different ways, including one in which one considers all coarse grid operations as acting on  [2.x.5210]  block systems where we only consider the top left block. Alternatively, we can implement things by really only considering a linear system on the velocity part of the overall finite element discretization. The latter is the way we want to use here. 

To implement this, one would need to be able to ask questions such as "May I have just part of a DoFHandler?". This is not possible at the time when this program was written, so in order to answer this request for our needs, we simply create a separate, second DoFHandler for just the velocities. We then build linear systems for the multigrid preconditioner based on only this second DoFHandler, and simply transfer the first block of (overall) vectors into corresponding vectors for the entire second DoFHandler. To make this work, we have to assure that the [1.x.2046] in which the (velocity) degrees of freedom are ordered in the two DoFHandler objects is the same. This is in fact the case by first distributing degrees of freedom on both, and then using the same sequence of DoFRenumbering operations on both. 




[1.x.2047] 

The main difference between step-56 and step-22 is that we use block solvers instead of the Schur Complement approach used in step-22. Details of this approach can be found under the "Block Schur complement preconditioner" subsection of the "Possible Extensions" section of step-22. For the preconditioner of the velocity block, we borrow a class from [1.x.2048] called  [2.x.5211]  that has the option to solve for the inverse of  [2.x.5212]  or just apply one preconditioner sweep for it instead, which provides us with an expensive and cheap approach, respectively. 


examples/step-56/doc/results.dox 



[1.x.2049] 

[1.x.2050] 

We first run the code and confirm that the finite element solution converges with the correct rates as predicted by the error analysis of mixed finite element problems. Given sufficiently smooth exact solutions  [2.x.5213]  and  [2.x.5214] , the errors of the Taylor-Hood element  [2.x.5215]  should be 

[1.x.2051] 



see for example Ern/Guermond "Theory and Practice of Finite Elements", Section 4.2.5 p195. This is indeed what we observe, using the  [2.x.5216]  element as an example (this is what is done in the code, but is easily changed in  [2.x.5217] ): 

 [2.x.5218]  

[1.x.2052] 

Let us compare the direct solver approach using UMFPACK to the two methods in which we choose  [2.x.5219]  and  [2.x.5220]  by solving linear systems with  [2.x.5221]  using CG. The preconditioner for CG is then either ILU or GMG. The following table summarizes solver iterations, timings, and virtual memory (VM) peak usage: 

 [2.x.5222]  

As can be seen from the table: 

1. UMFPACK uses large amounts of memory, especially in 3d. Also, UMFPACK timings do not scale favorably with problem size. 

2. Because we are using inner solvers for  [2.x.5223]  and  [2.x.5224] , ILU and GMG require the same number of outer iterations. 

3. The number of (inner) iterations for  [2.x.5225]  increases for ILU with refinement, leading to worse than linear scaling in solve time. In contrast, the number of inner iterations for  [2.x.5226]  stays constant with GMG leading to nearly perfect scaling in solve time. 

4. GMG needs slightly more memory than ILU to store the level and interface matrices. 

[1.x.2053] 

[1.x.2054] 

Experiment with higher order stable FE pairs and check that you observe the correct convergence rates. 

[1.x.2055] 

The introduction also outlined another option to precondition the overall system, namely one in which we do not choose  [2.x.5227]  as in the table above, but in which  [2.x.5228]  is only a single preconditioner application with GMG or ILU, respectively. 

This is in fact implemented in the code: Currently, the boolean  [2.x.5229]  is set to  [2.x.5230]  The option mentioned above is obtained by setting it to  [2.x.5231]  

What you will find is that the number of FGMRES iterations stays constant under refinement if you use GMG this way. This means that the Multigrid is optimal and independent of  [2.x.5232] . 


examples/step-57/doc/intro.dox 

 [2.x.5233]  

[1.x.2056] 

 [2.x.5234]  

[1.x.2057] 

[1.x.2058] 

[1.x.2059] 

In this tutorial we show how to solve the incompressible Navier Stokes equations (NSE) with Newton's method. The flow we consider here is assumed to be steady. In a domain  [2.x.5235] ,  [2.x.5236] , with a piecewise smooth boundary  [2.x.5237] , and a given force field  [2.x.5238] , we seek a velocity field  [2.x.5239]  and a pressure field  [2.x.5240]  satisfying 

[1.x.2060] 



Unlike the Stokes equations as discussed in step-22, the NSE are a nonlinear system of equations because of the convective term  [2.x.5241] . The first step of computing a numerical solution is to linearize the system and this will be done using Newton's method. A time-dependent problem is discussed in step-35, where the system is linearized using the solution from the last time step and no nonlinear solve is necessary. 

[1.x.2061] 

We define a nonlinear function whose root is a solution to the NSE by 

[1.x.2062] 



Assuming the initial guess is good enough to guarantee the convergence of Newton's iteration and denoting  [2.x.5242] , Newton's iteration on a vector function can be defined as 

[1.x.2063] 



where  [2.x.5243]  is the approximate solution in step  [2.x.5244] ,  [2.x.5245]  represents the solution from the previous step, and  [2.x.5246]  is the Jacobian matrix evaluated at  [2.x.5247] . A similar iteration can be found in step-15. 

The Newton iteration formula implies the new solution is obtained by adding an update term to the old solution. Instead of evaluating the Jacobian matrix and taking its inverse, we consider the update term as a whole, that is 

[1.x.2064] 



where  [2.x.5248] . 

We can find the update term by solving the system 

[1.x.2065] 



Here, the left of the previous equation represents the directional gradient of  [2.x.5249]  along  [2.x.5250]  at  [2.x.5251] . By definition, the directional gradient is given by 

[1.x.2066] 



Therefore, we arrive at the linearized system: 

[1.x.2067] 



where  [2.x.5252]  and  [2.x.5253]  are the solutions from the previous iteration. Additionally, the right hand side of the second equation is not zero since the discrete solution is not exactly divergence free (divergence free for the continuous solution). The right hand side here acts as a correction which leads the discrete solution of the velocity to be divergence free along Newton's iteration. In this linear system, the only unknowns are the update terms  [2.x.5254]  and  [2.x.5255] , and we can use a similar strategy to the one used in step-22 (and derive the weak form in the same way). 

Now, Newton's iteration can be used to solve for the update terms: 

<ol>    [2.x.5256] Initialization: Initial guess  [2.x.5257]  and  [2.x.5258] , tolerance  [2.x.5259] ; [2.x.5260]     [2.x.5261] Linear solve to compute update term  [2.x.5262]  and        [2.x.5263] ; [2.x.5264]     [2.x.5265] Update the approximation:        [2.x.5266]  and        [2.x.5267] ; [2.x.5268]     [2.x.5269] Check residual norm:  [2.x.5270] :        [2.x.5271]           [2.x.5272] If  [2.x.5273] , STOP. [2.x.5274]           [2.x.5275] If  [2.x.5276] , back to step 2. [2.x.5277]         [2.x.5278]  [2.x.5279]   [2.x.5280]  

[1.x.2068] 

The initial guess needs to be close enough to the solution for Newton's method to converge; hence, finding a good starting value is crucial to the nonlinear solver. 

When the viscosity  [2.x.5281]  is large, a good initial guess can be obtained by solving the Stokes equation with viscosity  [2.x.5282] . While problem dependent, this works for  [2.x.5283]  for the test problem considered here. 

However, the convective term  [2.x.5284]  will be dominant if the viscosity is small, like  [2.x.5285]  in test case 2.  In this situation, we use a continuation method to set up a series of auxiliary NSEs with viscosity approaching the one in the target NSE. Correspondingly, we create a sequence  [2.x.5286]  with  [2.x.5287] , and accept that the solutions to two NSE with viscosity  [2.x.5288]  and  [2.x.5289]  are close if  [2.x.5290]  is small.  Then we use the solution to the NSE with viscosity  [2.x.5291]  as the initial guess of the NSE with  [2.x.5292] . This can be thought of as a staircase from the Stokes equations to the NSE we want to solve. 

That is, we first solve a Stokes problem 

[1.x.2069] 



to get the initial guess for 

[1.x.2070] 



which also acts as the initial guess of the continuation method. Here  [2.x.5293]  is relatively large so that the solution to the Stokes problem with viscosity  [2.x.5294]  can be used as an initial guess for the NSE in Newton's iteration. 

Then the solution to 

[1.x.2071] 



acts as the initial guess for 

[1.x.2072] 



This process is repeated with a sequence of viscosities  [2.x.5295]  that is determined experimentally so that the final solution can used as a starting guess for the Newton iteration. 

[1.x.2073] 

At each step of Newton's iteration, the problem results in solving a saddle point systems of the form 

[1.x.2074] 



This system matrix has the same block structure as the one in step-22. However, the matrix  [2.x.5296]  at the top left corner is not symmetric because of the nonlinear term. Instead of solving the above system, we can solve the equivalent system 

[1.x.2075] 



with a parameter  [2.x.5297]  and an invertible matrix  [2.x.5298] . Here  [2.x.5299]  is the Augmented Lagrangian term; see [1] for details. 

Denoting the system matrix of the new system by  [2.x.5300]  and the right-hand side by  [2.x.5301] , we solve it iteratively with right preconditioning  [2.x.5302]  as  [2.x.5303] , where 

[1.x.2076] 



with  [2.x.5304]  and  [2.x.5305]  is the corresponding Schur complement  [2.x.5306] . We let  [2.x.5307]  where  [2.x.5308]  is the pressure mass matrix, then  [2.x.5309]  can be approximated by 

[1.x.2077] 



See [1] for details. 

We decompose  [2.x.5310]  as 

[1.x.2078] 



Here two inexact solvers will be needed for  [2.x.5311]  and  [2.x.5312] , respectively (see [1]). Since the pressure mass matrix is symmetric and positive definite, CG with ILU as a preconditioner is appropriate to use for  [2.x.5313] . For simplicity, we use the direct solver UMFPACK for  [2.x.5314] . The last ingredient is a sparse matrix-vector product with  [2.x.5315] . Instead of computing the matrix product in the augmented Lagrangian term in  [2.x.5316] , we assemble Grad-Div stabilization  [2.x.5317] , as explained in [2]. 

[1.x.2079] 

We use the lid driven cavity flow as our test case; see [3] for details. The computational domain is the unit square and the right-hand side is  [2.x.5318] . The boundary condition is 

[1.x.2080] 



When solving this problem, the error consists of the nonlinear error (from Newton's iteration) and the discretization error (dependent on mesh size). The nonlinear part decreases with each Newton iteration and the discretization error reduces with mesh refinement. In this example, the solution from the coarse mesh is transferred to successively finer meshes and used as an initial guess. Therefore, the nonlinear error is always brought below the tolerance of Newton's iteration and the discretization error is reduced with each mesh refinement. 

Inside the loop, we involve three solvers: one for  [2.x.5319] , one for  [2.x.5320]  and one for  [2.x.5321] . The first two solvers are invoked in the preconditioner and the outer solver gives us the update term. Overall convergence is controlled by the nonlinear residual; as Newton's method does not require an exact Jacobian, we employ FGMRES with a relative tolerance of only 1e-4 for the outer linear solver. In fact, we use the truncated Newton solve for this system. As described in step-22, the inner linear solves are also not required to be done very accurately. Here we use CG with a relative tolerance of 1e-6 for the pressure mass matrix. As expected, we still see convergence of the nonlinear residual down to 1e-14. Also, we use a simple line search algorithm for globalization of the Newton method. 

The cavity reference values for  [2.x.5322]  and  [2.x.5323]  are from [4] and [5], respectively, where  [2.x.5324]  is the Reynolds number and can be located at [8]. Here the viscosity is defined by  [2.x.5325] . Even though we can still find a solution for  [2.x.5326]  and the references contain results for comparison, we limit our discussion here to  [2.x.5327] . This is because the solution is no longer stationary starting around  [2.x.5328]  but instead becomes periodic, see [7] for details. 

[1.x.2081] <ol> 

   [2.x.5329]   An Augmented Lagrangian-Based Approach to the Oseen Problem, M. Benzi and M. Olshanskii, SIAM J. SCI. COMPUT. 2006    [2.x.5330]   Efficient augmented Lagrangian-type preconditioning for the Oseen problem using Grad-Div stabilization, Timo Heister and Gerd Rapin    [2.x.5331]   http://www.cfd-online.com/Wiki/Lid-driven_cavity_problem    [2.x.5332]   High-Re solution for incompressible flow using the Navier-Stokes Equations and a Multigrid Method, U. Ghia, K. N. Ghia, and C. T. Shin    [2.x.5333]   Numerical solutions of 2-D steady incompressible driven cavity flow at high Reynolds numbers, E. Erturk, T.C. Corke and C. Gokcol    [2.x.5334]  Implicit Weighted ENO Schemes for the Three-Dimensional Incompressible Navier-Stokes Equations, Yang et al, 1998    [2.x.5335]  The 2D lid-driven cavity problem revisited, C. Bruneau and M. Saad, 2006    [2.x.5336]  https://en.wikipedia.org/wiki/Reynolds_number  [2.x.5337]  


examples/step-57/doc/results.dox 



[1.x.2082] 

Now we use the method we discussed above to solve Navier Stokes equations with viscosity  [2.x.5338]  and  [2.x.5339] . 

[1.x.2083] 

In the first test case the viscosity is set to be  [2.x.5340] . As we discussed in the introduction, the initial guess is the solution to the corresponding Stokes problem. In the following table, the residuals at each Newton's iteration on every mesh is shown. The data in the table shows that Newton's iteration converges quadratically. 

 [2.x.5341]  








The following figures show the sequence of generated grids. For the case of  [2.x.5342] , the initial guess is obtained by solving Stokes on an  [2.x.5343]  mesh, and the mesh is refined adaptively. Between meshes, the solution from the coarse mesh is interpolated to the fine mesh to be used as an initial guess. 

 [2.x.5344]  

This picture is the graphical streamline result of lid-driven cavity with  [2.x.5345] .  [2.x.5346]  

Then the solution is compared with a reference solution from [4] and the reference solution data can be found in the file "ref_2d_ghia_u.txt". 

 [2.x.5347]  

[1.x.2084] 

Newton's iteration requires a good initial guess. However, the nonlinear term dominates when the Reynolds number is large, so that the solution to the Stokes equations may be far away from the exact solution. If the Stokes solution acts as the initial guess, the convergence will be lost. The following picture shows that the nonlinear iteration gets stuck and the residual no longer decreases in further iterations. 

 [2.x.5348]  

The initial guess, therefore, has to be obtained via a continuation method which has been discussed in the introduction. Here the step size in the continuation method, that is  [2.x.5349] , is 2000 and the initial mesh is of size  [2.x.5350] . After obtaining an initial guess, the mesh is refined as in the previous test case. The following picture shows that at each refinement Newton's iteration has quadratic convergence. 52 steps of Newton's iterations are executed for solving this test case. 

 [2.x.5351]  

We also show the residual from each step of Newton's iteration on every mesh. The quadratic convergence is clearly visible in the table. 

 [2.x.5352]  








The sequence of generated grids looks like this:  [2.x.5353]  We compare our solution with reference solution from [5].  [2.x.5354]  The following picture presents the graphical result.  [2.x.5355]  

Furthermore, the error consists of the nonlinear error, which decreases as we perform Newton iterations, and the discretization error, which depends on the mesh size. That is why we have to refine the mesh and repeat Newton's iteration on the next finer mesh. From the table above, we can see that the residual (nonlinear error) is below 1e-12 on each mesh, but the following picture shows us the difference between solutions on subsequently finer meshes. 

 [2.x.5356]  

[1.x.2085] 

[1.x.2086] 

[1.x.2087] 

It is easy to compare the currently implemented linear solver to just using UMFPACK for the whole linear system. You need to remove the nullspace containing the constant pressures and it is done in step-56. More interesting is the comparison to other state of the art preconditioners like PCD. It turns out that the preconditioner here is very competitive, as can be seen in the paper [2]. 

The following table shows the timing results between our iterative approach (FGMRES) compared to a direct solver (UMFPACK) for the whole system with viscosity set to 1/400. Even though we use the same direct solver for the velocity block in the iterative solver, it is considerably faster and consumes less memory. This will be even more pronounced in 3d. 

 [2.x.5357]  




[1.x.2088] 

The code is set up to also run in 3d. Of course the reference values are different, see [6] for example. High resolution computations are not doable with this example as is, because a direct solver for the velocity block does not work well in 3d. Rather, a parallel solver based on algebraic or geometric multigrid is needed. See below. 

[1.x.2089] 

For larger computations, especially in 3d, it is necessary to implement MPI parallel solvers and preconditioners. A good starting point would be step-55, which uses algebraic multigrid for the velocity block for the Stokes equations. Another option would be to take a look at the list of codes in the [1.x.2090], which already contains parallel Navier-Stokes solvers. 


examples/step-58/doc/intro.dox 

 [2.x.5358]  

[1.x.2091][1.x.2092] 

[1.x.2093] 

[1.x.2094] 

The [1.x.2095] for a function  [2.x.5359]  and a potential  [2.x.5360]  is a model often used in quantum mechanics and nonlinear optics. If one measures in appropriate quantities (so that  [2.x.5361] ), then it reads as follows: 

[1.x.2096] 

If there is no potential, i.e.  [2.x.5362] , then it can be used to describe the propagation of light in optical fibers. If  [2.x.5363] , the equation is also sometimes called the [1.x.2097] and can be used to model the time dependent behavior of [1.x.2098]. 

For this particular tutorial program, the physical interpretation of the equation is not of much concern to us. Rather, we want to use it as a model that allows us to explain two aspects: 

- It is a [1.x.2099] for  [2.x.5364] . We have previously seen complex-valued equations in step-29,   but there have opted to split the equations into real and imaginary   parts and consequently ended up solving a system of two real-valued   equations. In contrast, the goal here is to show how to solve   problems in which we keep everything as complex numbers. 

- The equation is a nice model problem to explain how [1.x.2100] work. This is because it has terms with   fundamentally different character: on the one hand,  [2.x.5365]  is a regular spatial operator in the way we have seen   many times before; on the other hand,  [2.x.5366]  has no spatial or temporal derivatives, i.e., it is a purely   local operator. It turns out that we have efficient methods for each   of these terms (in particular, we have analytic solutions for the   latter), and that we may be better off treating these terms   differently and separately. We will explain this in more detail   below. 




[1.x.2101] 

At first glance, the equations appear to be parabolic and similar to the heat equation (see step-26) as there is only a single time derivative and two spatial derivatives. But this is misleading. Indeed, that this is not the correct interpretation is more easily seen if we assume for a moment that the potential  [2.x.5367]  and  [2.x.5368] . Then we have the equation 

[1.x.2102] 

If we separate the solution into real and imaginary parts,  [2.x.5369] , with  [2.x.5370] , then we can split the one equation into its real and imaginary parts in the same way as we did in step-29: 

[1.x.2103] 

Not surprisingly, the factor  [2.x.5371]  in front of the time derivative couples the real and imaginary parts of the equation. If we want to understand this equation further, take the time derivative of one of the equations, say 

[1.x.2104] 

(where we have assumed that, at least in some formal sense, we can commute the spatial and temporal derivatives), and then insert the other equation into it: 

[1.x.2105] 

This equation is hyperbolic and similar in character to the wave equation. (This will also be obvious if you look at the video in the "Results" section of this program.) Furthermore, we could have arrived at the same equation for  [2.x.5372]  as well. Consequently, a better assumption for the NLSE is to think of it as a hyperbolic, wave-propagation equation than as a diffusion equation such as the heat equation. (You may wonder whether it is correct that the operator  [2.x.5373]  appears with a positive sign whereas in the wave equation,  [2.x.5374]  has a negative sign. This is indeed correct: After multiplying by a test function and integrating by parts, we want to come out with a positive (semi-)definite form. So, from  [2.x.5375]  we obtain  [2.x.5376] . Likewise, after integrating by parts twice, we obtain from  [2.x.5377]  the form  [2.x.5378] . In both cases do we get the desired positive sign.) 

The real NLSE, of course, also has the terms  [2.x.5379]  and  [2.x.5380] . However, these are of lower order in the spatial derivatives, and while they are obviously important, they do not change the character of the equation. 

In any case, the purpose of this discussion is to figure out what time stepping scheme might be appropriate for the equation. The conclusions is that, as a hyperbolic-kind of equation, we need to choose a time step that satisfies a CFL-type condition. If we were to use an explicit method (which we will not), we would have to investigate the eigenvalues of the matrix that corresponds to the spatial operator. If you followed the discussions of the video lectures ( [2.x.5381]  then you will remember that the pattern is that one needs to make sure that  [2.x.5382]  where  [2.x.5383]  is the time step,  [2.x.5384]  the mesh width, and  [2.x.5385]  are the orders of temporal and spatial derivatives. Whether you take the original equation ( [2.x.5386] ) or the reformulation for only the real or imaginary part, the outcome is that we would need to choose  [2.x.5387]  if we were to use an explicit time stepping method. This is not feasible for the same reasons as in step-26 for the heat equation: It would yield impractically small time steps for even only modestly refined meshes. Rather, we have to use an implicit time stepping method and can then choose a more balanced  [2.x.5388] . Indeed, we will use the implicit Crank-Nicolson method as we have already done in step-23 before for the regular wave equation. 




[1.x.2106] 

 [2.x.5389]  

If one thought of the NLSE as an ordinary differential equation in which the right hand side happens to have spatial derivatives, i.e., write it as 

[1.x.2107] 

one may be tempted to "formally solve" it by integrating both sides over a time interval  [2.x.5390]  and obtain 

[1.x.2108] 

Of course, it's not that simple: the  [2.x.5391]  in the integrand is still changing over time in accordance with the differential equation, so we cannot just evaluate the integral (or approximate it easily via quadrature) because we don't know  [2.x.5392] . But we can write this with separate contributions as follows, and this will allow us to deal with different terms separately: 

[1.x.2109] 

The way this equation can now be read is as follows: For each time interval  [2.x.5393] , the change  [2.x.5394]  in the solution consists of three contributions: 

- The contribution of the Laplace operator. 

- The contribution of the potential  [2.x.5395] . 

- The contribution of the "phase" term  [2.x.5396] . 

[1.x.2110] is now an approximation technique that allows us to treat each of these contributions separately. (If we want: In practice, we will treat the first two together, and the last one separate. But that is a detail, conceptually we could treat all of them differently.) To this end, let us introduce three separate "solutions": 

[1.x.2111] 



These three "solutions" can be thought of as satisfying the following differential equations: 

[1.x.2112] 

In other words, they are all trajectories  [2.x.5397]  that start at  [2.x.5398]  and integrate up the effects of exactly one of the three terms. The increments resulting from each of these terms over our time interval are then  [2.x.5399] ,  [2.x.5400] , and  [2.x.5401] . 

It is now reasonable to assume (this is an approximation!) that the change due to all three of the effects in question is well approximated by the sum of the three separate increments: 

[1.x.2113] 

This intuition is indeed correct, though the approximation is not exact: the difference between the exact left hand side and the term  [2.x.5402]  (i.e., the difference between the [1.x.2114] increment for the exact solution  [2.x.5403]  when moving from  [2.x.5404]  to  [2.x.5405] , and the increment composed of the three parts on the right hand side), is proportional to  [2.x.5406] . In other words, this approach introduces an error of size  [2.x.5407] . Nothing we have done so far has discretized anything in time or space, so the [1.x.2115] error is going to be  [2.x.5408]  plus whatever error we commit when approximating the integrals (the temporal discretization error) plus whatever error we commit when approximating the spatial dependencies of  [2.x.5409]  (the spatial error). 

Before we continue with discussions about operator splitting, let us talk about why one would even want to go this way? The answer is simple: For some of the separate equations for the  [2.x.5410] , we may have ways to solve them more efficiently than if we throw everything together and try to solve it at once. For example, and particularly pertinent in the current case: The equation for  [2.x.5411] , i.e., 

[1.x.2116] 

or equivalently, 

[1.x.2117] 

can be solved exactly: the equation is solved by 

[1.x.2118] 

This is easy to see if (i) you plug this solution into the differential equation, and (ii) realize that the magnitude  [2.x.5412]  is constant, i.e., the term  [2.x.5413]  in the exponent is in fact equal to  [2.x.5414] . In other words, the solution of the ODE for  [2.x.5415]  only changes its [1.x.2119], but the [1.x.2120] of the complex-valued function  [2.x.5416]  remains constant. This makes computing  [2.x.5417]  particularly convenient: we don't actually need to solve any ODE, we can write the solution down by hand. Using the operator splitting approach, none of the methods to compute  [2.x.5418]  therefore have to deal with the nonlinear term and all of the associated unpleasantries: we can get away with solving only [1.x.2121] problems, as long as we allow ourselves the luxury of using an operator splitting approach. 

Secondly, one often uses operator splitting if the different physical effects described by the different terms have different time scales. Imagine, for example, a case where we really did have some sort of diffusion equation. Diffusion acts slowly, but if  [2.x.5419]  is large, then the "phase rotation" by the term  [2.x.5420]  acts quickly. If we treated everything together, this would imply having to take rather small time steps. But with operator splitting, we can take large time steps  [2.x.5421]  for the diffusion, and (assuming we didn't have an analytic solution) use an ODE solver with many small time steps to integrate the "phase rotation" equation for  [2.x.5422]  from  [2.x.5423]  to  [2.x.5424] . In other words, operator splitting allows us to decouple slow and fast time scales and treat them differently, with methods adjusted to each case. 




[1.x.2122] 

While the method above allows to compute the three contributions  [2.x.5425]  in parallel, if we want, the method can be made slightly more accurate and easy to implement if we don't let the trajectories for the  [2.x.5426]  start all at  [2.x.5427] , but instead let the trajectory for  [2.x.5428]  start at the [1.x.2123] of the trajectory for  [2.x.5429] , namely  [2.x.5430] ; similarly, we will start the trajectory for  [2.x.5431]  start at the end point of the trajectory for  [2.x.5432] , namely  [2.x.5433] . This method is then called "Lie splitting" and has the same order of error as the method above, i.e., the splitting error is  [2.x.5434] . 

This variation of operator splitting can be written as follows (carefully compare the initial conditions to the ones above): 

[1.x.2124] 

(Obviously, while the formulas above imply that we should solve these problems in this particular order, it is equally valid to first solve for trajectory 3, then 2, then 1, or any other permutation.) 

The integrated forms of these equations are then 

[1.x.2125] 

From a practical perspective, this has the advantage that we need to keep around fewer solution vectors: Once  [2.x.5435]  has been computed, we don't need  [2.x.5436]  any more; once  [2.x.5437]  has been computed, we don't need  [2.x.5438]  any more. And once  [2.x.5439]  has been computed, we can just call it  [2.x.5440]  because, if you insert the first into the second, and then into the third equation, you see that the right hand side of  [2.x.5441]  now contains the contributions of all three physical effects: 

[1.x.2126] 

(Compare this again with the "exact" computation of  [2.x.5442] : It only differs in how we approximate  [2.x.5443]  in each of the three integrals.) In other words, Lie splitting is a lot simpler to implement that the original method outlined above because data handling is so much simpler. 




[1.x.2127] 

As mentioned above, Lie splitting is only  [2.x.5444]  accurate. This is acceptable if we were to use a first order time discretization, for example using the explicit or implicit Euler methods to solve the differential equations for  [2.x.5445] . This is because these time integration methods introduce an error proportional to  [2.x.5446]  themselves, and so the splitting error is proportional to an error that we would introduce anyway, and does not diminish the overall convergence order. 

But we typically want to use something higher order -- say, a [1.x.2128] or [1.x.2129] method -- since these are often not more expensive than a simple Euler method. It would be a shame if we were to use a time stepping method that is  [2.x.5447] , but then lose the accuracy again through the operator splitting. 

This is where the [1.x.2130] method comes in. It is easier to explain if we had only two parts, and so let us combine the effects of the Laplace operator and of the potential into one, and the phase rotation into a second effect. (Indeed, this is what we will do in the code since solving the equation with the Laplace equation with or without the potential costs the same -- so we merge these two steps.) The Lie splitting method from above would then do the following: It computes solutions of the following two ODEs, 

[1.x.2131] 

and then uses the approximation  [2.x.5448] . In other words, we first make one full time step for physical effect one, then one full time step for physical effect two. The solution at the end of the time step is simply the sum of the increments due to each of these physical effects separately. 

In contrast, [1.x.2132] (one of the titans of numerical analysis starting in the mid-20th century) figured out that it is more accurate to first do one half-step for one physical effect, then a full time step for the other physical effect, and then another half step for the first. Which one is which does not matter, but because it is so simple to do the phase rotation, we will use this effect for the half steps and then only need to do one spatial solve with the Laplace operator plus potential. This operator splitting method is now  [2.x.5449]  accurate. Written in formulas, this yields the following sequence of steps: 

[1.x.2133] 

As before, the first and third step can be computed exactly for this particular equation, yielding 

[1.x.2134] 



This is then how we are going to implement things in this program: In each time step, we execute three steps, namely 

- Update the solution value at each node by analytically integrating   the phase rotation equation by one half time step; 

- Solving the space-time equation that corresponds to the full step   for  [2.x.5450] , namely    [2.x.5451] ,   with initial conditions equal to the solution of the first half step   above. 

- Update the solution value at each node by analytically integrating   the phase rotation equation by another half time step. 

This structure will be reflected in an obvious way in the main time loop of the program. 




[1.x.2135] 

From the discussion above, it should have become clear that the only partial differential equation we have to solve in each time step is 

[1.x.2136] 

This equation is linear. Furthermore, we only have to solve it from  [2.x.5452]  to  [2.x.5453] , i.e., for exactly one time step. 

To do this, we will apply the second order accurate Crank-Nicolson scheme that we have already used in some of the other time dependent codes (specifically: step-23 and step-26). It reads as follows: 

[1.x.2137] 

Here, the "previous" solution  [2.x.5454]  (or the "initial condition" for this part of the time step) is the output of the first phase rotation half-step; the output of the current step will be denoted by  [2.x.5455] .  [2.x.5456]  is the length of the time step. (One could argue whether  [2.x.5457]  and  [2.x.5458]  live at time step  [2.x.5459]  or  [2.x.5460]  and what their upper indices should be. This is a philosophical discussion without practical impact, and one might think of  [2.x.5461]  as something like  [2.x.5462] , and  [2.x.5463]  as  [2.x.5464]  if that helps clarify things -- though, again  [2.x.5465]  is not to be understood as "one third time step after  [2.x.5466] " but more like "we've already done one third of the work necessary for time step  [2.x.5467] ".) 

If we multiply the whole equation with  [2.x.5468]  and sort terms with the unknown  [2.x.5469]  to the left and those with the known  [2.x.5470]  to the right, then we obtain the following (spatial) partial differential equation that needs to be solved in each time step: 

[1.x.2138] 






[1.x.2139] 

As mentioned above, the previous tutorial program dealing with complex-valued solutions (namely, step-29) separated real and imaginary parts of the solution. It thus reduced everything to real arithmetic. In contrast, we here want to keep things complex-valued. 

The first part of this is that we need to define the discretized solution as  [2.x.5471]  where the  [2.x.5472]  are the usual shape functions (which are real valued) but the expansion coefficients  [2.x.5473]  at time step  [2.x.5474]  are now complex-valued. This is easily done in deal.II: We just have to use  [2.x.5475]  instead of Vector<double> to store these coefficients. 

Of more interest is how to build and solve the linear system. Obviously, this will only be necessary for the second step of the Strang splitting discussed above, with the time discretization of the previous subsection. We obtain the fully discrete version through straightforward substitution of  [2.x.5476]  by  [2.x.5477]  and multiplication by a test function: 

[1.x.2140] 

or written in a more compact way: 

[1.x.2141] 

Here, the matrices are defined in their obvious ways: 

[1.x.2142] 

Note that all matrices individually are in fact symmetric, real-valued, and at least positive semidefinite, though the same is obviously not true for the system matrix  [2.x.5478]  and the corresponding matrix  [2.x.5479]  on the right hand side. 




[1.x.2143] 

 [2.x.5480]  

The only remaining important question about the solution procedure is how to solve the complex-valued linear system 

[1.x.2144] 

with the matrix  [2.x.5481]  and a right hand side that is easily computed as the product of a known matrix and the previous part-step's solution. As usual, this comes down to the question of what properties the matrix  [2.x.5482]  has. If it is symmetric and positive definite, then we can for example use the Conjugate Gradient method. 

Unfortunately, the matrix's only useful property is that it is complex symmetric, i.e.,  [2.x.5483] , as is easy to see by recalling that  [2.x.5484]  are all symmetric. It is not, however, [1.x.2145], which would require that  [2.x.5485]  where the bar indicates complex conjugation. 

Complex symmetry can be exploited for iterative solvers as a quick literature search indicates. We will here not try to become too sophisticated (and indeed leave this to the [1.x.2146] section below) and instead simply go with the good old standby for problems without properties: A direct solver. That's not optimal, especially for large problems, but it shall suffice for the purposes of a tutorial program. Fortunately, the SparseDirectUMFPACK class allows solving complex-valued problems. 




[1.x.2147] 

Initial conditions for the NLSE are typically chosen to represent particular physical situations. This is beyond the scope of this program, but suffice it to say that these initial conditions are (i) often superpositions of the wave functions of particles located at different points, and that (ii) because  [2.x.5486]  corresponds to a particle density function, the integral 

[1.x.2148] 

corresponds to the number of particles in the system. (Clearly, if one were to be physically correct,  [2.x.5487]  better be a constant if the system is closed, or  [2.x.5488]  if one has absorbing boundary conditions.) The important point is that one should choose initial conditions so that 

[1.x.2149] 

makes sense. 

What we will use here, primarily because it makes for good graphics, is the following: 

[1.x.2150] 

where  [2.x.5489]  is the distance from the (fixed) locations  [2.x.5490] , and  [2.x.5491]  are chosen so that each of the Gaussians that we are adding up adds an integer number of particles to  [2.x.5492] . We achieve this by making sure that 

[1.x.2151] 

is a positive integer. In other words, we need to choose  [2.x.5493]  as an integer multiple of 

[1.x.2152] 

assuming for the moment that  [2.x.5494]  -- which is of course not the case, but we'll ignore the small difference in integral. 

Thus, we choose  [2.x.5495]  for all, and  [2.x.5496] . This  [2.x.5497]  is small enough that the difference between the exact (infinite) integral and the integral over  [2.x.5498]  should not be too concerning. We choose the four points  [2.x.5499]  as  [2.x.5500]  -- also far enough away from the boundary of  [2.x.5501]  to keep ourselves on the safe side. 

For simplicity, we pose the problem on the square  [2.x.5502] . For boundary conditions, we will use time-independent Neumann conditions of the form 

[1.x.2153] 

This is not a realistic choice of boundary conditions but sufficient for what we want to demonstrate here. We will comment further on this in the [1.x.2154] section below. 

Finally, we choose  [2.x.5503] , and the potential as 

[1.x.2155] 

Using a large potential makes sure that the wave function  [2.x.5504]  remains small outside the circle of radius 0.7. All of the Gaussians that make up the initial conditions are within this circle, and the solution will mostly oscillate within it, with a small amount of energy radiating into the outside. The use of a large potential also makes sure that the nonphysical boundary condition does not have too large an effect. 


examples/step-58/doc/results.dox 



[1.x.2156] 

Running the code results in screen output like the following: ``` Number of active cells: 4096 Number of degrees of freedom: 16641 

Time step 1 at t=0 Time step 2 at t=0.00390625 Time step 3 at t=0.0078125 Time step 4 at t=0.0117188 [...] ``` Running the program also yields a good number of output files that we will visualize in the following. 




[1.x.2157] 

The `output_results()` function of this program generates output files that consist of a number of variables: The solution (split into its real and imaginary parts), the amplitude, and the phase. If we visualize these four fields, we get images like the following after a few time steps (at time  [2.x.5505] , to be precise: 

<div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step-58.re.png"          alt="Real part of the solution at t=0.242"          width="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-58.im.png"          alt="Imaginary part of the solution at t=0.242"          width="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-58.magnitude.png"          alt="Amplitude of the solution at t=0.242"          width="400">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step-58.phase.png"          alt="Phase of the solution at t=0.242"          width="400">   </div> </div> 

While the real and imaginary parts of the solution shown above are not particularly interesting (because, from a physical perspective, the global offset of the phase and therefore the balance between real and imaginary components, is meaningless), it is much more interesting to visualize the amplitude  [2.x.5506]  and phase  [2.x.5507]  of the solution and, in particular, their evolution. This leads to pictures like the following: 

The phase picture shown here clearly has some flaws: 

- First, phase is a "cyclic quantity", but the color scale uses a   fundamentally different color for values close to  [2.x.5508]  than   for values close to  [2.x.5509] . This is a nuisance -- what we need   is a "cyclic color map" that uses the same colors for the two   extremes of the range of the phase. Such color maps exist,   see [1.x.2158] or   [1.x.2159], for example. The problem is that the   author's favorite   one of the two big visualization packages, VisIt, does not have any   of these color maps built in. In an act of desperation, I therefore   had to resort to using Paraview given that it has several of the   color maps mentioned in the post above implemented. The picture   below uses the `nic_Edge` map in which both of the extreme values are shown   as black. 

- There is a problem on cells in which the phase wraps around. If   at some evaluation point of the cell the phase value is close to    [2.x.5510]  and at another evaluation point it is close to  [2.x.5511] , then   what we would really like to happen is for the entire cell to have a   color close to the extremes. But, instead, visualization programs   produce a linear interpolation in which the values within the cell,   i.e., between the evaluation points, is linearly interpolated between   these two values, covering essentially the entire range of possible   phase values and, consequently, cycling through the entire   rainbow of colors from dark red to dark green over the course of   one cell. The solution to this problem is to just output   the phase value on each cell as a piecewise constant. Because   averaging values close to the  [2.x.5512]  and  [2.x.5513]  is going to   result in an average that has nothing to do with the actual phase   angle, the `ComplexPhase` class just uses the *maximal* phase   angle encountered on each cell. 

With these modifications, the phase plot now looks as follows: 

<p align="center">   <img src="https://www.dealii.org/images/steps/developer/step-58.phase-cyclic.png"          alt="Phase of the solution at t=0.242, with a cyclic color map"          width="400">  [2.x.5514]  

Finally, we can generate a movie out of this. (To be precise, the video uses two more global refinement cycles and a time step half the size of what is used in the program above.) The author of these lines made the movie with VisIt, because that's what he's more familiar with, and using a hacked color map that is also cyclic -- though this color map lacks all of the skill employed by the people who wrote the posts mentioned in the links above. It does, however, show the character of the solution as a wave equation if you look at the shaded part of the domain outside the circle of radius 0.7 in which the potential is zero -- you can see how every time one of the bumps (showing the amplitude  [2.x.5515] ) bumps into the area where the potential is large: a wave travels outbound from there. Take a look at the video: 

[1.x.2160] 



So why did I end up shading the area where the potential  [2.x.5516]  is large? In that outside region, the solution is relatively small. It is also relatively smooth. As a consequence, to some approximate degree, the equation in that region simplifies to 

[1.x.2161] 

or maybe easier to read: 

[1.x.2162] 

To the degree to which this approximation is valid (which, among other things, eliminates the traveling waves you can see in the video), this equation has a solution 

[1.x.2163] 

Because  [2.x.5517]  is large, this means that the phase *rotates quite rapidly*. If you focus on the semi-transparent outer part of the domain, you can see that. If one colors this region in the same way as the inner part of the domain, this rapidly flashing outer part may be psychedelic, but is also distracting of what's happening on the inside; it's also quite hard to actually see the radiating waves that are easy to see at the beginning of the video. 


[1.x.2164] 

[1.x.2165] 

[1.x.2166] 

The solver chosen here is just too simple. It is also not efficient. What we do here is give the matrix to a sparse direct solver in every time step and let it find the solution of the linear system. But we know that we could do far better: 

- First, we should make use of the fact that the matrix doesn't   actually change from time step to time step. This is an artifact   of the fact that we here have constant boundary values and that   we don't change the time step size -- two assumptions that might   not be true in actual applications. But at least in cases where this   does happen to be the case, it would make sense to only factorize   the matrix once (i.e., compute  [2.x.5518]  and  [2.x.5519]  factors once) and then   use these factors for all following time steps until the matrix    [2.x.5520]  changes and requires a new factorization. The interface of the   SparseDirectUMFPACK class allows for this. 

- Ultimately, however, sparse direct solvers are only efficient for   relatively small problems, say up to a few 100,000 unknowns. Beyond   this, one needs iterative solvers such as the Conjugate Gradient method (for   symmetric and positive definite problems) or GMRES. We have used many   of these in other tutorial programs. In all cases, they need to be   accompanied by good preconditioners. For the current case, one   could in principle use GMRES -- a method that does not require   any specific properties of the matrix -- but would be better   advised to implement an iterative scheme that exploits the one   structural feature we know is true for this problem: That the matrix   is complex-symmetric (albeit not Hermitian). 




[1.x.2167] 

In order to be usable for actual, realistic problems, solvers for the nonlinear Schr&ouml;dinger equation need to utilize boundary conditions that make sense for the problem at hand. We have here restricted ourselves to simple Neumann boundary conditions -- but these do not actually make sense for the problem. Indeed, the equations are generally posed on an infinite domain. But, since we can't compute on infinite domains, we need to truncate it somewhere and instead pose boundary conditions that make sense for this artificially small domain. The approach widely used is to use the [1.x.2168] method that corresponds to a particular kind of attenuation. It is, in a different context, also used in step-62. 




[1.x.2169] 

Finally, we know from experience and many other tutorial programs that it is worthwhile to use adaptively refined meshes, rather than the uniform meshes used here. It would, in fact, not be very difficult to add this here: It just requires periodic remeshing and transfer of the solution from one mesh to the next. step-26 will be a good guide for how this could be implemented. 


examples/step-59/doc/intro.dox 

 [2.x.5521]  

[1.x.2170] 

[1.x.2171] 

[1.x.2172] 

Matrix-free operator evaluation enables very efficient implementations of discretization with high-order polynomial bases due to a method called sum factorization. This concept has been introduced in the step-37 and step-48 tutorial programs. In this tutorial program, we extend those concepts to discontinuous Galerkin (DG) schemes that include face integrals, a class of methods where high orders are particularly widespread. 

The underlying idea of the matrix-free evaluation is the same as for continuous elements: The matrix-vector product that appears in an iterative solver or multigrid smoother is not implemented by a classical sparse matrix kernel, but instead applied implicitly by the evaluation of the underlying integrals on the fly. For tensor product shape functions that are integrated with a tensor product quadrature rule, this evaluation is particularly efficient by using the sum-factorization technique, which decomposes the initially  [2.x.5522]  operations for interpolation involving  [2.x.5523]  vector entries with associated shape functions at degree  [2.x.5524]  in  [2.x.5525]  dimensions to  [2.x.5526]  quadrature points into  [2.x.5527]  one-dimensional operations of cost  [2.x.5528]  each. In 3D, this reduces the order of complexity by two powers in  [2.x.5529] . When measured as the complexity per degree of freedom, the complexity is  [2.x.5530]  in the polynomial degree. Due to the presence of face integrals in DG, and due to the fact that operations on quadrature points involve more memory transfer, which both scale as  [2.x.5531] , the observed complexity is often constant for moderate  [2.x.5532] . This means that a high order method can be evaluated with the same throughput in terms of degrees of freedom per second as a low-order method. 

More information on the algorithms are available in the preprint  [2.x.5533]  [1.x.2173] by Martin Kronbichler and Katharina Kormann, arXiv:1711.03590. 

[1.x.2174] 

For this tutorial program, we exemplify the matrix-free DG framework for the interior penalty discretization of the Laplacian, i.e., the same scheme as the one used for the step-39 tutorial program. The discretization of the Laplacian is given by the following weak form 

[1.x.2175] 

where  [2.x.5534]  denotes the directed jump of the quantity  [2.x.5535]  from the two associated cells  [2.x.5536]  and  [2.x.5537] , and  [2.x.5538]  is the average from both sides. 

The terms in the equation represent the cell integral after integration by parts, the primal consistency term that arises at the element interfaces due to integration by parts and insertion of an average flux, the adjoint consistency term that is added for restoring symmetry of the underlying matrix, and a penalty term with factor  [2.x.5539] , whose magnitude is equal the length of the cells in direction normal to face multiplied by  [2.x.5540] , see step-39. The penalty term is chosen such that an inverse estimate holds and the final weak form is coercive, i.e., positive definite in the discrete setting. The adjoint consistency term and the penalty term involve the jump  [2.x.5541]  at the element interfaces, which disappears for the analytic solution  [2.x.5542] . Thus, these terms are consistent with the original PDE, ensuring that the method can retain optimal orders of convergence. 

In the implementation below, we implement the weak form above by moving the normal vector  [2.x.5543]  from the jump terms to the derivatives to form a [1.x.2176] derivative of the form  [2.x.5544] . This makes the implementation on quadrature points slightly more efficient because we only need to work with scalar terms rather than tensors, and is mathematically equivalent. 

For boundary conditions, we use the so-called mirror principle that defines [1.x.2177] exterior values  [2.x.5545]  by extrapolation from the interior solution  [2.x.5546]  combined with the given boundary data, setting  [2.x.5547]  and  [2.x.5548]  on Dirichlet boundaries and  [2.x.5549]  and  [2.x.5550]  on Neumann boundaries, for given Dirichlet values  [2.x.5551]  and Neumann values  [2.x.5552] . These expressions are then inserted in the above weak form. Contributions involving the known quantities  [2.x.5553]  and  [2.x.5554]  are eventually moved to the right hand side, whereas the unknown value  [2.x.5555]  is retained on the left hand side and contributes to the matrix terms similarly as interior faces. Upon these manipulations, the same weak form as in step-39 is obtained. 

[1.x.2178] 

The matrix-free framework of deal.II provides the necessary infrastructure to implement the action of the discretized equation above. As opposed to the  [2.x.5556]  that we used in step-37 and step-48, we now build a code in terms of  [2.x.5557]  that takes three function pointers, one for the cell integrals, one for the inner face integrals, and one for the boundary face integrals (in analogy to the design of MeshWorker used in the step-39 tutorial program). In each of these three functions, we then implement the respective terms on the quadrature points. For interpolation between the vector entries and the values and gradients on quadrature points, we use the class FEEvaluation for cell contributions and FEFaceEvaluation for face contributions. The basic usage of these functions has been discussed extensively in the step-37 tutorial program. 

In  [2.x.5558]  all interior faces are visited exactly once, so one must make sure to compute the contributions from both the test functions  [2.x.5559]  and  [2.x.5560] . Given the fact that the test functions on both sides are indeed independent, the weak form above effectively means that we submit the same contribution to both an FEFaceEvaluation object called `phi_inner` and `phi_outer` for testing with the normal derivative of the test function, and values with opposite sign for testing with the values of the test function, because the latter involves opposite signs due to the jump term. For faces between cells of different refinement level, the integration is done from the refined side, and FEFaceEvaluation automatically performs interpolation to a subface on the coarse side. Thus, a hanging node never appears explicitly in a user implementation of a weak form. 

The fact that each face is visited exactly once also applies to those faces at subdomain boundaries between different processors when parallelized with MPI, where one cell belongs to one processor and one to the other. The setup in  [2.x.5561]  splits the faces between the two sides, and eventually only reports the faces actually handled locally in  [2.x.5562]  and  [2.x.5563]  respectively. Note that, in analogy to the cell integrals discussed in step-37, deal.II applies vectorization over several faces to use SIMD, working on something we call a [1.x.2179] with a single instruction. The face batches are independent from the cell batches, even though the time at which face integrals are processed is kept close to the time when the cell integrals of the respective cells are processed, in order to increase the data locality. 

Another thing that is new in this program is the fact that we no longer split the vector access like  [2.x.5564]  or  [2.x.5565]  from the evaluation and integration steps, but call combined functions  [2.x.5566]  and  [2.x.5567]  respectively. This is useful for face integrals because, depending on what gets evaluated on the faces, not all vector entries of a cell must be touched in the first place. Think for example of the case of the nodal element FE_DGQ with node points on the element surface: If we are interested in the shape function values on a face, only  [2.x.5568]  degrees of freedom contribute to them in a non-trivial way (in a more technical way of speaking, only  [2.x.5569]  shape functions have a nonzero support on the face and return true for  [2.x.5570]  When compared to the  [2.x.5571]  degrees of freedom of a cell, this is one power less. 

Now of course we are not interested in only the function values, but also the derivatives on the cell. Fortunately, there is an element in deal.II that extends this property of reduced access also for derivatives on faces, the FE_DGQHermite element. 

[1.x.2180] 

The element FE_DGQHermite belongs to the family of FE_DGQ elements, i.e., its shape functions are a tensor product of 1D polynomials and the element is fully discontinuous. As opposed to the nodal character in the usual FE_DGQ element, the FE_DGQHermite element is a mixture of nodal contributions and derivative contributions based on a Hermite-like concept. The underlying polynomial class is  [2.x.5572]  and can be summarized as follows: For cubic polynomials, we use two polynomials to represent the function value and first derivative at the left end of the unit interval,  [2.x.5573] , and two polynomials to represent the function value and first derivative and the right end of the unit interval,  [2.x.5574] . At the opposite ends, both the value and first derivative of the shape functions are zero, ensuring that only two out of the four basis functions contribute to values and derivative on the respective end. However, we deviate from the classical Hermite interpolation in not strictly assigning one degree of freedom for the value and one for the first derivative, but rather allow the first derivative to be a linear combination of the first and the second shape function. This is done to improve the conditioning of the interpolation. Also, when going to degrees beyond three, we add node points in the element interior in a Lagrange-like fashion, combined with double zeros in the points  [2.x.5575]  and  [2.x.5576] . The position of these extra nodes is determined by the zeros of some Jacobi polynomials as explained in the description of the class  [2.x.5577]  

Using this element, we only need to access  [2.x.5578]  degrees of freedom for computing both values and derivatives on a face. The check whether the Hermite property is fulfilled is done transparently inside  [2.x.5579]  and  [2.x.5580]  that check the type of the basis and reduce the access to data if possible. Obviously, this would not be possible if we had separated  [2.x.5581]  from  [2.x.5582]  because the amount of entries we need to read depends on the type of the derivative (only values, first derivative, etc.) and thus must be given to `read_dof_values()`. 

This optimization is not only useful for computing the face integrals, but also for the MPI ghost layer exchange: In a naive exchange, we would need to send all degrees of freedom of a cell to another processor if the other processor is responsible for computing the face's contribution. Since we know that only some of the degrees of freedom in the evaluation with FEFaceEvaluation are touched, it is natural to only exchange the relevant ones. The  [2.x.5583]  function has support for a selected data exchange when combined with  [2.x.5584]  To make this happen, we need to tell the loop what kind of evaluation on faces we are going to do, using an argument of type  [2.x.5585]  as can be seen in the implementation of  [2.x.5586]  below. The way data is exchanged in that case is as follows: The ghost layer data in the vector still pretends to represent all degrees of freedom, such that FEFaceEvaluation can continue to read the values as if the cell were a locally owned one. The data exchange routines take care of the task for packing and unpacking the data into this format. While this sounds pretty complicated, we will show in the results section below that this really pays off by comparing the performance to a baseline code that does not specify the data access on faces. 

[1.x.2181] 

In the tradition of the step-37 program, we again solve a Poisson problem with a geometric multigrid preconditioner inside a conjugate gradient solver. Instead of computing the diagonal and use the basic PreconditionChebyshev as a smoother, we choose a different strategy in this tutorial program. We implement a block-Jacobi preconditioner, where a block refers to all degrees of freedom on a cell. Rather than building the full cell matrix and applying its LU factorization (or inverse) in the preconditioner &mdash; an operation that would be heavily memory bandwidth bound and thus pretty slow &mdash; we approximate the inverse of the block by a special technique called fast diagonalization method. 

The idea of the method is to take use of the structure of the cell matrix. In case of the Laplacian with constant coefficients discretized on a Cartesian mesh, the cell matrix  [2.x.5587]  can be written as 

[1.x.2182] 

in 2D and 

[1.x.2183] 

in 3D. The matrices  [2.x.5588]  and  [2.x.5589]  denote the 1D Laplace matrix (including the cell and face term associated to the current cell values  [2.x.5590]  and  [2.x.5591] ) and  [2.x.5592]  and  [2.x.5593]  are the mass matrices. Note that this simple tensor product structure is lost once there are non-constant coefficients on the cell or the geometry is not constant any more. We mention that a similar setup could also be used to replace the computed integrals with this final tensor product form of the matrices, which would cut the operations for the operator evaluation into less than half. However, given the fact that this only holds for Cartesian cells and constant coefficients, which is a pretty narrow case, we refrain from pursuing this idea. 

Interestingly, the exact inverse of the matrix  [2.x.5594]  can be found through tensor products due to a method introduced by [1.x.2184] from 1964, 

[1.x.2185] 

where  [2.x.5595]  is the matrix of eigenvectors to the generalized eigenvalue problem in the given tensor direction  [2.x.5596] : 

[1.x.2186] 

and  [2.x.5597]  is the diagonal matrix representing the generalized eigenvalues  [2.x.5598] . Note that the vectors  [2.x.5599]  are such that they simultaneously diagonalize  [2.x.5600]  and  [2.x.5601] , i.e.  [2.x.5602]  and  [2.x.5603] . 

The deal.II library implements a class using this concept, called TensorProductMatrixSymmetricSum. 

For the sake of this program, we stick with constant coefficients and Cartesian meshes, even though an approximate version based on tensor products would still be possible for a more general mesh, and the operator evaluation itself is of course generic. Also, we do not bother with adaptive meshes where the multigrid algorithm would need to get access to flux matrices over the edges of different refinement, as explained in step-39. One thing we do, however, is to still wrap our block-Jacobi preconditioner inside PreconditionChebyshev. That class relieves us from finding an appropriate relaxation parameter (which would be around 0.7 in 2D and 0.5 in 3D for the block-Jacobi smoother), and often increases smoothing efficiency a bit over plain Jacobi smoothing in that it enables lower the time to solution when setting the degree of the Chebyshev polynomial to one or two. 

Note that the block-Jacobi smoother has an additional benefit: The fast diagonalization method can also be interpreted as a change from the Hermite-like polynomials underlying FE_DGQHermite to a basis where the cell Laplacian is diagonal. Thus, it cancels the effect of the basis, and we get the same iteration counts irrespective of whether we use FE_DGQHermite or FE_DGQ. This is in contrast to using the PreconditionChebyshev class with only the diagonal (a point-Jacobi scheme), where FE_DGQ and FE_DGQHermite do indeed behave differently and FE_DGQ needs 2-5 less iterations than FE_DGQHermite, despite the modification made to the Hermite-like shape functions to ensure a good conditioning. 


examples/step-59/doc/results.dox 



[1.x.2187] 

[1.x.2188] 

Like in step-37, we evaluate the multigrid solver in terms of run time.  In two space dimensions with elements of degree 8, a possible output could look as follows: 

[1.x.2189] 



Like in step-37, the number of CG iterations remains constant with increasing problem size. The iteration counts are a bit higher, which is because we use a lower degree of the Chebyshev polynomial (2 vs 5 in step-37) and because the interior penalty discretization has a somewhat larger spread in eigenvalues. Nonetheless, 13 iterations to reduce the residual by 12 orders of magnitude, or almost a factor of 9 per iteration, indicates an overall very efficient method. In particular, we can solve a system with 21 million degrees of freedom in 5 seconds when using 12 cores, which is a very good efficiency. Of course, in 2D we are well inside the regime of roundoff for a polynomial degree of 8 &ndash; as a matter of fact, around 83k DoFs or 0.025s would have been enough to fully converge this (simple) analytic solution here. 

Not much changes if we run the program in three spatial dimensions, except for the fact that we now use do something more useful with the higher polynomial degree and increasing mesh sizes, as the roundoff errors are only obtained at the finest mesh. Still, it is remarkable that we can solve a 3D Laplace problem with a wave of three periods to roundoff accuracy on a twelve-core machine pretty easily - using about 3.5 GB of memory in total for the second to largest case with 24m DoFs, taking not more than eight seconds. The largest case uses 30GB of memory with 191m DoFs. 

[1.x.2190] 



[1.x.2191] 

In the introduction and in-code comments, it was mentioned several times that high orders are treated very efficiently with the FEEvaluation and FEFaceEvaluation evaluators. Now, we want to substantiate these claims by looking at the throughput of the 3D multigrid solver for various polynomial degrees. We collect the times as follows: We first run a solver at problem size close to ten million, indicated in the first four table rows, and record the timings. Then, we normalize the throughput by recording the number of million degrees of freedom solved per second (MDoFs/s) to be able to compare the efficiency of the different degrees, which is computed by dividing the number of degrees of freedom by the solver time. 

 [2.x.5604]  

We clearly see how the efficiency per DoF initially improves until it reaches a maximum for the polynomial degree  [2.x.5605] . This effect is surprising, not only because higher polynomial degrees often yield a vastly better solution, but especially also when having matrix-based schemes in mind where the denser coupling at higher degree leads to a monotonously decreasing throughput (and a drastic one in 3D, with  [2.x.5606]  being more than ten times slower than  [2.x.5607] !). For higher degrees, the throughput decreases a bit, which is both due to an increase in the number of iterations (going from 12 at  [2.x.5608]  to 19 at  [2.x.5609] ) and due to the  [2.x.5610]  complexity of operator evaluation. Nonetheless, efficiency as the time to solution would be still better for higher polynomial degrees because they have better convergence rates (at least for problems as simple as this one): For  [2.x.5611] , we reach roundoff accuracy already with 1 million DoFs (solver time less than a second), whereas for  [2.x.5612]  we need 24 million DoFs and 8 seconds. For  [2.x.5613] , the error is around  [2.x.5614]  with 57m DoFs and thus still far away from roundoff, despite taking 16 seconds. 

Note that the above numbers are a bit pessimistic because they include the time it takes the Chebyshev smoother to compute an eigenvalue estimate, which is around 10 percent of the solver time. If the system is solved several times (as e.g. common in fluid dynamics), this eigenvalue cost is only paid once and faster times become available. 

[1.x.2192] 

Finally, we take a look at some of the special ingredients presented in this tutorial program, namely the FE_DGQHermite basis in particular and the specification of  [2.x.5615]  In the following table, the third row shows the optimized solver above, the fourth row shows the timings with only the  [2.x.5616]  set to `unspecified` rather than the optimal `gradients`, and the last one with replacing FE_DGQHermite by the basic FE_DGQ elements where both the MPI exchange are more expensive and the operations done by  [2.x.5617]  and  [2.x.5618]  

 [2.x.5619]  

The data in the table shows that not using  [2.x.5620]  increases costs by around 10% for higher polynomial degrees. For lower degrees, the difference is obviously less pronounced because the volume-to-surface ratio is more beneficial and less data needs to be exchanged. The difference is larger when looking at the matrix-vector product only, rather than the full multigrid solver shown here, with around 20% worse timings just because of the MPI communication. 

For  [2.x.5621]  and  [2.x.5622] , the Hermite-like basis functions do obviously not really pay off (indeed, for  [2.x.5623]  the polynomials are exactly the same as for FE_DGQ) and the results are similar as with the FE_DGQ basis. However, for degrees starting at three, we see an increasing advantage for FE_DGQHermite, showing the effectiveness of these basis functions. 

[1.x.2193] 

As mentioned in the introduction, the fast diagonalization method is tied to a Cartesian mesh with constant coefficients. If we wanted to solve variable-coefficient problems, we would need to invest a bit more time in the design of the smoother parameters by selecting proper generalizations (e.g., approximating the inverse on the nearest box-shaped element). 

Another way of extending the program would be to include support for adaptive meshes, for which interface operations at edges of different refinement level become necessary, as discussed in step-39. 


examples/step-6/doc/intro.dox 

[1.x.2194] 

[1.x.2195] 

 [2.x.5624]  

This program is finally about one of the main features of deal.II: the use of adaptively (locally) refined meshes. The program is still based on step-4 and step-5, and, as you will see, it does not actually take very much code to enable adaptivity. Indeed, while we do a great deal of explaining, adaptive meshes can be added to an existing program with barely a dozen lines of additional code. The program shows what these lines are, as well as another important ingredient of adaptive mesh refinement (AMR): a criterion that can be used to determine whether it is necessary to refine a cell because the error is large on it, whether the cell can be coarsened because the error is particularly small on it, or whether we should just leave the cell as it is. We will discuss all of these issues in the following. 




[1.x.2196] 

There are a number of ways how one can adaptively refine meshes. The basic structure of the overall algorithm is always the same and consists of a loop over the following steps: 

- Solve the PDE on the current mesh; 

- Estimate the error on each cell using some criterion that is indicative   of the error; 

- Mark those cells that have large errors for refinement, mark those that have   particularly small errors for coarsening, and leave the rest alone; 

- Refine and coarsen the cells so marked to obtain a new mesh; 

- Repeat the steps above on the new mesh until the overall error is   sufficiently small. 

For reasons that are probably lost to history (maybe that these functions used to be implemented in FORTRAN, a language that does not care about whether something is spelled in lower or UPPER case letters, with programmers often choosing upper case letters habitually), the loop above is often referenced in publications about mesh adaptivity as the SOLVE-ESTIMATE-MARK-REFINE loop (with this spelling). 

Beyond this structure, however, there are a variety of ways to achieve this. Fundamentally, they differ in how exactly one generates one mesh from the previous one. 

If one were to use triangles (which deal.II does not do), then there are two essential possibilities: 

- Longest-edge refinement: In this strategy, a triangle marked for refinement   is cut into two by introducing one new edge from the midpoint of the longest   edge to the opposite vertex. Of course, the midpoint from the longest edge   has to somehow be balanced by *also* refining the cell on the other side of   that edge (if there is one). If the edge in question is also the longest   edge of the neighboring cell, then we can just run a new edge through the   neighbor to the opposite vertex; otherwise a slightly more involved   construction is necessary that adds more new vertices on at least one   other edge of the neighboring cell, and then may propagate to the neighbors   of the neighbor until the algorithm terminates. This is hard to describe   in words, and because deal.II does not use triangles not worth the time here.   But if you're curious, you can always watch video lecture 15 at the link   shown at the top of this introduction. 

- Red-green refinement: An alternative is what is called "red-green refinement".   This strategy is even more difficult to describe (but also discussed in the   video lecture) and has the advantage that the refinement does not propagate   beyond the immediate neighbors of the cell that we want to refine. It is,   however, substantially more difficult to implement. 

There are other variations of these approaches, but the important point is that they always generate a mesh where the lines where two cells touch are entire edges of both adjacent cells. With a bit of work, this strategy is readily adapted to three-dimensional meshes made from tetrahedra. 

Neither of these methods works for quadrilaterals in 2d and hexahedra in 3d, or at least not easily. The reason is that the transition elements created out of the quadrilateral neighbors of a quadrilateral cell that is to be refined would be triangles, and we don't want this. Consequently, the approach to adaptivity chosen in deal.II is to use grids in which neighboring cells may differ in refinement level by one. This then results in nodes on the interfaces of cells which belong to one side, but are unbalanced on the other. The common term for these is &ldquo;hanging nodes&rdquo;, and these meshes then look like this in a very simple situation: 

 [2.x.5625]  

A more complicated two-dimensional mesh would look like this (and is discussed in the "Results" section below): 

<img src="https://www.dealii.org/images/steps/developer/step_6_grid_5_ladutenko.svg"      alt="Fifth adaptively refined Ladutenko grid: the cells are clustered           along the inner circle."      width="300" height="300"> 

Finally, a three-dimensional mesh (from step-43) with such hanging nodes is shown here: 

<img src="https://www.dealii.org/images/steps/developer/step-43.3d.mesh.png" alt=""      width="300" height="300"> 

The first and third mesh are of course based on a square and a cube, but as the second mesh shows, this is not necessary. The important point is simply that we can refine a mesh independently of its neighbors (subject to the constraint that a cell can be only refined once more than its neighbors), but that we end up with these &ldquo;hanging nodes&rdquo; if we do this. 




[1.x.2197] 

Now that you have seen what these adaptively refined meshes look like, you should ask [1.x.2198] we would want to do this. After all, we know from theory that if we refine the mesh globally, the error will go down to zero as 

[1.x.2199] 

where  [2.x.5626]  is some constant independent of  [2.x.5627]  and  [2.x.5628] ,  [2.x.5629]  is the polynomial degree of the finite element in use, and  [2.x.5630]  is the diameter of the largest cell. So if the [1.x.2200] cell is important, then why would we want to make the mesh fine in some parts of the domain but not all? 

The answer lies in the observation that the formula above is not optimal. In fact, some more work shows that the following is a better estimate (which you should compare to the square of the estimate above): 

[1.x.2201] 

(Because  [2.x.5631] , this formula immediately implies the previous one if you just pull the mesh size out of the sum.) What this formula suggests is that it is not necessary to make the [1.x.2202] cell small, but that the cells really only need to be small [1.x.2203]! In other words: The mesh really only has to be fine where the solution has large variations, as indicated by the  [2.x.5632] st derivative. This makes intuitive sense: if, for example, we use a linear element  [2.x.5633] , then places where the solution is nearly linear (as indicated by  [2.x.5634]  being small) will be well resolved even if the mesh is coarse. Only those places where the second derivative is large will be poorly resolved by large elements, and consequently that's where we should make the mesh small. 

Of course, this [1.x.2204] is not very useful in practice since we don't know the exact solution  [2.x.5635]  of the problem, and consequently, we cannot compute  [2.x.5636] . But, and that is the approach commonly taken, we can compute numerical approximations of  [2.x.5637]  based only on the discrete solution  [2.x.5638]  that we have computed before. We will discuss this in slightly more detail below. This will then help us determine which cells have a large  [2.x.5639] st derivative, and these are then candidates for refining the mesh. 




[1.x.2205] 

The methods using triangular meshes mentioned above go to great lengths to make sure that each vertex is a vertex of all adjacent cells -- i.e., that there are no hanging nodes. This then automatically makes sure that we can define shape functions in such a way that they are globally continuous (if we use the common  [2.x.5640]  Lagrange finite element methods we have been using so far in the tutorial programs, as represented by the FE_Q class). 

On the other hand, if we define shape functions on meshes with hanging nodes, we may end up with shape functions that are not continuous. To see this, think about the situation above where the top right cell is not refined, and consider for a moment the use of a bilinear finite element. In that case, the shape functions associated with the hanging nodes are defined in the obvious way on the two small cells adjacent to each of the hanging nodes. But how do we extend them to the big adjacent cells? Clearly, the function's extension to the big cell cannot be bilinear because then it needs to be linear along each edge of the large cell, and that means that it needs to be zero on the entire edge because it needs to be zero on the two vertices of the large cell on that edge. But it is not zero at the hanging node itself when seen from the small cells' side -- so it is not continuous. The following three figures show three of the shape functions along the edges in question that turn out to not be continuous when defined in the usual way simply based on the cells they are adjacent to: 

<div class="threecolumn" style="width: 80%">   <div class="parent">     <div class="img" align="center">        [2.x.5641]      </div>   </div>   <div class="parent">     <div class="img" align="center">        [2.x.5642]      </div>   </div>   <div class="parent">     <div class="img" align="center">        [2.x.5643]      </div>   </div> </div> 


But we do want the finite element solution to be continuous so that we have a &ldquo;conforming finite element method&rdquo; where the discrete finite element space is a proper subset of the  [2.x.5644]  function space in which we seek the solution of the Laplace equation. To guarantee that the global solution is continuous at these nodes as well, we have to state some additional constraints on the values of the solution at these nodes. The trick is to realize that while the shape functions shown above are discontinuous (and consequently an [1.x.2206] linear combination of them is also discontinuous), that linear combinations in which the shape functions are added up as  [2.x.5645]  can be continuous [1.x.2207]. In other words, the coefficients  [2.x.5646]  can not be chosen arbitrarily but have to satisfy certain constraints so that the function  [2.x.5647]  is in fact continuous. What these constraints have to look is relatively easy to understand conceptually, but the implementation in software is complicated and takes several thousand lines of code. On the other hand, in user code, it is only about half a dozen lines you have to add when dealing with hanging nodes. 

In the program below, we will show how we can get these constraints from deal.II, and how to use them in the solution of the linear system of equations. Before going over the details of the program below, you may want to take a look at the  [2.x.5648]  documentation module that explains how these constraints can be computed and what classes in deal.II work on them. 




[1.x.2208] 

The practice of hanging node constraints is rather simpler than the theory we have outlined above. In reality, you will really only have to add about half a dozen lines of additional code to a program like step-4 to make it work with adaptive meshes that have hanging nodes. The interesting part about this is that it is entirely independent of the equation you are solving: The algebraic nature of these constraints has nothing to do with the equation and only depends on the choice of finite element. As a consequence, the code to deal with these constraints is entirely contained in the deal.II library itself, and you do not need to worry about the details. 

The steps you need to make this work are essentially like this: 

- You have to create an AffineConstraints object, which (as the name   suggests) will store all constraints on the finite element space. In   the current context, these are the constraints due to our desire to   keep the solution space continuous even in the presence of hanging   nodes. (Below we will also briefly mention that we will also put   boundary values into this same object, but that is a separate matter.) 

- You have to fill this object using the function    [2.x.5649]  to ensure continuity of   the elements of the finite element space. 

- You have to use this object when you copy the local contributions to   the matrix and right hand side into the global objects, by using    [2.x.5650]  Up until   now, we have done this ourselves, but now with constraints, this   is where the magic happens and we apply the constraints to the   linear system. What this function does is make sure that the   degrees of freedom located at hanging nodes are not, in fact,   really free. Rather, they are factually eliminated from the   linear system by setting their rows and columns to zero and putting   something on the diagonal to ensure the matrix remains invertible.   The matrix resulting from this process remains symmetric and   positive definite for the Laplace equation we solve here, so we can   continue to use the Conjugate Gradient method for it. 

- You then solve the linear system as usual, but at the end of this   step, you need to make sure that the degrees of "freedom" located   on hanging nodes get their correct (constrained) value so that the   solution you then visualize or evaluate in other ways is in   fact continuous. This is done by calling    [2.x.5651]  immediately after solving. 

These four steps are really all that is necessary -- it's that simple from a user perspective. The fact that, in the function calls mentioned above, you will run through several thousand lines of not-so-trivial code is entirely immaterial to this: In user code, there are really only four additional steps. 




[1.x.2209] 

The next question, now that we know how to [1.x.2210] with meshes that have these hanging nodes is how we [1.x.2211] them. 

A simple way has already been shown in step-1: If you [1.x.2212] where it is necessary to refine the mesh, then you can create one by hand. But in reality, we don't know this: We don't know the solution of the PDE up front (because, if we did, we wouldn't have to use the finite element method), and consequently we do not know where it is necessary to add local mesh refinement to better resolve areas where the solution has strong variations. But the discussion above shows that maybe we can get away with using the discrete solution  [2.x.5652]  on one mesh to estimate the derivatives  [2.x.5653] , and then use this to determine which cells are too large and which already small enough. We can then generate a new mesh from the current one using local mesh refinement. If necessary, this step is then repeated until we are happy with our numerical solution -- or, more commonly, until we run out of computational resources or patience. 

So that's exactly what we will do. The locally refined grids are produced using an [1.x.2213] which estimates the energy error for numerical solutions of the Laplace operator. Since it was developed by Kelly and co-workers, we often refer to it as the &ldquo;Kelly refinement indicator&rdquo; in the library, documentation, and mailing list. The class that implements it is called KellyErrorEstimator, and there is a great deal of information to be found in the documentation of that class that need not be repeated here. The summary, however, is that the class computes a vector with as many entries as there are  [2.x.5654]  "active cells", and where each entry contains an estimate of the error on that cell. This estimate is then used to refine the cells of the mesh: those cells that have a large error will be marked for refinement, those that have a particularly small estimate will be marked for coarsening. We don't have to do this by hand: The functions in namespace GridRefinement will do all of this for us once we have obtained the vector of error estimates. 

It is worth noting that while the Kelly error estimator was developed for Laplace's equation, it has proven to be a suitable tool to generate locally refined meshes for a wide range of equations, not even restricted to elliptic only problems. Although it will create non-optimal meshes for other equations, it is often a good way to quickly produce meshes that are well adapted to the features of solutions, such as regions of great variation or discontinuities. 




[1.x.2214] 

It turns out that one can see Dirichlet boundary conditions as just another constraint on the degrees of freedom. It's a particularly simple one, indeed: If  [2.x.5655]  is a degree of freedom on the boundary, with position  [2.x.5656] , then imposing the boundary condition  [2.x.5657]  on  [2.x.5658]  simply yields the constraint  [2.x.5659] . 

The AffineConstraints class can handle such constraints as well, which makes it convenient to let the same object we use for hanging node constraints also deal with these Dirichlet boundary conditions. This way, we don't need to apply the boundary conditions after assembly (like we did in the earlier steps). All that is necessary is that we call the variant of  [2.x.5660]  that returns its information in an AffineConstraints object, rather than the  [2.x.5661]  we have used in previous tutorial programs. 


 [1.x.2215] 


Since the concepts used for locally refined grids are so important, we do not show much other material in this example. The most important exception is that we show how to use biquadratic elements instead of the bilinear ones which we have used in all previous examples. In fact, the use of higher order elements is accomplished by only replacing three lines of the program, namely the initialization of the  [2.x.5662]  member variable in the constructor of the main class of this program, and the use of an appropriate quadrature formula in two places. The rest of the program is unchanged. 

The only other new thing is a method to catch exceptions in the  [2.x.5663]  function in order to output some information in case the program crashes for some reason. This is discussed below in more detail. 


examples/step-6/doc/results.dox 



[1.x.2216] 


The output of the program looks as follows: 

[1.x.2217] 






As intended, the number of cells roughly doubles in each cycle. The number of degrees is slightly more than four times the number of cells; one would expect a factor of exactly four in two spatial dimensions on an infinite grid (since the spacing between the degrees of freedom is half the cell width: one additional degree of freedom on each edge and one in the middle of each cell), but it is larger than that factor due to the finite size of the mesh and due to additional degrees of freedom which are introduced by hanging nodes and local refinement. 




The program outputs the solution and mesh in each cycle of the refinement loop. The solution looks as follows: 

 [2.x.5664]  

It is interesting to follow how the program arrives at the final mesh: 

<div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_0.svg"          alt="Initial grid: the five-cell circle grid with one global refinement."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_1.svg"          alt="First grid: the five-cell circle grid with two global refinements."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_2.svg"          alt="Second grid: the five-cell circle grid with one adaptive refinement."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_3.svg"          alt="Third grid: the five-cell circle grid with two adaptive          refinements, showing clustering around the inner circle."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_4.svg"          alt="Fourth grid: the five-cell circle grid with three adaptive          refinements, showing clustering around the inner circle."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_5.svg"          alt="Fifth grid: the five-cell circle grid with four adaptive          refinements, showing clustering around the inner circle."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_6.svg"          alt="Sixth grid: the five-cell circle grid with five adaptive          refinements, showing clustering around the inner circle."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_7.svg"          alt="Last grid: the five-cell circle grid with six adaptive          refinements, showing that most cells are clustered around the inner circle."          width="300" height="300">   </div> </div> 


It is clearly visible that the region where the solution has a kink, i.e. the circle at radial distance 0.5 from the center, is refined most. Furthermore, the central region where the solution is very smooth and almost flat, is almost not refined at all, but this results from the fact that we did not take into account that the coefficient is large there. The region outside is refined rather arbitrarily, since the second derivative is constant there and refinement is therefore mostly based on the size of the cells and their deviation from the optimal square. 




[1.x.2218] 

[1.x.2219] 

[1.x.2220] 


One thing that is always worth playing around with if one solves problems of appreciable size (much bigger than the one we have here) is to try different solvers or preconditioners. In the current case, the linear system is symmetric and positive definite, which makes the CG algorithm pretty much the canonical choice for solving. However, the SSOR preconditioner we use in the  [2.x.5665]  function is up for grabs. 

In deal.II, it is relatively simple to change the preconditioner. For example, by changing the existing lines of code 

[1.x.2221] 

into 

[1.x.2222] 

we can try out different relaxation parameters for SSOR. By using 

[1.x.2223] 

we can use Jacobi as a preconditioner. And by using 

[1.x.2224] 

we can use a simple incomplete LU decomposition without any thresholding or strengthening of the diagonal (to use this preconditioner, you have to also add the header file  [2.x.5666]  to the include list at the top of the file). 

Using these various different preconditioners, we can compare the number of CG iterations needed (available through the  [2.x.5667]  call, see step-4) as well as CPU time needed (using the Timer class, discussed, for example, in step-28) and get the following results (left: iterations; right: CPU time): 

 [2.x.5668]  

As we can see, all preconditioners behave pretty much the same on this simple problem, with the number of iterations growing like  [2.x.5669]  and because each iteration requires around  [2.x.5670]  operations the total CPU time grows like  [2.x.5671]  (for the few smallest meshes, the CPU time is so small that it doesn't record). Note that even though it is the simplest method, Jacobi is the fastest for this problem. 

The situation changes slightly when the finite element is not a bi-quadratic one as set in the constructor of this program, but a bi-linear one. If one makes this change, the results are as follows: 

 [2.x.5672]  

In other words, while the increase in iterations and CPU time is as before, Jacobi is now the method that requires the most iterations; it is still the fastest one, however, owing to the simplicity of the operations it has to perform. This is not to say that Jacobi is actually a good preconditioner -- for problems of appreciable size, it is definitely not, and other methods will be substantially better -- but really only that it is fast because its implementation is so simple that it can compensate for a larger number of iterations. 

The message to take away from this is not that simplicity in preconditioners is always best. While this may be true for the current problem, it definitely is not once we move to more complicated problems (elasticity or Stokes, for examples step-8 or step-22). Secondly, all of these preconditioners still lead to an increase in the number of iterations as the number  [2.x.5673]  of degrees of freedom grows, for example  [2.x.5674] ; this, in turn, leads to a total growth in effort as  [2.x.5675]  since each iteration takes  [2.x.5676]  work. This behavior is undesirable: we would really like to solve linear systems with  [2.x.5677]  unknowns in a total of  [2.x.5678]  work; there is a class of preconditioners that can achieve this, namely geometric (step-16, step-37, step-39) or algebraic multigrid (step-31, step-40, and several others) preconditioners. They are, however, significantly more complex than the preconditioners outlined above. 

Finally, the last message to take home is that when the data shown above was generated (in 2018), linear systems with 100,000 unknowns are easily solved on a desktop machine in about a second, making the solution of relatively simple 2d problems even to very high accuracy not that big a task as it used to be even in the past. At the time, the situation for 3d problems was entirely different, but even that has changed substantially in the intervening time -- though solving problems in 3d to high accuracy remains a challenge. 




[1.x.2225] 

If you look at the meshes above, you will see even though the domain is the unit disk, and the jump in the coefficient lies along a circle, the cells that make up the mesh do not track this geometry well. The reason, already hinted at in step-1, is that in the absence of other information, the Triangulation class only sees a bunch of coarse grid cells but has, of course, no real idea what kind of geometry they might represent when looked at together. For this reason, we need to tell the Triangulation what to do when a cell is refined: where should the new vertices at the edge midpoints and the cell midpoint be located so that the child cells better represent the desired geometry than the parent cell. 

To visualize what the triangulation actually knows about the geometry, it is not enough to just output the location of vertices and draw a straight line for each edge; instead, we have to output both interior and boundary lines as multiple segments so that they look curved. We can do this by making one change to the gnuplot part of  [2.x.5679] : 

[1.x.2226] 



In the code above, we already do this for faces that sit at the boundary: this happens automatically since we use  [2.x.5680]  which attaches a SphericalManifold to the boundary of the domain. To make the mesh [1.x.2227] also track a circular domain, we need to work a bit harder, though. First, recall that our coarse mesh consists of a central square cell and four cells around it. Now first consider what would happen if we also attached the SphericalManifold object not only to the four exterior faces but also the four cells at the perimeter as well as all of their faces. We can do this by adding the following snippet (testing that the center of a cell is larger than a small multiple, say one tenth, of the cell diameter away from center of the mesh only fails for the central square of the mesh): 

[1.x.2228] 



After a few global refinement steps, this would lead to a mesh of the following kind: 


  <div class="onecolumn" style="width: 80%">     <div>       <img src="https://www.dealii.org/images/steps/developer/step_6_bad_grid_4.svg"            alt="Grid where some central cells are nearly triangular."            width="300" height="300">     </div>   </div> 

This is not a good mesh: the central cell has been refined in such a way that the children located in the four corners of the original central cell [1.x.2229]: they all tend towards triangles as mesh refinement continues. This means that the Jacobian matrix of the transformation from reference cell to actual cell degenerates for these cells, and because all error estimates for finite element solutions contain the norm of the inverse of the Jacobian matrix, you will get very large errors on these cells and, in the limit as mesh refinement, a loss of convergence order because the cells in these corners become worse and worse under mesh refinement. 

So we need something smarter. To this end, consider the following solution originally developed by Konstantin Ladutenko. We will use the following code: 

[1.x.2230] 



This code then generates the following, much better sequence of meshes: 

<div class="twocolumn" style="width: 80%">   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_0_ladutenko.svg"          alt="Initial grid: the Ladutenko grid with one global refinement."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_1_ladutenko.svg"          alt="First adaptively refined Ladutenko grid."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_2_ladutenko.svg"          alt="Second adaptively refined Ladutenko grid."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_3_ladutenko.svg"          alt="Third adaptively refined Ladutenko grid."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_4_ladutenko.svg"          alt="Fourth adaptively refined Ladutenko grid. The cells are clustered          along the inner circle."          width="300" height="300">   </div>   <div>     <img src="https://www.dealii.org/images/steps/developer/step_6_grid_5_ladutenko.svg"          alt="Fifth adaptively refined Ladutenko grid: the cells are clustered          along the inner circle."          width="300" height="300">   </div> </div> 

Creating good meshes, and in particular making them fit the geometry you want, is a complex topic in itself. You can find much more on this in step-49, step-53, and step-54, among other tutorial programs that cover the issue. step-65 shows another, less manual way to achieve a mesh well fit to the problem here. Information on curved domains can also be found in the documentation module on  [2.x.5681]  "Manifold descriptions". 

Why does it make sense to choose a mesh that tracks the internal interface? There are a number of reasons, but the most essential one comes down to what we actually integrate in our bilinear form. Conceptually, we want to integrate the term  [2.x.5682]  as the contribution of cell  [2.x.5683]  to the matrix entry  [2.x.5684] . We can not compute it exactly and have to resort to quadrature. We know that quadrature is accurate if the integrand is smooth. That is because quadrature in essence computes a polynomial approximation to the integrand that coincides with the integrand in the quadrature points, and then computes the volume under this polynomial as an approximation to the volume under the original integrand. This polynomial interpolant is accurate if the integrand is smooth on a cell, but it is usually rather inaccurate if the integrand is discontinuous on a cell. 

Consequently, it is worthwhile to align cells in such a way that the interfaces across which the coefficient is discontinuous are aligned with cell interfaces. This way, the coefficient is constant on each cell, following which the integrand will be smooth, and its polynomial approximation and the quadrature approximation of the integral will both be accurate. Note that such an alignment is common in many practical cases, so deal.II provides a number of functions (such as  [2.x.5685]  "material_id") to help manage such a scenario. Refer to step-28 and step-46 for examples of how material ids can be applied. 

Finally, let us consider the case of a coefficient that has a smooth and non-uniform distribution in space. We can repeat once again all of the above discussion on the representation of such a function with the quadrature. So, to simulate it accurately there are a few readily available options: you could reduce the cell size, increase the order of the polynomial used in the quadrature formula, select a more appropriate quadrature formula, or perform a combination of these steps. The key is that providing the best fit of the coefficient's spatial dependence with the quadrature polynomial will lead to a more accurate finite element solution of the PDE. 

As a final note: The discussion in the previous paragraphs shows, we here have a very concrete way of stating what we think of a good mesh -- it should be aligned with the jump in the coefficient. But one could also have asked this kind of question in a more general setting: Given some equation with a smooth solution and smooth coefficients, can we say what a good mesh would look like? This is a question for which the answer is easier to state in intuitive terms than mathematically: A good mesh has cells that all, by and large, look like squares (or cubes, in 3d). A bad mesh would contain cells that are very elongated in some directions or, more generally, for which there are cells that have both short and long edges. There are many ways in which one could assign a numerical quality index to each cell that measures whether the cell is "good" or "bad"; some of these are often chosen because they are cheap and easy to compute, whereas others are based on what enters into proofs of convergence. An example of the former would be the ratio of the longest to the shortest edge of a cell: In the ideal case, that ratio would be one; bad cells have values much larger than one. An example of the latter kind would consider the gradient (the "Jacobian") of the mapping from the reference cell  [2.x.5686]  to the real cell  [2.x.5687] ; this gradient is a matrix, and a quantity that enters into error estimates is the maximum over all points on the reference cell of the ratio of the largest to the smallest eigenvalue of this matrix. It is again not difficult to see that this ratio is constant if the cell  [2.x.5688]  is an affine image of  [2.x.5689] , and that it is one for squares and cubes. 

In practice, it might be interesting to visualize such quality measures. The function  [2.x.5690]  provides one way to get this kind of information. Even better, visualization tools such as VisIt often allow you to visualize this sort of information for a variety of measures from within the visualization software itself; in the case of VisIt, just add a "pseudo-color" plot and select one of the mesh quality measures instead of the solution field. 




[1.x.2231] 

From a mathematical perspective, solutions of the Laplace equation 

[1.x.2232] 

on smoothly bounded, convex domains are known to be smooth themselves. The exact degree of smoothness, i.e., the function space in which the solution lives, depends on how smooth exactly the boundary of the domain is, and how smooth the right hand side is. Some regularity of the solution may be lost at the boundary, but one generally has that the solution is twice more differentiable in compact subsets of the domain than the right hand side. If, in particular, the right hand side satisfies  [2.x.5691] , then  [2.x.5692]  where  [2.x.5693]  is any compact subset of  [2.x.5694]  ( [2.x.5695]  is an open domain, so a compact subset needs to keep a positive distance from  [2.x.5696] ). 

The situation we chose for the current example is different, however: we look at an equation with a non-constant coefficient  [2.x.5697] : 

[1.x.2233] 

Here, if  [2.x.5698]  is not smooth, then the solution will not be smooth either, regardless of  [2.x.5699] . In particular, we expect that wherever  [2.x.5700]  is discontinuous along a line (or along a plane in 3d), the solution will have a kink. This is easy to see: if for example  [2.x.5701]  is continuous, then  [2.x.5702]  needs to be continuous. This means that  [2.x.5703]  must be continuously differentiable (not have a kink). Consequently, if  [2.x.5704]  has a discontinuity, then  [2.x.5705]  must have an opposite discontinuity so that the two exactly cancel and their product yields a function without a discontinuity. But for  [2.x.5706]  to have a discontinuity,  [2.x.5707]  must have a kink. This is of course exactly what is happening in the current example, and easy to observe in the pictures of the solution. 

In general, if the coefficient  [2.x.5708]  is discontinuous along a line in 2d, or a plane in 3d, then the solution may have a kink, but the gradient of the solution will not go to infinity. That means, that the solution is at least still in the [1.x.2234]  [2.x.5709]  (i.e., roughly speaking, in the space of functions whose derivatives are bounded). On the other hand, we know that in the most extreme cases -- i.e., where the domain has reentrant corners, the right hand side only satisfies  [2.x.5710] , or the coefficient  [2.x.5711]  is only in  [2.x.5712]  -- all we can expect is that  [2.x.5713]  (i.e., the [1.x.2235] of functions whose derivative is square integrable), a much larger space than  [2.x.5714] . It is not very difficult to create cases where the solution is in a space  [2.x.5715]  where we can get  [2.x.5716]  to become as small as we want. Such cases are often used to test adaptive finite element methods because the mesh will have to resolve the singularity that causes the solution to not be in  [2.x.5717]  any more. 

The typical example one uses for this is called the [1.x.2236] (referring to  [2.x.5718] ), which in the commonly used form has a coefficient  [2.x.5719]  that has different values in the four quadrants of the plane (or eight different values in the octants of  [2.x.5720] ). The exact degree of regularity (the  [2.x.5721]  in the index of the Sobolev space above) depends on the values of  [2.x.5722]  coming together at the origin, and by choosing the jumps large enough, the regularity of the solution can be made as close as desired to  [2.x.5723] . 

To implement something like this, one could replace the coefficient function by the following (shown here only for the 2d case): 

[1.x.2237] 

(Adding the  [2.x.5724]  at the end ensures that either an exception is thrown or that the program aborts if we ever get to that point 

-- which of course we shouldn't, but this is a good way to insure yourself: we all make mistakes by sometimes not thinking of all cases, for example by checking for  [2.x.5725]  to be less than and greater than zero, rather than greater-or-equal to zero, and thereby forgetting some cases that would otherwise lead to bugs that are awkward to find. The  [2.x.5726]  at the end is only there to avoid compiler warnings that the function does not end in a  [2.x.5727]  statement -- the compiler cannot see that the function would never actually get to that point because of the preceding  [2.x.5728]  statement.) 

By playing with such cases where four or more sectors come together and on which the coefficient has different values, one can construct cases where the solution has singularities at the origin. One can also see how the meshes are refined in such cases. 


examples/step-60/doc/intro.dox 

 [2.x.5729]  

[1.x.2238] 

 [2.x.5730]  




[1.x.2239] 

[1.x.2240] 


In this tutorial we consider the case of two domains,  [2.x.5731]  in  [2.x.5732]  and  [2.x.5733]  in  [2.x.5734] , where  [2.x.5735]  is embedded in  [2.x.5736]  ( [2.x.5737] ). We want to solve a partial differential equation on  [2.x.5738] , enforcing some conditions on the solution of the problem *on the embedded domain*  [2.x.5739] . 

There are two interesting scenarios: 

- the geometrical dimension `dim` of the embedded domain  [2.x.5740]  is the same of the domain  [2.x.5741]  (`spacedim`), that is, the spacedim-dimensional measure of  [2.x.5742]  is not zero, or 

- the embedded domain  [2.x.5743]  has an intrinsic dimension `dim` which is smaller than that of  [2.x.5744]  (`spacedim`), thus its spacedim-dimensional measure is zero; for example it is a curve embedded in a two dimensional domain, or a surface embedded in a three-dimensional domain. 

In both cases define the restriction operator  [2.x.5745]  as the operator that, given a continuous function on  [2.x.5746] , returns its (continuous) restriction on  [2.x.5747] , i.e., 

[1.x.2241] 

It is well known that the operator  [2.x.5748]  can be extended to a continuous operator on  [2.x.5749] , mapping functions in  [2.x.5750]  to functions in  [2.x.5751]  when the intrinsic dimension of  [2.x.5752]  is the same of  [2.x.5753] . 

The same is true, with a less regular range space (namely  [2.x.5754] ), when the dimension of  [2.x.5755]  is one less with respect to  [2.x.5756] , and  [2.x.5757]  does not have a boundary. In this second case, the operator  [2.x.5758]  is also known as the *trace* operator, and it is well defined for Lipschitz co-dimension one curves and surfaces  [2.x.5759]  embedded in  [2.x.5760]  (read  [1.x.2242] for further details on the trace operator). 

The co-dimension two case is a little more complicated, and in general it is not possible to construct a continuous trace operator, not even from  [2.x.5761]  to  [2.x.5762] , when the dimension of  [2.x.5763]  is zero or one respectively in two and three dimensions. 

In this tutorial program we're not interested in further details on  [2.x.5764] : we take the extension  [2.x.5765]  for granted, assuming that the dimension of the embedded domain (`dim`) is always smaller by one or equal with respect to the dimension of the embedding domain  [2.x.5766]  (`spacedim`). 

We are going to solve the following differential problem: given a sufficiently regular function  [2.x.5767]  on  [2.x.5768] , find the solution  [2.x.5769]  to 

[1.x.2243] 



This is a constrained problem, where we are looking for a harmonic function  [2.x.5770]  that satisfies homogeneous boundary conditions on  [2.x.5771] , subject to the constraint  [2.x.5772]  using a Lagrange multiplier. 

This problem has a physical interpretation: harmonic functions, i.e., functions that satisfy the Laplace equation, can be thought of as the displacements of a membrane whose boundary values are prescribed. The current situation then corresponds to finding the shape of a membrane for which not only the displacement at the boundary, but also on  [2.x.5773]  is prescribed. For example, if  [2.x.5774]  is a closed curve in 2d space, then that would model a soap film that is held in place by a wire loop along  [2.x.5775]  as well as a second loop along  [2.x.5776] . In cases where  [2.x.5777]  is a whole area, you can think of this as a membrane that is stretched over an obstacle where  [2.x.5778]  is the contact area. (If the contact area is not known we have a different problem -- called the "obstacle problem" -- which is modeled in step-41.) 

As a first example we study the zero Dirichlet boundary condition on  [2.x.5779] . The same equations apply if we apply zero Neumann boundary conditions on  [2.x.5780]  or a mix of the two. 

The variational formulation can be derived by introducing two infinite dimensional spaces  [2.x.5781]  and  [2.x.5782] , respectively for the solution  [2.x.5783]  and for the Lagrange multiplier  [2.x.5784] . 

Multiplying the first equation by  [2.x.5785]  and the second by  [2.x.5786] , integrating by parts when possible, and exploiting the boundary conditions on  [2.x.5787] , we obtain the following variational problem: 

Given a sufficiently regular function  [2.x.5788]  on  [2.x.5789] , find the solution  [2.x.5790]  to 

[1.x.2244] 



where  [2.x.5791]  and  [2.x.5792]  represent, respectively,  [2.x.5793]  scalar products in  [2.x.5794]  and in  [2.x.5795] . 

Inspection of the variational formulation tells us that the space  [2.x.5796]  can be taken to be  [2.x.5797] . The space  [2.x.5798] , in the co-dimension zero case, should be taken as  [2.x.5799] , while in the co-dimension one case should be taken as  [2.x.5800] . 

The function  [2.x.5801]  should therefore be either in  [2.x.5802]  (for the co-dimension zero case) or  [2.x.5803]  (for the co-dimension one case). This leaves us with a Lagrange multiplier  [2.x.5804]  in  [2.x.5805] , which is either  [2.x.5806]  or  [2.x.5807] . 

There are two options for the discretization of the problem above. One could choose matching discretizations, where the Triangulation for  [2.x.5808]  is aligned with the Triangulation for  [2.x.5809] , or one could choose to discretize the two domains in a completely independent way. 

The first option is clearly more indicated for the simple problem we proposed above: it is sufficient to use a single Triangulation for  [2.x.5810]  and then impose certain constraints depending  [2.x.5811] . An example of this approach is studied in step-40, where the solution has to stay above an obstacle and this is achieved imposing constraints on  [2.x.5812] . 

To solve more complex problems, for example one where the domain  [2.x.5813]  is time dependent, the second option could be a more viable solution. Handling non aligned meshes is complex by itself: to illustrate how is done we study a simple problem. 

The technique we describe here is presented in the literature using one of many names: the [1.x.2245], the [1.x.2246], the [1.x.2247], and others. The main principle is that the discretization of the two grids and of the two finite element spaces are kept complet