[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33]
* [1.x.34][1.x.35][1.x.36]
* 

*  [2.x.2] 
* This program is devoted to two aspects: the use of mixed finite elements
* 
*  -  inparticular Raviart-Thomas elements
* 
*  -  and using block matrices to definesolvers, preconditioners, and nested versions of those that use thesubstructure of the system matrix. The equation we are going to solve is againthe Poisson equation, though with a matrix-valued coefficient:[1.x.37]
*  [2.x.3]  is assumed to be uniformly positive definite, i.e., there is [2.x.4]  such that the eigenvalues  [2.x.5]  of  [2.x.6]  satisfy [2.x.7] . The use of the symbol  [2.x.8]  instead of the usual [2.x.9]  for the solution variable will become clear in the next section.
* After discussing the equation and the formulation we are going to use to solveit, this introduction will cover the use of block matrices and vectors, thedefinition of solvers and preconditioners, and finally the actual test case weare going to solve.
* We are going to extend this tutorial program in  [2.x.10]  tosolve not only the mixed Laplace equation, but add another equation thatdescribes the transport of a mixture of two fluids.
* The equations covered here fall into the class of vector-valued problems. Atoplevel overview of this topic can be found in the  [2.x.11]  module.
* 

* [1.x.38][1.x.39]
* 

* In the form above, the Poisson equation (i.e., the Laplace equation with a nonzeroright hand side) is generally considered a good model equationfor fluid flow in porous media. Of course, one typically models fluid flow throughthe [1.x.40] or, if fluid velocities are slow or the viscosity is large, the[1.x.41](which we cover in  [2.x.12] ).In the first of these two models, the forces that act are inertia andviscous friction, whereas in the second it is only viscous friction
* 
*  -  i.e.,forces that one fluid particle exerts on a nearby one. This is appropriateif you have free flow in a large domain, say a pipe, a river, or in the air.On the other hand, if the fluid is confined in pores, then friction forcesexerted by the pore walls on the fluid become more and more important andinternal viscous friction becomes less and less important. Modeling thisthen first leads to the[1.x.42] if both effects are important, and in the limit of very small poresto the [1.x.43].The latter is just a different name for the Poisson or Laplace equation,connotating it with the area to which one wants to apply it: slow flowin a porous medium. In essence it says that the velocity is proportionalto the negative pressure gradient that drives the fluid through theporous medium.
* The Darcy equation models this pressure that drives the flow. (Because thesolution variable is a pressure, we here use the name  [2.x.13]  instead of thename  [2.x.14]  more commonly used for the solution of partial differential equations.)Typical applications of this view of the Laplace equation are then modelinggroundwater flow, or the flow of hydrocarbons in oil reservoirs. In theseapplications,  [2.x.15]  is the permeability tensor, i.e., a measure for how muchresistance the soil or rock matrix asserts on the fluid flow.
* In the applications named above, a desirable feature for a numericalscheme is that it should be locally conservative, i.e., that whateverflows into a cell also flows out of it (or the difference is equal tothe integral over the source terms over each cell, if the sources arenonzero). However, as it turns out, the usual discretizations of theLaplace equation (such as those used in  [2.x.16] ,  [2.x.17] , or  [2.x.18] ) donot satisfy this property. But, one can achieve this by choosing adifferent formulation of the problem and a particular combination offinite element spaces.
* 

* [1.x.44][1.x.45]
* 

* To this end, one first introduces a second variable, called the velocity, [2.x.19] . By its definition, the velocity is a vector in thenegativedirection of the pressure gradient, multiplied by the permeability tensor. Ifthe permeability tensor is proportional to the unit matrix, this equation iseasy to understand and intuitive: the higher the permeability, the higher thevelocity; and the velocity is proportional to the gradient of the pressure, going fromareas of high pressure to areas of low pressure (thus the negative sign).
* With this second variable, one then finds an alternative version of theLaplace equation, called the [1.x.46]:[1.x.47]
* Here, we have multiplied the equation defining the velocity  [2.x.20]  by  [2.x.21]  because this makes the set of equations symmetric: oneof the equations has the gradient, the second the negative divergence,and these two are of course adjoints of each other, resulting in asymmetric bilinear form and a consequently symmetric system matrixunder the common assumption that  [2.x.22]  is a symmetric tensor.
* The weak formulation of this problem is found by multiplying the twoequations with test functions and integrating some terms by parts:[1.x.48]
* where[1.x.49]
* Here,  [2.x.23]  is the outward normal vector at the boundary. Note how in thisformulation, Dirichlet boundary values of the original problem areincorporated in the weak form.
* To be well-posed, we have to look for solutions and test functions in thespace  [2.x.24] for  [2.x.25] , [2.x.26] , and  [2.x.27]  for  [2.x.28] . It is a well-known fact stated inalmost every book on finite element theory that if one chooses discrete finiteelement spaces for the approximation of  [2.x.29]  inappropriately, then theresulting discrete problem is instable and the discrete solutionwill not converge to the exact solution. (Some details on the problemconsidered here
* 
*  -  which falls in the class of "saddle-point problems"
* 
*  -  can be found on the Wikipedia page on the [1.x.50].)
* To overcome this, a number of different finite element pairs for  [2.x.30] have been developed that lead to a stable discrete problem. One such pair isto use the Raviart-Thomas spaces  [2.x.31]  for the velocity  [2.x.32]  anddiscontinuous elements of class  [2.x.33]  for the pressure  [2.x.34] . For detailsabout these spaces, we refer in particular to the book on mixed finite elementmethods by Brezzi and Fortin, but many other books on the theory of finiteelements, for example the classic book by Brenner and Scott, also state therelevant results. In any case, with appropriate choices of functionspaces, the discrete formulation reads as follows: Find  [2.x.35]  so that[1.x.51]
* 
* 

* Before continuing, let us briefly pause and show that the choice offunction spaces above provides us with the desired local conservationproperty. In particular, because the pressure space consists ofdiscontinuous piecewise polynomials, we can choose the test function [2.x.36]  as the function that is equal to one on any given cell  [2.x.37]  andzero everywhere else. If we also choose  [2.x.38]  everywhere(remember that the weak form above has to hold for [1.x.52] discretetest functions  [2.x.39] ), then putting these choices of test functionsinto the weak formulation above implies in particular that[1.x.53]
* which we can of course write in more explicit form as[1.x.54]
* Applying the divergence theorem results in the fact that  [2.x.40]  has to satisfy, for every choice of cell  [2.x.41] , the relationship[1.x.55]
* If you now recall that  [2.x.42]  was the velocity, then theintegral on the left is exactly the (discrete) flux across theboundary of the cell  [2.x.43] . The statement is then that the flux must be equalto the integral over the sources within  [2.x.44] . In particular, if thereare no sources (i.e.,  [2.x.45]  in  [2.x.46] ), then the statement is that[1.x.56] flux is zero, i.e., whatever flows into a cell must flow outof it through some other part of the cell boundary. This is what we call[1.x.57] because it holds for every cell.
* On the other hand, the usual continuous  [2.x.47]  elements would not result inthis kind of property when used for the pressure (as, for example, wedo in  [2.x.48] ) because one can not choose a discrete test function [2.x.49]  that is one on a cell  [2.x.50]  and zero everywhere else: It would bediscontinuous and consequently not in the finite elementspace. (Strictly speaking, all we can say is that the proof abovewould not work for continuous elements. Whether these elements mightstill result in local conservation is a different question as onecould think that a different kind of proof might still work; inreality, however, the property really does not hold.)
* 

* 
* [1.x.58][1.x.59]
* 

* The deal.II library (of course) implements Raviart-Thomas elements  [2.x.51]  ofarbitrary order  [2.x.52] , as well as discontinuous elements  [2.x.53] . If we forgetabout their particular properties for a second, we then have to solve adiscrete problem[1.x.60]
* with the bilinear form and right hand side as stated above, and  [2.x.54] ,  [2.x.55] . Both  [2.x.56]  and  [2.x.57]  are from the space [2.x.58] , where  [2.x.59]  is itself a space of  [2.x.60] -dimensionalfunctions to accommodate for the fact that the flow velocity is vector-valued.The necessary question then is: how do we do this in a program?
* Vector-valued elements have already been discussed in previous tutorialprograms, the first time and in detail in  [2.x.61] . The main difference therewas that the vector-valued space  [2.x.62]  is uniform in all its components: the [2.x.63]  components of the displacement vector are all equal and from the samefunction space. What we could therefore do was to build  [2.x.64]  as the outerproduct of the  [2.x.65]  times the usual  [2.x.66]  finite element space, and by thismake sure that all our shape functions have only a single non-zero vectorcomponent. Instead of dealing with vector-valued shape functions, all we didin  [2.x.67]  was therefore to look at the (scalar) only non-zero component anduse the  [2.x.68]  call to figure outwhich component this actually is.
* This doesn't work with Raviart-Thomas elements: following from theirconstruction to satisfy certain regularity properties of the space [2.x.69] , the shape functions of  [2.x.70]  are usually nonzero in alltheir vector components at once. For this reason, were [2.x.71]  applied to determine the onlynonzero component of shape function  [2.x.72] , an exception would be generated. Whatwe really need to do is to get at  [2.x.73] all [2.x.74]  vector components of a shapefunction. In deal.II diction, we call such finite elements [2.x.75] non-primitive [2.x.76] , whereas finite elements that are either scalar or forwhich every vector-valued shape function is nonzero only in a single vectorcomponent are called  [2.x.77] primitive [2.x.78] .
* So what do we have to do for non-primitive elements? To figure this out, letus go back in the tutorial programs, almost to the very beginnings. There, welearned that we use the  [2.x.79]  class to determine the values andgradients of shape functions at quadrature points. For example, we would call [2.x.80]  to obtain the value of the [2.x.81] th shape function on the quadrature point with number [2.x.82] . Later, in  [2.x.83]  and other tutorial programs, we learnedthat this function call also works for vector-valued shape functions (ofprimitive finite elements), and that it returned the value of the onlynon-zero component of shape function  [2.x.84]  at quadrature point [2.x.85] .
* For non-primitive shape functions, this is clearly not going to work: there isno single non-zero vector component of shape function  [2.x.86] , and the callto  [2.x.87]  would consequently not makemuch sense. However, deal.II offers a second function call, [2.x.88]  that returns thevalue of the  [2.x.89]  atquadrature point  [2.x.90]  is an index betweenzero and the number of vector components of the present finite element; forexample, the element we will use to describe velocities and pressures is goingto have  [2.x.91]  components. It is worth noting that this function call canalso be used for primitive shape functions: it will simply return zero for allcomponents except one; for non-primitive shape functions, it will in generalreturn a non-zero value for more than just one component.
* We could now attempt to rewrite the bilinear form above in terms of vectorcomponents. For example, in 2d, the first term could be rewritten like this(note that  [2.x.92] ):[1.x.61]
* If we implemented this, we would get code like this:
* [1.x.62]
* 
* This is, at best, tedious, error prone, and not dimension independent. Thereare obvious ways to make things dimension independent, but in the end, thecode is simply not pretty. What would be much nicer is if we could simplyextract the  [2.x.93]  and  [2.x.94]  components of a shape function  [2.x.95] . In theprogram we do that in the following way:
* [1.x.63]
* 
* This is, in fact, not only the first term of the bilinear form, but thewhole thing (sans boundary contributions).
* What this piece of code does is, given an  [2.x.96]  object, to extractthe values of the first  [2.x.97]  components of shape function  [2.x.98]  atquadrature points  [2.x.99] , that is the velocity components of that shapefunction. Put differently, if we write shape functions  [2.x.100]  as the tuple [2.x.101] , then the function returns the velocity part of thistuple. Note that the velocity is of course a  [2.x.102] -dimensional tensor, andthat the function returns a corresponding object. Similarly, where wesubscript with the pressure extractor, we extract the scalar pressurecomponent. The whole mechanism is described in more detail in the [2.x.103]  module.
* In practice, it turns out that we can do a bit better if we evaluate the shapefunctions, their gradients and divergences only once per outermost loop, andstore the result, as this saves us a few otherwise repeated computations (it ispossible to save even more repeated operations by calculating all relevantquantities in advance and then only inserting the results in the actual loop,see  [2.x.104]  for a realization of that approach).The final result then looks like this, working in every space dimension:
* [1.x.64]
* 
* This very closely resembles the form in which we have originally written downthe bilinear form and right hand side.
* There is one final term that we have to take care of: the right hand sidecontained the term  [2.x.105] , constituting theweak enforcement of pressure boundary conditions. We have already seen in [2.x.106]  how to deal with face integrals: essentially exactly the same as withdomain integrals, except that we have to use the FEFaceValues classinstead of  [2.x.107] . To compute the boundary term we then simply haveto loop over all boundary faces and integrate there. The mechanism works inthe same way as above, i.e. the extractor classes also work on FEFaceValues objects:
* [1.x.65]
* 
* You will find the exact same code as above in the sources for the presentprogram. We will therefore not comment much on it below.
* 

* [1.x.66][1.x.67]
* 

* After assembling the linear system we are faced with the task of solvingit. The problem here is that the matrix possesses two undesirable properties:
* 
*  - It is [1.x.68],  i.e., it has both positive and negative eigenvalues.  We don't want to prove this property here, but note that this is true  for all matrices of the form   [2.x.108]   such as the one here where  [2.x.109]  is positive definite.
* 
*  - The matrix has a zero block at the bottom right (there is no term in  the bilinear form that couples the pressure  [2.x.110]  with the  pressure test function  [2.x.111] ).
* At least it is symmetric, but the first issue above still means thatthe Conjugate Gradient method is not going to work since it is onlyapplicable to problems in which the matrix is symmetric and positive definite.We would have to resort to other iterative solvers instead, such asMinRes, SymmLQ, or GMRES, that can deal with indefinite systems. However, thenthe next problem immediately surfaces: Due to the zero block, there are zeroson the diagonal and none of the usual, "simple" preconditioners (Jacobi, SSOR)will work as they require division by diagonal elements.
* For the matrix sizes we expect to run with this program, the by far simplestapproach would be to just use a direct solver (in particular, theSparseDirectUMFPACK class that is bundled with deal.II).  [2.x.112]  goes thisroute and shows that solving [1.x.69] linear system can be done in just3 or 4 lines of code.
* But then, this is a tutorial: We teach how to do things. Consequently,in the following, we will introduce some techniques that can be used in caseslike these. Namely, we will consider the linear system as not consisting of onelarge matrix and vectors, but we will want to decompose matricesinto [1.x.70] that correspond to the individual operators that appear inthe system. We note that the resulting solver is not optimal
* 
*  -  there aremuch better ways to efficiently compute the system, for example thoseexplained in the results section of  [2.x.113]  or the one we use in  [2.x.114] for a problem similar to the current one. Here, our goal is simply tointroduce new solution techniques and how they can be implemented indeal.II.
* 

* [1.x.71][1.x.72]
* 

* In view of the difficulties using standard solvers and preconditionersmentioned above, let us take another look at the matrix. If we sort ourdegrees of freedom so that all velocity come before all pressure variables,then we can subdivide the linear system  [2.x.115]  into the following blocks:[1.x.73]
* where  [2.x.116]  are the values of velocity and pressure degrees of freedom,respectively,  [2.x.117]  is the mass matrix on the velocity space,  [2.x.118]  corresponds tothe negative divergence operator, and  [2.x.119]  is its transpose and correspondsto the gradient.
* By block elimination, we can then re-order this system in the following way(multiply the first row of the system by  [2.x.120]  and then subtract thesecond row from it):[1.x.74]
* Here, the matrix  [2.x.121]  (called the[1.x.75]of  [2.x.122] )is obviously symmetric and, owing to the positive definiteness of  [2.x.123]  and thefact that  [2.x.124]  has full column rank,  [2.x.125]  is also positivedefinite.
* Consequently, if we could compute  [2.x.126] , we could apply the Conjugate Gradientmethod to it. However, computing  [2.x.127]  is expensive because it requires usto compute the inverse of the (possibly large) matrix  [2.x.128] ; and  [2.x.129]  is in factalso a full matrix because even though  [2.x.130]  is sparse, its inverse  [2.x.131] will generally be a dense matrix.On the other hand, the CG algorithm doesn't requireus to actually have a representation of  [2.x.132] : It is sufficient to formmatrix-vector products with it. We can do so in steps, using the fact thatmatrix products are associative (i.e., we can set parentheses in such away that the product is more convenient to compute):To compute  [2.x.133] , we [2.x.134]   [2.x.135]  compute  [2.x.136] ;  [2.x.137]  solve  [2.x.138]  for  [2.x.139] , using the CG method applied to the  positive definite and symmetric mass matrix  [2.x.140] ;  [2.x.141]  compute  [2.x.142]  to obtain  [2.x.143] . [2.x.144] Note how we evaluate the expression  [2.x.145]  right to left toavoid matrix-matrix products; this way, all we have to do is evaluatematrix-vector products.
* In the following, we will then have to come up with ways to represent thematrix  [2.x.146]  so that it can be used in a Conjugate Gradient solver,as well as to define ways in which we can precondition the solutionof the linear system involving  [2.x.147] , and deal with solving linear systemswith the matrix  [2.x.148]  (the second step above).
*  [2.x.149]  The key point in this consideration is to recognize that to implementan iterative solver such as CG or GMRES, we never actually need the actual[1.x.76] of a matrix! All that is required is that we can formmatrix-vector products. The same is true for preconditioners. In deal.II weencode this requirement by only requiring that matrices and preconditionersgiven to solver classes have a  [2.x.150]  member function thatdoes the matrix-vector product. How a class chooses to implement thisfunction is not important to the solver. Consequently, classes canimplement it by, for example, doing a sequence of products and linearsolves as discussed above.
* 

* [1.x.77][1.x.78]
* 

* deal.II includes support for describing such linear operations in a verygeneral way. This is done with the LinearOperator class that, like [2.x.151]  "the MatrixType concept",defines a minimal interface for [1.x.79] a linear operation to avector:
* [1.x.80]
* The key difference between a LinearOperator and an ordinary matrix ishowever that a LinearOperator does not allow any further access to theunderlying object. All you can do with a LinearOperator is to apply its"action" to a vector! We take the opportunity to introduce theLinearOperator concept at this point because it is a very useful tool thatallows you to construct complex solvers and preconditioners in a veryintuitive manner.
* As a first example let us construct a LinearOperator object that represents [2.x.152] . This means that whenever the  [2.x.153]  function ofthis operator is called it has to solve a linear system. This requires usto specify a solver (and corresponding) preconditioner. Assuming that [2.x.154]  is a reference to the upper left block of the system matrixwe can write:
* [1.x.81]
* Rather than using a SolverControl we use the ReductionControl class herethat stops iterations when either an absolute tolerance is reached (forwhich we choose  [2.x.155] ) or when the residual is reduced by a certainfactor (here,  [2.x.156] ). In contrast the SolverControl class only checksfor absolute tolerances. We have to use ReductionControl in our case towork around a minor issue: The right hand sides that we  will feed to [2.x.157]  are essentially formed by residuals that naturallydecrease vastly in norm as the outer iterations progress. This makescontrol by an absolute tolerance very error prone.
* We now have a LinearOperator  [2.x.158]  that we can use toconstruct more complicated operators such as the Schur complement  [2.x.159] .Assuming that  [2.x.160]  is a reference to the upper right blockconstructing a LinearOperator  [2.x.161]  is a matter of two lines:
* [1.x.82]
* Here, the multiplication of three LinearOperator objects yields a compositeobject  [2.x.162]  function first applies [2.x.163] , then  [2.x.164]  (i.e. solving an equation with  [2.x.165] ), and finally  [2.x.166] to any given input vector. In that sense  [2.x.167]  issimilar to the following code:
* [1.x.83]
* ( [2.x.168]  are two temporary vectors). Thekey point behind this approach is the fact that we never actually create aninner product of matrices. Instead, whenever we have to perform a matrixvector multiplication with  [2.x.169]  we simply run all individual [2.x.170]  operations in above sequence.
*  [2.x.171]  We could have achieved the same goal of creating a "matrix like"object by implementing a specialized class  [2.x.172] that provides a suitable  [2.x.173]  function. Skipping over somedetails this might have looked like the following:
* [1.x.84]
* Even though both approaches are exactly equivalent, the LinearOperatorclass has a big advantage over this manual approach.It provides so-called[1.x.85][1.x.86]:Mathematically, we think about  [2.x.174]  as being the composite matrix [2.x.175]  and the LinearOperator class allows you to write this outmore or less verbatim,
* [1.x.87]
* The manual approach on the other hand obscures this fact.
* All that is left for us to do now is to form the right hand sides of thetwo equations defining  [2.x.176]  and  [2.x.177] , and then solve them with the Schurcomplement matrix and the mass matrix, respectively. For example the righthand side of the first equation reads  [2.x.178] . This could beimplemented as follows:
* [1.x.88]
* Again, this is a perfectly valid approach, but the fact that deal.IIrequires us to manually resize the final and temporary vector, and thatevery operation takes up a new line makes this hard to read. This is thepoint where a second class in the linear operator framework can will helpus. Similarly in spirit to LinearOperator, a PackagedOperation stores a"computation":
* [1.x.89]
* The class allows[1.x.90]of expressions involving vectors and linear operators. This is done bystoring the computational expression and only performing the computationwhen either the object is converted to a vector object, or [2.x.179]  (or  [2.x.180]  is invokedby hand. Assuming that  [2.x.181]  are the twovectors of the right hand side we can simply write:
* [1.x.91]
* Here,  [2.x.182]  is a PackagedOperation that [1.x.92] thecomputation we specified. It does not create a vector with the actualresult immediately.
* With these prerequisites at hand, solving for  [2.x.183]  and  [2.x.184]  is a matter ofcreating another solver and inverse:
* [1.x.93]
* 
*  [2.x.185]  The functionality that we developed in this example step by hand isalready readily available in the library. Have a look atschur_complement(), condense_schur_rhs(), and postprocess_schur_solution().
* 

* [1.x.94][1.x.95]
* 

* One may ask whether it would help if we had a preconditioner for the Schurcomplement  [2.x.186] . The general answer, as usual, is: of course. Theproblem is only, we don't know anything about this Schur complement matrix. Wedo not know its entries, all we know is its action. On the other hand, we haveto realize that our solver is expensive since in each iteration we have to doone matrix-vector product with the Schur complement, which means that we haveto do invert the mass matrix once in each iteration.
* There are different approaches to preconditioning such a matrix. On the oneextreme is to use something that is cheap to apply and therefore has no realimpact on the work done in each iteration. The other extreme is apreconditioner that is itself very expensive, but in return really brings downthe number of iterations required to solve with  [2.x.187] .
* We will try something along the second approach, as much to improve theperformance of the program as to demonstrate some techniques. To this end, letus recall that the ideal preconditioner is, of course,  [2.x.188] , but that isunattainable. However, how about[1.x.96]
* as a preconditioner? That would mean that every time we have to do onepreconditioning step, we actually have to solve with  [2.x.189] . At first,this looks almost as expensive as solving with  [2.x.190]  right away. However, notethat in the inner iteration, we do not have to calculate  [2.x.191] , but onlythe inverse of its diagonal, which is cheap.
* Thankfully, the LinearOperator framework makes this very easy to write out.We already used a Jacobi preconditioner ( [2.x.192] ) forthe  [2.x.193]  matrix earlier. So all that is left to do is to write out how theapproximate Schur complement should look like:
* [1.x.97]
* Note how this operator differs in simply doing one Jacobi sweep(i.e. multiplying with the inverses of the diagonal) instead of multiplyingwith the full  [2.x.194] . (This is how a single Jacobi preconditioner stepwith  [2.x.195]  is defined: it is the multiplication with the inverse of thediagonal of  [2.x.196] ; in other words, the operation  [2.x.197] on a vector  [2.x.198]  is exactly what PreconditionJacobi does.)
* With all this we almost have the preconditioner completed: it should be theinverse of the approximate Schur complement. We implement this again bycreating a linear operator with inverse_operator() function. This timehowever we would like to choose a relatively modest tolerance for the CGsolver (that inverts  [2.x.199] ). The reasoning is that [2.x.200] , so weactually do not need to invert it exactly. This, however creates a subtleproblem:  [2.x.201]  will be used in the final outer CGiteration to create an orthogonal basis. But for this to work, it must beprecisely the same linear operation for every invocation. We ensure this byusing an IterationNumberControl that allows us to fix the number of CGiterations that are performed to a fixed small number (in our case 30):
* [1.x.98]
* 
* That's all!
* Obviously, applying this inverse of the approximate Schur complement is a veryexpensive preconditioner, almost as expensive as inverting the Schurcomplement itself. We can expect it to significantly reduce the number ofouter iterations required for the Schur complement. In fact it does: in atypical run on 7 times refined meshes using elements of order 0, the number ofouter iterations drops from 592 to 39. On the other hand, we now have to applya very expensive preconditioner 25 times. A better measure is therefore simplythe run-time of the program: on a current laptop (as of January 2019), itdrops from 3.57 to 2.05 seconds for this test case. That doesn't seem tooimpressive, but the savings become more pronounced on finer meshes and withelements of higher order. For example, an seven times refined mesh andusing elements of order 2 (which amounts to about 0.4 million degrees offreedom) yields an improvement of 1134 to 83 outer iterations, at a runtimeof 168 seconds to 40 seconds. Not earth shattering, but significant.
* 

* [1.x.99][1.x.100]
* 

* In this tutorial program, we will solve the Laplace equation in mixedformulation as stated above. Since we want to monitor convergence of thesolution inside the program, we choose right hand side, boundary conditions,and the coefficient so that we recover a solution function known to us. Inparticular, we choose the pressure solution[1.x.101]
* and for the coefficient we choose the unit matrix  [2.x.202]  forsimplicity. Consequently, the exact velocity satisfies[1.x.102]
* This solution was chosen since it is exactly divergence free, making it arealistic test case for incompressible fluid flow. By consequence, the righthand side equals  [2.x.203] , and as boundary values we have to choose [2.x.204] .
* For the computations in this program, we choose  [2.x.205] . You canfind the resulting solution in the [1.x.103], after the commented program.
* 

*  [1.x.104] [1.x.105]
*   [1.x.106]  [1.x.107]
* 

* 
*  Since this program is only an adaptation of  [2.x.206] , there is not much new stuff in terms of header files. In deal.II, we usually list include files in the order base-lac-grid-dofs-fe-numerics, followed by C++ standard include files:
* 

* 
* [1.x.108]
* 
*  The only two new header files that deserve some attention are those for the LinearOperator and PackagedOperation classes:
* 

* 
* [1.x.109]
* 
*  This is the only significant new header, namely the one in which the Raviart-Thomas finite element is declared:
* 

* 
* [1.x.110]
* 
*  Finally, as a bonus in this program, we will use a tensorial coefficient. Since it may have a spatial dependence, we consider it a tensor-valued function. The following include file provides the  [2.x.207]  class that offers such functionality:
* 

* 
* [1.x.111]
* 
*  The last step is as in all previous programs: We put all of the code relevant to this program into a namespace. (This idea was first introduced in  [2.x.208] .)
* 

* 
* [1.x.112]
* 
*   [1.x.113]  [1.x.114]
* 

* 
*  Again, since this is an adaptation of  [2.x.209] , the main class is almost the same as the one in that tutorial program. In terms of member functions, the main differences are that the constructor takes the degree of the Raviart-Thomas element as an argument (and that there is a corresponding member variable to store this value) and the addition of the  [2.x.210]  function in which, no surprise, we will compute the difference between the exact and the numerical solution to determine convergence of our computations:
* 

* 
* [1.x.115]
* 
*  The second difference is that the sparsity pattern, the system matrix, and solution and right hand side vectors are now blocked. What this means and what one can do with such objects is explained in the introduction to this program as well as further down below when we explain the linear solvers and preconditioners for this problem:
* 

* 
* [1.x.116]
* 
*   [1.x.117]  [1.x.118]
* 

* 
*  Our next task is to define the right hand side of our problem (i.e., the scalar right hand side for the pressure in the original Laplace equation), boundary values for the pressure, and a function that describes both the pressure and the velocity of the exact solution for later computations of the error. Note that these functions have one, one, and  [2.x.211]  components, respectively, and that we pass the number of components down to the  [2.x.212]  base class. For the exact solution, we only declare the function that actually returns the entire solution vector (i.e. all components of it) at once. Here are the respective declarations:
* 

* 
* [1.x.119]
* 
*  And then we also have to define these respective functions, of course. Given our discussion in the introduction of how the solution should look, the following computations should be straightforward:
* 

* 
* [1.x.120]
* 
*   [1.x.121]  [1.x.122]
* 

* 
*  In addition to the other equation data, we also want to use a permeability tensor, or better
* 
*  -  because this is all that appears in the weak form
* 
*  -  the inverse of the permeability tensor,  [2.x.213] . For the purpose of verifying the exactness of the solution and determining convergence orders, this tensor is more in the way than helpful. We will therefore simply set it to the identity matrix.     
*   However, a spatially varying permeability tensor is indispensable in real-life porous media flow simulations, and we would like to use the opportunity to demonstrate the technique to use tensor valued functions.     
*   Possibly unsurprisingly, deal.II also has a base class not only for scalar and generally vector-valued functions (the  [2.x.214]  base class) but also for functions that return tensors of fixed dimension and rank, the  [2.x.215]  template. Here, the function under consideration returns a dim-by-dim matrix, i.e. a tensor of rank 2 and dimension  [2.x.216] . We then choose the template arguments of the base class appropriately.     
*   The interface that the  [2.x.217]  class provides is essentially equivalent to the  [2.x.218]  class. In particular, there exists a  [2.x.219]  function that takes a list of points at which to evaluate the function, and returns the values of the function in the second argument, a list of tensors:
* 

* 
* [1.x.123]
* 
*  The implementation is less interesting. As in previous examples, we add a check to the beginning of the class to make sure that the sizes of input and output parameters are the same (see  [2.x.220]  for a discussion of this technique). Then we loop over all evaluation points, and for each one set the output tensor to the identity matrix.     
*   There is an oddity at the top of the function (the `(void)points;` statement) that is worth discussing. The values we put into the output `values` array does not actually depend on the `points` arrays of coordinates at which the function is evaluated. In other words, the `points` argument is in fact unused, and we could have just not given it a name if we had wanted. But we want to use the `points` object for checking that the `values` object has the correct size. The problem is that in release mode, `AssertDimension` is defined as a macro that expands to nothing; the compiler will then complain that the `points` object is unused. The idiomatic approach to silencing this warning is to have a statement that evaluates (reads) variable but doesn't actually do anything: That's what `(void)points;` does: It reads from `points`, and then casts the result of the read to `void`, i.e., nothing. This statement is, in other words, completely pointless and implies no actual action except to explain to the compiler that yes, this variable is in fact used even in release mode. (In debug mode, the `AssertDimension` macro expands to something that reads from the variable, and so the funny statement would not be necessary in debug mode.)
* 

* 
* [1.x.124]
* 
*   [1.x.125]  [1.x.126]
* 

* 
*   [1.x.127]  [1.x.128]
* 

* 
*  In the constructor of this class, we first store the value that was passed in concerning the degree of the finite elements we shall use (a degree of zero, for example, means to use RT(0) and DG(0)), and then construct the vector valued element belonging to the space  [2.x.221]  described in the introduction. The rest of the constructor is as in the early tutorial programs.   
*   The only thing worth describing here is the constructor call of the  [2.x.222]  class to which this variable belongs has a number of different constructors that all refer to binding simpler elements together into one larger element. In the present case, we want to couple a single RT(degree) element with a single DQ(degree) element. The constructor to  [2.x.223]  that does this requires us to specify first the first base element (the  [2.x.224]  object of given degree) and then the number of copies for this base element, and then similarly the kind and number of  [2.x.225]  elements. Note that the Raviart-Thomas element already has  [2.x.226]  vector components, so that the coupled element will have  [2.x.227]  vector components, the first  [2.x.228]  of which correspond to the velocity variable whereas the last one corresponds to the pressure.   
*   It is also worth comparing the way we constructed this element from its base elements, with the way we have done so in  [2.x.229] : there, we have built it as  [2.x.230] , i.e. we have simply used  [2.x.231]  element, one copy for the displacement in each coordinate direction.
* 

* 
* [1.x.129]
* 
*   [1.x.130]  [1.x.131]
* 

* 
*  This next function starts out with well-known functions calls that create and refine a mesh, and then associate degrees of freedom with it:
* 

* 
* [1.x.132]
* 
*  However, then things become different. As mentioned in the introduction, we want to subdivide the matrix into blocks corresponding to the two different kinds of variables, velocity and pressure. To this end, we first have to make sure that the indices corresponding to velocities and pressures are not intermingled: First all velocity degrees of freedom, then all pressure DoFs. This way, the global matrix separates nicely into a  [2.x.232]  system. To achieve this, we have to renumber degrees of freedom based on their vector component, an operation that conveniently is already implemented:
* 

* 
* [1.x.133]
* 
*  The next thing is that we want to figure out the sizes of these blocks so that we can allocate an appropriate amount of space. To this end, we call the  [2.x.233]  function that counts how many shape functions are non-zero for a particular vector component. We have  [2.x.234]  vector components, and  [2.x.235]  will count how many shape functions belong to each of these components.     
*   There is one problem here. As described in the documentation of that function, it [1.x.134] to put the number of  [2.x.236] -velocity shape functions into  [2.x.237] , the number of  [2.x.238] -velocity shape functions into  [2.x.239]  (and similar in 3d), and the number of pressure shape functions into  [2.x.240] . But, the Raviart-Thomas element is special in that it is non- [2.x.241]  "primitive", i.e., for Raviart-Thomas elements all velocity shape functions are nonzero in all components. In other words, the function cannot distinguish between  [2.x.242]  and  [2.x.243]  velocity functions because there [1.x.135] no such distinction. It therefore puts the overall number of velocity into each of  [2.x.244] ,  [2.x.245] . On the other hand, the number of pressure variables equals the number of shape functions that are nonzero in the dim-th component.     
*   Using this knowledge, we can get the number of velocity shape functions from any of the first  [2.x.246]  elements of  [2.x.247] , and then use this below to initialize the vector and matrix block sizes, as well as create output.     
*  

* 
*  [2.x.248]  If you find this concept difficult to understand, you may want to consider using the function  [2.x.249]  instead, as we do in the corresponding piece of code in  [2.x.250] . You might also want to read up on the difference between  [2.x.251]  "blocks" and  [2.x.252]  "components" in the glossary.
* 

* 
* [1.x.136]
* 
*  The next task is to allocate a sparsity pattern for the matrix that we will create. We use a compressed sparsity pattern like in the previous steps, but as  [2.x.253]  is a block matrix we use the class  [2.x.254]  instead of just  [2.x.255] . This block sparsity pattern has four blocks in a  [2.x.256]  pattern. The blocks' sizes depend on  [2.x.257] , which hold the number of velocity and pressure variables. In the second step we have to instruct the block system to update its knowledge about the sizes of the blocks it manages; this happens with the  [2.x.258]  call.
* 

* 
* [1.x.137]
* 
*  We use the compressed block sparsity pattern in the same way as the non-block version to create the sparsity pattern and then the system matrix:
* 

* 
* [1.x.138]
* 
*  Then we have to resize the solution and right hand side vectors in exactly the same way as the block compressed sparsity pattern:
* 

* 
* [1.x.139]
* 
*   [1.x.140]  [1.x.141]
* 

* 
*  Similarly, the function that assembles the linear system has mostly been discussed already in the introduction to this example. At its top, what happens are all the usual steps, with the addition that we do not only allocate quadrature and  [2.x.259]  objects for the cell terms, but also for face terms. After that, we define the usual abbreviations for variables, and the allocate space for the local matrix and right hand side contributions, and the array that holds the global numbers of the degrees of freedom local to the present cell.
* 

* 
* [1.x.142]
* 
*  The next step is to declare objects that represent the source term, pressure boundary value, and coefficient in the equation. In addition to these objects that represent continuous functions, we also need arrays to hold their values at the quadrature points of individual cells (or faces, for the boundary values). Note that in the case of the coefficient, the array has to be one of matrices.
* 

* 
* [1.x.143]
* 
*  Finally, we need a couple of extractors that we will use to get at the velocity and pressure components of vector-valued shape functions. Their function and use is described in detail in the  [2.x.260]  vector_valued report. Essentially, we will use them as subscripts on the FEValues objects below: the FEValues object describes all vector components of shape functions, while after subscription, it will only refer to the velocities (a set of  [2.x.261]  components starting at component zero) or the pressure (a scalar component located at position  [2.x.262] ):
* 

* 
* [1.x.144]
* 
*  With all this in place, we can go on with the loop over all cells. The body of this loop has been discussed in the introduction, and will not be commented any further here:
* 

* 
* [1.x.145]
* 
*  The final step in the loop over all cells is to transfer local contributions into the global matrix and right hand side vector. Note that we use exactly the same interface as in previous examples, although we now use block matrices and vectors instead of the regular ones. In other words, to the outside world, block objects have the same interface as matrices and vectors, but they additionally allow to access individual blocks.
* 

* 
* [1.x.146]
* 
*   [1.x.147]  [1.x.148]
* 

* 
*  The linear solvers and preconditioners we use in this example have been discussed in significant detail already in the introduction. We will therefore not discuss the rationale for our approach here any more, but rather only comment on some remaining implementational aspects.
* 

* 
*   [1.x.149]  [1.x.150]
* 

* 
*  As already outlined in the introduction, the solve function consists essentially of two steps. First, we have to form the first equation involving the Schur complement and solve for the pressure (component 1 of the solution). Then, we can reconstruct the velocities from the second equation (component 0 of the solution).
* 

* 
* [1.x.151]
* 
*  As a first step we declare references to all block components of the matrix, the right hand side and the solution vector that we will need.
* 

* 
* [1.x.152]
* 
*  Then, we will create corresponding LinearOperator objects and create the  [2.x.263]  operator:
* 

* 
* [1.x.153]
* 
*  This allows us to declare the Schur complement  [2.x.264]  and the approximate Schur complement  [2.x.265] :
* 

* 
* [1.x.154]
* 
*  We now create a preconditioner out of  [2.x.266]  that applies a fixed number of 30 (inexpensive) CG iterations:
* 

* 
* [1.x.155]
* 
*  Now on to the first equation. The right hand side of it is  [2.x.267] , which is what we compute in the first few lines. We then solve the first equation with a CG solver and the preconditioner we just declared.
* 

* 
* [1.x.156]
* 
*  After we have the pressure, we can compute the velocity. The equation reads  [2.x.268] , and we solve it by first computing the right hand side, and then multiplying it with the object that represents the inverse of the mass matrix:
* 

* 
* [1.x.157]
* 
*   [1.x.158]  [1.x.159]
* 

* 
*   [1.x.160]  [1.x.161]
* 

* 
*  After we have dealt with the linear solver and preconditioners, we continue with the implementation of our main class. In particular, the next task is to compute the errors in our numerical solution, in both the pressures as well as velocities.   
*   To compute errors in the solution, we have already introduced the  [2.x.269]  function in  [2.x.270]  and  [2.x.271] . However, there we only dealt with scalar solutions, whereas here we have a vector-valued solution with components that even denote different quantities and may have different orders of convergence (this isn't the case here, by choice of the used finite elements, but is frequently the case in mixed finite element applications). What we therefore have to do is to `mask' the components that we are interested in. This is easily done: the  [2.x.272]  function takes as one of its arguments a pointer to a weight function (the parameter defaults to the null pointer, meaning unit weights). What we have to do is to pass a function object that equals one in the components we are interested in, and zero in the other ones. For example, to compute the pressure error, we should pass a function that represents the constant vector with a unit value in component  [2.x.273] , whereas for the velocity the constant vector should be one in the first  [2.x.274]  components, and zero in the location of the pressure.   
*   In deal.II, the  [2.x.275]  does exactly this: it wants to know how many vector components the function it is to represent should have (in our case this would be  [2.x.276] , for the joint velocity-pressure space) and which individual or range of components should be equal to one. We therefore define two such masks at the beginning of the function, following by an object representing the exact solution and a vector in which we will store the cellwise errors as computed by  [2.x.277] :
* 

* 
* [1.x.162]
* 
*  As already discussed in  [2.x.278] , we have to realize that it is impossible to integrate the errors exactly. All we can do is approximate this integral using quadrature. This actually presents a slight twist here: if we naively chose an object of type  [2.x.279]  as one may be inclined to do (this is what we used for integrating the linear system), one realizes that the error is very small and does not follow the expected convergence curves at all. What is happening is that for the mixed finite elements used here, the Gauss points happen to be superconvergence points in which the pointwise error is much smaller (and converges with higher order) than anywhere else. These are therefore not particularly good points for integration. To avoid this problem, we simply use a trapezoidal rule and iterate it  [2.x.280]  times in each coordinate direction (again as explained in  [2.x.281] ):
* 

* 
* [1.x.163]
* 
*  With this, we can then let the library compute the errors and output them to the screen:
* 

* 
* [1.x.164]
* 
*   [1.x.165]  [1.x.166]
* 

* 
*  The last interesting function is the one in which we generate graphical output. Note that all velocity components get the same solution name "u". Together with using  [2.x.282]  this will cause  [2.x.283]  to generate a vector representation of the individual velocity components, see  [2.x.284]  or the  [2.x.285]  "Generating graphical output" section of the  [2.x.286]  module for more information. Finally, it seems inappropriate for higher order elements to only show a single bilinear quadrilateral per cell in the graphical output. We therefore generate patches of size (degree+1)x(degree+1) to capture the full information content of the solution. See the  [2.x.287]  tutorial program for more information on this.
* 

* 
* [1.x.167]
* 
*   [1.x.168]  [1.x.169]
* 

* 
*  This is the final function of our main class. It's only job is to call the other functions in their natural order:
* 

* 
* [1.x.170]
* 
*   [1.x.171]  [1.x.172]
* 

* 
*  The main function we stole from  [2.x.288]  instead of  [2.x.289] . It is almost equal to the one in  [2.x.290]  (apart from the changed class names, of course), the only exception is that we pass the degree of the finite element space to the constructor of the mixed Laplace problem (here, we use zero-th order elements).
* 

* 
* [1.x.173]
* [1.x.174][1.x.175]
* 

* [1.x.176][1.x.177]
* 

* 
* If we run the program as is, we get this output for the  [2.x.291] mesh we use (for a total of 1024 cells with 1024 pressure degrees offreedom since we use piecewise constants, and 2112 velocities becausethe Raviart-Thomas element defines one degree per freedom per face andthere are  [2.x.292]  faces parallel to the  [2.x.293] -axis and the samenumber parallel to the  [2.x.294] -axis):
* [1.x.178]
* 
* The fact that the number of iterations is so small, of course, is due tothe good (but expensive!) preconditioner we have developed. To getconfidence in the solution, let us take a look at it. The following threeimages show (from left to right) the x-velocity, the y-velocity, and thepressure:
*  [2.x.295] 
* 

* 
* Let us start with the pressure: it is highest at the left and lowest at theright, so flow will be from left to right. In addition, though hardly visiblein the graph, we have chosen the pressure field such that the flow left-rightflow first channels towards the center and then outward again. Consequently,the x-velocity has to increase to get the flow through the narrow part,something that can easily be seen in the left image. The middle imagerepresents inward flow in y-direction at the left end of the domain, andoutward flow in y-direction at the right end of the domain.
* 

* 
* As an additional remark, note how the x-velocity in the left image is onlycontinuous in x-direction, whereas the y-velocity is continuous iny-direction. The flow fields are discontinuous in the other directions. Thisvery obviously reflects the continuity properties of the Raviart-Thomaselements, which are, in fact, only in the space H(div) and not in the space [2.x.296] . Finally, the pressure field is completely discontinuous, butthat should not surprise given that we have chosen  [2.x.297]  asthe finite element for that solution component.
* 

* 
* [1.x.179][1.x.180]
* 

* 
* The program offers two obvious places where playing and observing convergenceis in order: the degree of the finite elements used (passed to the constructorof the  [2.x.298] ), andthe refinement level (determined in [2.x.299] ). What one can do is tochange these values and observe the errors computed later on in the course ofthe program run.
* 

* 
* If one does this, one finds the following pattern for the  [2.x.300]  errorin the pressure variable: [2.x.301] 
* The theoretically expected convergence orders are very nicely reflected by theexperimentally observed ones indicated in the last row of the table.
* 

* 
* One can make the same experiment with the  [2.x.302]  errorin the velocity variables: [2.x.303] The result concerning the convergence order is the same here.
* 

* 
* [1.x.181][1.x.182][1.x.183]
* 

* [1.x.184][1.x.185]
* 

* Realistic flow computations for ground water or oil reservoir simulations willnot use a constant permeability. Here's a first, rather simple way to changethis situation: we use a permeability that decays very rapidly away from acentral flowline until it hits a background value of 0.001. This is to mimicthe behavior of fluids in sandstone: in most of the domain, the sandstone ishomogeneous and, while permeable to fluids, not overly so; on the other stone,the stone has cracked, or faulted, along one line, and the fluids flow mucheasier along this large crack. Here is how we could implement something likethis:
* [1.x.186]
* Remember that the function returns the inverse of the permeability tensor.
* 

* 
* With a significantly higher mesh resolution, we can visualize this, here withx- and y-velocity:
*  [2.x.304] 
* It is obvious how fluids flow essentially only along the middle line, and notanywhere else.
* 

* 
* Another possibility would be to use a random permeability field. A simple wayto achieve this would be to scatter a number of centers around the domain andthen use a permeability field that is the sum of (negative) exponentials foreach of these centers. Flow would then try to hop from one center of highpermeability to the next one. This is an entirely unscientific attempt atdescribing a random medium, but one possibility to implement this behaviorwould look like this:
* [1.x.187]
* 
* A piecewise constant interpolation of the diagonal elements of theinverse of this tensor (i.e., of  [2.x.305] )looks as follows:
*  [2.x.306] 
* 

* With a permeability field like this, we would get x-velocities and pressures asfollows:
*  [2.x.307] 
* We will use these permeability fields again in  [2.x.308]  and  [2.x.309] .
* 

* [1.x.188][1.x.189]
* 

* As mentioned in the introduction, the Schur complement solver used here is notthe best one conceivable (nor is it intended to be a particularly goodone). Better ones can be found in the literature and can be built using thesame block matrix techniques that were introduced here. We pick up on thistheme again in  [2.x.310] , where we first build a Schur complement solver for theStokes equation as we did here, and then in the [1.x.190] section discuss betterways based on solving the system as a whole but preconditioning based onindividual blocks. We will also come back to this in  [2.x.311] .
* 

* [1.x.191][1.x.192] [2.x.312] 
* [0.x.1]