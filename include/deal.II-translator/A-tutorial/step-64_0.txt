[0.x.0]*
 [2.x.0] 
* This tutorial depends on  [2.x.1] ,  [2.x.2] .
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15]
*  [2.x.3] 
* [1.x.16]
* 

* [1.x.17][1.x.18]
* 

* This example shows how to implement a matrix-free method on the GPU using CUDAfor the Helmholtz equation with variable coefficients on a hypercube. The linearsystem will be solved using the conjugate gradient method and is parallelized with MPI.
* In the last few years, heterogeneous computing in general and GPUs in particularhave gained a lot of popularity. This is because GPUs offer better computingcapabilities and memory bandwidth than CPUs for a given power budget.Among the architectures available in early 2019, GPUs are about 2x-3x as powerefficient than server CPUs with wide [1.x.19] for PDE-relatedtasks. GPUs are alsothe most popular architecture for machine learning. On the other hand,GPUs are not easy to program. This program explores the deal.IIcapabilities to see how efficiently such a program can be implemented.
* While we have tried for the interface of the matrix-free classes for the CPU andthe GPU to be as close as possible, there are a few differences. When usingthe matrix-free framework on a GPU, one must write some CUDA code. However, theamount is fairly small and the use of CUDA is limited to a few keywords.
* 

* [1.x.20][1.x.21]
* 

* In this example, we consider the Helmholtz problem [1.x.22]
* where  [2.x.4]  is a variable coefficient.
* We choose as domain  [2.x.5]  and  [2.x.6] . Since the coefficient is symmetric around the origin butthe domain is not, we will end up with a non-symmetric solution.
* If you've made it this far into the tutorial, you will know how theweak formulation of this problem looks like and how, in principle, oneassembles linear systems for it. Of course, in this program we will infact not actually form the matrix, but rather only represent itsaction when one multiplies with it.
* 

* [1.x.23][1.x.24]
* 

* GPUs (we will use the term "device" from now on to refer to the GPU) have their own memorythat is separate from the memory accessible to the CPU (we will use the term"host" from now on). A normal calculation on the device can be divided in threeseparate steps:
* 

* 
* 
*  - the data is moved from the host to the device,
* 

* 
* 
*  - the computation is done on the device,
* 

* 
* 
*  - the result is moved back from the device to the host
* The data movements can either be done explicitly by the user code or doneautomatically using UVM (Unified Virtual Memory). In deal.II, only the firstmethod is supported. While it means an extra burden for the user, thisallows forbetter control of data movement and more importantly it avoids to mistakenly runimportant kernels on the host instead of the device.
* The data movement in deal.II is done using  [2.x.7]  Thesevectors can be seen as buffers on the host that are used to either store datareceived from the device or to send data to the device. There are two types of vectorsthat can be used on the device:
* 
*  -  [2.x.8]  which is similar to the more commonVector<Number>, and
* 
*  -  [2.x.9]  [2.x.10]  which is a regular [2.x.11]  where we have specified which memoryspace to use.
* If no memory space is specified, the default is  [2.x.12] 
* Next, we show how to move data to/from the device using [2.x.13] 
* [1.x.25]
* Both of the vector classes used here only work on a single machine,i.e., one memory space on a host and one on a device.
* But there are cases where one wants to run a parallel computationbetween multiple MPI processes on a number of machines, each of whichis equipped with GPUs. In that case, one wants to use [2.x.14] which is similar but the `import()` stage may involve MPI communication:
* [1.x.26]
* The `relevant_rw_vector` is an object that stores a subset of allelements of the vector. Typically, these are the [2.x.15]  "locally relevant DoFs",which implies that they overlap between different MPIprocesses. Consequently, the elements stored in that vector on onemachine may not coincide with the ones stored by the GPU on thatmachine, requiring MPI communication to import them.
* In all of these cases, while importing a vector, values can either beinserted (using  [2.x.16]  or added to prior content ofthe vector (using  [2.x.17] 
* 

* [1.x.27][1.x.28]
* 

* The code necessary to evaluate the matrix-free operator on the device is verysimilar to the one on the host. However, there are a few differences, the mainones being that the `local_apply()` function in  [2.x.18]  and the loop overquadrature points both need to be encapsulated in their own functors.
* 

*  [1.x.29] [1.x.30]
*  First include the necessary files from the deal.II library known from the previous tutorials.
* 

* 
* [1.x.31]
* 
*  The following ones include the data structures for the implementation of matrix-free methods on GPU:
* 

* 
* [1.x.32]
* 
*  As usual, we enclose everything into a namespace of its own:
* 

* 
* [1.x.33]
* 
*   [1.x.34]  [1.x.35]
* 

* 
*  Next, we define a class that implements the varying coefficients we want to use in the Helmholtz operator. Later, we want to pass an object of this type to a  [2.x.19]  object that expects the class to have an `operator()` that fills the values provided in the constructor for a given cell. This operator needs to run on the device, so it needs to be marked as `__device__` for the compiler.
* 

* 
* [1.x.36]
* 
*  Since  [2.x.20]  doesn't know about the size of its arrays, we need to store the number of quadrature points and the numbers of degrees of freedom in this class to do necessary index conversions.
* 

* 
* [1.x.37]
* 
*  The following function implements this coefficient. Recall from the introduction that we have defined it as  [2.x.21] 
* 

* 
* [1.x.38]
* 
*   [1.x.39]  [1.x.40]
* 

* 
*  The class `HelmholtzOperatorQuad` implements the evaluation of the Helmholtz operator at each quadrature point. It uses a similar mechanism as the MatrixFree framework introduced in  [2.x.22] . In contrast to there, the actual quadrature point index is treated implicitly by converting the current thread index. As before, the functions of this class need to run on the device, so need to be marked as `__device__` for the compiler.
* 

* 
* [1.x.41]
* 
*  The Helmholtz problem we want to solve here reads in weak form as follows: [1.x.42]
*  If you have seen  [2.x.23] , then it will be obvious that the two terms on the left-hand side correspond to the two function calls here:
* 

* 
* [1.x.43]
* 
*   [1.x.44]  [1.x.45]
* 

* 
*  Finally, we need to define a class that implements the whole operator evaluation that corresponds to a matrix-vector product in matrix-based approaches.
* 

* 
* [1.x.46]
* 
*  Again, the  [2.x.24]  object doesn't know about the number of degrees of freedom and the number of quadrature points so we need to store these for index calculations in the call operator.
* 

* 
* [1.x.47]
* 
*  This is the call operator that performs the Helmholtz operator evaluation on a given cell similar to the MatrixFree framework on the CPU. In particular, we need access to both values and gradients of the source vector and we write value and gradient information to the destination vector.
* 

* 
* [1.x.48]
* 
*   [1.x.49]  [1.x.50]
* 

* 
*  The `HelmholtzOperator` class acts as wrapper for `LocalHelmholtzOperator` defining an interface that can be used with linear solvers like SolverCG. In particular, like every class that implements the interface of a linear operator, it needs to have a `vmult()` function that performs the action of the linear operator on a source vector.
* 

* 
* [1.x.51]
* 
*  The following is the implementation of the constructor of this class. In the first part, we initialize the `mf_data` member variable that is going to provide us with the necessary information when evaluating the operator.   
*   In the second half, we need to store the value of the coefficient for each quadrature point in every active, locally owned cell. We can ask the parallel triangulation for the number of active, locally owned cells but only have a DoFHandler object at hand. Since  [2.x.25]  returns a Triangulation object, not a  [2.x.26]  object, we have to downcast the return value. This is safe to do here because we know that the triangulation is a  [2.x.27]  object in fact.
* 

* 
* [1.x.52]
* 
*  The key step then is to use all of the previous classes to loop over all cells to perform the matrix-vector product. We implement this in the next function.   
*   When applying the Helmholtz operator, we have to be careful to handle boundary conditions correctly. Since the local operator doesn't know about constraints, we have to copy the correct values from the source to the destination vector afterwards.
* 

* 
* [1.x.53]
* 
*   [1.x.54]  [1.x.55]
* 

* 
*  This is the main class of this program. It defines the usual framework we use for tutorial programs. The only point worth commenting on is the `solve()` function and the choice of vector types.
* 

* 
* [1.x.56]
* 
*  Since all the operations in the `solve()` function are executed on the graphics card, it is necessary for the vectors used to store their values on the GPU as well.  [2.x.28]  can be told which memory space to use. There is also  [2.x.29]  that always uses GPU memory storage but doesn't work with MPI. It might be worth noticing that the communication between different MPI processes can be improved if the MPI implementation is CUDA-aware and the configure flag `DEAL_II_MPI_WITH_CUDA_SUPPORT` is enabled. (The value of this flag needs to be set at the time you call `cmake` when installing deal.II.)     
*   In addition, we also keep a solution vector with CPU storage such that we can view and display the solution as usual.
* 

* 
* [1.x.57]
* 
*  The implementation of all the remaining functions of this class apart from  [2.x.30]  doesn't contain anything new and we won't further comment much on the overall approach.
* 

* 
* [1.x.58]
* 
*  Unlike programs such as  [2.x.31]  or  [2.x.32] , we will not have to assemble the whole linear system but only the right hand side vector. This looks in essence like we did in  [2.x.33] , for example, but we have to pay attention to using the right constraints object when copying local contributions into the global vector. In particular, we need to make sure the entries that correspond to boundary nodes are properly zeroed out. This is necessary for CG to converge.  (Another solution would be to modify the `vmult()` function above in such a way that we pretend the source vector has zero entries by just not taking them into account in matrix-vector products. But the approach used here is simpler.)   
*   At the end of the function, we can't directly copy the values from the host to the device but need to use an intermediate object of type  [2.x.34]  to construct the correct communication pattern necessary.
* 

* 
* [1.x.59]
* 
*  This solve() function finally contains the calls to the new classes previously discussed. Here we don't use any preconditioner, i.e., precondition by the identity matrix, to focus just on the peculiarities of the  [2.x.35]  framework. Of course, in a real application the choice of a suitable preconditioner is crucial but we have at least the same restrictions as in  [2.x.36]  since matrix entries are computed on the fly and not stored.   
*   After solving the linear system in the first part of the function, we copy the solution from the device to the host to be able to view its values and display it in `output_results()`. This transfer works the same as at the end of the previous function.
* 

* 
* [1.x.60]
* 
*  The output results function is as usual since we have already copied the values back from the GPU to the CPU.   
*   While we're already doing something with the function, we might as well compute the  [2.x.37]  norm of the solution. We do this by calling  [2.x.38]  That function is meant to compute the error by evaluating the difference between the numerical solution (given by a vector of values for the degrees of freedom) and an object representing the exact solution. But we can easily compute the  [2.x.39]  norm of the solution by passing in a zero function instead. That is, instead of evaluating the error  [2.x.40] , we are just evaluating  [2.x.41]  instead.
* 

* 
* [1.x.61]
* 
*  There is nothing surprising in the `run()` function either. We simply compute the solution on a series of (globally) refined meshes.
* 

* 
* [1.x.62]
* 
*   [1.x.63]  [1.x.64]
* 

* 
*  Finally for the `main()` function.  By default, all the MPI ranks will try to access the device with number 0, which we assume to be the GPU device associated with the CPU on which a particular MPI rank runs. This works, but if we are running with MPI support it may be that multiple MPI processes are running on the same machine (for example, one per CPU core) and then they would all want to access the same GPU on that machine. If there is only one GPU in the machine, there is nothing we can do about it: All MPI ranks on that machine need to share it. But if there are more than one GPU, then it is better to address different graphic cards for different processes. The choice below is based on the MPI process id by assigning GPUs round robin to GPU ranks. (To work correctly, this scheme assumes that the MPI ranks on one machine are consecutive. If that were not the case, then the rank-GPU association may just not be optimal.) To make this work, MPI needs to be initialized before using this function.
* 

* 
* [1.x.65]
* [1.x.66][1.x.67]
* 

* Since the main purpose of this tutorial is to demonstrate how to use the [2.x.42]  interface, not to compute anything useful initself, we just show the expected output here:
* [1.x.68]
* 
* One can make two observations here: First, the norm of the numerical solutionconverges, presumably to the norm of the exact (but unknown)solution. And second, the number of iterations roughly doubles witheach refinement of the mesh. (This is in keeping with the expectationthat the number of CG iterations grows with the square root of thecondition number of the matrix; and that we know that the conditionnumber of the matrix of a second-order differential operation growslike  [2.x.43] .) This is of course rather inefficient, as anoptimal solver would have a number of iterations that is independentof the size of the problem. But having such a solver would requireusing a better preconditioner than the identity matrix we have used here.
* 

* [1.x.69][1.x.70][1.x.71]
* 

* Currently, this program uses no preconditioner at all. This is mainlysince constructing an efficient matrix-free preconditioner isnon-trivial.  However, simple choices just requiring the diagonal ofthe corresponding matrix are good candidates and these can be computedin a matrix-free way as well. Alternatively, and maybe even better,one could extend the tutorial to use multigrid with Chebyshevsmoothers similar to  [2.x.44] .
* 

* [1.x.72][1.x.73] [2.x.45] 
* [0.x.1]