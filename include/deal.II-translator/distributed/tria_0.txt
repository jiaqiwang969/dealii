[0.x.0]*
     This class acts like the  [2.x.0]  class, but it     distributes the mesh across a number of different processors when using     MPI. The class's interface does not add a lot to the      [2.x.1]  class but there are a number of difficult     algorithms under the hood that ensure we always have a load-balanced,     fully distributed mesh. Use of this class is explained in  [2.x.2] ,      [2.x.3] , the      [2.x.4]      documentation module, as well as the      [2.x.5] .     See there for more information. This class satisfies the      [2.x.6]  "MeshType concept".        
*  [2.x.7]  This class does not support anisotropic refinement, because it     relies on the p4est library that does not support this. Attempts to     refine cells anisotropically will result in errors.        
*  [2.x.8]  There is currently no support for distributing 1d triangulations.             [1.x.0]         Refining and coarsening a distributed triangulation is a complicated     process because cells may have to be migrated from one processor to     another. On a single processor, materializing that part of the global     mesh that we want to store here from what we have stored before     therefore may involve several cycles of refining and coarsening the     locally stored set of cells until we have finally gotten from the     previous to the next triangulation. This process is described in more     detail in the      [2.x.9] .     Unfortunately, in this process, some information can get lost relating     to flags that are set by user code and that are inherited from mother     to child cell but that are not moved along with a cell if that cell is     migrated from one processor to another.         An example are boundary indicators. Assume, for example, that you start     with a single cell that is refined once globally, yielding four     children. If you have four processors, each one owns one cell. Assume     now that processor 1 sets the boundary indicators of the external     boundaries of the cell it owns to 42. Since processor 0 does not own     this cell, it doesn't set the boundary indicators of its ghost cell     copy of this cell. Now, assume we do several mesh refinement cycles and     end up with a configuration where this processor suddenly finds itself     as the owner of this cell. If boundary indicator 42 means that we need     to integrate Neumann boundary conditions along this boundary, then     processor 0 will forget to do so because it has never set the boundary     indicator along this cell's boundary to 42.         The way to avoid this dilemma is to make sure that things like setting     boundary indicators or material ids is done immediately every time a     parallel triangulation is refined. This is not necessary for sequential     triangulations because, there, these flags are inherited from mother to     child cell and remain with a cell even if it is refined and the     children are later coarsened again, but this does not hold for     distributed triangulations. It is made even more difficult by the fact     that in the process of refining a parallel distributed triangulation,     the triangulation may call      [2.x.10]  multiple times     and this function needs to know about boundaries. In other words, it is     [1.x.1] enough to just set boundary indicators on newly created     faces only [1.x.2] calling      [2.x.11]      it actually has to happen while that function is still running.         The way to do this is by writing a function that sets boundary     indicators and that will be called by the  [2.x.12]  class.     The triangulation does not provide a pointer to itself to the function     being called, nor any other information, so the trick is to get this     information into the function. C++ provides a nice mechanism for this     that is best explained using an example:    
* [1.x.3]
*          The object passed as argument to  [2.x.13]  is an object     that can be called like a function with no arguments. It does so by     wrapping a function that does, in fact, take an argument but this one     argument is stored as a reference to the coarse grid triangulation when     the lambda function is created. After each refinement step, the     triangulation will then call the object so created which will in turn     call  [2.x.14]  with the reference to the coarse     grid as argument.         This approach can be generalized. In the example above, we have used a     global function that will be called. However, sometimes it is necessary     that this function is in fact a member function of the class that     generates the mesh, for example because it needs to access run-time     parameters. This can be achieved as follows: assuming the      [2.x.15]  function has been declared as a (non-     static, but possibly private) member function of the      [2.x.16]  class, then the following will work:    
* [1.x.4]
*      The lambda function above again is an object that can     be called like a global function with no arguments, and this object in     turn calls the current object's member function      [2.x.17]  with a reference to the triangulation to     work on. Note that     because the  [2.x.18]  function is declared as      [2.x.19] , it is necessary that the      [2.x.20]  function is also declared      [2.x.21] .         [1.x.5]For reasons that have to do with the way the      [2.x.22]  is implemented, functions that     have been attached to the post-refinement signal of the triangulation     are called more than once, sometimes several times, every time the     triangulation is actually refined.            
*  [2.x.23]     
* [0.x.1]*
       An alias that is used to identify cell iterators. The concept of       iterators is discussed at length in the        [2.x.24]  "iterators documentation module".             The current alias identifies cells in a triangulation. You can find       the exact type it refers to in the base class's own alias, but it       should be TriaIterator<CellAccessor<dim,spacedim> >. The TriaIterator       class works like a pointer that when you dereference it yields an       object of type CellAccessor. CellAccessor is a class that identifies       properties that are specific to cells in a triangulation, but it is       derived (and consequently inherits) from TriaAccessor that describes       what you can ask of more general objects (lines, faces, as well as       cells) in a triangulation.            
*  [2.x.25]       
* [0.x.2]*
       An alias that is used to identify        [2.x.26]  "active cell iterators".       The concept of iterators is discussed at length in the        [2.x.27]  "iterators documentation module".             The current alias identifies active cells in a triangulation. You       can find the exact type it refers to in the base class's own alias,       but it should be TriaActiveIterator<CellAccessor<dim,spacedim> >. The       TriaActiveIterator class works like a pointer to active objects that       when you dereference it yields an object of type CellAccessor.       CellAccessor is a class that identifies properties that are specific       to cells in a triangulation, but it is derived (and consequently       inherits) from TriaAccessor that describes what you can ask of more       general objects (lines, faces, as well as cells) in a triangulation.            
*  [2.x.28]       
* [0.x.3]*
       Configuration flags for distributed Triangulations to be set in the       constructor. Settings can be combined using bitwise OR.      
* [0.x.4]*
         Default settings, other options are disabled.        
* [0.x.5]*
         If set, the deal.II mesh will be reconstructed from the coarse mesh         every time a repartitioning in p4est happens. This can be a bit more         expensive, but guarantees the same memory layout and therefore cell         ordering in the deal.II mesh. As assembly is done in the deal.II         cell ordering, this flag is required to get reproducible behavior         after snapshot/resume.        
* [0.x.6]*
         This flags needs to be set to use the geometric multigrid         functionality. This option requires additional computation and         communication.        
* [0.x.7]*
         Setting this flag will disable automatic repartitioning of the cells         after a refinement cycle. It can be executed manually by calling         repartition().        
* [0.x.8]*
       Constructor.              [2.x.29]  mpi_communicator The MPI communicator to be used for       the triangulation.              [2.x.30]  smooth_grid Degree and kind of mesh smoothing to be applied to       the mesh. See the  [2.x.31]  class for a description of       the kinds of smoothing operations that can be applied.              [2.x.32]  settings See the description of the Settings enumerator.       Providing  [2.x.33]  enforces        [2.x.34]        for smooth_grid.            
*  [2.x.35]  This class does not currently support the        [2.x.36]  argument provided by the base       class.            
*  [2.x.37]  While it is possible to pass all of the mesh smoothing flags       listed in the base class to objects of this type, it is not always       possible to honor all of these smoothing options if they would       require knowledge of refinement/coarsening flags on cells not locally       owned by this processor. As a consequence, for some of these flags,       the ultimate number of cells of the parallel triangulation may depend       on the number of processors into which it is partitioned. On the       other hand, if no smoothing flags are passed, if you always mark the       same cells of the mesh, you will always get the exact same refined       mesh independent of the number of processors into which the       triangulation is partitioned.      
* [0.x.9]*
       Destructor.      
* [0.x.10]*
       Reset this triangulation into a virgin state by deleting all data.             Note that this operation is only allowed if no subscriptions to this       object exist any more, such as DoFHandler objects using it.      
* [0.x.11]*
       Return if multilevel hierarchy is supported and has been constructed.      
* [0.x.12]*
       Transfer data across forests.             Besides the actual  [2.x.38]  which has been already refined       and repartitioned, this function also needs information about its       previous state, i.e. the locally owned intervals in p4est's       sc_array of each processor. This information needs to be memcopyied       out of the old p4est object and has to be provided via the parameter        [2.x.39]              Data has to be previously packed with        [2.x.40]       
* [0.x.13]*
       Implementation of the same function as in the base class.            
*  [2.x.41]  This function can be used to copy a serial Triangulation to a        [2.x.42]  but only if the serial       Triangulation has never been refined.      
* [0.x.14]*
       Create a triangulation as documented in the base class.             This function also sets up the various data structures necessary to       distribute a mesh across a number of processors. This will be       necessary once the mesh is being refined, though we will always keep       the entire coarse mesh that is generated by this function on all       processors.      
* [0.x.15]*
        [2.x.43]   [2.x.44]             
*  [2.x.45]  Not implemented yet.      
* [0.x.16]*
       Coarsen and refine the mesh according to refinement and coarsening       flags set.             Since the current processor only has control over those cells it owns       (i.e. the ones for which <code>cell- [2.x.46]  ==       this- [2.x.47]  refinement and coarsening       flags are only respected for those locally owned cells. Flags may be       set on other cells as well (and may often, in fact, if you call        [2.x.48]  but will       be largely ignored: the decision to refine the global mesh will only       be affected by flags set on locally owned cells.            
*  [2.x.49]  This function by default partitions the mesh in such a way that       the number of cells on all processors is roughly equal. If you want       to set weights for partitioning, e.g. because some cells are more       expensive to compute than others, you can use the signal cell_weight       as documented in the  [2.x.50]  class. This function will       check whether a function is connected to the signal and if so use it.       If you prefer to repartition the mesh yourself at user-defined       intervals only, you can create your triangulation object by passing       the  [2.x.51]        flag to the constructor, which ensures that calling the current       function only refines and coarsens the triangulation, but doesn't       partition it. You can then call the repartition() function manually.       The usage of the cell_weights signal is identical in both cases, if a       function is connected to the signal it will be used to balance the       calculated weights, otherwise the number of cells is balanced.      
* [0.x.17]*
       Override the implementation of prepare_coarsening_and_refinement from       the base class. This is necessary if periodic boundaries are enabled       and the level difference over vertices over the periodic boundary       must not be more than 2:1.      
* [0.x.18]*
       Manually repartition the active cells between processors. Normally       this repartitioning will happen automatically when calling       execute_coarsening_and_refinement() (or refine_global()) unless the        [2.x.52]  is set in the constructor. Setting the       flag and then calling repartition() gives the same result.             If you want to transfer data (using SolutionTransfer or manually with       register_data_attach() and notify_ready_to_unpack()), you need to set       it up twice: once when calling execute_coarsening_and_refinement(),       which will handle coarsening and refinement but obviously won't ship       any data between processors, and a second time when calling       repartition().  Here, no coarsening and refinement will be done but       information will be packed and shipped to different processors. In       other words, you probably want to treat a call to repartition() in       the same way as execute_coarsening_and_refinement() with respect to       dealing with data movement (SolutionTransfer, etc.).            
*  [2.x.53]  If no function is connected to the cell_weight signal described       in the  [2.x.54]  class, this function will balance the       number of cells on each processor. If one or more functions are       connected, it will calculate the sum of the weights and balance the       weights across processors. The only requirement on the weights is       that every cell's weight is positive and that the sum over all       weights on all processors can be formed using a 64-bit integer.       Beyond that, it is your choice how you want to interpret the weights.       A common approach is to consider the weights proportional to the cost       of doing computations on a cell, e.g., by summing the time for       assembly and solving. In practice, determining this cost is of course       not trivial since we don't solve on isolated cells, but on the entire       mesh. In such cases, one could, for example, choose the weight equal       to the number of unknowns per cell (in the context of hp-finite       element methods), or using a heuristic that estimates the cost on       each cell depending on whether, for example, one has to run some       expensive algorithm on some cells but not others (such as forming       boundary integrals during the assembly only on cells that are       actually at the boundary, or computing expensive nonlinear terms only       on some cells but not others, e.g., in the elasto-plastic problem in        [2.x.55] ).      
* [0.x.19]*
       Return true if the triangulation has hanging nodes.             In the context of parallel distributed triangulations, every       processor stores only that part of the triangulation it locally owns.       However, it also stores the entire coarse mesh, and to guarantee the       2:1 relationship between cells, this may mean that there are hanging       nodes between cells that are not locally owned or ghost cells (i.e.,       between ghost cells and artificial cells, or between artificial and       artificial cells; see        [2.x.56]  "the glossary").       One is not typically interested in this case, so the function returns       whether there are hanging nodes between any two cells of the "global"       mesh, i.e., the union of locally owned cells on all processors.      
* [0.x.20]*
       Return the local memory consumption in bytes.      
* [0.x.21]*
       Return the local memory consumption contained in the p4est data       structures alone. This is already contained in memory_consumption()       but made available separately for debugging purposes.      
* [0.x.22]*
       A collective operation that produces a sequence of output files with       the given file base name that contain the mesh in VTK format.             More than anything else, this function is useful for debugging the       interface between deal.II and p4est.      
* [0.x.23]*
       Produce a check sum of the triangulation.  This is a collective       operation and is mostly useful for debugging purposes.      
* [0.x.24]*
       Save the refinement information from the coarse mesh into the given       file. This file needs to be reachable from all nodes in the       computation on a shared network file system. See the SolutionTransfer       class on how to store solution vectors into this file. Additional       cell-based data can be saved using        [2.x.57]       
* [0.x.25]*
       Load the refinement information saved with save() back in. The mesh       must contain the same coarse mesh that was used in save() before       calling this function.             You do not need to load with the same number of MPI processes that       you saved with. Rather, if a mesh is loaded with a different number       of MPI processes than used at the time of saving, the mesh is       repartitioned appropriately. Cell-based data that was saved with        [2.x.58]  can       be read in with        [2.x.59]        after calling load().             If you use p4est version > 0.3.4.2 the  [2.x.60]  flag tells       p4est to ignore the partitioning that the triangulation had when it       was saved and make it uniform upon loading. If  [2.x.61]  is       set to false, the triangulation is only repartitioned if needed (i.e.       if a different number of MPI processes is encountered).      
* [0.x.26]*
       Load the refinement information from a given parallel forest. This       forest might be obtained from the function call to        [2.x.62]       
* [0.x.27]*
       Return a permutation vector for the order the coarse cells are handed       off to p4est. For example the value of the  [2.x.63] th element in this       vector is the index of the deal.II coarse cell (counting from       begin(0)) that corresponds to the  [2.x.64] th tree managed by p4est.      
* [0.x.28]*
       Return a permutation vector for the mapping from the coarse deal       cells to the p4est trees. This is the inverse of       get_p4est_tree_to_coarse_cell_permutation.      
* [0.x.29]*
       This returns a pointer to the internally stored p4est object (of type       p4est_t or p8est_t depending on  [2.x.65]               [2.x.66]  If you modify the p4est object, internal data structures       can become inconsistent.      
* [0.x.30]*
       In addition to the action in the base class Triangulation, this       function joins faces in the p4est forest for periodic boundary       conditions. As a result, each pair of faces will differ by at most one       refinement level and ghost neighbors will be available across these       faces.             The vector can be filled by the function        [2.x.67]              For more information on periodic boundary conditions see        [2.x.68]         [2.x.69]  and  [2.x.70] .            
*  [2.x.71]  Before this function can be used the Triangulation has to be       initialized and must not be refined. Calling this function more than       once is possible, but not recommended: The function destroys and       rebuilds the p4est forest each time it is called.      
* [0.x.31]*
       store the Settings.      
* [0.x.32]*
       A flag that indicates whether the triangulation has actual content.      
* [0.x.33]*
       A data structure that holds the connectivity between trees. Since       each tree is rooted in a coarse grid cell, this data structure holds       the connectivity between the cells of the coarse grid.      
* [0.x.34]*
       A data structure that holds the local part of the global       triangulation.      
* [0.x.35]*
       A data structure that holds some information about the ghost cells of       the triangulation.      
* [0.x.36]*
       Go through all p4est trees and record the relations between locally       owned p4est quadrants and active deal.II cells in the private member       vector local_cell_relations.             The vector contains an active cell iterator for every locally owned       p4est quadrant, as well as a CellStatus flag to describe their       relation.             The stored vector will be ordered by the occurrence of quadrants in       the corresponding local sc_array of the parallel_forest. p4est requires       this specific ordering for its transfer functions. Therefore, the size       of this vector will be equal to the number of locally owned quadrants       in the parallel_forest object.             These relations will be established for example in the mesh refinement       process: after adapting the parallel_forest, but before applying these       changes to this triangulation, we will record how cells will change in       the refinement process. With this information, we can prepare all       buffers for data transfer accordingly.      
* [0.x.37]*
       Two arrays that store which p4est tree corresponds to which coarse       grid cell and vice versa. We need these arrays because p4est goes       with the original order of coarse cells when it sets up its forest,       and then applies the Morton ordering within each tree. But if coarse       grid cells are badly ordered this may mean that individual parts of       the forest stored on a local machine may be split across coarse grid       cells that are not geometrically close. Consequently, we apply a       hierarchical preordering according to        [2.x.72]  to ensure that the part of the       forest stored by p4est is located on geometrically close coarse grid       cells.      
* [0.x.38]*
       Return a pointer to the p4est tree that belongs to the given       dealii_coarse_cell_index()      
* [0.x.39]*
       The function that computes the permutation between the two data       storage schemes.      
* [0.x.40]*
       Take the contents of a newly created triangulation we are attached to       and copy it to p4est data structures.             This function exists in 2d and 3d variants.      
* [0.x.41]*
       Copy the local part of the refined forest from p4est into the       attached triangulation.      
* [0.x.42]*
       Internal function notifying all registered slots to provide their       weights before repartitioning occurs. Called from       execute_coarsening_and_refinement() and repartition().              [2.x.73]  A vector of unsigned integers representing the weight or       computational load of every cell after the refinement/coarsening/       repartition cycle. Note that the number of entries does not need to       be equal to either n_active_cells() or n_locally_owned_active_cells(),       because the triangulation is not updated yet. The weights are sorted       in the order that p4est will encounter them while iterating over       them.      
* [0.x.43]*
       This method returns a bit vector of length tria.n_vertices()       indicating the locally active vertices on a level, i.e., the vertices       touched by the locally owned level cells for use in geometric       multigrid (possibly including the vertices due to periodic boundary       conditions) are marked by true.             Used by  [2.x.74]       
* [0.x.44]*
     Specialization of the general template for the 1d case. There is     currently no support for distributing 1d triangulations. Consequently,     all this class does is throw an exception.    
* [0.x.45]*
       dummy settings      
* [0.x.46]*
       Constructor. The argument denotes the MPI communicator to be used for       the triangulation.      
* [0.x.47]*
       Destructor.      
* [0.x.48]*
       Return a permutation vector for the order the coarse cells are       handed of to p4est. For example the first element i in this vector       denotes that the first cell in hierarchical ordering is the ith deal       cell starting from begin(0).      
* [0.x.49]*
       This function is not implemented, but needs to be present for the       compiler.      
* [0.x.50]*
       This function is not implemented, but needs to be present for the       compiler.      
* [0.x.51]*
       This function is not implemented, but needs to be present for the       compiler.      
* [0.x.52]*
       This function is not implemented, but needs to be present for the       compiler.      
* [0.x.53]*
       Dummy arrays. This class isn't usable but the compiler wants to see       these variables at a couple places anyway.      
* [0.x.54]*
       This method, which is only implemented for dim = 2 or 3,       needs a stub because it is used in dof_handler_policy.cc      
* [0.x.55]*
       Like above, this method, which is only implemented for dim = 2 or 3,       needs a stub because it is used in dof_handler_policy.cc      
* [0.x.56]*
     Dummy class the compiler chooses for parallel distributed     triangulations if we didn't actually configure deal.II with the p4est     library. The existence of this class allows us to refer to      [2.x.75]  objects throughout the library     even if it is disabled.         Since the constructor of this class is deleted, no such objects     can actually be created as this would be pointless given that     p4est is not available.    
* [0.x.57]*
       Dummy settings to allow defining the deleted constructor.      
* [0.x.58]*
       Constructor. Deleted to make sure that objects of this type cannot be       constructed (see also the class documentation).      
* [0.x.59]*
       Dummy replacement to allow for better error messages when compiling       this class.      
* [0.x.60]*
       Dummy replacement to allow for better error messages when compiling       this class.      
* [0.x.61]*
       Dummy replacement to allow for better error messages when compiling       this class.      
* [0.x.62]*
       Dummy replacement to allow for better error messages when compiling       this class.      
* [0.x.63]*
     This class temporarily modifies the refine and coarsen flags of all     active cells to match the p4est oracle.         The modification only happens on  [2.x.76]      objects, and persists for the lifetime of an instantiation of this     class.         The TemporarilyMatchRefineFlags class should only be used in     combination with the  [2.x.77]      signal. At this stage, the p4est oracle already has been refined, but     the triangulation is still unchanged. After the modification, all     refine and coarsen flags describe how the traingulation will actually     be refined.         The use of this class is demonstrated in  [2.x.78] .    
* [0.x.64]*
       Constructor.             Stores the refine and coarsen flags of all active cells if the       provided Triangulation is of type        [2.x.79]              Adjusts them to be consistent with the p4est oracle.      
* [0.x.65]*
       Destructor.             Returns the refine and coarsen flags of all active cells on the        [2.x.80]  into their previous state.      
* [0.x.66]*
       The modified  [2.x.81]       
* [0.x.67]*
       A vector that temporarily stores the refine flags before they have       been modified on the  [2.x.82]       
* [0.x.68]*
       A vector that temporarily stores the coarsen flags before they have       been modified on the  [2.x.83]       
* [0.x.69]