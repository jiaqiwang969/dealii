[0.x.0]*
   This class describes the interface for all triangulation classes that   work in parallel, namely  [2.x.0]     [2.x.1]  and    [2.x.2]      It is, consequently, a class that can be used to test whether a   pointer of reference to a triangulation object refers to a   sequential triangulation, or whether the triangulation is in fact   parallel. In other words, one could write a function like this:  
* [1.x.0]
*      All parallel triangulations share certain traits, such as the fact that   they communicate via    [2.x.3]  "MPI communicators"   or that they have    [2.x.4]  "locally owned",    [2.x.5]  "ghost", and possibly    [2.x.6]  "artificial cells".   This class provides   a number of member functions that allows querying some information   about the triangulation that is independent of how exactly a   parallel triangulation is implemented (i.e., which of the various   classes derived from the current one it actually is).  
* [0.x.1]*
     Constructor.    
* [0.x.2]*
     Destructor.    
* [0.x.3]*
     Return MPI communicator used by this triangulation.    
* [0.x.4]*
     Return if multilevel hierarchy is supported and has been constructed.    
* [0.x.5]*
     Implementation of the same function as in the base class.        
*  [2.x.7]  This function copies the cells, but not the communicator,     of the source triangulation. In other words, the resulting     triangulation will operate on the communicator it was constructed     with.    
* [0.x.6]*
     Return the number of active cells in the triangulation that are locally     owned, i.e. that have a subdomain_id equal to     locally_owned_subdomain(). Note that there may be more active cells in     the triangulation stored on the present processor, such as for example     ghost cells, or cells further away from the locally owned block of     cells but that are needed to ensure that the triangulation that stores     this processor's set of active cells still remains balanced with     respect to the 2:1 size ratio of adjacent cells.         As a consequence of the remark above, the result of this function is     always smaller or equal to the result of the function with the same     name in the ::Triangulation base class, which includes the active ghost     and artificial cells (see also      [2.x.8]      and      [2.x.9] ).    
* [0.x.7]*
     Return the sum over all processors of the number of active cells owned     by each processor. This equals the overall number of active cells in     the triangulation.    
* [0.x.8]*
     Return the local memory consumption in bytes.    
* [0.x.9]*
     Return the global maximum level. This may be bigger than the number      [2.x.10]  (a function in this class's base     class) returns if the current processor only stores cells in parts of     the domain that are not very refined, but if other processors store     cells in more deeply refined parts of the domain.    
* [0.x.10]*
     Return the subdomain id of those cells that are owned by the current     processor. All cells in the triangulation that do not have this     subdomain id are either owned by another processor or have children     that only exist on other processors.    
* [0.x.11]*
     Return a set of MPI ranks of the processors that have at least one     ghost cell adjacent to the cells of the local processor. In other     words, this is the set of subdomain_id() for all ghost cells.         The returned sets are symmetric, that is if  [2.x.11]  is contained in the     list of processor  [2.x.12]  then  [2.x.13]  will also be contained in the list of     processor  [2.x.14]     
* [0.x.12]*
     Return a set of MPI ranks of the processors that have at least one     level ghost cell adjacent to our cells used in geometric multigrid. In     other words, this is the set of level_subdomain_id() for all level     ghost cells.         The returned sets are symmetric, that is if  [2.x.15]  is contained in the     list of processor  [2.x.16]  then  [2.x.17]  will also be contained in the list of     processor  [2.x.18]         
*  [2.x.19]  The level ghost owners can only be determined if the multigrid     ownership has been assigned (by setting the     construct_multigrid_hierarchy flag at construction time), otherwise the     returned set will be empty.    
* [0.x.13]*
     Return partitioner for the global indices of the cells on the active     level of the triangulation.    
* [0.x.14]*
     Return partitioner for the global indices of the cells on the given  [2.x.20]      level of the triangulation.    
* [0.x.15]*
     Return a map that, for each vertex, lists all the processors whose     subdomains are adjacent to that vertex.          [2.x.21]  Use  [2.x.22]      instead of      [2.x.23]     
* [0.x.16]*
      [2.x.24]   [2.x.25]         
*  [2.x.26]  This function involves a global communication gathering all current       IDs from all processes.    
* [0.x.17]*
      [2.x.27]   [2.x.28]         
*  [2.x.29]  This function involves a global communication gathering all current       IDs from all processes.    
* [0.x.18]*
     When vertices have been moved locally, for example using code like    
* [1.x.1]
*      then this function can be used to update the location of vertices     between MPI processes.         All the vertices that have been moved and might be in the ghost layer     of a process have to be reported in the  [2.x.30]      argument. This ensures that that part of the information that has to     be send between processes is actually sent. Additionally, it is quite     important that vertices on the boundary between processes are     reported on exactly one process (e.g. the one with the highest id).     Otherwise we could expect undesirable results if multiple processes     move a vertex differently. A typical strategy is to let processor  [2.x.31]      move those vertices that are adjacent to cells whose owners include     processor  [2.x.32]  but no other processor  [2.x.33]  with  [2.x.34] ; in other words,     for vertices at the boundary of a subdomain, the processor with the     lowest subdomain id "owns" a vertex.        
*  [2.x.35]  It only makes sense to move vertices that are either located on     locally owned cells or on cells in the ghost layer. This is because     you can be sure that these vertices indeed exist on the finest mesh     aggregated over all processors, whereas vertices on artificial cells     but not at least in the ghost layer may or may not exist on the     globally finest mesh. Consequently, the  [2.x.36]      argument may not contain vertices that aren't at least on ghost     cells.        
*  [2.x.37]  This function moves vertices in such a way that on every     processor, the vertices of every locally owned and ghost cell is     consistent with the corresponding location of these cells on other     processors. On the other hand, the locations of artificial cells will     in general be wrong since artificial cells may or may not exist on     other processors and consequently it is not possible to determine     their location in any way. This is not usually a problem since one     never does anything on artificial cells. However, it may lead to     problems if the mesh with moved vertices is refined in a later step.     If that's what you want to do, the right way to do it is to save the     offset applied to every vertex, call this function, and before     refining or coarsening the mesh apply the opposite offset and call     this function again.          [2.x.38]  vertex_locally_moved A bitmap indicating which vertices have     been moved. The size of this array must be equal to      [2.x.39]  and must be a subset of those vertices     flagged by  [2.x.40]           [2.x.41]  This function is used, for example, in      [2.x.42]     
* [0.x.19]*
     MPI communicator to be used for the triangulation. We create a unique     communicator for this class, which is a duplicate of the one passed to     the constructor.    
* [0.x.20]*
     The subdomain id to be used for the current processor. This is the MPI     rank.    
* [0.x.21]*
     The total number of subdomains (or the size of the MPI communicator).    
* [0.x.22]*
     A structure that contains information about the distributed     triangulation.    
* [0.x.23]*
       Number of locally owned active cells of this MPI rank.      
* [0.x.24]*
       The total number of active cells (sum of  [2.x.43]        n_locally_owned_active_cells).      
* [0.x.25]*
       The global number of levels computed as the maximum number of levels       taken over all MPI ranks, so <tt>n_levels()<=n_global_levels =       max(n_levels() on proc i)</tt>.      
* [0.x.26]*
       A set containing the subdomain_id (MPI rank) of the owners of the       ghost cells on this processor.      
* [0.x.27]*
       A set containing the MPI ranks of the owners of the level ghost cells       on this processor (for all levels).      
* [0.x.28]*
       Partitioner for the global active cell indices.      
* [0.x.29]*
       Partitioner for the global level cell indices for each level.      
* [0.x.30]*
     Update the number_cache variable after mesh creation or refinement.    
* [0.x.31]*
      [2.x.44]   [2.x.45]     
* [0.x.32]*
     Reset global active cell indices and global level cell indices.    
* [0.x.33]*
   A base class for distributed triangulations, i.e., triangulations that   do not store all cells on all processors. This implies that not   every detail of a triangulation may be known on each processor.   In particular, you have to expect that triangulations of classes   derived from this one only store some of the active cells (namely,   the    [2.x.46]  "locally owned cells"),   along with    [2.x.47]  "ghost cells"   and possibly    [2.x.48]  "artificial cells".   In contrast to the classes   derived from  [2.x.49]  it is certain that the   classes derived from the current class will not store the entire   triangulation as long as it has a large enough number of cells. (The   difference to  [2.x.50]  is that the    [2.x.51]  is derived from    [2.x.52]  but not from the current class.) The   distinction is not large in practice: Everything that is difficult for   parallel distributed triangulation is generally also difficult for any   other kind of parallel triangulation classes; however, this intermediate   base class allows to further differentiate between the different kinds of   classes providing parallel mesh functionality.     This class can, then, be used to test whether a   pointer or reference to a triangulation object refers to any kind of   parallel triangulation, or whether the triangulation is in fact   parallel distributed. In other words, one could write a function like   this:  
* [1.x.2]
*   
* [0.x.34]*
     Constructor.    
* [0.x.35]*
     Reset this triangulation into a virgin state by deleting all data.         Note that this operation is only allowed if no subscriptions to this     object exist any more, such as DoFHandler objects using it.    
* [0.x.36]*
     Save the triangulation into the given file. This file needs to be     reachable from all nodes in the computation on a shared network file     system. See the SolutionTransfer class on how to store solution vectors     into this file. Additional cell-based data can be saved using     register_data_attach().    
* [0.x.37]*
     Load the triangulation saved with save() back in. Cell-based data that     was saved with register_data_attach() can be read in with     notify_ready_to_unpack() after calling load().    
* [0.x.38]*
     Register a function that can be used to attach data of fixed size     to cells. This is useful for two purposes: (i) Upon refinement and     coarsening of a triangulation ( [2.x.53]  e.g. in      [2.x.54]      one needs to be able to store one or more data vectors per cell that     characterizes the solution values on the cell so that this data can     then be transferred to the new owning processor of the cell (or     its parent/children) when the mesh is re-partitioned; (ii) when     serializing a computation to a file, it is necessary to attach     data to cells so that it can be saved ( [2.x.55]  e.g. in      [2.x.56]  along with the cell's     other information and, if necessary, later be reloaded from disk     with a different subdivision of cells among the processors.         The way this function works is that it allows any number of interest     parties to register their intent to attach data to cells. One example     of classes that do this is  [2.x.57]      where each  [2.x.58]  object that works     on the current Triangulation object then needs to register its intent.     Each of these parties registers a callback function (the first     argument here,  [2.x.59]  that will be called whenever the     triangulation's execute_coarsening_and_refinement() or save()     functions are called.         The current function then returns an integer handle that corresponds     to the number of data set that the callback provided here will attach.     While this number could be given a precise meaning, this is     not important: You will never actually have to do anything with     this number except return it to the notify_ready_to_unpack() function.     In other words, each interested party (i.e., the caller of the current     function) needs to store their respective returned handle for later use     when unpacking data in the callback provided to     notify_ready_to_unpack().         Whenever  [2.x.60]  is then called by     execute_coarsening_and_refinement() or load() on a given cell, it     receives a number of arguments. In particular, the first     argument passed to the callback indicates the cell for which     it is supposed to attach data. This is always an active cell.         The second, CellStatus, argument provided to the callback function     will tell you if the given cell will be coarsened, refined, or will     persist as is. (This status may be different than the refinement     or coarsening flags set on that cell, to accommodate things such as     the "one hanging node per edge" rule.). These flags need to be     read in context with the p4est quadrant they belong to, as their     relations are gathered in local_cell_relations.         Specifically, the values for this argument mean the following:
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - `CELL_PERSIST`: The cell won't be refined/coarsened, but might be     moved to a different processor. If this is the case, the callback     will want to pack up the data on this cell into an array and store     it at the provided address for later unpacking wherever this cell     may land.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - `CELL_REFINE`: This cell will be refined into 4 or 8 cells (in 2d     and 3d, respectively). However, because these children don't exist     yet, you cannot access them at the time when the callback is     called. Thus, in local_cell_relations, the corresponding     p4est quadrants of the children cells are linked to the deal.II     cell which is going to be refined. To be specific, only the very     first child is marked with `CELL_REFINE`, whereas the others will be     marked with `CELL_INVALID`, which indicates that these cells will be     ignored by default during the packing or unpacking process. This     ensures that data is only transferred once onto or from the parent     cell. If the callback is called with `CELL_REFINE`, the callback     will want to pack up the data on this cell into an array and store     it at the provided address for later unpacking in a way so that     it can then be transferred to the children of the cell that will     then be available. In other words, if the data the callback     will want to pack up corresponds to a finite element field, then     the prolongation from parent to (new) children will have to happen     during unpacking.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - `CELL_COARSEN`: The children of this cell will be coarsened into the     given cell. These children still exist, so if this is the value     given to the callback as second argument, the callback will want     to transfer data from the children to the current parent cell and     pack it up so that it can later be unpacked again on a cell that     then no longer has any children (and may also be located on a     different processor). In other words, if the data the callback     will want to pack up corresponds to a finite element field, then     it will need to do the restriction from children to parent at     this point.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - `CELL_INVALID`: See `CELL_REFINE`.        
*  [2.x.61]  If this function is used for serialization of data       using save() and load(), then the cell status argument with which       the callback is called will always be `CELL_PERSIST`.         The callback function is expected to return a memory chunk of the     format  [2.x.62]  representing the packed data on a     certain cell.         The second parameter  [2.x.63]  indicates whether     the returned size of the memory region from the callback function     varies by cell (<tt>=true</tt>) or stays constant on each one     throughout the whole domain (<tt>=false</tt>).        
*  [2.x.64]  The purpose of this function is to register intent to       attach data for a single, subsequent call to       execute_coarsening_and_refinement() and notify_ready_to_unpack(),       save(), load(). Consequently, notify_ready_to_unpack(), save(),       and load() all forget the registered callbacks once these       callbacks have been called, and you will have to re-register       them with a triangulation if you want them to be active for       another call to these functions.    
* [0.x.39]*
     This function is the opposite of register_data_attach(). It is called     [1.x.3] the execute_coarsening_and_refinement() or save()/load()     functions are done when classes and functions that have previously     attached data to a triangulation for either transfer to other     processors, across mesh refinement, or serialization of data to     a file are ready to receive that data back. The important part about     this process is that the triangulation cannot do this right away from     the end of execute_coarsening_and_refinement() or load() via a     previously attached callback function (as the register_data_attach()     function does) because the classes that eventually want the data     back may need to do some setup between the point in time where the     mesh has been recreated and when the data can actually be received.     An example is the  [2.x.65]  class     that can really only receive the data once not only the mesh is     completely available again on the current processor, but only     after a DoFHandler has been reinitialized and distributed     degrees of freedom. In other words, there is typically a significant     amount of set up that needs to happen in user space before the classes     that can receive data attached to cell are ready to actually do so.     When they are, they use the current function to tell the triangulation     object that now is the time when they are ready by calling the     current function.         The supplied callback function is then called for each newly locally     owned cell. The first argument to the callback is an iterator that     designates the cell; the second argument indicates the status of the     cell in question; and the third argument localizes a memory area by     two iterators that contains the data that was previously saved from     the callback provided to register_data_attach().         The CellStatus will indicate if the cell was refined, coarsened, or     persisted unchanged. The  [2.x.66]  argument to the callback     will then either be an active,     locally owned cell (if the cell was not refined), or the immediate     parent if it was refined during execute_coarsening_and_refinement().     Therefore, contrary to during register_data_attach(), you can now     access the children if the status is `CELL_REFINE` but no longer for     callbacks with status `CELL_COARSEN`.         The first argument to this function, `handle`, corresponds to     the return value of register_data_attach(). (The precise     meaning of what the numeric value of this handle is supposed     to represent is neither important, nor should you try to use     it for anything other than transmit information between a     call to register_data_attach() to the corresponding call to     notify_ready_to_unpack().)    
* [0.x.40]*
     Save additional cell-attached data into the given file. The first     arguments are used to determine the offsets where to write buffers to.         Called by  [2.x.67] .    
* [0.x.41]*
     Load additional cell-attached data from the given file, if any was saved.     The first arguments are used to determine the offsets where to read     buffers from.         Called by  [2.x.68] .    
* [0.x.42]*
     A function to record the CellStatus of currently active cells that     are locally owned. This information is mandatory to transfer data     between meshes during adaptation or serialization, e.g., using      [2.x.69]          Relations will be stored in the private member local_cell_relations. For     an extensive description of CellStatus, see the documentation for the     member function register_data_attach().    
* [0.x.43]*
     Auxiliary data structure for assigning a CellStatus to a deal.II cell     iterator. For an extensive description of the former, see the     documentation for the member function register_data_attach().    
* [0.x.44]*
     Vector of pairs, each containing a deal.II cell iterator and its     respective CellStatus. To update its contents, use the     update_cell_relations() member function.    
* [0.x.45]*
     A structure that stores information about the data that has been, or     will be, attached to cells via the register_data_attach() function     and later retrieved via notify_ready_to_unpack().    
* [0.x.46]*
       number of functions that get attached to the Triangulation through       register_data_attach() for example SolutionTransfer.      
* [0.x.47]*
       number of functions that need to unpack their data after a call from       load()      
* [0.x.48]*
       These callback functions will be stored in the order in which they       have been registered with the register_data_attach() function.      
* [0.x.49]*
     This class in the private scope of  [2.x.70]      is dedicated to the data transfer across repartitioned meshes     and to/from the file system.         It is designed to store all data buffers intended for transfer.    
* [0.x.50]*
       Prepare data transfer by calling the pack callback functions on each       cell       in  [2.x.71]              All registered callback functions in  [2.x.72]  will write       into the fixed size buffer, whereas each entry of  [2.x.73]        will write its data into the variable size buffer.      
* [0.x.51]*
       Unpack the CellStatus information on each entry of        [2.x.74]              Data has to be previously transferred with execute_transfer()       or read from the file system via load().      
* [0.x.52]*
       Unpack previously transferred data on each cell registered in        [2.x.75]  with the provided  [2.x.76]  function.             The parameter  [2.x.77]  corresponds to the position where the        [2.x.78]  function is allowed to read from the memory. Its       value needs to be in accordance with the corresponding pack_callback       function that has been registered previously.             Data has to be previously transferred with execute_transfer()       or read from the file system via load().      
* [0.x.53]*
       Transfer data to file system.             The data will be written in a separate file, whose name       consists of the stem  [2.x.79]  and an attached identifier       <tt>_fixed.data</tt> for fixed size data and <tt>_variable.data</tt>       for variable size data.             All processors write into these files simultaneously via MPIIO.       Each processor's position to write to will be determined       from the provided input parameters.             Data has to be previously packed with pack_data().      
* [0.x.54]*
       Transfer data from file system.             The data will be read from separate file, whose name       consists of the stem  [2.x.80]  and an attached identifier       <tt>_fixed.data</tt> for fixed size data and <tt>_variable.data</tt>       for variable size data.       The  [2.x.81]  and  [2.x.82]        parameters are required to gather the memory offsets for each       callback.             All processors read from these files simultaneously via MPIIO.       Each processor's position to read from will be determined       from the provided input arguments.             After loading, unpack_data() needs to be called to finally       distribute data across the associated triangulation.      
* [0.x.55]*
       Clears all containers and associated data, and resets member       values to their default state.             Frees memory completely.      
* [0.x.56]*
       Flag that denotes if variable size data has been packed.      
* [0.x.57]*
       Cumulative size in bytes that those functions that have called       register_data_attach() want to attach to each cell. This number       only pertains to fixed-sized buffers where the data attached to       each cell has exactly the same size.             The last entry of this container corresponds to the data size       packed per cell in the fixed size buffer (which can be accessed       calling <tt>sizes_fixed_cumulative.back()</tt>).      
* [0.x.58]*
       Consecutive buffers designed for the fixed size transfer       functions of p4est.      
* [0.x.59]*
       Consecutive buffers designed for the variable size transfer       functions of p4est.      
* [0.x.60]