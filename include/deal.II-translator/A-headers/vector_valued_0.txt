[0.x.0]*


* 
*  [2.x.0] 
* 

*  Vector-valued problems are systems of partial differential equations. These are problems where the solution variable is not a scalar function, but a vector-valued function or a set of functions. This includes, for example:  [2.x.1]     [2.x.2] The elasticity equation discussed in  [2.x.3] ,        [2.x.4] , and  [2.x.5]  in which the       solution is the vector-valued displacement at each point.    [2.x.6] The mixed Laplace equation and its extensions discussed in        [2.x.7] , and  [2.x.8]  in which the       solution is the scalar pressure and the vector-valued velocity       at each point.    [2.x.9] The Stokes equation and its extensions discussed in        [2.x.10] , and  [2.x.11]  in which again       the solution is the scalar pressure and the vector-valued velocity       at each point.    [2.x.12] Complex-valued solutions consisting of real and imaginary parts, as       discussed for example in  [2.x.13] .  [2.x.14] 
*  This page gives an overview of how to implement such vector-valued problems easily in deal.II. In particular, it explains the usage of the class FESystem, which allows us to write code for systems of partial differential very much like we write code for single equations.
*   [2.x.15] 
*   [2.x.16] 
* 

* 
*   [2.x.17]  VVExamples [1.x.1]
*  The way one deals systematically with vector-valued problems is not fundamentally different from scalar problems: first, we need a weak (variational) formulation of the problem that takes into account all the solution variables. After we did so, generating the system matrix and solving the linear system follows the same outlines that we are used to already.
*  [1.x.2]
*  Let us take for example the elasticity problem from  [2.x.18]  and even simplify it by choosing  [2.x.19]  and  [2.x.20]  to highlight the important concepts. Therefore, let consider the following weak formulation: find  [2.x.21]  such that for all  [2.x.22]  holds [1.x.3] Here, [1.x.4] denotes the symmetric gradient defined by  [2.x.23]  and the colon indicates double contraction of two tensors of rank 2 (the Frobenius inner product). This bilinear form looks indeed very much like the bilinear form of the Poisson problem in  [2.x.24] . The only differences are  [2.x.25]   [2.x.26] We replaced the gradient operator by the symmetric gradient; this is actually not a significant difference, and everything said here is true if your replace  [2.x.27]  by  [2.x.28] . Indeed, let us do this to simplify the discussion: [1.x.5] Note though, that this system is not very exciting, since we could solve for the three components of [1.x.6] separately.
*   [2.x.29]  The trial and test functions are now from the space  [2.x.30] , which can be viewed as three copies of the scalar space  [2.x.31] . And this is exactly, how we are going to implement this space below, using FESystem.  [2.x.32] 
*  But for now, let us look at the system a little more closely. First, let us exploit that [1.x.7]=([1.x.8]<sub>1</sub>,[1.x.9]<sub>2</sub>,[1.x.10]<sub>3</sub>)<sup>T</sup> and [1.x.11] accordingly. Then, we can write the simplified equation in coordinates as [1.x.12] We see, that this is just three copies of the bilinear form of the Laplacian, one applied to each component (this is where the formulation with the  [2.x.33]  is more exciting, and we want to derive a framework that applies to that one as well). We can make this weak form a system of differential equations again by choosing special test functions: first, choose [1.x.13]=([1.x.14]<sub>1</sub>,0,0)<sup>T</sup>, then [1.x.15]=(0,[1.x.16]<sub>2</sub>,0)<sup>T</sup>, and finally [1.x.17]=(0,0,[1.x.18]<sub>3</sub>)<sup>T</sup>. writing the outcomes below each other, we obtain the system [1.x.19] where we used the standard inner product notation  [2.x.34] . It is important for our understanding, that we keep in mind that the latter form as a system of PDE is completely equivalent to the original definition of the bilinear form [1.x.20]([1.x.21],[1.x.22]), which does not immediately exhibit this system structure. Let us close by writing the full system of the elastic equation with symmetric gradient [1.x.23]: [1.x.24] Very formally, if we believe in operator valued matrices, we can rewrite this in the form [1.x.25]<sup>T</sup>[1.x.26] = [1.x.27]<sup>T</sup>[1.x.28] or [1.x.29]
*  [1.x.30] Now, let us consider a more complex example, the mixed Laplace equations discussed in  [2.x.35]  in three dimensions:[1.x.31]
* 
*  Here, we have four solution components: the scalar pressure  [2.x.36]  and the vector-valued velocity  [2.x.37]  with three vector components. Note as important difference to the previous example, that the vector space [1.x.32] is not just simply a copy of three identical spaces/
*  A systematic way to get a weak or variational form for this and other vector problems is to first consider it as a problem where the operators and solution variables are written in vector and matrix form. For the example, this would read as follows:[1.x.33]
* 
*  This makes it clear that the solution[1.x.34]
*  indeed has four components. We note that we could change the ordering of the solution components  [2.x.38]  and  [2.x.39]  inside  [2.x.40]  if we also change columns of the matrix operator.
*  Next, we need to think about test functions  [2.x.41] . We want to multiply both sides of the equation with them, then integrate over  [2.x.42] . The result should be a scalar equality. We can achieve this by choosing  [2.x.43]  also vector valued as[1.x.35]
* 
*  It is convenient to multiply the matrix-vector equation by the test function from the left, since this way we automatically get the correct matrix later on (in the linear system, the matrix is also multiplied from the right with the solution variable, not from the left), whereas if we multiplied from the right then the matrix so assembled is the transpose of the one we really want.
*  With this in mind, let us multiply by  [2.x.44]  and integrate to get the following equation which has to hold for all test functions  [2.x.45] :[1.x.36]
*  or equivalently:[1.x.37]
* 
*
* We get the final form by integrating by part the second term:[1.x.38]
* 
*  It is this form that we will later use in assembling the discrete weak form into a matrix and a right hand side vector: the form in which we have solution and test functions  [2.x.46]  that each consist of a number of vector components that we can extract.
* 

*   [2.x.47]  VVFEs [1.x.39]
*  Once we have settled on a bilinear form and a functional setting, we need to find a way to describe the vector-valued finite element spaces from which we draw solution and test functions. This is where the FESystem class comes in: it composes vector-valued finite element spaces from simpler ones. In the example of the elasticity problem, we need  [2.x.48]  copies of the same element, for instance

* 
* [1.x.40]
*  This will generate a vector valued space of dimension  [2.x.49] , where each component is a continuous bilinear element of type FE_Q. It will have  [2.x.50]  times as many basis functions as the corresponding FE_Q, and each of these basis functions is a basis function of FE_Q, lifted into one of the components of the vector.
*  For the mixed Laplacian, the situation is more complex. First, we have to settle on a pair of discrete spaces  [2.x.51] . One option would be the stable Raviart-Thomas pair

* 
* [1.x.41]
*  The first element in this system is already a vector valued element of dimension  [2.x.52] , while the second is a regular scalar element.
*  Alternatively to using the stable Raviart-Thomas pair, we could consider a stabilized formulation for the mixed Laplacian, for instance the LDG method. There, we have the option of using the same spaces for velocity components and pressure, namely

* 
* [1.x.42]
*  This system just has  [2.x.53]  equal copies of the same discontinuous element, which not really reflects the structure of the system. Therefore, we prefer

* 
* [1.x.43]
*  Here, we have a system of two elements, one vector-valued and one scalar, very much like with the  [2.x.54] . Indeed, in many codes, the two can be interchanged. This element also allows us easily to switch to an LDG method with lower order approximation in the velocity, namely

* 
* [1.x.44]
*  It must be pointed out, that this element is different from

* 
* [1.x.45]
*  While the constructor call is very similar to  [2.x.55] , the result actually resembles more  [2.x.56]  in that this element produces  [2.x.57]  independent components. A more detailed comparison of the resulting FESystem objects is below.
*  [1.x.46]
*  FESystem has a few internal variables which reflect the internal structure set up by the constructor. These can then also be used by application programs to give structure to matrix assembling and linear algebra. We give the names and values of these variables for the examples above in the following table: <table border="1"> <tr><th>System Element</th>  [2.x.58]   [2.x.59]   [2.x.60]  </tr> <tr><td> [2.x.61] </td><td>1</td> </tr> <tr><td> [2.x.62] </td><td>2</td> </tr> <tr><td> [2.x.63] </td><td>2</td> </tr> <tr><td> [2.x.64] </td><td>1</td> </tr> <tr><td> [2.x.65] </td><td>2</td> </tr> </table>
*  From this table, it is clear that the FESystem reflects a lot of the structure of the system of differential equations in the cases of the  [2.x.66]  and the  [2.x.67] , in that we have a vector valued and a scalar variable. On the other hand, the convoluted elements do not have this structure and we have to reconstruct it somehow when assembling systems, as described below.
*  At this point, it is important to note that nesting of two FESystem object can give the whole FESystem a richer structure than just concatenating them. This structure can be exploited by application programs, but is not automatically so.
*   [2.x.68]  VVAssembling [1.x.47] The next step is to assemble the linear system. How to do this for the simple case of a scalar problem has been shown in many tutorial programs, starting with  [2.x.69] . Here we will show how to do it for vector problems. Corresponding to the different characterizations of weak formulations above and the different system elements created, we have a few options which are outlined below.
*  The whole concept is probably best explained by showing an example illustrating how the local contribution of a cell to the weak form of above mixed Laplace equations could be assembled.
*  [1.x.48] This is essentially how  [2.x.70]  does it:

* 
* [1.x.49]
* 
*  So here's what is happening:  [2.x.71]     [2.x.72]  The first thing we do is to declare "extractors" (see the        FEValuesExtractors namespace). These are objects that don't        do much except store which components of a vector-valued finite        element constitute a single scalar component, or a tensor of        rank 1 (i.e. what we call a "physical vector", always consisting        of  [2.x.73]  components). Here, we declare        an object that represents the velocities consisting of         [2.x.74]  components starting at component zero, and the        extractor for the pressure, which is a scalar component at        position  [2.x.75] .
*     [2.x.76]  We then do our usual loop over all cells, shape functions, and        quadrature points. In the innermost loop, we compute the local        contribution of a pair of shape functions to the global matrix        and right hand side vector. Recall that the cell contributions        to the bilinear form (i.e. neglecting boundary terms) looked as        follows, based on shape functions         [2.x.77] :          [1.x.50]
*         whereas the implementation looked like this:       
* [1.x.51]
*         The similarities are pretty obvious.
*     [2.x.78]  Essentially, what happens in above code is this: when you do         [2.x.79] , a so-called "view" is created, i.e.        an object that unlike the full FEValues object represents not all        components of a finite element, but only the one(s) represented by        the extractor object  [2.x.80]  or         [2.x.81] .
*     [2.x.82]  These views can then be asked for information about these individual        components. For example, when you write         [2.x.83]  you get the        value of the pressure component of the  [2.x.84] th shape function  [2.x.85]  at        the  [2.x.86] th quadrature point. Because the extractor         [2.x.87]  represents a scalar component, the results of        the operator  [2.x.88]  is a scalar        number. On the other hand, the call         [2.x.89]  would produce the        value of a whole set of  [2.x.90]  components, which would        be of type  [2.x.91] .
*     [2.x.92]  Other things that can be done with views is to ask for the gradient        of a particular shape function's components described by an        extractor. For example,  [2.x.93]         would represent the gradient of the scalar pressure component, which        is of type  [2.x.94] , whereas the gradient of the        velocities components,  [2.x.95]         is a  [2.x.96] , i.e. a matrix  [2.x.97]  that consists        of entries  [2.x.98] . Finally,        both scalar and vector views can be asked for the second derivatives        ("Hessians") and vector views can be asked for the symmetric gradient,        defined as  [2.x.99]  as well as the        divergence  [2.x.100] .  [2.x.101]  Other examples of using extractors and views are shown in tutorial programs  [2.x.102] ,  [2.x.103] ,  [2.x.104]  and several other programs.
* 

* 
*  [2.x.105]  In the current context, when we talk about a vector (for example in extracting the velocity components above), we mean the word in the sense physics uses it: it has  [2.x.106]  components that behave in specific ways under coordinate system transformations. Examples include velocity or displacement fields. This is opposed to how mathematics uses the word "vector" (and how we use this word in other contexts in the library, for example in the Vector class), where it really stands for a collection of numbers. An example of this latter use of the word could be the set of concentrations of chemical species in a flame; however, these are really just a collection of scalar variables, since they do not change if the coordinate system is rotated, unlike the components of a velocity vector, and consequently, this  [2.x.107]  class should not be used for this case.
* 

*   [2.x.108]  VVAlternative [1.x.52]
*  There are situations in which one can optimize the assembly of a matrix or right hand side vector a bit, using knowledge of the finite element in use. Consider, for example, the bilinear form of the elasticity equations which we are concerned with first in  [2.x.109] :
* [1.x.53]
*  Here,  [2.x.110]  is a vector function with  [2.x.111]  components,  [2.x.112]  the corresponding test function, and  [2.x.113]  are material parameters. Given our discussions above, the obvious way to implement this bilinear form would be as follows, using an extractor object that interprets all  [2.x.114]  components of the finite element as single vector, rather than disjoint scalar components:
* 

* 
* [1.x.54]
* 
*  Now, this is not the code used in  [2.x.115] . In fact, if one used the above code over the one implemented in that program, it would run about 8 per cent slower. It can be improved (bringing down the penalty to about 4 per cent) by taking a close look at the bilinear form. In fact, we can transform it as follows:[1.x.55]
*  where  [2.x.116]  is the symmetrized gradient. In the second to last step, we used that the scalar product between an arbitrary tensor  [2.x.117]  and a symmetric tensor  [2.x.118]  equals the scalar product of the symmetric part of the former with the second tensor. Using the techniques discussed above, the obvious way to implement this goes like this:
* 

* 
* [1.x.56]
* 
*  So if, again, this is not the code we use in  [2.x.119] , what do we do there? The answer rests on the finite element we use. In  [2.x.120] , we use the following element:

* 
* [1.x.57]
*  In other words, the finite element we use consists of  [2.x.121]  copies of the same scalar element. This is what we call a  [2.x.122]  "primitive" element: an element that may be vector-valued but where each shape function has exactly one non-zero component. In other words: if the  [2.x.123] -component of a displacement shape function is nonzero, then the  [2.x.124] - and  [2.x.125] -components must be zero and similarly for the other components. What this means is that also derived quantities based on shape functions inherit this sparsity property. For example: the divergence  [2.x.126]  of a vector-valued shape function  [2.x.127]  is, in the present case, either  [2.x.128] ,  [2.x.129] , or  [2.x.130] , because exactly one of the  [2.x.131]  is nonzero. Knowing this means that we can save a number of computations that, if we were to do them, would only yield zeros to add up.
*  In a similar vein, if only one component of a shape function is nonzero, then only one row of its gradient  [2.x.132]  is nonzero. What this means for terms like  [2.x.133] , where the scalar product between two tensors is defined as  [2.x.134] , is that the term is only nonzero if both tensors have their nonzero entries in the same row, which means that the two shape functions have to have their single nonzero component in the same location.
*  If we use this sort of knowledge, then we can in a first step avoid computing gradient tensors if we can determine up front that their scalar product will be nonzero, in a second step avoid building the entire tensors and only get its nonzero components, and in a final step simplify the scalar product by only considering that index  [2.x.135]  for the one nonzero row, rather than multiplying and adding up zeros.
*  The vehicle for all this is the ability to determine which vector component is going to be nonzero. This information is provided by the  [2.x.136]  function. What can be done with it, using the example above, is explained in detail in  [2.x.137] .
* 

*   [2.x.138]  VVBlockSolvers [1.x.58]
*  Using techniques as shown above, it isn't particularly complicated to assemble the linear system, i.e. matrix and right hand side, for a vector-valued problem. However, then it also has to be solved. This is more complicated. Naively, one could just consider the matrix as a whole. For most problems, this matrix is not going to be definite (except for special cases like the elasticity equations covered in  [2.x.139]  and  [2.x.140] ). It will, often, also not be symmetric. This rather general class of matrices presents problems for iterative solvers: the lack of structural properties prevents the use of most efficient methods and preconditioners. While it can be done, the solution process will therefore most often be slower than necessary.
*  The answer to this problem is to make use of the structure of the problem. For example, for the mixed Laplace equations discussed above, the operator has the form[1.x.59]
* 
*  It would be nice if this structure could be recovered in the linear system as well. For example, after discretization, we would like to have a matrix with the following block structure:[1.x.60]
*  where  [2.x.141]  represents the mass matrix that results from discretizing the identity operator  [2.x.142]  and  [2.x.143]  the equivalent of the gradient operator.
*  By default, this is not what happens, however. Rather, deal.II assigns %numbers to degrees of freedom in a rather random manner. Consequently, if you form a vector out of the values of degrees of freedom will not be neatly ordered in a vector like[1.x.61]
*  Rather, it will be a permutation of this, with %numbers of degrees of freedom corresponding to velocities and pressures intermixed. Consequently, the system matrix will also not have the nice structure mentioned above, but with the same permutation or rows and columns.
*  What is needed is to re-enumerate degrees of freedom so that velocities come first and pressures last. This can be done using the  [2.x.144]  function, as explained in  [2.x.145]  " [2.x.146] ",  [2.x.147] ,  [2.x.148] , and  [2.x.149]  " [2.x.150] ". After this, at least the degrees of freedom are partitioned properly.
*  But then we still have to make use of it, i.e. we have to come up with a solver that uses the structure. For example, in  [2.x.151] , we do a block elimination of the linear system[1.x.62]
*  What this system means, of course, is[1.x.63]
* 
*  So, if we multiply the first equation by  [2.x.152]  and subtract the second from the result, we get[1.x.64]
* 
*  This is an equation that now only contains the pressure variables. If we can solve it, we can in a second step solve for the velocities using[1.x.65]
* 
*  This has the advantage that the matrices  [2.x.153]  and  [2.x.154]  that we have to solve with are both symmetric and positive definite, as opposed to the large whole matrix we had before.
*  How a solver like this is implemented is explained in more detail in  [2.x.155]  step_20 " [2.x.156] ",  [2.x.157] , and a few other tutorial programs. What we would like to point out here is that we now need a way to extract certain parts of a matrix or vector: if we are to multiply, say, the  [2.x.158]  part of the solution vector by the  [2.x.159]  part of the global matrix, then we need to have a way to access these parts of the whole.
*  This is where the BlockVector, BlockSparseMatrix, and similar classes come in. For all practical purposes, then can be used as regular vectors or sparse matrices, i.e. they offer element access, provide the usual vector operations and implement, for example, matrix-vector multiplications. In other words, assembling matrices and right hand sides works in exactly the same way as for the non-block versions. That said, internally they store the elements of vectors and matrices in "blocks"; for example, instead of using one large array, the BlockVector class stores it as a set of arrays each of which we call a block. The advantage is that, while the whole thing can be used as a vector, one can also access an individual block which then, again, is a vector with all the vector operations.
*  To show how to do this, let us consider the second equation  [2.x.160]  to be solved above. This can be achieved using the following sequence similar to what we have in  [2.x.161] :

* 
* [1.x.66]
* 
*  What's happening here is that we allocate a temporary vector with as many elements as the first block of the solution vector, i.e. the velocity component  [2.x.162] , has. We then set this temporary vector equal to the  [2.x.163]  block of the matrix, i.e.  [2.x.164] , times component 1 of the solution which is the previously computed pressure  [2.x.165] . The result is multiplied by  [2.x.166] , and component 0 of the right hand side,  [2.x.167]  is added to it. The temporary vector now contains  [2.x.168] . The rest of the code snippet simply solves a linear system with  [2.x.169]  as right hand side and the  [2.x.170]  block of the global matrix, i.e.  [2.x.171] . Using block vectors and matrices in this way therefore allows us to quite easily write rather complicated solvers making use of the block structure of a linear system.
* 

* 
*   [2.x.172]  VVExtracting [1.x.67]
*  Once one has computed a solution, it is often necessary to evaluate it at quadrature points, for example to evaluate nonlinear residuals for the next Newton iteration, to evaluate the finite element residual for error estimators, or to compute the right hand side for the next time step in a time dependent problem.
*  The way this is done us to again use an FEValues object to evaluate the shape functions at quadrature points, and with those also the values of a finite element function. For the example of the mixed Laplace problem above, consider the following code after solving:

* 
* [1.x.68]
* 
*  After this, the variable  [2.x.173]  is a list of vectors of a length equal to the number of quadrature points we have initialized the FEValues object with; each of these vectors has  [2.x.174]  elements containing the values of the  [2.x.175]  velocities and the one pressure at a quadrature point.
*  We can use these values to then construct other things like residuals. However, the construct is a bit awkward. First, we have a  [2.x.176] s, which always looks strange. It is also inefficient because it implies dynamic memory allocation for the outer vector as well as for all the inner vectors. Secondly, maybe we are only interested in the velocities, for example to solve an advection problem in a second stage (as, for example, in  [2.x.177]  or  [2.x.178] ). In that case, one would have to hand-extract these values like so:

* 
* [1.x.69]
*  Note how we convert from a  [2.x.179]  (which is simply a collection of vector elements) into a  [2.x.180]  because the velocity is a quantity characterized by  [2.x.181]  elements that have certain transformation properties under rotations of the coordinate system.
*  This code can be written more elegantly and efficiently using code like the following:

* 
* [1.x.70]
* 
*  As a result, we here get the velocities right away, and in the right data type (because we have described, using the extractor, that the first  [2.x.182]  components of the finite element belong together, forming a tensor). The code is also more efficient: it requires less dynamic memory allocation because the Tensor class allocates its components as member variables rather than on the heap, and we save cycles because we don't even bother computing the values of the pressure variable at the quadrature points. On the other hand, if we had been interested in only the pressure and not the velocities, then the following code extracting scalar values would have done:

* 
* [1.x.71]
* 
*  In similar cases, one sometimes needs the gradients or second derivatives of the solution, or of individual scalar or vector components. To get at those of all components of the solution, the functions  [2.x.183]  and  [2.x.184]  are the equivalent of the function  [2.x.185]  used above.
*  Likewise, to extract the gradients of scalar components,  [2.x.186]  and  [2.x.187]  do the job. For vector- (tensor-)valued quantities, there are functions  [2.x.188]  and  [2.x.189]  and in addition  [2.x.190]  and  [2.x.191] 
*  Moreover, there is a shortcut available in case when only the Laplacians of the solution (which is the trace of the hessians) is needed, usable for both scalar and vector-valued problems as  [2.x.192]  and  [2.x.193] 
* 

*   [2.x.194]  VVOutput [1.x.72]
*  As mentioned above, an FESystem object may hold multiple vector components, but it doesn't have a notion what they actually mean. As an example, take the object

* 
* [1.x.73]
*  It has  [2.x.195]  vector components, but what do they mean? Are they the  [2.x.196]  components of a velocity vector plus one pressure? Are they the pressure plus the  [2.x.197]  velocity components? Or are they a collection of scalars?
*  The point is that the FESystem class doesn't care. The [1.x.74] of what the components mean is up to the person who uses the element later, for example in assembling a linear form, or in extracting data solution components for a linearized system in the next Newton step. In almost all cases, this interpretation happens at the place where it is needed.
*  There is one case where one has to be explicit, however, and that is in generating graphical output. The reason is that many file formats for visualization want data that represents vectors (e.g. velocities, displacements, etc) to be stored separately from scalars (pressures, densities, etc), and there often is no way to group a bunch of scalars into a vector field from within a visualization program.
*  To achieve this, we need to let the DataOut class and friends know which components of the FESystem form vectors (with  [2.x.198]  components) and which are scalars. This is shown, for example, in  [2.x.199]  where we generate output as follows:

* 
* [1.x.75]
*  In other words, we here create an array of  [2.x.200]  elements in which we store which elements of the finite element are vectors and which are scalars; the array is filled with  [2.x.201]  copies of  [2.x.202]  and a single trailing element of  [2.x.203]  . The array is then given as an extra argument to  [2.x.204]  to explain how the data in the given solution vector is to be interpreted. Visualization programs like VisIt and Paraview will then offer to show these  [2.x.205]  components as vector fields, rather than as individual scalar fields.
* 

* 

* 
*  [2.x.206] 

* 
* [0.x.1]