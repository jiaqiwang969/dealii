[0.x.0]*


* 
*  [2.x.0] 

* 
*  [2.x.1] 
*  This module deals with constraints on degrees of freedom. The central class to deal with constraints is the AffineConstraints class.
*  Constraints typically come from several sources, for example:
* 

* 
* 
*  - If you have Dirichlet-type boundary conditions,  [2.x.2] ,   one usually enforces   them by requiring that degrees of freedom on the boundary have   particular values, for example  [2.x.3]  if the boundary condition    [2.x.4]  requires that the finite element solution  [2.x.5]    at the location of degree   of freedom 12 has the value 42. Such constraints are generated by   those versions of the  [2.x.6]    function that take a AffineConstraints argument (though there are   also other ways of dealing with Dirichlet conditions, using    [2.x.7]  see for example  [2.x.8]  and  [2.x.9] ).
* 

* 
* 
*  - If you have boundary conditions that set a certain part of the   solution's value, for example no normal flux,  [2.x.10]  (as happens in flow problems and is handled by the    [2.x.11]  function) or   prescribed tangential components,  [2.x.12]  (as happens in electromagnetic problems and   is handled by the  [2.x.13]    function). For the former case, imagine for example that we are at   at vertex where the normal vector has the form  [2.x.14]  and that the  [2.x.15] -,  [2.x.16] - and  [2.x.17] -components of the flow   field at this vertex are associated with degrees of freedom 12, 28,   and 40. Then the no-normal-flux condition means that we need to have   the condition  [2.x.18] .   The prescribed tangential component leads to similar constraints   though there is often something on the right hand side.
* 

* 
* 
*  - If you have hanging node constraints, for example in a mesh like this:         [2.x.19]    Let's assume the bottom right one of the two red degrees of freedom   is  [2.x.20]  and that the two yellow neighbors on its left and right   are  [2.x.21]  and  [2.x.22] . Then, requiring that the finite element   function be continuous is equivalent to requiring that  [2.x.23] . A similar situation occurs in the   context of hp-adaptive finite element methods.   For example, when using Q1 and Q2 elements (i.e. using   FE_Q(1) and FE_Q(2)) on the two marked cells of the mesh        [2.x.24]    there are three constraints: first  [2.x.25] ,   then  [2.x.26] , and finally the identity    [2.x.27] . Similar constraints occur as hanging nodes even if all   cells used the same finite elements. In all of these cases, you   would use the  [2.x.28]  function to   compute such constraints.
* 

* 
* 
*  - Other linear constraints, for example when you try to impose a certain   average value for a problem that would otherwise not have a unique   solution. An example of this is given in the  [2.x.29]  tutorial program.
*  In all of these examples, constraints on degrees of freedom are linear and possibly inhomogeneous. In other words, they always have the form  [2.x.30] . The deal.II class that deals with storing and using these constraints is AffineConstraints.
* 

*  [1.x.0]
*  When building the global system matrix and the right hand sides, one can build them without taking care of the constraints, i.e. by simply looping over cells and adding the local contributions to the global matrix and right hand side objects. In order to do actual calculations, you have to 'condense' the linear system: eliminate constrained degrees of freedom and distribute the appropriate values to the unconstrained dofs. This changes the sparsity pattern of the sparse matrices used in finite element calculations and is thus a quite expensive operation. The general scheme of things is then that you build your system, you eliminate (condense) away constrained nodes using the  [2.x.31]  functions, then you solve the remaining system, and finally you compute the values of constrained nodes from the values of the unconstrained ones using the  [2.x.32]  function. Note that the  [2.x.33]  function is applied to matrix and right hand side of the linear system, while the  [2.x.34]  function is applied to the solution vector.
*  This scheme of first building a linear system and then eliminating constrained degrees of freedom is inefficient, and a bottleneck if there are many constraints and matrices are full, i.e. especially for 3d and/or higher order or hp-finite elements. Furthermore, it is impossible to implement for %parallel computations where a process may not have access to elements of the matrix. We therefore offer a second way of building linear systems, using the  [2.x.35]  and  [2.x.36]  functions discussed below. The resulting linear systems are equivalent to those one gets after calling the  [2.x.37]  functions.
* 

* 
*  [2.x.38]  Both ways of applying constraints set the value of the matrix diagonals to constrained entries to a [1.x.1] entry of the same magnitude as the other entries in the matrix. As a consequence, you need to set up your problem such that the weak form describing the main matrix contribution is not [1.x.2]. Otherwise, iterative solvers such as CG will break down or be considerably slower as GMRES.
* 

* 
*  [2.x.39]  While these two ways are [1.x.3], i.e., the solution of linear systems computed via either approach is the same, the linear systems themselves do not necessarily have the same matrix and right hand side vector entries. Specifically, the matrix diagonal and right hand side entries corresponding to constrained degrees of freedom may be different as a result of the way in which we compute them; they are, however, always chosen in such a way that the solution to the linear system is the same.
*  [1.x.4]
*  As mentioned above, the first way of using constraints is to build linear systems without regards to constraints and then "condensing" them away. Condensation of a matrix is done in four steps:
* 

* 
* 
*  - first one builds the   sparsity pattern (e.g. using  [2.x.40] 
* 

* 
* 
*  - then the sparsity pattern of the condensed matrix is made out of   the original sparsity pattern and the constraints;
* 

* 
* 
*  - third, the global matrix is assembled;
* 

* 
* 
*  - and fourth, the matrix is finally condensed.
*  In the condensation process, we are not actually changing the number of rows or columns of the sparsity pattern, matrix, and vectors. Instead, the condense functions add nonzero entries to the sparsity pattern of the matrix (with constrained nodes in it) where the condensation process of the matrix will create additional nonzero elements. In the condensation process itself, rows and columns subject to constraints are distributed to the rows and columns of unconstrained nodes. The constrained degrees of freedom remain in place. In order not to disturb the solution process, these rows and columns are filled with zeros and an appropriate positive value on the main diagonal (we choose an average of the magnitudes of the other diagonal elements, so as to make sure that the new diagonal entry has the same order of magnitude as the other entries; this preserves the scaling properties of the matrix). The corresponding value in the right hand sides is set to zero. This way, the constrained node will always get the value zero upon solution of the equation system and will not couple to other nodes any more.
*  Keeping the entries in the matrix has the advantage over creating a new and smaller matrix, that only one matrix and sparsity pattern is needed thus less memory is required. Additionally, the condensation process is less expensive, since not all but only constrained values in the matrix have to be copied. On the other hand, the solution process will take a bit longer, since matrix vector multiplications will incur multiplications with zeroes in the lines subject to constraints. Additionally, the vector size is larger, resulting in more memory consumption for those iterative solution methods using a larger number of auxiliary vectors (e.g. methods using explicit orthogonalization procedures). Nevertheless, this process is more efficient due to its lower memory consumption.
*  The condensation functions exist for different argument types: SparsityPattern, SparseMatrix and BlockSparseMatrix. Note that there are no versions for arguments of type  [2.x.41]  or any of the other PETSc or Trilinos matrix wrapper classes. This is due to the fact that it is relatively hard to get a representation of the sparsity structure of PETSc matrices, and to modify them efficiently; this holds in particular, if the matrix is actually distributed across a cluster of computers. If you want to use PETSc/Trilinos matrices, you can either copy an already condensed deal.II matrix, or assemble the PETSc/Trilinos matrix in the already condensed form, see the discussion below.
* 

*  [1.x.5]
*  Condensing vectors works exactly as described above for matrices. Note that condensation is an idempotent operation, i.e. doing it more than once on a vector or matrix yields the same result as doing it only once: once an object has been condensed, further condensation operations don't change it any more.
*  In contrast to the matrix condensation functions, the vector condensation functions exist in variants for PETSc and Trilinos vectors. However, using them is typically expensive, and should be avoided. You should use the same techniques as mentioned above to avoid their use.
* 

*  [1.x.6]
*  Sometimes, one wants to avoid explicit condensation of a linear system after it has been built at all. There are two main reasons for wanting to do so:
*   [2.x.42]   [2.x.43]  Condensation is an expensive operation, in particular if there are many constraints and/or if the matrix has many nonzero entries. Both is typically the case for 3d, or high polynomial degree computations, as well as for hp-finite element methods, see for example the  [2.x.44]  "hp-paper". This is the case discussed in the hp-tutorial program,  [2.x.45]  step_27 " [2.x.46] ", as well as in  [2.x.47]  and  [2.x.48]  " [2.x.49] ".
*   [2.x.50]  There may not be an  [2.x.51]  function for the matrix you use (this is, for example, the case for the PETSc and Trilinos wrapper classes where we have no access to the underlying representation of the matrix, and therefore cannot efficiently implement the  [2.x.52]  operation). This is the case discussed in  [2.x.53] ,  [2.x.54] ,  [2.x.55] , and  [2.x.56] .  [2.x.57] 
*  In this case, one possibility is to distribute local entries to the final destinations right at the moment of transferring them into the global matrices and vectors, and similarly build a sparsity pattern in the condensed form at the time it is set up originally.
*  The AffineConstraints class offers support for these operations as well. For example, the  [2.x.58]  function adds nonzero entries to a sparsity pattern object. It not only adds a given entry, but also all entries that we will have to write to if the current entry corresponds to a constrained degree of freedom that will later be eliminated. Similarly, one can use the  [2.x.59]  functions to directly distribute entries in vectors and matrices when copying local contributions into a global matrix or vector. These calls make a subsequent call to  [2.x.60]  unnecessary. For examples of their use see the tutorial programs referenced above.
*  Note that, despite their name which describes what the function really does, the  [2.x.61]  functions has to be applied to matrices and right hand side vectors, whereas the  [2.x.62]  function discussed below is applied to the solution vector after solving the linear system.
* 

*  [1.x.7]
*  After solving the condensed system of equations, the solution vector has to be "distributed": the modification to the original linear system that results from calling  [2.x.63]  leads to a linear system that solves correctly for all degrees of freedom that are unconstrained but leaves the values of constrained degrees of freedom undefined. To get the correct values also for these degrees of freedom, you need to "distribute" the unconstrained values also to their constrained colleagues. This is done by the  [2.x.64]  function. The operation of distribution undoes the condensation process in some sense, but it should be noted that it is not the inverse operation. Basically, distribution sets the values of the constrained nodes to the value that is computed from the constraint given the values of the unconstrained nodes plus possible inhomogeneities.
* 

*  [1.x.8]
*  In case some constraint lines have inhomogeneities (which is typically the case if the constraint comes from implementation of inhomogeneous boundary conditions), the situation is a bit more complicated than if the only constraints were due to hanging nodes alone. This is because the elimination of the non-diagonal values in the matrix generate contributions in the eliminated rows in the vector. This means that inhomogeneities can only be handled with functions that act simultaneously on a matrix and a vector. This means that all inhomogeneities are ignored in case the respective condense function is called without any matrix (or if the matrix has already been condensed before).
*  The use of the AffineConstraints class for implementing Dirichlet boundary conditions is discussed in the  [2.x.65]  tutorial program. A further example that utilizes AffineConstraints is  [2.x.66] . The situation here is little more complicated, because there we have some constraints which are not at the boundary. There are two ways to apply inhomogeneous constraints after creating an AffineConstraints object:
*  First approach:
* 

* 
* 
*  - Apply the  [2.x.67]  function to the   system matrix and the right-hand-side with the parameter   use_inhomogeneities_for_rhs = false (i.e., the default)
* 

* 
* 
*  - Set the solution to zero in the inhomogeneous constrained components   using the  [2.x.68]  function (or start with a solution   vector equal to zero)
* 

* 
* 
*  - solve() the linear system
* 

* 
* 
*  - Apply  [2.x.69]  to the solution
*  Second approach:
* 

* 
* 
*  - Use the  [2.x.70]  function with   the parameter use_inhomogeneities_for_rhs = true and apply it to the   system matrix and the right-hand-side
* 

* 
* 
*  - Set the concerning components of the solution to the inhomogeneous   constrained values (for example using  [2.x.71] 
* 

* 
* 
*  - solve() the linear system
* 

* 
* 
*  - Depending on the solver now you have to apply the    [2.x.72]  function to the solution, because the   solver could change the constrained values in the solution. For a   Krylov based solver this should not be strictly necessary, but it is   still possible that there is a difference between the inhomogeneous   value and the solution value in the order of machine precision, and   you may want to call  [2.x.73]  anyway if you   have additional constraints such as from hanging nodes.
*  Of course, both approaches lead to the same final answer but in different ways. Using the first approach (i.e., when using  [2.x.74]  in  [2.x.75]  the linear system we build has zero entries in the right hand side in all those places where a degree of freedom is constrained, and some positive value on the matrix diagonal of these lines. Consequently, the solution vector of the linear system will have a zero value for inhomogeneously constrained degrees of freedom and we need to call  [2.x.76]  to give these degrees of freedom their correct nonzero values.
*  On the other hand, in the second approach, the matrix diagonal element and corresponding right hand side entry for inhomogeneously constrained degrees of freedom are so that the solution of the linear system already has the correct value (e.g., if the constraint is that  [2.x.77]  then row  [2.x.78]  if the matrix is empty with the exception of the diagonal entry, and  [2.x.79]  so that the solution of  [2.x.80]  must satisfy  [2.x.81]  as desired). As a consequence, we do not need to call  [2.x.82]  after solving to fix up inhomogeneously constrained components of the solution, though there is also no harm in doing so.
*  There remains the question of which of the approaches to take and why we need to set to zero the values of the solution vector in the first approach. The answer to both questions has to do with how iterative solvers solve the linear system. To this end, consider that we typically stop iterations when the residual has dropped below a certain fraction of the norm of the right hand side, or, alternatively, a certain fraction of the norm of the initial residual. Now consider this:
* 

* 
* 
*  - In the first approach, the right hand side entries for constrained   degrees of freedom are zero, i.e., the norm of the right hand side   really only consists of those parts that we care about. On the other   hand, if we start with a solution vector that is not zero in   constrained entries, then the initial residual is very large because   the value that is currently in the solution vector does not match the   solution of the linear system (which is zero in these components).   Thus, if we stop iterations once we have reduced the initial residual   by a certain factor, we may reach the threshold after a single   iteration because constrained degrees of freedom are resolved by   iterative solvers in just one iteration. If the initial residual   was dominated by these degrees of freedom, then we see a steep   reduction in the first step although we did not really make much   progress on the remainder of the linear system in this just one   iteration. We can avoid this problem by either stopping iterations   once the norm of the residual reaches a certain fraction of the   [1.x.9], or we can set the solution   components to zero (thus reducing the initial residual) and iterating   until we hit a certain fraction of the [1.x.10].
* 

* 
* 
*  - In the second approach, we get the same problem if the starting vector   in the iteration is zero, since then the residual may be   dominated by constrained degrees of freedom having values that do not   match the values we want for them at the solution. We can again   circumvent this problem by setting the corresponding elements of the   solution vector to their correct values, by calling    [2.x.83]  [1.x.11] solving the linear system   (and then, as necessary, a second time after solving).
*  In addition to these considerations, consider the case where we have inhomogeneous constraints of the kind  [2.x.84] , e.g., from a hanging node constraint of the form  [2.x.85]  where  [2.x.86]  is itself constrained by boundary values to  [2.x.87] . In this case, the AffineConstraints container can of course not figure out what the final value of  [2.x.88]  should be and, consequently, can not set the solution vector's third component correctly. Thus, the second approach will not work and you should take the first.
* 

*  [1.x.12]
*  There are situations where degrees of freedom are constrained in more than one way, and sometimes in conflicting ways. Consider, for example the following situation:      [2.x.89]  Here, degree of freedom  [2.x.90]  marked in blue is a hanging node. If we used trilinear finite elements, i.e. FE_Q(1), then it would carry the constraint  [2.x.91] . On the other hand, it is at the boundary, and if we have imposed boundary conditions  [2.x.92]  then we will have the constraint  [2.x.93]  where  [2.x.94]  is the value of the boundary function  [2.x.95]  at the location of this degree of freedom.
*  So, which one will win? Or maybe: which one [1.x.13] win? There is no good answer to this question:
* 

* 
* 
*  - If the hanging node constraint is the one that is ultimately enforced,   then the resulting solution does not satisfy boundary   conditions any more for general boundary functions  [2.x.96] .
* 

* 
* 
*  - If it had been done the other way around, the solution would not satisfy   hanging node constraints at this point and consequently would not   satisfy the regularity properties of the element chosen (e.g. would not   be continuous despite using a  [2.x.97]  element).
* 

* 
* 
*  - The situation becomes completely hopeless if you consider   curved boundaries since then the edge midpoint (i.e. the hanging node)   does in general not lie on the mother edge. Consequently, the solution   will not be  [2.x.98]  conforming anyway, regardless of the priority of   the two competing constraints. If the hanging node constraint wins, then   the solution will be neither conforming, nor have the right boundary   values. In other words, it is not entirely clear what the "correct" solution would be. In most cases, it will not matter much: in either case, the error introduced either by the non-conformity or the incorrect boundary values will be at worst at the same order as the discretization's overall error.
*  That said, what should you do if you know what you want is this:
* 

* 
* 
*  - If you want the hanging node constraints to win, then first build   these through the  [2.x.99]  function.   Then interpolate the boundary values using    [2.x.100]  into the same   AffineConstraints object. If the latter function encounters a boundary   node that already is constrained, it will simply ignore the boundary   values at this node and leave the constraint untouched.
* 

* 
* 
*  - If you want the boundary value constraint to win, build the hanging   node constraints as above and use these to assemble the matrix using   the  [2.x.101]  function (or,   alternatively, assemble the matrix and then use    [2.x.102]  on it). In a second step, use the    [2.x.103]  function that returns   a  [2.x.104]  and use it as input for  [2.x.105]    to set boundary nodes to their correct value.
*  Either behavior can also be achieved by building two separate AffineConstraints objects and calling  [2.x.106]  function with a particular second argument.
* 

*  [1.x.14]
*  Sometimes it is either not desirable, or not possible to directly condense, or eliminate constraints from a system of linear equations. In particular if there is no underlying matrix object that could be condensed (or taken care of constraints during assembly). This is usually the case if the system is described by a LinearOperator.
*  In this case we can solve the modified system [1.x.15] instead [1] (M. S. Shephard. Linear multipoint constraints applied via transformation as part of a direct stiffness assembly process. [1.x.16] 20(11):2107-2112, 1985).
*  Here,  [2.x.107]  is a given (unconstrained) system matrix for which we only assume that we can apply it to a vector but can not necessarily access individual matrix entries.  [2.x.108]  is the corresponding right hand side of a system of linear equations  [2.x.109] . The matrix  [2.x.110]  describes the homogeneous part of the linear constraints stored in an AffineConstraints object and the vector  [2.x.111]  is the vector of corresponding inhomogeneities. More precisely, the  [2.x.112]  operation applied on a vector  [2.x.113]  is the operation [1.x.17] And finally,  [2.x.114]  denotes the identity on the subspace of constrained degrees of freedom.
*  The corresponding solution of  [2.x.115]  that obeys these constraints is then recovered by distributing constraints:  [2.x.116] .
*  The whole system can be set up and solved with the following snippet of code:

* 
* [1.x.18]
* 

* 
* [0.x.1]