[0.x.0]*
  [2.x.0]  TrilinosWrappers  [2.x.1] 

* 
* [0.x.1]*
 A namespace in which wrapper classes for Trilinos objects reside.
* 

* 
*  [2.x.2] 

* 
* [0.x.2]*
    [2.x.3]  internal  
* [0.x.3]*
   A namespace for internal implementation details of the TrilinosWrapper   members.    
*  [2.x.4]   
* [0.x.4]*
     Declare type for container size.    
* [0.x.5]*
     This class implements a wrapper for accessing the Trilinos vector in     the same way as we access deal.II objects: it is initialized with a     vector and an element within it, and has a conversion operator to     extract the scalar value of this element. It also has a variety of     assignment operator for writing to this one element.        
*  [2.x.5]     
* [0.x.6]*
       Constructor. It is made private so as to only allow the actual vector       class to create it.      
* [0.x.7]*
       Copy constructor.      
* [0.x.8]*
       This looks like a copy operator, but does something different than       usual. In particular, it does not copy the member variables of this       reference. Rather, it handles the situation where we have two vectors        [2.x.6]  and  [2.x.7]  and assign elements like in <tt>v(i)=w(i)</tt>. Here,       both left and right hand side of the assignment have data type       VectorReference, but what we really mean is to assign the vector       elements represented by the two references. This operator implements       this operation. Note also that this allows us to make the assignment       operator const.      
* [0.x.9]*
       Same as above but for non-const reference objects.      
* [0.x.10]*
       Set the referenced element of the vector to <tt>s</tt>.      
* [0.x.11]*
       Add <tt>s</tt> to the referenced element of the vector->      
* [0.x.12]*
       Subtract <tt>s</tt> from the referenced element of the vector->      
* [0.x.13]*
       Multiply the referenced element of the vector by <tt>s</tt>.      
* [0.x.14]*
       Divide the referenced element of the vector by <tt>s</tt>.      
* [0.x.15]*
       Convert the reference to an actual value, i.e. return the value of       the referenced element of the vector.      
* [0.x.16]*
       Exception      
* [0.x.17]*
       Point to the vector we are referencing.      
* [0.x.18]*
       Index of the referenced element of the vector.      
* [0.x.19]*
    [2.x.8]   
* [0.x.20]*
   Namespace for Trilinos vector classes that work in parallel over MPI.    
*  [2.x.9]   
* [0.x.21]*
     This class implements a wrapper to use the Trilinos distributed vector     class Epetra_FEVector, the (parallel) partitioning of which     is governed by an Epetra_Map.     The Epetra_FEVector is precisely the kind of vector     we deal with all the time
* 
*  - we probably get it from some assembly     process, where also entries not locally owned might need to written and     hence need to be forwarded to the owner.         The interface of this class is modeled after the existing Vector class in     deal.II. It has almost the same member functions, and is often     exchangeable. However, since Trilinos only supports a single scalar type     (double), it is not templated, and only works with that type.         Note that Trilinos only guarantees that operations do what you expect     if the function  [2.x.10]  has been called after vector assembly     in order to distribute the data. This is necessary since some processes     might have accumulated data of elements that are not owned by     themselves, but must be sent to the owning process. In order to avoid     using the wrong data, you need to call  [2.x.11]  before you     actually use the vectors.         [1.x.0]         The parallel functionality of Trilinos is built on top of the Message     Passing Interface (MPI). MPI's communication model is built on     collective communications: if one process wants something from another,     that other process has to be willing to accept this communication. A     process cannot query data from another process by calling a remote     function, without that other process expecting such a transaction. The     consequence is that most of the operations in the base class of this     class have to be called collectively. For example, if you want to     compute the l2 norm of a parallel vector,  [2.x.12]  all processes across     which this vector is shared have to call the  [2.x.13]  function. If     you don't do this, but instead only call the  [2.x.14]  function on one     process, then the following happens: This one process will call one of     the collective MPI functions and wait for all the other processes to     join in on this. Since the other processes don't call this function,     you will either get a time-out on the first process, or, worse, by the     time the next a call to a Trilinos function generates an MPI message on     the other processes, you will get a cryptic message that only a subset     of processes attempted a communication. These bugs can be very hard to     figure out, unless you are well-acquainted with the communication model     of MPI, and know which functions may generate MPI messages.         One particular case, where an MPI message may be generated unexpectedly     is discussed below.             [1.x.1]         Trilinos does of course allow read access to individual elements of a     vector, but in the distributed case only to elements that are stored     locally. We implement this through calls like <tt>d=vec(i)</tt>.     However, if you access an element outside the locally stored range, an     exception is generated.         In contrast to read access, Trilinos (and the respective deal.II     wrapper classes) allow to write (or add) to individual elements of     vectors, even if they are stored on a different process. You can do     this by writing into or adding to elements using the syntax     <tt>vec(i)=d</tt> or <tt>vec(i)+=d</tt>, or similar operations. There     is one catch, however, that may lead to very confusing error messages:     Trilinos requires application programs to call the compress() function     when they switch from performing a set of operations that add to     elements, to performing a set of operations that write to elements. The     reasoning is that all processes might accumulate addition operations to     elements, even if multiple processes write to the same elements. By the     time we call compress() the next time, all these additions are     executed. However, if one process adds to an element, and another     overwrites to it, the order of execution would yield non-deterministic     behavior if we don't make sure that a synchronization with compress()     happens in between.         In order to make sure these calls to compress() happen at the     appropriate time, the deal.II wrappers keep a state variable that store     which is the presently allowed operation: additions or writes. If it     encounters an operation of the opposite kind, it calls compress() and     flips the state. This can sometimes lead to very confusing behavior, in     code that may for example look like this:        
* [1.x.2]
*          This code can run into trouble: by the time we see the first addition     operation, we need to flush the overwrite buffers for the vector, and     the deal.II library will do so by calling compress(). However, it will     only do so for all processes that actually do an addition
* 
*  -  if the     condition is never true for one of the processes, then this one will     not get to the actual compress() call, whereas all the other ones do.     This gets us into trouble, since all the other processes hang in the     call to flush the write buffers, while the one other process advances     to the call to compute the l2 norm. At this time, you will get an error     that some operation was attempted by only a subset of processes. This     behavior may seem surprising, unless you know that write/addition     operations on single elements may trigger this behavior.         The problem described here may be avoided by placing additional calls     to compress(), or making sure that all processes do the same type of     operations at the same time, for example by placing zero additions if     necessary.             [1.x.3]         Parallel vectors come in two kinds: without and with ghost elements.     Vectors without ghost elements uniquely partition the vector elements     between processors: each vector entry has exactly one processor that     owns it. For such vectors, you can read those elements that the     processor you are currently on owns, and you can write into any element     whether you own it or not: if you don't own it, the value written or     added to a vector element will be shipped to the processor that owns     this vector element the next time you call compress(), as described     above.         What we call a 'ghosted' vector (see      [2.x.15]  "vectors with ghost elements"     ) is simply a view of the parallel vector where the element     distributions overlap. The 'ghosted' Trilinos vector in itself has no     idea of which entries are ghosted and which are locally owned. In fact,     a ghosted vector may not even store all of the elements a non- ghosted     vector would store on the current processor.  Consequently, for     Trilinos vectors, there is no notion of an 'owner' of vector elements     in the way we have it in the non-ghost case view.         This explains why we do not allow writing into ghosted vectors on the     Trilinos side: Who would be responsible for taking care of the     duplicated entries, given that there is not such information as locally     owned indices? In other words, since a processor doesn't know which     other processors own an element, who would it send a value to if one     were to write to it? The only possibility would be to send this     information to [1.x.4] other processors, but that is clearly not     practical. Thus, we only allow reading from ghosted vectors, which     however we do very often.         So how do you fill a ghosted vector if you can't write to it? This only     happens through the assignment with a non-ghosted vector. It can go     both ways (non-ghosted is assigned to a ghosted vector, and a ghosted     vector is assigned to a non-ghosted one; the latter one typically only     requires taking out the locally owned part as most often ghosted     vectors store a superset of elements of non-ghosted ones). In general,     you send data around with that operation and it all depends on the     different views of the two vectors. Trilinos also allows you to get     subvectors out of a big vector that way.             [1.x.5]         When writing into Trilinos vectors from several threads in shared     memory, several things must be kept in mind as there is no built-in     locks in this class to prevent data races. Simultaneous access to the     same vector entry at the same time results in data races and must be     explicitly avoided by the user. However, it is possible to access     [1.x.6] entries of the vector from several threads     simultaneously when only one MPI process is present or the vector has     been constructed with an additional index set for ghost entries in     write mode.        
*  [2.x.16]     
*  [2.x.17]              2008, 2009, 2017    
* [0.x.22]*
       Declare some of the standard types used in all containers. These types       parallel those in the <tt>C</tt> standard libraries       <tt>vector<...></tt> class.      
* [0.x.23]*
        [2.x.18]  1: Basic Object-handling      
* [0.x.24]*
       Default constructor that generates an empty (zero size) vector. The       function <tt>reinit()</tt> will have to give the vector the correct       size and distribution among processes in case of an MPI run.      
* [0.x.25]*
       Copy constructor using the given vector.      
* [0.x.26]*
       This constructor takes an IndexSet that defines how to distribute the       individual components among the MPI processors. Since it also       includes information about the size of the vector, this is all we       need to generate a %parallel vector.             Depending on whether the  [2.x.19]  argument uniquely       subdivides elements among processors or not, the resulting vector may       or may not have ghost elements. See the general documentation of this       class for more information.             In case the provided IndexSet forms an overlapping partitioning,       it is not clear which elements are owned by which process and       locally_owned_elements() will return an IndexSet of size zero.              [2.x.20]         [2.x.21]  "vectors with ghost elements"      
* [0.x.27]*
       Creates a ghosted parallel vector.             Depending on whether the  [2.x.22]  argument uniquely subdivides       elements among processors or not, the resulting vector may or may not       have ghost elements. See the general documentation of this class for       more information.              [2.x.23]         [2.x.24]  "vectors with ghost elements"      
* [0.x.28]*
       Copy constructor from the TrilinosWrappers vector class. Since a       vector of this class does not necessarily need to be distributed       among processes, the user needs to supply us with an IndexSet and an       MPI communicator that set the partitioning details.             Depending on whether the  [2.x.25]  argument uniquely       subdivides elements among processors or not, the resulting vector may       or may not have ghost elements. See the general documentation of this       class for more information.              [2.x.26]         [2.x.27]  "vectors with ghost elements"      
* [0.x.29]*
       Copy-constructor from deal.II vectors. Sets the dimension to that of       the given vector, and copies all the elements.             Depending on whether the  [2.x.28]  argument uniquely       subdivides elements among processors or not, the resulting vector may       or may not have ghost elements. See the general documentation of this       class for more information.              [2.x.29]         [2.x.30]  "vectors with ghost elements"      
* [0.x.30]*
       Move constructor. Creates a new vector by stealing the internal data       of the vector  [2.x.31]       
* [0.x.31]*
       Destructor.      
* [0.x.32]*
       Release all memory and return to a state just like after having called       the default constructor.      
* [0.x.33]*
       Reinit functionality. This function sets the calling vector to the       dimension and the parallel distribution of the input vector, but does       not copy the elements in <tt>v</tt>. If <tt>omit_zeroing_entries</tt>       is not <tt>true</tt>, the elements in the vector are initialized with       zero. If it is set to <tt>true</tt>, the vector entries are in an       unspecified state and the user has to set all elements. In the       current implementation, this method does not touch the vector entries       in case the vector layout is unchanged from before, otherwise entries       are set to zero.  Note that this behavior might change between       releases without notification.             This function has a third argument, <tt>allow_different_maps</tt>,       that allows for an exchange of data between two equal-sized vectors       (but being distributed differently among the processors). A trivial       application of this function is to generate a replication of a whole       vector on each machine, when the calling vector is built with a map       consisting of all indices on each process, and <tt>v</tt>       is a distributed vector. In this case, the variable       <tt>omit_zeroing_entries</tt> needs to be set to <tt>false</tt>,       since it does not make sense to exchange data between differently       parallelized vectors without touching the elements.      
* [0.x.34]*
       Reinit functionality. This function destroys the old vector content       and generates a new one based on the input partitioning.  The flag       <tt>omit_zeroing_entries</tt> determines whether the vector should be       filled with zero (false). If the flag is set to <tt>true</tt>, the       vector entries are in an unspecified state and the user has to set       all elements. In the current implementation, this method still sets       the entries to zero, but this might change between releases without       notification.             Depending on whether the  [2.x.32]  argument uniquely       subdivides elements among processors or not, the resulting vector may       or may not have ghost elements. See the general documentation of this       class for more information.             In case  [2.x.33]  is overlapping, it is not clear which       process should own which elements. Hence, locally_owned_elements()       returns an empty IndexSet in this case.              [2.x.34]         [2.x.35]  "vectors with ghost elements"      
* [0.x.35]*
       Reinit functionality. This function destroys the old vector content       and generates a new one based on the input partitioning. In addition       to just specifying one index set as in all the other methods above,       this method allows to supply an additional set of ghost entries.       There are two different versions of a vector that can be created. If       the flag  [2.x.36]  is set to  [2.x.37]  the vector only       allows read access to the joint set of  [2.x.38]  and        [2.x.39]  The effect of the reinit method is then equivalent       to calling the other reinit method with an index set containing both       the locally owned entries and the ghost entries.             If the flag  [2.x.40]  is set to true, this creates an       alternative storage scheme for ghost elements that allows multiple       threads to write into the vector (for the other reinit methods, only       one thread is allowed to write into the ghost entries at a time).             Depending on whether the  [2.x.41]  argument uniquely       subdivides elements among processors or not, the resulting vector may       or may not have ghost elements. See the general documentation of this       class for more information.              [2.x.42]         [2.x.43]  "vectors with ghost elements"      
* [0.x.36]*
       Create vector by merging components from a block vector.      
* [0.x.37]*
       Compress the underlying representation of the Trilinos object, i.e.       flush the buffers of the vector object if it has any. This function is       necessary after writing into a vector element-by-element and before       anything else can be done on it.             The (defaulted) argument can be used to specify the compress mode       ( [2.x.44] ) in case the vector has not       been written to since the last time this function was called. The       argument is ignored if the vector has been added or written to since       the last time compress() was called.             See        [2.x.45]  "Compressing distributed objects"       for more information.      
* [0.x.38]*
       Set all components of the vector to the given number  [2.x.46]  Simply       pass this down to the base class, but we still need to declare this       function to make the example given in the discussion about making the       constructor explicit work.       the constructor explicit work.             Since the semantics of assigning a scalar to a vector are not       immediately clear, this operator can only be used if you want       to set the entire vector to zero. This allows the intuitive notation       <tt>v=0</tt>.      
* [0.x.39]*
       Copy the given vector. Resize the present vector if necessary. In       this case, also the Epetra_Map that designs the parallel partitioning       is taken from the input vector.      
* [0.x.40]*
       Move the given vector. This operator replaces the present vector with        [2.x.47]  by efficiently swapping the internal data structures.      
* [0.x.41]*
       Another copy function. This one takes a deal.II vector and copies it       into a TrilinosWrapper vector. Note that since we do not provide any       Epetra_map that tells about the partitioning of the vector among the       MPI processes, the size of the TrilinosWrapper vector has to be the       same as the size of the input vector.      
* [0.x.42]*
       This reinit function is meant to be used for parallel calculations       where some non-local data has to be used. The typical situation where       one needs this function is the call of the        [2.x.48]  function (or of some derivatives)       in parallel. Since it is usually faster to retrieve the data in       advance, this function can be called before the assembly forks out to       the different processors. What this function does is the following:       It takes the information in the columns of the given matrix and looks       which data couples between the different processors. That data is       then queried from the input vector. Note that you should not write to       the resulting vector any more, since the some data can be stored       several times on different processors, leading to unpredictable       results. In particular, such a vector cannot be used for matrix-       vector products as for example done during the solution of linear       systems.      
* [0.x.43]*
       Imports all the elements present in the vector's IndexSet from the       input vector  [2.x.49]   [2.x.50]   [2.x.51]  is used to decide if       the elements in  [2.x.52]  should be added to the current vector or replace the       current elements.      
* [0.x.44]*
       Test for equality. This function assumes that the present vector and       the one to compare with have the same size already, since comparing       vectors of different sizes makes not much sense anyway.      
* [0.x.45]*
       Test for inequality. This function assumes that the present vector and       the one to compare with have the same size already, since comparing       vectors of different sizes makes not much sense anyway.      
* [0.x.46]*
       Return the global dimension of the vector.      
* [0.x.47]*
       Return the local dimension of the vector, i.e. the number of elements       stored on the present MPI process. For sequential vectors, this number       is the same as size(), but for parallel vectors it may be smaller.             To figure out which elements exactly are stored locally, use       local_range().             If the vector contains ghost elements, they are included in this       number.              [2.x.53]  This function is deprecated.      
* [0.x.48]*
       Return the local size of the vector, i.e., the number of indices       owned locally.      
* [0.x.49]*
       Return a pair of indices indicating which elements of this vector are       stored locally. The first number is the index of the first element       stored, the second the index of the one past the last one that is       stored locally. If this is a sequential vector, then the result will be       the pair  [2.x.54] , otherwise it will be a pair        [2.x.55]  and        [2.x.56]  is the first element of the vector stored on this       processor, corresponding to the half open interval  [2.x.57]             
*  [2.x.58]  The description above is true most of the time, but not always.       In particular, Trilinos vectors need not store contiguous ranges of       elements such as  [2.x.59] . Rather, it can store vectors where the       elements are distributed in an arbitrary way across all processors and       each processor simply stores a particular subset, not necessarily       contiguous. In this case, this function clearly makes no sense since it       could, at best, return a range that includes all elements that are       stored locally. Thus, the function only succeeds if the locally stored       range is indeed contiguous. It will trigger an assertion if the local       portion of the vector is not contiguous.      
* [0.x.50]*
       Return whether  [2.x.60]  is in the local range or not, see also       local_range().            
*  [2.x.61]  The same limitation for the applicability of this function       applies as listed in the documentation of local_range().      
* [0.x.51]*
       Return an index set that describes which elements of this vector are       owned by the current processor. Note that this index set does not       include elements this vector may store locally as ghost elements but       that are in fact owned by another processor. As a consequence, the       index sets returned on different processors if this is a distributed       vector will form disjoint sets that add up to the complete index set.       Obviously, if a vector is created on only one processor, then the       result would satisfy      
* [1.x.7]
*       
* [0.x.52]*
       Return if the vector contains ghost elements. This answer is true if       there are ghost elements on at least one process.              [2.x.62]         [2.x.63]  "vectors with ghost elements"      
* [0.x.53]*
       This function only exists for compatibility with the  [2.x.64]         [2.x.65]  class and does nothing: this class       implements ghost value updates in a different way that is a better fit       with the underlying Trilinos vector object.      
* [0.x.54]*
       Return the scalar (inner) product of two vectors. The vectors must have       the same size.      
* [0.x.55]*
       Return the square of the  [2.x.66] -norm.      
* [0.x.56]*
       Mean value of the elements of this vector.      
* [0.x.57]*
       Compute the minimal value of the elements of this vector.      
* [0.x.58]*
       Compute the maximal value of the elements of this vector.      
* [0.x.59]*
        [2.x.67] -norm of the vector.  The sum of the absolute values.      
* [0.x.60]*
        [2.x.68] -norm of the vector.  The square root of the sum of the squares of       the elements.      
* [0.x.61]*
        [2.x.69] -norm of the vector. The [1.x.8]th root of the sum of the       [1.x.9]th powers of the absolute values of the elements.      
* [0.x.62]*
       Maximum absolute value of the elements.      
* [0.x.63]*
       Performs a combined operation of a vector addition and a subsequent       inner product, returning the value of the inner product. In other       words, the result of this function is the same as if the user called      
* [1.x.10]
*              The reason this function exists is for compatibility with deal.II's own       vector classes which can implement this functionality with less memory       transfer. However, for Trilinos vectors such a combined operation is       not natively supported and thus the cost is completely equivalent as       calling the two methods separately.             For complex-valued vectors, the scalar product in the second step is       implemented as        [2.x.70] .      
* [0.x.64]*
       Return whether the vector contains only elements with value zero. This       is a collective operation. This function is expensive, because       potentially all elements have to be checked.      
* [0.x.65]*
       Return  [2.x.71]  if the vector has no negative entries, i.e. all entries       are zero or positive. This function is used, for example, to check       whether refinement indicators are really all positive (or zero).      
* [0.x.66]*
        [2.x.72]  2: Data-Access      
* [0.x.67]*
       Provide access to a given element, both read and write.             When using a vector distributed with MPI, this operation only makes       sense for elements that are actually present on the calling processor.       Otherwise, an exception is thrown.      
* [0.x.68]*
       Provide read-only access to an element.             When using a vector distributed with MPI, this operation only makes       sense for elements that are actually present on the calling processor.       Otherwise, an exception is thrown.      
* [0.x.69]*
       Provide access to a given element, both read and write.             Exactly the same as operator().      
* [0.x.70]*
       Provide read-only access to an element.             Exactly the same as operator().      
* [0.x.71]*
       Instead of getting individual elements of a vector via operator(),       this function allows getting a whole set of elements at once. The       indices of the elements to be read are stated in the first argument,       the corresponding values are returned in the second.             If the current vector is called  [2.x.73]  then this function is the equivalent       to the code      
* [1.x.11]
*               [2.x.74]  The sizes of the  [2.x.75]  and  [2.x.76]  arrays must be identical.      
* [0.x.72]*
       Instead of getting individual elements of a vector via operator(),       this function allows getting a whole set of elements at once. In       contrast to the previous function, this function obtains the       indices of the elements by dereferencing all elements of the iterator       range provided by the first two arguments, and puts the vector       values into memory locations obtained by dereferencing a range       of iterators starting at the location pointed to by the third       argument.             If the current vector is called  [2.x.77]  then this function is the equivalent       to the code      
* [1.x.12]
*               [2.x.78]  It must be possible to write into as many memory locations         starting at  [2.x.79]  as there are iterators between          [2.x.80]  and  [2.x.81]       
* [0.x.73]*
       Make the Vector class a bit like the <tt>vector<></tt> class of the C++       standard library by returning iterators to the start and end of the       locally owned elements of this vector. The ordering of local elements       corresponds to the one given by the global indices in case the vector       is constructed from an IndexSet or other methods in deal.II (note that       an Epetra_Map can contain elements in arbitrary orders, though).             It holds that end()
* 
*  - begin() == local_size().      
* [0.x.74]*
       Return constant iterator to the start of the locally owned elements of       the vector.      
* [0.x.75]*
       Return an iterator pointing to the element past the end of the array of       locally owned entries.      
* [0.x.76]*
       Return a constant iterator pointing to the element past the end of the       array of the locally owned entries.      
* [0.x.77]*
        [2.x.82]  3: Modification of vectors      
* [0.x.78]*
       A collective set operation: instead of setting individual elements of a       vector, this function allows to set a whole set of elements at once.       The indices of the elements to be set are stated in the first argument,       the corresponding values in the second.      
* [0.x.79]*
       This is a second collective set operation. As a difference, this       function takes a deal.II vector of values.      
* [0.x.80]*
       This collective set operation is of lower level and can handle anything       else &mdash; the only thing you have to provide is an address where all       the indices are stored and the number of elements to be set.      
* [0.x.81]*
       A collective add operation: This function adds a whole set of values       stored in  [2.x.83]  to the vector components specified by  [2.x.84]       
* [0.x.82]*
       This is a second collective add operation. As a difference, this       function takes a deal.II vector of values.      
* [0.x.83]*
       Take an address where <tt>n_elements</tt> are stored contiguously and       add them into the vector. Handles all cases which are not covered by       the other two <tt>add()</tt> functions above.      
* [0.x.84]*
       Multiply the entire vector by a fixed factor.      
* [0.x.85]*
       Divide the entire vector by a fixed factor.      
* [0.x.86]*
       Add the given vector to the present one.      
* [0.x.87]*
       Subtract the given vector from the present one.      
* [0.x.88]*
       Addition of  [2.x.85]  to all components. Note that  [2.x.86]  is a scalar and not       a vector.      
* [0.x.89]*
       Simple vector addition, equal to the <tt>operator+=</tt>.             Though, if the second argument <tt>allow_different_maps</tt> is set,       then it is possible to add data from a vector that uses a different       map, i.e., a vector whose elements are split across processors       differently. This may include vectors with ghost elements, for example.       In general, however, adding vectors with a different element-to-       processor map requires communicating data among processors and,       consequently, is a slower operation than when using vectors using the       same map.      
* [0.x.90]*
       Simple addition of a multiple of a vector, i.e. <tt>*this += a*V</tt>.      
* [0.x.91]*
       Multiple addition of scaled vectors, i.e. <tt>*this += a*V + b*W</tt>.      
* [0.x.92]*
       Scaling and simple vector addition, i.e.  <tt>*this = s*(*this) +       V</tt>.      
* [0.x.93]*
       Scaling and simple addition, i.e.  <tt>*this = s*(*this) + a*V</tt>.      
* [0.x.94]*
       Scale each element of this vector by the corresponding element in the       argument. This function is mostly meant to simulate multiplication (and       immediate re-assignment) by a diagonal scaling matrix.      
* [0.x.95]*
       Assignment <tt>*this = a*V</tt>.      
* [0.x.96]*
        [2.x.87]  4: Mixed stuff      
* [0.x.97]*
       Return a const reference to the underlying Trilinos Epetra_MultiVector       class.      
* [0.x.98]*
       Return a (modifiable) reference to the underlying Trilinos       Epetra_FEVector class.      
* [0.x.99]*
       Return a const reference to the underlying Trilinos Epetra_BlockMap       that sets the parallel partitioning of the vector.      
* [0.x.100]*
       Print to a stream.  [2.x.88]  denotes the desired precision with       which values shall be printed,  [2.x.89]  whether scientific       notation shall be used. If  [2.x.90]  is  [2.x.91]  then the vector is       printed in a line, while if  [2.x.92]  then the elements are printed on a       separate line each.      
* [0.x.101]*
       Swap the contents of this vector and the other vector  [2.x.93]  One could       do this operation with a temporary variable and copying over the data       elements, but this function is significantly more efficient since it       only swaps the pointers to the data of the two vectors and therefore       does not need to allocate temporary storage and move data around. Note       that the vectors need to be of the same size and base on the same map.             This function is analogous to the  [2.x.94]  function of all C++       standard containers. Also, there is a global function       <tt>swap(u,v)</tt> that simply calls <tt>u.swap(v)</tt>, again in       analogy to standard functions.      
* [0.x.102]*
       Estimate for the memory consumption in bytes.      
* [0.x.103]*
       Return a reference to the MPI communicator object in use with this       object.      
* [0.x.104]*
       Exception      
* [0.x.105]*
       Exception      
* [0.x.106]*
       Exception      
* [0.x.107]*
       Trilinos doesn't allow to mix additions to matrix entries and       overwriting them (to make synchronization of parallel computations       simpler). The way we do it is to, for each access operation, store       whether it is an insertion or an addition. If the previous one was of       different type, then we first have to flush the Trilinos buffers;       otherwise, we can simply go on.  Luckily, Trilinos has an object for       this which does already all the parallel communications in such a case,       so we simply use their model, which stores whether the last operation       was an addition or an insertion.      
* [0.x.108]*
       A boolean variable to hold information on whether the vector is       compressed or not.      
* [0.x.109]*
       Whether this vector has ghost elements. This is true on all processors       even if only one of them has any ghost elements.      
* [0.x.110]*
       Pointer to the actual Epetra vector object. This may represent a vector       that is in fact distributed among multiple processors. The object       requires an existing Epetra_Map for storing data when setting it up.      
* [0.x.111]*
       A vector object in Trilinos to be used for collecting the non-local       elements if the vector was constructed with an additional IndexSet       describing ghost elements.      
* [0.x.112]*
       An IndexSet storing the indices this vector owns exclusively.      
* [0.x.113]*
     Global function  [2.x.95]  which overloads the default implementation of     the C++ standard library which uses a temporary object. The function     simply exchanges the data of the two vectors.          [2.x.96]   [2.x.97]     
* [0.x.114]*
     A helper class used internally in linear_operator.h. Specialization for      [2.x.98]     
* [0.x.115]*
 Declare  [2.x.99]  as distributed vector.

* 
* [0.x.116]