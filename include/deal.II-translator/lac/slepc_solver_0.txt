[0.x.0]*
 Base namespace for solver classes using the SLEPc solvers which are selected based on flags passed to the eigenvalue problem solver context. Derived classes set the right flags to set the right solver.
*  The SLEPc solvers are intended to be used for solving the generalized eigenspectrum problem  [2.x.0] , for  [2.x.1] ; where  [2.x.2]  is a system matrix,  [2.x.3]  is a mass matrix, and  [2.x.4]  are a set of eigenvalues and eigenvectors respectively. The emphasis is on methods and techniques appropriate for problems in which the associated matrices are sparse. Most of the methods offered by the SLEPc library are projection methods or other methods with similar properties; and wrappers are provided to interface to SLEPc solvers that handle both of these problem sets.
*  SLEPcWrappers can be implemented in application codes in the following way:

* 
* [1.x.0]
*  for the generalized eigenvalue problem  [2.x.5] , where the variable  [2.x.6]  tells SLEPc the number of eigenvector/eigenvalue pairs to solve for. Additional options and solver parameters can be passed to the SLEPc solvers before calling  [2.x.7] . For example, if the matrices of the general eigenspectrum problem are not hermitian and the lower eigenvalues are wanted only, the following code can be implemented before calling  [2.x.8] :

* 
* [1.x.1]
*  These options can also be set at the command line.
*  See also  [2.x.9]  for a hands-on example.
*  For cases when spectral transformations are used in conjunction with Krylov-type solvers or Davidson-type eigensolvers are employed one can additionally specify which linear solver and preconditioner to use. This can be achieved as follows

* 
* [1.x.2]
* 
*  In order to support this usage case, different from PETSc wrappers, the classes in this namespace are written in such a way that the underlying SLEPc objects are initialized in constructors. By doing so one also avoid caching of different settings (such as target eigenvalue or type of the problem); instead those are applied straight away when the corresponding functions of the wrapper classes are called.
*  An alternative implementation to the one above is to use the API internals directly within the application code. In this way the calling sequence requires calling several of SolverBase functions rather than just one. This freedom is intended for use of the SLEPcWrappers that require a greater handle on the eigenvalue problem solver context. See also the API of, for example:

* 
* [1.x.3]
*  as an example on how to do this.
*  For further information and explanations on handling the  [2.x.10]  "SLEPcWrappers", see also the  [2.x.11]  "PETScWrappers", on which they depend.
* 

* 
*  [2.x.12] 

* 
* [0.x.1]*
   Base class for solver classes using the SLEPc solvers. Since solvers in   SLEPc are selected based on flags passed to a generic solver object,   basically all the actual solver calls happen in this class, and derived   classes simply set the right flags to select one solver or another, or to   set certain parameters for individual solvers.     For examples of how this and its derived classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.  
* [0.x.2]*
     Constructor. Takes the MPI communicator over which parallel     computations are to happen.    
* [0.x.3]*
     Destructor.    
* [0.x.4]*
     Composite method that solves the eigensystem  [2.x.13] . The     eigenvector sent in has to have at least one element that we can use as     a template when resizing, since we do not know the parameters of the     specific vector class used (i.e. local_dofs for MPI vectors). However,     while copying eigenvectors, at least twice the memory size of     <tt>eigenvectors</tt> is being used (and can be more). To avoid doing     this, the fairly standard calling sequence executed here is used: Set     up matrices for solving; Actually solve the system; Gather the     solution(s).        
*  [2.x.14]  Note that the number of converged eigenvectors can be larger than     the number of eigenvectors requested; this is due to a round off error     (success) of the eigenproblem solver context. If this is found to be     the case we simply do not bother with more eigenpairs than requested,     but handle that it may be more than specified by ignoring any extras.     By default one eigenvector/eigenvalue pair is computed.    
* [0.x.5]*
     Same as above, but here a composite method for solving the system  [2.x.15] , for real matrices, vectors, and values  [2.x.16] .    
* [0.x.6]*
     Same as above, but here a composite method for solving the system  [2.x.17]  with real matrices  [2.x.18]  and imaginary eigenpairs  [2.x.19] .    
* [0.x.7]*
     Set the initial vector space for the solver.         By default, SLEPc initializes the starting vector or the initial     subspace randomly.    
* [0.x.8]*
     Set the spectral transformation to be used.    
* [0.x.9]*
     Set target eigenvalues in the spectrum to be computed. By default, no     target is set.    
* [0.x.10]*
     Indicate which part of the spectrum is to be computed. By default     largest magnitude eigenvalues are computed.        
*  [2.x.20]  For other allowed values see the SLEPc documentation.    
* [0.x.11]*
     Specify the type of the eigenspectrum problem. This can be used to     exploit known symmetries of the matrices that make up the     standard/generalized eigenspectrum problem.  By default a non-Hermitian     problem is assumed.        
*  [2.x.21]  For other allowed values see the SLEPc documentation.    
* [0.x.12]*
     Take the information provided from SLEPc and checks it against     deal.II's own SolverControl objects to see if convergence has been     reached.    
* [0.x.13]*
     Exception. Standard exception.    
* [0.x.14]*
     Exception. SLEPc error with error number.    
* [0.x.15]*
     Exception. Convergence failure on the number of eigenvectors.    
* [0.x.16]*
     Access to the object that controls convergence.    
* [0.x.17]*
     Reference to the object that controls convergence of the iterative     solver.    
* [0.x.18]*
     Copy of the MPI communicator object to be used for the solver.    
* [0.x.19]*
     Solve the linear system for  [2.x.22]  eigenstates.     Parameter  [2.x.23]  contains the actual number of     eigenstates that have  converged; this can be both fewer or more than     n_eigenpairs, depending on the SLEPc eigensolver used.    
* [0.x.20]*
     Access the real parts of solutions for a solved eigenvector problem,     pair index solutions,  [2.x.24] .    
* [0.x.21]*
     Access the real and imaginary parts of solutions for a solved     eigenvector problem, pair index solutions,  [2.x.25] .    
* [0.x.22]*
     Initialize solver for the linear system  [2.x.26] . (Note: this is     required before calling solve ())    
* [0.x.23]*
     Same as above, but here initialize solver for the linear system  [2.x.27] .    
* [0.x.24]*
     Objects for Eigenvalue Problem Solver.    
* [0.x.25]*
     Convergence reason.    
* [0.x.26]*
     A function that can be used in SLEPc as a callback to check on     convergence.        
*  [2.x.28]  This function is not used currently.    
* [0.x.27]*
   An implementation of the solver interface using the SLEPc Krylov-Schur   solver. Usage: All spectrum, all problem types, complex.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.29]   
* [0.x.28]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.29]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.30]*
     Store a copy of the flags for this particular solver.    
* [0.x.31]*
   An implementation of the solver interface using the SLEPc Arnoldi solver.   Usage: All spectrum, all problem types, complex.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.30]   
* [0.x.32]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.33]*
       Constructor. By default, set the option of delayed       reorthogonalization to false, i.e. don't do it.      
* [0.x.34]*
       Flag for delayed reorthogonalization.      
* [0.x.35]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.36]*
     Store a copy of the flags for this particular solver.    
* [0.x.37]*
   An implementation of the solver interface using the SLEPc Lanczos solver.   Usage: All spectrum, all problem types, complex.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.31]   
* [0.x.38]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.39]*
       The type of reorthogonalization used during the Lanczos iteration.      
* [0.x.40]*
       Constructor. By default sets the type of reorthogonalization used       during the Lanczos iteration to full.      
* [0.x.41]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.42]*
     Store a copy of the flags for this particular solver.    
* [0.x.43]*
   An implementation of the solver interface using the SLEPc Power solver.   Usage: Largest values of spectrum only, all problem types, complex.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.32]   
* [0.x.44]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.45]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.46]*
     Store a copy of the flags for this particular solver.    
* [0.x.47]*
   An implementation of the solver interface using the SLEPc Davidson   solver. Usage: All problem types.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.33]   
* [0.x.48]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.49]*
       Use double expansion in search subspace.      
* [0.x.50]*
       Constructor. By default set double_expansion to false.      
* [0.x.51]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.52]*
     Store a copy of the flags for this particular solver.    
* [0.x.53]*
   An implementation of the solver interface using the SLEPc Jacobi-Davidson   solver. Usage: All problem types.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.34]   
* [0.x.54]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.55]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.56]*
     Store a copy of the flags for this particular solver.    
* [0.x.57]*
   An implementation of the solver interface using the SLEPc LAPACK direct   solver.     For examples of how this and its sibling classes can be used, including   how to provide preconditioners to the matrix of which eigenvalues are   to be computed, see the documentation of the SolverBase class as well   as the extensive discussions in the documentation of the SLEPcWrappers   namespace.    
*  [2.x.35]   
* [0.x.58]*
     Standardized data struct to pipe additional data to the solver, should     it be needed.    
* [0.x.59]*
     SLEPc solvers will want to have an MPI communicator context over which     computations are parallelized. By default, this carries the same     behavior as the PETScWrappers, but you can change that.    
* [0.x.60]*
     Store a copy of the flags for this particular solver.    
* [0.x.61]*
   This is declared here to make it possible to take a  [2.x.36]  of   different PETScWrappers vector types  
* [0.x.62]