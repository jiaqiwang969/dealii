[0.x.0]*
   Implementation of a sequential sparse matrix class based on PETSc. All   the functionality is actually in the base class, except for the calls to   generate a sequential sparse matrix. This is possible since PETSc only   works on an abstract matrix type and internally distributes to functions   that do the actual work depending on the actual matrix type (much like   using virtual functions). Only the functions creating a matrix of   specific type differ, and are implemented in this particular class.    
*  [2.x.0]   
*  [2.x.1]   
* [0.x.1]*
     A structure that describes some of the traits of this class in terms of     its run-time behavior. Some other classes (such as the block matrix     classes) that take one or other of the matrix classes as its template     parameters can tune their behavior based on the variables in this     class.    
* [0.x.2]*
       It is safe to elide additions of zeros to individual elements of this       matrix.      
* [0.x.3]*
     Default constructor. Create an empty matrix.    
* [0.x.4]*
     Create a sparse matrix of dimensions  [2.x.2]  times  [2.x.3]  with an initial     guess of  [2.x.4]  nonzero elements per row. PETSc is able     to cope with the situation that more than this number of elements is     later allocated for a row, but this involves copying data, and is thus     expensive.         The  [2.x.5]  flag determines whether we should tell PETSc that     the matrix is going to be symmetric (as indicated by the call     <tt>MatSetOption(mat, MAT_SYMMETRIC)</tt>. Note that the PETSc     documentation states that one cannot form an ILU decomposition of a     matrix for which this flag has been set to  [2.x.6]  only an ICC. The     default value of this flag is  [2.x.7]     
* [0.x.5]*
     Initialize a rectangular matrix with  [2.x.8]  rows and  [2.x.9]  columns.  The     maximal number of nonzero entries for each row separately is given by     the  [2.x.10]  array.         Just as for the other constructors: PETSc is able to cope with the     situation that more than this number of elements is later allocated for     a row, but this involves copying data, and is thus expensive.         The  [2.x.11]  flag determines whether we should tell PETSc that     the matrix is going to be symmetric (as indicated by the call     <tt>MatSetOption(mat, MAT_SYMMETRIC)</tt>. Note that the PETSc     documentation states that one cannot form an ILU decomposition of a     matrix for which this flag has been set to  [2.x.12]  only an ICC. The     default value of this flag is  [2.x.13]     
* [0.x.6]*
     Initialize a sparse matrix using the given sparsity pattern.         Note that PETSc can be very slow if you do not provide it with a good     estimate of the lengths of rows. Using the present function is a very     efficient way to do this, as it uses the exact number of nonzero     entries for each row of the matrix by using the given sparsity pattern     argument. If the  [2.x.14]  flag is  [2.x.15]  this     function in addition not only sets the correct row sizes up front, but     also pre-allocated the correct nonzero entries in the matrix.         PETsc allows to later add additional nonzero entries to a matrix, by     simply writing to these elements. However, this will then lead to     additional memory allocations which are very inefficient and will     greatly slow down your program. It is therefore significantly more     efficient to get memory allocation right from the start.    
* [0.x.7]*
     This operator assigns a scalar to a matrix. Since this does usually not     make much sense (should we set all matrix entries to this value? Only     the nonzero entries of the sparsity pattern?), this operation is only     allowed if the actual value to be assigned is zero. This operator only     exists to allow for the obvious notation <tt>matrix=0</tt>, which sets     all elements of the matrix to zero, but keep the sparsity pattern     previously used.    
* [0.x.8]*
     The copy constructor is deleted.    
* [0.x.9]*
     The copy assignment operator is deleted.    
* [0.x.10]*
     Throw away the present matrix and generate one that has the same     properties as if it were created by the constructor of this class with     the same argument list as the present function.    
* [0.x.11]*
     Throw away the present matrix and generate one that has the same     properties as if it were created by the constructor of this class with     the same argument list as the present function.    
* [0.x.12]*
     Initialize a sparse matrix using the given sparsity pattern.         Note that PETSc can be very slow if you do not provide it with a good     estimate of the lengths of rows. Using the present function is a very     efficient way to do this, as it uses the exact number of nonzero     entries for each row of the matrix by using the given sparsity pattern     argument. If the  [2.x.16]  flag is  [2.x.17]  this     function in addition not only sets the correct row sizes up front, but     also pre-allocated the correct nonzero entries in the matrix.         PETsc allows to later add additional nonzero entries to a matrix, by     simply writing to these elements. However, this will then lead to     additional memory allocations which are very inefficient and will     greatly slow down your program. It is therefore significantly more     efficient to get memory allocation right from the start.         Despite the fact that it would seem to be an obvious win, setting the      [2.x.18]  flag to  [2.x.19]  doesn't seem to accelerate     program. Rather on the contrary, it seems to be able to slow down     entire programs somewhat. This is surprising, since we can use     efficient function calls into PETSc that allow to create multiple     entries at once; nevertheless, given the fact that it is inefficient,     the respective flag has a default value equal to  [2.x.20]     
* [0.x.13]*
     Return a reference to the MPI communicator object in use with this     matrix. Since this is a sequential matrix, it returns the MPI_COMM_SELF     communicator.    
* [0.x.14]*
     Return the number of rows of this matrix.    
* [0.x.15]*
     Return the number of columns of this matrix.    
* [0.x.16]*
     Perform the matrix-matrix multiplication  [2.x.21] , or,      [2.x.22]  given a compatible vector  [2.x.23] .         This function calls  [2.x.24]  to do the actual work.    
* [0.x.17]*
     Perform the matrix-matrix multiplication with the transpose of     <tt>this</tt>, i.e.,  [2.x.25] , or,      [2.x.26]  given a compatible vector  [2.x.27] .         This function calls  [2.x.28]  to do the actual work.    
* [0.x.18]*
     Do the actual work for the respective reinit() function and the     matching constructor, i.e. create a matrix. Getting rid of the previous     matrix is left to the caller.    
* [0.x.19]*
     Same as previous function.    
* [0.x.20]*
     Same as previous function.    
* [0.x.21]*
     Implementation of a parallel sparse matrix class based on PETSc, with     rows of the matrix distributed across an MPI network. All the     functionality is actually in the base class, except for the calls to     generate a parallel sparse matrix. This is possible since PETSc only     works on an abstract matrix type and internally distributes to     functions that do the actual work depending on the actual matrix type     (much like using virtual functions). Only the functions creating a     matrix of specific type differ, and are implemented in this particular     class.         There are a number of comments on the communication model as well as     access to individual elements in the documentation to the parallel     vector class. These comments apply here as well.             [1.x.0]         PETSc partitions parallel matrices so that each MPI process "owns" a     certain number of rows (i.e. only this process stores the respective     entries in these rows). The number of rows each process owns has to be     passed to the constructors and reinit() functions via the argument  [2.x.29]      local_rows. The individual values passed as  [2.x.30]  on all the     MPI processes of course have to add up to the global number of rows of     the matrix.         In addition to this, PETSc also partitions the rectangular chunk of the     matrix it owns (i.e. the  [2.x.31]  times n() elements in the     matrix), so that matrix vector multiplications can be performed     efficiently. This column-partitioning therefore has to match the     partitioning of the vectors with which the matrix is multiplied, just     as the row-partitioning has to match the partitioning of destination     vectors. This partitioning is passed to the constructors and reinit()     functions through the  [2.x.32]  variable, which again has to add     up to the global number of columns in the matrix. The name  [2.x.33]      local_columns may be named inappropriately since it does not reflect     that only these columns are stored locally, but it reflects the fact     that these are the columns for which the elements of incoming vectors     are stored locally.         To make things even more complicated, PETSc needs a very good estimate     of the number of elements to be stored in each row to be efficient.     Otherwise it spends most of the time with allocating small chunks of     memory, a process that can slow down programs to a crawl if it happens     to often. As if a good estimate of the number of entries per row isn't     even, it even needs to split this as follows: for each row it owns, it     needs an estimate for the number of elements in this row that fall into     the columns that are set apart for this process (see above), and the     number of elements that are in the rest of the columns.         Since in general this information is not readily available, most of the     initializing functions of this class assume that all of the number of     elements you give as an argument to  [2.x.34]  or by  [2.x.35]      row_lengths fall into the columns "owned" by this process, and none     into the other ones. This is a fair guess for most of the rows, since     in a good domain partitioning, nodes only interact with nodes that are     within the same subdomain. It does not hold for nodes on the interfaces     of subdomain, however, and for the rows corresponding to these nodes,     PETSc will have to allocate additional memory, a costly process.         The only way to avoid this is to tell PETSc where the actual entries of     the matrix will be. For this, there are constructors and reinit()     functions of this class that take a DynamicSparsityPattern object     containing all this information. While in the general case it is     sufficient if the constructors and reinit() functions know the number     of local rows and columns, the functions getting a sparsity pattern     also need to know the number of local rows ( [2.x.36]      and columns ( [2.x.37]  for all other processes, in     order to compute which parts of the matrix are which. Thus, it is not     sufficient to just count the number of degrees of freedom that belong     to a particular process, but you have to have the numbers for all     processes available at all processes.        
*  [2.x.38]     
*  [2.x.39]     
* [0.x.22]*
       Declare type for container size.      
* [0.x.23]*
       A structure that describes some of the traits of this class in terms       of its run-time behavior. Some other classes (such as the block       matrix classes) that take one or other of the matrix classes as its       template parameters can tune their behavior based on the variables in       this class.      
* [0.x.24]*
         It is not safe to elide additions of zeros to individual elements         of this matrix. The reason is that additions to the matrix may         trigger collective operations synchronizing buffers on multiple         processes. If an addition is elided on one process, this may lead         to other processes hanging in an infinite waiting loop.        
* [0.x.25]*
       Default constructor. Create an empty matrix.      
* [0.x.26]*
       Destructor to free the PETSc object.      
* [0.x.27]*
       Initialize using the given sparsity pattern with communication       happening over the provided  [2.x.40]              For the meaning of the  [2.x.41]  and  [2.x.42]        local_columns_per_process parameters, see the class documentation.             Note that PETSc can be very slow if you do not provide it with a good       estimate of the lengths of rows. Using the present function is a very       efficient way to do this, as it uses the exact number of nonzero       entries for each row of the matrix by using the given sparsity       pattern argument. If the  [2.x.43]  flag is  [2.x.44]        this function in addition not only sets the correct row sizes up       front, but also pre-allocated the correct nonzero entries in the       matrix.             PETsc allows to later add additional nonzero entries to a matrix, by       simply writing to these elements. However, this will then lead to       additional memory allocations which are very inefficient and will       greatly slow down your program. It is therefore significantly more       efficient to get memory allocation right from the start.      
* [0.x.28]*
       This operator assigns a scalar to a matrix. Since this does usually       not make much sense (should we set all matrix entries to this value?       Only the nonzero entries of the sparsity pattern?), this operation is       only allowed if the actual value to be assigned is zero. This       operator only exists to allow for the obvious notation       <tt>matrix=0</tt>, which sets all elements of the matrix to zero, but       keep the sparsity pattern previously used.      
* [0.x.29]*
       Make a copy of the PETSc matrix  [2.x.45]  It is assumed that both       matrices have the same SparsityPattern.      
* [0.x.30]*
       Initialize using the given sparsity pattern with communication       happening over the provided  [2.x.46]              Note that PETSc can be very slow if you do not provide it with a good       estimate of the lengths of rows. Using the present function is a very       efficient way to do this, as it uses the exact number of nonzero       entries for each row of the matrix by using the given sparsity       pattern argument. If the  [2.x.47]  flag is  [2.x.48]        this function in addition not only sets the correct row sizes up       front, but also pre-allocated the correct nonzero entries in the       matrix.             PETsc allows to later add additional nonzero entries to a matrix, by       simply writing to these elements. However, this will then lead to       additional memory allocations which are very inefficient and will       greatly slow down your program. It is therefore significantly more       efficient to get memory allocation right from the start.      
* [0.x.31]*
       Create a matrix where the size() of the IndexSets determine the       global number of rows and columns and the entries of the IndexSet       give the rows and columns for the calling processor. Note that only       ascending, 1:1 IndexSets are supported.      
* [0.x.32]*
       Initialize this matrix to have the same structure as  [2.x.49]  This       will not copy the values of the other matrix, but you can use       copy_from() for this.      
* [0.x.33]*
       Return a reference to the MPI communicator object in use with this       matrix.      
* [0.x.34]*
        [2.x.50]  Exceptions        [2.x.51]       
* [0.x.35]*
       Exception      
* [0.x.36]*
       Return the square of the norm of the vector  [2.x.52]  with respect to the       norm induced by this matrix, i.e.  [2.x.53] . This is       useful, e.g. in the finite element context, where the  [2.x.54]  norm of a       function equals the matrix norm with respect to the mass matrix of       the vector representing the nodal values of the finite element       function.             Obviously, the matrix needs to be quadratic for this operation.             The implementation of this function is not as efficient as the one in       the  [2.x.55]  class used in deal.II (i.e. the original one, not       the PETSc wrapper class) since PETSc doesn't support this operation       and needs a temporary vector.      
* [0.x.37]*
       Compute the matrix scalar product  [2.x.56] .             The implementation of this function is not as efficient as the one in       the  [2.x.57]  class used in deal.II (i.e. the original one, not       the PETSc wrapper class) since PETSc doesn't support this operation       and needs a temporary vector.      
* [0.x.38]*
       Return the partitioning of the domain space of this matrix, i.e., the       partitioning of the vectors this matrix has to be multiplied with.      
* [0.x.39]*
       Return the partitioning of the range space of this matrix, i.e., the       partitioning of the vectors that result from matrix-vector       products.      
* [0.x.40]*
       Perform the matrix-matrix multiplication  [2.x.58] , or,        [2.x.59]  given a compatible vector  [2.x.60] .             This function calls  [2.x.61]  to do the actual work.      
* [0.x.41]*
       Perform the matrix-matrix multiplication with the transpose of       <tt>this</tt>, i.e.,  [2.x.62] , or,        [2.x.63]  given a compatible vector  [2.x.64] .             This function calls  [2.x.65]  to do the actual work.      
* [0.x.42]*
       Copy of the communicator object to be used for this parallel vector.      
* [0.x.43]*
       Same as previous functions.      
* [0.x.44]*
       Same as previous functions.      
* [0.x.45]