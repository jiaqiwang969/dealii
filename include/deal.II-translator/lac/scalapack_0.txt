[0.x.0]*
 A wrapper class around ScaLAPACK parallel dense linear algebra.
*  ScaLAPACK assumes that matrices are distributed according to the block-cyclic decomposition scheme. An  [2.x.0]  by  [2.x.1]  matrix is first decomposed into  [2.x.2]  by  [2.x.3]  blocks which are then uniformly distributed across the 2D process grid with  [2.x.4]  processes, where  [2.x.5]  are grid dimensions and  [2.x.6]  is the total number of processes. The parameters MB and NB are referred to as row and column block size and determine the granularity of the block-cyclic distribution.
*  In the following the block-cyclic distribution of a  [2.x.7]  matrix onto a  [2.x.8]  Cartesian process grid with block sizes  [2.x.9]  is displayed.
*  \htmlonly <style>div.image img[src="scalapack_block_cycling.png"]{width:35%;}</style> \endhtmlonly  [2.x.10] 
*  Note that the odd number of columns of the local matrices owned by the processes P2, P5 and P8 accounts for  [2.x.11]  not being an integral multiple of  [2.x.12] .
*  The choice of the block sizes is a compromise between a sufficiently large size for efficient local/serial BLAS, but one that is also small enough to achieve good parallel load balance.
*  Below we show a strong scaling example of  [2.x.13]  on up to 5 nodes each composed of two Intel Xeon 2660v2 IvyBridge sockets 2.20GHz, 10 cores/socket. Calculations are performed on square processor grids 1x1, 2x2, 3x3, 4x4, 5x5, 6x6, 7x7, 8x8, 9x9, 10x10.
*   [2.x.14] 
* 

* 
*  [2.x.15] 

* 
* [0.x.1]*
   Declare the type for container size.  
* [0.x.2]*
   Constructor for a rectangular matrix with  [2.x.16]  and  [2.x.17]    and distributed using the grid  [2.x.18]      The parameters  [2.x.19]  and  [2.x.20]  are the block sizes used   for the block-cyclic distribution of the matrix.   In general, it is recommended to use powers of  [2.x.21] , e.g.  [2.x.22] .  
* [0.x.3]*
   Constructor for a square matrix of size  [2.x.23]  and distributed   using the process grid in  [2.x.24]      The parameter  [2.x.25]  is used for the block-cyclic distribution of the matrix.   An identical block size is used for the rows and columns of the matrix.   In general, it is recommended to use powers of  [2.x.26] , e.g.  [2.x.27] .  
* [0.x.4]*
   Constructor for a general rectangular matrix that is read from   the file  [2.x.28]  and distributed using the grid  [2.x.29]      Loads the matrix from file  [2.x.30]  using HDF5.   In case that deal.II was built without HDF5   a call to this function will cause an exception to be thrown.     The parameters  [2.x.31]  and  [2.x.32]  are the block sizes used   for the block-cyclic distribution of the matrix.   In general, it is recommended to use powers of  [2.x.33] , e.g.  [2.x.34] .  
* [0.x.5]*
   Destructor  
* [0.x.6]*
   Initialize the rectangular matrix with  [2.x.35]  and  [2.x.36]    and distributed using the grid  [2.x.37]      The parameters  [2.x.38]  and  [2.x.39]  are the block sizes used   for the block-cyclic distribution of the matrix.   In general, it is recommended to use powers of  [2.x.40] , e.g.  [2.x.41] .  
* [0.x.7]*
   Initialize the square matrix of size  [2.x.42]  and distributed using the grid  [2.x.43]      The parameter  [2.x.44]  is used for the block-cyclic distribution of the matrix.   An identical block size is used for the rows and columns of the matrix.   In general, it is recommended to use powers of  [2.x.45] , e.g.  [2.x.46] .  
* [0.x.8]*
   Assign  [2.x.47]  to this matrix.  
* [0.x.9]*
   Return current  [2.x.48]  of this matrix  
* [0.x.10]*
   Return current  [2.x.49]  of this matrix  
* [0.x.11]*
   Assignment operator from a regular FullMatrix.    
*  [2.x.50]  This function should only be used for relatively small matrix   dimensions. It is primarily intended for debugging purposes.  
* [0.x.12]*
   Copies the content of the locally owned  [2.x.51]  to the distributed matrix.   The distributed matrix and  [2.x.52]  on process  [2.x.53]  must have matching dimensions.     For all processes except the process with rank  [2.x.54]  the serial  [2.x.55]  is not referenced.   The user has to ensure that all processes call this with identical  [2.x.56]    The  [2.x.57]  refers to a process of the MPI communicator used to create the process grid   of the distributed matrix.  
* [0.x.13]*
   Copy the contents of the distributed matrix into  [2.x.58]     
*  [2.x.59]  This function should only be used for relatively small matrix   dimensions. It is primarily intended for debugging purposes.  
* [0.x.14]*
   Copies the content of the distributed matrix into the locally replicated  [2.x.60]    on the process with rank  [2.x.61]  For all processes except  [2.x.62]   [2.x.63]  is not referenced.   The distributed matrix and  [2.x.64]  on the process  [2.x.65]  must have matching dimensions.     The user has to ensure that all processes call this with identical  [2.x.66]    The  [2.x.67]  refers to a process of the MPI communicator used to create the process grid   of the distributed matrix.  
* [0.x.15]*
   Copy the contents of the distributed matrix into a differently distributed matrix  [2.x.68]    The function also works for matrices with different process grids   or block-cyclic distributions.  
* [0.x.16]*
   Copy a submatrix (subset) of the distributed matrix A to a submatrix of the distributed matrix  [2.x.69] 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - The global row and column index of the first element of the submatrix A is provided by  [2.x.70]      with row index= [2.x.71]  and column   index= [2.x.72] .
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - The global row and column index of the first element of the submatrix B is provided by  [2.x.73]      with row index= [2.x.74]  and column   index= [2.x.75] .
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - The dimension of the submatrix to be copied is given by  [2.x.76]      with number of rows= [2.x.77]  and number of   columns= [2.x.78] .       If it is necessary to copy complete matrices with an identical block-cyclic   distribution, use    [2.x.79]  &dest)   with only one argument to avoid communication.     The underlying process grids of the matrices  [2.x.80]  and  [2.x.81]  must have been built   with the same MPI communicator.  
* [0.x.17]*
   Transposing assignment:  [2.x.82]      The matrices  [2.x.83]  and  [2.x.84]  must have the same process grid.     The following alignment conditions have to be fulfilled:  [2.x.85]  and    [2.x.86] .  
* [0.x.18]*
   The operations based on the input parameter  [2.x.87]  and the   alignment conditions are summarized in the following table:     | transpose_B |          Block Sizes         |                    Operation                  |   | :---------: | :--------------------------: | :-------------------------------------------: |   |   false     |  [2.x.88]   [2.x.89]   [2.x.90]  |   [2.x.91]    |   |   true      |  [2.x.92]   [2.x.93]   [2.x.94]  |  [2.x.95]   |     The matrices  [2.x.96]  and  [2.x.97]  must have the same process grid.  
* [0.x.19]*
   Matrix-addition:    [2.x.98]      The matrices  [2.x.99]  and  [2.x.100]  must have the same process grid.     The following alignment conditions have to be fulfilled:  [2.x.101]  and    [2.x.102] .  
* [0.x.20]*
   Matrix-addition:    [2.x.103]      The matrices  [2.x.104]  and  [2.x.105]  must have the same process grid.     The following alignment conditions have to be fulfilled:  [2.x.106]  and    [2.x.107] .  
* [0.x.21]*
   Matrix-matrix-multiplication:     The operations based on the input parameters and the alignment conditions   are summarized in the following table:     | transpose_A | transpose_B |                  Block Sizes                  |                             Operation                           |   | :---------: | :---------: | :-------------------------------------------: | :-------------------------------------------------------------: |   | false       |   false     |  [2.x.108]   [2.x.109]   [2.x.110]   [2.x.111]   [2.x.112]  |    [2.x.113]    |   | false       |   true      |  [2.x.114]   [2.x.115]   [2.x.116]   [2.x.117]   [2.x.118]  |   [2.x.119]   |   | true        |   false     |  [2.x.120]   [2.x.121]   [2.x.122]   [2.x.123]   [2.x.124]  |  [2.x.125]    |   | true        |   true      |  [2.x.126]   [2.x.127]   [2.x.128]   [2.x.129]   [2.x.130]  |  [2.x.131]  |     It is assumed that  [2.x.132]  and  [2.x.133]  have compatible sizes and   that    [2.x.134]  already has the right size.     The matrices  [2.x.135] ,  [2.x.136]  and  [2.x.137]  must have the same   process grid.  
* [0.x.22]*
   Matrix-matrix-multiplication.     The optional parameter  [2.x.138]  determines whether the result is   stored in  [2.x.139]  or added to  [2.x.140] .     if ( [2.x.141]   [2.x.142]      else  [2.x.143]      It is assumed that  [2.x.144]  and  [2.x.145]  have compatible sizes and   that    [2.x.146]  already has the right size.     The following alignment conditions have to be fulfilled:  [2.x.147] ,    [2.x.148]  and  [2.x.149] .  
* [0.x.23]*
   Matrix-matrix-multiplication using transpose of  [2.x.150] .     The optional parameter  [2.x.151]  determines whether the result is   stored in  [2.x.152]  or added to  [2.x.153] .     if ( [2.x.154]   [2.x.155]      else  [2.x.156]      It is assumed that  [2.x.157]  and  [2.x.158]  have compatible sizes and   that    [2.x.159]  already has the right size.     The following alignment conditions have to be fulfilled:  [2.x.160] ,    [2.x.161]  and  [2.x.162] .  
* [0.x.24]*
   Matrix-matrix-multiplication using the transpose of  [2.x.163] .     The optional parameter  [2.x.164]  determines whether the result is   stored in  [2.x.165]  or added to  [2.x.166] .     if ( [2.x.167]   [2.x.168]      else  [2.x.169]      It is assumed that  [2.x.170]  and  [2.x.171]  have compatible sizes and   that    [2.x.172]  already has the right size.     The following alignment conditions have to be fulfilled:  [2.x.173] ,    [2.x.174]  and  [2.x.175] .  
* [0.x.25]*
   Matrix-matrix-multiplication using transpose of  [2.x.176]  and    [2.x.177] .     The optional parameter  [2.x.178]  determines whether the result is   stored in  [2.x.179]  or added to  [2.x.180] .     if ( [2.x.181]   [2.x.182]      else  [2.x.183]      It is assumed that  [2.x.184]  and  [2.x.185]  have compatible sizes and   that    [2.x.186]  already has the right size.     The following alignment conditions have to be fulfilled:  [2.x.187] ,    [2.x.188]  and  [2.x.189] .  
* [0.x.26]*
   Stores the distributed matrix in  [2.x.190]  using HDF5.     In case that deal.II was built without HDF5   a call to this function will cause an exception to be thrown.     If HDF5 was built with MPI, parallel I/O is used to save the matrix.   Otherwise, just one process will do the output. This means that   internally the distributed matrix is copied to one process, which   does the output. Therefore, the matrix has to fit into the memory   of one process.     To tweak the I/O performance, especially for parallel I/O, the user may define the optional parameter  [2.x.191]    All MPI processes need to call the function with the same value.   The matrix is written in chunks to the file, therefore the properties of   the system define the optimal chunk size. Internally, HDF5 splits the   matrix into <tt>chunk_size.first</tt> x <tt>chunk_size.second</tt> sized   blocks, with <tt>chunk_size.first</tt> being the number of rows of a chunk   and <tt>chunk_size.second</tt> the number of columns.  
* [0.x.27]*
   Loads the distributed matrix from file  [2.x.192]  using HDF5.   In case that deal.II was built without HDF5   a call to this function will cause an exception to be thrown.     The matrix must have the same dimensions as the matrix stored in the file.     If HDF5 was build with MPI, parallel I/O is used to load the matrix.   Otherwise, just one process will load the matrix from storage   and distribute the content to the other processes subsequently.  
* [0.x.28]*
   Compute the Cholesky factorization of the matrix using ScaLAPACK   function  [2.x.193] . The result of the factorization is stored in   this object.  
* [0.x.29]*
   Compute the LU factorization of the matrix using ScaLAPACK   function  [2.x.194]  and partial pivoting with row interchanges.   The result of the factorization is stored in this object.  
* [0.x.30]*
   Invert the matrix by first computing a Cholesky for symmetric matrices   or a LU factorization for general matrices and then   building the actual inverse using  [2.x.195]  or    [2.x.196] . If the matrix is triangular, the LU factorization   step is skipped, and  [2.x.197]  is used directly.     If a Cholesky or LU factorization has been applied previously,    [2.x.198]  are called directly.     The inverse is stored in this object.  
* [0.x.31]*
   Computing selected eigenvalues and, optionally, the eigenvectors of the   real symmetric matrix  [2.x.199] .     The eigenvalues/eigenvectors are selected by prescribing a range of indices  [2.x.200]      If successful, the computed eigenvalues are arranged in ascending order.   The eigenvectors are stored in the columns of the matrix, thereby   overwriting the original content of the matrix.     If all eigenvalues/eigenvectors have to be computed, pass the closed interval  [2.x.201]  in  [2.x.202]      Pass the closed interval  [2.x.203]  if the  [2.x.204]  largest   eigenvalues/eigenvectors are desired.  
* [0.x.32]*
   Computing selected eigenvalues and, optionally, the eigenvectors.   The eigenvalues/eigenvectors are selected by prescribing a range of values  [2.x.205]  for the eigenvalues.     If successful, the computed eigenvalues are arranged in ascending order.   The eigenvectors are stored in the columns of the matrix, thereby   overwriting the original content of the matrix.  
* [0.x.33]*
   Computing selected eigenvalues and, optionally, the eigenvectors of the   real symmetric matrix  [2.x.206]  using the   MRRR algorithm.     The eigenvalues/eigenvectors are selected by prescribing a range of indices  [2.x.207]      If successful, the computed eigenvalues are arranged in ascending order.   The eigenvectors are stored in the columns of the matrix, thereby   overwriting the original content of the matrix.     If all eigenvalues/eigenvectors have to be computed, pass the closed interval  [2.x.208]  in  [2.x.209]      Pass the closed interval  [2.x.210]  if the  [2.x.211]  largest   eigenvalues/eigenvectors are desired.  
* [0.x.34]*
   Computing selected eigenvalues and, optionally, the eigenvectors of the   real symmetric matrix  [2.x.212]  using the   MRRR algorithm.   The eigenvalues/eigenvectors are selected by prescribing a range of values  [2.x.213]  for the eigenvalues.     If successful, the computed eigenvalues are arranged in ascending order.   The eigenvectors are stored in the columns of the matrix, thereby   overwriting the original content of the matrix.  
* [0.x.35]*
   Computing the singular value decomposition (SVD) of a   matrix  [2.x.214] , optionally computing the   left and/or right singular vectors. The SVD is written as  [2.x.215]  with  [2.x.216]  as a diagonal matrix,    [2.x.217]  and  [2.x.218]  as orthogonal matrices. The diagonal elements of    [2.x.219]  are the singular values of  [2.x.220]  and the columns of    [2.x.221]  and  [2.x.222]  are the corresponding left and right singular   vectors, respectively. The singular values are returned in decreasing order   and only the first  [2.x.223]  columns of  [2.x.224]  and rows of    [2.x.225]  are computed.     Upon return the content of the matrix is unusable.   The matrix  [2.x.226]  must have identical block cyclic distribution for   the rows and column.     If left singular vectors are required matrices  [2.x.227]  and    [2.x.228]  have to be constructed with the same process grid and block   cyclic distribution. If right singular vectors are required matrices    [2.x.229]  and  [2.x.230]  have to be constructed with the same   process grid  and block cyclic distribution.     To avoid computing the left and/or right singular vectors the function   accepts  [2.x.231]    for  [2.x.232]  and/or  [2.x.233]   
* [0.x.36]*
   Solving overdetermined or underdetermined real linear   systems involving matrix  [2.x.234] , or its   transpose  [2.x.235] , using a QR or LQ factorization of  [2.x.236]    for  [2.x.237]  RHS vectors in the columns of matrix  [2.x.238]      It is assumed that  [2.x.239]  has full rank:  [2.x.240] .     The following options are supported:
* 

* 
* 

* 
* 

* 
* 
*  - If(!transpose) and  [2.x.241] : least squares solution of overdetermined   system       [2.x.242] .\n      Upon exit the rows  [2.x.243]  to  [2.x.244]  of  [2.x.245]  contain the least square   solution vectors. The residual sum of squares for each column is given by   the sum of squares of elements  [2.x.246]  to  [2.x.247]  in that column.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - If(!transpose) and  [2.x.248] : find minimum norm solutions of   underdetermined systems       [2.x.249] .\n      Upon exit the columns of  [2.x.250]  contain the minimum norm solution   vectors.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - If(transpose) and  [2.x.251] : find minimum norm solutions of   underdetermined system  [2.x.252] .\n      Upon exit the columns of  [2.x.253]  contain the minimum norm solution   vectors.
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 

* 
* 
*  - If(transpose) and  [2.x.254] : least squares solution of overdetermined   system       [2.x.255] .\n      Upon exit the rows  [2.x.256]  to  [2.x.257]  contain the least square solution   vectors. The residual sum of squares for each column is given by the sum of   squares of elements  [2.x.258]  to  [2.x.259]  in that column.     If(!tranpose) then  [2.x.260] ,   otherwise  [2.x.261] .   The matrices  [2.x.262]  and  [2.x.263]  must have an identical block   cyclic distribution for rows and columns.  
* [0.x.37]*
   Compute the pseudoinverse  [2.x.264]    (Moore-Penrose inverse) of a real matrix  [2.x.265]  using the singular value decomposition    [2.x.266] .     Unlike the inverse, the pseudoinverse  [2.x.267]  exists for both rectangular as well   as singular matrices  [2.x.268] .     For a rectangular  [2.x.269]  the pseudoinverse is computed by taking   the reciprocal of each non-zero element on the diagonal, leaving the zeros   in place, and then transposing  [2.x.270] . For the numerical   computation only the singular values  [2.x.271]  are taken into account. Upon successful exit, the function   returns the number of singular values fulfilling that condition. That value   can be interpreted as the rank of  [2.x.272] .     Upon return this object contains the pseudoinverse  [2.x.273] .     The following alignment conditions have to be fulfilled:  [2.x.274] .  
* [0.x.38]*
   Estimate the condition number of a SPD matrix in the  [2.x.275] -norm.   The matrix has to be in the Cholesky state (see   compute_cholesky_factorization()). The reciprocal of the condition number   is returned in order to avoid the possibility of overflow when the   condition number is very large.      [2.x.276]  must contain the  [2.x.277] -norm of the matrix prior to calling   Cholesky factorization (see l1_norm()).    
*  [2.x.278]  An alternative is to compute the inverse of the matrix   explicitly and manually construct  [2.x.279] .  
* [0.x.39]*
   Compute the  [2.x.280] -norm of the matrix.  
* [0.x.40]*
   Compute the  [2.x.281]  norm of the matrix.  
* [0.x.41]*
   Compute the Frobenius norm of the matrix.  
* [0.x.42]*
   Number of rows of the  [2.x.282]  matrix.  
* [0.x.43]*
   Number of columns of the  [2.x.283]  matrix.  
* [0.x.44]*
   Number of local rows on this MPI processes.  
* [0.x.45]*
   Number of local columns on this MPI process.  
* [0.x.46]*
   Return the global row number for the given local row  [2.x.284]  .  
* [0.x.47]*
   Return the global column number for the given local column  [2.x.285]   
* [0.x.48]*
   Read access to local element.  
* [0.x.49]*
   Write access to local element.  
* [0.x.50]*
   Scale the columns of the distributed matrix by the scalars provided in the array  [2.x.286]      The array  [2.x.287]  must have as many entries as the matrix columns.     Copies of  [2.x.288]  have to be available on all processes of the underlying MPI communicator.    
*  [2.x.289]  The fundamental prerequisite for the  [2.x.290]  is that it must be possible to   create an ArrayView from it; this is satisfied by the  [2.x.291]  and Vector classes.  
* [0.x.51]*
   Scale the rows of the distributed matrix by the scalars provided in the array  [2.x.292]      The array  [2.x.293]  must have as many entries as the matrix rows.     Copies of  [2.x.294]  have to be available on all processes of the underlying MPI communicator.    
*  [2.x.295]  The fundamental prerequisite for the  [2.x.296]  is that it must be possible to   create an ArrayView from it; this is satisfied by the  [2.x.297]  and Vector classes.  
* [0.x.52]*
   Calculate the norm of a distributed symmetric dense matrix using   ScaLAPACK's internal function.  
* [0.x.53]*
   Calculate the norm of a distributed dense matrix using ScaLAPACK's   internal function.  
* [0.x.54]*
   Computing selected eigenvalues and, optionally, the eigenvectors.   The eigenvalues/eigenvectors are selected by either prescribing a range of indices  [2.x.298]    or a range of values  [2.x.299]  for the eigenvalues. The function will throw an exception   if both ranges are prescribed (meaning that both ranges differ from the   default value) as this ambiguity is prohibited. If successful, the computed   eigenvalues are arranged in ascending order. The eigenvectors are stored in   the columns of the matrix, thereby overwriting the original content of the   matrix.  
* [0.x.55]*
   Computing selected eigenvalues and, optionally, the eigenvectors of the   real symmetric matrix  [2.x.300]  using the   MRRR algorithm.   The eigenvalues/eigenvectors are selected by either prescribing a range of indices  [2.x.301]    or a range of values  [2.x.302]  for the eigenvalues. The function will throw an exception   if both ranges are prescribed (meaning that both ranges differ from the   default value) as this ambiguity is prohibited.     By calling this function the original content of the matrix will be   overwritten. If requested, the eigenvectors are stored in the columns of   the matrix. Also in the case that just the eigenvalues are required, the   content of the matrix will be overwritten.     If successful, the computed eigenvalues are arranged in ascending order.    
*  [2.x.303]  Due to a bug in Netlib-ScaLAPACK, either all or no eigenvectors can be computed.   Therefore, the input  [2.x.304]  has to be set accordingly. Using Intel-MKL this restriction is not required.  
* [0.x.56]   Stores the distributed matrix in  [2.x.305]    using serial routines  
* [0.x.57]   Loads the distributed matrix from file  [2.x.306]    using serial routines  
* [0.x.58]   Stores the distributed matrix in  [2.x.307]    using parallel routines  
* [0.x.59]   Loads the distributed matrix from file  [2.x.308]    using parallel routines  
* [0.x.60]*
   Since ScaLAPACK operations notoriously change the meaning of the matrix   entries, we record the current state after the last operation here.  
* [0.x.61]*
   Additional property of the matrix which may help to select more   efficient ScaLAPACK functions.  
* [0.x.62]*
   A shared pointer to a  [2.x.309]  object which contains a   BLACS context and a MPI communicator, as well as other necessary data   structures.  
* [0.x.63]*
   Number of rows in the matrix.  
* [0.x.64]*
   Number of columns in the matrix.  
* [0.x.65]*
   Row block size.  
* [0.x.66]*
   Column block size.  
* [0.x.67]*
   Number of rows in the matrix owned by the current process.  
* [0.x.68]*
   Number of columns in the matrix owned by the current process.  
* [0.x.69]*
   ScaLAPACK description vector.  
* [0.x.70]*
   Workspace array.  
* [0.x.71]*
   Integer workspace array.  
* [0.x.72]*
   Integer array holding pivoting information required   by ScaLAPACK's matrix factorization routines.  
* [0.x.73]*
   A character to define where elements are stored in case   ScaLAPACK operations support this.  
* [0.x.74]*
   The process row of the process grid over which the first row   of the global matrix is distributed.  
* [0.x.75]*
   The process column of the process grid over which the first column   of the global matrix is distributed.  
* [0.x.76]*
   Global row index that determines where to start a submatrix.   Currently this equals unity, as we don't use submatrices.  
* [0.x.77]*
   Global column index that determines where to start a submatrix.   Currently this equals unity, as we don't use submatrices.  
* [0.x.78]*
   Thread mutex.  
* [0.x.79]