[0.x.0]*
 This class collects all the data that is stored for the matrix free implementation. The storage scheme is tailored towards several loops performed with the same data, i.e., typically doing many matrix-vector products or residual computations on the same mesh. The class is used in  [2.x.0]  and  [2.x.1] .
*  This class does not implement any operations involving finite element basis functions, i.e., regarding the operation performed on the cells. For these operations, the class FEEvaluation is designed to use the data collected in this class.
*  The stored data can be subdivided into three main components:
* 

* 
* 
*  - DoFInfo: It stores how local degrees of freedom relate to global degrees of freedom. It includes a description of constraints that are evaluated as going through all local degrees of freedom on a cell.
* 

* 
* 
*  - MappingInfo: It stores the transformations from real to unit cells that are necessary in order to build derivatives of finite element functions and find location of quadrature weights in physical space.
* 

* 
* 
*  - ShapeInfo: It contains the shape functions of the finite element, evaluated on the unit cell.
*  Besides the initialization routines, this class implements only a single operation, namely a loop over all cells (cell_loop()). This loop is scheduled in such a way that cells that share degrees of freedom are not worked on simultaneously, which implies that it is possible to write to vectors (or matrices) in parallel without having to explicitly synchronize access to these vectors and matrices. This class does not implement any shape values, all it does is to cache the respective data. To implement finite element operations, use the class FEEvaluation (or some of the related classes).
*  This class traverses the cells in a different order than the usual Triangulation class in deal.II, in order to be flexible with respect to parallelization in shared memory and vectorization.
*  Vectorization is implemented by merging several topological cells into one so-called macro cell. This enables the application of all cell-related operations for several cells with one CPU instruction and is one of the main features of this framework.
*  For details on usage of this class, see the description of FEEvaluation or the  [2.x.2]  "matrix-free module".
* 

* 
*  [2.x.3] 

* 
* [0.x.1]*
   An alias for the underlying number type specified by the template   argument.  
* [0.x.2]*
   The dimension set by the template argument `dim`.  
* [0.x.3]*
   Collects the options for initialization of the MatrixFree class. The   first parameter specifies the MPI communicator to be used, the second the   parallelization options in shared memory (task-based parallelism, where   one can choose between no parallelism and three schemes that avoid that   cells with access to the same vector entries are accessed   simultaneously), the third with the block size for task parallel   scheduling, the fourth the update flags that should be stored by this   class.     The fifth parameter specifies the level in the triangulation from which   the indices are to be used. If the level is set to    [2.x.4]  the active cells are traversed, and   otherwise the cells in the given level. This option has no effect in case   a DoFHandler is given.     The parameter  [2.x.5]  indicates whether the DoFInfo   class should also allow for access to vectors without resolving   constraints.     The two parameters `initialize_indices` and `initialize_mapping` allow   the user to disable some of the initialization processes. For example, if   only the scheduling that avoids touching the same vector/matrix indices   simultaneously is to be found, the mapping needs not be   initialized. Likewise, if the mapping has changed from one iteration to   the next but the topology has not (like when using a deforming mesh with   MappingQEulerian), it suffices to initialize the mapping only.     The two parameters `cell_vectorization_categories` and   `cell_vectorization_categories_strict` control the formation of batches   for vectorization over several cells. It is used implicitly when working   with hp-adaptivity but can also be useful in other contexts, such as in   local time stepping where one would like to control which elements   together form a batch of cells. The array `cell_vectorization_categories`   is accessed by the number given by cell->active_cell_index() when working   on the active cells with `mg_level` set to  [2.x.6]    and by cell->index() for the level cells. By default, the different   categories in `cell_vectorization_category` can be mixed and the algorithm   is allowed to merge lower category numbers with the next higher categories   if it is necessary inside the algorithm, in order to avoid partially   filled SIMD lanes as much as possible. This gives a better utilization of   the vectorization but might need special treatment, in particular for   face integrals. If set to  [2.x.7]  the algorithm will instead keep   different categories separate and not mix them in a single vectorized   array.  
* [0.x.4]*
     Collects options for task parallelism. See the documentation of the     member variable  [2.x.8]  for a     thorough description.    
* [0.x.5]*
       Perform application in serial.      
* [0.x.6]*
       Partition the cells into two levels and afterwards form chunks.      
* [0.x.7]*
       Partition on the global level and color cells within the partitions.      
* [0.x.8]*
       Use the traditional coloring algorithm: this is like        [2.x.9]  but only uses one partition.      
* [0.x.9]*
     Constructor for AdditionalData.    
* [0.x.10]*
     Copy constructor.    
* [0.x.11]*
     Copy assignment.    
* [0.x.12]*
     Set the scheme for task parallelism. There are four options available.     If set to  [2.x.10]  the operator application is done in serial without     shared memory parallelism. If this class is used together with MPI and     MPI is also used for parallelism within the nodes, this flag should be     set to  [2.x.11]  The default value is  [2.x.12]  i.e. we     actually use multithreading with the first option described below.         The first option  [2.x.13]  is to partition the cells on     two levels in onion-skin-like partitions and forming chunks of     tasks_block_size after the partitioning. The partitioning finds sets of     independent cells that enable working in parallel without accessing the     same vector entries at the same time.         The second option  [2.x.14]  is to use a partition on the     global level and color cells within the partitions (where all chunks     within a color are independent). Here, the subdivision into chunks of     cells is done before the partitioning, which might give worse     partitions but better cache performance if degrees of freedom are not     renumbered.         The third option  [2.x.15]  is to use a traditional algorithm of coloring     on the global level. This scheme is a special case of the second option     where only one partition is present. Note that for problems with     hanging nodes, there are quite many colors (50 or more in 3D), which     might degrade parallel performance (bad cache behavior, many     synchronization points).        
*  [2.x.16]  Threading support is currently experimental for the case inner     face integrals are performed and it is recommended to use MPI     parallelism if possible. While the scheme has been verified to work     with the `partition_partition` option in case of usual DG elements, no     comprehensive tests have been performed for systems of more general     elements, like combinations of continuous and discontinuous elements     that add face integrals to all terms.    
* [0.x.13]*
     Set the number of so-called macro cells that should form one     partition. If zero size is given, the class tries to find a good size     for the blocks based on  [2.x.17]  and the number of     cells present. Otherwise, the given number is used. If the given number     is larger than one third of the number of total cells, this means no     parallelism. Note that in the case vectorization is used, a macro cell     consists of more than one physical cell.    
* [0.x.14]*
     This flag determines the mapping data on cells that is cached. This     class can cache data needed for gradient computations (inverse     Jacobians), Jacobian determinants (JxW), quadrature points as well as     data for Hessians (derivative of Jacobians). By default, only data for     gradients and Jacobian determinants times quadrature weights, JxW, are     cached. If quadrature points or second derivatives are needed, they     must be specified by this field (even though second derivatives might     still be evaluated on Cartesian cells without this option set here,     since there the Jacobian describes the mapping completely).    
* [0.x.15]*
     This flag determines the mapping data on boundary faces to be     cached. Note that MatrixFree uses a separate loop layout for face     integrals in order to effectively vectorize also in the case of hanging     nodes (which require different subface settings on the two sides) or     some cells in the batch of a VectorizedArray of cells that are adjacent     to the boundary and others that are not.         If set to a value different from update_general (default), the face     information is explicitly built. Currently, MatrixFree supports to     cache the following data on faces: inverse Jacobians, Jacobian     determinants (JxW), quadrature points, data for Hessians (derivative of     Jacobians), and normal vectors.        
*  [2.x.18]  In order to be able to perform a `face_operation` or     `boundary_operation` in the  [2.x.19]  either this field or      [2.x.20]  must be set to a value different     from  [2.x.21]     
* [0.x.16]*
     This flag determines the mapping data on interior faces to be     cached. Note that MatrixFree uses a separate loop layout for face     integrals in order to effectively vectorize also in the case of hanging     nodes (which require different subface settings on the two sides) or     some cells in the batch of a VectorizedArray of cells that are adjacent     to the boundary and others that are not.         If set to a value different from update_general (default), the face     information is explicitly built. Currently, MatrixFree supports to     cache the following data on faces: inverse Jacobians, Jacobian     determinants (JxW), quadrature points, data for Hessians (derivative of     Jacobians), and normal vectors.        
*  [2.x.22]  In order to be able to perform a `face_operation` or     `boundary_operation` in the  [2.x.23]  either this field or      [2.x.24]  must be set to a value different     from  [2.x.25]     
* [0.x.17]*
     This flag determines the mapping data for faces in a different layout     with respect to vectorizations. Whereas     `mapping_update_flags_inner_faces` and     `mapping_update_flags_boundary_faces` trigger building the data in a     face-centric way with proper vectorization, the current data field     attaches the face information to the cells and their way of     vectorization. This is only needed in special situations, as for     example for block-Jacobi methods where the full operator to a cell     including its faces are evaluated. This data is accessed by      [2.x.26]      face_number)</code>. However, currently no coupling terms to neighbors     can be computed with this approach because the neighbors are not laid     out by the VectorizedArray data layout with an     array-of-struct-of-array-type data structures.         Note that you should only compute this data field in case you really     need it as it more than doubles the memory required by the mapping data     on faces.         If set to a value different from update_general (default), the face     information is explicitly built. Currently, MatrixFree supports to     cache the following data on faces: inverse Jacobians, Jacobian     determinants (JxW), quadrature points, data for Hessians (derivative of     Jacobians), and normal vectors.    
* [0.x.18]*
     This option can be used to define whether we work on a certain level of     the mesh, and not the active cells. If set to invalid_unsigned_int     (which is the default value), the active cells are gone through,     otherwise the level given by this parameter. Note that if you specify     to work on a level, its dofs must be distributed by using      [2.x.27] .    
* [0.x.19]*
     Controls whether to enable reading from vectors without resolving     constraints, i.e., just read the local values of the vector. By     default, this option is enabled. In case you want to use      [2.x.28]  this flag needs to be set.    
* [0.x.20]*
     Option to control whether the indices stored in the DoFHandler     should be read and the pattern for task parallelism should be     set up in the initialize method of MatrixFree. The default     value is true. Can be disabled in case the mapping should be     recomputed (e.g. when using a deforming mesh described through     MappingEulerian) but the topology of cells has remained the     same.    
* [0.x.21]*
     Option to control whether the mapping information should be     computed in the initialize method of MatrixFree. The default     value is true. Can be disabled when only some indices should be     set up (e.g. when only a set of independent cells should be     computed).    
* [0.x.22]*
     Option to control whether the loops should overlap communications and     computations as far as possible in case the vectors passed to the loops     support non-blocking data exchange. In most situations, overlapping is     faster in case the amount of data to be sent is more than a few     kilobytes. If less data is sent, the communication is latency bound on     most clusters (point-to-point latency is around 1 microsecond on good     clusters by 2016 standards). Depending on the MPI implementation and     the fabric, it may be faster to not overlap and wait for the data to     arrive. The default is true, i.e., communication and computation are     overlapped.    
* [0.x.23]*
     By default, the face part will only hold those faces (and ghost     elements behind faces) that are going to be processed locally. In case     MatrixFree should have access to all neighbors on locally owned cells,     this option enables adding the respective faces at the end of the face     range.    
* [0.x.24]*
     This data structure allows to assign a fraction of cells to different     categories when building the information for vectorization. It is used     implicitly when working with hp-adaptivity but can also be useful in     other contexts, such as in local time stepping where one would like to     control which elements together form a batch of cells.         This array is accessed by the number given by cell->active_cell_index()     when working on the active cells with  [2.x.29]  set to  [2.x.30]  and     by cell->index() for the level cells.        
*  [2.x.31]  This field is empty upon construction of AdditionalData. It is     the responsibility of the user to resize this field to     `triangulation.n_active_cells()` or `triangulation.n_cells(level)` when     filling data.    
* [0.x.25]*
     By default, the different categories in  [2.x.32]      can be mixed and the algorithm is allowed to merge lower categories with     the next higher categories if it is necessary inside the algorithm. This     gives a better utilization of the vectorization but might need special     treatment, in particular for face integrals. If set to  [2.x.33]  the     algorithm will instead keep different categories separate and not mix     them in a single vectorized array.    
* [0.x.26]*
     Shared-memory MPI communicator. Default: MPI_COMM_SELF.    
* [0.x.27]*
    [2.x.34]  1: Construction and initialization  
* [0.x.28]*
   Default empty constructor. Does nothing.  
* [0.x.29]*
   Copy constructor, calls copy_from  
* [0.x.30]*
   Destructor.  
* [0.x.31]*
   Extracts the information needed to perform loops over cells. The   DoFHandler and AffineConstraints objects describe the layout of degrees   of freedom, the DoFHandler and the mapping describe the   transformations from unit to real cell, and the finite element   underlying the DoFHandler together with the quadrature formula   describe the local operations. Note that the finite element underlying   the DoFHandler must either be scalar or contain several copies of the   same element. Mixing several different elements into one FESystem is   not allowed. In that case, use the initialization function with   several DoFHandler arguments.  
* [0.x.32]*
   Initializes the data structures. Same as above, but using a  [2.x.35]    mapping.      [2.x.36]  Use the overload taking a Mapping object instead.  
* [0.x.33]*
   Extracts the information needed to perform loops over cells. The   DoFHandler and AffineConstraints objects describe the layout of degrees of   freedom, the DoFHandler and the mapping describe the transformations from   unit to real cell, and the finite element underlying the DoFHandler   together with the quadrature formula describe the local operations. As   opposed to the scalar case treated with the other initialization   functions, this function allows for problems with two or more different   finite elements. The DoFHandlers to each element must be passed as   pointers to the initialization function. Alternatively, a system of   several components may also be represented by a single DoFHandler with an   FESystem element. The prerequisite for this case is that each base   element of the FESystem must be compatible with the present class, such   as the FE_Q or FE_DGQ classes.     This function also allows for using several quadrature formulas, e.g.   when the description contains independent integrations of elements of   different degrees. However, the number of different quadrature formulas   can be sets independently from the number of DoFHandlers, when several   elements are always integrated with the same quadrature formula.  
* [0.x.34]*
   Initializes the data structures. Same as above, but  using DoFHandlerType.      [2.x.37]  Use the overload taking a DoFHandler object instead.  
* [0.x.35]*
   Initializes the data structures. Same as above, but  using a  [2.x.38]    mapping.      [2.x.39]  Use the overload taking a Mapping object instead.  
* [0.x.36]*
   Initializes the data structures. Same as above, but  using DoFHandlerType.      [2.x.40]  Use the overload taking a DoFHandler object instead.  
* [0.x.37]*
   Initializes the data structures. Same as before, but now the index set   description of the locally owned range of degrees of freedom is taken   from the DoFHandler. Moreover, only a single quadrature formula is used,   as might be necessary when several components in a vector-valued problem   are integrated together based on the same quadrature formula.  
* [0.x.38]*
   Initializes the data structures. Same as above, but  using DoFHandlerType.      [2.x.41]  Use the overload taking a DoFHandler object instead.  
* [0.x.39]*
   Initializes the data structures. Same as above, but  using a  [2.x.42]    mapping.      [2.x.43]  Use the overload taking a Mapping object instead.  
* [0.x.40]*
   Initializes the data structures. Same as above, but  using DoFHandlerType.      [2.x.44]  Use the overload taking a DoFHandler object instead.  
* [0.x.41]*
   Copy function. Creates a deep copy of all data structures. It is usually   enough to keep the data for different operations once, so this function   should not be needed very often.  
* [0.x.42]*
   Refreshes the geometry data stored in the MappingInfo fields when the   underlying geometry has changed (e.g. by a mapping that can deform   through a change in the spatial configuration like MappingFEField)   whereas the topology of the mesh and unknowns have remained the   same. Compared to reinit(), this operation only has to re-generate the   geometry arrays and can thus be significantly cheaper (depending on the   cost to evaluate the geometry).  
* [0.x.43]*
   Same as above but with  [2.x.45]   
* [0.x.44]*
   Clear all data fields and brings the class into a condition similar to   after having called the default constructor.  
* [0.x.45]*
   This class defines the type of data access for face integrals in loop ()   that is passed on to the `update_ghost_values` and `compress` functions   of the parallel vectors, with the purpose of being able to reduce the   amount of data that must be exchanged. The data exchange is a real   bottleneck in particular for high-degree DG methods, therefore a more   restrictive way of exchange is clearly beneficial. Note that this   selection applies to FEFaceEvaluation objects assigned to the exterior   side of cells accessing  [2.x.46]  only; all   [1.x.0] objects are available in any case.  
* [0.x.46]*
     The loop does not involve any FEFaceEvaluation access into neighbors,     as is the case with only boundary integrals (but no interior face     integrals) or when doing mass matrices in a  [2.x.47]      like setup.    
* [0.x.47]*
     The loop does only involve FEFaceEvaluation access into neighbors by     function values, such as  [2.x.48]  with     argument  [2.x.49]  but no access to shape function     derivatives (which typically need to access more data). For FiniteElement     types where only some of the shape functions have support on a face, such     as an FE_DGQ element with Lagrange polynomials with nodes on the element     surface, the data exchange is reduced from `(k+1)^dim` to     `(k+1)^(dim-1)`.    
* [0.x.48]*
     Same as above. To be used if data has to be accessed from exterior faces     if FEFaceEvaluation was reinitialized by providing the cell batch number     and a face number. This configuration is useful in the context of     cell-centric loops.          [2.x.50]   [2.x.51]  has to enabled.    
* [0.x.49]*
     The loop does involve FEFaceEvaluation access into neighbors by     function values and gradients, but no second derivatives, such as      [2.x.52]  with  [2.x.53]  and      [2.x.54]  set. For FiniteElement types where only some     of the shape functions have non-zero value and first derivative on a     face, such as an FE_DGQHermite element, the data exchange is reduced,     e.g. from `(k+1)^dim` to `2(k+1)^(dim-1)`. Note that for bases that do     not have this special property, the full neighboring data is sent anyway.    
* [0.x.50]*
     Same as above. To be used if data has to be accessed from exterior faces     if FEFaceEvaluation was reinitialized by providing the cell batch number     and a face number. This configuration is useful in the context of     cell-centric loops.          [2.x.55]   [2.x.56]  has to enabled.    
* [0.x.51]*
     General setup where the user does not want to make a restriction. This     is typically more expensive than the other options, but also the most     conservative one because the full data of elements behind the faces to     be computed locally will be exchanged.    
* [0.x.52]*
    [2.x.57]  2: Matrix-free loops  
* [0.x.53]*
   This method runs the loop over all cells (in parallel) and performs the   MPI data exchange on the source vector and destination vector.      [2.x.58]  cell_operation  [2.x.59]  with the signature <tt>cell_operation   (const MatrixFree<dim,Number> &, OutVector &, InVector &,    [2.x.60]  int,unsigned int> &)</tt> where the first argument   passes the data of the calling class and the last argument defines the   range of cells which should be worked on (typically more than one cell   should be worked on in order to reduce overheads).  One can pass a pointer   to an object in this place if it has an `operator()` with the correct set   of arguments since such a pointer can be converted to the function object.      [2.x.61]  dst Destination vector holding the result. If the vector is of   type  [2.x.62]  (or composite objects thereof   such as  [2.x.63]  the loop calls    [2.x.64]  at the end of the call   internally. For other vectors, including parallel Trilinos or PETSc   vectors, no such call is issued. Note that Trilinos/Epetra or PETSc   vectors do currently not work in parallel because the present class uses   MPI-local index addressing, as opposed to the global addressing implied   by those external libraries.      [2.x.65]  src Input vector. If the vector is of type    [2.x.66]  (or composite objects thereof such as    [2.x.67]  the loop calls    [2.x.68]  at the start of   the call internally to make sure all necessary data is locally   available. Note, however, that the vector is reset to its original state   at the end of the loop, i.e., if the vector was not ghosted upon entry of   the loop, it will not be ghosted upon finishing the loop.      [2.x.69]  zero_dst_vector If this flag is set to `true`, the vector `dst`   will be set to zero inside the loop. Use this case in case you perform a   typical `vmult()` operation on a matrix object, as it will typically be   faster than calling `dst = 0;` before the loop separately. This is   because the vector entries are set to zero only on subranges of the   vector, making sure that the vector entries stay in caches as much as   possible.  
* [0.x.54]*
   This is the second variant to run the loop over all cells, now providing   a function pointer to a member function of class `CLASS`. This method   obviates the need to define a lambda function or to call  [2.x.70]  to bind   the class into the given   function in case the local function needs to access data in the class   (i.e., it is a non-static member function).      [2.x.71]  cell_operation Pointer to member function of `CLASS` with the   signature <tt>cell_operation (const MatrixFree<dim,Number> &, OutVector &,   InVector &,  [2.x.72]  int,unsigned int> &)</tt> where the first   argument passes the data of the calling class and the last argument   defines the range of cells which should be worked on (typically more than   one cell should be worked on in order to reduce overheads).      [2.x.73]  owning_class The object which provides the `cell_operation`   call. To be compatible with this interface, the class must allow to call   `owning_class->cell_operation(...)`.      [2.x.74]  dst Destination vector holding the result. If the vector is of   type  [2.x.75]  (or composite objects thereof   such as  [2.x.76]  the loop calls    [2.x.77]  at the end of the call   internally. For other vectors, including parallel Trilinos or PETSc   vectors, no such call is issued. Note that Trilinos/Epetra or PETSc   vectors do currently not work in parallel because the present class uses   MPI-local index addressing, as opposed to the global addressing implied   by those external libraries.      [2.x.78]  src Input vector. If the vector is of type    [2.x.79]  (or composite objects thereof such as    [2.x.80]  the loop calls    [2.x.81]  at the start of   the call internally to make sure all necessary data is locally   available. Note, however, that the vector is reset to its original state   at the end of the loop, i.e., if the vector was not ghosted upon entry of   the loop, it will not be ghosted upon finishing the loop.      [2.x.82]  zero_dst_vector If this flag is set to `true`, the vector `dst`   will be set to zero inside the loop. Use this case in case you perform a   typical `vmult()` operation on a matrix object, as it will typically be   faster than calling `dst = 0;` before the loop separately. This is   because the vector entries are set to zero only on subranges of the   vector, making sure that the vector entries stay in caches as much as   possible.  
* [0.x.55]*
   Same as above, but for class member functions which are non-const.  
* [0.x.56]*
   This function is similar to the cell_loop with an  [2.x.83]  object to   specify to operation to be performed on cells, but adds two additional   functors to execute some additional work before and after the cell   integrals are computed.     The two additional functors work on a range of degrees of freedom,   expressed in terms of the degree-of-freedom numbering of the selected   DoFHandler `dof_handler_index_pre_post` in MPI-local indices. The   arguments to the functors represent a range of degrees of freedom at a   granularity of    [2.x.84]  entries   (except for the last chunk which is set to the number of locally owned   entries) in the form `[first, last)`. The idea of these functors is to   bring operations on vectors closer to the point where they accessed in a   matrix-free loop, with the goal to increase cache hits by temporal   locality. This loop guarantees that the `operation_before_loop` hits all   relevant unknowns before they are first touched in the cell_operation   (including the MPI data exchange), allowing to execute some vector update   that the `src` vector depends upon. The `operation_after_loop` is similar
* 

* 
* 

* 
* 

* 
* 
*  - it starts to execute on a range of DoFs once all DoFs in that range   have been touched for the last time by the `cell_operation`   (including the MPI data exchange), allowing e.g. to compute some vector   operations that depend on the result of the current cell loop in `dst` or   want to modify `src`. The efficiency of caching depends on the numbering   of the degrees of freedom because of the granularity of the ranges.      [2.x.85]  cell_operation Pointer to member function of `CLASS` with the   signature <tt>cell_operation (const MatrixFree<dim,Number> &, OutVector &,   InVector &,  [2.x.86]  int,unsigned int> &)</tt> where the first   argument passes the data of the calling class and the last argument   defines the range of cells which should be worked on (typically more than   one cell should be worked on in order to reduce overheads).      [2.x.87]  owning_class The object which provides the `cell_operation`   call. To be compatible with this interface, the class must allow to call   `owning_class->cell_operation(...)`.      [2.x.88]  dst Destination vector holding the result. If the vector is of   type  [2.x.89]  (or composite objects thereof   such as  [2.x.90]  the loop calls    [2.x.91]  at the end of the call   internally. For other vectors, including parallel Trilinos or PETSc   vectors, no such call is issued. Note that Trilinos/Epetra or PETSc   vectors do currently not work in parallel because the present class uses   MPI-local index addressing, as opposed to the global addressing implied   by those external libraries.      [2.x.92]  src Input vector. If the vector is of type    [2.x.93]  (or composite objects thereof such as    [2.x.94]  the loop calls    [2.x.95]  at the start of   the call internally to make sure all necessary data is locally   available. Note, however, that the vector is reset to its original state   at the end of the loop, i.e., if the vector was not ghosted upon entry of   the loop, it will not be ghosted upon finishing the loop.      [2.x.96]  operation_before_loop This functor can be used to perform an   operation on entries of the `src` and `dst` vectors (or other vectors)   before the operation on cells first touches a particular DoF according to   the general description in the text above. This function is passed a   range of the locally owned degrees of freedom on the selected   `dof_handler_index_pre_post` (in MPI-local numbering).      [2.x.97]  operation_after_loop This functor can be used to perform an   operation on entries of the `src` and `dst` vectors (or other vectors)   after the operation on cells last touches a particular DoF according to   the general description in the text above. This function is passed a   range of the locally owned degrees of freedom on the selected   `dof_handler_index_pre_post` (in MPI-local numbering).      [2.x.98]  dof_handler_index_pre_post Since MatrixFree can be initialized   with a vector of DoFHandler objects, each of them will in general have   vector sizes and thus different ranges returned to   `operation_before_loop` and `operation_after_loop`. Use this variable to   specify which one of the DoFHandler objects the index range should be   associated to. Defaults to the `dof_handler_index` 0.    
*  [2.x.99]  The close locality of the `operation_before_loop` and   `operation_after_loop` is currently only implemented for the MPI-only   case. In case threading is enabled, the complete `operation_before_loop`   is scheduled before the parallel loop, and `operation_after_loop` is   scheduled strictly afterwards, due to the complicated dependencies.  
* [0.x.57]*
   Same as above, but for class member functions which are non-const.  
* [0.x.58]*
   Same as above, but taking an  [2.x.100]  as the `cell_operation`   rather than a class member function.  
* [0.x.59]*
   This method runs a loop over all cells (in parallel) and performs the MPI   data exchange on the source vector and destination vector. As opposed to   the other variants that only runs a function on cells, this method also   takes as arguments a function for the interior faces and for the boundary   faces, respectively.      [2.x.101]  cell_operation  [2.x.102]  with the signature <tt>cell_operation   (const MatrixFree<dim,Number> &, OutVector &, InVector &,    [2.x.103]  int,unsigned int> &)</tt> where the first argument   passes the data of the calling class and the last argument defines the   range of cells which should be worked on (typically more than one cell   should be worked on in order to reduce overheads). One can pass a pointer   to an object in this place if it has an  [2.x.104]  with the   correct set of arguments since such a pointer can be converted to the   function object.      [2.x.105]  face_operation  [2.x.106]  with the signature <tt>face_operation   (const MatrixFree<dim,Number> &, OutVector &, InVector &,    [2.x.107]  int,unsigned int> &)</tt> in analogy to   `cell_operation`, but now the part associated to the work on interior   faces. Note that the MatrixFree framework treats periodic faces as interior   ones, so they will be assigned their correct neighbor after applying   periodicity constraints within the face_operation calls.      [2.x.108]  boundary_operation  [2.x.109]  with the signature   <tt>boundary_operation (const MatrixFree<dim,Number> &, OutVector &,   InVector &,  [2.x.110]  int,unsigned int> &)</tt> in analogy to   `cell_operation` and `face_operation`, but now the part associated to the   work on boundary faces. Boundary faces are separated by their   `boundary_id` and it is possible to query that id using    [2.x.111]  Note that both interior and faces use the   same numbering, and faces in the interior are assigned lower numbers than   the boundary faces.      [2.x.112]  dst Destination vector holding the result. If the vector is of   type  [2.x.113]  (or composite objects thereof   such as  [2.x.114]  the loop calls    [2.x.115]  at the end of the call   internally.      [2.x.116]  src Input vector. If the vector is of type    [2.x.117]  (or composite objects thereof such as    [2.x.118]  the loop calls    [2.x.119]  at the start of   the call internally to make sure all necessary data is locally   available. Note, however, that the vector is reset to its original state   at the end of the loop, i.e., if the vector was not ghosted upon entry of   the loop, it will not be ghosted upon finishing the loop.      [2.x.120]  zero_dst_vector If this flag is set to `true`, the vector `dst`   will be set to zero inside the loop. Use this case in case you perform a   typical `vmult()` operation on a matrix object, as it will typically be   faster than calling `dst = 0;` before the loop separately. This is   because the vector entries are set to zero only on subranges of the   vector, making sure that the vector entries stay in caches as much as   possible.      [2.x.121]  dst_vector_face_access Set the type of access into the vector   `dst` that will happen inside the body of the  [2.x.122]    function. As explained in the description of the DataAccessOnFaces   struct, the purpose of this selection is to reduce the amount of data   that must be exchanged over the MPI network (or via `memcpy` if within   the shared memory region of a node) to gain performance. Note that there   is no way to communicate this setting with the FEFaceEvaluation class,   therefore this selection must be made at this site in addition to what is   implemented inside the `face_operation` function. As a consequence, there   is also no way to check that the setting passed to this call is   consistent with what is later done by `FEFaceEvaluation`, and it is the   user's responsibility to ensure correctness of data.      [2.x.123]  src_vector_face_access Set the type of access into the vector   `src` that will happen inside the body of the  [2.x.124]  function,   in analogy to `dst_vector_face_access`.  
* [0.x.60]*
   This is the second variant to run the loop over all cells, interior   faces, and boundary faces, now providing three function pointers to   member functions of class  [2.x.125]  with the signature <code>operation   (const MatrixFree<dim,Number> &, OutVector &, InVector &,    [2.x.126]  int,unsigned int>&)const</code>. This method obviates   the need to define a lambda function or to call  [2.x.127]  to bind   the class into the given   function in case the local function needs to access data in the class   (i.e., it is a non-static member function).      [2.x.128]  cell_operation Pointer to member function of `CLASS` with the   signature <tt>cell_operation (const MatrixFree<dim,Number> &, OutVector &,   InVector &,  [2.x.129]  int,unsigned int> &)</tt> where the first   argument passes the data of the calling class and the last argument   defines the range of cells which should be worked on (typically more than   one cell should be worked on in order to reduce overheads). Note that the   loop will typically split the `cell_range` into smaller pieces and work   on `cell_operation`, `face_operation`, and `boundary_operation`   alternately, in order to increase the potential reuse of vector entries   in caches.      [2.x.130]  face_operation Pointer to member function of `CLASS` with the   signature <tt>face_operation (const MatrixFree<dim,Number> &, OutVector &,   InVector &,  [2.x.131]  int,unsigned int> &)</tt> in analogy to   `cell_operation`, but now the part associated to the work on interior   faces. Note that the MatrixFree framework treats periodic faces as   interior ones, so they will be assigned their correct neighbor after   applying periodicity constraints within the face_operation calls.      [2.x.132]  boundary_operation Pointer to member function of `CLASS` with the   signature <tt>boundary_operation (const MatrixFree<dim,Number> &, OutVector   &, InVector &,  [2.x.133]  int,unsigned int> &)</tt> in analogy to   `cell_operation` and `face_operation`, but now the part associated to the   work on boundary faces. Boundary faces are separated by their   `boundary_id` and it is possible to query that id using    [2.x.134]  Note that both interior and faces use the   same numbering, and faces in the interior are assigned lower numbers than   the boundary faces.      [2.x.135]  owning_class The object which provides the `cell_operation`   call. To be compatible with this interface, the class must allow to call   `owning_class->cell_operation(...)`, `owning_class->face_operation(...)`,   and `owning_class->boundary_operation(...)`.      [2.x.136]  dst Destination vector holding the result. If the vector is of   type  [2.x.137]  (or composite objects thereof   such as  [2.x.138]  the loop calls    [2.x.139]  at the end of the call   internally.      [2.x.140]  src Input vector. If the vector is of type    [2.x.141]  (or composite objects thereof such as    [2.x.142]  the loop calls    [2.x.143]  at the start of   the call internally to make sure all necessary data is locally   available. Note, however, that the vector is reset to its original state   at the end of the loop, i.e., if the vector was not ghosted upon entry of   the loop, it will not be ghosted upon finishing the loop.      [2.x.144]  zero_dst_vector If this flag is set to `true`, the vector `dst`   will be set to zero inside the loop. Use this case in case you perform a   typical `vmult()` operation on a matrix object, as it will typically be   faster than calling `dst = 0;` before the loop separately. This is   because the vector entries are set to zero only on subranges of the   vector, making sure that the vector entries stay in caches as much as   possible.      [2.x.145]  dst_vector_face_access Set the type of access into the vector   `dst` that will happen inside the body of the  [2.x.146]    function. As explained in the description of the DataAccessOnFaces   struct, the purpose of this selection is to reduce the amount of data   that must be exchanged over the MPI network (or via `memcpy` if within   the shared memory region of a node) to gain performance. Note that there   is no way to communicate this setting with the FEFaceEvaluation class,   therefore this selection must be made at this site in addition to what is   implemented inside the `face_operation` function. As a consequence, there   is also no way to check that the setting passed to this call is   consistent with what is later done by `FEFaceEvaluation`, and it is the   user's responsibility to ensure correctness of data.      [2.x.147]  src_vector_face_access Set the type of access into the vector   `src` that will happen inside the body of the  [2.x.148]  function,   in analogy to `dst_vector_face_access`.  
* [0.x.61]*
   Same as above, but for class member functions which are non-const.  
* [0.x.62]*
   This method runs the loop over all cells (in parallel) similarly as   cell_loop() does. However, this function is intended to be used   for the case if face and boundary integrals should be also   evaluated. In contrast to loop(), the user provides only a single function   that should contain the cell integral over a cell (or batch of cells when   vectorizing) and the face and boundary integrals over all its faces. This   is referred to in the literature as `element-centric loop` or `cell-centric   loop`.     To be able to evaluate all face integrals (with values or gradients   from the neighboring cells), all ghost values from neighboring cells are   updated. Use    [2.x.149]  face_no) to access quantities on arbitrary   faces of a cell and the respective neighbors.      [2.x.150]  cell_operation Pointer to member function of `CLASS` with the   signature <tt>cell_operation (const MatrixFree<dim,Number> &, OutVector &,   InVector &,  [2.x.151]  int,unsigned int> &)</tt> where the first   argument passes the data of the calling class and the last argument   defines the range of cells which should be worked on (typically more than   one cell is passed in from the loop in order to reduce overheads).      [2.x.152]  owning_class The object which provides the `cell_operation`   call. To be compatible with this interface, the class must allow to call   `owning_class->cell_operation(...)`.      [2.x.153]  dst Destination vector holding the result. If the vector is of   type  [2.x.154]  (or composite objects thereof   such as  [2.x.155]  the loop calls    [2.x.156]  at the end of the call   internally.      [2.x.157]  src Input vector. If the vector is of type    [2.x.158]  (or composite objects thereof such as    [2.x.159]  the loop calls    [2.x.160]  at the start of   the call internally to make sure all necessary data is locally   available. Note, however, that the vector is reset to its original state   at the end of the loop, i.e., if the vector was not ghosted upon entry of   the loop, it will not be ghosted upon finishing the loop.      [2.x.161]  zero_dst_vector If this flag is set to `true`, the vector `dst`   will be set to zero inside the loop. Use this case in case you perform a   typical `vmult()` operation on a matrix object, as it will typically be   faster than calling `dst = 0;` before the loop separately. This is   because the vector entries are set to zero only on subranges of the   vector, making sure that the vector entries stay in caches as much as   possible.      [2.x.162]  src_vector_face_access Set the type of access into the vector   `src` that will happen inside the body of the  [2.x.163]  function   during face integrals.   As explained in the description of the DataAccessOnFaces   struct, the purpose of this selection is to reduce the amount of data   that must be exchanged over the MPI network (or via `memcpy` if within   the shared memory region of a node) to gain performance. Note that there   is no way to communicate this setting with the FEFaceEvaluation class,   therefore this selection must be made at this site in addition to what is   implemented inside the `face_operation` function. As a consequence, there   is also no way to check that the setting passed to this call is   consistent with what is later done by `FEFaceEvaluation`, and it is the   user's responsibility to ensure correctness of data.  
* [0.x.63]*
   Same as above, but for the class member function which is non-const.  
* [0.x.64]*
   Same as above, but with  [2.x.164]   
* [0.x.65]*
   In the hp-adaptive case, a subrange of cells as computed during the cell   loop might contain elements of different degrees. Use this function to   compute what the subrange for an individual finite element degree is. The   finite element degree is associated to the vector component given in the   function call.  
* [0.x.66]*
   In the hp-adaptive case, a subrange of cells as computed during the cell   loop might contain elements of different degrees. Use this function to   compute what the subrange for a given index the hp-finite element, as   opposed to the finite element degree in the other function.  
* [0.x.67]*
   In the hp adaptive case, return number of active_fe_indices.  
* [0.x.68]*
   In the hp-adaptive case, return the active_fe_index of a cell range.  
* [0.x.69]*
   In the hp-adaptive case, return the active_fe_index of a face range.  
* [0.x.70]*
    [2.x.165]  3: Initialization of vectors  
* [0.x.71]*
   Initialize function for a general vector. The length of the vector is   equal to the total number of degrees in the DoFHandler. If the vector is   of class  [2.x.166]  the ghost entries   are set accordingly. For vector-valued problems with several DoFHandlers   underlying this class, the parameter  [2.x.167]  defines which   component is to be used.     For the vectors used with MatrixFree and in FEEvaluation, a vector needs   to hold all    [2.x.168]  "locally active DoFs"   and also some of the    [2.x.169]  "locally relevant DoFs".   The selection of DoFs is such that one can read all degrees of freedom on   all locally relevant elements (locally active) plus the degrees of freedom   that constraints expand into from the locally owned cells. However, not   all locally relevant DoFs are stored because most of them would never be   accessed in matrix-vector products and result in too much data sent   around which impacts the performance.  
* [0.x.72]*
   Initialize function for a distributed vector. The length of the vector is   equal to the total number of degrees in the DoFHandler. If the vector is   of class  [2.x.170]  the ghost entries   are set accordingly. For vector-valued problems with several DoFHandlers   underlying this class, the parameter  [2.x.171]  defines which   component is to be used.     For the vectors used with MatrixFree and in FEEvaluation, a vector needs   to hold all    [2.x.172]  "locally active DoFs"   and also some of the    [2.x.173]  "locally relevant DoFs".   The selection of DoFs is such that one can read all degrees of freedom on   all locally relevant elements (locally active) plus the degrees of freedom   that constraints expand into from the locally owned cells. However, not   all locally relevant DoFs are stored because most of them would never be   accessed in matrix-vector products and result in too much data sent   around which impacts the performance.  
* [0.x.73]*
   Return the partitioner that represents the locally owned data and the   ghost indices where access is needed to for the cell loop. The   partitioner is constructed from the locally owned dofs and ghost dofs   given by the respective fields. If you want to have specific information   about these objects, you can query them with the respective access   functions. If you just want to initialize a (parallel) vector, you should   usually prefer this data structure as the data exchange information can   be reused from one vector to another.  
* [0.x.74]*
   Return the set of cells that are owned by the processor.  
* [0.x.75]*
   Return the set of ghost cells needed but not owned by the processor.  
* [0.x.76]*
   Return a list of all degrees of freedom that are constrained. The list   is returned in MPI-local index space for the locally owned range of the   vector, not in global MPI index space that spans all MPI processors. To   get numbers in global index space, call   <tt>get_vector_partitioner()->local_to_global</tt> on an entry of the   vector. In addition, it only returns the indices for degrees of freedom   that are owned locally, not for ghosts.  
* [0.x.77]*
   Computes a renumbering of degrees of freedom that better fits with the   data layout in MatrixFree according to the given layout of data. Note that   this function does not re-arrange the information stored in this class,   but rather creates a renumbering for consumption of    [2.x.174]  To have any effect a MatrixFree object must be   set up again using the renumbered DoFHandler and AffineConstraints. Note   that if a DoFHandler calls  [2.x.175]  all information in   MatrixFree becomes invalid.  
* [0.x.78]*
    [2.x.176]  4: General information  
* [0.x.79]*
   Return whether a given FiniteElement  [2.x.177]  is supported by this class.  
* [0.x.80]*
   Return the number of different DoFHandlers specified at initialization.  
* [0.x.81]*
   For the finite element underlying the DoFHandler specified by  [2.x.178]    dof_handler_index, return the number of base elements.  
* [0.x.82]*
   Return the number of cells this structure is based on. If you are using a   usual DoFHandler, it corresponds to the number of (locally owned) active   cells. Note that most data structures in this class do not directly act   on this number but rather on n_cell_batches() which gives the number of   cells as seen when lumping several cells together with vectorization.  
* [0.x.83]*
    [2.x.179]  Use n_cell_batches() instead.  
* [0.x.84]*
   Return the number of cell batches that this structure works on. The   batches are formed by application of vectorization over several cells in   general. The cell range in  [2.x.180]  runs from zero to   n_cell_batches() (exclusive), so this is the appropriate size if you want   to store arrays of data for all cells to be worked on. This number is   approximately  [2.x.181]    (depending on how many cell batches that do not get filled up completely).  
* [0.x.85]*
   Return the number of additional cell batches that this structure keeps   for face integration. Note that not all cells that are ghosted in the   triangulation are kept in this data structure, but only the ones which   are necessary for evaluating face integrals from both sides.  
* [0.x.86]*
   Return the number of interior face batches that this structure works on.   The batches are formed by application of vectorization over several faces   in general. The face range in  [2.x.182]  runs from zero to   n_inner_face_batches() (exclusive), so this is the appropriate size if   you want to store arrays of data for all interior faces to be worked on.  
* [0.x.87]*
   Return the number of boundary face batches that this structure works on.   The batches are formed by application of vectorization over several faces   in general. The face range in  [2.x.183]  runs from n_inner_face_batches() to   n_inner_face_batches()+n_boundary_face_batches() (exclusive), so if you   need to store arrays that hold data for all boundary faces but not the   interior ones, this number gives the appropriate size.  
* [0.x.88]*
   Return the number of faces that are not processed locally but belong to   locally owned faces.  
* [0.x.89]*
   In order to apply different operators to different parts of the boundary,   this method can be used to query the boundary id of a given face in the   faces' own sorting by lanes in a VectorizedArray. Only valid for an index   indicating a boundary face.  
* [0.x.90]*
   Return the boundary ids for the faces within a cell, using the cells'   sorting by lanes in the VectorizedArray.  
* [0.x.91]*
   Return the DoFHandler with the index as given to the respective    [2.x.184]  argument in the reinit() function.  
* [0.x.92]*
   Return the DoFHandler with the index as given to the respective    [2.x.185]  argument in the reinit() function. Note that if you want to   call this function with a template parameter different than the default   one, you will need to use the `template` before the function call, i.e.,   you will have something like `matrix_free.template    [2.x.186]       [2.x.187]  Use the non-templated equivalent of this function.  
* [0.x.93]*
   Return the cell iterator in deal.II speak to a given cell batch   (populating several lanes in a VectorizedArray) and the lane index within   the vectorization across cells in the renumbering of this structure.     Note that the cell iterators in deal.II go through cells differently to   what the cell loop of this class does. This is because several cells are   processed together (vectorization across cells), and since cells with   neighbors on different MPI processors need to be accessed at a certain   time when accessing remote data and overlapping communication with   computation.  
* [0.x.94]*
   This returns the level and index for the cell that would be returned by   get_cell_iterator() for the same arguments `cell_batch_index` and   `lane_index`.  
* [0.x.95]*
   Return the cell iterator in deal.II speak to an interior/exterior cell of   a face in a pair of a face batch and lane index. The second element of   the pair is the face number so that the face iterator can be accessed:   `pair.first()->face(pair.second());`     Note that the face iterators in deal.II go through cells differently to   what the face/boundary loop of this class does. This is because several   faces are worked on together (vectorization), and since faces with neighbor   cells on different MPI processors need to be accessed at a certain time   when accessing remote data and overlapping communication with computation.  
* [0.x.96]*
    [2.x.188]   [2.x.189]       [2.x.190]  Use get_cell_iterator() instead.  
* [0.x.97]*
   Since this class uses vectorized data types with usually more than one   value in the data field, a situation might occur when some components of   the vector type do not correspond to an actual cell in the mesh. When   using only this class, one usually does not need to bother about that   fact since the values are padded with zeros. However, when this class is   mixed with deal.II access to cells, care needs to be taken. This function   returns  [2.x.191]  if not all `n_lanes` cells for the given   `cell_batch_index` correspond to actual cells of the mesh and some are   merely present for padding reasons. To find out how many cells are   actually used, use the function n_active_entries_per_cell_batch().  
* [0.x.98]*
    [2.x.192]  Use n_active_entries_per_cell_batch() instead.  
* [0.x.99]*
   This query returns how many cells among the  [2.x.193]    many cells within a cell batch to actual cells in the mesh, rather than   being present for padding reasons. For most given cell batches in   n_cell_batches(), this number is equal to  [2.x.194]    but there might be one or a few cell batches in the mesh (where the   numbers do not add up) where only some of the cells within a batch are   used, indicated by the function at_irregular_cell().  
* [0.x.100]*
   Use this function to find out how many faces over the length of   vectorization data types correspond to real faces (both interior and   boundary faces, as those use the same indexing but with different ranges)   in the mesh. For most given indices in n_inner_faces_batches() and   n_boundary_face_batches(), this is just  [2.x.195]  many, but   there might be one or a few meshes (where the numbers do not add up)   where there are less such lanes filled.  
* [0.x.101]*
   Return the number of degrees of freedom per cell for a given hp-index.  
* [0.x.102]*
   Return the number of quadrature points per cell for a given hp-index.  
* [0.x.103]*
   Return the number of degrees of freedom on each face of the cell for   given hp-index.  
* [0.x.104]*
   Return the number of quadrature points on each face of the cell for   given hp-index.  
* [0.x.105]*
   Return the quadrature rule for given hp-index.  
* [0.x.106]*
   Return the quadrature rule for given hp-index.  
* [0.x.107]*
   Return the category the current batch of cells was assigned to. Categories   run between the given values in the field    [2.x.196]  for non-hp-DoFHandler types   and return the active FE index in the hp-adaptive case.  
* [0.x.108]*
   Return the category on the cells on the two sides of the current batch of   faces.  
* [0.x.109]*
   Queries whether or not the indexation has been set.  
* [0.x.110]*
   Queries whether or not the geometry-related information for the cells has   been set.  
* [0.x.111]*
   Return the level of the mesh to be worked on. Returns    [2.x.197]  if working on active cells.  
* [0.x.112]*
   Return an approximation of the memory consumption of this class in   bytes.  
* [0.x.113]*
   Prints a detailed summary of memory consumption in the different   structures of this class to the given output stream.  
* [0.x.114]*
   Prints a summary of this class to the given output stream. It is focused   on the indices, and does not print all the data stored.  
* [0.x.115]*
    [2.x.198]  5: Access of internal data structure     Note: Expert mode, interface not stable between releases.  
* [0.x.116]*
   Return information on task graph.  
* [0.x.117]   Return geometry-dependent information on the cells.  
* [0.x.118]*
   Return information on indexation degrees of freedom.  
* [0.x.119]*
   Return the number of weights in the constraint pool.  
* [0.x.120]*
   Return a pointer to the first number in the constraint pool data with   index  [2.x.199]  (to be used together with  [2.x.200]   
* [0.x.121]*
   Return a pointer to one past the last number in the constraint pool data   with index  [2.x.201]  (to be used together with  [2.x.202]    constraint_pool_begin()).  
* [0.x.122]*
   Return the unit cell information for given hp-index.  
* [0.x.123]*
   Return the connectivity information of a face.  
* [0.x.124]*
   Return the table that translates a triple of the macro cell number,   the index of a face within a cell and the index within the cell batch of   vectorization into the index within the faces array.  
* [0.x.125]*
   Obtains a scratch data object for internal use. Make sure to release it   afterwards by passing the pointer you obtain from this object to the   release_scratch_data() function. This interface is used by FEEvaluation   objects for storing their data structures.     The organization of the internal data structure is a thread-local storage   of a list of vectors. Multiple threads will each get a separate storage   field and separate vectors, ensuring thread safety. The mechanism to   acquire and release objects is similar to the mechanisms used for the   local contributions of WorkStream, see    [2.x.203]  "the WorkStream paper".  
* [0.x.126]*
   Makes the object of the scratchpad available again.  
* [0.x.127]*
   Obtains a scratch data object for internal use. Make sure to release it   afterwards by passing the pointer you obtain from this object to the   release_scratch_data_non_threadsafe() function. Note that, as opposed to   acquire_scratch_data(), this method can only be called by a single thread   at a time, but opposed to the acquire_scratch_data() it is also possible   that the thread releasing the scratch data can be different than the one   that acquired it.  
* [0.x.128]*
   Makes the object of the scratch data available again.  
* [0.x.129]*
   This is the actual reinit function that sets up the indices for the   DoFHandler case.  
* [0.x.130]*
   Initializes the fields in DoFInfo together with the constraint pool that   holds all different weights in the constraints (not part of DoFInfo   because several DoFInfo classes can have the same weights which   consequently only need to be stored once).  
* [0.x.131]*
   Initializes the DoFHandlers based on a DoFHandler<dim> argument.  
* [0.x.132]*
   Pointers to the DoFHandlers underlying the current problem.  
* [0.x.133]*
   Contains the information about degrees of freedom on the individual cells   and constraints.  
* [0.x.134]*
   Contains the weights for constraints stored in DoFInfo. Filled into a   separate field since several vector components might share similar   weights, which reduces memory consumption. Moreover, it obviates template   arguments on DoFInfo and keeps it a plain field of indices only.  
* [0.x.135]*
   Contains an indicator to the start of the ith index in the constraint   pool data.  
* [0.x.136]*
   Holds information on transformation of cells from reference cell to real   cell that is needed for evaluating integrals.  
* [0.x.137]*
   Contains shape value information on the unit cell.  
* [0.x.138]*
   Describes how the cells are gone through. With the cell level (first   index in this field) and the index within the level, one can reconstruct   a deal.II cell iterator and use all the traditional things deal.II offers   to do with cell iterators.  
* [0.x.139]*
   For discontinuous Galerkin, the cell_level_index includes cells that are   not on the local processor but that are needed to evaluate the cell   integrals. In cell_level_index_end_local, we store the number of local   cells.  
* [0.x.140]*
   Stores the basic layout of the cells and faces to be treated, including   the task layout for the shared memory parallelization and possible   overlaps between communications and computations with MPI.  
* [0.x.141]*
   Vector holding face information. Only initialized if   build_face_info=true.  
* [0.x.142]*
   Stores whether indices have been initialized.  
* [0.x.143]*
   Stores whether indices have been initialized.  
* [0.x.144]*
   Scratchpad memory for use in evaluation. We allow more than one   evaluation object to attach to this field (this, the outer    [2.x.204]  so we need to keep tracked of whether a certain data   field is already used (first part of pair) and keep a list of   objects.  
* [0.x.145]*
   Scratchpad memory for use in evaluation and other contexts, non-thread   safe variant.  
* [0.x.146]*
   Stored the level of the mesh to be worked on.  
* [0.x.147]*
   Internal class for exchanging data between vectors.  
* [0.x.148]*
     Constructor. Takes MF data, flag for face access in DG and     number of components.    
* [0.x.149]*
     Destructor.    
* [0.x.150]*
     Go through all components in MF object and choose the one     whose partitioner is compatible with the Partitioner in this component.    
* [0.x.151]*
     Get partitioner for the given  [2.x.205]  taking into     account vector_face_access set in constructor.    
* [0.x.152]*
     Start update_ghost_value for serial vectors    
* [0.x.153]*
     Start update_ghost_value for vectors that do not support     the split into _start() and finish() stages    
* [0.x.154]*
     Start update_ghost_value for vectors that _do_ support     the split into _start() and finish() stages, but don't support     exchange on a subset of DoFs    
* [0.x.155]*
     Finally, start update_ghost_value for vectors that _do_ support     the split into _start() and finish() stages and also support     exchange on a subset of DoFs,     i.e.  [2.x.206]     
* [0.x.156]*
     Finish update_ghost_value for vectors that do not support     the split into _start() and finish() stages and serial vectors    
* [0.x.157]*
     Finish update_ghost_value for vectors that _do_ support     the split into _start() and finish() stages, but don't support     exchange on a subset of DoFs    
* [0.x.158]*
     Finish update_ghost_value for vectors that _do_ support     the split into _start() and finish() stages and also support     exchange on a subset of DoFs,     i.e.  [2.x.207]     
* [0.x.159]*
     Start compress for serial vectors    
* [0.x.160]*
     Start compress for vectors that do not support     the split into _start() and finish() stages    
* [0.x.161]*
     Start compress for vectors that _do_ support     the split into _start() and finish() stages, but don't support     exchange on a subset of DoFs    
* [0.x.162]*
     Start compress for vectors that _do_ support     the split into _start() and finish() stages and also support     exchange on a subset of DoFs,     i.e.  [2.x.208]     
* [0.x.163]*
     Finish compress for vectors that do not support     the split into _start() and finish() stages and serial vectors    
* [0.x.164]*
     Finish compress for vectors that _do_ support     the split into _start() and finish() stages, but don't support     exchange on a subset of DoFs    
* [0.x.165]*
     Start compress for vectors that _do_ support     the split into _start() and finish() stages and also support     exchange on a subset of DoFs,     i.e.  [2.x.209]     
* [0.x.166]*
     Reset all ghost values for serial vectors    
* [0.x.167]*
     Reset all ghost values for vector that don't support     exchange on a subset of DoFs    
* [0.x.168]*
     Reset all ghost values for vector that _do_ support     exchange on a subset of DoFs, i.e.      [2.x.210]     
* [0.x.169]*
     Zero out vector region for vector that _do_ support     exchange on a subset of DoFs <==> begin() + ind == local_element(ind),     i.e.  [2.x.211]     
* [0.x.170]*
     Zero out vector region for vector that do _not_ support exchange on a     subset of DoFs <==> begin() + ind == local_element(ind) but are still a     vector type    
* [0.x.171]*
     Zero out vector region for non-vector types, i.e., classes that do not     have  [2.x.212]     
* [0.x.172]*
   An internal class to convert three function pointers to the   scheme with virtual functions above.  
* [0.x.173]