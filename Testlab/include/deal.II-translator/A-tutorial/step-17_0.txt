include/deal.II-translator/A-tutorial/step-17_0.txt
[0.x.0]*
 [2.x.0]
* 本教程依赖于 [2.x.1] 。
* [1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21]
*[1.x.22][1.x.23][1.x.24] 。


*[1.x.25][1.x.26]


* 这个程序没有引入任何新的数学思想；事实上，它所做的只是做与[2.x.2]已经做的完全相同的计算，但它以一种不同的方式来做：我们没有使用deal.II的线性代数类，而是在deal.II提供的类之上建立一切，这些类包裹着[1.x.27]库的线性代数实现。由于PETSc允许将矩阵和向量分布在MPI网络中的几台计算机上，因此产生的代码甚至能够以%并行方式解决该问题。如果你不知道PETSc是什么，那么现在就是快速浏览其主页的好时机。
* 作为这个程序的先决条件，你需要安装PETSc，如果你想在一个集群上以%并行方式运行，你还需要[1.x.28]来划分网格。在[1.x.29]文件中描述了deal.II和这两个附加库的安装。
* 现在，关于细节：如前所述，该程序不计算任何新的东西，所以对有限元类等的使用与以前完全相同。与以前的程序不同的是，我们用近似的类[2.x.4]和[2.x.5]代替了几乎所有的类[2.x.3]的使用，它们存储数据的方式使MPI网络中的每个处理器只存储矩阵或矢量的一部分。更具体地说，每个处理器将只存储与它 "拥有 "的自由度相对应的那些矩阵行。对于向量，它们要么只存储与处理器拥有的自由度相对应的元素（这是右手边所需要的），要么也存储一些额外的元素，以确保每个处理器都能访问处理器拥有的单元（所谓的[2.x.6]"本地活动的自由度"）或邻近的单元（所谓的[2.x.7]"本地相关的自由度"）上的解组件。
* PETScWrapper命名空间的类所提供的接口与deal.II线性代数类的接口非常相似，但它们不是自己实现这一功能，而是简单地传递给其相应的PETScfunctions。因此，包装器只是用来给PETSc提供一个更现代的、面向对象的接口，并使PETSc和deal.II对象的使用尽可能地互换。使用PETSc的主要意义在于它可以以%并行方式运行。我们将利用这一点，将域划分为与MPI网络中的进程数量相同的块（"子域"）。同时，PETSc还提供了假的MPI存根，所以如果PETSc的配置中没有MPI，你可以在一台机器上运行这个程序。
*

*[1.x.30][1.x.31]


* 开发通过MPI并行运行的软件需要改变一下思维方式，因为通常需要分割所有的数据结构，使每个处理器只存储整个问题的一部分。因此，你通常不能在每个处理器上访问一个解决方案向量的所有组成部分。
*
* - 每个处理器可能根本没有足够的内存来容纳整个解决方案向量。由于数据被分割或 "分布 "在处理器之间，我们把MPI使用的编程模型称为 "分布式内存计算"（与 "共享内存计算 "相对，后者意味着多个处理器可以在一个内存空间内访问所有数据，例如，当一台机器的多个核心在一个共同任务上工作时）。分布式内存计算的一些基本原理在[2.x.8]"使用分布式内存的多处理器并行计算 "文档模块中讨论，该模块本身就是[2.x.9]"并行计算 "模块的一个子模块。
* 一般来说，为了真正能够扩展到大量的处理器，我们需要在可用的处理器之间分割[1.x.32]数据结构，其大小随着整个问题的大小而变化。关于程序 "扩展 "的定义，见[2.x.10]"本词汇表条目"）。例如，这包括三角形、矩阵和所有全局向量（解决方案，右侧）。如果不拆分所有这些对象，其中一个对象将在所有处理器上复制，如果问题大小（和可用处理器的数量）变得很大，最终会变得太大。（另一方面，在每个处理器上保留与整个问题大小无关的对象是完全可以的。例如，每个可执行文件的副本将创建它自己的有限元对象，或我们在汇编中使用的本地矩阵）。)
* 在当前的程序中（以及相关的[2.x.11]中），我们不会走得这么远，而是对使用MPI做一个比较温和的介绍。更具体地说，我们要并行化的数据结构只有矩阵和向量。然而，我们并没有拆分Triangulation和DoFHandler类：每个进程仍然拥有这些对象的完整副本，而且所有进程都拥有其他进程所拥有的确切副本。然后，我们只需在每个处理器上的三角形的每个副本中，标记哪个处理器拥有哪些单元。这个过程被称为将网格 "分割 "为[2.x.12]"子域"。
* 对于较大的问题，在每个处理器上存储[1.x.33]网格显然会产生一个瓶颈。分割网格稍微复杂一些，但并不复杂（从用户的角度来看，虽然它[1.x.34]在引擎下更复杂），我们将在[2.x.13]和其他一些程序中展示如何做到这一点。在讨论这个程序的某个功能如何工作的过程中，我们会多次评论它不能扩展到大型问题的事实，以及为什么不能。所有这些问题都将在[2.x.14]，特别是[2.x.15]中解决，它可以扩展到非常多的进程。
* 从哲学上讲，MPI的运行方式如下。你通常通过以下方式运行一个程序
* [1.x.35]
* 这意味着在（比如）32个处理器上运行它。如果你是在一个集群系统上，你通常需要[1.x.36]程序在32个处理器可用时运行；这将在你的集群的文档中描述。但是在系统内部，每当这些处理器可用时，通常会执行上述相同的调用）。)这样做的目的是，MPI系统将启动32个[1.x.37]的[2.x.16]可执行文件。(这些运行中的可执行文件的MPI术语是你有32个[2.x.17]"MPI进程"。)这可能发生在不同的机器上，甚至不能从对方的内存空间中读取，也可能发生在同一台机器上，但最终结果是一样的：这32个副本中的每一个都将在操作系统分配给它的一些内存中运行，而且它将不能直接读取其他31个副本的内存。为了在一个共同的任务中合作，这32个副本就必须[1.x.38]相互协作。MPI是[1.x.39]的缩写，通过允许程序[1.x.40]来实现这一目标。你可以把它看作是邮件服务：你可以把一封写给特定地址的信放入邮件中，它将被送达。但这就是你能控制的范围。如果你想让收信人对信的内容做些什么，例如把你想要的数据从那边返回给你，那么需要发生两件事。(i)接收方需要实际去检查他们的邮箱里是否有任何东西，(ii)如果有的话，做出适当的反应，比如说发送数据回来。如果你在等待这个返回信息，但原来的接收者却心不在焉，没有注意到，那么你就不走运了：你只能等待，直到你的请求在那里得到解决。在某些情况下，错误会导致原始接收者永远不检查你的邮件，在这种情况下，你将永远等待。
*
* - 这被称为[1.x.41]。( [2.x.18] 。
* 在实践中，人们通常不在发送和接收单个消息的层面上编程，而是使用更高层次的操作。例如，在程序中，我们将使用函数调用，从每个处理器中获取一个数字，将它们全部相加，然后将总和返回给所有处理器。在内部，这是用单个消息实现的，但对用户来说，这是透明的。我们称这种操作为[1.x.42]，因为[1.x.43]处理器参与其中。集合体允许我们编写程序，其中不是每个可执行文件的副本都在做完全不同的事情（这将是令人难以置信的编程难度），但在本质上，所有的副本都在为自己做同样的事情（尽管在不同的数据上），通过相同的代码块运行；然后他们通过集合体进行数据通信；然后再回到为自己做事情，再次通过相同的数据块运行。这是能够编写程序的关键部分，也是确保程序能够在任何数量的处理器上运行的关键部分，因为我们不需要为每个参与的处理器编写不同的代码。
* 这并不是说程序永远不会以不同的处理器在其可执行文件副本中运行不同的代码块的方式编写。程序内部也经常以其他方式而不是通过集合体进行通信。但是在实践中，%并行有限元代码几乎总是遵循这样的方案：程序的每个副本在同一时间运行相同的代码块，中间穿插着所有处理器相互交流的阶段。)
* 在现实中，即使是调用MPI集体函数的水平也是很低的。相反，下面的程序根本不包含任何对MPI的直接调用，而只包含对deal.II的用户隐藏这种通信的函数。这样做的好处是，你不需要学习MPI的细节和相当复杂的函数调用。尽管如此，你还是必须理解上文所述的MPI背后的一般哲学。
*

*[1.x.44][1.x.45]


* 本程序随后演示的技术是。
*
* - 如何使用PETSc封装类；这在本程序的主类的声明中已经可以看到， [2.x.19] 。
*
* - 如何将网格划分为子域；这发生在 [2.x.20] 函数中。
*
* - 如何对运行在MPI网络上的作业进行并行化操作；在这里，这是在很多地方都要注意的，最明显的是在 [2.x.21] 函数中。
*
* - 如何处理只存储向量项子集的向量，对于这些向量，我们必须确保它们在当前处理器上存储我们需要的东西。例如参见[2.x.22]函数。
*
* - 如何处理同时在多个处理器上运行的程序的状态输出。这是通过程序中的[2.x.23]变量完成的，在构造函数中初始化。
* 由于这一切只能用实际的代码来演示，让我们直接进入代码，不再多说。
*

* [1.x.46] [1.x.47]。
* [1.x.48][1.x.49] 。
*

*
* 首先是我们在以前的例子程序中已经使用过的常见的各种头文件。
*


* [1.x.50]
*
* 这里是我们在这个例子程序中特别需要的东西，这些东西在[2.x.24]中没有。首先，我们替换掉标准输出[2.x.25]，它用于并行计算，只在其中一个MPI进程中产生输出。
*


* [1.x.51]
*
* 我们将通过调用[2.x.26]命名空间中的相应函数来查询进程的数量和当前进程的数量。
*


* [1.x.52]
*
* 然后，我们要把所有涉及（全局）线性系统的线性代数组件替换成类，这些类围绕PETSc提供的接口与我们自己的线性代数类相似（PETSc是一个用C语言编写的库，而deal.II附带的包装类提供的PETSc功能的接口与我们自己的线性代数类已有的接口相似）。特别是，我们需要在MPI程序中分布在几个[2.x.27]"进程 "中的向量和矩阵（如果只有一个进程，也就是说，如果你只在一台机器上运行，并且没有MPI支持，则简单地映射为顺序的、本地的向量和矩阵）。
*


* [1.x.53]
*
* 然后我们还需要PETSc提供的求解器和预处理器的接口。
*


* [1.x.54]
*
* 此外，我们还需要一些算法来划分我们的网格，以便它们可以有效地分布在MPI网络上。分区算法在[2.x.28]命名空间中实现，我们需要一个额外的包含文件，用于[2.x.29]中的一个函数，该函数允许对与自由度相关的索引进行排序，以便根据它们所关联的子域进行编号。
*


* [1.x.55]

* 而这又是简单的C++。
*


* [1.x.56]

* 最后一步和以前的所有程序一样。



* [1.x.57]
*
* [1.x.58] [1.x.59]。


*
* 该程序的第一个真正的部分是主类的声明。  正如在介绍中提到的，几乎所有的内容都是从[2.x.30]中逐字复制过来的，所以我们只对这两个教程之间的少数差异进行评论。  有一个（表面上的）变化是，我们让[2.x.31]返回一个值，即收敛所需的迭代次数，这样我们就可以在适当的地方将其输出到屏幕上。
*


* [1.x.60]
*
* 第一个变化是，我们必须声明一个变量，表明我们应该通过它来分配我们的计算的[2.x.32]"MPI通信器"。
*


* [1.x.61]
*
* 然后我们有两个变量，告诉我们在并行世界中的位置。下面的第一个变量，[2.x.33] ，告诉我们总共有多少个MPI进程，而第二个变量，[2.x.34] ，表明在这个进程空间中，哪个是当前进程的编号（在MPI语言中，这相当于进程的[2.x.35] "等级"）。后者对每个进程都有一个唯一的值，介于零和（小于）[2.x.36] 之间。如果这个程序运行在没有MPI支持的单机上，那么它们的值分别是[2.x.37] ，。
*


* [1.x.62]
*
* 接下来是一个类似流的变量 [2.x.38] 。从本质上讲，它只是我们为了方便而使用的东西：在一个并行程序中，如果每个进程都输出状态信息，那么很快就会有很多杂乱的东西。相反，我们希望只让一个[2.x.39]"进程 "输出一次所有信息，例如[2.x.40]"等级 "为零的那个。同时，在我们创建输出的地方用[2.x.41]的条件给[1.x.63]加前缀似乎很傻。     
* 为了使这个问题更简单，ConditionalOStream类在引擎盖下正是这样做的：它就像一个流一样，但只有在一个标志被设置时才转发到一个真正的、底层的流。通过将这个条件设置为[2.x.42]（其中[2.x.43]对应于MPI进程的等级），我们确保输出只从第一个进程中产生，并且我们不会在每个进程中重复得到相同的输出行。因此，我们可以在每一个地方和每一个进程中使用[2.x.44]，但是除了一个进程之外，其他的进程都不会发生通过[2.x.45]输送到对象中的信息。
*


* [1.x.64]
*
* 成员变量列表的其余部分与 [2.x.46] 中的内容基本相同。然而，我们改变了矩阵和向量类型的声明，改为使用并行的PETSc对象。请注意，我们没有使用单独的稀疏模式，因为PETSc将其作为其矩阵数据结构的一部分进行内部管理。
*


* [1.x.65]
*
* [1.x.66] [1.x.67]。


*
* 以下内容取自[2.x.47]，没有改动。
*


* [1.x.68]
*
* [1.x.69] [1.x.70]。



* [1.x.71] [1.x.72]。


*
* 实际实现的第一步是主类的构造函数。除了初始化我们在[2.x.48]中已经有的相同成员变量外，我们在这里用连接所有进程的全局MPI通信器来初始化我们将使用的MPI通信器变量（在更复杂的应用中，我们可以在这里使用一个只连接所有进程的一个子集的通信器对象），并调用[2.x.49]辅助函数来确定进程的数量以及当前进程在这个画面中的地位。此外，我们确保输出只由（全局）第一个进程产生。我们通过将我们想要输出的流传给[2.x.50]和一个真/假标志作为参数，后者是通过测试当前执行构造函数调用的进程是否是MPI宇宙中的第一个来确定的。
*


* [1.x.73]
*
* [1.x.74] [1.x.75]。


*
* 接下来，需要实现我们为要解决的全局线性系统设置各种变量的函数。   
* 然而，在我们进行这项工作之前，对于一个并行程序来说，有一件事要做：我们需要确定哪个MPI进程负责每个单元。在进程之间分割单元，通常称为 "划分网格"，是通过给每个单元分配一个[2.x.51]"子域ID "来完成的。我们通过调用METIS库来做到这一点，该库以一种非常有效的方式，试图尽量减少子域之间的接口上的节点数量。我们没有尝试直接调用METIS，而是通过调用[2.x.52]函数来实现，该函数在更高的编程水平上完成这一工作。   
*


* [2.x.53] 正如在介绍中提到的，如果我们使用[2.x.54]类来代替三角形对象，我们可以避免这个手动划分的步骤（就像我们在[2.x.55]中做的那样）。该类实质上做了所有常规三角形的工作，但它也会在每次创建或细化网格操作后自动划分网格。   
* 分割之后，我们需要像往常一样列举所有的自由度。  然而，我们希望列举自由度的方式是：所有与子域0（位于过程0）的单元相关的自由度都在与子域1的单元相关的自由度之前，在过程2的单元之前，以此类推。我们需要这样做，因为我们必须将全局向量的右手边和解决方案，以及矩阵分割成连续的行块，住在每个处理器上，而且我们希望以一种需要最小通信的方式来做这件事。这种特殊的列举可以通过使用[2.x.56]对自由度指数重新排序得到。
* 这个初始设置的最后一步是，我们为自己得到一个IndexSet，表示这个过程所负责的全球未知数的子集。(注意，一个自由度不一定是由拥有一个单元的进程所拥有，只是因为这个自由度生活在这个单元上：有些自由度生活在子域之间的接口上，因此只由这个接口附近的一个进程所拥有)。   
* 在我们继续之前，让我们回顾一下介绍中已经讨论过的一个事实。我们在这里使用的三角形是在所有进程中复制的，每个进程都有整个三角形的完整副本，包括所有单元。分区只提供了一种方法来确定每个进程 "拥有 "哪些单元，但它知道所有单元的一切。同样，DoFHandler对象知道每个单元的一切，特别是每个单元上的自由度，不管它是否是当前进程拥有的。这不能扩展到大型问题，因为如果问题足够大，最终只是在每个进程中存储整个网格以及与之相关的所有内容将变得不可行。另一方面，如果我们将三角形分割成若干部分，使每个进程只存储它 "拥有 "的单元格，而不存储其他的单元格（或者，至少是其他所有单元格中足够小的一部分），那么我们就可以解决大型问题，只要我们将足够多的MPI进程扔给它们。这就是我们在[2.x.57]中要做的事情，例如，使用[2.x.58]类。  另一方面，我们在当前程序中演示的其余大部分内容实际上将继续工作，无论我们有整个三角形的可用，还是只有其中的一部分。
*


* [1.x.76]
*
* 我们需要初始化表示当前网格的悬挂节点约束的对象。与三角形和DoFHandler对象一样，我们将简单地在每个进程上存储[1.x.77]约束；同样，这不会有规模，但我们在[2.x.59]中展示了如何通过在每个MPI进程上只存储对该特定进程实际重要的自由度约束来解决这一问题。
*


* [1.x.78]
*
* 现在我们为系统矩阵创建稀疏模式。请注意，我们再次计算并存储所有的条目，而不仅仅是与这个过程有关的条目（见[2.x.60]或[2.x.61]，以获得更有效的处理方式）。
*


* [1.x.79]
*
* 现在我们确定本地拥有的DoF的集合，并使用它来初始化并行向量和矩阵。由于矩阵和向量需要并行工作，我们必须向它们传递一个MPI通信对象，以及IndexSet[2.x.62]中包含的分区信息。IndexSet包含全局大小（[1.x.80]自由度数）的信息，也包含要在本地存储哪些行的子集。  注意，系统矩阵需要该行和列的分区信息。对于正方形矩阵，就像这里的情况一样，列的划分方式应该与行的划分方式相同，但是对于矩形矩阵，我们必须以向量的划分方式来划分列，而行的划分方式必须与矩阵-向量乘法的目的向量相同。
*


* [1.x.81]
*
* [1.x.82] [1.x.83]。


*
* 我们现在组装矩阵和问题的右手边。在我们进行详细讨论之前，有一些事情值得一提。首先，我们将平行组装系统，也就是说，每个进程将负责在属于这个特定进程的单元上组装。请注意，自由度的分割方式是，单元内部和属于同一子域的单元之间的所有自由度都属于[2.x.63]该单元的过程。然而，即使如此，我们有时也需要在一个单元上与属于不同过程的邻居集合，在这些情况下，当我们将局部贡献加到全局矩阵或右手向量中时，我们必须将这些条目转移到拥有这些元素的过程中。幸运的是，我们不需要用手去做这件事。PETSc为我们做了这一切，它在本地缓存了这些元素，当我们在这个函数的末尾调用矩阵和向量的[2.x.64]函数时，根据需要将它们发送到其他进程。   
* 第二点是，一旦我们把矩阵和向量的贡献交给了PETSc，那么，a）很难，b）要把它们拿回来修改，效率非常低。这不仅是PETSc的错，也是这个程序的分布式性质的结果：如果一个条目存在于另一个处理器上，那么获取它的成本必然很高。这样做的后果是，我们不应该试图首先将矩阵和右手边组装起来，就像没有悬挂的节点约束和边界值一样，然后在第二步中消除这些约束（例如使用[2.x.65]），相反，我们应该在将这些条目移交给PETSc之前尝试消除悬挂的节点约束。这很容易：我们不是用手将元素复制到全局矩阵中（就像我们在[2.x.66]中做的那样），而是使用[2.x.67]函数同时处理悬空节点的问题。我们在 [2.x.68] 中也已经这样做了。第二步，消除边界节点，也可以这样做，把边界值放到与悬挂节点相同的AffineConstraints对象中（例如，见[2.x.69]中的方法）；但是，严格来说，在这里没有必要这样做，因为消除边界值可以只用每个进程本身存储的数据来完成，因此，我们使用之前在[2.x.70]中使用的方法，即通过[2.x.71]
* 说了这么多，下面是实际的实现，从辅助变量的一般设置开始。  (注意，我们仍然使用deal.II的全矩阵和向量类型的本地系统，因为这些类型很小，不需要在不同的进程中共享。)
*


* [1.x.84]
*
* 接下来是所有元素的循环。请注意，我们不必在每个进程上做[1.x.85]的工作：我们在这里的工作只是在实际属于这个MPI进程的单元上组装系统，所有其他的单元将由其他进程来处理。这就是紧随for-loop之后的if-clause所要处理的：它查询每个单元的子域标识符，这是一个与每个单元相关的数字，告诉我们所有者进程的情况。在更大的范围内，子域标识被用来将一个域分成几个部分（我们在上面[2.x.72]的开头就这样做了），并允许识别一个单元生活在哪个子域。在这个应用中，我们让每个进程恰好处理一个子域，所以我们确定了这些术语 [2.x.73] 。     
* 除此以外，如果你已经了解了在 [2.x.74] 中是如何完成的，那么组装本地系统就相对不难了。如上所述，将局部贡献分配到全局矩阵和右手边，也是以与[2.x.75]中相同的方式处理悬挂节点约束。
*


* [1.x.86]

* 下一步是 "压缩 "向量和系统矩阵。这意味着每个进程将对矩阵和向量中那些自己不拥有的条目所做的补充发送给拥有这些条目的进程。在收到其他进程的这些加法后，每个进程再把它们加到它已经拥有的值上。这些加法是将生活在几个单元上的形状函数的积分贡献结合起来，就像在串行计算中一样，不同的是这些单元被分配给不同的进程。
*


* [1.x.87]
*
* 全局矩阵和右手边的向量现在已经形成。我们仍然要应用边界值，方法与我们在[2.x.76]、[2.x.77]和其他一些程序中的方法相同。     
* 下面调用[2.x.78]的最后一个参数允许进行一些优化。它控制我们是否应该删除对应于边界节点的矩阵列中的条目（即设置为零），或者保留它们（通过[2.x.79]意味着：是的，消除这些列）。如果我们消除了列，那么结果矩阵将再次成为对称的，如果我们不这样做，那么它将不会。不过，结果系统的解应该是一样的。我们想让系统重新成为对称的唯一原因是我们想使用CG方法，该方法只对对称矩阵有效。我们可能[1.x.88]想让矩阵对称的原因是，这将要求我们写进实际存在于其他进程中的列项，即涉及到数据的交流。这总是很昂贵的。     
* 经验告诉我们，如果我们不删除与边界节点相关的列，CG也可以工作（而且工作得几乎一样好），这可以用这种特殊的非对称性结构来解释。为了避免通信的费用，我们因此不消除受影响列中的条目。
*


* [1.x.89]
*
* [1.x.90] [1.x.91]。


*
* 组建了线性系统后，我们接下来需要解决它。PETSc提供了各种顺序和并行求解器，我们为这些求解器编写了包装器，其界面与之前所有示例程序中使用的deal.II求解器几乎相同。因此，下面的代码看起来应该相当熟悉。   
* 在函数的顶部，我们设置了一个收敛监视器，并为它指定了我们希望解决线性系统的精度。接下来，我们使用PETSc的CG求解器创建一个实际的求解器对象，该求解器也适用于并行（分布式）矢量和矩阵。最后是一个预处理程序；我们选择使用一个块状雅可比预处理程序，它通过计算矩阵的每个对角线块的不完全LU分解来工作。  换句话说，每个MPI进程从其存储的行中计算出一个ILU，丢掉与本地未存储的行指数相对应的列；这就产生了一个方形矩阵块，我们可以从中计算出一个ILU。这意味着如果你只用一个进程来运行程序，那么你将使用一个ILU(0)作为预处理程序，而如果它在许多进程上运行，那么我们将在对角线上有许多块，预处理程序是这些块中每个块的ILU(0)。在每个处理器只有一个自由度的极端情况下，这个预处理程序只是一个雅可比预处理程序，因为对角线矩阵块只由一个条目组成。这样的预处理程序相对容易计算，因为它不需要处理器之间的任何形式的通信，但一般来说，对于大量的处理器来说，它的效率并不高。)   
* 按照这样的设置，我们就可以解决线性系统。
*


* [1.x.92]
*
* 下一步是分配悬挂的节点约束。这有点麻烦，因为要填入一个约束节点的值，你需要访问它所约束的节点的值（例如，对于2d中的Q1元素，我们需要访问悬挂节点面大边上的两个节点，以计算中间的约束节点的值）。     
* 问题是，我们已经以这样的方式建立了我们的向量（在[2.x.80]中），即每个进程只负责存储解向量中对应于该进程 "拥有 "的自由度的那些元素。然而，在有些情况下，为了计算一个进程中受限自由度的向量项的值，我们需要访问存储在其他进程中的向量项。  PETSc（以及它所基于的MPI模型）不允许简单地查询存储在其他进程上的向量条目，所以我们在这里所做的是获得一个 "分布式 "向量的副本，我们将所有元素存储在本地。这很简单，因为deal.II包装器有一个针对deal.II Vector类的转换构造函数。这种转换当然需要通信，但实质上每个进程只需要将其数据批量发送给其他每个进程一次，而不需要对单个元素的查询做出回应）。
*


* [1.x.93]
*
* 当然，和以前的讨论一样，如果你想在大量的进程上解决大问题，这样的步骤显然不能很好地扩展，因为现在每个进程都存储了[1.x.94]的解向量。(我们将在[2.x.81]中展示如何更好地做到这一点。) 另一方面，在这个本地副本上分配悬挂节点约束很简单，使用通常的函数[2.x.82] 特别是，我们可以计算[1.x.95]约束自由度的值，无论当前进程是否拥有它们。
*


* [1.x.96]
*
* 然后把所有的东西都转回全局向量中。下面的操作是复制那些我们在本地存储在分布式解决方案中的本地化解决方案的元素，而不碰其他的元素。由于我们在所有处理器上做同样的操作，我们最终得到一个分布式向量（即在每个进程上只存储对应于该进程所拥有的自由度的向量项的向量），该向量的所有约束节点是固定的。     
* 我们通过返回收敛所花的迭代次数来结束这个函数，以允许一些输出。
*


* [1.x.97]
*
* [1.x.98] [1.x.99]。


*
* 使用某种细化指标，可以对网格进行细化。问题基本上与分布悬挂节点约束相同：为了计算误差指标（即使我们只是对当前进程拥有的单元格的指标感兴趣），我们需要访问解向量的更多元素，而不仅仅是当前处理器存储的那些。为了实现这一点，我们基本上做了我们在[2.x.83]中已经做过的事情，即在每个进程中获取[1.x.100]的解向量副本，并使用它来计算。如上所述，这本身就很昂贵，而且特别没有必要，因为我们刚刚在[2.x.84]中创建并销毁了这样一个向量，但效率不是这个程序的重点，所以让我们选择一个设计，其中每个函数都是尽可能独立的。   
* 一旦我们有了这样一个包含[1.x.101]解向量元素的 "本地化 "向量，我们就可以计算属于当前过程的单元的指标。事实上，我们当然可以计算[1.x.102]细化指标，因为我们的Triangulation和DoFHandler对象存储了所有单元的信息，而且我们有一个完整的解向量副本。但是为了展示如何平行操作，让我们演示一下，如果只计算[1.x.103]错误指标，然后与其他进程交换剩余的指标，会如何操作。最终，每个进程都需要一套完整的细化指标，因为每个进程都需要细化他们的网格，并且需要以与所有其他进程完全相同的方式细化它）。   
* 因此，要做到这一切，我们需要。
*


*
* - 首先，获得一份分布式解决方案向量的本地副本。
*


*
* - 第二，创建一个矢量来存储细化指标。
*


*
* - 第三，让KellyErrorEstimator计算属于当前子域/过程的所有单元的细化指标。调用的最后一个参数表明我们对哪个子域感兴趣。在它之前的三个参数是其他各种默认参数，通常不需要（也不说明数值，而是使用默认值），但我们必须在这里明确说明，因为我们想修改下面一个参数的值（即表示子域的参数）。
*


* [1.x.104]
*
* 现在，所有进程都计算了自己单元的错误指标，并将其存储在[2.x.85]向量的各个元素中。这个向量中不属于本进程的单元的元素为零。然而，由于所有进程都有整个三角形的副本，并需要保持这些副本的同步性，他们需要三角形所有单元的细化指标值。因此，我们需要分配我们的结果。我们通过创建一个分布式向量来做到这一点，每个进程都有自己的份额，并设置它所计算的元素。因此，当你把这个向量看作是一个存在于所有进程中的向量时，那么这个向量的每个元素都被设置过一次。然后，我们可以将这个并行向量分配给每个进程上的一个本地非并行向量，使[1.x.105]错误指标在每个进程上都可用。     
* 所以在第一步，我们需要设置一个并行向量。为了简单起见，每个进程都将拥有一个元素块，其数量与该进程拥有的单元格一样多，因此第一个元素块被存储在进程0，下一个元素块被存储在进程1，以此类推。然而，需要注意的是，这些元素不一定是我们要写入的元素。这是单元格排列顺序的结果，也就是说，向量中的元素对应单元格的顺序并不是根据这些单元格所属的子域来排序的。换句话说，如果在这个过程中，我们计算某个子域的单元的指标，我们可能会把结果写到分布式向量的或多或少的随机元素中；特别是，它们不一定位于我们在这个过程中拥有的向量块中。它们随后将不得不被复制到另一个进程的内存空间中，当我们调用[2.x.86]函数时，PETSc为我们做了这项操作。这种低效率可以通过更多的代码来避免，但我们避免这样做，因为它不是程序总运行时间的主要因素。     
* 所以我们是这样做的：计算有多少个单元属于这个过程，用这么多的元素建立一个分布式向量存储在本地，将我们在本地计算的元素复制过去，最后将结果压缩。事实上，我们实际上只复制了非零的元素，所以我们可能会错过一些我们计算为零的元素，但这不会有什么影响，因为反正矢量的原始值是零。
*


* [1.x.106]
*
* 所以现在我们有了这个分布式向量，它包含了所有单元的细化指标。为了使用它，我们需要获得一个本地副本，然后用它来标记要细化或粗化的单元，并实际进行细化和粗化。重要的是要认识到，[1.x.107]进程对它自己的三角图副本做了这件事，并且以完全相同的方式进行。
*


* [1.x.108]
*
* [1.x.109] [1.x.110]。


*
* 最后一个有意义的函数是创建图形输出的函数。它的工作方式与[2.x.87]中的相同，但有两个小区别。在讨论这些之前，让我们说明这个函数的一般工作原理：我们打算让所有的数据都在一个单一的进程中产生，然后写入一个文件。正如本程序的许多其他部分已经讨论过的那样，这不是一个可以扩展的东西。之前，我们认为我们会在三角计算、DoFHandlers和解决方案向量的副本方面遇到麻烦，每个进程都必须存储所有的数据，而且会出现一个点，即每个进程根本没有足够的内存来存储这么多数据。在这里，情况是不同的：不仅是内存，而且运行时间也是一个问题。如果一个进程负责处理[1.x.111]的数据，而其他所有的进程什么都不做，那么这一个函数最终会在程序的整个运行时间中占主导地位。  特别是，这个函数花费的时间将与问题的整体大小（以单元数或自由度数计算）成正比，与我们扔给它的进程数量无关。   
* 这种情况需要避免，我们将在[2.x.88]和[2.x.89]中展示如何解决这个问题。对于目前的问题，解决方案是让每个进程只为自己的本地单元产生输出数据，并将它们写入单独的文件，每个进程一个文件。这就是[2.x.90]的操作方式。另外，我们可以简单地把所有的东西放在一组独立的文件中，让可视化软件读取所有的文件（可能也使用多个处理器），并从所有的文件中创建一个单一的可视化；这就是[2.x.91]、[2.x.92]以及后来开发的所有其他并行程序所采取的方法。   
* 更具体地说，对于当前的函数，所有的进程都调用这个函数，但不是所有的进程都需要做与生成输出有关的工作。事实上，它们不应该这样做，因为我们会试图一次多次地写到同一个文件。所以我们只让第一个进程做这件事，而其他所有的进程在这段时间内闲置（或者为下一次迭代开始工作，或者干脆把它们的CPU让给碰巧在同一时间运行的其他作业）。第二件事是，我们不仅要输出解决方案的向量，还要输出一个向量，表明每个单元属于哪个子域。这将使一些分区域的图片变得漂亮。   
* 为了实现这一点，过程零需要一个完整的本地向量中的解决方案组件。就像之前的函数一样，有效的方法是重新使用已经在[2.x.93]函数中创建的向量，但是为了使事情更加自洽，我们在这里只是从分布式解向量中重新创建一个。   
* 需要认识到的一个重要问题是，我们在所有进程中都做了这个定位操作，而不仅仅是实际需要数据的那一个。然而，这一点是无法避免的，我们在这个教程程序中对向量使用的MPI的简化通信模型。MPI没有办法查询另一个进程的数据，双方必须在同一时间启动通信。因此，即使大多数进程不需要本地化的解决方案，我们也必须把将分布式转换为本地化向量的语句放在所有进程中执行。   
*（这项工作的一部分事实上可以避免。我们所做的是将所有进程的局部部分发送给所有其他进程。我们真正需要做的是在所有进程上发起一个操作，每个进程只需将其本地的数据块发送给进程0，因为只有这个进程真正需要它，也就是说，我们需要类似于收集操作的东西。PETSc可以做到这一点，但是为了简单起见，我们在这里并不试图利用这一点。我们没有这样做，因为我们所做的事情在整个计划中并不昂贵：它是所有进程之间的一个矢量通信，这必须与我们在求解线性系统、为预处理程序设置块状ILU以及其他操作时必须进行的通信数量相比较）。)
*


* [1.x.112]
*
* 这样做了，进程0继续设置输出文件，如[2.x.94]，并将（本地化的）解决方案向量附加到输出对象上。
*


* [1.x.113]
*
* 我们在这里做的唯一其他事情是，我们也为每个单元格输出一个值，表明它属于哪个子域（即MPI进程）。这需要一些转换工作，因为库提供给我们的数据不是输出类所期望的数据，但这并不困难。首先，设置一个整数向量，每个单元格一个，然后由每个单元格的子域id填充。         
* 在第二步中，这个向量的元素被转换为浮点向量，这个向量被添加到DataOut对象中，然后去创建VTK格式的输出。
*


* [1.x.114]
*
* [1.x.115] [1.x.116]。


*
* 最后，这里是驱动函数。它与[2.x.95]几乎完全没有变化，只是我们替换了[2.x.96]流。除此以外，唯一的表面变化是我们输出了每个进程有多少个自由度，以及线性求解器花了多少次收敛。
*


* [1.x.117]
*
* [1.x.118] [1.x.119]。


*
* [2.x.97]的工作方式与其他例子程序中的大多数主函数相同，即它将工作委托给管理对象的[2.x.98]函数，只将所有的东西包装成一些代码来捕捉异常。
*


* [1.x.120]

* 这里是唯一真正的区别。MPI和PETSc都要求我们在程序开始时初始化这些库，并在结束时解除初始化。MPI_InitFinalize类处理了所有这些。后面的参数`1`意味着我们确实想用单线程运行每个MPI进程，这是PETSc并行线性代数的前提条件。
*


* [1.x.121]
* [1.x.122][1.x.123] 。


*
* 如果上述程序被编译并在单处理器机器上运行，它产生的结果应该与我们已经得到的[2.x.99]非常相似。然而，如果我们在多核机器或计算机集群上运行它，它就会变得更有趣。运行MPI程序的最基本方法是使用命令行，如
* [1.x.124]
* 在32个处理器上运行[2.x.100]的可执行程序。
* 如果你在一个集群上工作，那么中间通常有一个步骤，你需要设置一个工作脚本，并将该脚本提交给调度器。只要调度器能够为你的工作分配32个未使用的处理器，它就会执行这个脚本。如何编写这样的作业脚本在不同的集群中是不同的，你应该找到你的集群的文档来了解如何做到这一点。在我的系统上，我必须使用[2.x.101]这个命令，加上一大堆选项来并行运行一个作业）。)
* 无论是直接还是通过调度器，如果你在8个处理器上运行这个程序，你应该得到如下的输出。
* [1.x.125]
* (这次运行比examples/目录中的代码多用了一些细化周期。该运行还使用了2004年的METIS版本，产生了不同的分区；因此，你今天得到的数字略有不同）。)
* 可以看出，我们可以很容易地达到近400万个未知数。事实上，该代码在8个进程中的运行时间不到7分钟，直到（包括）第14个周期，14分钟包括倒数第二步。我失去了最后一步的时间信息，但你会明白的。所有这些都是在通过运行[2.x.102]启用发布模式之后，并且由于上述程序注释中提到的原因，关闭了图形输出的生成。 ([2.x.103]我做的最大的2D计算大约有710万个未知数，在32个进程中完成。毫不奇怪，一个人可以走多远的限制因素是他有多少内存，因为每个进程都必须持有整个网格和DoFHandler对象，尽管矩阵和向量被分割了。对于7.1M的计算，每个未知数的内存消耗约为600字节，这还算不错，但我们必须考虑到这是针对每个未知数的，无论我们是否在本地存储矩阵和向量ntries。
*

*
* 以下是在程序的第12个周期产生的一些输出，即大约有30万个未知数。
* [2.x.104]
* 正如人们所希望的那样，这里显示的X-（左）和Y-位移（右）与我们在[2.x.105]中看到的情况非常接近。正如那里和[2.x.106]中所示，我们也可以制作位移场的矢量图，而不是把它绘制成两个独立的标量场。不过，更有趣的是，在这一步看一下网格和分区的情况。
* [2.x.107]
* 同样，网格（左边）显示了与之前相同的细化模式。右边的面板显示了8个过程中的域的划分，每个过程用不同的颜色表示。图中显示，在网格单元较小的地方，子域较小，考虑到分区算法试图平衡每个子域的单元数，这一事实是需要预期的；这种平衡在上图的输出中也很容易识别，其中每个子域的度数大致相同。
*

*
* 值得注意的是，如果我们用不同数量的进程来运行同一个程序，我们可能会得到稍微不同的输出：不同的网格，不同的未知数和迭代收敛的次数。其原因是，虽然矩阵和右手边是相同的，不受所用进程数量的影响，但预处理程序不是：它对每个处理器的[2.x.108]矩阵块分别执行ILU(0)[2.x.109]。因此，随着进程数的增加，它作为预处理程序的有效性会降低，这使得迭代次数增加。由于不同的预处理程序导致计算出的解有细微的变化，这将导致细化时标记的网格单元略有不同，在随后的步骤中差异更大。不过，解决方案看起来总是非常相似的。
*

*
* 最后，这里是3D模拟的一些结果。你可以通过改变以下内容来重复这些结果
* [1.x.126]
* 改为
* [1.x.127]
* 在主函数中。然后，如果你并行运行该程序，你会得到类似这样的结果（这是针对有16个进程的工作）。
* [1.x.128]
*
*

*
* 最后一步，达到150万个未知数，在8台双处理器机器（2003年可用的那种）上使用16个进程，大约需要55分钟。这项工作产生的图形输出相当大（第5周期已经打印了大约82MB的数据），所以我们要显示第4周期的输出。
* [2.x.110]
*


* 左图显示的是将立方体划分为16个过程，右图显示的是沿两个切面通过立方体的X位移。
*


* [1.x.129][1.x.130][1.x.131] 。
*

* 该程序在每个处理器上都保留一份三角形和DoFHandler对象的完整副本。它还创建了解决方案向量的完整副本，并且只在一个处理器上创建输出。就并行化而言，所有这些显然都是瓶颈。
* 在内部，在deal.II中，分级和非结构化三角计算中使用的数据结构的并行化是一个困难的问题，我们花了几年时间才实现了这一点。[2.x.111]教程程序和[2.x.112]文档模块讲述了如何完成这些步骤，以及从应用角度来看需要做什么。当前程序的一个明显的扩展是使用这个功能将计算完全分配给比这里更多的处理器。
*

* [1.x.132][1.x.133] [2.x.113]。
* [0.x.1]

